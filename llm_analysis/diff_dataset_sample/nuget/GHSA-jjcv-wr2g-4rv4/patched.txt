# ====================================================================
# FILE: src/coreclr/debug/daccess/dacdbiimpl.cpp
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-4817 ---
     1| #include "stdafx.h"
     2| #include "dacdbiinterface.h"
     3| #include "typestring.h"
     4| #include "holder.h"
     5| #include "debuginfostore.h"
     6| #include "peimagelayout.inl"
     7| #include "encee.h"
     8| #include "switches.h"
     9| #include "generics.h"
    10| #include "stackwalk.h"
    11| #include "virtualcallstub.h"
    12| #include "dacdbiimpl.h"
    13| #ifdef FEATURE_COMINTEROP
    14| #include "runtimecallablewrapper.h"
    15| #include "comcallablewrapper.h"
    16| #endif // FEATURE_COMINTEROP
    17| #include "request_common.h"
    18| IDacDbiInterface::IAllocator * g_pAllocator = NULL;
    19| forDbiWorker forDbi;
    20| void * operator new(size_t lenBytes, const forDbiWorker &)
    21| {
    22|     _ASSERTE(g_pAllocator != NULL);
    23|     void *result = g_pAllocator->Alloc(lenBytes);
    24|     if (result == NULL)
    25|     {
    26|         ThrowOutOfMemory();
    27|     }
    28|     return result;
    29| }
    30| void * operator new[](size_t lenBytes, const forDbiWorker &)
    31| {
    32|     _ASSERTE(g_pAllocator != NULL);
    33|     void *result = g_pAllocator->Alloc(lenBytes);
    34|     if (result == NULL)
    35|     {
    36|         ThrowOutOfMemory();
    37|     }
    38|     return result;
    39| }
    40| void operator delete(void *p, const forDbiWorker &)
    41| {
    42|     if (p == NULL)
    43|     {
    44|         return;
    45|     }
    46|     _ASSERTE(g_pAllocator != NULL);
    47|     g_pAllocator->Free((BYTE*) p);
    48| }
    49| void operator delete[](void *p, const forDbiWorker &)
    50| {
    51|     if (p == NULL)
    52|     {
    53|         return;
    54|     }
    55|     _ASSERTE(g_pAllocator != NULL);
    56|     g_pAllocator->Free((BYTE*) p);
    57| }
    58| template<class T> void DeleteDbiMemory(T *p)
    59| {
    60|     if (p == NULL)
    61|     {
    62|         return;
    63|     }
    64|     p->~T();
    65|     _ASSERTE(g_pAllocator != NULL);
    66|     g_pAllocator->Free((BYTE*) p);
    67| }
    68| void* AllocDbiMemory(size_t size)
    69| {
    70|     void *result;
    71|     if (g_pAllocator != nullptr)
    72|     {
    73|         result = g_pAllocator->Alloc(size);
    74|     }
    75|     else
    76|     {
    77|         result = new (nothrow) BYTE[size];
    78|     }
    79|     if (result == NULL)
    80|     {
    81|         ThrowOutOfMemory();
    82|     }
    83|     return result;
    84| }
    85| void DeleteDbiMemory(void* p)
    86| {
    87|     if (p == NULL)
    88|     {
    89|         return;
    90|     }
    91|     if (g_pAllocator != nullptr)
    92|     {
    93|         g_pAllocator->Free((BYTE*)p);
    94|     }
    95|     else
    96|     {
    97|         ::delete [] (BYTE*)p;
    98|     }
    99| }
   100| template<class T> void DeleteDbiArrayMemory(T *p, int count)
   101| {
   102|     if (p == NULL)
   103|     {
   104|         return;
   105|     }
   106|     for (T *cur = p; cur < p + count; cur++)
   107|     {
   108|         cur->~T();
   109|     }
   110|     _ASSERTE(g_pAllocator != NULL);
   111|     g_pAllocator->Free((BYTE*) p);
   112| }
   113| STDAPI
   114| DLLEXPORT
   115| DacDbiInterfaceInstance(
   116|     ICorDebugDataTarget * pTarget,
   117|     CORDB_ADDRESS baseAddress,
   118|     IDacDbiInterface::IAllocator * pAllocator,
   119|     IDacDbiInterface::IMetaDataLookup * pMetaDataLookup,
   120|     IDacDbiInterface ** ppInterface)
   121| {
   122|     SUPPORTS_DAC_HOST_ONLY;
   123|     if ((ppInterface == NULL) || (pTarget == NULL) || (baseAddress == 0))
   124|     {
   125|         return E_INVALIDARG;
   126|     }
   127|     *ppInterface = NULL;
   128|     DacDbiInterfaceImpl * pDac = new (nothrow) DacDbiInterfaceImpl(pTarget, baseAddress, pAllocator, pMetaDataLookup);
   129|     if (!pDac)
   130|     {
   131|         return E_OUTOFMEMORY;
   132|     }
   133|     HRESULT hrStatus = pDac->Initialize();
   134|     if (SUCCEEDED(hrStatus))
   135|     {
   136|         *ppInterface = pDac;
   137|     }
   138|     else
   139|     {
   140|         delete pDac;
   141|     }
   142|     return hrStatus;
   143| }
   144| DacDbiInterfaceImpl::DacDbiInterfaceImpl(
   145|     ICorDebugDataTarget* pTarget,
   146|     CORDB_ADDRESS baseAddress,
   147|     IAllocator * pAllocator,
   148|     IMetaDataLookup * pMetaDataLookup
   149| ) : ClrDataAccess(pTarget),
   150|     m_pAllocator(pAllocator),
   151|     m_pMetaDataLookup(pMetaDataLookup),
   152|     m_pCachedPEAssembly(VMPTR_PEAssembly::NullPtr()),
   153|     m_pCachedImporter(NULL),
   154|     m_isCachedHijackFunctionValid(FALSE)
   155| {
   156|     _ASSERTE(baseAddress != NULL);
   157|     m_globalBase = CORDB_ADDRESS_TO_TADDR(baseAddress);
   158|     _ASSERTE(pMetaDataLookup != NULL);
   159|     _ASSERTE(pAllocator != NULL);
   160|     _ASSERTE(pTarget != NULL);
   161| #ifdef _DEBUG
   162|     m_fEnableDllVerificationAsserts = true;
   163| #endif
   164| }
   165| DacDbiInterfaceImpl::~DacDbiInterfaceImpl()
   166| {
   167|     SUPPORTS_DAC_HOST_ONLY;
   168| }
   169| interface IMDInternalImport* DacDbiInterfaceImpl::GetMDImport(
   170|     const PEAssembly* pPEAssembly,
   171|     const ReflectionModule * pReflectionModule,
   172|     bool fThrowEx)
   173| {
   174|     SUPPORTS_DAC;
   175|     IDacDbiInterface::IMetaDataLookup * pLookup = m_pMetaDataLookup;
   176|     _ASSERTE(pLookup != NULL);
   177|     VMPTR_PEAssembly vmPEAssembly = VMPTR_PEAssembly::NullPtr();
   178|     if (pPEAssembly != NULL)
   179|     {
   180|         vmPEAssembly.SetHostPtr(pPEAssembly);
   181|     }
   182|     else if (pReflectionModule != NULL)
   183|     {
   184|         vmPEAssembly.SetHostPtr(pReflectionModule->GetPEAssembly());
   185|     }
   186|     if (m_pCachedPEAssembly == vmPEAssembly)
   187|     {
   188|         return m_pCachedImporter;
   189|     }
   190|     IMDInternalImport * pInternal = NULL;
   191|     bool isILMetaDataForNI = false;
   192|     EX_TRY
   193|     {
   194|         pInternal = pLookup->LookupMetaData(vmPEAssembly, isILMetaDataForNI);
   195|     }
   196|     EX_CATCH
   197|     {
   198|         if ((GET_EXCEPTION()->GetHR() != HRESULT_FROM_WIN32(ERROR_PARTIAL_COPY)) &&
   199|             (GET_EXCEPTION()->GetHR() != CORDBG_E_READVIRTUAL_FAILURE) &&
   200|             (GET_EXCEPTION()->GetHR() != CORDBG_E_SYMBOLS_NOT_AVAILABLE) &&
   201|             (GET_EXCEPTION()->GetHR() != CORDBG_E_MODULE_LOADED_FROM_DISK))
   202|         {
   203|             EX_RETHROW;
   204|         }
   205|     }
   206|     EX_END_CATCH(SwallowAllExceptions)
   207|     if (pInternal == NULL)
   208|     {
   209|         SIMPLIFYING_ASSUMPTION(!"MD lookup failed");
   210|         if (fThrowEx)
   211|         {
   212|             ThrowHR(E_FAIL);
   213|         }
   214|         return NULL;
   215|     }
   216|     else
   217|     {
   218|         m_pCachedPEAssembly   = vmPEAssembly;
   219|         m_pCachedImporter = pInternal;
   220|     }
   221|     return pInternal;
   222| }
   223| void DacDbiInterfaceImpl::Destroy()
   224| {
   225|     m_pAllocator = NULL;
   226|     this->Release();
   227| }
   228| HRESULT DacDbiInterfaceImpl::CheckDbiVersion(const DbiVersion * pVersion)
   229| {
   230|     DD_ENTER_MAY_THROW;
   231|     if (pVersion->m_dwFormat != kCurrentDbiVersionFormat)
   232|     {
   233|         return CORDBG_E_INCOMPATIBLE_PROTOCOL;
   234|     }
   235|     if ((pVersion->m_dwProtocolBreakingChangeCounter != kCurrentDacDbiProtocolBreakingChangeCounter) ||
   236|         (pVersion->m_dwReservedMustBeZero1 != 0))
   237|     {
   238|         return CORDBG_E_INCOMPATIBLE_PROTOCOL;
   239|     }
   240|     return S_OK;
   241| }
   242| HRESULT DacDbiInterfaceImpl::FlushCache()
   243| {
   244|     DD_NON_REENTRANT_MAY_THROW;
   245|     m_pCachedPEAssembly = VMPTR_PEAssembly::NullPtr();
   246|     m_pCachedImporter = NULL;
   247|     m_isCachedHijackFunctionValid = FALSE;
   248|     HRESULT hr  = ClrDataAccess::Flush();
   249|     _ASSERTE(SUCCEEDED(hr));
   250|     return hr;
   251| }
   252| void DacDbiInterfaceImpl::DacSetTargetConsistencyChecks(bool fEnableAsserts)
   253| {
   254|     ClrDataAccess::SetTargetConsistencyChecks(fEnableAsserts);
   255| }
   256| BOOL DacDbiInterfaceImpl::IsLeftSideInitialized()
   257| {
   258|     DD_ENTER_MAY_THROW;
   259|     if (g_pDebugger != NULL)
   260|     {
   261|         return (g_pDebugger->m_fLeftSideInitialized != 0);
   262|     }
   263|     return FALSE;
   264| }
   265| BOOL DacDbiInterfaceImpl::IsTransitionStub(CORDB_ADDRESS address)
   266| {
   267|     DD_ENTER_MAY_THROW;
   268|     BOOL fIsStub = FALSE;
   269| #if defined(TARGET_UNIX)
   270|     ThrowHR(E_NOTIMPL);
   271| #else // !TARGET_UNIX
   272|     TADDR ip = (TADDR)address;
   273|     if (ip == NULL)
   274|     {
   275|         fIsStub = FALSE;
   276|     }
   277|     else
   278|     {
   279|         fIsStub = StubManager::IsStub(ip);
   280|     }
   281|     if (fIsStub == FALSE)
   282|     {
   283|         fIsStub = IsIPInModule(m_globalBase, ip);
   284|     }
   285| #endif // TARGET_UNIX
   286|     return fIsStub;
   287| }
   288| IDacDbiInterface::AddressType DacDbiInterfaceImpl::GetAddressType(CORDB_ADDRESS address)
   289| {
   290|     DD_ENTER_MAY_THROW;
   291|     TADDR taAddr = CORDB_ADDRESS_TO_TADDR(address);
   292|     if (IsPossibleCodeAddress(taAddr) == S_OK)
   293|     {
   294|         if (ExecutionManager::IsManagedCode(taAddr))
   295|         {
   296|             return kAddressManagedMethod;
   297|         }
   298|         if (StubManager::IsStub(taAddr))
   299|         {
   300|             return kAddressRuntimeUnmanagedStub;
   301|         }
   302|     }
   303|     return kAddressUnrecognized;
   304| }
   305| VMPTR_AppDomain DacDbiInterfaceImpl::GetAppDomainFromId(ULONG appdomainId)
   306| {
   307|     DD_ENTER_MAY_THROW;
   308|     VMPTR_AppDomain vmAppDomain;
   309|     IXCLRDataProcess *   pDAC = this;
   310|     ReleaseHolder<IXCLRDataAppDomain> pDacAppDomain;
   311|     HRESULT hrStatus = pDAC->GetAppDomainByUniqueID(appdomainId, &pDacAppDomain);
   312|     IfFailThrow(hrStatus);
   313|     IXCLRDataAppDomain * pIAppDomain = pDacAppDomain;
   314|     AppDomain * pAppDomain = (static_cast<ClrDataAppDomain *> (pIAppDomain))->GetAppDomain();
   315|     SIMPLIFYING_ASSUMPTION(pAppDomain != NULL);
   316|     if (pAppDomain == NULL)
   317|     {
   318|         ThrowHR(E_FAIL); // corrupted left-side?
   319|     }
   320|     TADDR addrAppDomain = PTR_HOST_TO_TADDR(pAppDomain);
   321|     vmAppDomain.SetDacTargetPtr(addrAppDomain);
   322|     return vmAppDomain;
   323| }
   324| ULONG DacDbiInterfaceImpl::GetAppDomainId(VMPTR_AppDomain   vmAppDomain)
   325| {
   326|     DD_ENTER_MAY_THROW;
   327|     if (vmAppDomain.IsNull())
   328|     {
   329|         return 0;
   330|     }
   331|     else
   332|     {
   333|         AppDomain * pAppDomain = vmAppDomain.GetDacPtr();
   334|         return DefaultADID;
   335|     }
   336| }
   337| VMPTR_OBJECTHANDLE DacDbiInterfaceImpl::GetAppDomainObject(VMPTR_AppDomain vmAppDomain)
   338| {
   339|     DD_ENTER_MAY_THROW;
   340|     AppDomain* pAppDomain = vmAppDomain.GetDacPtr();
   341|     OBJECTHANDLE hAppDomainManagedObject = pAppDomain->GetRawExposedObjectHandleForDebugger();
   342|     VMPTR_OBJECTHANDLE vmObj = VMPTR_OBJECTHANDLE::NullPtr();
   343|     vmObj.SetDacTargetPtr(hAppDomainManagedObject);
   344|     return vmObj;
   345| }
   346| void DacDbiInterfaceImpl::GetAppDomainFullName(
   347|     VMPTR_AppDomain   vmAppDomain,
   348|     IStringHolder *   pStrName )
   349| {
   350|     DD_ENTER_MAY_THROW;
   351|     AppDomain * pAppDomain = vmAppDomain.GetDacPtr();
   352|     bool fIsUtf8;
   353|     PVOID pRawName = pAppDomain->GetFriendlyNameNoSet(&fIsUtf8);
   354|     if (!pRawName)
   355|     {
   356|         ThrowHR(E_NOINTERFACE);
   357|     }
   358|     HRESULT hrStatus = S_OK;
   359|     if (fIsUtf8)
   360|     {
   361|         ULONG32 dwNameLen = 0;
   362|         hrStatus = ConvertUtf8((LPCUTF8)pRawName, 0, &dwNameLen, NULL);
   363|         if (SUCCEEDED( hrStatus ))
   364|         {
   365|             NewArrayHolder<WCHAR> pwszName(new WCHAR[dwNameLen]);
   366|             hrStatus = ConvertUtf8((LPCUTF8)pRawName, dwNameLen, &dwNameLen, pwszName );
   367|             IfFailThrow(hrStatus);
   368|             hrStatus =  pStrName->AssignCopy(pwszName);
   369|         }
   370|     }
   371|     else
   372|     {
   373|         hrStatus =  pStrName->AssignCopy(static_cast<PCWSTR>(pRawName));
   374|     }
   375|     IfFailThrow(hrStatus);
   376| }
   377| void DacDbiInterfaceImpl::GetCompilerFlags (
   378|     VMPTR_DomainAssembly vmDomainAssembly,
   379|     BOOL *pfAllowJITOpts,
   380|     BOOL *pfEnableEnC)
   381| {
   382|     DD_ENTER_MAY_THROW;
   383|     DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
   384|     if (pDomainAssembly == NULL)
   385|     {
   386|         ThrowHR(E_FAIL);
   387|     }
   388|     Module * pModule = pDomainAssembly->GetModule();
   389|     DWORD dwBits = pModule->GetDebuggerInfoBits();
   390|     *pfAllowJITOpts = !CORDisableJITOptimizations(dwBits);
   391|     *pfEnableEnC = pModule->IsEditAndContinueEnabled();
   392| } //GetCompilerFlags
   393| bool DacDbiInterfaceImpl::CanSetEnCBits(Module * pModule)
   394| {
   395|     _ASSERTE(pModule != NULL);
   396| #ifdef EnC_SUPPORTED
   397|     bool fIgnorePdbs = ((pModule->GetDebuggerInfoBits() & DACF_IGNORE_PDBS) != 0);
   398|     bool fAllowEnc = pModule->IsEditAndContinueCapable() &&
   399| #ifdef PROFILING_SUPPORTED_DATA
   400|         !CORProfilerPresent() && // this queries target
   401| #endif
   402|         fIgnorePdbs;
   403| #else   // ! EnC_SUPPORTED
   404|     bool fAllowEnc = false;
   405| #endif
   406|     return fAllowEnc;
   407| } // DacDbiInterfaceImpl::SetEnCBits
   408| HRESULT DacDbiInterfaceImpl::SetCompilerFlags(VMPTR_DomainAssembly vmDomainAssembly,
   409|                                            BOOL             fAllowJitOpts,
   410|                                            BOOL             fEnableEnC)
   411| {
   412|     DD_ENTER_MAY_THROW;
   413|     DWORD        dwBits      = 0;
   414|     DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
   415|     Module *     pModule     = pDomainAssembly->GetModule();
   416|     HRESULT      hr          = S_OK;
   417|     _ASSERTE(pModule != NULL);
   418|     dwBits = (pModule->GetDebuggerInfoBits() & ~(DACF_ALLOW_JIT_OPTS | DACF_ENC_ENABLED));
   419|     dwBits &= DACF_CONTROL_FLAGS_MASK;
   420|     if (fAllowJitOpts)
   421|     {
   422|         dwBits |= DACF_ALLOW_JIT_OPTS;
   423|     }
   424|     if (fEnableEnC)
   425|     {
   426|         if (CanSetEnCBits(pModule))
   427|         {
   428|             dwBits |= DACF_ENC_ENABLED;
   429|         }
   430|         else
   431|         {
   432|             hr = CORDBG_S_NOT_ALL_BITS_SET;
   433|         }
   434|     }
   435|     dwBits |= DACF_USER_OVERRIDE;
   436|     pModule->SetDebuggerInfoBits((DebuggerAssemblyControlFlags)dwBits);
   437|     LOG((LF_CORDB, LL_INFO100, "D::HIPCE, Changed Jit-Debug-Info: fOpt=%d, fEnableEnC=%d, new bits=0x%08x\n",
   438|            (dwBits & DACF_ALLOW_JIT_OPTS) != 0,
   439|            (dwBits & DACF_ENC_ENABLED) != 0,
   440|             dwBits));
   441|     _ASSERTE(SUCCEEDED(hr));
   442|     return hr;
   443| } // DacDbiInterfaceImpl::SetCompilerFlags
   444| void DacDbiInterfaceImpl::GetNativeCodeSequencePointsAndVarInfo(VMPTR_MethodDesc  vmMethodDesc,
   445|                                                                 CORDB_ADDRESS     startAddr,
   446|                                                                 BOOL              fCodeAvailable,
   447|                                                                 NativeVarData *   pNativeVarData,
   448|                                                                 SequencePoints *  pSequencePoints)
   449| {
   450|     DD_ENTER_MAY_THROW;
   451|     _ASSERTE(!vmMethodDesc.IsNull());
   452|     MethodDesc * pMD = vmMethodDesc.GetDacPtr();
   453|     _ASSERTE(fCodeAvailable != 0);
   454|     GetNativeVarData(pMD, startAddr, GetArgCount(pMD), pNativeVarData);
   455|     GetSequencePoints(pMD, startAddr, pSequencePoints);
   456| } // GetNativeCodeSequencePointsAndVarInfo
   457| SIZE_T DacDbiInterfaceImpl::GetArgCount(MethodDesc * pMD)
   458| {
   459|     PCCOR_SIGNATURE pCallSig;
   460|     DWORD cbCallSigSize;
   461|     pMD->GetSig(&pCallSig, &cbCallSigSize);
   462|     if (pCallSig == NULL)
   463|     {
   464|         CONSISTENCY_CHECK_MSGF(false, ("Corrupted image, null sig.(%s::%s)",
   465|                                pMD->m_pszDebugClassName, pMD->m_pszDebugMethodName));
   466|         return 0;
   467|     }
   468|     MetaSig msig(pCallSig, cbCallSigSize, pMD->GetModule(), NULL, MetaSig::sigMember);
   469|     UINT32 NumArguments = msig.NumFixedArgs();
   470|     if (!pMD->IsStatic())
   471|     {
   472|         NumArguments++;
   473|     }
   474| /*
   475|     SigParser sigParser(pCallSig, cbCallSigSize);
   476|     sigParser.SkipMethodHeaderSignature(&m_allArgsCount);
   477| */
   478|     return NumArguments;
   479| } //GetArgCount
   480| BYTE* InfoStoreNew(void * pData, size_t cBytes)
   481| {
   482|     return new BYTE[cBytes];
   483| }
   484| void DacDbiInterfaceImpl::GetNativeVarData(MethodDesc *    pMethodDesc,
   485|                                            CORDB_ADDRESS   startAddr,
   486|                                            SIZE_T          fixedArgCount,
   487|                                            NativeVarData * pVarInfo)
   488| {
   489|     if (pVarInfo->IsInitialized())
   490|     {
   491|         return;
   492|     }
   493|     NewArrayHolder<ICorDebugInfo::NativeVarInfo> nativeVars(NULL);
   494|     DebugInfoRequest request;
   495|     request.InitFromStartingAddr(pMethodDesc, CORDB_ADDRESS_TO_TADDR(startAddr));
   496|     ULONG32 entryCount;
   497|     BOOL success = DebugInfoManager::GetBoundariesAndVars(request,
   498|                                                 InfoStoreNew, NULL, // allocator
   499|                                                 NULL, NULL,
   500|                                                 &entryCount, &nativeVars);
   501|     if (!success)
   502|         ThrowHR(E_FAIL);
   503|     pVarInfo->InitVarDataList(nativeVars, (int)fixedArgCount, (int)entryCount);
   504| } // GetNativeVarData
   505| void DacDbiInterfaceImpl::ComposeMapping(const InstrumentedILOffsetMapping * pProfilerILMap, ICorDebugInfo::OffsetMapping nativeMap[], ULONG32* pEntryCount)
   506| {
   507|     ULONG32 entryCount = *pEntryCount;
   508|     if (pProfilerILMap && !pProfilerILMap->IsNull())
   509|     {
   510|         ULONG32 cDuplicate = 0;
   511|         ULONG32 prevILOffset = (ULONG32)(ICorDebugInfo::MAX_ILNUM);
   512|         for (ULONG32 i = 0; i < entryCount; i++)
   513|         {
   514|             ULONG32 origILOffset = TranslateInstrumentedILOffsetToOriginal(nativeMap[i].ilOffset, pProfilerILMap);
   515|             if (origILOffset == prevILOffset)
   516|             {
   517|                 nativeMap[i].ilOffset = (ULONG32)(ICorDebugInfo::MAX_ILNUM);
   518|                 cDuplicate += 1;
   519|             }
   520|             else
   521|             {
   522|                 nativeMap[i].ilOffset = origILOffset;
   523|                 prevILOffset = origILOffset;
   524|             }
   525|         }
   526|         ULONG32 realIndex = 0;
   527|         for (ULONG32 curIndex = 0; curIndex < entryCount; curIndex++)
   528|         {
   529|             if (nativeMap[curIndex].ilOffset != (ULONG32)(ICorDebugInfo::MAX_ILNUM))
   530|             {
   531|                 nativeMap[realIndex] = nativeMap[curIndex];
   532|                 realIndex += 1;
   533|             }
   534|         }
   535|         _ASSERTE((realIndex + cDuplicate) == entryCount);
   536|         entryCount -= cDuplicate;
   537|         *pEntryCount = entryCount;
   538|     }
   539| }
   540| void DacDbiInterfaceImpl::GetSequencePoints(MethodDesc *     pMethodDesc,
   541|                                             CORDB_ADDRESS    startAddr,
   542|                                             SequencePoints * pSeqPoints)
   543| {
   544|     if (pSeqPoints->IsInitialized())
   545|     {
   546|         return;
   547|     }
   548|     DebugInfoRequest request;
   549|     request.InitFromStartingAddr(pMethodDesc, CORDB_ADDRESS_TO_TADDR(startAddr));
   550|     NewArrayHolder<ICorDebugInfo::OffsetMapping> mapCopy(NULL);
   551|     ULONG32 entryCount;
   552|     BOOL success = DebugInfoManager::GetBoundariesAndVars(request,
   553|                                                       InfoStoreNew, NULL, // allocator
   554|                                                       &entryCount, &mapCopy,
   555|                                                       NULL, NULL);
   556|     if (!success)
   557|         ThrowHR(E_FAIL);
   558| #ifdef FEATURE_REJIT
   559|     CodeVersionManager * pCodeVersionManager = pMethodDesc->GetCodeVersionManager();
   560|     ILCodeVersion ilVersion;
   561|     NativeCodeVersion nativeCodeVersion = pCodeVersionManager->GetNativeCodeVersion(dac_cast<PTR_MethodDesc>(pMethodDesc), (PCODE)startAddr);
   562|     if (!nativeCodeVersion.IsNull())
   563|     {
   564|         ilVersion = nativeCodeVersion.GetILCodeVersion();
   565|     }
   566|     if (!ilVersion.IsNull() && !ilVersion.IsDefaultVersion())
   567|     {
   568|         const InstrumentedILOffsetMapping * pRejitMapping = ilVersion.GetInstrumentedILMap();
   569|         ComposeMapping(pRejitMapping, mapCopy, &entryCount);
   570|     }
   571|     else
   572|     {
   573| #endif
   574|         InstrumentedILOffsetMapping loadTimeMapping =
   575|             pMethodDesc->GetModule()->GetInstrumentedILOffsetMapping(pMethodDesc->GetMemberDef());
   576|         ComposeMapping(&loadTimeMapping, mapCopy, &entryCount);
   577| #ifdef FEATURE_REJIT
   578|     }
   579| #endif
   580|     pSeqPoints->InitSequencePoints(entryCount);
   581|     pSeqPoints->CopyAndSortSequencePoints(mapCopy);
   582| } // GetSequencePoints
   583| ULONG DacDbiInterfaceImpl::TranslateInstrumentedILOffsetToOriginal(ULONG                               ilOffset,
   584|                                                                    const InstrumentedILOffsetMapping * pMapping)
   585| {
   586|     SIZE_T               cMap  = pMapping->GetCount();
   587|     ARRAY_PTR_COR_IL_MAP rgMap = pMapping->GetOffsets();
   588|     _ASSERTE((cMap == 0) == (rgMap == NULL));
   589|     if ((cMap == 0) || ((int)ilOffset < 0))
   590|     {
   591|         return ilOffset;
   592|     }
   593|     SIZE_T i = 0;
   594|     for (i = 1; i < cMap; i++)
   595|     {
   596|         if (ilOffset < rgMap[i].newOffset)
   597|         {
   598|             return rgMap[i - 1].oldOffset;
   599|         }
   600|     }
   601|     return rgMap[i - 1].oldOffset;
   602| }
   603| void DacDbiInterfaceImpl::GetILCodeAndSig(VMPTR_DomainAssembly vmDomainAssembly,
   604|                                           mdToken          functionToken,
   605|                                           TargetBuffer *   pCodeInfo,
   606|                                           mdToken *        pLocalSigToken)
   607| {
   608|     DD_ENTER_MAY_THROW;
   609|     DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
   610|     Module *     pModule     = pDomainAssembly->GetModule();
   611|     RVA          methodRVA   = 0;
   612|     DWORD        implFlags;
   613|     pCodeInfo->Clear();
   614|     *pLocalSigToken = mdSignatureNil;
   615|     IfFailThrow(pModule->GetMDImport()->GetMethodImplProps(functionToken,
   616|                                                            &methodRVA,
   617|                                                            &implFlags));
   618|     MethodDesc* pMethodDesc =
   619|         FindLoadedMethodRefOrDef(pModule, functionToken);
   620|     if (methodRVA == 0)
   621|     {
   622|         LOG((LF_CORDB,LL_INFO100000, "DDI::GICAS: Function is not IL - methodRVA == NULL!\n"));
   623|         if(!pMethodDesc || !pMethodDesc->IsIL())
   624|         {
   625|             LOG((LF_CORDB,LL_INFO100000, "DDI::GICAS: And the MD agrees..\n"));
   626|             ThrowHR(CORDBG_E_FUNCTION_NOT_IL);
   627|         }
   628|         else
   629|         {
   630|             LOG((LF_CORDB,LL_INFO100000, "DDI::GICAS: But the MD says it's IL..\n"));
   631|         }
   632|         if (pMethodDesc != NULL && pMethodDesc->GetRVA() == 0)
   633|         {
   634|             LOG((LF_CORDB,LL_INFO100000, "DDI::GICAS: Actually, MD says RVA is 0 too - keep going...!\n"));
   635|         }
   636|     }
   637|     if (IsMiNative(implFlags))
   638|     {
   639|         LOG((LF_CORDB,LL_INFO100000, "DDI::GICAS: Function is not IL - IsMiNative!\n"));
   640|         ThrowHR(CORDBG_E_FUNCTION_NOT_IL);
   641|     }
   642|     *pLocalSigToken = GetILCodeAndSigHelper(pModule, pMethodDesc, functionToken, methodRVA, pCodeInfo);
   643| } // GetILCodeAndSig
   644| mdSignature DacDbiInterfaceImpl::GetILCodeAndSigHelper(Module *       pModule,
   645|                                                        MethodDesc *   pMD,
   646|                                                        mdMethodDef    mdMethodToken,
   647|                                                        RVA            methodRVA,
   648|                                                        TargetBuffer * pIL)
   649| {
   650|     _ASSERTE(pModule != NULL);
   651|     _ASSERTE((pMD == NULL) || ((pMD->GetMemberDef() == mdMethodToken) && (pMD->GetRVA() == methodRVA)));
   652|     TADDR pTargetIL; // target address of start of IL blob
   653|     pTargetIL = pModule->GetDynamicIL(mdMethodToken, TRUE);
   654|     if (pTargetIL == 0 && !pModule->IsReflection())
   655|     {
   656|         pTargetIL = (TADDR)pModule->GetIL(methodRVA);
   657|     }
   658|     mdSignature mdSig = mdSignatureNil;
   659|     if (pTargetIL == 0)
   660|     {
   661|         _ASSERTE(pMD->IsDynamicMethod());
   662|         _ASSERTE(pMD->AsDynamicMethodDesc()->IsLCGMethod()||
   663|                  pMD->AsDynamicMethodDesc()->IsILStub());
   664|         pIL->Clear();
   665|     }
   666|     else
   667|     {
   668|         COR_ILMETHOD * pHostIL = DacGetIlMethod(pTargetIL);     // host address of start of IL blob
   669|         COR_ILMETHOD_DECODER header(pHostIL);                   // host address of header
   670|         pIL->pAddress = pTargetIL + ((SIZE_T)(header.Code) - (SIZE_T)pHostIL);
   671|         pIL->cbSize = header.GetCodeSize();
   672|         if (header.LocalVarSigTok != NULL)
   673|         {
   674|             mdSig = header.GetLocalVarSigTok();
   675|         }
   676|         else
   677|         {
   678|             mdSig = mdSignatureNil;
   679|         }
   680|     }
   681|     return mdSig;
   682| }
   683| bool DacDbiInterfaceImpl::GetMetaDataFileInfoFromPEFile(VMPTR_PEAssembly vmPEAssembly,
   684|                                                         DWORD &dwTimeStamp,
   685|                                                         DWORD &dwSize,
   686|                                                         bool  &isNGEN,
   687|                                                         IStringHolder* pStrFilename)
   688| {
   689|     DD_ENTER_MAY_THROW;
   690|     DWORD dwDataSize;
   691|     DWORD dwRvaHint;
   692|     PEAssembly * pPEAssembly = vmPEAssembly.GetDacPtr();
   693|     _ASSERTE(pPEAssembly != NULL);
   694|     if (pPEAssembly == NULL)
   695|         return false;
   696|     WCHAR wszFilePath[MAX_LONGPATH] = {0};
   697|     DWORD cchFilePath = MAX_LONGPATH;
   698|     bool ret = ClrDataAccess::GetMetaDataFileInfoFromPEFile(pPEAssembly,
   699|                                                             dwTimeStamp,
   700|                                                             dwSize,
   701|                                                             dwDataSize,
   702|                                                             dwRvaHint,
   703|                                                             isNGEN,
   704|                                                             wszFilePath,
   705|                                                             cchFilePath);
   706|     pStrFilename->AssignCopy(wszFilePath);
   707|     return ret;
   708| }
   709| bool DacDbiInterfaceImpl::GetILImageInfoFromNgenPEFile(VMPTR_PEAssembly vmPEAssembly,
   710|                                                        DWORD &dwTimeStamp,
   711|                                                        DWORD &dwSize,
   712|                                                        IStringHolder* pStrFilename)
   713| {
   714|     return false;
   715| }
   716| void DacDbiInterfaceImpl::GetMethodRegionInfo(MethodDesc *             pMethodDesc,
   717|                                               NativeCodeFunctionData * pCodeInfo)
   718| {
   719|     CONTRACTL
   720|     {
   721|         GC_NOTRIGGER;
   722|         PRECONDITION(CheckPointer(pCodeInfo));
   723|     }
   724|     CONTRACTL_END;
   725|     IJitManager::MethodRegionInfo methodRegionInfo = {NULL, 0, NULL, 0};
   726|     PCODE functionAddress = pMethodDesc->GetNativeCode();
   727|     pCodeInfo->m_rgCodeRegions[kHot].pAddress = CORDB_ADDRESS(PCODEToPINSTR(functionAddress));
   728|     if (functionAddress != NULL)
   729|     {
   730|         EECodeInfo codeInfo(functionAddress);
   731|         _ASSERTE(codeInfo.IsValid());
   732|         codeInfo.GetMethodRegionInfo(&methodRegionInfo);
   733|         pCodeInfo->m_rgCodeRegions[kHot].cbSize = (ULONG)methodRegionInfo.hotSize;
   734|         pCodeInfo->m_rgCodeRegions[kCold].Init(PCODEToPINSTR(methodRegionInfo.coldStartAddress),
   735|                                                (ULONG)methodRegionInfo.coldSize);
   736|         _ASSERTE(pCodeInfo->IsValid());
   737|     }
   738|     else
   739|     {
   740|         _ASSERTE(!pCodeInfo->IsValid());
   741|     }
   742| } // GetMethodRegionInfo
   743| void DacDbiInterfaceImpl::GetNativeCodeInfo(VMPTR_DomainAssembly         vmDomainAssembly,
   744|                                             mdToken                  functionToken,
   745|                                             NativeCodeFunctionData * pCodeInfo)
   746| {
   747|     DD_ENTER_MAY_THROW;
   748|     _ASSERTE(pCodeInfo != NULL);
   749|     pCodeInfo->Clear();
   750|     DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
   751|     Module *     pModule     = pDomainAssembly->GetModule();
   752|     MethodDesc* pMethodDesc = FindLoadedMethodRefOrDef(pModule, functionToken);
   753|     pCodeInfo->vmNativeCodeMethodDescToken.SetHostPtr(pMethodDesc);
   754|     if(pMethodDesc != NULL)
   755|     {
   756|         GetMethodRegionInfo(pMethodDesc, pCodeInfo);
   757|         if (pCodeInfo->m_rgCodeRegions[kHot].pAddress != NULL)
   758|         {
   759|             pCodeInfo->isInstantiatedGeneric = pMethodDesc->HasClassOrMethodInstantiation();
   760|             LookupEnCVersions(pModule,
   761|                               pCodeInfo->vmNativeCodeMethodDescToken,
   762|                               functionToken,
   763|                               pCodeInfo->m_rgCodeRegions[kHot].pAddress,
   764|                               &(pCodeInfo->encVersion));
   765|         }
   766|     }
   767| } // GetNativeCodeInfo
   768| void DacDbiInterfaceImpl::GetNativeCodeInfoForAddr(VMPTR_MethodDesc         vmMethodDesc,
   769|                                                    CORDB_ADDRESS            hotCodeStartAddr,
   770|                                                    NativeCodeFunctionData * pCodeInfo)
   771| {
   772|     DD_ENTER_MAY_THROW;
   773|     _ASSERTE(pCodeInfo != NULL);
   774|     if (hotCodeStartAddr == NULL)
   775|     {
   776|         _ASSERTE(!pCodeInfo->IsValid());
   777|         return;
   778|     }
   779|     IJitManager::MethodRegionInfo methodRegionInfo = {NULL, 0, NULL, 0};
   780|     TADDR codeAddr = CORDB_ADDRESS_TO_TADDR(hotCodeStartAddr);
   781| #ifdef TARGET_ARM
   782|     _ASSERTE((codeAddr & THUMB_CODE) == 0);
   783|     codeAddr &= ~THUMB_CODE;
   784| #endif
   785|     EECodeInfo codeInfo(codeAddr);
   786|     _ASSERTE(codeInfo.IsValid());
   787|     EX_TRY_ALLOW_DATATARGET_MISSING_MEMORY
   788|     {
   789|         codeInfo.GetMethodRegionInfo(&methodRegionInfo);
   790|     }
   791|     EX_END_CATCH_ALLOW_DATATARGET_MISSING_MEMORY;
   792|     _ASSERTE(methodRegionInfo.hotStartAddress == codeAddr);
   793|     pCodeInfo->m_rgCodeRegions[kHot].Init(PCODEToPINSTR(methodRegionInfo.hotStartAddress),
   794|                                           (ULONG)methodRegionInfo.hotSize);
   795|     pCodeInfo->m_rgCodeRegions[kCold].Init(PCODEToPINSTR(methodRegionInfo.coldStartAddress),
   796|                                                (ULONG)methodRegionInfo.coldSize);
   797|     _ASSERTE(pCodeInfo->IsValid());
   798|     MethodDesc* pMethodDesc = vmMethodDesc.GetDacPtr();
   799|     pCodeInfo->isInstantiatedGeneric = pMethodDesc->HasClassOrMethodInstantiation();
   800|     pCodeInfo->vmNativeCodeMethodDescToken = vmMethodDesc;
   801|     SIZE_T unusedLatestEncVersion;
   802|     Module * pModule = pMethodDesc->GetModule();
   803|     _ASSERTE(pModule != NULL);
   804|     LookupEnCVersions(pModule,
   805|                       vmMethodDesc,
   806|                       pMethodDesc->GetMemberDef(),
   807|                       codeAddr,
   808|                       &unusedLatestEncVersion, //unused by caller
   809|                       &(pCodeInfo->encVersion));
   810| } // GetNativeCodeInfo
   811| void DacDbiInterfaceImpl::GetTypeHandles(VMPTR_TypeHandle  vmThExact,
   812|                                          VMPTR_TypeHandle  vmThApprox,
   813|                                          TypeHandle *      pThExact,
   814|                                          TypeHandle *      pThApprox)
   815|  {
   816|      _ASSERTE((pThExact != NULL) && (pThApprox != NULL));
   817|      *pThExact = TypeHandle::FromPtr(vmThExact.GetDacPtr());
   818|      *pThApprox = TypeHandle::FromPtr(vmThApprox.GetDacPtr());
   819|     if ((pThApprox->IsNull()) || ((!pThApprox->IsValueType()) && (!pThApprox->IsRestored())))
   820|     {
   821|         LOG((LF_CORDB, LL_INFO10000, "D::GASCI: class isn't loaded.\n"));
   822|         ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
   823|     }
   824|     if (!pThExact->IsNull() && !pThExact->IsRestored())
   825|     {
   826|         *pThExact = TypeHandle();
   827|     }
   828|  }  // DacDbiInterfaceImpl::GetTypeHandles
   829| unsigned int DacDbiInterfaceImpl::GetTotalFieldCount(TypeHandle thApprox)
   830| {
   831|     MethodTable *pMT = thApprox.GetMethodTable();
   832|     unsigned int IFCount = pMT->GetNumIntroducedInstanceFields();
   833|     unsigned int SFCount = pMT->GetNumStaticFields();
   834| #ifdef EnC_SUPPORTED
   835|     PTR_Module pModule = pMT->GetModule();
   836|     if (pModule->IsEditAndContinueEnabled())
   837|     {
   838|         PTR_EnCEEClassData pEncData =
   839|             (dac_cast<PTR_EditAndContinueModule>(pModule))->GetEnCEEClassData(pMT, TRUE);
   840|         if (pEncData != NULL)
   841|         {
   842|             _ASSERTE(pEncData->GetMethodTable() == pMT);
   843|             IFCount += pEncData->GetAddedInstanceFields();
   844|             SFCount += pEncData->GetAddedStaticFields();
   845|         }
   846|     }
   847| #endif
   848|     return IFCount + SFCount;
   849| } // DacDbiInterfaceImpl::GetTotalFieldCount
   850| void DacDbiInterfaceImpl::InitClassData(TypeHandle  thApprox,
   851|                                         BOOL        fIsInstantiatedType,
   852|                                         ClassInfo * pData)
   853| {
   854|     pData->m_fieldList.Alloc(GetTotalFieldCount(thApprox));
   855|     pData->m_objectSize = 0;
   856|     if ((!thApprox.GetNumGenericArgs()) || fIsInstantiatedType)
   857|     {
   858|         pData->m_objectSize = thApprox.GetMethodTable()->GetNumInstanceFieldBytes();
   859|     }
   860| } // DacDbiInterfaceImpl::InitClassData
   861| void DacDbiInterfaceImpl::GetStaticsBases(TypeHandle thExact,
   862|                                          AppDomain * pAppDomain,
   863|                                          PTR_BYTE *  ppGCStaticsBase,
   864|                                          PTR_BYTE *  ppNonGCStaticsBase)
   865|  {
   866|     MethodTable * pMT = thExact.GetMethodTable();
   867|     Module * pModuleForStatics = pMT->GetModuleForStatics();
   868|     if (pModuleForStatics != NULL)
   869|     {
   870|         PTR_DomainLocalModule pLocalModule = pModuleForStatics->GetDomainLocalModule();
   871|         if (pLocalModule != NULL)
   872|         {
   873|             *ppGCStaticsBase = pLocalModule->GetGCStaticsBasePointer(pMT);
   874|             *ppNonGCStaticsBase = pLocalModule->GetNonGCStaticsBasePointer(pMT);
   875|         }
   876|     }
   877| } // DacDbiInterfaceImpl::GetStaticsBases
   878| void DacDbiInterfaceImpl::ComputeFieldData(PTR_FieldDesc pFD,
   879|                                            PTR_BYTE    pGCStaticsBase,
   880|                                            PTR_BYTE    pNonGCStaticsBase,
   881|                                            FieldData * pCurrentFieldData)
   882| {
   883|     pCurrentFieldData->Initialize(pFD->IsStatic(), pFD->IsPrimitive(), pFD->GetMemberDef());
   884| #ifdef EnC_SUPPORTED
   885|     if (pFD->IsEnCNew())
   886|     {
   887|         pCurrentFieldData->m_vmFieldDesc.SetHostPtr(pFD);
   888|         pCurrentFieldData->m_fFldStorageAvailable = FALSE;
   889|         pCurrentFieldData->m_fFldIsTLS = FALSE;
   890|         pCurrentFieldData->m_fFldIsRVA = FALSE;
   891|         pCurrentFieldData->m_fFldIsCollectibleStatic = FALSE;
   892|     }
   893|     else
   894| #endif // EnC_SUPPORTED
   895|     {
   896|         pCurrentFieldData->m_fFldStorageAvailable = TRUE;
   897|         pCurrentFieldData->m_vmFieldDesc.SetHostPtr(pFD);
   898|         pCurrentFieldData->m_fFldIsTLS = (pFD->IsThreadStatic() == TRUE);
   899|         pCurrentFieldData->m_fFldIsRVA = (pFD->IsRVA() == TRUE);
   900|         pCurrentFieldData->m_fFldIsCollectibleStatic = (pFD->IsStatic() == TRUE &&
   901|             pFD->GetEnclosingMethodTable()->Collectible());
   902|         if (pFD->IsStatic())
   903|         {
   904|             if (pFD->IsRVA())
   905|             {
   906|                 DWORD offset = pFD->GetOffset();
   907|                 PTR_VOID addr = pFD->GetModule()->GetRvaField(offset);
   908|                 if (pCurrentFieldData->OkToGetOrSetStaticAddress())
   909|                 {
   910|                     pCurrentFieldData->SetStaticAddress(PTR_TO_TADDR(addr));
   911|                 }
   912|             }
   913|             else if (pFD->IsThreadStatic() ||
   914|                 pCurrentFieldData->m_fFldIsCollectibleStatic)
   915|             {
   916|             }
   917|             else
   918|             {
   919|                 PTR_BYTE base = pFD->IsPrimitive() ? pNonGCStaticsBase : pGCStaticsBase;
   920|                 if (base == NULL)
   921|                 {
   922|                     if (pCurrentFieldData->OkToGetOrSetStaticAddress())
   923|                     {
   924|                         pCurrentFieldData->SetStaticAddress(NULL);
   925|                     }
   926|                 }
   927|                 else
   928|                 {
   929|                     if (pCurrentFieldData->OkToGetOrSetStaticAddress())
   930|                     {
   931|                         pCurrentFieldData->SetStaticAddress(PTR_TO_TADDR(base) + pFD->GetOffset());
   932|                     }
   933|                 }
   934|             }
   935|         }
   936|         else
   937|         {
   938|             if (pCurrentFieldData->OkToGetOrSetInstanceOffset())
   939|             {
   940|                 pCurrentFieldData->SetInstanceOffset(pFD->GetOffset());
   941|             }
   942|         }
   943|     }
   944| } // DacDbiInterfaceImpl::ComputeFieldData
   945| void DacDbiInterfaceImpl::CollectFields(TypeHandle                   thExact,
   946|                                         TypeHandle                   thApprox,
   947|                                         AppDomain *                  pAppDomain,
   948|                                         DacDbiArrayList<FieldData> * pFieldList)
   949| {
   950|     PTR_BYTE pGCStaticsBase = NULL;
   951|     PTR_BYTE pNonGCStaticsBase = NULL;
   952|     if (!thExact.IsNull() && !thExact.GetMethodTable()->Collectible())
   953|     {
   954|         GetStaticsBases(thExact, pAppDomain, &pGCStaticsBase, &pNonGCStaticsBase);
   955|     }
   956|     unsigned int fieldCount = 0;
   957|     EncApproxFieldDescIterator fdIterator(thApprox.GetMethodTable(),
   958|                                           ApproxFieldDescIterator::ALL_FIELDS); // don't fixup EnC (we can't, we're stopped)
   959|     PTR_FieldDesc pCurrentFD;
   960|     unsigned int index = 0;
   961|     while (((pCurrentFD = fdIterator.Next()) != NULL) && (index < pFieldList->Count()))
   962|     {
   963|         ComputeFieldData(pCurrentFD, pGCStaticsBase, pNonGCStaticsBase, &((*pFieldList)[index]));
   964|         fieldCount++;
   965|         index++;
   966|     }
   967|     _ASSERTE(fieldCount == (unsigned int)pFieldList->Count());
   968| } // DacDbiInterfaceImpl::CollectFields
   969| BOOL DacDbiInterfaceImpl::IsValueType (VMPTR_TypeHandle vmTypeHandle)
   970| {
   971|     DD_ENTER_MAY_THROW;
   972|     TypeHandle th = TypeHandle::FromPtr(vmTypeHandle.GetDacPtr());
   973|     return th.IsValueType();
   974| }
   975| BOOL DacDbiInterfaceImpl::HasTypeParams (VMPTR_TypeHandle vmTypeHandle)
   976| {
   977|     DD_ENTER_MAY_THROW;
   978|     TypeHandle th = TypeHandle::FromPtr(vmTypeHandle.GetDacPtr());
   979|     return th.ContainsGenericVariables();
   980| }
   981| void DacDbiInterfaceImpl::GetClassInfo(VMPTR_AppDomain  vmAppDomain,
   982|                                        VMPTR_TypeHandle vmThExact,
   983|                                        ClassInfo *      pData)
   984| {
   985|     DD_ENTER_MAY_THROW;
   986|     AppDomain * pAppDomain = vmAppDomain.GetDacPtr();
   987|     TypeHandle  thExact;
   988|     TypeHandle  thApprox;
   989|     GetTypeHandles(vmThExact, vmThExact, &thExact, &thApprox);
   990|     InitClassData(thApprox, false, pData);
   991|     if (pAppDomain != NULL)
   992|         CollectFields(thExact, thApprox, pAppDomain, &(pData->m_fieldList));
   993| } // DacDbiInterfaceImpl::GetClassInfo
   994| void DacDbiInterfaceImpl::GetInstantiationFieldInfo (VMPTR_DomainAssembly             vmDomainAssembly,
   995|                                                      VMPTR_TypeHandle             vmThExact,
   996|                                                      VMPTR_TypeHandle             vmThApprox,
   997|                                                      DacDbiArrayList<FieldData> * pFieldList,
   998|                                                      SIZE_T *                     pObjectSize)
   999| {
  1000|     DD_ENTER_MAY_THROW;
  1001|     DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
  1002|     _ASSERTE(pDomainAssembly != NULL);
  1003|     AppDomain * pAppDomain = pDomainAssembly->GetAppDomain();
  1004|     TypeHandle  thExact;
  1005|     TypeHandle  thApprox;
  1006|     GetTypeHandles(vmThExact, vmThApprox, &thExact, &thApprox);
  1007|     *pObjectSize = thApprox.GetMethodTable()->GetNumInstanceFieldBytes();
  1008|     pFieldList->Alloc(GetTotalFieldCount(thApprox));
  1009|     CollectFields(thExact, thApprox, pAppDomain, pFieldList);
  1010| } // DacDbiInterfaceImpl::GetInstantiationFieldInfo
  1011| DacDbiInterfaceImpl::TypeDataWalk::TypeDataWalk(DebuggerIPCE_TypeArgData * pData, unsigned int nData)
  1012| {
  1013|     m_pCurrentData = pData;
  1014|     m_nRemaining = nData;
  1015| } // DacDbiInterfaceImpl::TypeDataWalk::TypeDataWalk
  1016| DebuggerIPCE_TypeArgData * DacDbiInterfaceImpl::TypeDataWalk::ReadOne()
  1017| {
  1018|     LIMITED_METHOD_CONTRACT;
  1019|     if (m_nRemaining)
  1020|     {
  1021|         m_nRemaining--;
  1022|         return m_pCurrentData++;
  1023|     }
  1024|     else
  1025|     {
  1026|         return NULL;
  1027|     }
  1028| } // DacDbiInterfaceImpl::TypeDataWalk::ReadOne
  1029| void DacDbiInterfaceImpl::TypeDataWalk::Skip()
  1030| {
  1031|     LIMITED_METHOD_CONTRACT;
  1032|     DebuggerIPCE_TypeArgData * pData = ReadOne();
  1033|     if (pData)
  1034|     {
  1035|         for (unsigned int i = 0; i < pData->numTypeArgs; i++)
  1036|         {
  1037|             Skip();
  1038|         }
  1039|     }
  1040| } // DacDbiInterfaceImpl::TypeDataWalk::Skip
  1041| TypeHandle DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedTypeArg(TypeHandleReadType retrieveWhich)
  1042| {
  1043|     CONTRACTL
  1044|     {
  1045|         NOTHROW;
  1046|         GC_NOTRIGGER;
  1047|     }
  1048|     CONTRACTL_END;
  1049| #if !defined(FEATURE_SHARE_GENERIC_CODE)
  1050|     return ReadLoadedTypeHandle(kGetExact);
  1051| #else
  1052|     if (retrieveWhich == kGetExact)
  1053|         return ReadLoadedTypeHandle(kGetExact);
  1054|     DebuggerIPCE_TypeArgData * pData = ReadOne();
  1055|     if (!pData)
  1056|         return TypeHandle();
  1057|     CorElementType elementType = pData->data.elementType;
  1058|     switch (elementType)
  1059|     {
  1060|         case ELEMENT_TYPE_PTR:
  1061|             _ASSERTE(pData->numTypeArgs == 1);
  1062|             return PtrOrByRefTypeArg(pData, retrieveWhich);
  1063|             break;
  1064|         case ELEMENT_TYPE_CLASS:
  1065|         case ELEMENT_TYPE_VALUETYPE:
  1066|             return ClassTypeArg(pData, retrieveWhich);
  1067|             break;
  1068|         case ELEMENT_TYPE_FNPTR:
  1069|             return FnPtrTypeArg(pData, retrieveWhich);
  1070|             break;
  1071|         default:
  1072|             return ObjRefOrPrimitiveTypeArg(pData, elementType);
  1073|             break;
  1074|     }
  1075| #endif // FEATURE_SHARE_GENERIC_CODE
  1076| } // DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedTypeArg
  1077| BOOL DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedTypeHandles(TypeHandleReadType retrieveWhich,
  1078|                                                               unsigned int       nTypeArgs,
  1079|                                                               TypeHandle *       ppResults)
  1080| {
  1081|     WRAPPER_NO_CONTRACT;
  1082|     BOOL allOK = true;
  1083|     for (unsigned int i = 0; i < nTypeArgs; i++)
  1084|     {
  1085|         ppResults[i] = ReadLoadedTypeArg(retrieveWhich);
  1086|         allOK &= !ppResults[i].IsNull();
  1087|     }
  1088|     return allOK;
  1089| } // DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedTypeHandles
  1090| TypeHandle DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedInstantiation(TypeHandleReadType retrieveWhich,
  1091|                                                                       Module *           pModule,
  1092|                                                                       mdTypeDef          mdToken,
  1093|                                                                       unsigned int       nTypeArgs)
  1094| {
  1095|     WRAPPER_NO_CONTRACT;
  1096|     NewArrayHolder<TypeHandle> pInst(new TypeHandle[nTypeArgs]);
  1097|     if (!ReadLoadedTypeHandles(retrieveWhich, nTypeArgs, pInst))
  1098|     {
  1099|         return TypeHandle();
  1100|     }
  1101|     return FindLoadedInstantiation(pModule, mdToken, nTypeArgs, pInst);
  1102| } // DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedInstantiation
  1103| TypeHandle DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedTypeHandle(TypeHandleReadType retrieveWhich)
  1104| {
  1105|     CONTRACTL
  1106|     {
  1107|         NOTHROW;
  1108|         GC_NOTRIGGER;
  1109|     }
  1110|     CONTRACTL_END;
  1111|     DebuggerIPCE_TypeArgData * pData = ReadOne();
  1112|     if (!pData)
  1113|       return TypeHandle();
  1114|     TypeHandle typeHandle;
  1115|     switch (pData->data.elementType)
  1116|     {
  1117|         case ELEMENT_TYPE_ARRAY:
  1118|         case ELEMENT_TYPE_SZARRAY:
  1119|             typeHandle = ArrayTypeArg(pData, retrieveWhich);
  1120|             break;
  1121|         case ELEMENT_TYPE_PTR:
  1122|         case ELEMENT_TYPE_BYREF:
  1123|             typeHandle = PtrOrByRefTypeArg(pData, retrieveWhich);
  1124|             break;
  1125|         case ELEMENT_TYPE_CLASS:
  1126|         case ELEMENT_TYPE_VALUETYPE:
  1127|             {
  1128|                 Module *     pModule = pData->data.ClassTypeData.vmModule.GetDacPtr();
  1129|                 typeHandle = ReadLoadedInstantiation(retrieveWhich,
  1130|                                                      pModule,
  1131|                                                      pData->data.ClassTypeData.metadataToken,
  1132|                                                      pData->numTypeArgs);
  1133|             }
  1134|             break;
  1135|         case ELEMENT_TYPE_FNPTR:
  1136|             {
  1137|                 typeHandle = FnPtrTypeArg(pData, retrieveWhich);
  1138|             }
  1139|             break;
  1140|     default:
  1141|             typeHandle = FindLoadedElementType(pData->data.elementType);
  1142|         break;
  1143|     }
  1144|     return typeHandle;
  1145| } // DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedTypeHandle
  1146| TypeHandle DacDbiInterfaceImpl::TypeDataWalk::ArrayTypeArg(DebuggerIPCE_TypeArgData * pArrayTypeInfo,
  1147|                                                            TypeHandleReadType         retrieveWhich)
  1148| {
  1149|     TypeHandle arrayElementTypeArg = ReadLoadedTypeArg(retrieveWhich);
  1150|     if (!arrayElementTypeArg.IsNull())
  1151|     {
  1152|         return FindLoadedArrayType(pArrayTypeInfo->data.elementType,
  1153|                                    arrayElementTypeArg,
  1154|                                    pArrayTypeInfo->data.ArrayTypeData.arrayRank);
  1155|     }
  1156|     return TypeHandle();
  1157| } // DacDbiInterfaceImpl::TypeDataWalk::ArrayTypeArg
  1158| TypeHandle DacDbiInterfaceImpl::TypeDataWalk::PtrOrByRefTypeArg(DebuggerIPCE_TypeArgData * pPtrOrByRefTypeInfo,
  1159|                                                                 TypeHandleReadType         retrieveWhich)
  1160| {
  1161|     TypeHandle referentTypeArg = ReadLoadedTypeArg(retrieveWhich);
  1162|     if (!referentTypeArg.IsNull())
  1163|     {
  1164|         return FindLoadedPointerOrByrefType(pPtrOrByRefTypeInfo->data.elementType, referentTypeArg);
  1165|     }
  1166|     return TypeHandle();
  1167| } // DacDbiInterfaceImpl::TypeDataWalk::PtrOrByRefTypeArg
  1168| TypeHandle DacDbiInterfaceImpl::TypeDataWalk::ClassTypeArg(DebuggerIPCE_TypeArgData * pClassTypeInfo,
  1169|                                                            TypeHandleReadType         retrieveWhich)
  1170| {
  1171|     Module *     pModule = pClassTypeInfo->data.ClassTypeData.vmModule.GetDacPtr();
  1172|     TypeHandle   typeDef = ClassLoader::LookupTypeDefOrRefInModule(pModule,
  1173|                                                                    pClassTypeInfo->data.ClassTypeData.metadataToken);
  1174|     if ((!typeDef.IsNull() && typeDef.IsValueType()) || (pClassTypeInfo->data.elementType == ELEMENT_TYPE_VALUETYPE))
  1175|     {
  1176|         return ReadLoadedInstantiation(retrieveWhich,
  1177|                                        pModule,
  1178|                                        pClassTypeInfo->data.ClassTypeData.metadataToken,
  1179|                                        pClassTypeInfo->numTypeArgs);
  1180|     }
  1181|     else
  1182|     {
  1183|         _ASSERTE(retrieveWhich == kGetCanonical);
  1184|         for (unsigned int i = 0; i < pClassTypeInfo->numTypeArgs; i++)
  1185|         {
  1186|             Skip();
  1187|         }
  1188|         return TypeHandle(g_pCanonMethodTableClass);
  1189|     }
  1190| }// DacDbiInterfaceImpl::TypeDataWalk::ClassTypeArg
  1191| TypeHandle DacDbiInterfaceImpl::TypeDataWalk::FnPtrTypeArg(DebuggerIPCE_TypeArgData * pFnPtrTypeInfo,
  1192|                                                            TypeHandleReadType         retrieveWhich)
  1193| {
  1194|     NewArrayHolder<TypeHandle> pInst(new TypeHandle[sizeof(TypeHandle) * pFnPtrTypeInfo->numTypeArgs]);
  1195|     if (ReadLoadedTypeHandles(retrieveWhich, pFnPtrTypeInfo->numTypeArgs, pInst))
  1196|     {
  1197|         return FindLoadedFnptrType(pFnPtrTypeInfo->numTypeArgs, pInst);
  1198|     }
  1199|     return TypeHandle();
  1200| } // DacDbiInterfaceImpl::TypeDataWalk::FnPtrTypeArg
  1201| TypeHandle DacDbiInterfaceImpl::TypeDataWalk::ObjRefOrPrimitiveTypeArg(DebuggerIPCE_TypeArgData * pArgInfo,
  1202|                                                                        CorElementType             elementType)
  1203| {
  1204|     for (unsigned int i = 0; i < pArgInfo->numTypeArgs; i++)
  1205|     {
  1206|         Skip();
  1207|     }
  1208|     if (CorTypeInfo::IsObjRef_NoThrow(elementType))
  1209|     {
  1210|         return TypeHandle(g_pCanonMethodTableClass);
  1211|     }
  1212|     else
  1213|     {
  1214|         return FindLoadedElementType(elementType);
  1215|     }
  1216| } // DacDbiInterfaceImpl::TypeDataWalk::ObjRefOrPrimitiveTypeArg
  1217| TypeHandle DacDbiInterfaceImpl::FindLoadedArrayType(CorElementType arrayType,
  1218|                                                     TypeHandle     typeArg,
  1219|                                                     unsigned       rank)
  1220| {
  1221|     ENABLE_FORBID_GC_LOADER_USE_IN_THIS_SCOPE();
  1222|     if (typeArg.IsNull())
  1223|     {
  1224|         return TypeHandle();
  1225|     }
  1226|     else
  1227|     {
  1228|         return ClassLoader::LoadArrayTypeThrowing(typeArg,
  1229|                                                   arrayType,
  1230|                                                   rank,
  1231|                                                   ClassLoader::DontLoadTypes );
  1232|     }
  1233| } // DacDbiInterfaceImpl::FindLoadedArrayType;
  1234| TypeHandle DacDbiInterfaceImpl::FindLoadedPointerOrByrefType(CorElementType addressType, TypeHandle typeArg)
  1235| {
  1236|     ENABLE_FORBID_GC_LOADER_USE_IN_THIS_SCOPE();
  1237|     return ClassLoader::LoadPointerOrByrefTypeThrowing(addressType,
  1238|                                                        typeArg,
  1239|                                                        ClassLoader::DontLoadTypes);
  1240| } // DacDbiInterfaceImpl::FindLoadedPointerOrByrefType
  1241| TypeHandle DacDbiInterfaceImpl::FindLoadedFnptrType(DWORD numTypeArgs, TypeHandle * pInst)
  1242| {
  1243|     ENABLE_FORBID_GC_LOADER_USE_IN_THIS_SCOPE();
  1244|     return  ClassLoader::LoadFnptrTypeThrowing(0,
  1245|                                                numTypeArgs - 1,
  1246|                                                pInst,
  1247|                                                ClassLoader::DontLoadTypes);
  1248| } // DacDbiInterfaceImpl::FindLoadedFnptrType
  1249| TypeHandle DacDbiInterfaceImpl::FindLoadedInstantiation(Module *     pModule,
  1250|                                                         mdTypeDef    mdToken,
  1251|                                                         DWORD        nTypeArgs,
  1252|                                                         TypeHandle * pInst)
  1253| {
  1254|     ENABLE_FORBID_GC_LOADER_USE_IN_THIS_SCOPE();
  1255|     return ClassLoader::LoadGenericInstantiationThrowing(pModule,
  1256|                                                          mdToken,
  1257|                                                          Instantiation(pInst,nTypeArgs),
  1258|                                                          ClassLoader::DontLoadTypes);
  1259| } // DacDbiInterfaceImpl::FindLoadedInstantiation
  1260| TypeHandle DacDbiInterfaceImpl::FindLoadedElementType(CorElementType elementType)
  1261| {
  1262|     ENABLE_FORBID_GC_LOADER_USE_IN_THIS_SCOPE();
  1263|     MethodTable * pMethodTable = (&g_CoreLib)->GetElementType(elementType);
  1264|     return TypeHandle(pMethodTable);
  1265| } // DacDbiInterfaceImpl::FindLoadedElementType
  1266| void DacDbiInterfaceImpl::GetArrayTypeInfo(TypeHandle                      typeHandle,
  1267|                                            DebuggerIPCE_ExpandedTypeData * pTypeInfo,
  1268|                                            AppDomain *                     pAppDomain)
  1269| {
  1270|     _ASSERTE(typeHandle.IsArray());
  1271|     pTypeInfo->ArrayTypeData.arrayRank = typeHandle.GetRank();
  1272|     TypeHandleToBasicTypeInfo(typeHandle.GetArrayElementTypeHandle(),
  1273|                               &(pTypeInfo->ArrayTypeData.arrayTypeArg),
  1274|                               pAppDomain);
  1275| } // DacDbiInterfaceImpl::GetArrayTypeInfo
  1276| void DacDbiInterfaceImpl::GetPtrTypeInfo(AreValueTypesBoxed              boxed,
  1277|                                          TypeHandle                      typeHandle,
  1278|                                          DebuggerIPCE_ExpandedTypeData * pTypeInfo,
  1279|                                          AppDomain *                     pAppDomain)
  1280| {
  1281|     if (boxed == AllBoxed)
  1282|     {
  1283|         GetClassTypeInfo(typeHandle, pTypeInfo, pAppDomain);
  1284|     }
  1285|     else
  1286|     {
  1287|         _ASSERTE(typeHandle.IsTypeDesc());
  1288|         TypeHandleToBasicTypeInfo(typeHandle.AsTypeDesc()->GetTypeParam(),
  1289|                                   &(pTypeInfo->UnaryTypeData.unaryTypeArg),
  1290|                                   pAppDomain);
  1291|     }
  1292| } // DacDbiInterfaceImpl::GetPtrTypeInfo
  1293| void DacDbiInterfaceImpl::GetFnPtrTypeInfo(AreValueTypesBoxed              boxed,
  1294|                                            TypeHandle                      typeHandle,
  1295|                                            DebuggerIPCE_ExpandedTypeData * pTypeInfo,
  1296|                                            AppDomain *                     pAppDomain)
  1297| {
  1298|     if (boxed == AllBoxed)
  1299|     {
  1300|         GetClassTypeInfo(typeHandle, pTypeInfo, pAppDomain);
  1301|     }
  1302|     else
  1303|     {
  1304|         pTypeInfo->NaryTypeData.typeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
  1305|     }
  1306| } // DacDbiInterfaceImpl::GetFnPtrTypeInfo
  1307| void DacDbiInterfaceImpl::GetClassTypeInfo(TypeHandle                      typeHandle,
  1308|                                            DebuggerIPCE_ExpandedTypeData * pTypeInfo,
  1309|                                            AppDomain *                     pAppDomain)
  1310| {
  1311|     Module * pModule = typeHandle.GetModule();
  1312|     if (typeHandle.HasInstantiation()) // the type handle represents a generic instantiation
  1313|     {
  1314|         pTypeInfo->ClassTypeData.typeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
  1315|     }
  1316|     else // non-generic
  1317|     {
  1318|         pTypeInfo->ClassTypeData.typeHandle = VMPTR_TypeHandle::NullPtr();
  1319|     }
  1320|     pTypeInfo->ClassTypeData.metadataToken = typeHandle.GetCl();
  1321|     _ASSERTE(pModule);
  1322|     pTypeInfo->ClassTypeData.vmModule.SetDacTargetPtr(PTR_HOST_TO_TADDR(pModule));
  1323|     if (pAppDomain)
  1324|     {
  1325|         pTypeInfo->ClassTypeData.vmDomainAssembly.SetDacTargetPtr(PTR_HOST_TO_TADDR(pModule->GetDomainAssembly()));
  1326|     }
  1327|     else
  1328|     {
  1329|         pTypeInfo->ClassTypeData.vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
  1330|     }
  1331| } // DacDbiInterfaceImpl::GetClassTypeInfo
  1332| CorElementType DacDbiInterfaceImpl::GetElementType (TypeHandle typeHandle)
  1333| {
  1334|     if (typeHandle.IsNull())
  1335|     {
  1336|         return ELEMENT_TYPE_VOID;
  1337|     }
  1338|     else if (typeHandle.GetMethodTable() == g_pObjectClass)
  1339|     {
  1340|        return ELEMENT_TYPE_OBJECT;
  1341|     }
  1342|     else if (typeHandle.GetMethodTable() == g_pStringClass)
  1343|     {
  1344|         return ELEMENT_TYPE_STRING;
  1345|     }
  1346|     else
  1347|     {
  1348|         return typeHandle.GetSignatureCorElementType();
  1349|     }
  1350| } // DacDbiInterfaceImpl::GetElementType
  1351| void DacDbiInterfaceImpl::TypeHandleToBasicTypeInfo(TypeHandle                   typeHandle,
  1352|                                                     DebuggerIPCE_BasicTypeData * pTypeInfo,
  1353|                                                     AppDomain *                  pAppDomain)
  1354| {
  1355|     pTypeInfo->elementType = GetElementType(typeHandle);
  1356|     switch (pTypeInfo->elementType)
  1357|     {
  1358|         case ELEMENT_TYPE_ARRAY:
  1359|         case ELEMENT_TYPE_SZARRAY:
  1360|         case ELEMENT_TYPE_FNPTR:
  1361|         case ELEMENT_TYPE_PTR:
  1362|         case ELEMENT_TYPE_BYREF:
  1363|             pTypeInfo->vmTypeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
  1364|             pTypeInfo->metadataToken = mdTokenNil;
  1365|             pTypeInfo->vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
  1366|             break;
  1367|         case ELEMENT_TYPE_CLASS:
  1368|         case ELEMENT_TYPE_VALUETYPE:
  1369|         {
  1370|             Module * pModule = typeHandle.GetModule();
  1371|             if (typeHandle.HasInstantiation())   // only set if instantiated
  1372|             {
  1373|                 pTypeInfo->vmTypeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
  1374|             }
  1375|             else
  1376|             {
  1377|                 pTypeInfo->vmTypeHandle = VMPTR_TypeHandle::NullPtr();
  1378|             }
  1379|             pTypeInfo->metadataToken = typeHandle.GetCl();
  1380|             _ASSERTE(pModule);
  1381|             pTypeInfo->vmModule.SetDacTargetPtr(PTR_HOST_TO_TADDR(pModule));
  1382|             if (pAppDomain)
  1383|             {
  1384|                 pTypeInfo->vmDomainAssembly.SetDacTargetPtr(PTR_HOST_TO_TADDR(pModule->GetDomainAssembly()));
  1385|             }
  1386|             else
  1387|             {
  1388|                 pTypeInfo->vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
  1389|             }
  1390|             break;
  1391|         }
  1392|         default:
  1393|             pTypeInfo->vmTypeHandle = VMPTR_TypeHandle::NullPtr();
  1394|             pTypeInfo->metadataToken = mdTokenNil;
  1395|             pTypeInfo->vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
  1396|             break;
  1397|     }
  1398|     return;
  1399| } // DacDbiInterfaceImpl::TypeHandleToBasicTypeInfo
  1400| void DacDbiInterfaceImpl::GetObjectExpandedTypeInfoFromID(AreValueTypesBoxed boxed,
  1401|                                        VMPTR_AppDomain vmAppDomain,
  1402|                                        COR_TYPEID id,
  1403|                                        DebuggerIPCE_ExpandedTypeData *pTypeInfo)
  1404| {
  1405|     DD_ENTER_MAY_THROW;
  1406|     TypeHandleToExpandedTypeInfoImpl(boxed, vmAppDomain, TypeHandle::FromPtr(TO_TADDR(id.token1)), pTypeInfo);
  1407| }
  1408| void DacDbiInterfaceImpl::GetObjectExpandedTypeInfo(AreValueTypesBoxed boxed,
  1409|                                        VMPTR_AppDomain vmAppDomain,
  1410|                                        CORDB_ADDRESS addr,
  1411|                                        DebuggerIPCE_ExpandedTypeData *pTypeInfo)
  1412| {
  1413|     DD_ENTER_MAY_THROW;
  1414|     PTR_Object obj(TO_TADDR(addr));
  1415|     TypeHandleToExpandedTypeInfoImpl(boxed, vmAppDomain, obj->GetGCSafeTypeHandle(), pTypeInfo);
  1416| }
  1417| void DacDbiInterfaceImpl::TypeHandleToExpandedTypeInfo(AreValueTypesBoxed              boxed,
  1418|                                                        VMPTR_AppDomain                 vmAppDomain,
  1419|                                                        VMPTR_TypeHandle                vmTypeHandle,
  1420|                                                        DebuggerIPCE_ExpandedTypeData * pTypeInfo)
  1421| {
  1422|     DD_ENTER_MAY_THROW;
  1423|     TypeHandle typeHandle = TypeHandle::FromPtr(vmTypeHandle.GetDacPtr());
  1424|     TypeHandleToExpandedTypeInfoImpl(boxed, vmAppDomain, typeHandle, pTypeInfo);
  1425| }
  1426| void DacDbiInterfaceImpl::TypeHandleToExpandedTypeInfoImpl(AreValueTypesBoxed              boxed,
  1427|                                                        VMPTR_AppDomain                 vmAppDomain,
  1428|                                                        TypeHandle                      typeHandle,
  1429|                                                        DebuggerIPCE_ExpandedTypeData * pTypeInfo)
  1430| {
  1431|     AppDomain * pAppDomain = vmAppDomain.GetDacPtr();
  1432|     pTypeInfo->elementType = GetElementType(typeHandle);
  1433|     switch (pTypeInfo->elementType)
  1434|     {
  1435|         case ELEMENT_TYPE_ARRAY:
  1436|         case ELEMENT_TYPE_SZARRAY:
  1437|             GetArrayTypeInfo(typeHandle, pTypeInfo, pAppDomain);
  1438|             break;
  1439|         case ELEMENT_TYPE_PTR:
  1440|         case ELEMENT_TYPE_BYREF:
  1441|             GetPtrTypeInfo(boxed, typeHandle, pTypeInfo, pAppDomain);
  1442|             break;
  1443|         case ELEMENT_TYPE_VALUETYPE:
  1444|             if (boxed == OnlyPrimitivesUnboxed || boxed == AllBoxed)
  1445|             {
  1446|                 pTypeInfo->elementType = ELEMENT_TYPE_CLASS;
  1447|             }
  1448|             GetClassTypeInfo(typeHandle, pTypeInfo, pAppDomain);
  1449|            break;
  1450|         case ELEMENT_TYPE_CLASS:
  1451|             GetClassTypeInfo(typeHandle, pTypeInfo, pAppDomain);
  1452|             break;
  1453|         case ELEMENT_TYPE_FNPTR:
  1454|                 GetFnPtrTypeInfo(boxed, typeHandle, pTypeInfo, pAppDomain);
  1455|                 break;
  1456|         default:
  1457|             if (boxed == AllBoxed)
  1458|             {
  1459|                 pTypeInfo->elementType = ELEMENT_TYPE_CLASS;
  1460|                 GetClassTypeInfo(typeHandle, pTypeInfo, pAppDomain);
  1461|             }
  1462|             break;
  1463|     }
  1464|     LOG((LF_CORDB, LL_INFO10000, "D::THTETI: converted left-side type handle to expanded right-side type info, pTypeInfo->ClassTypeData.typeHandle = 0x%08x.\n", pTypeInfo->ClassTypeData.typeHandle.GetRawPtr()));
  1465|     return;
  1466| } // DacDbiInterfaceImpl::TypeHandleToExpandedTypeInfo
  1467| VMPTR_TypeHandle DacDbiInterfaceImpl::GetTypeHandle(VMPTR_Module vmModule,
  1468|                                                     mdTypeDef metadataToken)
  1469| {
  1470|     DD_ENTER_MAY_THROW;
  1471|     Module* pModule = vmModule.GetDacPtr();
  1472|     VMPTR_TypeHandle vmTypeHandle = VMPTR_TypeHandle::NullPtr();
  1473|     TypeHandle th = ClassLoader::LookupTypeDefOrRefInModule(pModule, metadataToken);
  1474|     if (th.IsNull())
  1475|     {
  1476|         LOG((LF_CORDB, LL_INFO10000, "D::GTH: class isn't loaded.\n"));
  1477|         ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
  1478|     }
  1479|     vmTypeHandle.SetDacTargetPtr(th.AsTAddr());
  1480|     return vmTypeHandle;
  1481| }
  1482| VMPTR_TypeHandle DacDbiInterfaceImpl::GetApproxTypeHandle(TypeInfoList * pTypeData)
  1483| {
  1484|     DD_ENTER_MAY_THROW;
  1485|     LOG((LF_CORDB, LL_INFO10000, "D::GATH: getting info.\n"));
  1486|     TypeDataWalk walk(&((*pTypeData)[0]), pTypeData->Count());
  1487|     TypeHandle typeHandle = walk.ReadLoadedTypeHandle(TypeDataWalk::kGetCanonical);
  1488|     VMPTR_TypeHandle vmTypeHandle = VMPTR_TypeHandle::NullPtr();
  1489|     vmTypeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
  1490|     if (!typeHandle.IsNull())
  1491|     {
  1492|         vmTypeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
  1493|     }
  1494|     else
  1495|     {
  1496|         ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
  1497|     }
  1498|     LOG((LF_CORDB, LL_INFO10000,
  1499|         "D::GATH: sending result, result = 0x%0x8\n",
  1500|         typeHandle));
  1501|     return vmTypeHandle;
  1502| } // DacDbiInterfaceImpl::GetApproxTypeHandle
  1503| HRESULT DacDbiInterfaceImpl::GetExactTypeHandle(DebuggerIPCE_ExpandedTypeData * pTypeData,
  1504|                                                 ArgInfoList *   pArgInfo,
  1505|                                                 VMPTR_TypeHandle& vmTypeHandle)
  1506| {
  1507|     DD_ENTER_MAY_THROW;
  1508|     LOG((LF_CORDB, LL_INFO10000, "D::GETH: getting info.\n"));
  1509|     HRESULT hr = S_OK;
  1510|     EX_TRY
  1511|     {
  1512|         vmTypeHandle = vmTypeHandle.NullPtr();
  1513|         TypeHandle typeHandle = ExpandedTypeInfoToTypeHandle(pTypeData, pArgInfo);
  1514|         _ASSERTE(!typeHandle.IsNull());
  1515|         vmTypeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
  1516|     }
  1517|     EX_CATCH_HRESULT(hr);
  1518|     return hr;
  1519| } // DacDbiInterfaceImpl::GetExactTypeHandle
  1520| void DacDbiInterfaceImpl::GetMethodDescParams(
  1521|     VMPTR_AppDomain     vmAppDomain,
  1522|     VMPTR_MethodDesc    vmMethodDesc,
  1523|     GENERICS_TYPE_TOKEN genericsToken,
  1524|     UINT32 *            pcGenericClassTypeParams,
  1525|     TypeParamsList *    pGenericTypeParams)
  1526| {
  1527|     DD_ENTER_MAY_THROW;
  1528|     if (vmAppDomain.IsNull() || vmMethodDesc.IsNull())
  1529|     {
  1530|         ThrowHR(E_INVALIDARG);
  1531|     }
  1532|     _ASSERTE((pcGenericClassTypeParams != NULL) && (pGenericTypeParams != NULL));
  1533|     MethodDesc * pMD = vmMethodDesc.GetDacPtr();
  1534|     UINT32 cGenericClassTypeParams  = pMD->GetNumGenericClassArgs();
  1535|     UINT32 cGenericMethodTypeParams = pMD->GetNumGenericMethodArgs();
  1536|     UINT32 cTotalGenericTypeParams  = cGenericClassTypeParams + cGenericMethodTypeParams;
  1537|     *pcGenericClassTypeParams = cGenericClassTypeParams;
  1538|     TypeHandle   thSpecificClass;
  1539|     MethodDesc * pSpecificMethod = NULL;
  1540|     BOOL fExact = FALSE;
  1541|     ALLOW_DATATARGET_MISSING_MEMORY(
  1542|         fExact = Generics::GetExactInstantiationsOfMethodAndItsClassFromCallInformation(
  1543|             pMD,
  1544|             PTR_VOID((TADDR)genericsToken),
  1545|             &thSpecificClass,
  1546|             &pSpecificMethod);
  1547|             );
  1548|     if (!fExact ||
  1549|         !thSpecificClass.GetMethodTable()->SanityCheck() ||
  1550|         !pSpecificMethod->GetMethodTable()->SanityCheck())
  1551|     {
  1552|         thSpecificClass = TypeHandle(pMD->GetMethodTable());
  1553|         pSpecificMethod = pMD;
  1554|     }
  1555|     Instantiation classInst  = pSpecificMethod->GetExactClassInstantiation(thSpecificClass);
  1556|     Instantiation methodInst = pSpecificMethod->GetMethodInstantiation();
  1557|     _ASSERTE((classInst.IsEmpty())  == (cGenericClassTypeParams  == 0));
  1558|     _ASSERTE((methodInst.IsEmpty()) == (cGenericMethodTypeParams == 0));
  1559|     pGenericTypeParams->Alloc(cTotalGenericTypeParams);
  1560|     for (UINT32 i = 0; i < cTotalGenericTypeParams; i++)
  1561|     {
  1562|         TypeHandle thCurrent;
  1563|         if (i < cGenericClassTypeParams)
  1564|         {
  1565|             thCurrent = classInst[i];
  1566|         }
  1567|         else
  1568|         {
  1569|             thCurrent = methodInst[i - cGenericClassTypeParams];
  1570|         }
  1571|         EX_TRY_ALLOW_DATATARGET_MISSING_MEMORY_WITH_HANDLER
  1572|         {
  1573|             VMPTR_TypeHandle vmTypeHandle = VMPTR_TypeHandle::NullPtr();
  1574|             vmTypeHandle.SetDacTargetPtr(thCurrent.AsTAddr());
  1575|             TypeHandleToExpandedTypeInfo(NoValueTypeBoxing,
  1576|                                          vmAppDomain,
  1577|                                          vmTypeHandle,
  1578|                                          &((*pGenericTypeParams)[i]));
  1579|         }
  1580|         EX_CATCH_ALLOW_DATATARGET_MISSING_MEMORY_WITH_HANDLER
  1581|         {
  1582|             VMPTR_TypeHandle vmTHCanon = VMPTR_TypeHandle::NullPtr();
  1583|             TypeHandle thCanon = TypeHandle(g_pCanonMethodTableClass);
  1584|             vmTHCanon.SetDacTargetPtr(thCanon.AsTAddr());
  1585|             TypeHandleToExpandedTypeInfo(NoValueTypeBoxing,
  1586|                                          vmAppDomain,
  1587|                                          vmTHCanon,
  1588|                                          &((*pGenericTypeParams)[i]));
  1589|         }
  1590|         EX_END_CATCH_ALLOW_DATATARGET_MISSING_MEMORY_WITH_HANDLER
  1591|     }
  1592| }
  1593| TypeHandle DacDbiInterfaceImpl::GetClassOrValueTypeHandle(DebuggerIPCE_BasicTypeData * pData)
  1594| {
  1595|     TypeHandle typeHandle;
  1596|     if (!pData->vmTypeHandle.IsNull())
  1597|     {
  1598|         typeHandle = TypeHandle::FromPtr(pData->vmTypeHandle.GetDacPtr());
  1599|     }
  1600|     else
  1601|     {
  1602|         DomainAssembly * pDomainAssembly = pData->vmDomainAssembly.GetDacPtr();
  1603|         Module *     pModule = pDomainAssembly->GetModule();
  1604|         typeHandle = ClassLoader::LookupTypeDefOrRefInModule(pModule, pData->metadataToken);
  1605|         if (typeHandle.IsNull())
  1606|         {
  1607|             LOG((LF_CORDB, LL_INFO10000, "D::BTITTH: class isn't loaded.\n"));
  1608|             ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
  1609|         }
  1610|         _ASSERTE(typeHandle.GetNumGenericArgs() == 0);
  1611|     }
  1612|     return typeHandle;
  1613| } // DacDbiInterfaceImpl::GetClassOrValueTypeHandle
  1614| TypeHandle DacDbiInterfaceImpl::GetExactArrayTypeHandle(DebuggerIPCE_ExpandedTypeData * pTopLevelTypeData,
  1615|                                                         ArgInfoList *                   pArgInfo)
  1616| {
  1617|     TypeHandle typeArg;
  1618|     _ASSERTE(pArgInfo->Count() == 1);
  1619|     typeArg = BasicTypeInfoToTypeHandle(&((*pArgInfo)[0]));
  1620|     return FindLoadedArrayType(pTopLevelTypeData->elementType,
  1621|                                typeArg,
  1622|                                pTopLevelTypeData->ArrayTypeData.arrayRank);
  1623| } // DacDbiInterfaceImpl::GetExactArrayTypeHandle
  1624| TypeHandle DacDbiInterfaceImpl::GetExactPtrOrByRefTypeHandle(DebuggerIPCE_ExpandedTypeData * pTopLevelTypeData,
  1625|                                                              ArgInfoList *                   pArgInfo)
  1626| {
  1627|     TypeHandle typeArg;
  1628|     _ASSERTE(pArgInfo->Count() == 1);
  1629|     typeArg = BasicTypeInfoToTypeHandle(&((*pArgInfo)[0]));
  1630|     return FindLoadedPointerOrByrefType(pTopLevelTypeData->elementType, typeArg);
  1631| } // DacDbiInterfaceImpl::GetExactPtrOrByRefTypeHandle
  1632| TypeHandle DacDbiInterfaceImpl::GetExactClassTypeHandle(DebuggerIPCE_ExpandedTypeData * pTopLevelTypeData,
  1633|                                                         ArgInfoList *                   pArgInfo)
  1634| {
  1635|     Module *     pModule = pTopLevelTypeData->ClassTypeData.vmModule.GetDacPtr();
  1636|     int          argCount = pArgInfo->Count();
  1637|     TypeHandle typeConstructor =
  1638|         ClassLoader::LookupTypeDefOrRefInModule(pModule, pTopLevelTypeData->ClassTypeData.metadataToken);
  1639|     if (typeConstructor.IsNull())
  1640|     {
  1641|         LOG((LF_CORDB, LL_INFO10000, "D::ETITTH: class isn't loaded.\n"));
  1642|         ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
  1643|     }
  1644|     if (argCount == 0)
  1645|     {
  1646|         return typeConstructor;
  1647|     }
  1648|     if ((unsigned int)argCount != typeConstructor.GetNumGenericArgs())
  1649|     {
  1650|         LOG((LF_CORDB, LL_INFO10000,
  1651|             "D::ETITTH: wrong number of type parameters, %d given, %d expected\n",
  1652|             argCount, typeConstructor.GetNumGenericArgs()));
  1653|         _ASSERTE((unsigned int)argCount == typeConstructor.GetNumGenericArgs());
  1654|         ThrowHR(E_FAIL);
  1655|     }
  1656|     S_UINT32 allocSize = S_UINT32(argCount) * S_UINT32(sizeof(TypeHandle));
  1657|     if (allocSize.IsOverflow())
  1658|     {
  1659|         ThrowHR(E_OUTOFMEMORY);
  1660|     }
  1661|     NewArrayHolder<TypeHandle> pInst(new TypeHandle[allocSize.Value()]);
  1662|     for (unsigned int i = 0; i < (unsigned int)argCount; i++)
  1663|     {
  1664|         pInst[i] = BasicTypeInfoToTypeHandle(&((*pArgInfo)[i]));
  1665|     }
  1666|     return FindLoadedInstantiation(typeConstructor.GetModule(),
  1667|                                    typeConstructor.GetCl(),
  1668|                                    argCount,
  1669|                                    pInst);
  1670| } // DacDbiInterfaceImpl::GetExactClassTypeHandle
  1671| TypeHandle DacDbiInterfaceImpl::GetExactFnPtrTypeHandle(ArgInfoList * pArgInfo)
  1672| {
  1673|     S_UINT32 allocSize = S_UINT32(pArgInfo->Count()) * S_UINT32(sizeof(TypeHandle));
  1674|     if( allocSize.IsOverflow() )
  1675|     {
  1676|         ThrowHR(E_OUTOFMEMORY);
  1677|     }
  1678|     NewArrayHolder<TypeHandle> pInst(new TypeHandle[allocSize.Value()]);
  1679|     for (unsigned int i = 0; i < pArgInfo->Count(); i++)
  1680|     {
  1681|         pInst[i] = BasicTypeInfoToTypeHandle(&((*pArgInfo)[i]));
  1682|     }
  1683|     return FindLoadedFnptrType(pArgInfo->Count(), pInst);
  1684| } // DacDbiInterfaceImpl::GetExactFnPtrTypeHandle
  1685| TypeHandle DacDbiInterfaceImpl::BasicTypeInfoToTypeHandle(DebuggerIPCE_BasicTypeData * pArgTypeData)
  1686| {
  1687|     LOG((LF_CORDB, LL_INFO10000,
  1688|         "D::BTITTH: expanding basic right-side type to left-side type, ELEMENT_TYPE: %d.\n",
  1689|         pArgTypeData->elementType));
  1690|     TypeHandle typeHandle = TypeHandle();
  1691|     switch (pArgTypeData->elementType)
  1692|     {
  1693|         case ELEMENT_TYPE_ARRAY:
  1694|         case ELEMENT_TYPE_SZARRAY:
  1695|         case ELEMENT_TYPE_PTR:
  1696|         case ELEMENT_TYPE_BYREF:
  1697|         case ELEMENT_TYPE_FNPTR:
  1698|             _ASSERTE(!pArgTypeData->vmTypeHandle.IsNull());
  1699|             typeHandle = TypeHandle::FromPtr(pArgTypeData->vmTypeHandle.GetDacPtr());
  1700|             break;
  1701|         case ELEMENT_TYPE_CLASS:
  1702|         case ELEMENT_TYPE_VALUETYPE:
  1703|             typeHandle = GetClassOrValueTypeHandle(pArgTypeData);
  1704|             break;
  1705|         default:
  1706|             typeHandle = FindLoadedElementType(pArgTypeData->elementType);
  1707|             break;
  1708|     }
  1709|     if (typeHandle.IsNull())
  1710|     {
  1711|         ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
  1712|     }
  1713|     return typeHandle;
  1714| } // DacDbiInterfaceImpl::BasicTypeInfoToTypeHandle
  1715| TypeHandle DacDbiInterfaceImpl::ExpandedTypeInfoToTypeHandle(DebuggerIPCE_ExpandedTypeData * pTopLevelTypeData,
  1716|                                                              ArgInfoList *                   pArgInfo)
  1717| {
  1718|     WRAPPER_NO_CONTRACT;
  1719|     LOG((LF_CORDB, LL_INFO10000,
  1720|         "D::ETITTH: expanding right-side type to left-side type, ELEMENT_TYPE: %d.\n",
  1721|         pData->elementType));
  1722|     TypeHandle typeHandle = TypeHandle();
  1723|     switch (pTopLevelTypeData->elementType)
  1724|     {
  1725|         case ELEMENT_TYPE_ARRAY:
  1726|         case ELEMENT_TYPE_SZARRAY:
  1727|             typeHandle = GetExactArrayTypeHandle(pTopLevelTypeData, pArgInfo);
  1728|             break;
  1729|         case ELEMENT_TYPE_PTR:
  1730|         case ELEMENT_TYPE_BYREF:
  1731|             typeHandle = GetExactPtrOrByRefTypeHandle(pTopLevelTypeData, pArgInfo);
  1732|             break;
  1733|         case ELEMENT_TYPE_CLASS:
  1734|         case ELEMENT_TYPE_VALUETYPE:
  1735|             typeHandle = GetExactClassTypeHandle(pTopLevelTypeData, pArgInfo);
  1736|             break;
  1737|         case ELEMENT_TYPE_FNPTR:
  1738|             typeHandle = GetExactFnPtrTypeHandle(pArgInfo);
  1739|             break;
  1740|         default:
  1741|             typeHandle = FindLoadedElementType(pTopLevelTypeData->elementType);
  1742|             break;
  1743|     } // end switch (pData->elementType)
  1744|     if (typeHandle.IsNull())
  1745|     {
  1746|         LOG((LF_CORDB, LL_INFO10000, "D::ETITTH: type isn't loaded.\n"));
  1747|         ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
  1748|     }
  1749|     return typeHandle;
  1750| } // DacDbiInterfaceImpl::ExpandedTypeInfoToTypeHandle
  1751| CORDB_ADDRESS DacDbiInterfaceImpl::GetThreadStaticAddress(VMPTR_FieldDesc vmField,
  1752|                                                           VMPTR_Thread    vmRuntimeThread)
  1753| {
  1754|     DD_ENTER_MAY_THROW;
  1755|     Thread * pRuntimeThread = vmRuntimeThread.GetDacPtr();
  1756|     PTR_FieldDesc pFieldDesc = vmField.GetDacPtr();
  1757|     TADDR fieldAddress = NULL;
  1758|     _ASSERTE(pRuntimeThread != NULL);
  1759|     if (pFieldDesc->IsThreadStatic())
  1760|     {
  1761|         fieldAddress = pRuntimeThread->GetStaticFieldAddrNoCreate(pFieldDesc);
  1762|     }
  1763|     else
  1764|     {
  1765|         ThrowHR(E_NOTIMPL);
  1766|     }
  1767|     return fieldAddress;
  1768| } // DacDbiInterfaceImpl::GetThreadStaticAddress
  1769| CORDB_ADDRESS DacDbiInterfaceImpl::GetCollectibleTypeStaticAddress(VMPTR_FieldDesc vmField,
  1770|                                                                    VMPTR_AppDomain vmAppDomain)
  1771| {
  1772|     DD_ENTER_MAY_THROW;
  1773|     AppDomain * pAppDomain = vmAppDomain.GetDacPtr();
  1774|     PTR_FieldDesc pFieldDesc = vmField.GetDacPtr();
  1775|     _ASSERTE(pAppDomain != NULL);
  1776|     if(!pFieldDesc->IsStatic() ||
  1777|        pFieldDesc->IsSpecialStatic())
  1778|     {
  1779|         _ASSERTE(!"BUG: Unsupported static field type for collectible types");
  1780|     }
  1781|     /* TODO: Ideally we should be checking if the class is allocated first, however
  1782|              we don't appear to be doing this even for non-collectible statics and
  1783|              we have never seen an issue.
  1784|     */
  1785|     PTR_VOID base = pFieldDesc->GetBase();
  1786|     if (base == PTR_NULL)
  1787|     {
  1788|         return PTR_HOST_TO_TADDR(NULL);
  1789|     }
  1790|     PTR_VOID addr = pFieldDesc->GetStaticAddressHandle(base);
  1791|     return PTR_TO_TADDR(addr);
  1792| } // DacDbiInterfaceImpl::GetCollectibleTypeStaticAddress
  1793| void DacDbiInterfaceImpl::GetTypeHandleParams(VMPTR_AppDomain  vmAppDomain,
  1794|                                               VMPTR_TypeHandle vmTypeHandle,
  1795|                                               TypeParamsList * pParams)
  1796| {
  1797|     DD_ENTER_MAY_THROW
  1798|     TypeHandle typeHandle = TypeHandle::FromPtr(vmTypeHandle.GetDacPtr());
  1799|     LOG((LF_CORDB, LL_INFO10000, "D::GTHP: getting type parameters for 0x%08x 0x%0x8.\n",
  1800|          vmAppDomain.GetDacPtr(), typeHandle.AsPtr()));
  1801|     _ASSERTE(pParams->IsEmpty());
  1802|     pParams->Alloc(typeHandle.GetNumGenericArgs());
  1803|     for (unsigned int i = 0; i < pParams->Count(); ++i)
  1804|     {
  1805|         VMPTR_TypeHandle thInst = VMPTR_TypeHandle::NullPtr();
  1806|         thInst.SetDacTargetPtr(typeHandle.GetInstantiation()[i].AsTAddr());
  1807|         TypeHandleToExpandedTypeInfo(NoValueTypeBoxing,
  1808|                                      vmAppDomain,
  1809|                                      thInst,
  1810|                                      &((*pParams)[i]));
  1811|     }
  1812|     LOG((LF_CORDB, LL_INFO10000, "D::GTHP: sending  result"));
  1813| } // DacDbiInterfaceImpl::GetTypeHandleParams
  1814| void DacDbiInterfaceImpl::GetSimpleType(VMPTR_AppDomain    vmAppDomain,
  1815|                                         CorElementType     simpleType,
  1816|                                         mdTypeDef         *pMetadataToken,
  1817|                                         VMPTR_Module      *pVmModule,
  1818|                                         VMPTR_DomainAssembly  *pVmDomainAssembly)
  1819| {
  1820|     DD_ENTER_MAY_THROW;
  1821|     AppDomain *pAppDomain = vmAppDomain.GetDacPtr();
  1822|     _ASSERTE(pVmDomainAssembly != NULL);
  1823|     *pVmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
  1824|     TypeHandle typeHandle =  FindLoadedElementType(simpleType);
  1825|     if (typeHandle.IsNull())
  1826|     {
  1827|         ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
  1828|     }
  1829|     else
  1830|     {
  1831|         _ASSERTE(pMetadataToken != NULL);
  1832|         *pMetadataToken = typeHandle.GetCl();
  1833|         Module * pModule = typeHandle.GetModule();
  1834|         if (pModule == NULL)
  1835|             ThrowHR(CORDBG_E_TARGET_INCONSISTENT);
  1836|         pVmModule->SetHostPtr(pModule);
  1837|         if (pAppDomain)
  1838|         {
  1839|             pVmDomainAssembly->SetHostPtr(pModule->GetDomainAssembly());
  1840|             if (pVmDomainAssembly->IsNull())
  1841|                 ThrowHR(CORDBG_E_TARGET_INCONSISTENT);
  1842|         }
  1843|     }
  1844|     LOG((LF_CORDB, LL_INFO10000, "D::STI: sending result.\n"));
  1845| } // DacDbiInterfaceImpl::GetSimpleType
  1846| BOOL DacDbiInterfaceImpl::IsExceptionObject(VMPTR_Object vmObject)
  1847| {
  1848|     DD_ENTER_MAY_THROW;
  1849|     Object* objPtr = vmObject.GetDacPtr();
  1850|     MethodTable* pMT = objPtr->GetMethodTable();
  1851|     return IsExceptionObject(pMT);
  1852| }
  1853| BOOL DacDbiInterfaceImpl::IsExceptionObject(MethodTable* pMT)
  1854| {
  1855|     PTR_MethodTable pExMT = g_pExceptionClass;
  1856|     TADDR targetMT = dac_cast<TADDR>(pMT);
  1857|     TADDR exceptionMT = dac_cast<TADDR>(pExMT);
  1858|     do
  1859|     {
  1860|         if (targetMT == exceptionMT)
  1861|             return TRUE;
  1862|         pMT = pMT->GetParentMethodTable();
  1863|         targetMT = dac_cast<TADDR>(pMT);
  1864|     } while (pMT);
  1865|     return FALSE;
  1866| }
  1867| HRESULT DacDbiInterfaceImpl::GetMethodDescPtrFromIpEx(TADDR funcIp, VMPTR_MethodDesc* ppMD)
  1868| {
  1869|     DD_ENTER_MAY_THROW;
  1870|     CLRDATA_ADDRESS mdAddr;
  1871|     HRESULT hr = g_dacImpl->GetMethodDescPtrFromIP(TO_CDADDR(funcIp), &mdAddr);
  1872|     if (S_OK == hr)
  1873|     {
  1874|         ppMD->SetDacTargetPtr(CLRDATA_ADDRESS_TO_TADDR(mdAddr));
  1875|         return hr;
  1876|     }
  1877|     MethodDesc* pMD = MethodTable::GetMethodDescForSlotAddress(PINSTRToPCODE(funcIp));
  1878|     if (pMD == NULL)
  1879|         return E_INVALIDARG;
  1880|     ppMD->SetDacTargetPtr(PTR_HOST_TO_TADDR(pMD));
  1881|     return S_OK;
  1882| }
  1883| BOOL DacDbiInterfaceImpl::IsDelegate(VMPTR_Object vmObject)
  1884| {
  1885|     DD_ENTER_MAY_THROW;
  1886|     if (vmObject.IsNull())
  1887|         return FALSE;
  1888|     Object *pObj = vmObject.GetDacPtr();
  1889|     return pObj->GetGCSafeMethodTable()->IsDelegate();
  1890| }
  1891| HRESULT DacDbiInterfaceImpl::GetDelegateType(VMPTR_Object delegateObject, DelegateType *delegateType)
  1892| {
  1893|     DD_ENTER_MAY_THROW;
  1894|     _ASSERTE(!delegateObject.IsNull());
  1895|     _ASSERTE(delegateType != NULL);
  1896| #ifdef _DEBUG
  1897|     IsDelegate(delegateObject);
  1898| #endif
  1899|     *delegateType = DelegateType::kUnknownDelegateType;
  1900|     PTR_DelegateObject pDelObj = dac_cast<PTR_DelegateObject>(delegateObject.GetDacPtr());
  1901|     INT_PTR invocationCount = pDelObj->GetInvocationCount();
  1902|     if (invocationCount == -1)
  1903|     {
  1904|         *delegateType = kUnmanagedFunctionDelegate;
  1905|         return S_OK;
  1906|     }
  1907|     PTR_Object pInvocationList = OBJECTREFToObject(pDelObj->GetInvocationList());
  1908|     if (invocationCount == NULL)
  1909|     {
  1910|         if (pInvocationList == NULL)
  1911|         {
  1912|             TADDR targetMethodPtr = PCODEToPINSTR(pDelObj->GetMethodPtrAux());
  1913|             if (targetMethodPtr == NULL)
  1914|             {
  1915|                 *delegateType = kClosedDelegate;
  1916|             }
  1917|             else {
  1918|                 *delegateType = kOpenDelegate;
  1919|             }
  1920|             return S_OK;
  1921|         }
  1922|     }
  1923|     else
  1924|     {
  1925|         if (pInvocationList != NULL)
  1926|         {
  1927|             PTR_MethodTable invocationListMT = pInvocationList->GetGCSafeMethodTable();
  1928|             if (invocationListMT->IsArray())
  1929|                 *delegateType = kTrueMulticastDelegate;
  1930|             if (invocationListMT->IsDelegate())
  1931|                 *delegateType = kWrapperDelegate;
  1932|             return S_OK;
  1933|         }
  1934|     }
  1935|     _ASSERT(FALSE);
  1936|     *delegateType = kUnknownDelegateType;
  1937|     return CORDBG_E_UNSUPPORTED_DELEGATE;
  1938| }
  1939| HRESULT DacDbiInterfaceImpl::GetDelegateFunctionData(
  1940|     DelegateType delegateType,
  1941|     VMPTR_Object delegateObject,
  1942|     OUT VMPTR_DomainAssembly *ppFunctionDomainAssembly,
  1943|     OUT mdMethodDef *pMethodDef)
  1944| {
  1945|     DD_ENTER_MAY_THROW;
  1946| #ifdef _DEBUG
  1947|     IsDelegate(delegateObject);
  1948| #endif
  1949|     HRESULT hr = S_OK;
  1950|     PTR_DelegateObject pDelObj = dac_cast<PTR_DelegateObject>(delegateObject.GetDacPtr());
  1951|     TADDR targetMethodPtr = NULL;
  1952|     VMPTR_MethodDesc pMD;
  1953|     switch (delegateType)
  1954|     {
  1955|     case kClosedDelegate:
  1956|         targetMethodPtr = PCODEToPINSTR(pDelObj->GetMethodPtr());
  1957|         break;
  1958|     case kOpenDelegate:
  1959|         targetMethodPtr = PCODEToPINSTR(pDelObj->GetMethodPtrAux());
  1960|         break;
  1961|     default:
  1962|         return E_FAIL;
  1963|     }
  1964|     hr = GetMethodDescPtrFromIpEx(targetMethodPtr, &pMD);
  1965|     if (hr != S_OK)
  1966|         return hr;
  1967|     ppFunctionDomainAssembly->SetDacTargetPtr(dac_cast<TADDR>(pMD.GetDacPtr()->GetModule()->GetDomainAssembly()));
  1968|     *pMethodDef = pMD.GetDacPtr()->GetMemberDef();
  1969|     return hr;
  1970| }
  1971| HRESULT DacDbiInterfaceImpl::GetDelegateTargetObject(
  1972|     DelegateType delegateType,
  1973|     VMPTR_Object delegateObject,
  1974|     OUT VMPTR_Object *ppTargetObj,
  1975|     OUT VMPTR_AppDomain *ppTargetAppDomain)
  1976| {
  1977|     DD_ENTER_MAY_THROW;
  1978| #ifdef _DEBUG
  1979|     IsDelegate(delegateObject);
  1980| #endif
  1981|     HRESULT hr = S_OK;
  1982|     PTR_DelegateObject pDelObj = dac_cast<PTR_DelegateObject>(delegateObject.GetDacPtr());
  1983|     switch (delegateType)
  1984|     {
  1985|         case kClosedDelegate:
  1986|         {
  1987|             PTR_Object pRemoteTargetObj = OBJECTREFToObject(pDelObj->GetTarget());
  1988|             ppTargetObj->SetDacTargetPtr(pRemoteTargetObj.GetAddr());
  1989|             ppTargetAppDomain->SetDacTargetPtr(dac_cast<TADDR>(pRemoteTargetObj->GetGCSafeMethodTable()->GetDomain()->AsAppDomain()));
  1990|             break;
  1991|         }
  1992|         default:
  1993|             ppTargetObj->SetDacTargetPtr(NULL);
  1994|             ppTargetAppDomain->SetDacTargetPtr(dac_cast<TADDR>(pDelObj->GetGCSafeMethodTable()->GetDomain()->AsAppDomain()));
  1995|             break;
  1996|     }
  1997|     return hr;
  1998| }
  1999| static bool TrackMemoryRangeHelper(PTR_VOID pvArgs, PTR_VOID pvAllocationBase, SIZE_T cbReserved)
  2000| {
  2001|     CQuickArrayList<COR_MEMORY_RANGE> *rangeCollection =
  2002|                                         (CQuickArrayList<COR_MEMORY_RANGE>*)(dac_cast<TADDR>(pvArgs));
  2003|     TADDR rangeStart = dac_cast<TADDR>(pvAllocationBase);
  2004|     TADDR rangeEnd = rangeStart + cbReserved;
  2005|     rangeCollection->Push({rangeStart, rangeEnd});
  2006|     return false;
  2007| }
  2008| void DacDbiInterfaceImpl::EnumerateMemRangesForLoaderAllocator(PTR_LoaderAllocator pLoaderAllocator, CQuickArrayList<COR_MEMORY_RANGE> *rangeAcummulator)
  2009| {
  2010|     CQuickArrayList<PTR_LoaderHeap> heapsToEnumerate;
  2011|     _ASSERTE(pLoaderAllocator->GetLowFrequencyHeap() != NULL);
  2012|     heapsToEnumerate.Push(pLoaderAllocator->GetLowFrequencyHeap());
  2013|     _ASSERTE(pLoaderAllocator->GetHighFrequencyHeap() != NULL);
  2014|     heapsToEnumerate.Push(pLoaderAllocator->GetHighFrequencyHeap());
  2015|     _ASSERTE(pLoaderAllocator->GetStubHeap() != NULL);
  2016|     heapsToEnumerate.Push(pLoaderAllocator->GetStubHeap());
  2017|     VirtualCallStubManager *pVcsMgr = pLoaderAllocator->GetVirtualCallStubManager();
  2018|     LOG((LF_CORDB, LL_INFO10000, "DDBII::EMRFLA: VirtualCallStubManager 0x%x\n", PTR_HOST_TO_TADDR(pVcsMgr)));
  2019|     if (pVcsMgr)
  2020|     {
  2021|         if (pVcsMgr->indcell_heap != NULL) heapsToEnumerate.Push(pVcsMgr->indcell_heap);
  2022|         if (pVcsMgr->cache_entry_heap != NULL) heapsToEnumerate.Push(pVcsMgr->cache_entry_heap);
  2023|     }
  2024|     TADDR rangeAccumAsTaddr = TO_TADDR(rangeAcummulator);
  2025|     for (uint32_t i = 0; i < (uint32_t)heapsToEnumerate.Size(); i++)
  2026|     {
  2027|         LOG((LF_CORDB, LL_INFO10000, "DDBII::EMRFLA: LoaderHeap 0x%x\n", heapsToEnumerate[i].GetAddr()));
  2028|         heapsToEnumerate[i]->EnumPageRegions(TrackMemoryRangeHelper, rangeAccumAsTaddr);
  2029|     }
  2030| }
  2031| void DacDbiInterfaceImpl::EnumerateMemRangesForJitCodeHeaps(CQuickArrayList<COR_MEMORY_RANGE> *rangeAcummulator)
  2032| {
  2033|     EEJitManager *pEM = ExecutionManager::GetEEJitManager();
  2034|     _ASSERTE(pEM != NULL && pEM->m_pCodeHeap.IsValid());
  2035|     PTR_HeapList pHeapList = pEM->m_pCodeHeap;
  2036|     while (pHeapList != NULL)
  2037|     {
  2038|         CodeHeap *pHeap = pHeapList->pHeap;
  2039|         DacpJitCodeHeapInfo jitCodeHeapInfo = DACGetHeapInfoForCodeHeap(pHeap);
  2040|         switch (jitCodeHeapInfo.codeHeapType)
  2041|         {
  2042|             case CODEHEAP_LOADER:
  2043|             {
  2044|                 TADDR targetLoaderHeap = CLRDATA_ADDRESS_TO_TADDR(jitCodeHeapInfo.LoaderHeap);
  2045|                 LOG((LF_CORDB, LL_INFO10000,
  2046|                     "DDBII::EMRFJCH: LoaderCodeHeap 0x%x with LoaderHeap at 0x%x\n",
  2047|                     PTR_HOST_TO_TADDR(pHeap), targetLoaderHeap));
  2048|                 PTR_ExplicitControlLoaderHeap pLoaderHeap = PTR_ExplicitControlLoaderHeap(targetLoaderHeap);
  2049|                 pLoaderHeap->EnumPageRegions(TrackMemoryRangeHelper, TO_TADDR(rangeAcummulator));
  2050|                 break;
  2051|             }
  2052|             case CODEHEAP_HOST:
  2053|             {
  2054|                 LOG((LF_CORDB, LL_INFO10000,
  2055|                     "DDBII::EMRFJCH: HostCodeHeap 0x%x\n",
  2056|                     PTR_HOST_TO_TADDR(pHeap)));
  2057|                 rangeAcummulator->Push({
  2058|                     CLRDATA_ADDRESS_TO_TADDR(jitCodeHeapInfo.HostData.baseAddr),
  2059|                     CLRDATA_ADDRESS_TO_TADDR(jitCodeHeapInfo.HostData.currentAddr)
  2060|                 });
  2061|                 break;
  2062|             }
  2063|             default:
  2064|             {
  2065|                 LOG((LF_CORDB, LL_INFO10000, "DDBII::EMRFJCH: unknown heap type at 0x%x\n\n", pHeap));
  2066|                 _ASSERTE("Unknown heap type enumerating code ranges.");
  2067|                 break;
  2068|             }
  2069|         }
  2070|         pHeapList = pHeapList->GetNext();
  2071|     }
  2072| }
  2073| HRESULT DacDbiInterfaceImpl::GetLoaderHeapMemoryRanges(DacDbiArrayList<COR_MEMORY_RANGE> *pRanges)
  2074| {
  2075|     LOG((LF_CORDB, LL_INFO10000, "DDBII::GLHMR\n"));
  2076|     DD_ENTER_MAY_THROW;
  2077|     HRESULT hr = S_OK;
  2078|     EX_TRY
  2079|     {
  2080|         CQuickArrayList<COR_MEMORY_RANGE> memoryRanges;
  2081|         PTR_LoaderAllocator pGlobalAllocator = SystemDomain::System()->GetLoaderAllocator();
  2082|         _ASSERTE(pGlobalAllocator);
  2083|         EnumerateMemRangesForLoaderAllocator(pGlobalAllocator, &memoryRanges);
  2084|         EnumerateMemRangesForJitCodeHeaps(&memoryRanges);
  2085|         _ASSERTE(memoryRanges.Size() < INT_MAX);
  2086|         pRanges->Init(memoryRanges.Ptr(), (UINT) memoryRanges.Size());
  2087|     }
  2088|     EX_CATCH_HRESULT(hr);
  2089|     return hr;
  2090| }
  2091| void DacDbiInterfaceImpl::GetStackFramesFromException(VMPTR_Object vmObject, DacDbiArrayList<DacExceptionCallStackData>& dacStackFrames)
  2092| {
  2093|     DD_ENTER_MAY_THROW;
  2094|     PTR_Object objPtr = vmObject.GetDacPtr();
  2095| #ifdef _DEBUG
  2096|     MethodTable* pMT = objPtr->GetMethodTable();
  2097|     _ASSERTE(IsExceptionObject(pMT));
  2098| #endif
  2099|     OBJECTREF objRef = ObjectToOBJECTREF(objPtr);
  2100|     DebugStackTrace::GetStackFramesData stackFramesData;
  2101|     stackFramesData.pDomain = NULL;
  2102|     stackFramesData.skip = 0;
  2103|     stackFramesData.NumFramesRequested = 0;
  2104|     DebugStackTrace::GetStackFramesFromException(&objRef, &stackFramesData);
  2105|     INT32 dacStackFramesLength = stackFramesData.cElements;
  2106|     if (dacStackFramesLength > 0)
  2107|     {
  2108|         dacStackFrames.Alloc(dacStackFramesLength);
  2109|         for (INT32 index = 0; index < dacStackFramesLength; ++index)
  2110|         {
  2111|             DebugStackTrace::DebugStackTraceElement const& currentElement = stackFramesData.pElements[index];
  2112|             DacExceptionCallStackData& currentFrame = dacStackFrames[index];
  2113|             Module* pModule = currentElement.pFunc->GetModule();
  2114|             BaseDomain* pBaseDomain = currentElement.pFunc->GetAssembly()->GetDomain();
  2115|             AppDomain* pDomain = NULL;
  2116|             DomainAssembly* pDomainAssembly = NULL;
  2117|             pDomain = pBaseDomain->AsAppDomain();
  2118|             _ASSERTE(pDomain != NULL);
  2119|             pDomainAssembly = pModule->GetDomainAssembly();
  2120|             _ASSERTE(pDomainAssembly != NULL);
  2121|             currentFrame.vmAppDomain.SetHostPtr(pDomain);
  2122|             currentFrame.vmDomainAssembly.SetHostPtr(pDomainAssembly);
  2123|             currentFrame.ip = currentElement.ip;
  2124|             currentFrame.methodDef = currentElement.pFunc->GetMemberDef();
  2125|             currentFrame.isLastForeignExceptionFrame = (currentElement.flags & STEF_LAST_FRAME_FROM_FOREIGN_STACK_TRACE) != 0;
  2126|         }
  2127|     }
  2128| }
  2129| #ifdef FEATURE_COMINTEROP
  2130| PTR_RCW GetRcwFromVmptrObject(VMPTR_Object vmObject)
  2131| {
  2132|     PTR_RCW pRCW = NULL;
  2133|     Object* objPtr = vmObject.GetDacPtr();
  2134|     PTR_SyncBlock pSyncBlock = NULL;
  2135|     pSyncBlock = objPtr->PassiveGetSyncBlock();
  2136|     if (pSyncBlock == NULL)
  2137|         return pRCW;
  2138|     PTR_InteropSyncBlockInfo pInfo = NULL;
  2139|     pInfo = pSyncBlock->GetInteropInfoNoCreate();
  2140|     if (pInfo == NULL)
  2141|         return pRCW;
  2142|     pRCW = dac_cast<PTR_RCW>(pInfo->DacGetRawRCW());
  2143|     return pRCW;
  2144| }
  2145| #endif
  2146| BOOL DacDbiInterfaceImpl::IsRcw(VMPTR_Object vmObject)
  2147| {
  2148| #ifdef FEATURE_COMINTEROP
  2149|     DD_ENTER_MAY_THROW;
  2150|     return GetRcwFromVmptrObject(vmObject) != NULL;
  2151| #else
  2152|     return FALSE;
  2153| #endif // FEATURE_COMINTEROP
  2154| }
  2155| void DacDbiInterfaceImpl::GetRcwCachedInterfaceTypes(
  2156|                         VMPTR_Object vmObject,
  2157|                         VMPTR_AppDomain vmAppDomain,
  2158|                         BOOL bIInspectableOnly,
  2159|                         DacDbiArrayList<DebuggerIPCE_ExpandedTypeData> * pDacInterfaces)
  2160| {
  2161|     pDacInterfaces->Alloc(0);
  2162| }
  2163| void DacDbiInterfaceImpl::GetRcwCachedInterfacePointers(
  2164|                     VMPTR_Object vmObject,
  2165|                     BOOL bIInspectableOnly,
  2166|                     DacDbiArrayList<CORDB_ADDRESS> * pDacItfPtrs)
  2167| {
  2168| #ifdef FEATURE_COMINTEROP
  2169|     DD_ENTER_MAY_THROW;
  2170|     Object* objPtr = vmObject.GetDacPtr();
  2171|     InlineSArray<TADDR, INTERFACE_ENTRY_CACHE_SIZE> rgUnks;
  2172|     PTR_RCW pRCW = GetRcwFromVmptrObject(vmObject);
  2173|     if (pRCW != NULL)
  2174|     {
  2175|         pRCW->GetCachedInterfacePointers(bIInspectableOnly, &rgUnks);
  2176|         pDacItfPtrs->Alloc(rgUnks.GetCount());
  2177|         for (COUNT_T i = 0; i < rgUnks.GetCount(); ++i)
  2178|         {
  2179|             (*pDacItfPtrs)[i] = (CORDB_ADDRESS)(rgUnks[i]);
  2180|         }
  2181|     }
  2182|     else
  2183| #endif // FEATURE_COMINTEROP
  2184|     {
  2185|         pDacItfPtrs->Alloc(0);
  2186|     }
  2187| }
  2188| void DacDbiInterfaceImpl::GetCachedWinRTTypesForIIDs(
  2189|                     VMPTR_AppDomain vmAppDomain,
  2190| 					DacDbiArrayList<GUID> & iids,
  2191|     				OUT DacDbiArrayList<DebuggerIPCE_ExpandedTypeData> * pTypes)
  2192| {
  2193|     pTypes->Alloc(0);
  2194| }
  2195| void DacDbiInterfaceImpl::GetCachedWinRTTypes(
  2196|                     VMPTR_AppDomain vmAppDomain,
  2197|                     OUT DacDbiArrayList<GUID> * pGuids,
  2198|                     OUT DacDbiArrayList<DebuggerIPCE_ExpandedTypeData> * pTypes)
  2199| {
  2200|     pTypes->Alloc(0);
  2201| }
  2202| PTR_FieldDesc  DacDbiInterfaceImpl::FindField(TypeHandle thApprox, mdFieldDef fldToken)
  2203| {
  2204|     EncApproxFieldDescIterator fdIterator(thApprox.GetMethodTable(),
  2205|                                           ApproxFieldDescIterator::ALL_FIELDS); // don't fixup EnC (we can't, we're stopped)
  2206|     PTR_FieldDesc pCurrentFD;
  2207|     while ((pCurrentFD = fdIterator.Next()) != NULL)
  2208|     {
  2209|         if (pCurrentFD->GetMemberDef() == fldToken)
  2210|         {
  2211|             return pCurrentFD;
  2212|         }
  2213|     }
  2214|     return NULL;
  2215| } // DacDbiInterfaceImpl::FindField
  2216| FieldDesc * DacDbiInterfaceImpl::GetEnCFieldDesc(const EnCHangingFieldInfo * pEnCFieldInfo)
  2217| {
  2218|         FieldDesc * pFD = NULL;
  2219|         DomainAssembly * pDomainAssembly = pEnCFieldInfo->GetObjectTypeData().vmDomainAssembly.GetDacPtr();
  2220|         Module     * pModule     = pDomainAssembly->GetModule();
  2221|         TypeHandle typeHandle = ClassLoader::LookupTypeDefOrRefInModule(pModule,
  2222|                                              pEnCFieldInfo->GetObjectTypeData().metadataToken);
  2223|         if (typeHandle == NULL)
  2224|         {
  2225|             ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
  2226|         }
  2227|         pFD = FindField(typeHandle, pEnCFieldInfo->GetFieldToken());
  2228|         if (pFD == NULL)
  2229|         {
  2230|             ThrowHR(CORDBG_E_ENC_HANGING_FIELD);
  2231|         }
  2232|         return pFD;
  2233| } // DacDbiInterfaceImpl::GetEnCFieldDesc
  2234| PTR_CBYTE DacDbiInterfaceImpl::GetPtrToEnCField(FieldDesc * pFD, const EnCHangingFieldInfo * pEnCFieldInfo)
  2235| {
  2236| #ifndef EnC_SUPPORTED
  2237|     _ASSERTE(!"Trying to get the address of an EnC field where EnC is not supported! ");
  2238|     return NULL;
  2239| #else
  2240|     PTR_EditAndContinueModule pEnCModule;
  2241|     DomainAssembly * pDomainAssembly = pEnCFieldInfo->GetObjectTypeData().vmDomainAssembly.GetDacPtr();
  2242|     Module     * pModule     = pDomainAssembly->GetModule();
  2243|     _ASSERTE(pModule->IsEditAndContinueCapable());
  2244|     pEnCModule = dac_cast<PTR_EditAndContinueModule>(pModule);
  2245|     _ASSERTE(pFD->IsEnCNew());
  2246|     EnCFieldDesc * pEnCFieldDesc;
  2247|     pEnCFieldDesc = dac_cast<PTR_EnCFieldDesc>(pFD);
  2248|     if (pEnCFieldDesc->NeedsFixup())
  2249|     {
  2250|         ThrowHR(CORDBG_E_ENC_HANGING_FIELD);
  2251|     }
  2252|     PTR_CBYTE pORField = NULL;
  2253|     PTR_Object pObject = pEnCFieldInfo->GetVmObject().GetDacPtr();
  2254|     pORField = pEnCModule->ResolveField(ObjectToOBJECTREF(pObject),
  2255|                                         pEnCFieldDesc);
  2256|     if (pORField == NULL)
  2257|     {
  2258|         ThrowHR(CORDBG_E_ENC_HANGING_FIELD);
  2259|     }
  2260|     return pORField;
  2261| #endif // EnC_SUPPORTED
  2262| } // DacDbiInterfaceImpl::GetPtrToEnCField
  2263| void DacDbiInterfaceImpl::InitFieldData(const FieldDesc *           pFD,
  2264|                                         const PTR_CBYTE             pORField,
  2265|                                         const EnCHangingFieldInfo * pEnCFieldData,
  2266|                                         FieldData *           pFieldData)
  2267| {
  2268|     pFieldData->ClearFields();
  2269|     pFieldData->m_fFldIsStatic = (pFD->IsStatic() != 0);
  2270|     pFieldData->m_vmFieldDesc.SetHostPtr(pFD);
  2271|     pFieldData->m_fFldIsTLS = (pFD->IsThreadStatic() == TRUE);
  2272|     pFieldData->m_fldMetadataToken = pFD->GetMemberDef();
  2273|     pFieldData->m_fFldIsRVA = (pFD->IsRVA() == TRUE);
  2274|     pFieldData->m_fFldIsCollectibleStatic = FALSE;
  2275|     pFieldData->m_fFldStorageAvailable = true;
  2276|     if (pFieldData->m_fFldIsStatic)
  2277|     {
  2278|         _ASSERTE(!pFieldData->m_fFldIsTLS);
  2279|         _ASSERTE(!pFieldData->m_fFldIsRVA);
  2280|         pFieldData->SetStaticAddress(PTR_TO_TADDR(pORField));
  2281|     }
  2282|     else
  2283|     {
  2284|         pFieldData->SetInstanceOffset(PTR_TO_TADDR(pORField) -
  2285|                                       (PTR_TO_TADDR(pEnCFieldData->GetVmObject().GetDacPtr()) +
  2286|                                                    pEnCFieldData->GetOffsetToVars()));
  2287|     }
  2288| } // DacDbiInterfaceImpl::InitFieldData
  2289| void DacDbiInterfaceImpl::GetEnCHangingFieldInfo(const EnCHangingFieldInfo * pEnCFieldInfo,
  2290|                                                  FieldData *           pFieldData,
  2291|                                                  BOOL *                pfStatic)
  2292| {
  2293|     DD_ENTER_MAY_THROW;
  2294|     LOG((LF_CORDB, LL_INFO100000, "DDI::IEnCHFI: Obj:0x%x, objType"
  2295|         ":0x%x, offset:0x%x\n", pEnCFieldInfo->m_pObject, pEnCFieldInfo->m_objectTypeData.elementType,
  2296|         pEnCFieldInfo->m_offsetToVars));
  2297|     FieldDesc *  pFD      = NULL;
  2298|     PTR_CBYTE    pORField = NULL;
  2299|     pFD = GetEnCFieldDesc(pEnCFieldInfo);
  2300|     _ASSERTE(pFD->IsEnCNew()); // We shouldn't be here if it wasn't added to an
  2301| #ifdef EnC_SUPPORTED
  2302|     pORField = GetPtrToEnCField(pFD, pEnCFieldInfo);
  2303| #else
  2304|     _ASSERTE(!"We shouldn't be here: EnC not supported");
  2305| #endif // EnC_SUPPORTED
  2306|     InitFieldData(pFD, pORField, pEnCFieldInfo, pFieldData);
  2307|     *pfStatic = (pFD->IsStatic() != 0);
  2308| } // DacDbiInterfaceImpl::GetEnCHangingFieldInfo
  2309| void DacDbiInterfaceImpl::GetAssemblyFromDomainAssembly(VMPTR_DomainAssembly vmDomainAssembly, VMPTR_Assembly *vmAssembly)
  2310| {
  2311|     DD_ENTER_MAY_THROW;
  2312|     _ASSERTE(vmAssembly != NULL);
  2313|     DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
  2314|     vmAssembly->SetHostPtr(pDomainAssembly->GetAssembly());
  2315| }
  2316| BOOL DacDbiInterfaceImpl::IsAssemblyFullyTrusted(VMPTR_DomainAssembly vmDomainAssembly)
  2317| {
  2318|     DD_ENTER_MAY_THROW;
  2319|     return TRUE;
  2320| }
  2321| BOOL DacDbiInterfaceImpl::GetAssemblyPath(
  2322|     VMPTR_Assembly  vmAssembly,
  2323|     IStringHolder * pStrFilename)
  2324| {
  2325|     DD_ENTER_MAY_THROW;
  2326|     Assembly * pAssembly = vmAssembly.GetDacPtr();
  2327|     Module * pManifestModule = pAssembly->GetModule();
  2328|     const WCHAR * szPath = pManifestModule->GetPath().DacGetRawUnicode();
  2329|     HRESULT hrStatus = pStrFilename->AssignCopy(szPath);
  2330|     IfFailThrow(hrStatus);
  2331|     if(szPath == NULL || *szPath=='\0')
  2332|     {
  2333|         return FALSE;
  2334|     }
  2335|     return TRUE;
  2336| }
  2337| void DacDbiInterfaceImpl::ResolveTypeReference(const TypeRefData * pTypeRefInfo,
  2338|                                                TypeRefData *       pTargetRefInfo)
  2339| {
  2340|     DD_ENTER_MAY_THROW;
  2341|     DomainAssembly * pDomainAssembly        = pTypeRefInfo->vmDomainAssembly.GetDacPtr();
  2342|     Module *     pReferencingModule = pDomainAssembly->GetModule();
  2343|     BOOL         fSuccess = FALSE;
  2344|     Module * pTargetModule = NULL;
  2345|     mdTypeDef targetTypeDef = mdTokenNil;
  2346|     ENABLE_FORBID_GC_LOADER_USE_IN_THIS_SCOPE();
  2347|     fSuccess = ClassLoader::ResolveTokenToTypeDefThrowing(pReferencingModule,
  2348|                                                           pTypeRefInfo->typeToken,
  2349|                                                           &pTargetModule,
  2350|                                                           &targetTypeDef,
  2351|                                                           Loader::SafeLookup   //don't load, no locks/allocations
  2352|                                                           );
  2353|     if (fSuccess)
  2354|     {
  2355|         _ASSERTE(pTargetModule != NULL);
  2356|         _ASSERTE( TypeFromToken(targetTypeDef) == mdtTypeDef );
  2357|         AppDomain * pAppDomain = pDomainAssembly->GetAppDomain();
  2358|         pTargetRefInfo->vmDomainAssembly.SetDacTargetPtr(PTR_HOST_TO_TADDR(pTargetModule->GetDomainAssembly()));
  2359|         pTargetRefInfo->typeToken = targetTypeDef;
  2360|     }
  2361|     else
  2362|     {
  2363|         ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
  2364|     }
  2365| } // DacDbiInterfaceImpl::ResolveTypeReference
  2366| BOOL DacDbiInterfaceImpl::GetModulePath(VMPTR_Module vmModule,
  2367|                                         IStringHolder *  pStrFilename)
  2368| {
  2369|     DD_ENTER_MAY_THROW;
  2370|     Module * pModule = vmModule.GetDacPtr();
  2371|     PEAssembly * pPEAssembly = pModule->GetPEAssembly();
  2372|     if (pPEAssembly != NULL)
  2373|     {
  2374|         if( !pPEAssembly->GetPath().IsEmpty() )
  2375|         {
  2376|             const WCHAR * szPath = pPEAssembly->GetPath().DacGetRawUnicode();
  2377|             if (szPath == NULL)
  2378|             {
  2379|                 szPath = pPEAssembly->GetModuleFileNameHint().DacGetRawUnicode();
  2380|                 if (szPath == NULL)
  2381|                 {
  2382|                     goto NoFileName;
  2383|                 }
  2384|             }
  2385|             IfFailThrow(pStrFilename->AssignCopy(szPath));
  2386|             return TRUE;
  2387|         }
  2388|     }
  2389| NoFileName:
  2390|     IfFailThrow(pStrFilename->AssignCopy(W("")));
  2391|     return FALSE;
  2392| }
  2393| BOOL DacDbiInterfaceImpl::GetModuleNGenPath(VMPTR_Module vmModule,
  2394|                                             IStringHolder *  pStrFilename)
  2395| {
  2396|     DD_ENTER_MAY_THROW;
  2397|     IfFailThrow(pStrFilename->AssignCopy(W("")));
  2398|     return FALSE;
  2399| }
  2400| void DacDbiInterfaceImpl::GetModuleSimpleName(VMPTR_Module vmModule, IStringHolder * pStrFilename)
  2401| {
  2402|     DD_ENTER_MAY_THROW;
  2403|     _ASSERTE(pStrFilename != NULL);
  2404|     Module * pModule = vmModule.GetDacPtr();
  2405|     LPCUTF8 szNameUtf8 = pModule->GetSimpleName();
  2406|     SString convert(SString::Utf8, szNameUtf8);
  2407|     IfFailThrow(pStrFilename->AssignCopy(convert.GetUnicode()));
  2408| }
  2409| HRESULT DacDbiInterfaceImpl::IsModuleMapped(VMPTR_Module pModule, OUT BOOL *isModuleMapped)
  2410| {
  2411|     LOG((LF_CORDB, LL_INFO10000, "DDBII::IMM - TADDR 0x%x\n", pModule));
  2412|     DD_ENTER_MAY_THROW;
  2413|     HRESULT hr = S_FALSE;
  2414|     PTR_Module pTargetModule = pModule.GetDacPtr();
  2415|     EX_TRY
  2416|     {
  2417|         PTR_PEAssembly pPEAssembly = pTargetModule->GetPEAssembly();
  2418|         _ASSERTE(pPEAssembly != NULL);
  2419|         if (pPEAssembly->HasLoadedPEImage())
  2420|         {
  2421|             *isModuleMapped = pPEAssembly->GetLoadedLayout()->IsMapped();
  2422|             hr = S_OK;
  2423|         }
  2424|     }
  2425|     EX_CATCH_HRESULT(hr);
  2426|     return hr;
  2427| }
  2428| bool DacDbiInterfaceImpl::MetadataUpdatesApplied()
  2429| {
  2430|     DD_ENTER_MAY_THROW;
  2431| #ifdef EnC_SUPPORTED
  2432|     return g_metadataUpdatesApplied;
  2433| #else
  2434|     return false;
  2435| #endif
  2436| }
  2437| void InitTargetBufferFromMemoryRange(const MemoryRange memoryRange, TargetBuffer * pTargetBuffer)
  2438| {
  2439|     SUPPORTS_DAC;
  2440|     _ASSERTE(pTargetBuffer != NULL);
  2441|     PTR_CVOID p = memoryRange.StartAddress();
  2442|     CORDB_ADDRESS addr = PTR_TO_CORDB_ADDRESS(PTR_TO_TADDR(p));
  2443|     _ASSERTE(memoryRange.Size() <= 0xffffffff);
  2444|     pTargetBuffer->Init(addr, (ULONG)memoryRange.Size());
  2445| }
  2446| void InitTargetBufferFromTargetSBuffer(PTR_SBuffer pBuffer, TargetBuffer * pTargetBuffer)
  2447| {
  2448|     SUPPORTS_DAC;
  2449|     _ASSERTE(pTargetBuffer != NULL);
  2450|     SBuffer * pBufferHost = pBuffer;
  2451|     if (pBufferHost == NULL)
  2452|     {
  2453|         pTargetBuffer->Clear();
  2454|         return;
  2455|     }
  2456|     MemoryRange m = pBufferHost->DacGetRawBuffer();
  2457|     InitTargetBufferFromMemoryRange(m, pTargetBuffer);
  2458| }
  2459| void DacDbiInterfaceImpl::GetMetadata(VMPTR_Module vmModule, TargetBuffer * pTargetBuffer)
  2460| {
  2461|     DD_ENTER_MAY_THROW;
  2462|     pTargetBuffer->Clear();
  2463|     Module     * pModule = vmModule.GetDacPtr();
  2464|     _ASSERTE(pModule->IsVisibleToDebugger());
  2465|     if (pModule->IsReflection())
  2466|     {
  2467|         ReflectionModule * pReflectionModule = pModule->GetReflectionModule();
  2468|         InitTargetBufferFromTargetSBuffer(pReflectionModule->GetDynamicMetadataBuffer(), pTargetBuffer);
  2469|     }
  2470|     else
  2471|     {
  2472|         PEAssembly * pPEAssembly = pModule->GetPEAssembly();
  2473|         COUNT_T size;
  2474|         CORDB_ADDRESS address = PTR_TO_CORDB_ADDRESS(dac_cast<TADDR>(pPEAssembly->GetLoadedMetadata(&size)));
  2475|         pTargetBuffer->Init(address, (ULONG) size);
  2476|     }
  2477|     if (pTargetBuffer->IsEmpty())
  2478|     {
  2479|         ThrowHR(CORDBG_E_MISSING_METADATA);
  2480|     }
  2481| }
  2482| void DacDbiInterfaceImpl::GetSymbolsBuffer(VMPTR_Module vmModule, TargetBuffer * pTargetBuffer, SymbolFormat * pSymbolFormat)
  2483| {
  2484|     DD_ENTER_MAY_THROW;
  2485|     pTargetBuffer->Clear();
  2486|     *pSymbolFormat = kSymbolFormatNone;
  2487|     Module * pModule = vmModule.GetDacPtr();
  2488|     _ASSERTE(pModule->IsVisibleToDebugger());
  2489|     PTR_CGrowableStream pStream = pModule->GetInMemorySymbolStream();
  2490|     if (pStream == NULL)
  2491|     {
  2492|         return;
  2493|     }
  2494|     const MemoryRange m = pStream->GetRawBuffer();
  2495|     if (m.Size() == 0)
  2496|     {
  2497|         return;
  2498|     }
  2499|     InitTargetBufferFromMemoryRange(m, pTargetBuffer);
  2500|     *pSymbolFormat = kSymbolFormatPDB;
  2501| }
  2502| void DacDbiInterfaceImpl::GetModuleForDomainAssembly(VMPTR_DomainAssembly vmDomainAssembly, OUT VMPTR_Module * pModule)
  2503| {
  2504|     DD_ENTER_MAY_THROW;
  2505|     _ASSERTE(pModule != NULL);
  2506|     DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
  2507|     pModule->SetHostPtr(pDomainAssembly->GetModule());
  2508| }
  2509| void DacDbiInterfaceImpl::GetDomainAssemblyData(VMPTR_DomainAssembly vmDomainAssembly, DomainAssemblyInfo * pData)
  2510| {
  2511|     DD_ENTER_MAY_THROW;
  2512|     _ASSERTE(pData != NULL);
  2513|     ZeroMemory(pData, sizeof(*pData));
  2514|     DomainAssembly * pDomainAssembly  = vmDomainAssembly.GetDacPtr();
  2515|     AppDomain  * pAppDomain   = pDomainAssembly->GetAppDomain();
  2516|     pData->vmDomainAssembly.SetHostPtr(pDomainAssembly);
  2517|     pData->vmAppDomain.SetHostPtr(pAppDomain);
  2518| }
  2519| void DacDbiInterfaceImpl::GetModuleData(VMPTR_Module vmModule, ModuleInfo * pData)
  2520| {
  2521|     DD_ENTER_MAY_THROW;
  2522|     _ASSERTE(pData != NULL);
  2523|     ZeroMemory(pData, sizeof(*pData));
  2524|     Module     * pModule      = vmModule.GetDacPtr();
  2525|     PEAssembly * pPEAssembly        = pModule->GetPEAssembly();
  2526|     pData->vmPEAssembly.SetHostPtr(pPEAssembly);
  2527|     pData->vmAssembly.SetHostPtr(pModule->GetAssembly());
  2528|     BOOL fIsDynamic = pModule->IsReflection();
  2529|     pData->fIsDynamic = fIsDynamic;
  2530|     pData->pPEBaseAddress = NULL;
  2531|     pData->nPESize = 0;
  2532|     if (!fIsDynamic)
  2533|     {
  2534|         COUNT_T size = 0;
  2535|         pData->pPEBaseAddress = PTR_TO_TADDR(pPEAssembly->GetDebuggerContents(&size));
  2536|         pData->nPESize = (ULONG) size;
  2537|     }
  2538|     pData->fInMemory = FALSE;
  2539|     if (pPEAssembly != NULL)
  2540|     {
  2541|         pData->fInMemory = pPEAssembly->GetPath().IsEmpty();
  2542|     }
  2543| }
  2544| void DacDbiInterfaceImpl::EnumerateAppDomains(
  2545|     FP_APPDOMAIN_ENUMERATION_CALLBACK fpCallback,
  2546|     void * pUserData)
  2547| {
  2548|     DD_ENTER_MAY_THROW;
  2549|     _ASSERTE(fpCallback != NULL);
  2550|     AppDomain * pAppDomain = AppDomain::GetCurrentDomain();
  2551|     VMPTR_AppDomain vmAppDomain = VMPTR_AppDomain::NullPtr();
  2552|     vmAppDomain.SetHostPtr(pAppDomain);
  2553|     fpCallback(vmAppDomain, pUserData);
  2554| }
  2555| void  DacDbiInterfaceImpl::EnumerateAssembliesInAppDomain(
  2556|     VMPTR_AppDomain vmAppDomain,
  2557|     FP_ASSEMBLY_ENUMERATION_CALLBACK fpCallback,
  2558|     void * pUserData
  2559| )
  2560| {
  2561|     DD_ENTER_MAY_THROW;
  2562|     _ASSERTE(fpCallback != NULL);
  2563|     AppDomain::AssemblyIterator iterator;
  2564|     AppDomain * pAppDomain = vmAppDomain.GetDacPtr();
  2565|     if (pAppDomain == nullptr)
  2566|     {
  2567|         return;
  2568|     }
  2569|     iterator = pAppDomain->IterateAssembliesEx((AssemblyIterationFlags)(kIncludeLoading | kIncludeLoaded | kIncludeExecution));
  2570|     CollectibleAssemblyHolder<DomainAssembly *> pDomainAssembly;
  2571|     while (iterator.Next(pDomainAssembly.This()))
  2572|     {
  2573|         if (!pDomainAssembly->IsVisibleToDebugger())
  2574|         {
  2575|             continue;
  2576|         }
  2577|         VMPTR_DomainAssembly vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
  2578|         vmDomainAssembly.SetHostPtr(pDomainAssembly);
  2579|         fpCallback(vmDomainAssembly, pUserData);
  2580|     }
  2581| }
  2582| void DacDbiInterfaceImpl::EnumerateModulesInAssembly(
  2583|     VMPTR_DomainAssembly vmAssembly,
  2584|     FP_MODULE_ENUMERATION_CALLBACK fpCallback,
  2585|     void * pUserData)
  2586| {
  2587|     DD_ENTER_MAY_THROW;
  2588|     _ASSERTE(fpCallback != NULL);
  2589|     DomainAssembly * pDomainAssembly = vmAssembly.GetDacPtr();
  2590|     if (pDomainAssembly->GetModule()->IsVisibleToDebugger())
  2591|     {
  2592|         if (!pDomainAssembly->IsLoaded())
  2593|             return;
  2594|         VMPTR_DomainAssembly vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
  2595|         vmDomainAssembly.SetHostPtr(pDomainAssembly);
  2596|         fpCallback(vmDomainAssembly, pUserData);
  2597|     }
  2598| }
  2599| VMPTR_DomainAssembly DacDbiInterfaceImpl::ResolveAssembly(
  2600|     VMPTR_DomainAssembly vmScope,
  2601|     mdToken tkAssemblyRef)
  2602| {
  2603|     DD_ENTER_MAY_THROW;
  2604|     DomainAssembly * pDomainAssembly  = vmScope.GetDacPtr();
  2605|     AppDomain  * pAppDomain   = pDomainAssembly->GetAppDomain();
  2606|     Module     * pModule      = pDomainAssembly->GetModule();
  2607|     VMPTR_DomainAssembly vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
  2608|     Assembly * pAssembly = pModule->LookupAssemblyRef(tkAssemblyRef);
  2609|     if (pAssembly != NULL)
  2610|     {
  2611|         DomainAssembly * pDomainAssembly = pAssembly->GetDomainAssembly();
  2612|         vmDomainAssembly.SetHostPtr(pDomainAssembly);
  2613|     }
  2614|     return vmDomainAssembly;
  2615| }
  2616| void DacDbiInterfaceImpl::RequestSyncAtEvent()
  2617| {
  2618|     DD_ENTER_MAY_THROW;
  2619|     if (g_pDebugger != NULL)
  2620|     {
  2621|         TADDR addr = PTR_HOST_MEMBER_TADDR(Debugger, g_pDebugger, m_RSRequestedSync);
  2622|         BOOL fTrue = TRUE;
  2623|         SafeWriteStructOrThrow<BOOL>(addr, &fTrue);
  2624|     }
  2625| }
  2626| HRESULT DacDbiInterfaceImpl::SetSendExceptionsOutsideOfJMC(BOOL sendExceptionsOutsideOfJMC)
  2627| {
  2628|     DD_ENTER_MAY_THROW
  2629|     HRESULT hr = S_OK;
  2630|     EX_TRY
  2631|     {
  2632|         if (g_pDebugger != NULL)
  2633|         {
  2634|             TADDR addr = PTR_HOST_MEMBER_TADDR(Debugger, g_pDebugger, m_sendExceptionsOutsideOfJMC);
  2635|             SafeWriteStructOrThrow<BOOL>(addr, &sendExceptionsOutsideOfJMC);
  2636|         }
  2637|     }
  2638|     EX_CATCH_HRESULT(hr);
  2639|     return hr;
  2640| }
  2641| void DacDbiInterfaceImpl::MarkDebuggerAttachPending()
  2642| {
  2643|     DD_ENTER_MAY_THROW;
  2644|     if (g_pDebugger != NULL)
  2645|     {
  2646|         DWORD flags = g_CORDebuggerControlFlags;
  2647|         flags |= DBCF_PENDING_ATTACH;
  2648|         g_CORDebuggerControlFlags = flags;
  2649|     }
  2650|     else
  2651|     {
  2652|         ThrowHR(CORDBG_E_NOTREADY);
  2653|     }
  2654| }
  2655| void DacDbiInterfaceImpl::MarkDebuggerAttached(BOOL fAttached)
  2656| {
  2657|     DD_ENTER_MAY_THROW;
  2658|     if (g_pDebugger != NULL)
  2659|     {
  2660|         DWORD flags = g_CORDebuggerControlFlags;
  2661|         if (fAttached)
  2662|         {
  2663|             flags |= DBCF_ATTACHED;
  2664|         }
  2665|         else
  2666|         {
  2667|             flags &= ~ (DBCF_ATTACHED | DBCF_PENDING_ATTACH);
  2668|         }
  2669|         g_CORDebuggerControlFlags = flags;
  2670|     }
  2671|     else if (fAttached)
  2672|     {
  2673|         ThrowHR(CORDBG_E_NOTREADY);
  2674|     }
  2675| }
  2676| void DacDbiInterfaceImpl::EnumerateThreads(FP_THREAD_ENUMERATION_CALLBACK fpCallback, void * pUserData)
  2677| {
  2678|     DD_ENTER_MAY_THROW;
  2679|     if (ThreadStore::s_pThreadStore == NULL)
  2680|     {
  2681|         return;
  2682|     }
  2683|     Thread *pThread = ThreadStore::GetThreadList(NULL);
  2684|     while (pThread != NULL)
  2685|     {
  2686|         Thread::ThreadState threadState = pThread->GetSnapshotState();
  2687|         if (!((IsThreadMarkedDeadWorker(pThread)) || (threadState & Thread::TS_Unstarted)))
  2688|         {
  2689|             VMPTR_Thread vmThread = VMPTR_Thread::NullPtr();
  2690|             vmThread.SetHostPtr(pThread);
  2691|             fpCallback(vmThread, pUserData);
  2692|         }
  2693|         pThread = ThreadStore::GetThreadList(pThread);
  2694|     }
  2695| }
  2696| bool DacDbiInterfaceImpl::IsThreadMarkedDead(VMPTR_Thread vmThread)
  2697| {
  2698|     DD_ENTER_MAY_THROW;
  2699|     Thread * pThread = vmThread.GetDacPtr();
  2700|     return IsThreadMarkedDeadWorker(pThread);
  2701| }
  2702| bool DacDbiInterfaceImpl::IsThreadMarkedDeadWorker(Thread * pThread)
  2703| {
  2704|     _ASSERTE(pThread != NULL);
  2705|     Thread::ThreadState threadState = pThread->GetSnapshotState();
  2706|     bool fIsDead = (threadState & Thread::TS_Dead) != 0;
  2707|     return fIsDead;
  2708| }
  2709| HANDLE DacDbiInterfaceImpl::GetThreadHandle(VMPTR_Thread vmThread)
  2710| {
  2711|     DD_ENTER_MAY_THROW;
  2712|     Thread * pThread = vmThread.GetDacPtr();
  2713|     return pThread->GetThreadHandle();
  2714| }
  2715| VMPTR_OBJECTHANDLE DacDbiInterfaceImpl::GetThreadObject(VMPTR_Thread vmThread)
  2716| {
  2717|     DD_ENTER_MAY_THROW;
  2718|     Thread * pThread = vmThread.GetDacPtr();
  2719|     Thread::ThreadState threadState = pThread->GetSnapshotState();
  2720|     if ( (threadState & Thread::TS_Dead) ||
  2721|          (threadState & Thread::TS_Unstarted) ||
  2722|          (threadState & Thread::TS_Detached) ||
  2723|          g_fProcessDetach )
  2724|     {
  2725|         ThrowHR(CORDBG_E_BAD_THREAD_STATE);
  2726|     }
  2727|     else
  2728|     {
  2729|         VMPTR_OBJECTHANDLE vmObjHandle = VMPTR_OBJECTHANDLE::NullPtr();
  2730|         vmObjHandle.SetDacTargetPtr(pThread->GetExposedObjectHandleForDebugger());
  2731|         return vmObjHandle;
  2732|     }
  2733| }
  2734| void DacDbiInterfaceImpl::GetThreadAllocInfo(VMPTR_Thread        vmThread,
  2735|                                              DacThreadAllocInfo* threadAllocInfo)
  2736| {
  2737|     DD_ENTER_MAY_THROW;
  2738|     Thread * pThread = vmThread.GetDacPtr();
  2739|     gc_alloc_context* allocContext = pThread->GetAllocContext();
  2740|     threadAllocInfo->m_allocBytesSOH = allocContext->alloc_bytes - (allocContext->alloc_limit - allocContext->alloc_ptr);
  2741|     threadAllocInfo->m_allocBytesUOH = allocContext->alloc_bytes_uoh;
  2742| }
  2743| void DacDbiInterfaceImpl::SetDebugState(VMPTR_Thread        vmThread,
  2744|                                         CorDebugThreadState debugState)
  2745| {
  2746|     DD_ENTER_MAY_THROW;
  2747|     Thread * pThread = vmThread.GetDacPtr();
  2748|     if (debugState == THREAD_SUSPEND)
  2749|     {
  2750|         pThread->SetThreadStateNC(Thread::TSNC_DebuggerUserSuspend);
  2751|     }
  2752|     else if (debugState == THREAD_RUN)
  2753|     {
  2754|         pThread->ResetThreadStateNC(Thread::TSNC_DebuggerUserSuspend);
  2755|     }
  2756|     else
  2757|     {
  2758|         ThrowHR(E_INVALIDARG);
  2759|     }
  2760|     TADDR taThreadState = PTR_HOST_MEMBER_TADDR(Thread, pThread, m_StateNC);
  2761|     SafeWriteStructOrThrow<Thread::ThreadStateNoConcurrency>(taThreadState, &(pThread->m_StateNC));
  2762| }
  2763| BOOL DacDbiInterfaceImpl::HasUnhandledException(VMPTR_Thread vmThread)
  2764| {
  2765|     DD_ENTER_MAY_THROW;
  2766|     Thread * pThread = vmThread.GetDacPtr();
  2767|     if(pThread->IsLastThrownObjectUnhandled())
  2768|     {
  2769|         return TRUE;
  2770|     }
  2771|     OBJECTHANDLE ohException = pThread->GetThrowableAsHandle();
  2772|     if (ohException != NULL)
  2773|     {
  2774|         return pThread->GetExceptionState()->GetFlags()->IsUnhandled() &&
  2775|             !(pThread->GetExceptionState()->GetFlags()->DebuggerInterceptInfo());
  2776|     }
  2777|     return FALSE;
  2778| }
  2779| CorDebugUserState DacDbiInterfaceImpl::GetUserState(VMPTR_Thread vmThread)
  2780| {
  2781|     DD_ENTER_MAY_THROW;
  2782|     UINT result = 0;
  2783|     result = GetPartialUserState(vmThread);
  2784|     if (!IsThreadAtGCSafePlace(vmThread))
  2785|     {
  2786|         result |= USER_UNSAFE_POINT;
  2787|     }
  2788|     return (CorDebugUserState)result;
  2789| }
  2790| CONNID DacDbiInterfaceImpl::GetConnectionID(VMPTR_Thread vmThread)
  2791| {
  2792|     DD_ENTER_MAY_THROW;
  2793|     return INVALID_CONNECTION_ID;
  2794| }
  2795| TASKID DacDbiInterfaceImpl::GetTaskID(VMPTR_Thread vmThread)
  2796| {
  2797|     DD_ENTER_MAY_THROW;
  2798|     return INVALID_TASK_ID;
  2799| }
  2800| DWORD DacDbiInterfaceImpl::TryGetVolatileOSThreadID(VMPTR_Thread vmThread)
  2801| {
  2802|     DD_ENTER_MAY_THROW;
  2803|     Thread * pThread = vmThread.GetDacPtr();
  2804|     _ASSERTE(pThread != NULL);
  2805|     DWORD dwThreadId = pThread->GetOSThreadIdForDebugger();
  2806|     const DWORD dwSwitchedOutThreadId = SWITCHED_OUT_FIBER_OSID;
  2807|     if (dwThreadId == dwSwitchedOutThreadId)
  2808|     {
  2809|         return 0;
  2810|     }
  2811|     return dwThreadId;
  2812| }
  2813| DWORD DacDbiInterfaceImpl::GetUniqueThreadID(VMPTR_Thread vmThread)
  2814| {
  2815|     DD_ENTER_MAY_THROW;
  2816|     Thread * pThread = vmThread.GetDacPtr();
  2817|     _ASSERTE(pThread != NULL);
  2818|     return pThread->GetOSThreadId();
  2819| }
  2820| VMPTR_OBJECTHANDLE DacDbiInterfaceImpl::GetCurrentException(VMPTR_Thread vmThread)
  2821| {
  2822|     DD_ENTER_MAY_THROW;
  2823|     Thread * pThread = vmThread.GetDacPtr();
  2824|     OBJECTHANDLE ohException = pThread->GetThrowableAsHandle();        // ohException can be NULL
  2825|     if (ohException == NULL)
  2826|     {
  2827|         if (pThread->IsLastThrownObjectUnhandled())
  2828|         {
  2829|             ohException = pThread->LastThrownObjectHandle();
  2830|         }
  2831|     }
  2832|     VMPTR_OBJECTHANDLE vmObjHandle;
  2833|     vmObjHandle.SetDacTargetPtr(ohException);
  2834|     return vmObjHandle;
  2835| }
  2836| VMPTR_OBJECTHANDLE DacDbiInterfaceImpl::GetObjectForCCW(CORDB_ADDRESS ccwPtr)
  2837| {
  2838|     DD_ENTER_MAY_THROW;
  2839|     OBJECTHANDLE ohCCW = NULL;
  2840| #ifdef FEATURE_COMWRAPPERS
  2841|     if (DACTryGetComWrappersHandleFromCCW(ccwPtr, &ohCCW) != S_OK)
  2842|     {
  2843| #endif
  2844| #ifdef FEATURE_COMINTEROP
  2845|     ComCallWrapper *pCCW = DACGetCCWFromAddress(ccwPtr);
  2846|     if (pCCW)
  2847|     {
  2848|         ohCCW = pCCW->GetObjectHandle();
  2849|     }
  2850| #endif
  2851| #ifdef FEATURE_COMWRAPPERS
  2852|     }
  2853| #endif
  2854|     VMPTR_OBJECTHANDLE vmObjHandle;
  2855|     vmObjHandle.SetDacTargetPtr(ohCCW);
  2856|     return vmObjHandle;
  2857| }
  2858| VMPTR_OBJECTHANDLE DacDbiInterfaceImpl::GetCurrentCustomDebuggerNotification(VMPTR_Thread vmThread)
  2859| {
  2860|     DD_ENTER_MAY_THROW;
  2861|     Thread * pThread = vmThread.GetDacPtr();
  2862|     OBJECTHANDLE ohNotification = pThread->GetThreadCurrNotification();        // ohNotification can be NULL
  2863|     VMPTR_OBJECTHANDLE vmObjHandle;
  2864|     vmObjHandle.SetDacTargetPtr(ohNotification);
  2865|     return vmObjHandle;
  2866| }
  2867| VMPTR_AppDomain DacDbiInterfaceImpl::GetCurrentAppDomain(VMPTR_Thread vmThread)
  2868| {
  2869|     DD_ENTER_MAY_THROW;
  2870|     Thread *    pThread    = vmThread.GetDacPtr();
  2871|     AppDomain * pAppDomain = pThread->GetDomain();
  2872|     if (pAppDomain == NULL)
  2873|     {
  2874|         ThrowHR(E_FAIL);
  2875|     }
  2876|     VMPTR_AppDomain vmAppDomain = VMPTR_AppDomain::NullPtr();
  2877|     vmAppDomain.SetDacTargetPtr(PTR_HOST_TO_TADDR(pAppDomain));
  2878|     return vmAppDomain;
  2879| }
  2880| CLR_DEBUGGING_PROCESS_FLAGS DacDbiInterfaceImpl::GetAttachStateFlags()
  2881| {
  2882|     DD_ENTER_MAY_THROW;
  2883|     CLR_DEBUGGING_PROCESS_FLAGS res = (CLR_DEBUGGING_PROCESS_FLAGS)0;
  2884|     if (g_pDebugger != NULL)
  2885|     {
  2886|         res = g_pDebugger->GetAttachStateFlags();
  2887|     }
  2888|     else
  2889|     {
  2890|     }
  2891|     return res;
  2892| }
  2893| TADDR DacDbiInterfaceImpl::GetHijackAddress()
  2894| {
  2895|     TADDR addr = NULL;
  2896|     if (g_pDebugger != NULL)
  2897|     {
  2898|         addr = dac_cast<TADDR>(g_pDebugger->m_rgHijackFunction[Debugger::kUnhandledException].StartAddress());
  2899|     }
  2900|     if (addr == NULL)
  2901|     {
  2902|         ThrowHR(CORDBG_E_NOTREADY);
  2903|     }
  2904|     return addr;
  2905| }
  2906| bool DacDbiInterfaceImpl::IsRuntimeUnwindableStub(PCODE targetControlPC)
  2907| {
  2908|     TADDR controlPC = PCODEToPINSTR(targetControlPC);
  2909|     if(!m_isCachedHijackFunctionValid)
  2910|     {
  2911|         Debugger* pDebugger = g_pDebugger;
  2912|         if ((pDebugger == NULL) || (pDebugger->m_rgHijackFunction == NULL))
  2913|         {
  2914|             return false;
  2915|         }
  2916|         for (int i = 0; i < Debugger::kMaxHijackFunctions; i++)
  2917|         {
  2918|             InitTargetBufferFromMemoryRange(pDebugger->m_rgHijackFunction[i], &m_pCachedHijackFunction[i] );
  2919|         }
  2920|         m_isCachedHijackFunctionValid = TRUE;
  2921|     }
  2922|     for (int i = 0; i < Debugger::kMaxHijackFunctions; i++)
  2923|     {
  2924|         CORDB_ADDRESS start = m_pCachedHijackFunction[i].pAddress;
  2925|         CORDB_ADDRESS end = start + m_pCachedHijackFunction[i].cbSize;
  2926|         if ((start <= controlPC) && (controlPC < end))
  2927|         {
  2928|             return true;
  2929|         }
  2930|     }
  2931|     return false;
  2932| }
  2933| void DacDbiInterfaceImpl::AlignStackPointer(CORDB_ADDRESS * pEsp)
  2934| {
  2935|     SUPPORTS_DAC;
  2936| #if defined(HOST_64BIT)
  2937|     *pEsp &= ~((CORDB_ADDRESS) 0xF);
  2938| #endif
  2939| }
  2940| template <class T>
  2941| CORDB_ADDRESS DacDbiInterfaceImpl::PushHelper(CORDB_ADDRESS * pEsp,
  2942|                                               const T * pData,
  2943|                                               BOOL fAlignStack)
  2944| {
  2945|     SUPPORTS_DAC;
  2946|     if (fAlignStack == TRUE)
  2947|     {
  2948|         AlignStackPointer(pEsp);
  2949|     }
  2950|     *pEsp -= sizeof(T);
  2951|     if (fAlignStack == TRUE)
  2952|     {
  2953|         AlignStackPointer(pEsp);
  2954|     }
  2955|     SafeWriteStructOrThrow(*pEsp, pData);
  2956|     return *pEsp;
  2957| }
  2958| void DacDbiInterfaceImpl::WriteExceptionRecordHelper(CORDB_ADDRESS pRemotePtr,
  2959|                                                      const EXCEPTION_RECORD * pExcepRecord)
  2960| {
  2961|     ULONG32 cbSize = offsetof(EXCEPTION_RECORD, ExceptionInformation);
  2962|     cbSize += pExcepRecord->NumberParameters * sizeof(pExcepRecord->ExceptionInformation[0]);
  2963|     HRESULT hr = m_pMutableTarget->WriteVirtual(pRemotePtr,
  2964|                                                 reinterpret_cast<const BYTE *>(pExcepRecord),
  2965|                                                 cbSize);
  2966|     if (FAILED(hr))
  2967|     {
  2968|         ThrowHR(hr);
  2969|     }
  2970| }
  2971| void DacDbiInterfaceImpl::Hijack(
  2972|     VMPTR_Thread                 vmThread,
  2973|     ULONG32                      dwThreadId,
  2974|     const EXCEPTION_RECORD *     pRecord,
  2975|     T_CONTEXT *                  pOriginalContext,
  2976|     ULONG32                      cbSizeContext,
  2977|     EHijackReason::EHijackReason reason,
  2978|     void *                       pUserData,
  2979|     CORDB_ADDRESS *              pRemoteContextAddr)
  2980| {
  2981|     DD_ENTER_MAY_THROW;
  2982|     _ASSERTE((pOriginalContext == NULL) == (cbSizeContext == 0));
  2983|     _ASSERTE(EHijackReason::IsValid(reason));
  2984| #ifdef TARGET_UNIX
  2985|     _ASSERTE(!"Not supported on this platform");
  2986| #endif
  2987|     Thread* pThread = NULL;
  2988|     if(!vmThread.IsNull())
  2989|     {
  2990|         pThread = vmThread.GetDacPtr();
  2991|         _ASSERTE(pThread->GetOSThreadIdForDebugger() == dwThreadId);
  2992|     }
  2993|     TADDR pfnHijackFunction = GetHijackAddress();
  2994|     T_CONTEXT ctx;
  2995|     HRESULT hr = m_pTarget->GetThreadContext(
  2996|         dwThreadId,
  2997|         CONTEXT_FULL,
  2998|         sizeof(DT_CONTEXT),
  2999|         (BYTE*) &ctx);
  3000|     IfFailThrow(hr);
  3001|     if (pOriginalContext != NULL)
  3002|     {
  3003|         if (cbSizeContext != sizeof(T_CONTEXT))
  3004|         {
  3005|             ThrowHR(E_INVALIDARG);
  3006|         }
  3007|         memcpy(pOriginalContext, &ctx, cbSizeContext);
  3008|     }
  3009| #ifndef FEATURE_EMULATE_SINGLESTEP
  3010|     UnsetSSFlag(reinterpret_cast<DT_CONTEXT *>(&ctx));
  3011| #endif
  3012|     void* espContext = NULL;
  3013|     void* espRecord = NULL;
  3014|     const void* pData = pUserData;
  3015|     CORDB_ADDRESS esp = GetSP(&ctx);
  3016|     CORDB_ADDRESS espOSContext = NULL;
  3017|     CORDB_ADDRESS espOSRecord  = NULL;
  3018|     if (pThread != NULL && pThread->IsExceptionInProgress())
  3019|     {
  3020|         espOSContext = (CORDB_ADDRESS)PTR_TO_TADDR(pThread->GetExceptionState()->GetContextRecord());
  3021|         espOSRecord  = (CORDB_ADDRESS)PTR_TO_TADDR(pThread->GetExceptionState()->GetExceptionRecord());
  3022|         if (espOSContext < esp)
  3023|         {
  3024|             SafeWriteStructOrThrow(espOSContext, &ctx);
  3025|             espContext = CORDB_ADDRESS_TO_PTR(espOSContext);
  3026|             _ASSERTE(pRecord != NULL);
  3027|             WriteExceptionRecordHelper(espOSRecord, pRecord);
  3028|             espRecord  = CORDB_ADDRESS_TO_PTR(espOSRecord);
  3029|             esp = min(espOSContext, espOSRecord);
  3030|         }
  3031|     }
  3032|     if (espContext == NULL)
  3033|     {
  3034|         _ASSERTE(espRecord == NULL);
  3035|         espContext = CORDB_ADDRESS_TO_PTR(PushHelper(&esp, &ctx, TRUE));
  3036|         if (pRecord != NULL)
  3037|         {
  3038|             espRecord  = CORDB_ADDRESS_TO_PTR(PushHelper(&esp, pRecord, TRUE));
  3039|         }
  3040|     }
  3041|     if(pRemoteContextAddr != NULL)
  3042|     {
  3043|         *pRemoteContextAddr = PTR_TO_CORDB_ADDRESS(espContext);
  3044|     }
  3045| #if defined(TARGET_X86)  // TARGET
  3046|     PushHelper(&esp, &pData, TRUE);
  3047|     PushHelper(&esp, &reason, TRUE);
  3048|     PushHelper(&esp, &espRecord, TRUE);
  3049|     PushHelper(&esp, &espContext, TRUE);
  3050| #elif defined (TARGET_AMD64) // TARGET
  3051|     ctx.Rcx = (DWORD64) espContext;
  3052|     ctx.Rdx = (DWORD64) espRecord;
  3053|     ctx.R8  = (DWORD64) reason;
  3054|     ctx.R9  = (DWORD64) pData;
  3055|     PushHelper(&esp, reinterpret_cast<SIZE_T *>(&(ctx.R9)), FALSE);
  3056|     PushHelper(&esp, reinterpret_cast<SIZE_T *>(&(ctx.R8)), FALSE);
  3057|     PushHelper(&esp, reinterpret_cast<SIZE_T *>(&(ctx.Rdx)), FALSE);
  3058|     PushHelper(&esp, reinterpret_cast<SIZE_T *>(&(ctx.Rcx)), FALSE);
  3059| #elif defined(TARGET_ARM)
  3060|     ctx.R0 = (DWORD)espContext;
  3061|     ctx.R1 = (DWORD)espRecord;
  3062|     ctx.R2 = (DWORD)reason;
  3063|     ctx.R3 = (DWORD)pData;
  3064| #elif defined(TARGET_ARM64)
  3065|     ctx.X0 = (DWORD64)espContext;
  3066|     ctx.X1 = (DWORD64)espRecord;
  3067|     ctx.X2 = (DWORD64)reason;
  3068|     ctx.X3 = (DWORD64)pData;
  3069| #else
  3070|     PORTABILITY_ASSERT("CordbThread::HijackForUnhandledException is not implemented on this platform.");
  3071| #endif
  3072|     SetSP(&ctx, CORDB_ADDRESS_TO_TADDR(esp));
  3073|     SetIP(&ctx, pfnHijackFunction);
  3074|     hr = m_pMutableTarget->SetThreadContext(dwThreadId, sizeof(DT_CONTEXT), reinterpret_cast<BYTE*> (&ctx));
  3075|     IfFailThrow(hr);
  3076| }
  3077| VMPTR_CONTEXT DacDbiInterfaceImpl::GetManagedStoppedContext(VMPTR_Thread vmThread)
  3078| {
  3079|     DD_ENTER_MAY_THROW;
  3080|     VMPTR_CONTEXT vmContext = VMPTR_CONTEXT::NullPtr();
  3081|     Thread * pThread = vmThread.GetDacPtr();
  3082|     if (pThread->GetInteropDebuggingHijacked())
  3083|     {
  3084|         _ASSERTE(!ISREDIRECTEDTHREAD(pThread));
  3085|         vmContext = VMPTR_CONTEXT::NullPtr();
  3086|     }
  3087|     else
  3088|     {
  3089|         DT_CONTEXT * pLSContext = reinterpret_cast<DT_CONTEXT *>(pThread->GetFilterContext());
  3090|         if (pLSContext != NULL)
  3091|         {
  3092|             _ASSERTE(!ISREDIRECTEDTHREAD(pThread));
  3093|             vmContext.SetHostPtr(pLSContext);
  3094|         }
  3095|         else if (ISREDIRECTEDTHREAD(pThread))
  3096|         {
  3097|             pLSContext = reinterpret_cast<DT_CONTEXT *>(GETREDIRECTEDCONTEXT(pThread));
  3098|             _ASSERTE(pLSContext != NULL);
  3099|             if (pLSContext != NULL)
  3100|             {
  3101|                 vmContext.SetHostPtr(pLSContext);
  3102|             }
  3103|         }
  3104|     }
  3105|     return vmContext;
  3106| }
  3107| TargetBuffer DacDbiInterfaceImpl::GetVarArgSig(CORDB_ADDRESS   VASigCookieAddr,
  3108|                                                CORDB_ADDRESS * pArgBase)
  3109| {
  3110|     DD_ENTER_MAY_THROW;
  3111|     _ASSERTE(pArgBase != NULL);
  3112|     *pArgBase = NULL;
  3113|     TADDR taVASigCookie = NULL;
  3114|     SafeReadStructOrThrow(VASigCookieAddr, &taVASigCookie);
  3115|     VASigCookie * pVACookie = PTR_VASigCookie(taVASigCookie);
  3116| #if defined(TARGET_X86) // (STACK_GROWS_DOWN_ON_ARGS_WALK)
  3117|     *pArgBase = VASigCookieAddr + pVACookie->sizeOfArgs;
  3118| #else  // !TARGET_X86 (STACK_GROWS_UP_ON_ARGS_WALK)
  3119|     *pArgBase = VASigCookieAddr + sizeof(VASigCookie *);
  3120| #endif // !TARGET_X86 (STACK_GROWS_UP_ON_ARGS_WALK)
  3121|     return TargetBuffer(PTR_TO_CORDB_ADDRESS(pVACookie->signature.GetRawSig()),
  3122|                         pVACookie->signature.GetRawSigLen());
  3123| }
  3124| BOOL DacDbiInterfaceImpl::RequiresAlign8(VMPTR_TypeHandle thExact)
  3125| {
  3126|     DD_ENTER_MAY_THROW;
  3127| #ifdef FEATURE_64BIT_ALIGNMENT
  3128|     TypeHandle th = TypeHandle::FromPtr(thExact.GetDacPtr());
  3129|     PTR_MethodTable mt = th.AsMethodTable();
  3130|     return mt->RequiresAlign8();
  3131| #else
  3132|     ThrowHR(E_NOTIMPL);
  3133| #endif
  3134| }
  3135| GENERICS_TYPE_TOKEN DacDbiInterfaceImpl::ResolveExactGenericArgsToken(DWORD               dwExactGenericArgsTokenIndex,
  3136|                                                                       GENERICS_TYPE_TOKEN rawToken)
  3137| {
  3138|     DD_ENTER_MAY_THROW;
  3139|     if (dwExactGenericArgsTokenIndex == 0)
  3140|     {
  3141|         if (rawToken == 0)
  3142|         {
  3143|             return rawToken;
  3144|         }
  3145|         TADDR addrObjThis = CORDB_ADDRESS_TO_TADDR(rawToken);
  3146|         PTR_Object pObjThis = dac_cast<PTR_Object>(addrObjThis);
  3147|         PTR_MethodTable pMT = pObjThis->GetMethodTable();
  3148|         TADDR addrMT = dac_cast<TADDR>(pMT);
  3149|         GENERICS_TYPE_TOKEN realToken = (GENERICS_TYPE_TOKEN) addrMT;
  3150|         return realToken;
  3151|     }
  3152|     else if (dwExactGenericArgsTokenIndex == (DWORD)ICorDebugInfo::TYPECTXT_ILNUM)
  3153|     {
  3154|         return  rawToken;
  3155|     }
  3156|     _ASSERTE(!"DDII::REGAT - Unexpected generics type token index.");
  3157|     ThrowHR(CORDBG_E_TARGET_INCONSISTENT);
  3158| }
  3159| IDacDbiInterface::DynamicMethodType DacDbiInterfaceImpl::IsILStubOrLCGMethod(VMPTR_MethodDesc vmMethodDesc)
  3160| {
  3161|     DD_ENTER_MAY_THROW;
  3162|     MethodDesc * pMD = vmMethodDesc.GetDacPtr();
  3163|     if (pMD->IsILStub())
  3164|     {
  3165|         return kILStub;
  3166|     }
  3167|     else if (pMD->IsLCGMethod())
  3168|     {
  3169|         return kLCGMethod;
  3170|     }
  3171|     else
  3172|     {
  3173|         return kNone;
  3174|     }
  3175| }
  3176| BOOL DacDbiInterfaceImpl::IsThreadAtGCSafePlace(VMPTR_Thread vmThread)
  3177| {
  3178|     DD_ENTER_MAY_THROW;
  3179|     BOOL fIsGCSafe = FALSE;
  3180|     Thread * pThread = vmThread.GetDacPtr();
  3181|     if ((g_fEEShutDown & ShutDown_Finalize2) != 0)
  3182|     {
  3183|         fIsGCSafe = TRUE;
  3184|     }
  3185|     else
  3186|     {
  3187|         T_CONTEXT ctx;
  3188|         REGDISPLAY rd;
  3189|         SetUpRegdisplayForStackWalk(pThread, &ctx, &rd);
  3190|         ULONG32 flags = (QUICKUNWIND | HANDLESKIPPEDFRAMES | DISABLE_MISSING_FRAME_DETECTION);
  3191|         StackFrameIterator iter;
  3192|         iter.Init(pThread, pThread->GetFrame(), &rd, flags);
  3193|         CrawlFrame * pCF = &(iter.m_crawl);
  3194|         if (pCF->IsFrameless() && pCF->IsActiveFunc())
  3195|         {
  3196|             if (pCF->IsGcSafe())
  3197|             {
  3198|                 fIsGCSafe = TRUE;
  3199|             }
  3200|         }
  3201|     }
  3202|     return fIsGCSafe;
  3203| }
  3204| CorDebugUserState DacDbiInterfaceImpl::GetPartialUserState(VMPTR_Thread vmThread)
  3205| {
  3206|     DD_ENTER_MAY_THROW;
  3207|     Thread * pThread = vmThread.GetDacPtr();
  3208|     Thread::ThreadState ts = pThread->GetSnapshotState();
  3209|     UINT result = 0;
  3210|     if (ts & Thread::TS_Background)
  3211|     {
  3212|         result |= USER_BACKGROUND;
  3213|     }
  3214|     if (ts & Thread::TS_Unstarted)
  3215|     {
  3216|         result |= USER_UNSTARTED;
  3217|     }
  3218|     if (ts & Thread::TS_Dead)
  3219|     {
  3220|         result |= USER_STOPPED;
  3221|     }
  3222|     if (ts & Thread::TS_Interruptible || pThread->HasThreadStateNC(Thread::TSNC_DebuggerSleepWaitJoin))
  3223|     {
  3224|         result |= USER_WAIT_SLEEP_JOIN;
  3225|     }
  3226|     if (pThread->IsThreadPoolThread())
  3227|     {
  3228|         result |= USER_THREADPOOL;
  3229|     }
  3230|     return (CorDebugUserState)result;
  3231| }
  3232| void DacDbiInterfaceImpl::LookupEnCVersions(Module*          pModule,
  3233|                                             VMPTR_MethodDesc vmMethodDesc,
  3234|                                             mdMethodDef      mdMethod,
  3235|                                             CORDB_ADDRESS    pNativeStartAddress,
  3236|                                             SIZE_T *         pLatestEnCVersion,
  3237|                                             SIZE_T *         pJittedInstanceEnCVersion /* = NULL */)
  3238| {
  3239|     MethodDesc * pMD     = vmMethodDesc.GetDacPtr();
  3240|     _ASSERTE(pMD->GetMemberDef() == mdMethod);
  3241|     _ASSERTE(pLatestEnCVersion != NULL);
  3242|     DebuggerMethodInfo * pDMI = NULL;
  3243|     DebuggerJitInfo * pDJI = NULL;
  3244|     EX_TRY_ALLOW_DATATARGET_MISSING_MEMORY
  3245|     {
  3246|         if (g_pDebugger != NULL)
  3247|         {
  3248|             pDMI = g_pDebugger->GetOrCreateMethodInfo(pModule, mdMethod);
  3249|             if (pDMI != NULL)
  3250|             {
  3251|                 pDJI = pDMI->FindJitInfo(pMD, CORDB_ADDRESS_TO_TADDR(pNativeStartAddress));
  3252|             }
  3253|         }
  3254|     }
  3255|     EX_END_CATCH_ALLOW_DATATARGET_MISSING_MEMORY;
  3256|     if (pDJI != NULL)
  3257|     {
  3258|         if (pJittedInstanceEnCVersion != NULL)
  3259|         {
  3260|             *pJittedInstanceEnCVersion = pDJI->m_encVersion;
  3261|         }
  3262|         *pLatestEnCVersion = pDMI->GetCurrentEnCVersion();
  3263|     }
  3264|     else
  3265|     {
  3266|         if (pJittedInstanceEnCVersion != NULL)
  3267|         {
  3268|             *pJittedInstanceEnCVersion = CorDB_DEFAULT_ENC_FUNCTION_VERSION;
  3269|         }
  3270|         *pLatestEnCVersion = CorDB_DEFAULT_ENC_FUNCTION_VERSION;
  3271|     }
  3272| }
  3273| CORDB_ADDRESS DacDbiInterfaceImpl::GetDebuggerControlBlockAddress()
  3274| {
  3275|     DD_ENTER_MAY_THROW;
  3276|     if ((g_pDebugger != NULL) &&
  3277|         (g_pDebugger->m_pRCThread != NULL))
  3278|     {
  3279|     return CORDB_ADDRESS(dac_cast<TADDR>(g_pDebugger->m_pRCThread->GetDCB()));
  3280|     }
  3281|     return NULL;
  3282| }
  3283| void DacDbiInterfaceImpl::GetContext(VMPTR_Thread vmThread, DT_CONTEXT * pContextBuffer)
  3284| {
  3285|     DD_ENTER_MAY_THROW
  3286|     _ASSERTE(pContextBuffer != NULL);
  3287|     Thread *  pThread  = vmThread.GetDacPtr();
  3288|     DT_CONTEXT * pFilterContext = reinterpret_cast<DT_CONTEXT *>(pThread->GetFilterContext());
  3289|     if (pFilterContext == NULL)
  3290|     {
  3291|         pContextBuffer->ContextFlags = DT_CONTEXT_ALL;
  3292|         HRESULT hr = m_pTarget->GetThreadContext(pThread->GetOSThreadId(),
  3293|                                                 pContextBuffer->ContextFlags,
  3294|                                                 sizeof(DT_CONTEXT),
  3295|                                                 reinterpret_cast<BYTE *>(pContextBuffer));
  3296|         if (hr == E_NOTIMPL)
  3297|         {
  3298|             REGDISPLAY tmpRd = {};
  3299|             T_CONTEXT tmpContext = {};
  3300|             FillRegDisplay(&tmpRd, &tmpContext);
  3301|             Frame *frame = pThread->GetFrame();
  3302|             while (frame != NULL && frame != FRAME_TOP)
  3303|             {
  3304|                 frame->UpdateRegDisplay(&tmpRd);
  3305|                 if (GetRegdisplaySP(&tmpRd) != 0 && GetControlPC(&tmpRd) != 0)
  3306|                 {
  3307|                     UpdateContextFromRegDisp(&tmpRd, &tmpContext);
  3308|                     CopyMemory(pContextBuffer, &tmpContext, sizeof(*pContextBuffer));
  3309|                     pContextBuffer->ContextFlags = DT_CONTEXT_CONTROL;
  3310|                     return;
  3311|                 }
  3312|                 frame = frame->Next();
  3313|             }
  3314|             ZeroMemory(pContextBuffer, sizeof(*pContextBuffer));
  3315|         }
  3316|         else
  3317|         {
  3318|             IfFailThrow(hr);
  3319|         }
  3320|     }
  3321|     else
  3322|     {
  3323|         *pContextBuffer = *pFilterContext;
  3324|     }
  3325| } // DacDbiInterfaceImpl::GetContext
  3326| VMPTR_Object DacDbiInterfaceImpl::GetObject(CORDB_ADDRESS ptr)
  3327| {
  3328|     DD_ENTER_MAY_THROW;
  3329|     VMPTR_Object vmObj = VMPTR_Object::NullPtr();
  3330|     vmObj.SetDacTargetPtr(CORDB_ADDRESS_TO_TADDR(ptr));
  3331|     return vmObj;
  3332| }
  3333| HRESULT DacDbiInterfaceImpl::EnableNGENPolicy(CorDebugNGENPolicy ePolicy)
  3334| {
  3335|     return E_NOTIMPL;
  3336| }
  3337| HRESULT DacDbiInterfaceImpl::SetNGENCompilerFlags(DWORD dwFlags)
  3338| {
  3339|     DD_ENTER_MAY_THROW;
  3340|     return CORDBG_E_NGEN_NOT_SUPPORTED;
  3341| }
  3342| HRESULT DacDbiInterfaceImpl::GetNGENCompilerFlags(DWORD *pdwFlags)
  3343| {
  3344|     DD_ENTER_MAY_THROW;
  3345|     return CORDBG_E_NGEN_NOT_SUPPORTED;
  3346| }
  3347| typedef DPTR(OBJECTREF) PTR_ObjectRef;
  3348| VMPTR_Object DacDbiInterfaceImpl::GetObjectFromRefPtr(CORDB_ADDRESS ptr)
  3349| {
  3350|     DD_ENTER_MAY_THROW;
  3351|     VMPTR_Object vmObj = VMPTR_Object::NullPtr();
  3352|     PTR_ObjectRef objRef = PTR_ObjectRef(CORDB_ADDRESS_TO_TADDR(ptr));
  3353|     vmObj.SetDacTargetPtr(PTR_TO_TADDR(*objRef));
  3354|     return vmObj;
  3355| }
  3356| VMPTR_OBJECTHANDLE DacDbiInterfaceImpl::GetVmObjectHandle(CORDB_ADDRESS handleAddress)
  3357| {
  3358|     DD_ENTER_MAY_THROW;
  3359|     VMPTR_OBJECTHANDLE vmObjHandle = VMPTR_OBJECTHANDLE::NullPtr();
  3360|     vmObjHandle.SetDacTargetPtr(CORDB_ADDRESS_TO_TADDR(handleAddress));
  3361|     return vmObjHandle;
  3362| }
  3363| BOOL DacDbiInterfaceImpl::IsVmObjectHandleValid(VMPTR_OBJECTHANDLE vmHandle)
  3364| {
  3365|     DD_ENTER_MAY_THROW;
  3366|     BOOL ret = FALSE;
  3367|     EX_TRY
  3368|     {
  3369|         OBJECTREF objRef = ObjectFromHandle((OBJECTHANDLE)vmHandle.GetDacPtr());
  3370|         if (objRef != NULL)
  3371|         {
  3372|             if (objRef->ValidateObjectWithPossibleAV())
  3373|             {
  3374|                 ret = TRUE;
  3375|             }
  3376|         }
  3377|     }
  3378|     EX_CATCH
  3379|     {
  3380|     }
  3381|     EX_END_CATCH(SwallowAllExceptions);
  3382|     return ret;
  3383| }
  3384| HRESULT DacDbiInterfaceImpl::IsWinRTModule(VMPTR_Module vmModule, BOOL& isWinRT)
  3385| {
  3386|     DD_ENTER_MAY_THROW;
  3387|     HRESULT hr = S_OK;
  3388|     isWinRT = FALSE;
  3389|     return hr;
  3390| }
  3391| ULONG DacDbiInterfaceImpl::GetAppDomainIdFromVmObjectHandle(VMPTR_OBJECTHANDLE vmHandle)
  3392| {
  3393|     DD_ENTER_MAY_THROW;
  3394|     return DefaultADID;
  3395| }
  3396| CORDB_ADDRESS DacDbiInterfaceImpl::GetHandleAddressFromVmHandle(VMPTR_OBJECTHANDLE vmHandle)
  3397| {
  3398|     DD_ENTER_MAY_THROW;
  3399|     CORDB_ADDRESS handle = vmHandle.GetDacPtr();
  3400|     return handle;
  3401| }
  3402| TargetBuffer DacDbiInterfaceImpl::GetObjectContents(VMPTR_Object vmObj)
  3403| {
  3404|     DD_ENTER_MAY_THROW;
  3405|     PTR_Object objPtr = vmObj.GetDacPtr();
  3406|     _ASSERTE(objPtr->GetSize() <= 0xffffffff);
  3407|     return TargetBuffer(PTR_TO_TADDR(objPtr), (ULONG)objPtr->GetSize());
  3408| }
  3409| HRESULT DacDbiInterfaceImpl::FastSanityCheckObject(PTR_Object objPtr)
  3410| {
  3411|     CONTRACTL
  3412|     {
  3413|         NOTHROW;
  3414|         GC_NOTRIGGER;
  3415|     }
  3416|     CONTRACTL_END;
  3417|     HRESULT hr = S_OK;
  3418|     EX_TRY
  3419|     {
  3420|         if (objPtr != NULL)
  3421|         {
  3422|             if (!objPtr->ValidateObjectWithPossibleAV())
  3423|             {
  3424|                 LOG((LF_CORDB, LL_INFO10000, "GOI: object methodtable-class invariant doesn't hold.\n"));
  3425|                 hr = E_INVALIDARG;
  3426|             }
  3427|         }
  3428|     }
  3429|     EX_CATCH
  3430|     {
  3431|         LOG((LF_CORDB, LL_INFO10000, "GOI: exception indicated ref is bad.\n"));
  3432|         hr = E_INVALIDARG;
  3433|     }
  3434|     EX_END_CATCH(SwallowAllExceptions);
  3435|     return hr;
  3436| }   // DacDbiInterfaceImpl::FastSanityCheckObject
  3437| bool DacDbiInterfaceImpl::CheckRef(PTR_Object objPtr)
  3438| {
  3439|     bool objRefBad = false;
  3440|     if (objPtr == NULL)
  3441|     {
  3442|         LOG((LF_CORDB, LL_INFO10000, "D::GOI: ref is NULL.\n"));
  3443|         objRefBad = true;
  3444|     }
  3445|     else
  3446|     {
  3447|         if (FAILED(FastSanityCheckObject(objPtr)))
  3448|         {
  3449|             LOG((LF_CORDB, LL_INFO10000, "D::GOI: address is not a valid object.\n"));
  3450|             objRefBad = true;
  3451|         }
  3452|     }
  3453|     return objRefBad;
  3454| } // DacDbiInterfaceImpl::CheckRef
  3455| void DacDbiInterfaceImpl::InitObjectData(PTR_Object                objPtr,
  3456|                                          VMPTR_AppDomain           vmAppDomain,
  3457|                                          DebuggerIPCE_ObjectData * pObjectData)
  3458| {
  3459|     _ASSERTE(pObjectData != NULL);
  3460|     VMPTR_TypeHandle vmTypeHandle = VMPTR_TypeHandle::NullPtr();
  3461|     vmTypeHandle.SetDacTargetPtr(objPtr->GetGCSafeTypeHandle().AsTAddr());
  3462|     pObjectData->objSize = objPtr->GetSize();
  3463|     pObjectData->objOffsetToVars = dac_cast<TADDR>((objPtr)->GetData()) - dac_cast<TADDR>(objPtr);
  3464|     TypeHandleToExpandedTypeInfo(AllBoxed, vmAppDomain, vmTypeHandle, &(pObjectData->objTypeData));
  3465|     if (objPtr->GetGCSafeMethodTable() == g_pStringClass)
  3466|     {
  3467|         pObjectData->objTypeData.elementType = ELEMENT_TYPE_STRING;
  3468|         if(pObjectData->objSize < MIN_OBJECT_SIZE)
  3469|         {
  3470|             pObjectData->objSize = PtrAlign(pObjectData->objSize);
  3471|         }
  3472|     }
  3473| } // DacDbiInterfaceImpl::InitObjectData
  3474| void DacDbiInterfaceImpl::GetTypedByRefInfo(CORDB_ADDRESS             pTypedByRef,
  3475|                                             VMPTR_AppDomain           vmAppDomain,
  3476|                                             DebuggerIPCE_ObjectData * pObjectData)
  3477| {
  3478|      DD_ENTER_MAY_THROW;
  3479|     PTR_TypedByRef refAddr = PTR_TypedByRef(TADDR(pTypedByRef));
  3480|     _ASSERTE(refAddr != NULL);
  3481|     _ASSERTE(pObjectData != NULL);
  3482|     TypeHandleToBasicTypeInfo(refAddr->type,
  3483|                               &(pObjectData->typedByrefInfo.typedByrefType),
  3484|                               vmAppDomain.GetDacPtr());
  3485|     CORDB_ADDRESS tempRef = dac_cast<TADDR>(refAddr->data);
  3486|     pObjectData->objRef = CORDB_ADDRESS_TO_PTR(tempRef);
  3487|     LOG((LF_CORDB, LL_INFO10000, "D::GASOI: sending REFANY result: "
  3488|          "ref=0x%08x, cls=0x%08x, mod=0x%p\n",
  3489|          pObjectData->objRef,
  3490|          pObjectData->typedByrefType.metadataToken,
  3491|          pObjectData->typedByrefType.vmDomainAssembly.GetDacPtr()));
  3492| } // DacDbiInterfaceImpl::GetTypedByRefInfo
  3493| void DacDbiInterfaceImpl::GetStringData(CORDB_ADDRESS objectAddress, DebuggerIPCE_ObjectData * pObjectData)
  3494| {
  3495|     DD_ENTER_MAY_THROW;
  3496|     PTR_Object objPtr = PTR_Object(TADDR(objectAddress));
  3497|     LOG((LF_CORDB, LL_INFO10000, "D::GOI: The referent is a string.\n"));
  3498|     if (objPtr->GetGCSafeMethodTable() != g_pStringClass)
  3499|     {
  3500|         ThrowHR(CORDBG_E_TARGET_INCONSISTENT);
  3501|     }
  3502|     PTR_StringObject pStrObj = dac_cast<PTR_StringObject>(objPtr);
  3503|     _ASSERTE(pStrObj != NULL);
  3504|     pObjectData->stringInfo.length = pStrObj->GetStringLength();
  3505|     pObjectData->stringInfo.offsetToStringBase = (UINT_PTR) pStrObj->GetBufferOffset();
  3506| } // DacDbiInterfaceImpl::GetStringData
  3507| void DacDbiInterfaceImpl::GetArrayData(CORDB_ADDRESS objectAddress, DebuggerIPCE_ObjectData * pObjectData)
  3508| {
  3509|     DD_ENTER_MAY_THROW;
  3510|     PTR_Object objPtr = PTR_Object(TADDR(objectAddress));
  3511|     PTR_MethodTable pMT = objPtr->GetGCSafeMethodTable();
  3512|     if (!objPtr->GetGCSafeTypeHandle().IsArray())
  3513|     {
  3514|         LOG((LF_CORDB, LL_INFO10000,
  3515|              "D::GASOI: object should be an array.\n"));
  3516|         pObjectData->objRefBad = true;
  3517|     }
  3518|     else
  3519|     {
  3520|         PTR_ArrayBase arrPtr = dac_cast<PTR_ArrayBase>(objPtr);
  3521|         pObjectData->arrayInfo.rank = arrPtr->GetRank();
  3522|         pObjectData->arrayInfo.componentCount = arrPtr->GetNumComponents();
  3523|         pObjectData->arrayInfo.offsetToArrayBase = arrPtr->GetDataPtrOffset(pMT);
  3524|         if (arrPtr->IsMultiDimArray())
  3525|         {
  3526|             pObjectData->arrayInfo.offsetToUpperBounds = SIZE_T(arrPtr->GetBoundsOffset(pMT));
  3527|             pObjectData->arrayInfo.offsetToLowerBounds = SIZE_T(arrPtr->GetLowerBoundsOffset(pMT));
  3528|         }
  3529|         else
  3530|         {
  3531|             pObjectData->arrayInfo.offsetToUpperBounds = 0;
  3532|             pObjectData->arrayInfo.offsetToLowerBounds = 0;
  3533|         }
  3534|         pObjectData->arrayInfo.elementSize = arrPtr->GetComponentSize();
  3535|         LOG((LF_CORDB, LL_INFO10000, "D::GOI: array info: "
  3536|             "baseOff=%d, lowerOff=%d, upperOff=%d, cnt=%d, rank=%d, rank (2) = %d,"
  3537|              "eleSize=%d, eleType=0x%02x\n",
  3538|              pObjectData->arrayInfo.offsetToArrayBase,
  3539|              pObjectData->arrayInfo.offsetToLowerBounds,
  3540|              pObjectData->arrayInfo.offsetToUpperBounds,
  3541|              pObjectData->arrayInfo.componentCount,
  3542|              pObjectData->arrayInfo.rank,
  3543|              pObjectData->objTypeData.ArrayTypeData.arrayRank,
  3544|              pObjectData->arrayInfo.elementSize,
  3545|              pObjectData->objTypeData.ArrayTypeData.arrayTypeArg.elementType));
  3546|     }
  3547| } // DacDbiInterfaceImpl::GetArrayData
  3548| void DacDbiInterfaceImpl::GetBasicObjectInfo(CORDB_ADDRESS             objectAddress,
  3549|                                              CorElementType            type,
  3550|                                              VMPTR_AppDomain           vmAppDomain,
  3551|                                              DebuggerIPCE_ObjectData * pObjectData)
  3552| {
  3553|     DD_ENTER_MAY_THROW;
  3554|     PTR_Object objPtr = PTR_Object(TADDR(objectAddress));
  3555|     pObjectData->objRefBad = CheckRef(objPtr);
  3556|     if (pObjectData->objRefBad != true)
  3557|     {
  3558|         InitObjectData (objPtr, vmAppDomain, pObjectData);
  3559|     }
  3560| } // DacDbiInterfaceImpl::GetBasicObjectInfo
  3561| struct BlockingObjectUserDataWrapper
  3562| {
  3563|     CALLBACK_DATA pUserData;
  3564|     IDacDbiInterface::FP_BLOCKINGOBJECT_ENUMERATION_CALLBACK fpCallback;
  3565| };
  3566| void EnumerateBlockingObjectsCallback(PTR_DebugBlockingItem obj, VOID* pUserData)
  3567| {
  3568|     BlockingObjectUserDataWrapper* wrapper = (BlockingObjectUserDataWrapper*)pUserData;
  3569|     DacBlockingObject dacObj;
  3570|     dacObj.blockingReason = DacBlockReason_MonitorCriticalSection;
  3571|     dacObj.vmBlockingObject.SetDacTargetPtr(dac_cast<TADDR>(OBJECTREFToObject(obj->pMonitor->GetOwningObject())));
  3572|     dacObj.dwTimeout = obj->dwTimeout;
  3573|     dacObj.vmAppDomain.SetDacTargetPtr(dac_cast<TADDR>(obj->pAppDomain));
  3574|     switch(obj->type)
  3575|     {
  3576|         case DebugBlock_MonitorCriticalSection:
  3577|             dacObj.blockingReason = DacBlockReason_MonitorCriticalSection;
  3578|             break;
  3579|         case DebugBlock_MonitorEvent:
  3580|             dacObj.blockingReason = DacBlockReason_MonitorEvent;
  3581|             break;
  3582|         default:
  3583|             _ASSERTE(!"obj->type has an invalid value");
  3584|             return;
  3585|     }
  3586|     wrapper->fpCallback(dacObj, wrapper->pUserData);
  3587| }
  3588| void DacDbiInterfaceImpl::EnumerateBlockingObjects(VMPTR_Thread                           vmThread,
  3589|                                                    FP_BLOCKINGOBJECT_ENUMERATION_CALLBACK fpCallback,
  3590|                                                    CALLBACK_DATA                          pUserData)
  3591| {
  3592|     DD_ENTER_MAY_THROW;
  3593|     Thread * pThread = vmThread.GetDacPtr();
  3594|     _ASSERTE(pThread != NULL);
  3595|     BlockingObjectUserDataWrapper wrapper;
  3596|     wrapper.fpCallback = fpCallback;
  3597|     wrapper.pUserData = pUserData;
  3598|     pThread->DebugBlockingInfo.VisitBlockingItems((DebugBlockingItemVisitor)EnumerateBlockingObjectsCallback,
  3599|         (VOID*)&wrapper);
  3600| }
  3601| MonitorLockInfo DacDbiInterfaceImpl::GetThreadOwningMonitorLock(VMPTR_Object vmObject)
  3602| {
  3603|     DD_ENTER_MAY_THROW;
  3604|     MonitorLockInfo info;
  3605|     info.lockOwner = VMPTR_Thread::NullPtr();
  3606|     info.acquisitionCount = 0;
  3607|     Object* pObj = vmObject.GetDacPtr();
  3608|     DWORD threadId;
  3609|     DWORD acquisitionCount;
  3610|     if(!pObj->GetHeader()->GetThreadOwningMonitorLock(&threadId, &acquisitionCount))
  3611|     {
  3612|         return info;
  3613|     }
  3614|     Thread *pThread = ThreadStore::GetThreadList(NULL);
  3615|     while (pThread != NULL)
  3616|     {
  3617|         if(pThread->GetThreadId() == threadId)
  3618|         {
  3619|             info.lockOwner.SetDacTargetPtr(PTR_HOST_TO_TADDR(pThread));
  3620|             info.acquisitionCount = acquisitionCount;
  3621|             return info;
  3622|         }
  3623|         pThread = ThreadStore::GetThreadList(pThread);
  3624|     }
  3625|     _ASSERTE(!"A thread should have been found");
  3626|     return info;
  3627| }
  3628| struct ThreadUserDataWrapper
  3629| {
  3630|     CALLBACK_DATA pUserData;
  3631|     IDacDbiInterface::FP_THREAD_ENUMERATION_CALLBACK fpCallback;
  3632| };
  3633| void EnumerateThreadsCallback(PTR_Thread pThread, VOID* pUserData)
  3634| {
  3635|     ThreadUserDataWrapper* wrapper = (ThreadUserDataWrapper*)pUserData;
  3636|     VMPTR_Thread vmThread = VMPTR_Thread::NullPtr();
  3637|     vmThread.SetDacTargetPtr(dac_cast<TADDR>(pThread));
  3638|     wrapper->fpCallback(vmThread, wrapper->pUserData);
  3639| }
  3640| void DacDbiInterfaceImpl::EnumerateMonitorEventWaitList(VMPTR_Object                   vmObject,
  3641|                                                         FP_THREAD_ENUMERATION_CALLBACK fpCallback,
  3642|                                                         CALLBACK_DATA                  pUserData)
  3643| {
  3644|     DD_ENTER_MAY_THROW;
  3645|     Object* pObj = vmObject.GetDacPtr();
  3646|     SyncBlock* psb = pObj->PassiveGetSyncBlock();
  3647|     if(psb == NULL)
  3648|         return;
  3649|     ThreadUserDataWrapper wrapper;
  3650|     wrapper.fpCallback = fpCallback;
  3651|     wrapper.pUserData = pUserData;
  3652|     ThreadQueue::EnumerateThreads(psb, (FP_TQ_THREAD_ENUMERATION_CALLBACK)EnumerateThreadsCallback, (VOID*) &wrapper);
  3653| }
  3654| bool DacDbiInterfaceImpl::AreGCStructuresValid()
  3655| {
  3656|     return true;
  3657| }
  3658| HeapData::HeapData()
  3659|     : YoungestGenPtr(0), YoungestGenLimit(0), Gen0Start(0), Gen0End(0), SegmentCount(0), Segments(0)
  3660| {
  3661| }
  3662| HeapData::~HeapData()
  3663| {
  3664|     if (Segments)
  3665|         delete [] Segments;
  3666| }
  3667| LinearReadCache::LinearReadCache()
  3668|     : mCurrPageStart(0), mPageSize(0), mCurrPageSize(0), mPage(0)
  3669| {
  3670|     SYSTEM_INFO si;
  3671| 	GetSystemInfo(&si);
  3672|     mPageSize = si.dwPageSize;
  3673|     mPage = new (nothrow) BYTE[mPageSize];
  3674| }
  3675| LinearReadCache::~LinearReadCache()
  3676| {
  3677|     if (mPage)
  3678|         delete [] mPage;
  3679| }
  3680| bool LinearReadCache::MoveToPage(CORDB_ADDRESS addr)
  3681| {
  3682|     mCurrPageStart = addr - (addr % mPageSize);
  3683|     HRESULT hr = g_dacImpl->m_pTarget->ReadVirtual(mCurrPageStart, mPage, mPageSize, &mCurrPageSize);
  3684|     if (hr != S_OK)
  3685|     {
  3686|         mCurrPageStart = 0;
  3687|         mCurrPageSize = 0;
  3688|         return false;
  3689|     }
  3690|     return true;
  3691| }
  3692| CORDB_ADDRESS DacHeapWalker::HeapStart = 0;
  3693| CORDB_ADDRESS DacHeapWalker::HeapEnd = ~0;
  3694| DacHeapWalker::DacHeapWalker()
  3695|     : mThreadCount(0), mAllocInfo(0), mHeapCount(0), mHeaps(0),
  3696|         mCurrObj(0), mCurrSize(0), mCurrMT(0),
  3697|         mCurrHeap(0), mCurrSeg(0), mStart((TADDR)HeapStart), mEnd((TADDR)HeapEnd)
  3698| {
  3699| }
  3700| DacHeapWalker::~DacHeapWalker()
  3701| {
  3702|     if (mAllocInfo)
  3703|         delete [] mAllocInfo;
  3704|     if (mHeaps)
  3705|         delete [] mHeaps;
  3706| }
  3707| SegmentData *DacHeapWalker::FindSegment(CORDB_ADDRESS obj)
  3708| {
  3709|     for (size_t i = 0; i < mHeapCount; ++i)
  3710|         for (size_t j = 0; j < mHeaps[i].SegmentCount; ++j)
  3711|             if (mHeaps[i].Segments[j].Start <= obj && obj <= mHeaps[i].Segments[j].End)
  3712|                 return &mHeaps[i].Segments[j];
  3713|     return NULL;
  3714| }
  3715| HRESULT DacHeapWalker::Next(CORDB_ADDRESS *pValue, CORDB_ADDRESS *pMT, ULONG64 *pSize)
  3716| {
  3717|     if (!HasMoreObjects())
  3718|         return E_FAIL;
  3719|     if (pValue)
  3720|         *pValue = mCurrObj;
  3721|     if (pMT)
  3722|         *pMT = (CORDB_ADDRESS)mCurrMT;
  3723|     if (pSize)
  3724|         *pSize = (ULONG64)mCurrSize;
  3725|     HRESULT hr = MoveToNextObject();
  3726|     return FAILED(hr) ? hr : S_OK;
  3727| }
  3728| HRESULT DacHeapWalker::MoveToNextObject()
  3729| {
  3730|     do
  3731|     {
  3732|         mCurrObj += mCurrSize;
  3733|         bool isGen0 = IsRegionGCEnabled() ? (mHeaps[mCurrHeap].Segments[mCurrSeg].Generation == 0) :
  3734|                                    (mHeaps[mCurrHeap].Gen0Start <= mCurrObj && mHeaps[mCurrHeap].Gen0End > mCurrObj);
  3735|         if (isGen0)
  3736|             CheckAllocAndSegmentRange();
  3737|         if (mCurrObj >= mHeaps[mCurrHeap].Segments[mCurrSeg].End || mCurrObj > mEnd)
  3738|         {
  3739|             HRESULT hr = NextSegment();
  3740|             if (FAILED(hr) || hr == S_FALSE)
  3741|                 return hr;
  3742|         }
  3743|         if (!mCache.ReadMT(mCurrObj, &mCurrMT))
  3744|             return E_FAIL;
  3745|         if (!GetSize(mCurrMT, mCurrSize))
  3746|             return E_FAIL;
  3747|     } while (mCurrObj < mStart);
  3748|     _ASSERTE(mStart <= mCurrObj && mCurrObj <= mEnd);
  3749|     return S_OK;
  3750| }
  3751| bool DacHeapWalker::GetSize(TADDR tMT, size_t &size)
  3752| {
  3753|     bool ret = true;
  3754|     EX_TRY
  3755|     {
  3756|         MethodTable *mt = PTR_MethodTable(tMT);
  3757|         size_t cs = mt->GetComponentSize();
  3758|         if (cs)
  3759|         {
  3760|             DWORD tmp = 0;
  3761|             if (mCache.Read(mCurrObj + sizeof(TADDR), &tmp))
  3762|                 cs *= tmp;
  3763|             else
  3764|                 ret = false;
  3765|         }
  3766|         size = mt->GetBaseSize() + cs;
  3767|         if (mHeaps[mCurrHeap].Segments[mCurrSeg].Generation == 3
  3768|             || mHeaps[mCurrHeap].Segments[mCurrSeg].Generation == 4)
  3769|             size = AlignLarge(size);
  3770|         else
  3771|             size = Align(size);
  3772|         ret &= (0 < size);
  3773|         ret &= ((mCurrObj + size) <= mHeaps[mCurrHeap].Segments[mCurrSeg].End);
  3774|     }
  3775|     EX_CATCH
  3776|     {
  3777|         ret = false;
  3778|     }
  3779|     EX_END_CATCH(SwallowAllExceptions)
  3780|     return ret;
  3781| }
  3782| HRESULT DacHeapWalker::NextSegment()
  3783| {
  3784|     mCurrObj = 0;
  3785|     mCurrMT = 0;
  3786|     mCurrSize = 0;
  3787|     do
  3788|     {
  3789|         do
  3790|         {
  3791|             mCurrSeg++;
  3792|             while (mCurrSeg >= mHeaps[mCurrHeap].SegmentCount)
  3793|             {
  3794|                 mCurrSeg = 0;
  3795|                 mCurrHeap++;
  3796|                 if (mCurrHeap >= mHeapCount)
  3797|                 {
  3798|                     return S_FALSE;
  3799|                 }
  3800|             }
  3801|         } while (mHeaps[mCurrHeap].Segments[mCurrSeg].Start >= mHeaps[mCurrHeap].Segments[mCurrSeg].End);
  3802|         mCurrObj = mHeaps[mCurrHeap].Segments[mCurrSeg].Start;
  3803|         bool isGen0 = IsRegionGCEnabled() ? (mHeaps[mCurrHeap].Segments[mCurrSeg].Generation == 0) :
  3804|                                    (mHeaps[mCurrHeap].Gen0Start <= mCurrObj && mHeaps[mCurrHeap].Gen0End > mCurrObj);
  3805|         if (isGen0)
  3806|             CheckAllocAndSegmentRange();
  3807|         if (!mCache.ReadMT(mCurrObj, &mCurrMT))
  3808|         {
  3809|             return E_FAIL;
  3810|         }
  3811|         if (!GetSize(mCurrMT, mCurrSize))
  3812|         {
  3813|             return E_FAIL;
  3814|         }
  3815|     } while((mHeaps[mCurrHeap].Segments[mCurrSeg].Start > mEnd) || (mHeaps[mCurrHeap].Segments[mCurrSeg].End < mStart));
  3816|     return S_OK;
  3817| }
  3818| void DacHeapWalker::CheckAllocAndSegmentRange()
  3819| {
  3820|     const size_t MinObjSize = sizeof(TADDR)*3;
  3821|     for (int i = 0; i < mThreadCount; ++i)
  3822|         if (mCurrObj == mAllocInfo[i].Ptr)
  3823|         {
  3824|             mCurrObj = mAllocInfo[i].Limit + Align(MinObjSize);
  3825|             break;
  3826|         }
  3827|     if (mCurrObj == mHeaps[mCurrHeap].YoungestGenPtr)
  3828|     {
  3829|         mCurrObj = mHeaps[mCurrHeap].YoungestGenLimit + Align(MinObjSize);
  3830|     }
  3831| }
  3832| HRESULT DacHeapWalker::Init(CORDB_ADDRESS start, CORDB_ADDRESS end)
  3833| {
  3834|     ThreadStore* threadStore = ThreadStore::s_pThreadStore;
  3835|     if (threadStore != NULL)
  3836|     {
  3837|         int count = (int)threadStore->ThreadCountInEE();
  3838|         mAllocInfo = new (nothrow) AllocInfo[count + 1];
  3839|         if (mAllocInfo == NULL)
  3840|             return E_OUTOFMEMORY;
  3841|         Thread *thread = NULL;
  3842|         int j = 0;
  3843|         for (int i = 0; i < count; ++i)
  3844|         {
  3845|             thread = ThreadStore::GetThreadList(thread);
  3846|             if (thread == NULL)
  3847|                 continue;
  3848|             gc_alloc_context *ctx = thread->GetAllocContext();
  3849|             if (ctx == NULL)
  3850|                 continue;
  3851|             if ((CORDB_ADDRESS)ctx->alloc_ptr != NULL)
  3852|             {
  3853|                 mAllocInfo[j].Ptr = (CORDB_ADDRESS)ctx->alloc_ptr;
  3854|                 mAllocInfo[j].Limit = (CORDB_ADDRESS)ctx->alloc_limit;
  3855|                 j++;
  3856|             }
  3857|         }
  3858|         if ((&g_global_alloc_context)->alloc_ptr != nullptr)
  3859|         {
  3860|             mAllocInfo[j].Ptr = (CORDB_ADDRESS)(&g_global_alloc_context)->alloc_ptr;
  3861|             mAllocInfo[j].Limit = (CORDB_ADDRESS)(&g_global_alloc_context)->alloc_limit;
  3862|         }
  3863|         mThreadCount = j;
  3864|     }
  3865| #ifdef FEATURE_SVR_GC
  3866|     HRESULT hr = GCHeapUtilities::IsServerHeap() ? InitHeapDataSvr(mHeaps, mHeapCount) : InitHeapDataWks(mHeaps, mHeapCount);
  3867| #else
  3868|     HRESULT hr = InitHeapDataWks(mHeaps, mHeapCount);
  3869| #endif
  3870|     if (SUCCEEDED(hr))
  3871|         hr = Reset(start, end);
  3872|     return hr;
  3873| }
  3874| HRESULT DacHeapWalker::Reset(CORDB_ADDRESS start, CORDB_ADDRESS end)
  3875| {
  3876|     _ASSERTE(mHeaps);
  3877|     _ASSERTE(mHeapCount > 0);
  3878|     _ASSERTE(mHeaps[0].Segments);
  3879|     _ASSERTE(mHeaps[0].SegmentCount > 0);
  3880|     mStart = start;
  3881|     mEnd = end;
  3882|     mCurrObj = mHeaps[0].Segments[0].Start;
  3883|     mCurrMT = 0;
  3884|     mCurrSize = 0;
  3885|     mCurrHeap = 0;
  3886|     mCurrSeg = 0;
  3887|     HRESULT hr = S_OK;
  3888|     if (mCurrObj >= mHeaps[0].Segments[0].End)
  3889|         hr = MoveToNextObject();
  3890|     if (!mCache.ReadMT(mCurrObj, &mCurrMT))
  3891|         return E_FAIL;
  3892|     if (!GetSize(mCurrMT, mCurrSize))
  3893|         return E_FAIL;
  3894|     if (mCurrObj < mStart || mCurrObj > mEnd)
  3895|         hr = MoveToNextObject();
  3896|     return hr;
  3897| }
  3898| HRESULT DacHeapWalker::ListNearObjects(CORDB_ADDRESS obj, CORDB_ADDRESS *pPrev, CORDB_ADDRESS *pContaining, CORDB_ADDRESS *pNext)
  3899| {
  3900|     SegmentData *seg = FindSegment(obj);
  3901|     if (seg == NULL)
  3902|         return E_FAIL;
  3903|     HRESULT hr = Reset(seg->Start, seg->End);
  3904|     if (SUCCEEDED(hr))
  3905|     {
  3906|         CORDB_ADDRESS prev = 0;
  3907|         CORDB_ADDRESS curr = 0;
  3908|         ULONG64 size = 0;
  3909|         bool found = false;
  3910|         while (!found && HasMoreObjects())
  3911|         {
  3912|             prev = curr;
  3913|             hr = Next(&curr, NULL, &size);
  3914|             if (FAILED(hr))
  3915|                 break;
  3916|             if (obj >= curr && obj < curr + size)
  3917|                 found = true;
  3918|         }
  3919|         if (found)
  3920|         {
  3921|             if (pPrev)
  3922|                 *pPrev = prev;
  3923|             if (pContaining)
  3924|                 *pContaining = curr;
  3925|             if (pNext)
  3926|             {
  3927|                 if (HasMoreObjects())
  3928|                 {
  3929|                     hr = Next(&curr, NULL, NULL);
  3930|                     if (SUCCEEDED(hr))
  3931|                         *pNext = curr;
  3932|                 }
  3933|                 else
  3934|                 {
  3935|                     *pNext = 0;
  3936|                 }
  3937|             }
  3938|             hr = S_OK;
  3939|         }
  3940|         else if (SUCCEEDED(hr))
  3941|         {
  3942|             hr = E_FAIL;
  3943|         }
  3944|     }
  3945|     return hr;
  3946| }
  3947| HRESULT DacHeapWalker::InitHeapDataWks(HeapData *&pHeaps, size_t &pCount)
  3948| {
  3949|     bool regions = IsRegionGCEnabled();
  3950|     pCount = 1;
  3951|     pHeaps = new (nothrow) HeapData[1];
  3952|     if (pHeaps == NULL)
  3953|         return E_OUTOFMEMORY;
  3954|     dac_generation gen0 = GenerationTableIndex(g_gcDacGlobals->generation_table, 0);
  3955|     dac_generation gen1 = GenerationTableIndex(g_gcDacGlobals->generation_table, 1);
  3956|     dac_generation gen2 = GenerationTableIndex(g_gcDacGlobals->generation_table, 2);
  3957|     dac_generation loh  = GenerationTableIndex(g_gcDacGlobals->generation_table, 3);
  3958|     dac_generation poh  = GenerationTableIndex(g_gcDacGlobals->generation_table, 4);
  3959|     pHeaps[0].YoungestGenPtr = (CORDB_ADDRESS)gen0.allocation_context.alloc_ptr;
  3960|     pHeaps[0].YoungestGenLimit = (CORDB_ADDRESS)gen0.allocation_context.alloc_limit;
  3961|     if (!regions)
  3962|     {
  3963|         pHeaps[0].Gen0Start = (CORDB_ADDRESS)gen0.allocation_start;
  3964|         pHeaps[0].Gen0End = (CORDB_ADDRESS)*g_gcDacGlobals->alloc_allocated;
  3965|         pHeaps[0].Gen1Start = (CORDB_ADDRESS)gen1.allocation_start;
  3966|     }
  3967|     int count = GetSegmentCount(loh.start_segment);
  3968|     count += GetSegmentCount(poh.start_segment);
  3969|     count += GetSegmentCount(gen2.start_segment);
  3970|     if (regions)
  3971|     {
  3972|         count += GetSegmentCount(gen1.start_segment);
  3973|         count += GetSegmentCount(gen0.start_segment);
  3974|     }
  3975|     pHeaps[0].SegmentCount = count;
  3976|     pHeaps[0].Segments = new (nothrow) SegmentData[count];
  3977|     if (pHeaps[0].Segments == NULL)
  3978|         return E_OUTOFMEMORY;
  3979|     DPTR(dac_heap_segment) seg;
  3980|     int i = 0;
  3981|     if (regions)
  3982|     {
  3983|         seg = gen2.start_segment;
  3984|         for (; seg && (i < count); ++i)
  3985|         {
  3986|             pHeaps[0].Segments[i].Generation = seg->flags & HEAP_SEGMENT_FLAGS_READONLY ? CorDebug_NonGC : CorDebug_Gen2;
  3987|             pHeaps[0].Segments[i].Start = (CORDB_ADDRESS)seg->mem;
  3988|             pHeaps[0].Segments[i].End = (CORDB_ADDRESS)seg->allocated;
  3989|             seg = seg->next;
  3990|         }
  3991|         seg = gen1.start_segment;
  3992|         for (; seg && (i < count); ++i)
  3993|         {
  3994|             pHeaps[0].Segments[i].Generation = CorDebug_Gen1;
  3995|             pHeaps[0].Segments[i].Start = (CORDB_ADDRESS)seg->mem;
  3996|             pHeaps[0].Segments[i].End = (CORDB_ADDRESS)seg->allocated;
  3997|             seg = seg->next;
  3998|         }
  3999|         seg = gen0.start_segment;
  4000|         for (; seg && (i < count); ++i)
  4001|         {
  4002|             pHeaps[0].Segments[i].Start = (CORDB_ADDRESS)seg->mem;
  4003|             if (seg.GetAddr() == (TADDR)*g_gcDacGlobals->ephemeral_heap_segment)
  4004|             {
  4005|                 pHeaps[0].Segments[i].End = (CORDB_ADDRESS)*g_gcDacGlobals->alloc_allocated;
  4006|                 pHeaps[0].EphemeralSegment = i;
  4007|             }
  4008|             else
  4009|             {
  4010|                 pHeaps[0].Segments[i].End = (CORDB_ADDRESS)seg->allocated;
  4011|             }
  4012|             pHeaps[0].Segments[i].Generation = CorDebug_Gen0;
  4013|             seg = seg->next;
  4014|         }
  4015|     }
  4016|     else
  4017|     {
  4018|         DPTR(dac_heap_segment) seg = gen2.start_segment;
  4019|         for (; seg && (i < count); ++i)
  4020|         {
  4021|             pHeaps[0].Segments[i].Start = (CORDB_ADDRESS)seg->mem;
  4022|             if (seg.GetAddr() == (TADDR)*g_gcDacGlobals->ephemeral_heap_segment)
  4023|             {
  4024|                 pHeaps[0].Segments[i].End = (CORDB_ADDRESS)*g_gcDacGlobals->alloc_allocated;
  4025|                 pHeaps[0].Segments[i].Generation = CorDebug_Gen1;
  4026|                 pHeaps[0].EphemeralSegment = i;
  4027|             }
  4028|             else
  4029|             {
  4030|                 pHeaps[0].Segments[i].End = (CORDB_ADDRESS)seg->allocated;
  4031|                 pHeaps[0].Segments[i].Generation = seg->flags & HEAP_SEGMENT_FLAGS_READONLY ? CorDebug_NonGC : CorDebug_Gen2;
  4032|             }
  4033|             seg = seg->next;
  4034|         }
  4035|     }
  4036|     seg = loh.start_segment;
  4037|     for (; seg && (i < count); ++i)
  4038|     {
  4039|         pHeaps[0].Segments[i].Generation = CorDebug_LOH;
  4040|         pHeaps[0].Segments[i].Start = (CORDB_ADDRESS)seg->mem;
  4041|         pHeaps[0].Segments[i].End = (CORDB_ADDRESS)seg->allocated;
  4042|         seg = seg->next;
  4043|     }
  4044|     seg = poh.start_segment;
  4045|     for (; seg && (i < count); ++i)
  4046|     {
  4047|         pHeaps[0].Segments[i].Generation = CorDebug_POH;
  4048|         pHeaps[0].Segments[i].Start = (CORDB_ADDRESS)seg->mem;
  4049|         pHeaps[0].Segments[i].End = (CORDB_ADDRESS)seg->allocated;
  4050|         seg = seg->next;
  4051|     }
  4052|     _ASSERTE(count == i);
  4053|     return S_OK;
  4054| }
  4055|  HRESULT DacDbiInterfaceImpl::CreateHeapWalk(IDacDbiInterface::HeapWalkHandle *pHandle)
  4056| {
  4057|     DD_ENTER_MAY_THROW;
  4058|     DacHeapWalker *data = new (nothrow) DacHeapWalker;
  4059|     if (data == NULL)
  4060|         return E_OUTOFMEMORY;
  4061|     HRESULT hr = data->Init();
  4062|     if (SUCCEEDED(hr))
  4063|         *pHandle = reinterpret_cast<HeapWalkHandle>(data);
  4064|     else
  4065|         delete data;
  4066|     return hr;
  4067| }
  4068| void DacDbiInterfaceImpl::DeleteHeapWalk(HeapWalkHandle handle)
  4069| {
  4070|     DD_ENTER_MAY_THROW;
  4071|     DacHeapWalker *data = reinterpret_cast<DacHeapWalker*>(handle);
  4072|     if (data)
  4073|         delete data;
  4074| }
  4075| HRESULT DacDbiInterfaceImpl::WalkHeap(HeapWalkHandle handle,
  4076|                     ULONG count,
  4077|                     OUT COR_HEAPOBJECT * objects,
  4078|                     OUT ULONG *fetched)
  4079| {
  4080|     DD_ENTER_MAY_THROW;
  4081|     if (fetched == NULL)
  4082|         return E_INVALIDARG;
  4083|     DacHeapWalker *walk = reinterpret_cast<DacHeapWalker*>(handle);
  4084|     *fetched = 0;
  4085|     if (!walk->HasMoreObjects())
  4086|         return S_FALSE;
  4087|     CORDB_ADDRESS freeMT = (CORDB_ADDRESS)g_pFreeObjectMethodTable.GetAddr();
  4088|     HRESULT hr = S_OK;
  4089|     CORDB_ADDRESS addr, mt;
  4090|     ULONG64 size;
  4091|     ULONG i = 0;
  4092|     while (i < count && walk->HasMoreObjects())
  4093|     {
  4094|         hr = walk->Next(&addr, &mt, &size);
  4095|         if (FAILED(hr))
  4096|             break;
  4097|         if (mt != freeMT)
  4098|         {
  4099|             objects[i].address = addr;
  4100|             objects[i].type.token1 = mt;
  4101|             objects[i].type.token2 = NULL;
  4102|             objects[i].size = size;
  4103|             i++;
  4104|         }
  4105|     }
  4106|     if (SUCCEEDED(hr))
  4107|         hr = (i < count) ? S_FALSE : S_OK;
  4108|     *fetched = i;
  4109|     return hr;
  4110| }
  4111| HRESULT DacDbiInterfaceImpl::GetHeapSegments(OUT DacDbiArrayList<COR_SEGMENT> *pSegments)
  4112| {
  4113|     DD_ENTER_MAY_THROW;
  4114|     size_t heapCount = 0;
  4115|     HeapData *heaps = 0;
  4116|     bool region = IsRegionGCEnabled();
  4117| #ifdef FEATURE_SVR_GC
  4118|     HRESULT hr = GCHeapUtilities::IsServerHeap() ? DacHeapWalker::InitHeapDataSvr(heaps, heapCount) : DacHeapWalker::InitHeapDataWks(heaps, heapCount);
  4119| #else
  4120|     HRESULT hr = DacHeapWalker::InitHeapDataWks(heaps, heapCount);
  4121| #endif
  4122|     NewArrayHolder<HeapData> _heapHolder = heaps;
  4123|     int total = 0;
  4124|     for (size_t i = 0; i < heapCount; ++i)
  4125|     {
  4126|         total += (int)heaps[i].SegmentCount;
  4127|         if (!region)
  4128|         {
  4129|             total++;
  4130|             const size_t eph = heaps[i].EphemeralSegment;
  4131|             _ASSERTE(eph < heaps[i].SegmentCount);
  4132|             if (heaps[i].Segments[eph].Start != heaps[i].Gen1Start)
  4133|                 total++;
  4134|         }
  4135|     }
  4136|     pSegments->Alloc(total);
  4137|     int curr = 0;
  4138|     for (size_t i = 0; i < heapCount; ++i)
  4139|     {
  4140|         _ASSERTE(curr < total);
  4141|         if (!region)
  4142|         {
  4143|             COR_SEGMENT &seg = (*pSegments)[curr++];
  4144|             seg.start = heaps[i].Gen0Start;
  4145|             seg.end = heaps[i].Gen0End;
  4146|             seg.type = CorDebug_Gen0;
  4147|             seg.heap = (ULONG)i;
  4148|         }
  4149|         for (size_t j = 0; j < heaps[i].SegmentCount; ++j)
  4150|         {
  4151|             if (region)
  4152|             {
  4153|                 _ASSERTE(curr < total);
  4154|                 COR_SEGMENT &seg = (*pSegments)[curr++];
  4155|                 seg.start = heaps[i].Segments[j].Start;
  4156|                 seg.end = heaps[i].Segments[j].End;
  4157|                 seg.type = (CorDebugGenerationTypes)heaps[i].Segments[j].Generation;
  4158|                 seg.heap = (ULONG)i;
  4159|             }
  4160|             else if (heaps[i].Segments[j].Generation == 1)
  4161|             {
  4162|                 _ASSERTE(heaps[i].Segments[j].Start <= heaps[i].Gen1Start);
  4163|                 _ASSERTE(heaps[i].Segments[j].End > heaps[i].Gen1Start);
  4164|                 {
  4165|                     _ASSERTE(curr < total);
  4166|                     COR_SEGMENT &seg = (*pSegments)[curr++];
  4167|                     seg.start = heaps[i].Gen1Start;
  4168|                     seg.end = heaps[i].Gen0Start;
  4169|                     seg.type = CorDebug_Gen1;
  4170|                     seg.heap = (ULONG)i;
  4171|                 }
  4172|                 if (heaps[i].Segments[j].Start != heaps[i].Gen1Start)
  4173|                 {
  4174|                     _ASSERTE(curr < total);
  4175|                     COR_SEGMENT &seg = (*pSegments)[curr++];
  4176|                     seg.start = heaps[i].Segments[j].Start;
  4177|                     seg.end = heaps[i].Gen1Start;
  4178|                     seg.type = CorDebug_Gen2;
  4179|                     seg.heap = (ULONG)i;
  4180|                 }
  4181|             }
  4182|             else
  4183|             {
  4184|                 _ASSERTE(curr < total);
  4185|                 COR_SEGMENT &seg = (*pSegments)[curr++];
  4186|                 seg.start = heaps[i].Segments[j].Start;
  4187|                 seg.end = heaps[i].Segments[j].End;
  4188|                 _ASSERTE(heaps[i].Segments[j].Generation <= CorDebug_NonGC);
  4189|                 seg.type = (CorDebugGenerationTypes)heaps[i].Segments[j].Generation;
  4190|                 seg.heap = (ULONG)i;
  4191|             }
  4192|         }
  4193|     }
  4194|     _ASSERTE(total == curr);
  4195|     return hr;
  4196| }
  4197| bool DacDbiInterfaceImpl::IsValidObject(CORDB_ADDRESS addr)
  4198| {
  4199|     DD_ENTER_MAY_THROW;
  4200|     bool isValid = false;
  4201|     if (addr != 0 && addr != (CORDB_ADDRESS)-1)
  4202|     {
  4203|         EX_TRY
  4204|         {
  4205|             PTR_Object obj(TO_TADDR(addr));
  4206|             PTR_MethodTable mt = obj->GetMethodTable();
  4207|             PTR_EEClass cls = mt->GetClass();
  4208|             if (mt == cls->GetMethodTable())
  4209|                 isValid = true;
  4210|             else if (!mt->IsCanonicalMethodTable())
  4211|                 isValid = cls->GetMethodTable()->GetClass() == cls;
  4212|         }
  4213|         EX_CATCH
  4214|         {
  4215|             isValid = false;
  4216|         }
  4217|         EX_END_CATCH(SwallowAllExceptions)
  4218|     }
  4219|     return isValid;
  4220| }
  4221| bool DacDbiInterfaceImpl::GetAppDomainForObject(CORDB_ADDRESS addr, OUT VMPTR_AppDomain * pAppDomain,
  4222|                                                 OUT VMPTR_Module *pModule, OUT VMPTR_DomainAssembly *pDomainAssembly)
  4223| {
  4224|     DD_ENTER_MAY_THROW;
  4225|     if (addr == 0 || addr == (CORDB_ADDRESS)-1)
  4226|     {
  4227|         return false;
  4228|     }
  4229|     PTR_Object obj(TO_TADDR(addr));
  4230|     MethodTable *mt = obj->GetMethodTable();
  4231|     PTR_Module module = mt->GetModule();
  4232|     PTR_Assembly assembly = module->GetAssembly();
  4233|     BaseDomain *baseDomain = assembly->GetDomain();
  4234|     if (baseDomain->IsAppDomain())
  4235|     {
  4236|         pAppDomain->SetDacTargetPtr(PTR_HOST_TO_TADDR(baseDomain->AsAppDomain()));
  4237|         pModule->SetDacTargetPtr(PTR_HOST_TO_TADDR(module));
  4238|         pDomainAssembly->SetDacTargetPtr(PTR_HOST_TO_TADDR(module->GetDomainAssembly()));
  4239|     }
  4240|     else
  4241|     {
  4242|         return false;
  4243|     }
  4244|     return true;
  4245| }
  4246| HRESULT DacDbiInterfaceImpl::CreateRefWalk(OUT RefWalkHandle * pHandle, BOOL walkStacks, BOOL walkFQ, UINT32 handleWalkMask)
  4247| {
  4248|     DD_ENTER_MAY_THROW;
  4249|     DacRefWalker *walker = new (nothrow) DacRefWalker(this, walkStacks, walkFQ, handleWalkMask, TRUE);
  4250|     if (walker == NULL)
  4251|         return E_OUTOFMEMORY;
  4252|     HRESULT hr = walker->Init();
  4253|     if (FAILED(hr))
  4254|     {
  4255|         delete walker;
  4256|     }
  4257|     else
  4258|     {
  4259|         *pHandle = reinterpret_cast<RefWalkHandle>(walker);
  4260|     }
  4261|     return hr;
  4262| }
  4263| void DacDbiInterfaceImpl::DeleteRefWalk(IN RefWalkHandle handle)
  4264| {
  4265|     DD_ENTER_MAY_THROW;
  4266|     DacRefWalker *walker = reinterpret_cast<DacRefWalker*>(handle);
  4267|     if (walker)
  4268|         delete walker;
  4269| }
  4270| HRESULT DacDbiInterfaceImpl::WalkRefs(RefWalkHandle handle, ULONG count, OUT DacGcReference * objects, OUT ULONG *pFetched)
  4271| {
  4272|     if (objects == NULL || pFetched == NULL)
  4273|         return E_POINTER;
  4274|     DD_ENTER_MAY_THROW;
  4275|     DacRefWalker *walker = reinterpret_cast<DacRefWalker*>(handle);
  4276|     if (!walker)
  4277|         return E_INVALIDARG;
  4278|    return walker->Next(count, objects, pFetched);
  4279| }
  4280| HRESULT DacDbiInterfaceImpl::GetTypeID(CORDB_ADDRESS dbgObj, COR_TYPEID *pID)
  4281| {
  4282|     DD_ENTER_MAY_THROW;
  4283|     TADDR obj[3];
  4284|     ULONG32 read = 0;
  4285|     HRESULT hr = g_dacImpl->m_pTarget->ReadVirtual(dbgObj, (BYTE*)obj, sizeof(obj), &read);
  4286|     if (FAILED(hr))
  4287|         return hr;
  4288|     pID->token1 = (UINT64)(obj[0] & ~1);
  4289|     pID->token2 = 0;
  4290|     return hr;
  4291| }
  4292| HRESULT DacDbiInterfaceImpl::GetTypeIDForType(VMPTR_TypeHandle vmTypeHandle, COR_TYPEID *pID)
  4293| {
  4294|     DD_ENTER_MAY_THROW;
  4295|     _ASSERTE(pID != NULL);
  4296|     _ASSERTE(!vmTypeHandle.IsNull());
  4297|     TypeHandle th = TypeHandle::FromPtr(vmTypeHandle.GetDacPtr());
  4298|     PTR_MethodTable pMT = th.GetMethodTable();
  4299|     pID->token1 = pMT.GetAddr();
  4300|     _ASSERTE(pID->token1 != 0);
  4301|     pID->token2 = 0;
  4302|     return S_OK;
  4303| }
  4304| HRESULT DacDbiInterfaceImpl::GetObjectFields(COR_TYPEID id, ULONG32 celt, COR_FIELD *layout, ULONG32 *pceltFetched)
  4305| {
  4306|     if (pceltFetched == NULL)
  4307|         return E_POINTER;
  4308|     if (id.token1 == 0)
  4309|         return CORDBG_E_CLASS_NOT_LOADED;
  4310|     DD_ENTER_MAY_THROW;
  4311|     HRESULT hr = S_OK;
  4312|     TypeHandle typeHandle = TypeHandle::FromPtr(TO_TADDR(id.token1));
  4313|     if (typeHandle.IsTypeDesc())
  4314|         return E_INVALIDARG;
  4315|     ApproxFieldDescIterator fieldDescIterator(typeHandle.AsMethodTable(), ApproxFieldDescIterator::INSTANCE_FIELDS);
  4316|     ULONG32 cFields = fieldDescIterator.Count();
  4317|     if (layout == NULL)
  4318|     {
  4319|         *pceltFetched = cFields;
  4320|         return S_FALSE;
  4321|     }
  4322|     if (celt < cFields)
  4323|     {
  4324|         cFields = celt;
  4325|         hr = S_FALSE;
  4326|     }
  4327|     *pceltFetched = celt;
  4328|     CorElementType componentType = typeHandle.AsMethodTable()->GetInternalCorElementType();
  4329|     BOOL fReferenceType = CorTypeInfo::IsObjRef_NoThrow(componentType);
  4330|     for (ULONG32 i = 0; i < cFields; ++i)
  4331|     {
  4332|         FieldDesc *pField = fieldDescIterator.Next();
  4333|         COR_FIELD* corField = layout + i;
  4334|         corField->token = pField->GetMemberDef();
  4335|         corField->offset = (ULONG32)pField->GetOffset() + (fReferenceType ? Object::GetOffsetOfFirstField() : 0);
  4336|         TypeHandle fieldHandle = pField->LookupFieldTypeHandle();
  4337|         if (fieldHandle.IsNull())
  4338|         {
  4339|             corField->id = {};
  4340|             corField->fieldType = (CorElementType)0;
  4341|         }
  4342|         else if (fieldHandle.IsByRef())
  4343|         {
  4344|             corField->fieldType = ELEMENT_TYPE_BYREF;
  4345|             corField->id.token1 = CoreLibBinder::GetElementType(ELEMENT_TYPE_I).GetAddr();
  4346|             corField->id.token2 = 0;
  4347|         }
  4348|         else
  4349|         {
  4350|             PTR_MethodTable mt = fieldHandle.GetMethodTable();
  4351|             corField->fieldType = mt->GetInternalCorElementType();
  4352|             corField->id.token1 = (ULONG64)mt.GetAddr();
  4353|             corField->id.token2 = 0;
  4354|         }
  4355|     }
  4356|     return hr;
  4357| }
  4358| HRESULT DacDbiInterfaceImpl::GetTypeLayout(COR_TYPEID id, COR_TYPE_LAYOUT *pLayout)
  4359| {
  4360|     if (pLayout == NULL)
  4361|         return E_POINTER;
  4362|     if (id.token1 == 0)
  4363|         return CORDBG_E_CLASS_NOT_LOADED;
  4364|     DD_ENTER_MAY_THROW;
  4365|     PTR_MethodTable mt = PTR_MethodTable(TO_TADDR(id.token1));
  4366|     PTR_MethodTable parentMT = mt->GetParentMethodTable();
  4367|     COR_TYPEID parent = {parentMT.GetAddr(), 0};
  4368|     pLayout->parentID = parent;
  4369|     DWORD size = mt->GetBaseSize();
  4370|     ApproxFieldDescIterator fieldDescIterator(mt, ApproxFieldDescIterator::INSTANCE_FIELDS);
  4371|     pLayout->objectSize = size;
  4372|     pLayout->numFields = fieldDescIterator.Count();
  4373|     CorElementType componentType = mt->IsString() ? ELEMENT_TYPE_STRING : mt->GetInternalCorElementType();
  4374|     pLayout->type = componentType;
  4375|     pLayout->boxOffset = CorTypeInfo::IsObjRef_NoThrow(componentType) ? 0 : sizeof(TADDR);
  4376|     return S_OK;
  4377| }
  4378| HRESULT DacDbiInterfaceImpl::GetArrayLayout(COR_TYPEID id, COR_ARRAY_LAYOUT *pLayout)
  4379| {
  4380|     if (pLayout == NULL)
  4381|         return E_POINTER;
  4382|     if (id.token1 == 0)
  4383|         return CORDBG_E_CLASS_NOT_LOADED;
  4384|     DD_ENTER_MAY_THROW;
  4385|     PTR_MethodTable mt = PTR_MethodTable(TO_TADDR(id.token1));
  4386|     if (!mt->IsStringOrArray())
  4387|         return E_INVALIDARG;
  4388|     if (mt->IsString())
  4389|     {
  4390|         COR_TYPEID token;
  4391|         token.token1 = CoreLibBinder::GetElementType(ELEMENT_TYPE_CHAR).GetAddr();
  4392|         token.token2 = 0;
  4393|         pLayout->componentID = token;
  4394|         pLayout->rankSize = 4;
  4395|         pLayout->numRanks = 1;
  4396|         pLayout->rankOffset = sizeof(TADDR);
  4397|         pLayout->firstElementOffset = sizeof(TADDR) + 4;
  4398|         pLayout->countOffset = sizeof(TADDR);
  4399|         pLayout->componentType = ELEMENT_TYPE_CHAR;
  4400|         pLayout->elementSize = 2;
  4401|     }
  4402|     else
  4403|     {
  4404|         DWORD ranks = mt->GetRank();
  4405|         pLayout->rankSize = 4;
  4406|         pLayout->numRanks = ranks;
  4407|         bool multiDim = (ranks > 1);
  4408|         pLayout->rankOffset = multiDim ? sizeof(TADDR)*2 : sizeof(TADDR);
  4409|         pLayout->countOffset = sizeof(TADDR);
  4410|         pLayout->firstElementOffset = ArrayBase::GetDataPtrOffset(mt);
  4411|         TypeHandle hnd = mt->GetArrayElementTypeHandle();
  4412|         PTR_MethodTable cmt = hnd.GetMethodTable();
  4413|         CorElementType componentType = cmt->GetInternalCorElementType();
  4414|         if ((UINT64)cmt.GetAddr() == (UINT64)g_pStringClass.GetAddr())
  4415|             componentType = ELEMENT_TYPE_STRING;
  4416|         COR_TYPEID token;
  4417|         token.token1 = cmt.GetAddr();  // This could be type handle
  4418|         token.token2 = 0;
  4419|         pLayout->componentID = token;
  4420|         pLayout->componentType = componentType;
  4421|         if (CorTypeInfo::IsObjRef_NoThrow(componentType))
  4422|             pLayout->elementSize = sizeof(TADDR);
  4423|         else if (CorIsPrimitiveType(componentType))
  4424|             pLayout->elementSize = gElementTypeInfo[componentType].m_cbSize;
  4425|         else
  4426|             pLayout->elementSize = cmt->GetNumInstanceFieldBytes();
  4427|     }
  4428|     return S_OK;
  4429| }
  4430| void DacDbiInterfaceImpl::GetGCHeapInformation(COR_HEAPINFO * pHeapInfo)
  4431| {
  4432|     DD_ENTER_MAY_THROW;
  4433|     size_t heapCount = 0;
  4434|     pHeapInfo->areGCStructuresValid = *g_gcDacGlobals->gc_structures_invalid_cnt == 0;
  4435| #ifdef FEATURE_SVR_GC
  4436|     if (GCHeapUtilities::IsServerHeap())
  4437|     {
  4438|         pHeapInfo->gcType = CorDebugServerGC;
  4439|         pHeapInfo->numHeaps = DacGetNumHeaps();
  4440|     }
  4441|     else
  4442| #endif
  4443|     {
  4444|         pHeapInfo->gcType = CorDebugWorkstationGC;
  4445|         pHeapInfo->numHeaps = 1;
  4446|     }
  4447|     pHeapInfo->pointerSize = sizeof(TADDR);
  4448|     pHeapInfo->concurrent = g_pConfig->GetGCconcurrent() ? TRUE : FALSE;
  4449| }
  4450| HRESULT DacDbiInterfaceImpl::GetPEFileMDInternalRW(VMPTR_PEAssembly vmPEAssembly, OUT TADDR* pAddrMDInternalRW)
  4451| {
  4452|     DD_ENTER_MAY_THROW;
  4453|     if (pAddrMDInternalRW == NULL)
  4454|         return E_INVALIDARG;
  4455|     PEAssembly * pPEAssembly = vmPEAssembly.GetDacPtr();
  4456|     *pAddrMDInternalRW = pPEAssembly->GetMDInternalRWAddress();
  4457|     return S_OK;
  4458| }
  4459| HRESULT DacDbiInterfaceImpl::GetReJitInfo(VMPTR_Module vmModule, mdMethodDef methodTk, OUT VMPTR_ReJitInfo* pvmReJitInfo)
  4460| {
  4461|     DD_ENTER_MAY_THROW;
  4462|     _ASSERTE(!"You shouldn't be calling this - use GetActiveRejitILCodeVersionNode instead");
  4463|     return S_OK;
  4464| }
  4465| HRESULT DacDbiInterfaceImpl::GetActiveRejitILCodeVersionNode(VMPTR_Module vmModule, mdMethodDef methodTk, OUT VMPTR_ILCodeVersionNode* pVmILCodeVersionNode)
  4466| {
  4467|     DD_ENTER_MAY_THROW;
  4468|     if (pVmILCodeVersionNode == NULL)
  4469|         return E_INVALIDARG;
  4470| #ifdef FEATURE_REJIT
  4471|     PTR_Module pModule = vmModule.GetDacPtr();
  4472|     CodeVersionManager * pCodeVersionManager = pModule->GetCodeVersionManager();
  4473|     ILCodeVersion activeILVersion = pCodeVersionManager->GetActiveILCodeVersion(pModule, methodTk);
  4474|     if (activeILVersion.IsNull() || activeILVersion.IsDefaultVersion() || activeILVersion.GetRejitState() != ILCodeVersion::kStateActive)
  4475|     {
  4476|         pVmILCodeVersionNode->SetDacTargetPtr(0);
  4477|     }
  4478|     else
  4479|     {
  4480|         pVmILCodeVersionNode->SetDacTargetPtr(PTR_TO_TADDR(activeILVersion.AsNode()));
  4481|     }
  4482| #else
  4483|     _ASSERTE(!"You shouldn't be calling this - rejit is not supported in this build");
  4484|     pVmILCodeVersionNode->SetDacTargetPtr(0);
  4485| #endif
  4486|     return S_OK;
  4487| }
  4488| HRESULT DacDbiInterfaceImpl::GetReJitInfo(VMPTR_MethodDesc vmMethod, CORDB_ADDRESS codeStartAddress, OUT VMPTR_ReJitInfo* pvmReJitInfo)
  4489| {
  4490|     DD_ENTER_MAY_THROW;
  4491|     _ASSERTE(!"You shouldn't be calling this - use GetNativeCodeVersionNode instead");
  4492|     return S_OK;
  4493| }
  4494| HRESULT DacDbiInterfaceImpl::AreOptimizationsDisabled(VMPTR_Module vmModule, mdMethodDef methodTk, OUT BOOL* pOptimizationsDisabled)
  4495| {
  4496|     DD_ENTER_MAY_THROW;
  4497| #ifdef FEATURE_REJIT
  4498|     PTR_Module pModule = vmModule.GetDacPtr();
  4499|     if (pModule == NULL || pOptimizationsDisabled == NULL || TypeFromToken(methodTk) != mdtMethodDef)
  4500|     {
  4501|         return E_INVALIDARG;
  4502|     }
  4503|     {
  4504|         CodeVersionManager * pCodeVersionManager = pModule->GetCodeVersionManager();
  4505|         ILCodeVersion activeILVersion = pCodeVersionManager->GetActiveILCodeVersion(pModule, methodTk);
  4506|         *pOptimizationsDisabled = activeILVersion.IsDeoptimized();
  4507|     }
  4508| #else
  4509|     pOptimizationsDisabled->SetDacTargetPtr(0);
  4510| #endif
  4511|     return S_OK;
  4512| }
  4513| HRESULT DacDbiInterfaceImpl::GetNativeCodeVersionNode(VMPTR_MethodDesc vmMethod, CORDB_ADDRESS codeStartAddress, OUT VMPTR_NativeCodeVersionNode* pVmNativeCodeVersionNode)
  4514| {
  4515|     DD_ENTER_MAY_THROW;
  4516|     if (pVmNativeCodeVersionNode == NULL)
  4517|         return E_INVALIDARG;
  4518| #ifdef FEATURE_REJIT
  4519|     PTR_MethodDesc pMD = vmMethod.GetDacPtr();
  4520|     CodeVersionManager * pCodeVersionManager = pMD->GetCodeVersionManager();
  4521|     NativeCodeVersion codeVersion = pCodeVersionManager->GetNativeCodeVersion(pMD, (PCODE)codeStartAddress);
  4522|     pVmNativeCodeVersionNode->SetDacTargetPtr(PTR_TO_TADDR(codeVersion.AsNode()));
  4523| #else
  4524|     pVmNativeCodeVersionNode->SetDacTargetPtr(0);
  4525| #endif
  4526|     return S_OK;
  4527| }
  4528| HRESULT DacDbiInterfaceImpl::GetSharedReJitInfo(VMPTR_ReJitInfo vmReJitInfo, OUT VMPTR_SharedReJitInfo* pvmSharedReJitInfo)
  4529| {
  4530|     DD_ENTER_MAY_THROW;
  4531|     _ASSERTE(!"You shouldn't be calling this - use GetLCodeVersionNode instead");
  4532|     return S_OK;
  4533| }
  4534| HRESULT DacDbiInterfaceImpl::GetILCodeVersionNode(VMPTR_NativeCodeVersionNode vmNativeCodeVersionNode, VMPTR_ILCodeVersionNode* pVmILCodeVersionNode)
  4535| {
  4536|     DD_ENTER_MAY_THROW;
  4537|     if (pVmILCodeVersionNode == NULL)
  4538|         return E_INVALIDARG;
  4539| #ifdef FEATURE_REJIT
  4540|     NativeCodeVersionNode* pNativeCodeVersionNode = vmNativeCodeVersionNode.GetDacPtr();
  4541|     ILCodeVersion ilCodeVersion = pNativeCodeVersionNode->GetILCodeVersion();
  4542|     if (ilCodeVersion.IsDefaultVersion())
  4543|     {
  4544|         pVmILCodeVersionNode->SetDacTargetPtr(0);
  4545|     }
  4546|     else
  4547|     {
  4548|         pVmILCodeVersionNode->SetDacTargetPtr(PTR_TO_TADDR(ilCodeVersion.AsNode()));
  4549|     }
  4550| #else
  4551|     _ASSERTE(!"You shouldn't be calling this - rejit is not supported in this build");
  4552|     pVmILCodeVersionNode->SetDacTargetPtr(0);
  4553| #endif
  4554|     return S_OK;
  4555| }
  4556| HRESULT DacDbiInterfaceImpl::GetSharedReJitInfoData(VMPTR_SharedReJitInfo vmSharedReJitInfo, DacSharedReJitInfo* pData)
  4557| {
  4558|     DD_ENTER_MAY_THROW;
  4559|     _ASSERTE(!"You shouldn't be calling this - use GetILCodeVersionNodeData instead");
  4560|     return S_OK;
  4561| }
  4562| HRESULT DacDbiInterfaceImpl::GetILCodeVersionNodeData(VMPTR_ILCodeVersionNode vmILCodeVersionNode, DacSharedReJitInfo* pData)
  4563| {
  4564|     DD_ENTER_MAY_THROW;
  4565| #ifdef FEATURE_REJIT
  4566|     ILCodeVersion ilCode(vmILCodeVersionNode.GetDacPtr());
  4567|     pData->m_state = ilCode.GetRejitState();
  4568|     pData->m_pbIL = PTR_TO_CORDB_ADDRESS(dac_cast<ULONG_PTR>(ilCode.GetIL()));
  4569|     pData->m_dwCodegenFlags = ilCode.GetJitFlags();
  4570|     const InstrumentedILOffsetMapping* pMapping = ilCode.GetInstrumentedILMap();
  4571|     if (pMapping)
  4572|     {
  4573|         pData->m_cInstrumentedMapEntries = (ULONG)pMapping->GetCount();
  4574|         pData->m_rgInstrumentedMapEntries = PTR_TO_CORDB_ADDRESS(dac_cast<ULONG_PTR>(pMapping->GetOffsets()));
  4575|     }
  4576|     else
  4577|     {
  4578|         pData->m_cInstrumentedMapEntries = 0;
  4579|         pData->m_rgInstrumentedMapEntries = 0;
  4580|     }
  4581| #else
  4582|     _ASSERTE(!"You shouldn't be calling this - rejit isn't supported in this build");
  4583| #endif
  4584|     return S_OK;
  4585| }
  4586| HRESULT DacDbiInterfaceImpl::GetDefinesBitField(ULONG32 *pDefines)
  4587| {
  4588|     DD_ENTER_MAY_THROW;
  4589|     if (pDefines == NULL)
  4590|         return E_INVALIDARG;
  4591|     if (g_pDebugger == NULL)
  4592|         return CORDBG_E_NOTREADY;
  4593|     *pDefines = g_pDebugger->m_defines;
  4594|     return S_OK;
  4595| }
  4596| HRESULT DacDbiInterfaceImpl::GetMDStructuresVersion(ULONG32* pMDStructuresVersion)
  4597| {
  4598|     DD_ENTER_MAY_THROW;
  4599|     if (pMDStructuresVersion == NULL)
  4600|         return E_INVALIDARG;
  4601|     if (g_pDebugger == NULL)
  4602|         return CORDBG_E_NOTREADY;
  4603|     *pMDStructuresVersion = g_pDebugger->m_mdDataStructureVersion;
  4604|     return S_OK;
  4605| }
  4606| HRESULT DacDbiInterfaceImpl::EnableGCNotificationEvents(BOOL fEnable)
  4607| {
  4608|     DD_ENTER_MAY_THROW
  4609|     HRESULT hr = S_OK;
  4610|     EX_TRY
  4611|     {
  4612|         if (g_pDebugger != NULL)
  4613|         {
  4614|             TADDR addr = PTR_HOST_MEMBER_TADDR(Debugger, g_pDebugger, m_isGarbageCollectionEventsEnabled);
  4615|             SafeWriteStructOrThrow<BOOL>(addr, &fEnable);
  4616|         }
  4617|     }
  4618|     EX_CATCH_HRESULT(hr);
  4619|     return hr;
  4620| }
  4621| DacRefWalker::DacRefWalker(ClrDataAccess *dac, BOOL walkStacks, BOOL walkFQ, UINT32 handleMask, BOOL resolvePointers)
  4622|     : mDac(dac), mWalkStacks(walkStacks), mWalkFQ(walkFQ), mHandleMask(handleMask), mStackWalker(NULL),
  4623|       mResolvePointers(resolvePointers), mHandleWalker(NULL), mFQStart(PTR_NULL), mFQEnd(PTR_NULL), mFQCurr(PTR_NULL)
  4624| {
  4625| }
  4626| DacRefWalker::~DacRefWalker()
  4627| {
  4628|     Clear();
  4629| }
  4630| HRESULT DacRefWalker::Init()
  4631| {
  4632|     HRESULT hr = S_OK;
  4633|     if (mHandleMask)
  4634|     {
  4635|         mHandleWalker = new DacHandleWalker();
  4636|         hr = mHandleWalker->Init(GetHandleWalkerMask());
  4637|     }
  4638|     if (mWalkStacks && SUCCEEDED(hr))
  4639|     {
  4640|         hr = NextThread();
  4641|     }
  4642|     return hr;
  4643| }
  4644| void DacRefWalker::Clear()
  4645| {
  4646|     if (mHandleWalker)
  4647|     {
  4648|         delete mHandleWalker;
  4649|         mHandleWalker = NULL;
  4650|     }
  4651|     if (mStackWalker)
  4652|     {
  4653|         delete mStackWalker;
  4654|         mStackWalker = NULL;
  4655|     }
  4656| }
  4657| UINT32 DacRefWalker::GetHandleWalkerMask()
  4658| {
  4659|     UINT32 result = 0;
  4660|     if (mHandleMask & CorHandleStrong)
  4661|         result |= (1 << HNDTYPE_STRONG);
  4662|     if (mHandleMask & CorHandleStrongPinning)
  4663|         result |= (1 << HNDTYPE_PINNED);
  4664|     if (mHandleMask & CorHandleWeakShort)
  4665|         result |= (1 << HNDTYPE_WEAK_SHORT);
  4666|     if (mHandleMask & CorHandleWeakLong)
  4667|         result |= (1 << HNDTYPE_WEAK_LONG);
  4668| #if defined(FEATURE_COMINTEROP) || defined(FEATURE_COMWRAPPERS) || defined(FEATURE_OBJCMARSHAL)
  4669|     if ((mHandleMask & CorHandleWeakRefCount) || (mHandleMask & CorHandleStrongRefCount))
  4670|         result |= (1 << HNDTYPE_REFCOUNTED);
  4671| #endif // FEATURE_COMINTEROP || FEATURE_COMWRAPPERS || FEATURE_OBJCMARSHAL
  4672|     if (mHandleMask & CorHandleStrongDependent)
  4673|         result |= (1 << HNDTYPE_DEPENDENT);
  4674|     if (mHandleMask & CorHandleStrongAsyncPinned)
  4675|         result |= (1 << HNDTYPE_ASYNCPINNED);
  4676|     if (mHandleMask & CorHandleStrongSizedByref)
  4677|         result |= (1 << HNDTYPE_SIZEDREF);
  4678|     return result;
  4679| }
  4680| HRESULT DacRefWalker::Next(ULONG celt, DacGcReference roots[], ULONG *pceltFetched)
  4681| {
  4682|     if (roots == NULL || pceltFetched == NULL)
  4683|         return E_POINTER;
  4684|     ULONG total = 0;
  4685|     HRESULT hr = S_OK;
  4686|     if (mHandleWalker)
  4687|     {
  4688|         hr = mHandleWalker->Next(celt, roots, &total);
  4689|         if (total == 0 || FAILED(hr))
  4690|         {
  4691|             delete mHandleWalker;
  4692|             mHandleWalker = NULL;
  4693|             if (FAILED(hr))
  4694|                 return hr;
  4695|         }
  4696|     }
  4697|     if (total < celt)
  4698|     {
  4699|         while (total < celt && mFQCurr < mFQEnd)
  4700|         {
  4701|             DacGcReference &ref = roots[total++];
  4702|             ref.vmDomain = VMPTR_AppDomain::NullPtr();
  4703|             ref.objHnd.SetDacTargetPtr(mFQCurr.GetAddr());
  4704|             ref.dwType = (DWORD)CorReferenceFinalizer;
  4705|             ref.i64ExtraData = 0;
  4706|             mFQCurr++;
  4707|         }
  4708|     }
  4709|     while (total < celt && mStackWalker)
  4710|     {
  4711|         ULONG fetched = 0;
  4712|         hr = mStackWalker->Next(celt-total, roots+total, &fetched);
  4713|         if (FAILED(hr))
  4714|             return hr;
  4715|         if (fetched == 0)
  4716|         {
  4717|             hr = NextThread();
  4718|             if (FAILED(hr))
  4719|                 return hr;
  4720|         }
  4721|         total += fetched;
  4722|     }
  4723|     *pceltFetched = total;
  4724|     return total < celt ? S_FALSE : S_OK;
  4725| }
  4726| HRESULT DacRefWalker::NextThread()
  4727| {
  4728|     Thread *pThread = NULL;
  4729|     if (mStackWalker)
  4730|     {
  4731|         pThread = mStackWalker->GetThread();
  4732|         delete mStackWalker;
  4733|         mStackWalker = NULL;
  4734|     }
  4735|     pThread = ThreadStore::GetThreadList(pThread);
  4736|     if (!pThread)
  4737|         return S_FALSE;
  4738|     mStackWalker = new DacStackReferenceWalker(mDac, pThread->GetOSThreadId(), mResolvePointers == TRUE);
  4739|     return mStackWalker->Init();
  4740| }
  4741| HRESULT DacHandleWalker::Next(ULONG count, DacGcReference roots[], ULONG *pFetched)
  4742| {
  4743|     SUPPORTS_DAC;
  4744|     if (roots == NULL || pFetched == NULL)
  4745|         return E_POINTER;
  4746|     if (!mEnumerated)
  4747|         WalkHandles();
  4748|     unsigned int i;
  4749|     for (i = 0; i < count && mIteratorIndex < mList.GetCount(); mIteratorIndex++, i++)
  4750|     {
  4751|         const SOSHandleData &handle = mList.Get(mIteratorIndex);
  4752|         roots[i].objHnd.SetDacTargetPtr(TO_TADDR(handle.Handle));
  4753|         roots[i].vmDomain.SetDacTargetPtr(TO_TADDR(handle.AppDomain));
  4754|         roots[i].i64ExtraData = 0;
  4755|         unsigned int refCnt = 0;
  4756|         switch (handle.Type)
  4757|         {
  4758|             case HNDTYPE_STRONG:
  4759|                 roots[i].dwType = (DWORD)CorHandleStrong;
  4760|                 break;
  4761|             case HNDTYPE_PINNED:
  4762|                 roots[i].dwType = (DWORD)CorHandleStrongPinning;
  4763|                 break;
  4764|             case HNDTYPE_WEAK_SHORT:
  4765|                 roots[i].dwType = (DWORD)CorHandleWeakShort;
  4766|                 break;
  4767|             case HNDTYPE_WEAK_LONG:
  4768|                 roots[i].dwType = (DWORD)CorHandleWeakLong;
  4769|                 break;
  4770|         #if defined(FEATURE_COMINTEROP) || defined(FEATURE_COMWRAPPERS) || defined(FEATURE_OBJCMARSHAL)
  4771|             case HNDTYPE_REFCOUNTED:
  4772|                 GetRefCountedHandleInfo((OBJECTREF)CLRDATA_ADDRESS_TO_TADDR(handle.Handle), handle.Type, &refCnt, NULL, NULL, NULL);
  4773|                 roots[i].i64ExtraData = refCnt;
  4774|                 roots[i].dwType = (DWORD)(roots[i].i64ExtraData ? CorHandleStrongRefCount : CorHandleWeakRefCount);
  4775|                 break;
  4776|         #endif // FEATURE_COMINTEROP || FEATURE_COMWRAPPERS || FEATURE_OBJCMARSHAL
  4777|             case HNDTYPE_DEPENDENT:
  4778|                 roots[i].dwType = (DWORD)CorHandleStrongDependent;
  4779|                 roots[i].i64ExtraData = GetDependentHandleSecondary(CLRDATA_ADDRESS_TO_TADDR(handle.Handle)).GetAddr();
  4780|                 break;
  4781|             case HNDTYPE_ASYNCPINNED:
  4782|                 roots[i].dwType = (DWORD)CorHandleStrongAsyncPinned;
  4783|                 break;
  4784|             case HNDTYPE_SIZEDREF:
  4785|                 roots[i].dwType = (DWORD)CorHandleStrongSizedByref;
  4786|                 break;
  4787|         }
  4788|     }
  4789|     *pFetched = i;
  4790|     return (unsigned)mIteratorIndex < mList.GetCount() ? S_FALSE : S_OK;
  4791| }
  4792| HRESULT DacStackReferenceWalker::Next(ULONG count, DacGcReference stackRefs[], ULONG *pFetched)
  4793| {
  4794|     if (stackRefs == NULL || pFetched == NULL)
  4795|         return E_POINTER;
  4796|     if (!mEnumerated)
  4797|         WalkStack();
  4798|     TADDR domain = AppDomain::GetCurrentDomain().GetAddr();
  4799|     unsigned int i;
  4800|     for (i = 0; i < count && mIteratorIndex < mList.GetCount(); mIteratorIndex++, i++)
  4801|     {
  4802|         stackRefs[i].dwType = CorReferenceStack;
  4803|         stackRefs[i].vmDomain.SetDacTargetPtr(domain);
  4804|         stackRefs[i].i64ExtraData = 0;
  4805|         const SOSStackRefData &sosStackRef = mList.Get(i);
  4806|         if (sosStackRef.Flags & GC_CALL_INTERIOR || sosStackRef.Address == 0)
  4807|         {
  4808|             stackRefs[i].pObject = CLRDATA_ADDRESS_TO_TADDR(sosStackRef.Object) | 1;
  4809|         }
  4810|         else
  4811|         {
  4812|             stackRefs[i].objHnd.SetDacTargetPtr(CLRDATA_ADDRESS_TO_TADDR(sosStackRef.Address));
  4813|         }
  4814|     }
  4815|     *pFetched = i;
  4816|     return S_OK;
  4817| }


# ====================================================================
# FILE: src/coreclr/debug/di/module.cpp
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-3126 ---
     1| #include "stdafx.h"
     2| #include "winbase.h"
     3| #include "metadataexports.h"
     4| #include "winbase.h"
     5| #include "corpriv.h"
     6| #include "corsym.h"
     7| #include "pedecoder.h"
     8| #include "stgpool.h"
     9| STDAPI ReOpenMetaDataWithMemoryEx(
    10|     void        *pUnk,
    11|     LPCVOID     pData,
    12|     ULONG       cbData,
    13|     DWORD       dwReOpenFlags)
    14| {
    15|     HRESULT hr = MDReOpenMetaDataWithMemoryEx(pUnk,pData, cbData, dwReOpenFlags);
    16|     return hr;
    17| }
    18| CordbModule::CordbModule(
    19|     CordbProcess *     pProcess,
    20|     VMPTR_Module        vmModule,
    21|     VMPTR_DomainAssembly    vmDomainAssembly)
    22| : CordbBase(pProcess, vmDomainAssembly.IsNull() ? VmPtrToCookie(vmModule) : VmPtrToCookie(vmDomainAssembly), enumCordbModule),
    23|     m_pAssembly(0),
    24|     m_pAppDomain(0),
    25|     m_classes(11),
    26|     m_functions(101),
    27|     m_vmDomainAssembly(vmDomainAssembly),
    28|     m_vmModule(vmModule),
    29|     m_EnCCount(0),
    30|     m_fForceMetaDataSerialize(FALSE),
    31|     m_nativeCodeTable(101)
    32| {
    33|     _ASSERTE(pProcess->GetProcessLock()->HasLock());
    34|     _ASSERTE(!vmModule.IsNull());
    35|     m_nLoadEventContinueCounter = 0;
    36| #ifdef _DEBUG
    37|     m_classes.DebugSetRSLock(pProcess->GetProcessLock());
    38|     m_functions.DebugSetRSLock(pProcess->GetProcessLock());
    39| #endif
    40|     ModuleInfo modInfo;
    41|     pProcess->GetDAC()->GetModuleData(vmModule, &modInfo); // throws
    42|     m_PEBuffer.Init(modInfo.pPEBaseAddress, modInfo.nPESize);
    43|     m_fDynamic  = modInfo.fIsDynamic;
    44|     m_fInMemory = modInfo.fInMemory;
    45|     m_vmPEFile = modInfo.vmPEAssembly;
    46|     if (!vmDomainAssembly.IsNull())
    47|     {
    48|         DomainAssemblyInfo dfInfo;
    49|         pProcess->GetDAC()->GetDomainAssemblyData(vmDomainAssembly, &dfInfo); // throws
    50|         m_pAppDomain = pProcess->LookupOrCreateAppDomain(dfInfo.vmAppDomain);
    51|         m_pAssembly  = m_pAppDomain->LookupOrCreateAssembly(dfInfo.vmDomainAssembly);
    52|     }
    53|     else
    54|     {
    55|         m_pAppDomain = pProcess->GetSharedAppDomain();
    56|         m_pAssembly = m_pAppDomain->LookupOrCreateAssembly(modInfo.vmAssembly);
    57|     }
    58| #ifdef _DEBUG
    59|     m_nativeCodeTable.DebugSetRSLock(GetProcess()->GetProcessLock());
    60| #endif
    61| }
    62| #ifdef _DEBUG
    63| void DbgAssertModuleDeletedCallback(VMPTR_DomainAssembly vmDomainAssembly, void * pUserData)
    64| {
    65|     CordbModule * pThis = reinterpret_cast<CordbModule *>(pUserData);
    66|     INTERNAL_DAC_CALLBACK(pThis->GetProcess());
    67|     if (!pThis->m_vmDomainAssembly.IsNull())
    68|     {
    69|         VMPTR_DomainAssembly vmDomainAssemblyDeleted = pThis->m_vmDomainAssembly;
    70|         CONSISTENCY_CHECK_MSGF((vmDomainAssemblyDeleted != vmDomainAssembly),
    71|             ("A Module Unload event was sent for a module, but it still shows up in the enumeration.\n vmDomainAssemblyDeleted=%p\n",
    72|             VmPtrToCookie(vmDomainAssemblyDeleted)));
    73|     }
    74| }
    75| void CordbModule::DbgAssertModuleDeleted()
    76| {
    77|     GetProcess()->GetDAC()->EnumerateModulesInAssembly(
    78|         m_pAssembly->GetDomainAssemblyPtr(),
    79|         DbgAssertModuleDeletedCallback,
    80|         this);
    81| }
    82| #endif // _DEBUG
    83| CordbModule::~CordbModule()
    84| {
    85|     _ASSERTE(IsNeutered());
    86|     _ASSERTE(m_pIMImport == NULL);
    87| }
    88| void CordbModule::Neuter()
    89| {
    90|     m_classes.NeuterAndClear(GetProcess()->GetProcessLock());
    91|     m_functions.NeuterAndClear(GetProcess()->GetProcessLock());
    92|     m_nativeCodeTable.NeuterAndClear(GetProcess()->GetProcessLock());
    93|     m_pClass.Clear();
    94|     m_pInternalMetaDataImport.Clear();
    95|     m_pIMImport.Clear();
    96|     CordbBase::Neuter();
    97| }
    98| void GetStreamFromTargetBuffer(CordbProcess * pProcess, TargetBuffer buffer, IStream ** ppStream)
    99| {
   100|     CONTRACTL
   101|     {
   102|         THROWS;
   103|     }
   104|     CONTRACTL_END;
   105|     _ASSERTE(ppStream != NULL);
   106|     _ASSERTE(*ppStream == NULL);
   107|     int cbSize = buffer.cbSize;
   108|     NewArrayHolder<BYTE> localBuffer(new BYTE[cbSize]);
   109|     pProcess->SafeReadBuffer(buffer, localBuffer);
   110|     HRESULT hr = E_FAIL;
   111|     hr = CInMemoryStream::CreateStreamOnMemoryCopy(localBuffer, cbSize, ppStream);
   112|     IfFailThrow(hr);
   113|     _ASSERTE(*ppStream != NULL);
   114| }
   115| IDacDbiInterface::SymbolFormat CordbModule::GetInMemorySymbolStream(IStream ** ppStream)
   116| {
   117|     CONTRACTL
   118|     {
   119|         THROWS;
   120|     }
   121|     CONTRACTL_END;
   122|     _ASSERTE(ppStream != NULL);
   123|     _ASSERTE(*ppStream == NULL);
   124|     *ppStream = NULL;
   125|     TargetBuffer bufferPdb;
   126|     IDacDbiInterface::SymbolFormat symFormat;
   127|     GetProcess()->GetDAC()->GetSymbolsBuffer(m_vmModule, &bufferPdb, &symFormat);
   128|     if (bufferPdb.IsEmpty())
   129|     {
   130|         _ASSERTE(symFormat == IDacDbiInterface::kSymbolFormatNone);
   131|         return IDacDbiInterface::kSymbolFormatNone;
   132|     }
   133|     else
   134|     {
   135|         _ASSERTE(symFormat != IDacDbiInterface::kSymbolFormatNone);
   136|         GetStreamFromTargetBuffer(GetProcess(), bufferPdb, ppStream);
   137|         return symFormat;
   138|     }
   139| }
   140| VMPTR_PEAssembly CordbModule::GetPEFile()
   141| {
   142|     return m_vmPEFile;
   143| }
   144| IMetaDataImport * CordbModule::GetMetaDataImporter()
   145| {
   146|     CONTRACTL
   147|     {
   148|         THROWS;
   149|     }
   150|     CONTRACTL_END;
   151|     if (m_pIMImport != NULL)
   152|     {
   153|         return m_pIMImport;
   154|     }
   155|     LOG((LF_CORDB,LL_INFO1000, "CM::GMI Lazy init refreshing metadata\n"));
   156|     ALLOW_DATATARGET_MISSING_MEMORY(
   157|         RefreshMetaData();
   158|     );
   159|     if (m_pIMImport == NULL)
   160|     {
   161|         bool isILMetaDataForNGENImage;  // Not currently used for anything.
   162|         CordbProcess * pProcess = GetProcess();
   163|         RSLockHolder processLockHolder(pProcess->GetProcessLock());
   164|         m_pInternalMetaDataImport.Clear();
   165|         pProcess->LookupMetaDataFromDebugger(m_vmPEFile, isILMetaDataForNGENImage, this);
   166|     }
   167|     if (m_pIMImport == NULL)
   168|     {
   169|         ThrowHR(CORDBG_E_MISSING_METADATA);
   170|     }
   171|     return m_pIMImport;
   172| }
   173| void CordbModule::UpdateMetaDataCacheIfNeeded(mdToken token)
   174| {
   175|     CONTRACTL
   176|     {
   177|         THROWS;
   178|     }
   179|     CONTRACTL_END;
   180|     LOG((LF_CORDB,LL_INFO10000, "CM::UMCIN token=0x%x\n", token));
   181|     if(GetProcess()->GetWriteableMetadataUpdateMode() != LegacyCompatPolicy)
   182|     {
   183|         return;
   184|     }
   185|     if(CheckIfTokenInMetaData(token))
   186|     {
   187|         LOG((LF_CORDB,LL_INFO10000, "CM::UMCIN token was present\n"));
   188|         return;
   189|     }
   190|     LOG((LF_CORDB,LL_INFO10000, "CM::UMCIN token was not present, refreshing\n"));
   191|     m_fForceMetaDataSerialize = TRUE;
   192|     RefreshMetaData();
   193| }
   194| BOOL CordbModule::CheckIfTokenInMetaData(mdToken token)
   195| {
   196|     CONTRACTL
   197|     {
   198|         THROWS;
   199|     }
   200|     CONTRACTL_END;
   201|     LOG((LF_CORDB,LL_INFO10000, "CM::CITIM token=0x%x\n", token));
   202|     _ASSERTE(TypeFromToken(token) == mdtSignature);
   203|     RSExtSmartPtr<IMetaDataTables> pTable;
   204|     HRESULT hr = GetMetaDataImporter()->QueryInterface(IID_IMetaDataTables, (void**) &pTable);
   205|     _ASSERTE(SUCCEEDED(hr));
   206|     if (FAILED(hr))
   207|     {
   208|         ThrowHR(hr);
   209|     }
   210|     ULONG cbRowsAvailable; // number of rows in the table
   211|     hr = pTable->GetTableInfo(
   212|         mdtSignature >> 24,                      // [IN] Which table.
   213|         NULL,                    // [OUT] Size of a row, bytes.
   214|         &cbRowsAvailable,                    // [OUT] Number of rows.
   215|         NULL,                    // [OUT] Number of columns in each row.
   216|         NULL,                     // [OUT] Key column, or -1 if none.
   217|         NULL);          // [OUT] Name of the table.
   218|     _ASSERTE(SUCCEEDED(hr));
   219|     if (FAILED(hr))
   220|     {
   221|         ThrowHR(hr);
   222|     }
   223|     ULONG rowRequested = RidFromToken(token);
   224|     LOG((LF_CORDB,LL_INFO10000, "CM::UMCIN requested=0x%x available=0x%x\n", rowRequested, cbRowsAvailable));
   225|     return (rowRequested <= cbRowsAvailable);
   226| }
   227| class CleanupRemoteBuffer
   228| {
   229| public:
   230|     CordbProcess* pProcess;
   231|     CordbModule* pModule;
   232|     TargetBuffer bufferMetaData;
   233|     BOOL fDoCleanup;
   234|     CleanupRemoteBuffer() :
   235|     fDoCleanup(FALSE) { }
   236|     ~CleanupRemoteBuffer()
   237|     {
   238|         if(fDoCleanup)
   239|         {
   240|             DebuggerIPCEvent event;
   241|             pProcess->InitIPCEvent(&event,
   242|                 DB_IPCE_RESOLVE_UPDATE_METADATA_2,
   243|                 true,
   244|                 pModule->GetAppDomain()->GetADToken());
   245|             event.MetadataUpdateRequest.pMetadataStart = CORDB_ADDRESS_TO_PTR(bufferMetaData.pAddress);
   246|             IfFailThrow(pProcess->SendIPCEvent(&event, sizeof(DebuggerIPCEvent)));
   247|             _ASSERTE(event.type == DB_IPCE_RESOLVE_UPDATE_METADATA_2_RESULT);
   248|         }
   249|     }
   250| };
   251| void CordbModule::RefreshMetaData()
   252| {
   253|     CONTRACTL
   254|     {
   255|         THROWS;
   256|     }
   257|     CONTRACTL_END;
   258|     LOG((LF_CORDB,LL_INFO1000, "CM::RM\n"));
   259|     CordbProcess * pProcess = GetProcess();
   260|     TargetBuffer bufferMetaData;
   261|     CleanupRemoteBuffer cleanup; // this local has a destructor to do some finally work
   262|     if (GetProcess()->GetShim() == NULL &&
   263|         GetProcess()->GetWriteableMetadataUpdateMode() == AlwaysShowUpdates &&
   264|         !m_fDynamic)
   265|     {
   266|         TADDR remoteMDInternalRWAddr = NULL;
   267|         GetProcess()->GetDAC()->GetPEFileMDInternalRW(m_vmPEFile, &remoteMDInternalRWAddr);
   268|         if (remoteMDInternalRWAddr != NULL)
   269|         {
   270|             _ASSERTE(m_pIMImport == NULL);
   271|             ULONG32 mdStructuresVersion;
   272|             HRESULT hr = GetProcess()->GetDAC()->GetMDStructuresVersion(&mdStructuresVersion);
   273|             IfFailThrow(hr);
   274|             ULONG32 mdStructuresDefines;
   275|             hr = GetProcess()->GetDAC()->GetDefinesBitField(&mdStructuresDefines);
   276|             IfFailThrow(hr);
   277|             IMetaDataDispenserCustom* pDispCustom = NULL;
   278|             hr = GetProcess()->GetDispenser()->QueryInterface(IID_IMetaDataDispenserCustom, (void**)&pDispCustom);
   279|             IfFailThrow(hr);
   280|             IMDCustomDataSource* pDataSource = NULL;
   281|             hr = CreateRemoteMDInternalRWSource(remoteMDInternalRWAddr, GetProcess()->GetDataTarget(), mdStructuresDefines, mdStructuresVersion, &pDataSource);
   282|             IfFailThrow(hr);
   283|             IMetaDataImport* pImport = NULL;
   284|             hr = pDispCustom->OpenScopeOnCustomDataSource(pDataSource, 0, IID_IMetaDataImport, (IUnknown**)&m_pIMImport);
   285|             IfFailThrow(hr);
   286|             UpdateInternalMetaData();
   287|             return;
   288|         }
   289|     }
   290|     if(!m_fForceMetaDataSerialize) // case 1 and 2
   291|     {
   292|         LOG((LF_CORDB,LL_INFO10000, "CM::RM !m_fForceMetaDataSerialize case\n"));
   293|         GetProcess()->GetDAC()->GetMetadata(m_vmModule, &bufferMetaData); // throws
   294|     }
   295|     else if (GetProcess()->GetShim() == NULL) // case 3 won't work on a dump so don't try
   296|     {
   297|         return;
   298|     }
   299|     else // case 3 on a live process
   300|     {
   301|         LOG((LF_CORDB,LL_INFO10000, "CM::RM m_fForceMetaDataSerialize case\n"));
   302|         DebuggerIPCEvent event;
   303|         pProcess->InitIPCEvent(&event,
   304|             DB_IPCE_RESOLVE_UPDATE_METADATA_1,
   305|             true,
   306|             GetAppDomain()->GetADToken());
   307|         event.MetadataUpdateRequest.vmModule = m_vmModule;
   308|         IfFailThrow(pProcess->SendIPCEvent(&event, sizeof(DebuggerIPCEvent)));
   309|         _ASSERTE(event.type == DB_IPCE_RESOLVE_UPDATE_METADATA_1_RESULT);
   310|         bufferMetaData.Init(PTR_TO_CORDB_ADDRESS(event.MetadataUpdateRequest.pMetadataStart), (ULONG) event.MetadataUpdateRequest.nMetadataSize);
   311|         cleanup.bufferMetaData = bufferMetaData;
   312|         cleanup.pProcess = pProcess;
   313|         cleanup.pModule = this;
   314|         cleanup.fDoCleanup = TRUE;
   315|     }
   316|     InitMetaData(bufferMetaData, IsFileMetaDataValid()); // throws
   317| }
   318| BOOL CordbModule::IsFileMetaDataValid()
   319| {
   320|     bool fOpenFromFile = true;
   321|     if (m_fDynamic || m_fInMemory || m_fForceMetaDataSerialize)
   322|     {
   323|         LOG((LF_CORDB,LL_INFO10000, "CM::IFMV: m_fDynamic=0x%x m_fInMemory=0x%x m_fForceMetaDataSerialize=0x%x\n",
   324|             m_fDynamic, m_fInMemory, m_fForceMetaDataSerialize));
   325|         fOpenFromFile = false;
   326|     }
   327| #ifdef _DEBUG
   328|     static DWORD openFromFile = 99;
   329|     if (openFromFile == 99)
   330|         openFromFile = CLRConfig::GetConfigValue(CLRConfig::INTERNAL_DbgNoOpenMDByFile);
   331|     if (openFromFile)
   332|     {
   333|         LOG((LF_CORDB,LL_INFO10000, "CM::IFMV: INTERNAL_DbgNoOpenMDByFile is set\n"));
   334|         fOpenFromFile = false;
   335|     }
   336| #endif
   337|     LOG((LF_CORDB,LL_INFO10000, "CM::IFMV: returns 0x%x\n", fOpenFromFile));
   338|     return fOpenFromFile;
   339| }
   340| IMDInternalImport * CordbModule::GetInternalMD()
   341| {
   342|     if (m_pInternalMetaDataImport == NULL)
   343|     {
   344|         UpdateInternalMetaData(); // throws
   345|     }
   346|     return m_pInternalMetaDataImport;
   347| }
   348| void CordbModule::InitMetaData(TargetBuffer buffer, BOOL allowFileMappingOptimization)
   349| {
   350|     CONTRACTL
   351|     {
   352|         THROWS;
   353|     }
   354|     CONTRACTL_END;
   355|     LOG((LF_CORDB,LL_INFO100000, "CM::IM: initing with remote buffer 0x%p length 0x%x\n",
   356|         CORDB_ADDRESS_TO_PTR(buffer.pAddress), buffer.cbSize));
   357|     m_pInternalMetaDataImport.Clear();
   358|     if (m_pIMImport == NULL)
   359|     {
   360|         HRESULT hr = S_OK;
   361|         if (allowFileMappingOptimization)
   362|         {
   363|             hr = InitPublicMetaDataFromFile();
   364|             if(FAILED(hr))
   365|             {
   366|                 LOG((LF_CORDB,LL_INFO1000000, "CM::IPM: File mapping failed with hr=0x%x\n", hr));
   367|             }
   368|         }
   369|         if(!allowFileMappingOptimization || FAILED(hr))
   370|         {
   371|             InitPublicMetaData(buffer);
   372|         }
   373|     }
   374|     else
   375|     {
   376|         UpdatePublicMetaDataFromRemote(buffer);
   377|     }
   378|     _ASSERTE(m_pIMImport != NULL);
   379|     UpdateInternalMetaData();
   380| }
   381| void CordbModule::UpdateInternalMetaData()
   382| {
   383|     CONTRACTL
   384|     {
   385|         THROWS;
   386|     }
   387|     CONTRACTL_END;
   388|     _ASSERTE(m_pInternalMetaDataImport == NULL);
   389|     IMetaDataImport * pImport = GetMetaDataImporter(); // throws
   390|     if (m_pInternalMetaDataImport == NULL)
   391|     {
   392|         HRESULT hr = GetMDInternalInterfaceFromPublic(
   393|             pImport,
   394|             IID_IMDInternalImport,
   395|             reinterpret_cast<void**> (&m_pInternalMetaDataImport));
   396|         if (m_pInternalMetaDataImport == NULL)
   397|         {
   398|             ThrowHR(hr);
   399|         }
   400|     }
   401|     _ASSERTE(m_pInternalMetaDataImport != NULL);
   402| }
   403| HRESULT CordbModule::InitPublicMetaDataFromFile()
   404| {
   405|     INTERNAL_API_ENTRY(this->GetProcess());
   406|     const WCHAR * szFullPathName = NULL;
   407|     bool fDebuggerLoadingNgen = false;
   408|     bool fDebuggeeLoadedNgen = false;
   409|     szFullPathName = GetNGenImagePath();
   410|     if(szFullPathName != NULL)
   411|     {
   412|         fDebuggeeLoadedNgen = true;
   413|         fDebuggerLoadingNgen = true;
   414| #ifndef TARGET_UNIX
   415|         if (NULL == WszGetModuleHandle(szFullPathName))
   416| #endif
   417|         {
   418|             szFullPathName = NULL;
   419|             fDebuggerLoadingNgen = false;
   420|         }
   421|     }
   422|     if (!fDebuggerLoadingNgen)
   423|     {
   424|         szFullPathName = GetModulePath();
   425|     }
   426|     if(fDebuggeeLoadedNgen && !fDebuggerLoadingNgen && GetProcess()->GetShim()!=NULL)
   427|     {
   428|         return CORDBG_E_MISSING_METADATA;
   429|     }
   430|     DWORD dwOpenFlags = 0;
   431|     if (fDebuggerLoadingNgen)
   432|     {
   433|         dwOpenFlags = ofReadOnly | ofTrustedImage;
   434|     }
   435|     return InitPublicMetaDataFromFile(szFullPathName, dwOpenFlags, true);
   436| }
   437| HRESULT CordbModule::InitPublicMetaDataFromFile(const WCHAR * pszFullPathName,
   438|                                                 DWORD dwOpenFlags,
   439|                                                 bool validateFileInfo)
   440| {
   441| #ifdef HOST_UNIX
   442|     return E_FAIL;
   443| #else
   444|     if (validateFileInfo)
   445|     {
   446|         DWORD dwImageTimeStamp = 0;
   447|         DWORD dwImageSize = 0;
   448|         bool isNGEN = false; // unused
   449|         StringCopyHolder filePath;
   450|         _ASSERTE(!m_vmPEFile.IsNull());
   451|         if (!this->GetProcess()->GetDAC()->GetMetaDataFileInfoFromPEFile(m_vmPEFile,
   452|                                                                          dwImageTimeStamp,
   453|                                                                          dwImageSize,
   454|                                                                          isNGEN,
   455|                                                                          &filePath))
   456|         {
   457|             LOG((LF_CORDB,LL_WARNING, "CM::IM: Couldn't get metadata info for file \"%s\"\n", pszFullPathName));
   458|             return CORDBG_E_MISSING_METADATA;
   459|         }
   460|         HandleHolder hMDFile = WszCreateFile(pszFullPathName,
   461|                                               GENERIC_READ,
   462|                                               FILE_SHARE_READ,
   463|                                               NULL,                 // default security descriptor
   464|                                               OPEN_EXISTING,
   465|                                               FILE_ATTRIBUTE_NORMAL,
   466|                                               NULL);
   467|         if (hMDFile == INVALID_HANDLE_VALUE)
   468|         {
   469|             LOG((LF_CORDB,LL_WARNING, "CM::IM: Couldn't open file \"%s\" (GLE=%x)\n", pszFullPathName, GetLastError()));
   470|             return CORDBG_E_MISSING_METADATA;
   471|         }
   472|         DWORD dwFileHigh = 0;
   473|         DWORD dwFileLow = GetFileSize(hMDFile, &dwFileHigh);
   474|         if (dwFileLow == INVALID_FILE_SIZE)
   475|         {
   476|             LOG((LF_CORDB,LL_WARNING, "CM::IM: File \"%s\" had invalid size.\n", pszFullPathName));
   477|             return CORDBG_E_MISSING_METADATA;
   478|         }
   479|         _ASSERTE(dwFileHigh == 0);
   480|         HandleHolder hMap = WszCreateFileMapping(hMDFile, NULL, PAGE_READONLY, dwFileHigh, dwFileLow, NULL);
   481|         if (hMap == NULL)
   482|         {
   483|             LOG((LF_CORDB,LL_WARNING, "CM::IM: Couldn't create mapping of file \"%s\" (GLE=%x)\n", pszFullPathName, GetLastError()));
   484|             return CORDBG_E_MISSING_METADATA;
   485|         }
   486|         MapViewHolder hMapView = MapViewOfFile(hMap, FILE_MAP_READ, 0, 0, 0);
   487|         if (hMapView == NULL)
   488|         {
   489|             LOG((LF_CORDB,LL_WARNING, "CM::IM: Couldn't map view of file \"%s\" (GLE=%x)\n", pszFullPathName, GetLastError()));
   490|             return CORDBG_E_MISSING_METADATA;
   491|         }
   492|         PEDecoder pedecoder(hMapView, (COUNT_T)dwFileLow);
   493|         if (!pedecoder.HasNTHeaders())
   494|         {
   495|             LOG((LF_CORDB,LL_WARNING, "CM::IM: \"%s\" did not have PE headers!\n", pszFullPathName));
   496|             return CORDBG_E_MISSING_METADATA;
   497|         }
   498|         if ((dwImageSize != pedecoder.GetVirtualSize()) ||
   499|             (dwImageTimeStamp != pedecoder.GetTimeDateStamp()))
   500|         {
   501|             LOG((LF_CORDB,LL_WARNING, "CM::IM: Validation of \"%s\" failed.  "
   502|                 "Expected size=%x, Expected timestamp=%x, Actual size=%x, Actual timestamp=%x\n",
   503|                 pszFullPathName,
   504|                 pedecoder.GetVirtualSize(),
   505|                 pedecoder.GetTimeDateStamp(),
   506|                 dwImageSize,
   507|                 dwImageTimeStamp));
   508|             return CORDBG_E_MISSING_METADATA;
   509|         }
   510|     }
   511|     IMetaDataDispenserEx * pDisp = GetProcess()->GetDispenser();
   512|     HRESULT hr = pDisp->OpenScope(pszFullPathName, dwOpenFlags, IID_IMetaDataImport, (IUnknown**)&m_pIMImport);
   513|     _ASSERTE(SUCCEEDED(hr) == (m_pIMImport != NULL));
   514|     if (FAILED(hr))
   515|     {
   516|         LOG((LF_CORDB,LL_WARNING, "CM::IM: Couldn't open metadata in file \"%s\" (hr=%x)\n", pszFullPathName, hr));
   517|     }
   518|     return hr;
   519| #endif // TARGET_UNIX
   520| }
   521| void CordbModule::InitPublicMetaData(TargetBuffer buffer)
   522| {
   523|     CONTRACTL
   524|     {
   525|         THROWS;
   526|     }
   527|     CONTRACTL_END;
   528|     INTERNAL_API_ENTRY(this->GetProcess());
   529|     LOG((LF_CORDB,LL_INFO100000, "CM::IPM: initing with remote buffer 0x%p length 0x%x\n",
   530|         CORDB_ADDRESS_TO_PTR(buffer.pAddress), buffer.cbSize));
   531|     ULONG nMetaDataSize = buffer.cbSize;
   532|     if (nMetaDataSize == 0)
   533|     {
   534|         SIMPLIFYING_ASSUMPTION(!"Error: missing the metadata");
   535|         return;
   536|     }
   537|     HRESULT hr = S_OK;
   538|     IMetaDataDispenserEx * pDisp = GetProcess()->GetDispenser();
   539|     CoTaskMemHolder<VOID> pMetaDataCopy;
   540|     CopyRemoteMetaData(buffer, pMetaDataCopy.GetAddr());
   541|     VARIANT valueOld;
   542|     hr = pDisp->GetOption(MetaDataSetUpdate, &valueOld);
   543|     SIMPLIFYING_ASSUMPTION(!FAILED(hr));
   544|     VARIANT valueRW;
   545|     V_VT(&valueRW) = VT_UI4;
   546|     V_I4(&valueRW) = MDUpdateFull;
   547|     hr = pDisp->SetOption(MetaDataSetUpdate, &valueRW);
   548|     SIMPLIFYING_ASSUMPTION(!FAILED(hr));
   549|     hr = pDisp->OpenScopeOnMemory(pMetaDataCopy,
   550|                                   nMetaDataSize,
   551|                                   ofTakeOwnership,
   552|                                   IID_IMetaDataImport,
   553|                                   reinterpret_cast<IUnknown**>( &m_pIMImport ));
   554|     pMetaDataCopy.SuppressRelease();
   555|     HRESULT hrRestore = pDisp->SetOption(MetaDataSetUpdate, &valueOld);
   556|     SIMPLIFYING_ASSUMPTION(!FAILED(hrRestore));
   557|     IfFailThrow(hr);
   558|     IfFailThrow(hrRestore);
   559| }
   560| void CordbModule::UpdatePublicMetaDataFromRemote(TargetBuffer bufferRemoteMetaData)
   561| {
   562|     CONTRACTL
   563|     {
   564|         THROWS;
   565|     }
   566|     CONTRACTL_END;
   567|     if (bufferRemoteMetaData.IsEmpty())
   568|     {
   569|         ThrowHR(E_INVALIDARG);
   570|     }
   571|     INTERNAL_API_ENTRY(this->GetProcess()); //
   572|     LOG((LF_CORDB,LL_INFO100000, "CM::UPMFR: updating with remote buffer 0x%p length 0x%x\n",
   573|         CORDB_ADDRESS_TO_PTR(bufferRemoteMetaData.pAddress), bufferRemoteMetaData.cbSize));
   574|     _ASSERTE(m_pIMImport != NULL);
   575|     HRESULT hr = S_OK;
   576|     ULONG dwMetaDataSize = bufferRemoteMetaData.cbSize;
   577|     CoTaskMemHolder<VOID> pLocalMetaDataPtr;
   578|     CopyRemoteMetaData(bufferRemoteMetaData, pLocalMetaDataPtr.GetAddr());
   579|     IMetaDataDispenserEx *  pDisp = GetProcess()->GetDispenser();
   580|     _ASSERTE(pDisp != NULL); // throws on error.
   581|     LOG((LF_CORDB,LL_INFO100000, "CM::RI: converting to new metadata\n"));
   582|     {
   583|         ReleaseHolder<IMetaDataImport> pIMImport;
   584|         hr = pDisp->OpenScopeOnMemory(pLocalMetaDataPtr,
   585|                                   dwMetaDataSize,
   586|                                   0,
   587|                                   IID_IMetaDataImport,
   588|                                   (IUnknown**)&pIMImport);
   589|         IfFailThrow(hr);
   590|     }
   591|     _ASSERTE(m_pIMImport != NULL); //
   592|     hr = ReOpenMetaDataWithMemoryEx(m_pIMImport, pLocalMetaDataPtr, dwMetaDataSize, ofTakeOwnership );
   593|     IfFailThrow(hr);
   594|     pLocalMetaDataPtr.SuppressRelease();
   595| }
   596| void CordbModule::CopyRemoteMetaData(
   597|     TargetBuffer buffer,
   598|     CoTaskMemHolder<VOID> * pLocalBuffer)
   599| {
   600|     CONTRACTL
   601|     {
   602|         THROWS;
   603|     }
   604|     CONTRACTL_END;
   605|     _ASSERTE(pLocalBuffer != NULL);
   606|     _ASSERTE(!buffer.IsEmpty());
   607|     LPVOID pRawBuffer = CoTaskMemAlloc(buffer.cbSize);
   608|     if (pRawBuffer == NULL)
   609|     {
   610|         ThrowOutOfMemory();
   611|     }
   612|     pLocalBuffer->Assign(pRawBuffer);
   613|     GetProcess()->SafeReadBuffer(buffer, (BYTE *)pRawBuffer);
   614|     return;
   615| }
   616| HRESULT CordbModule::QueryInterface(REFIID id, void **pInterface)
   617| {
   618|     if (id == IID_ICorDebugModule)
   619|     {
   620|         *pInterface = static_cast<ICorDebugModule*>(this);
   621|     }
   622|     else if (id == IID_ICorDebugModule2)
   623|     {
   624|         *pInterface = static_cast<ICorDebugModule2*>(this);
   625|     }
   626|     else if (id == IID_ICorDebugModule3)
   627|     {
   628|         *pInterface = static_cast<ICorDebugModule3*>(this);
   629|     }
   630|     else if (id == IID_ICorDebugModule4)
   631|     {
   632|         *pInterface = static_cast<ICorDebugModule4*>(this);
   633|     }
   634|     else if (id == IID_IUnknown)
   635|     {
   636|         *pInterface = static_cast<IUnknown*>(static_cast<ICorDebugModule*>(this));
   637|     }
   638|     else
   639|     {
   640|         *pInterface = NULL;
   641|         return E_NOINTERFACE;
   642|     }
   643|     ExternalAddRef();
   644|     return S_OK;
   645| }
   646| HRESULT CordbModule::GetProcess(ICorDebugProcess **ppProcess)
   647| {
   648|     PUBLIC_API_ENTRY(this);
   649|     FAIL_IF_NEUTERED(this);
   650|     VALIDATE_POINTER_TO_OBJECT(ppProcess, ICorDebugProcess **);
   651|     *ppProcess = static_cast<ICorDebugProcess*> (GetProcess());
   652|     GetProcess()->ExternalAddRef();
   653|     return S_OK;
   654| }
   655| HRESULT CordbModule::GetBaseAddress(CORDB_ADDRESS *pAddress)
   656| {
   657|     PUBLIC_API_ENTRY(this);
   658|     FAIL_IF_NEUTERED(this);
   659|     VALIDATE_POINTER_TO_OBJECT(pAddress, CORDB_ADDRESS *);
   660|     *pAddress = m_PEBuffer.pAddress;
   661|     return S_OK;
   662| }
   663| HRESULT CordbModule::GetAssembly(ICorDebugAssembly **ppAssembly)
   664| {
   665|     PUBLIC_API_ENTRY(this);
   666|     FAIL_IF_NEUTERED(this);
   667|     VALIDATE_POINTER_TO_OBJECT(ppAssembly, ICorDebugAssembly **);
   668|     *ppAssembly = static_cast<ICorDebugAssembly *> (m_pAssembly);
   669|     if (m_pAssembly != NULL)
   670|     {
   671|         m_pAssembly->ExternalAddRef();
   672|     }
   673|     return S_OK;
   674| }
   675| HRESULT CordbModule::GetName(ULONG32 cchName, ULONG32 *pcchName, _Out_writes_to_opt_(cchName, *pcchName) WCHAR szName[])
   676| {
   677|     HRESULT hr = S_OK;
   678|     PUBLIC_API_BEGIN(this)
   679|     {
   680|         EX_TRY
   681|         {
   682|             hr = GetNameWorker(cchName, pcchName, szName);
   683|         }
   684|         EX_CATCH_HRESULT(hr);
   685|         if ((hr == CORDBG_E_MISSING_METADATA) ||
   686|             (hr == CORDBG_E_READVIRTUAL_FAILURE) ||
   687|             (hr == HRESULT_FROM_WIN32(ERROR_PARTIAL_COPY)))
   688|         {
   689|             DWORD dwImageTimeStamp = 0; // unused
   690|             DWORD dwImageSize = 0;      // unused
   691|             bool isNGEN = false;
   692|             StringCopyHolder filePath;
   693|             _ASSERTE(!m_vmPEFile.IsNull());
   694|             if (this->GetProcess()->GetDAC()->GetMetaDataFileInfoFromPEFile(m_vmPEFile,
   695|                                                                              dwImageTimeStamp,
   696|                                                                              dwImageSize,
   697|                                                                              isNGEN,
   698|                                                                              &filePath))
   699|             {
   700|                 _ASSERTE(filePath.IsSet());
   701|                 if ((isNGEN) &&
   702|                     (this->GetProcess()->GetDAC()->GetILImageInfoFromNgenPEFile(m_vmPEFile,
   703|                                                                                 dwImageTimeStamp,
   704|                                                                                 dwImageSize,
   705|                                                                                 &filePath)))
   706|                 {
   707|                     _ASSERTE(filePath.IsSet());
   708|                 }
   709|                 hr = CopyOutString(filePath, cchName, pcchName, szName);
   710|             }
   711|         }
   712|     }
   713|     PUBLIC_API_END(hr);
   714|     return hr;
   715| }
   716| HRESULT CordbModule::GetNameWorker(ULONG32 cchName, ULONG32 *pcchName, _Out_writes_to_opt_(cchName, *pcchName) WCHAR szName[])
   717| {
   718|     CONTRACTL
   719|     {
   720|         THROWS;
   721|     }
   722|     CONTRACTL_END;
   723|     HRESULT hr = S_OK;
   724|     const WCHAR * szTempName = NULL;
   725|     ALLOW_DATATARGET_MISSING_MEMORY(
   726|         szTempName = GetModulePath();
   727|     );
   728| #if defined(FEATURE_DBGIPC_TRANSPORT_DI)
   729|     if (szTempName == NULL)
   730|     {
   731|         IMetaDataAssemblyImport *pAssemblyImport = NULL;
   732|         if (SUCCEEDED(hr = GetMetaDataImporter()->QueryInterface(IID_IMetaDataAssemblyImport, (void**)&pAssemblyImport)))
   733|         {
   734|             mdAssembly mda = TokenFromRid(1, mdtAssembly);
   735|             hr = pAssemblyImport->GetAssemblyProps(mda,          // [IN] The Assembly for which to get the properties.
   736|                                                    NULL,         // [OUT] Pointer to the Originator blob.
   737|                                                    NULL,         // [OUT] Count of bytes in the Originator Blob.
   738|                                                    NULL,         // [OUT] Hash Algorithm.
   739|                                                    szName,       // [OUT] Buffer to fill with name.
   740|                                                    cchName,      // [IN] Size of buffer in wide chars.
   741|                                                    (ULONG*)pcchName, // [OUT] Actual # of wide chars in name.
   742|                                                    NULL,         // [OUT] Assembly MetaData.
   743|                                                    NULL);        // [OUT] Flags.
   744|             pAssemblyImport->Release();
   745|             return hr;
   746|         }
   747|         hr = S_OK;
   748|     }
   749| #endif // FEATURE_DBGIPC_TRANSPORT_DI
   750|     EX_TRY_ALLOW_DATATARGET_MISSING_MEMORY
   751|     {
   752|         StringCopyHolder buffer;
   753|         if (!szTempName)
   754|         {
   755|             hr = HRESULT_FROM_WIN32(ERROR_PARTIAL_COPY);
   756|             m_pProcess->GetDAC()->GetModuleSimpleName(m_vmModule, &buffer);
   757|             _ASSERTE(buffer.IsSet());
   758|             szTempName = buffer;
   759|         }
   760|         hr = CopyOutString(szTempName, cchName, pcchName, szName);
   761|     }
   762|     EX_END_CATCH_ALLOW_DATATARGET_MISSING_MEMORY
   763|     return hr;
   764| }
   765| const WCHAR * CordbModule::GetModulePath()
   766| {
   767|     if (!m_strModulePath.IsSet())
   768|     {
   769|         IDacDbiInterface * pDac = m_pProcess->GetDAC(); // throws
   770|         pDac->GetModulePath(m_vmModule, &m_strModulePath); // throws
   771|         _ASSERTE(m_strModulePath.IsSet());
   772|     }
   773|     if (m_strModulePath.IsEmpty())
   774|     {
   775|         return NULL;    // module has no filename
   776|     }
   777|     return m_strModulePath;
   778| }
   779| const WCHAR * CordbModule::GetNGenImagePath()
   780| {
   781|     HRESULT hr = S_OK;
   782|     EX_TRY
   783|     {
   784|         if (!m_strNGenImagePath.IsSet())
   785|         {
   786|             IDacDbiInterface * pDac = m_pProcess->GetDAC(); // throws
   787|             BOOL fNonEmpty = pDac->GetModuleNGenPath(m_vmModule, &m_strNGenImagePath); // throws
   788|             (void)fNonEmpty; //prevent "unused variable" error from GCC
   789|             _ASSERTE(m_strNGenImagePath.IsSet() && (m_strNGenImagePath.IsEmpty() == !fNonEmpty));
   790|         }
   791|     }
   792|     EX_CATCH_HRESULT(hr);
   793|     if (FAILED(hr) ||
   794|         m_strNGenImagePath == NULL ||
   795|         m_strNGenImagePath.IsEmpty())
   796|     {
   797|         return NULL;    // module has no ngen filename
   798|     }
   799|     return m_strNGenImagePath;
   800| }
   801| HRESULT CordbModule::EnableJITDebugging(BOOL bTrackJITInfo, BOOL bAllowJitOpts)
   802| {
   803|     PUBLIC_API_ENTRY(this);
   804|     FAIL_IF_NEUTERED(this);
   805|     DWORD dwFlags = CORDEBUG_JIT_DEFAULT;
   806|     if (!bAllowJitOpts)
   807|     {
   808|         dwFlags |= CORDEBUG_JIT_DISABLE_OPTIMIZATION;
   809|     }
   810|     return SetJITCompilerFlags(dwFlags);
   811| }
   812| HRESULT CordbModule::EnableClassLoadCallbacks(BOOL bClassLoadCallbacks)
   813| {
   814|     PUBLIC_API_ENTRY(this);
   815|     FAIL_IF_NEUTERED(this);
   816|     ATT_ALLOW_LIVE_DO_STOPGO(GetProcess());
   817|     if (m_fDynamic && !bClassLoadCallbacks)
   818|         return E_INVALIDARG;
   819|     if (m_vmDomainAssembly.IsNull())
   820|         return E_UNEXPECTED;
   821|     CordbProcess *pProcess = GetProcess();
   822|     DebuggerIPCEvent event;
   823|     pProcess->InitIPCEvent(&event,
   824|                            DB_IPCE_SET_CLASS_LOAD_FLAG,
   825|                            false,
   826|                            (GetAppDomain()->GetADToken()));
   827|     event.SetClassLoad.vmDomainAssembly = this->m_vmDomainAssembly;
   828|     event.SetClassLoad.flag = (bClassLoadCallbacks == TRUE);
   829|     HRESULT hr = pProcess->m_cordb->SendIPCEvent(pProcess, &event,
   830|                                                  sizeof(DebuggerIPCEvent));
   831|     hr = WORST_HR(hr, event.hr);
   832|     return hr;
   833| }
   834| HRESULT CordbModule::GetFunctionFromToken(mdMethodDef token,
   835|                                           ICorDebugFunction **ppFunction)
   836| {
   837|     PUBLIC_API_ENTRY(this);
   838|     ATT_ALLOW_LIVE_DO_STOPGO(GetProcess()); // @todo - can this be RequiredStop?
   839|     FAIL_IF_NEUTERED(this);
   840|     VALIDATE_POINTER_TO_OBJECT(ppFunction, ICorDebugFunction **);
   841|     HRESULT hr = S_OK;
   842|     EX_TRY
   843|     {
   844|         RSLockHolder lockHolder(GetProcess()->GetProcessLock());
   845|         if ((token == mdMethodDefNil) ||
   846|             (TypeFromToken(token) != mdtMethodDef) ||
   847|             (!GetMetaDataImporter()->IsValidToken(token)))
   848|         {
   849|             ThrowHR(E_INVALIDARG);
   850|         }
   851|         CordbFunction * pFunction = LookupOrCreateFunctionLatestVersion(token);
   852|         *ppFunction = static_cast<ICorDebugFunction*> (pFunction);
   853|         pFunction->ExternalAddRef();
   854|     }
   855|     EX_CATCH_HRESULT(hr);
   856|     return hr;
   857| }
   858| HRESULT CordbModule::GetFunctionFromRVA(CORDB_ADDRESS rva,
   859|                                         ICorDebugFunction **ppFunction)
   860| {
   861|     PUBLIC_API_ENTRY(this);
   862|     FAIL_IF_NEUTERED(this);
   863|     VALIDATE_POINTER_TO_OBJECT(ppFunction, ICorDebugFunction **);
   864|     return E_NOTIMPL;
   865| }
   866| HRESULT CordbModule::LookupClassByToken(mdTypeDef token,
   867|                                         CordbClass **ppClass)
   868| {
   869|     INTERNAL_API_ENTRY(this->GetProcess()); //
   870|     FAIL_IF_NEUTERED(this);
   871|     HRESULT hr = S_OK;
   872|     EX_TRY // @dbgtodo  exceptions - push this up
   873|     {
   874|         *ppClass = NULL;
   875|         if ((token == mdTypeDefNil) || (TypeFromToken(token) != mdtTypeDef))
   876|         {
   877|             ThrowHR(E_INVALIDARG);
   878|         }
   879|         RSLockHolder lockHolder(GetProcess()->GetProcessLock()); // @dbgtodo  synchronization - Push this up
   880|         CordbClass *pClass = m_classes.GetBase(token);
   881|         if (pClass == NULL)
   882|         {
   883|             if (!GetMetaDataImporter()->IsValidToken(token))
   884|             {
   885|                 ThrowHR(E_INVALIDARG);
   886|             }
   887|             RSInitHolder<CordbClass> pClassInit(new CordbClass(this, token));
   888|             pClass = pClassInit.TransferOwnershipToHash(&m_classes);
   889|         }
   890|         *ppClass = pClass;
   891|     }
   892|     EX_CATCH_HRESULT(hr);
   893|     return hr;
   894| }
   895| HRESULT CordbModule::GetClassFromToken(mdTypeDef token,
   896|                                        ICorDebugClass **ppClass)
   897| {
   898|     PUBLIC_API_ENTRY(this);
   899|     FAIL_IF_NEUTERED(this);
   900|     ATT_ALLOW_LIVE_DO_STOPGO(this->GetProcess()); // @todo - could this be RequiredStopped?
   901|     VALIDATE_POINTER_TO_OBJECT(ppClass, ICorDebugClass **);
   902|     HRESULT hr = S_OK;
   903|     EX_TRY
   904|     {
   905|         CordbClass *pClass = NULL;
   906|         *ppClass = NULL;
   907|         if (!GetMetaDataImporter()->IsValidToken(token))
   908|         {
   909|             ThrowHR(E_INVALIDARG);
   910|         }
   911|         hr = LookupClassByToken(token, &pClass);
   912|         IfFailThrow(hr);
   913|         *ppClass = static_cast<ICorDebugClass*> (pClass);
   914|         pClass->ExternalAddRef();
   915|     }
   916|     EX_CATCH_HRESULT(hr);
   917|     return hr;
   918| }
   919| HRESULT CordbModule::CreateBreakpoint(ICorDebugModuleBreakpoint **ppBreakpoint)
   920| {
   921|     PUBLIC_API_ENTRY(this);
   922|     FAIL_IF_NEUTERED(this);
   923|     VALIDATE_POINTER_TO_OBJECT(ppBreakpoint, ICorDebugModuleBreakpoint **);
   924|     return E_NOTIMPL;
   925| }
   926| HRESULT CordbModule::GetToken(mdModule *pToken)
   927| {
   928|     PUBLIC_API_ENTRY(this);
   929|     FAIL_IF_NEUTERED(this);
   930|     VALIDATE_POINTER_TO_OBJECT(pToken, mdModule *);
   931|     HRESULT hr = S_OK;
   932|     EX_TRY
   933|     {
   934|         hr = GetMetaDataImporter()->GetModuleFromScope(pToken);
   935|         IfFailThrow(hr);
   936|     }
   937|     EX_CATCH_HRESULT(hr);
   938|     return hr;
   939| }
   940| HRESULT CordbModule::GetMetaDataInterface(REFIID riid, IUnknown **ppObj)
   941| {
   942|     PUBLIC_API_ENTRY(this);
   943|     FAIL_IF_NEUTERED(this);
   944|     VALIDATE_POINTER_TO_OBJECT(ppObj, IUnknown **);
   945|     HRESULT hr = S_OK;
   946|     EX_TRY
   947|     {
   948|         hr = GetMetaDataImporter()->QueryInterface(riid, (void**)ppObj);
   949|         IfFailThrow(hr);
   950|     }
   951|     EX_CATCH_HRESULT(hr);
   952|     return hr;
   953| }
   954| CordbFunction* CordbModule::LookupFunctionLatestVersion(mdMethodDef funcMetaDataToken)
   955| {
   956|     INTERNAL_API_ENTRY(this);
   957|     return m_functions.GetBase(funcMetaDataToken);
   958| }
   959| CordbFunction* CordbModule::LookupOrCreateFunctionLatestVersion(mdMethodDef funcMetaDataToken)
   960| {
   961|     INTERNAL_API_ENTRY(this);
   962|     CordbFunction * pFunction = m_functions.GetBase(funcMetaDataToken);
   963|     if (pFunction != NULL)
   964|     {
   965|         return pFunction;
   966|     }
   967|     return CreateFunction(funcMetaDataToken, CorDB_DEFAULT_ENC_FUNCTION_VERSION);
   968| }
   969| CordbFunction * CordbModule::LookupOrCreateFunction(mdMethodDef funcMetaDataToken, SIZE_T enCVersion)
   970| {
   971|     INTERNAL_API_ENTRY(this);
   972|     _ASSERTE(GetProcess()->ThreadHoldsProcessLock());
   973|     CordbFunction * pFunction = m_functions.GetBase(funcMetaDataToken);
   974|     if (pFunction == NULL)
   975|     {
   976|         return CreateFunction(funcMetaDataToken, enCVersion);
   977|     }
   978|     for (CordbFunction *pf=pFunction; pf != NULL; pf = pf->GetPrevVersion())
   979|     {
   980|         if (pf->GetEnCVersionNumber() == enCVersion)
   981|         {
   982|             return pf;
   983|         }
   984|     }
   985|     _ASSERTE(!"Couldn't find EnC version of function\n");
   986|     ThrowHR(E_FAIL);
   987| }
   988| HRESULT CordbModule::IsDynamic(BOOL *pDynamic)
   989| {
   990|     PUBLIC_API_ENTRY(this);
   991|     FAIL_IF_NEUTERED(this);
   992|     VALIDATE_POINTER_TO_OBJECT(pDynamic, BOOL *);
   993|     (*pDynamic) = m_fDynamic;
   994|     return S_OK;
   995| }
   996| BOOL CordbModule::IsDynamic()
   997| {
   998|     return m_fDynamic;
   999| }
  1000| HRESULT CordbModule::IsInMemory(BOOL *pInMemory)
  1001| {
  1002|     PUBLIC_API_ENTRY(this);
  1003|     FAIL_IF_NEUTERED(this);
  1004|     VALIDATE_POINTER_TO_OBJECT(pInMemory, BOOL *);
  1005|     (*pInMemory) = m_fInMemory;
  1006|     return S_OK;
  1007| }
  1008| HRESULT CordbModule::GetGlobalVariableValue(mdFieldDef fieldDef,
  1009|                                             ICorDebugValue **ppValue)
  1010| {
  1011|     PUBLIC_API_ENTRY(this);
  1012|     FAIL_IF_NEUTERED(this);
  1013|     VALIDATE_POINTER_TO_OBJECT(ppValue, ICorDebugValue **);
  1014|     ATT_REQUIRE_STOPPED_MAY_FAIL(this->GetProcess());
  1015|     HRESULT hr = S_OK;
  1016|     EX_TRY
  1017|     {
  1018|         if (m_pClass == NULL)
  1019|         {
  1020|             CordbClass * pGlobalClass = NULL;
  1021|             hr = LookupClassByToken(COR_GLOBAL_PARENT_TOKEN, &pGlobalClass);
  1022|             IfFailThrow(hr);
  1023|             m_pClass.Assign(pGlobalClass);
  1024|             _ASSERTE(m_pClass != NULL);
  1025|         }
  1026|         hr = m_pClass->GetStaticFieldValue(fieldDef, NULL, ppValue);
  1027|         IfFailThrow(hr);
  1028|     }
  1029|     EX_CATCH_HRESULT(hr);
  1030|     return hr;
  1031| }
  1032| CordbFunction * CordbModule::CreateFunction(mdMethodDef funcMetaDataToken, SIZE_T enCVersion)
  1033| {
  1034|     INTERNAL_API_ENTRY(this);
  1035|     RSInitHolder<CordbFunction> pFunction(new CordbFunction(this, funcMetaDataToken, enCVersion)); // throws
  1036|     CordbFunction * pCopy = pFunction.TransferOwnershipToHash(&m_functions);
  1037|     return pCopy;
  1038| }
  1039| #ifdef EnC_SUPPORTED
  1040| HRESULT CordbModule::UpdateFunction(mdMethodDef funcMetaDataToken,
  1041|                                     SIZE_T enCVersion,
  1042|                                     CordbFunction** ppFunction)
  1043| {
  1044|     INTERNAL_API_ENTRY(this);
  1045|     if (ppFunction)
  1046|         *ppFunction = NULL;
  1047|     _ASSERTE(funcMetaDataToken);
  1048|     RSLockHolder lockHolder(GetProcess()->GetProcessLock());
  1049|     CordbFunction* pOldVersion = LookupFunctionLatestVersion(funcMetaDataToken);
  1050|     if (!pOldVersion)
  1051|     {
  1052|         LOG((LF_ENC, LL_INFO10000, "CM::UF: adding %8.8x with version %d\n", funcMetaDataToken, enCVersion));
  1053|         HRESULT hr = S_OK;
  1054|         EX_TRY
  1055|         {
  1056|             pOldVersion = CreateFunction(funcMetaDataToken, CorDB_DEFAULT_ENC_FUNCTION_VERSION);
  1057|         }
  1058|         EX_CATCH_HRESULT(hr);
  1059|         if (FAILED(hr))
  1060|         {
  1061|             return hr;
  1062|         }
  1063|     }
  1064|     _ASSERTE( enCVersion > pOldVersion->GetEnCVersionNumber());
  1065|     LOG((LF_ENC, LL_INFO10000, "CM::UF: updating %8.8x with version %d\n", funcMetaDataToken, enCVersion));
  1066|     CordbFunction * pNewVersion = new (nothrow) CordbFunction(this, funcMetaDataToken, enCVersion);
  1067|     if (pNewVersion == NULL)
  1068|         return E_OUTOFMEMORY;
  1069|     pNewVersion->SetPrevVersion(pOldVersion);
  1070|     HRESULT hr = m_functions.SwapBase(pOldVersion, pNewVersion);
  1071|     if (FAILED(hr))
  1072|     {
  1073|         delete pNewVersion;
  1074|         return hr;
  1075|     }
  1076|     pNewVersion->GetPrevVersion()->MakeOld();
  1077|     if (ppFunction)
  1078|         *ppFunction = pNewVersion;
  1079|     return hr;
  1080| }
  1081| #endif // EnC_SUPPORTED
  1082| HRESULT CordbModule::LookupOrCreateClass(mdTypeDef classMetaDataToken,CordbClass** ppClass)
  1083| {
  1084|     INTERNAL_API_ENTRY(this);
  1085|     FAIL_IF_NEUTERED(this);
  1086|     RSLockHolder lockHolder(GetProcess()->GetProcessLock()); // @dbgtodo  exceptions synchronization-
  1087|     HRESULT hr = S_OK;
  1088|     *ppClass = LookupClass(classMetaDataToken);
  1089|     if (*ppClass == NULL)
  1090|     {
  1091|         hr = CreateClass(classMetaDataToken,ppClass);
  1092|         if (!SUCCEEDED(hr))
  1093|         {
  1094|             return hr;
  1095|         }
  1096|         _ASSERTE(*ppClass != NULL);
  1097|     }
  1098|     return hr;
  1099| }
  1100| CordbClass* CordbModule::LookupClass(mdTypeDef classMetaDataToken)
  1101| {
  1102|     INTERNAL_API_ENTRY(this);
  1103|     _ASSERTE(GetProcess()->ThreadHoldsProcessLock());
  1104|     return m_classes.GetBase(classMetaDataToken);
  1105| }
  1106| HRESULT CordbModule::CreateClass(mdTypeDef classMetaDataToken,
  1107|                                  CordbClass** ppClass)
  1108| {
  1109|     INTERNAL_API_ENTRY(this);
  1110|     FAIL_IF_NEUTERED(this);
  1111|     _ASSERTE(GetProcess()->ThreadHoldsProcessLock());
  1112|     CordbClass* pClass = new (nothrow) CordbClass(this, classMetaDataToken);
  1113|     if (pClass == NULL)
  1114|         return E_OUTOFMEMORY;
  1115|     HRESULT hr = m_classes.AddBase(pClass);
  1116|     if (SUCCEEDED(hr))
  1117|     {
  1118|         *ppClass = pClass;
  1119|         if (classMetaDataToken == COR_GLOBAL_PARENT_TOKEN)
  1120|         {
  1121|             _ASSERTE( m_pClass == NULL ); //redundant create
  1122|             m_pClass.Assign(pClass);
  1123|         }
  1124|     }
  1125|     else
  1126|     {
  1127|         delete pClass;
  1128|     }
  1129|     return hr;
  1130| }
  1131| HRESULT CordbModule::ResolveTypeRef(mdTypeRef token, CordbClass **ppClass)
  1132| {
  1133|     FAIL_IF_NEUTERED(this);
  1134|     INTERNAL_SYNC_API_ENTRY(GetProcess()); //
  1135|     CordbProcess * pProcess = GetProcess();
  1136|     _ASSERTE((pProcess->GetShim() == NULL) || pProcess->GetSynchronized());
  1137|     if ((token == mdTypeRefNil) || (TypeFromToken(token) != mdtTypeRef))
  1138|     {
  1139|         return E_INVALIDARG;
  1140|     }
  1141|     if (m_vmDomainAssembly.IsNull() || m_pAppDomain == NULL)
  1142|     {
  1143|         return E_UNEXPECTED;
  1144|     }
  1145|     HRESULT         hr = S_OK;
  1146|     *ppClass = NULL;
  1147|     EX_TRY
  1148|     {
  1149|         TypeRefData inData = {m_vmDomainAssembly, token};
  1150|         TypeRefData outData;
  1151|         {
  1152|             RSLockHolder lockHolder(pProcess->GetProcessLock());
  1153|             pProcess->GetDAC()->ResolveTypeReference(&inData, &outData);
  1154|         }
  1155|         CordbModule * pModule = m_pAppDomain->LookupOrCreateModule(outData.vmDomainAssembly);
  1156|         IfFailThrow(pModule->LookupClassByToken(outData.typeToken, ppClass));
  1157|     }
  1158|     EX_CATCH_HRESULT(hr);
  1159|     return hr;
  1160| } // CordbModule::ResolveTypeRef
  1161| HRESULT CordbModule::ResolveTypeRefOrDef(mdToken token, CordbClass **ppClass)
  1162| {
  1163|     FAIL_IF_NEUTERED(this);
  1164|     INTERNAL_SYNC_API_ENTRY(this->GetProcess()); //
  1165|     if ((token == mdTypeRefNil) ||
  1166|         (TypeFromToken(token) != mdtTypeRef && TypeFromToken(token) != mdtTypeDef))
  1167|         return E_INVALIDARG;
  1168|     if (TypeFromToken(token)==mdtTypeRef)
  1169|     {
  1170|         return ( ResolveTypeRef(token, ppClass) );
  1171|     }
  1172|     else
  1173|     {
  1174|         return ( LookupClassByToken(token, ppClass) );
  1175|     }
  1176| }
  1177| HRESULT CordbModule::GetSize(ULONG32 *pcBytes)
  1178| {
  1179|     PUBLIC_API_ENTRY(this);
  1180|     FAIL_IF_NEUTERED(this);
  1181|     VALIDATE_POINTER_TO_OBJECT(pcBytes, ULONG32 *);
  1182|     *pcBytes = m_PEBuffer.cbSize;
  1183|     return S_OK;
  1184| }
  1185| CordbAssembly *CordbModule::GetCordbAssembly()
  1186| {
  1187|     INTERNAL_API_ENTRY(this);
  1188|     return m_pAssembly;
  1189| }
  1190| HRESULT CordbModule::GetEditAndContinueSnapshot(
  1191|     ICorDebugEditAndContinueSnapshot **ppEditAndContinueSnapshot)
  1192| {
  1193|     return E_NOTIMPL;
  1194| }
  1195| HRESULT CordbModule::ApplyChanges(ULONG  cbMetaData,
  1196|                                   BYTE   pbMetaData[],
  1197|                                   ULONG  cbIL,
  1198|                                   BYTE   pbIL[])
  1199| {
  1200|     PUBLIC_API_ENTRY(this);
  1201|     FAIL_IF_NEUTERED(this);
  1202|     ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
  1203| #ifdef FEATURE_ENC_SUPPORTED
  1204|     LOG((LF_CORDB,LL_INFO10000, "CP::AC: applying changes"));
  1205|     VALIDATE_POINTER_TO_OBJECT_ARRAY(pbMetaData,
  1206|                                    BYTE,
  1207|                                    cbMetaData,
  1208|                                    true,
  1209|                                    true);
  1210|     VALIDATE_POINTER_TO_OBJECT_ARRAY(pbIL,
  1211|                                    BYTE,
  1212|                                    cbIL,
  1213|                                    true,
  1214|                                    true);
  1215|     HRESULT hr;
  1216|     RSExtSmartPtr<IUnknown> pUnk;
  1217|     RSExtSmartPtr<IMDInternalImport> pMDImport;
  1218|     RSExtSmartPtr<IMDInternalImport> pMDImport2;
  1219|     ++m_EnCCount;
  1220|     _ASSERTE(m_pIMImport != NULL); // must have metadata at this point in EnC
  1221|     IfFailGo(m_pIMImport->QueryInterface(IID_IUnknown, (void**)&pUnk));
  1222|     IfFailGo(GetMDInternalInterfaceFromPublic(pUnk, IID_IMDInternalImport,
  1223|                                                     (void **)&pMDImport));
  1224|     hr = pMDImport->ApplyEditAndContinue(pbMetaData, cbMetaData, &pMDImport2);
  1225|     if (pMDImport2 != NULL)
  1226|     {
  1227|         pMDImport2->AddRef();
  1228|     }
  1229|     IfFailGo(hr);
  1230|     m_pIMImport.Clear();
  1231|     IfFailGo(GetMDPublicInterfaceFromInternal(pMDImport2, IID_IMetaDataImport, (void **)&m_pIMImport));
  1232|     IfFailGo( ApplyChangesInternal(cbMetaData, pbMetaData, cbIL, pbIL) );
  1233|     EX_TRY
  1234|     {
  1235|         m_pInternalMetaDataImport.Clear();
  1236|         UpdateInternalMetaData();
  1237|     }
  1238|     EX_CATCH_HRESULT(hr);
  1239|     _ASSERTE(SUCCEEDED(hr));
  1240| ErrExit:
  1241|     return hr;
  1242| #else
  1243|     return E_NOTIMPL;
  1244| #endif
  1245| }
  1246| HRESULT CordbModule::ApplyChangesInternal(ULONG  cbMetaData,
  1247|                                           BYTE   pbMetaData[],
  1248|                                           ULONG  cbIL,
  1249|                                           BYTE   pbIL[])
  1250| {
  1251|     CONTRACTL
  1252|     {
  1253|         NOTHROW;
  1254|     }
  1255|     CONTRACTL_END;
  1256|     LOG((LF_ENC,LL_INFO100, "CordbProcess::ApplyChangesInternal\n"));
  1257|     FAIL_IF_NEUTERED(this);
  1258|     INTERNAL_SYNC_API_ENTRY(this->GetProcess()); //
  1259|     if (m_vmDomainAssembly.IsNull())
  1260|         return E_UNEXPECTED;
  1261| #ifdef FEATURE_ENC_SUPPORTED
  1262|     HRESULT hr;
  1263|     void * pRemoteBuf = NULL;
  1264|     EX_TRY
  1265|     {
  1266|         DebuggerIPCEvent event;
  1267|         GetProcess()->InitIPCEvent(&event, DB_IPCE_APPLY_CHANGES, false, VMPTR_AppDomain::NullPtr());
  1268|         event.ApplyChanges.vmDomainAssembly = this->m_vmDomainAssembly;
  1269|         ULONG cbSize = cbMetaData+cbIL;
  1270|         TargetBuffer tbFull = GetProcess()->GetRemoteBuffer(cbSize);
  1271|         pRemoteBuf = CORDB_ADDRESS_TO_PTR(tbFull.pAddress);
  1272|         TargetBuffer tbMetaData = tbFull.SubBuffer(0, cbMetaData); // 1st half
  1273|         TargetBuffer tbIL = tbFull.SubBuffer(cbMetaData); // 2nd half
  1274|         GetProcess()->SafeWriteBuffer(tbMetaData, pbMetaData); // throws
  1275|         GetProcess()->SafeWriteBuffer(tbIL, pbIL); // throws
  1276|         event.ApplyChanges.pDeltaMetadata = tbMetaData.pAddress;
  1277|         event.ApplyChanges.cbDeltaMetadata = tbMetaData.cbSize;
  1278|         event.ApplyChanges.pDeltaIL = tbIL.pAddress;
  1279|         event.ApplyChanges.cbDeltaIL = tbIL.cbSize;
  1280|         LOG((LF_ENC,LL_INFO100, "CordbProcess::ApplyChangesInternal sending event\n"));
  1281|         hr = GetProcess()->SendIPCEvent(&event, sizeof(event));
  1282|         hr = WORST_HR(hr, event.hr);
  1283|         IfFailThrow(hr);
  1284|         DebuggerIPCEvent *retEvent = (DebuggerIPCEvent *) _alloca(CorDBIPC_BUFFER_SIZE);
  1285|         {
  1286|             while (TRUE)
  1287|             {
  1288|                 hr = GetProcess()->m_cordb->WaitForIPCEventFromProcess(GetProcess(),
  1289|                                                                        GetAppDomain(),
  1290|                                                                        retEvent);
  1291|                 IfFailThrow(hr);
  1292|                 if (retEvent->type == DB_IPCE_APPLY_CHANGES_RESULT)
  1293|                 {
  1294|                     hr = retEvent->ApplyChangesResult.hr;
  1295|                     LOG((LF_CORDB, LL_INFO1000, "[%x] RCET::DRCE: EnC apply changes result %8.8x.\n", hr));
  1296|                     break;
  1297|                 }
  1298|                 _ASSERTE(retEvent->type == DB_IPCE_ENC_UPDATE_FUNCTION ||
  1299|                                   retEvent->type == DB_IPCE_ENC_ADD_FUNCTION ||
  1300|                                   retEvent->type == DB_IPCE_ENC_ADD_FIELD);
  1301|                 LOG((LF_CORDB, LL_INFO1000, "[%x] RCET::DRCE: EnC %s %8.8x to version %d.\n",
  1302|                         GetCurrentThreadId(),
  1303|                         retEvent->type == DB_IPCE_ENC_UPDATE_FUNCTION ? "Update function" :
  1304|                         retEvent->type == DB_IPCE_ENC_ADD_FUNCTION ? "Add function" : "Add field",
  1305|                         retEvent->EnCUpdate.memberMetadataToken, retEvent->EnCUpdate.newVersionNumber));
  1306|                 CordbAppDomain *pAppDomain = GetAppDomain();
  1307|                 _ASSERTE(NULL != pAppDomain);
  1308|                 CordbModule* pModule = NULL;
  1309|                 pModule = pAppDomain->LookupOrCreateModule(retEvent->EnCUpdate.vmDomainAssembly); // throws
  1310|                 _ASSERTE(pModule != NULL);
  1311|                 if (retEvent->type == DB_IPCE_ENC_UPDATE_FUNCTION ||
  1312|                      retEvent->type == DB_IPCE_ENC_ADD_FUNCTION)
  1313|                 {
  1314|                     hr = pModule->UpdateFunction(retEvent->EnCUpdate.memberMetadataToken, retEvent->EnCUpdate.newVersionNumber, NULL);
  1315|                 }
  1316|                 if (retEvent->type == DB_IPCE_ENC_ADD_FUNCTION ||
  1317|                      retEvent->type == DB_IPCE_ENC_ADD_FIELD)
  1318|                 {
  1319|                     RSLockHolder lockHolder(GetProcess()->GetProcessLock()); // @dbgtodo  synchronization -  push this up
  1320|                     CordbClass* pClass = pModule->LookupClass(retEvent->EnCUpdate.classMetadataToken);
  1321|                     if (pClass)
  1322|                     {
  1323|                         pClass->MakeOld();
  1324|                     }
  1325|                 }
  1326|             }
  1327|         }
  1328|         LOG((LF_ENC,LL_INFO100, "CordbProcess::ApplyChangesInternal complete.\n"));
  1329|     }
  1330|     EX_CATCH_HRESULT(hr);
  1331|     CordbProcess *pProcess = GetProcess();
  1332|     if (pProcess)
  1333|     {
  1334|         HRESULT hr2 = pProcess->ReleaseRemoteBuffer(&pRemoteBuf);
  1335|         TESTANDRETURNHR(hr2);
  1336|     }
  1337|     return hr;
  1338| #else // FEATURE_ENC_SUPPORTED
  1339|     return E_NOTIMPL;
  1340| #endif // FEATURE_ENC_SUPPORTED
  1341| }
  1342| HRESULT CordbModule::SetJMCStatus(
  1343|         BOOL fIsUserCode,
  1344|         ULONG32 cOthers,
  1345|         mdToken others[])
  1346| {
  1347|     PUBLIC_API_ENTRY(this);
  1348|     FAIL_IF_NEUTERED(this);
  1349|     ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
  1350|     if (m_vmDomainAssembly.IsNull())
  1351|         return E_UNEXPECTED;
  1352|     if (cOthers != 0)
  1353|     {
  1354|         _ASSERTE(!"not yet impl for cOthers != 0");
  1355|         return E_NOTIMPL;
  1356|     }
  1357|     CordbProcess* pProcess = this->GetProcess();
  1358|     _ASSERTE(pProcess != NULL);
  1359|     DebuggerIPCEvent event;
  1360|     pProcess->InitIPCEvent(&event, DB_IPCE_SET_MODULE_JMC_STATUS, true, this->GetAppDomain()->GetADToken());
  1361|     event.SetJMCFunctionStatus.vmDomainAssembly = m_vmDomainAssembly;
  1362|     event.SetJMCFunctionStatus.dwStatus = fIsUserCode;
  1363|     HRESULT hr = pProcess->m_cordb->SendIPCEvent(pProcess, &event, sizeof(DebuggerIPCEvent));
  1364|     if (!SUCCEEDED(hr))
  1365|     {
  1366|         LOG((LF_CORDB, LL_INFO10, "CordbModule::SetJMCStatus failed  0x%08x...\n", hr));
  1367|         return hr;
  1368|     }
  1369|     _ASSERTE(event.type == DB_IPCE_SET_MODULE_JMC_STATUS_RESULT);
  1370|     LOG((LF_CORDB, LL_INFO10, "returning from CordbModule::SetJMCStatus 0x%08x...\n", hr));
  1371|     return event.hr;
  1372| }
  1373| HRESULT CordbModule::ResolveAssembly(mdToken tkAssemblyRef,
  1374|                                     ICorDebugAssembly **ppAssembly)
  1375| {
  1376|     PUBLIC_API_ENTRY(this);
  1377|     FAIL_IF_NEUTERED(this);
  1378|     ATT_REQUIRE_STOPPED_MAY_FAIL(this->GetProcess());
  1379|     if(ppAssembly)
  1380|     {
  1381|         *ppAssembly = NULL;
  1382|     }
  1383|     HRESULT hr = S_OK;
  1384|     EX_TRY
  1385|     {
  1386|         CordbAssembly *pCordbAsm = ResolveAssemblyInternal(tkAssemblyRef);
  1387|         if (pCordbAsm == NULL)
  1388|         {
  1389|             hr = CORDBG_E_CANNOT_RESOLVE_ASSEMBLY;
  1390|         }
  1391|         else if(ppAssembly)
  1392|         {
  1393|             _ASSERTE(pCordbAsm != NULL);
  1394|             *ppAssembly = pCordbAsm;
  1395|             pCordbAsm->ExternalAddRef();
  1396|         }
  1397|     }
  1398|     EX_CATCH_HRESULT(hr);
  1399|     return hr;
  1400| }
  1401| CordbAssembly * CordbModule::ResolveAssemblyInternal(mdToken tkAssemblyRef)
  1402| {
  1403|     INTERNAL_SYNC_API_ENTRY(GetProcess()); //
  1404|     if (TypeFromToken(tkAssemblyRef) != mdtAssemblyRef || tkAssemblyRef == mdAssemblyRefNil)
  1405|     {
  1406|         ThrowHR(E_INVALIDARG);
  1407|     }
  1408|     CordbAssembly *    pAssembly = NULL;
  1409|     if (!m_vmDomainAssembly.IsNull())
  1410|     {
  1411|         VMPTR_DomainAssembly vmDomainAssembly = GetProcess()->GetDAC()->ResolveAssembly(m_vmDomainAssembly, tkAssemblyRef);
  1412|         if (!vmDomainAssembly.IsNull() && m_pAppDomain != NULL)
  1413|         {
  1414|             RSLockHolder lockHolder(GetProcess()->GetProcessLock());
  1415|             pAssembly = m_pAppDomain->LookupOrCreateAssembly(vmDomainAssembly);
  1416|         }
  1417|     }
  1418|     return pAssembly;
  1419| }
  1420| HRESULT CordbModule::CreateReaderForInMemorySymbols(REFIID riid, void** ppObj)
  1421| {
  1422|     PUBLIC_API_ENTRY(this);
  1423|     FAIL_IF_NEUTERED(this);
  1424|     CordbProcess *pProcess = GetProcess();
  1425|     ATT_REQUIRE_STOPPED_MAY_FAIL(pProcess);
  1426|     HRESULT hr = S_OK;
  1427|     EX_TRY
  1428|     {
  1429|         ReleaseHolder<IStream> pStream;
  1430|         IDacDbiInterface::SymbolFormat symFormat = GetInMemorySymbolStream(&pStream);
  1431|         ReleaseHolder<ISymUnmanagedBinder> pBinder;
  1432|         if (symFormat == IDacDbiInterface::kSymbolFormatPDB)
  1433|         {
  1434| #ifndef TARGET_UNIX
  1435|             InlineSString<_MAX_PATH> ssBuf;
  1436|             IfFailThrow(GetClrModuleDirectory(ssBuf));
  1437|             IfFailThrow(FakeCoCreateInstanceEx(CLSID_CorSymBinder_SxS,
  1438|                                                ssBuf.GetUnicode(),
  1439|                                                IID_ISymUnmanagedBinder,
  1440|                                                (void**)&pBinder,
  1441|                                                NULL));
  1442| #else
  1443|             IfFailThrow(FakeCoCreateInstance(CLSID_CorSymBinder_SxS,
  1444|                                              IID_ISymUnmanagedBinder,
  1445|                                              (void**)&pBinder));
  1446| #endif
  1447|         }
  1448|         else
  1449|         {
  1450|             _ASSERTE(symFormat == IDacDbiInterface::kSymbolFormatNone);
  1451|             if (m_fDynamic || m_fInMemory)
  1452|             {
  1453|                 ThrowHR(CORDBG_E_SYMBOLS_NOT_AVAILABLE);
  1454|             }
  1455|             ThrowHR(CORDBG_E_MODULE_LOADED_FROM_DISK);
  1456|         }
  1457|         if (m_pIMImport == NULL)
  1458|         {
  1459|             ThrowHR(CORDBG_E_SYMBOLS_NOT_AVAILABLE);
  1460|         }
  1461|         ReleaseHolder<ISymUnmanagedReader> pReader;
  1462|         IfFailThrow(pBinder->GetReaderFromStream(m_pIMImport, pStream, &pReader));
  1463|         IfFailThrow(pReader->QueryInterface(riid, ppObj));
  1464|     }
  1465|     EX_CATCH_HRESULT(hr);
  1466|     return hr;
  1467| }
  1468| /* ------------------------------------------------------------------------- *
  1469|  * Class class
  1470|  * ------------------------------------------------------------------------- */
  1471| void CordbModule::SetLoadEventContinueMarker()
  1472| {
  1473|     GetProcess()->TargetConsistencyCheck(m_nLoadEventContinueCounter == 0);
  1474|     m_nLoadEventContinueCounter = GetProcess()->m_continueCounter;
  1475| }
  1476| HRESULT CordbModule::EnsureModuleIsInLoadCallback()
  1477| {
  1478|     if (this->m_nLoadEventContinueCounter < GetProcess()->m_continueCounter)
  1479|     {
  1480|         return CORDBG_E_MUST_BE_IN_LOAD_MODULE;
  1481|     }
  1482|     else
  1483|     {
  1484|         return S_OK;
  1485|     }
  1486| }
  1487| HRESULT CordbModule::SetJITCompilerFlags(DWORD dwFlags)
  1488| {
  1489|     PUBLIC_REENTRANT_API_ENTRY(this);
  1490|     FAIL_IF_NEUTERED(this);
  1491|     CordbProcess *pProcess = GetProcess();
  1492|     ATT_REQUIRE_STOPPED_MAY_FAIL(pProcess);
  1493|     HRESULT hr = S_OK;
  1494|     EX_TRY
  1495|     {
  1496|         if ((dwFlags != CORDEBUG_JIT_DEFAULT) &&
  1497|             (dwFlags != CORDEBUG_JIT_DISABLE_OPTIMIZATION) &&
  1498|             (dwFlags != CORDEBUG_JIT_ENABLE_ENC))
  1499|         {
  1500|             hr = E_INVALIDARG;
  1501|         }
  1502|         else
  1503|         {
  1504|             BOOL fAllowJitOpts = ((dwFlags & CORDEBUG_JIT_DISABLE_OPTIMIZATION) != CORDEBUG_JIT_DISABLE_OPTIMIZATION);
  1505|             BOOL fEnableEnC = ((dwFlags & CORDEBUG_JIT_ENABLE_ENC) == CORDEBUG_JIT_ENABLE_ENC);
  1506|             hr = EnsureModuleIsInLoadCallback();
  1507|             if (SUCCEEDED(hr))
  1508|             {
  1509|                 hr = pProcess->GetDAC()->SetCompilerFlags(GetRuntimeDomainAssembly(), fAllowJitOpts, fEnableEnC);
  1510|             }
  1511|         }
  1512|     }
  1513|     EX_CATCH_HRESULT(hr);
  1514|     if (GetProcess()->GetShim() != NULL)
  1515|     {
  1516|         hr = GetProcess()->GetShim()->FilterSetJitFlagsHresult(hr);
  1517|     }
  1518|     return hr;
  1519| }
  1520| HRESULT CordbModule::GetJITCompilerFlags(DWORD *pdwFlags )
  1521| {
  1522|     PUBLIC_REENTRANT_API_ENTRY(this);
  1523|     FAIL_IF_NEUTERED(this);
  1524|     VALIDATE_POINTER_TO_OBJECT(pdwFlags, DWORD*);
  1525|     *pdwFlags = CORDEBUG_JIT_DEFAULT;;
  1526|     CordbProcess *pProcess = GetProcess();
  1527|     ATT_REQUIRE_STOPPED_MAY_FAIL(pProcess);
  1528|     HRESULT hr = S_OK;
  1529|     EX_TRY
  1530|     {
  1531|         BOOL fAllowJitOpts;
  1532|         BOOL fEnableEnC;
  1533|         pProcess->GetDAC()->GetCompilerFlags (
  1534|             GetRuntimeDomainAssembly(),
  1535|             &fAllowJitOpts,
  1536|             &fEnableEnC);
  1537|         if (fEnableEnC)
  1538|         {
  1539|             *pdwFlags = CORDEBUG_JIT_ENABLE_ENC;
  1540|         }
  1541|         else if (! fAllowJitOpts)
  1542|         {
  1543|             *pdwFlags = CORDEBUG_JIT_DISABLE_OPTIMIZATION;
  1544|         }
  1545|     }
  1546|     EX_CATCH_HRESULT(hr);
  1547|     return hr;
  1548| }
  1549| HRESULT CordbModule::IsMappedLayout(BOOL *isMapped)
  1550| {
  1551|     PUBLIC_API_ENTRY(this);
  1552|     VALIDATE_POINTER_TO_OBJECT(isMapped, BOOL*);
  1553|     FAIL_IF_NEUTERED(this);
  1554|     HRESULT hr = S_OK;
  1555|     *isMapped = FALSE;
  1556|     CordbProcess *pProcess = GetProcess();
  1557|     ATT_REQUIRE_STOPPED_MAY_FAIL(pProcess);
  1558|     EX_TRY
  1559|     {
  1560|         hr = pProcess->GetDAC()->IsModuleMapped(m_vmModule, isMapped);
  1561|     }
  1562|     EX_CATCH_HRESULT(hr);
  1563|     return hr;
  1564| }
  1565| /* ------------------------------------------------------------------------- *
  1566|  * CordbCode class
  1567|  * ------------------------------------------------------------------------- */
  1568| CordbCode::CordbCode(CordbFunction * pFunction, UINT_PTR id, SIZE_T encVersion, BOOL fIsIL)
  1569|   : CordbBase(pFunction->GetProcess(), id, enumCordbCode),
  1570|     m_fIsIL(fIsIL),
  1571|     m_nVersion(encVersion),
  1572|     m_rgbCode(NULL),
  1573|     m_continueCounterLastSync(0),
  1574|     m_pFunction(pFunction)
  1575| {
  1576|     _ASSERTE(pFunction != NULL);
  1577|     _ASSERTE(m_nVersion >= CorDB_DEFAULT_ENC_FUNCTION_VERSION);
  1578| } // CordbCode::CordbCode
  1579| CordbCode::~CordbCode()
  1580| {
  1581|     _ASSERTE(IsNeutered());
  1582| }
  1583| void CordbCode::Neuter()
  1584| {
  1585|     m_pFunction = NULL;
  1586|     delete [] m_rgbCode;
  1587|     m_rgbCode = NULL;
  1588|     CordbBase::Neuter();
  1589| }
  1590| HRESULT CordbCode::QueryInterface(REFIID id, void ** pInterface)
  1591| {
  1592|     if (id == IID_ICorDebugCode)
  1593|     {
  1594|         *pInterface = static_cast<ICorDebugCode*>(this);
  1595|     }
  1596|     else if (id == IID_IUnknown)
  1597|     {
  1598|         *pInterface = static_cast<IUnknown *>(static_cast<ICorDebugCode *>(this));
  1599|     }
  1600|     else
  1601|     {
  1602|         *pInterface = NULL;
  1603|         return E_NOINTERFACE;
  1604|     }
  1605|     ExternalAddRef();
  1606|     return S_OK;
  1607| }
  1608| HRESULT CordbCode::GetEnCRemapSequencePoints(ULONG32 cMap, ULONG32 * pcMap, ULONG32 offsets[])
  1609| {
  1610|     FAIL_IF_NEUTERED(this);
  1611|     VALIDATE_POINTER_TO_OBJECT_OR_NULL(pcMap, ULONG32*);
  1612|     VALIDATE_POINTER_TO_OBJECT_ARRAY_OR_NULL(offsets, ULONG32*, cMap, true, true);
  1613|     return E_NOTIMPL;
  1614| } // CordbCode::GetEnCRemapSequencePoints
  1615| HRESULT CordbCode::IsIL(BOOL *pbIL)
  1616| {
  1617|     PUBLIC_API_ENTRY(this);
  1618|     FAIL_IF_NEUTERED(this);
  1619|     VALIDATE_POINTER_TO_OBJECT(pbIL, BOOL *);
  1620|     *pbIL = IsIL();
  1621|     return S_OK;
  1622| }
  1623| HRESULT CordbCode::GetFunction(ICorDebugFunction **ppFunction)
  1624| {
  1625|     PUBLIC_API_ENTRY(this);
  1626|     FAIL_IF_NEUTERED(this);
  1627|     VALIDATE_POINTER_TO_OBJECT(ppFunction, ICorDebugFunction **);
  1628|     *ppFunction = static_cast<ICorDebugFunction*> (m_pFunction);
  1629|     m_pFunction->ExternalAddRef();
  1630|     return S_OK;
  1631| }
  1632| HRESULT CordbCode::GetSize(ULONG32 *pcBytes)
  1633| {
  1634|     PUBLIC_REENTRANT_API_ENTRY(this);
  1635|     FAIL_IF_NEUTERED(this);
  1636|     VALIDATE_POINTER_TO_OBJECT(pcBytes, ULONG32 *);
  1637|     *pcBytes = GetSize();
  1638|     return S_OK;
  1639| }
  1640| HRESULT CordbCode::CreateBreakpoint(ULONG32 offset,
  1641|                                     ICorDebugFunctionBreakpoint **ppBreakpoint)
  1642| {
  1643|     PUBLIC_REENTRANT_API_ENTRY(this);
  1644|     FAIL_IF_NEUTERED(this);
  1645|     VALIDATE_POINTER_TO_OBJECT(ppBreakpoint, ICorDebugFunctionBreakpoint **);
  1646|     HRESULT hr;
  1647|     ULONG32 size = GetSize();
  1648|     BOOL offsetIsIl = IsIL();
  1649|     LOG((LF_CORDB, LL_INFO10000, "CCode::CreateBreakpoint, offset=%d, size=%d, IsIl=%d, this=0x%p\n",
  1650|         offset, size, offsetIsIl, this));
  1651|     if (offset >= size)
  1652|     {
  1653|         return CORDBG_E_UNABLE_TO_SET_BREAKPOINT;
  1654|     }
  1655|     CordbFunctionBreakpoint *bp = new (nothrow) CordbFunctionBreakpoint(this, offset, offsetIsIl);
  1656|     if (bp == NULL)
  1657|         return E_OUTOFMEMORY;
  1658|     hr = bp->Activate(TRUE);
  1659|     if (SUCCEEDED(hr))
  1660|     {
  1661|         *ppBreakpoint = static_cast<ICorDebugFunctionBreakpoint*> (bp);
  1662|         bp->ExternalAddRef();
  1663|         return S_OK;
  1664|     }
  1665|     else
  1666|     {
  1667|         delete bp;
  1668|         return hr;
  1669|     }
  1670| }
  1671| HRESULT CordbCode::GetCode(ULONG32 startOffset,
  1672|                            ULONG32 endOffset,
  1673|                            ULONG32 cBufferAlloc,
  1674|                            BYTE buffer[],
  1675|                            ULONG32 *pcBufferSize)
  1676| {
  1677|     PUBLIC_REENTRANT_API_ENTRY(this);
  1678|     FAIL_IF_NEUTERED(this);
  1679|     VALIDATE_POINTER_TO_OBJECT_ARRAY(buffer, BYTE, cBufferAlloc, true, true);
  1680|     VALIDATE_POINTER_TO_OBJECT(pcBufferSize, ULONG32 *);
  1681|     LOG((LF_CORDB,LL_EVERYTHING, "CC::GC: for token:0x%x\n", m_pFunction->GetMetadataToken()));
  1682|     ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
  1683|     HRESULT hr = S_OK;
  1684|     *pcBufferSize = 0;
  1685|     ULONG32 totalSize = GetSize();
  1686|     if (cBufferAlloc < endOffset - startOffset)
  1687|         endOffset = startOffset + cBufferAlloc;
  1688|     if (endOffset > totalSize)
  1689|         endOffset = totalSize;
  1690|     if (startOffset > totalSize)
  1691|         startOffset = totalSize;
  1692|     if ((m_rgbCode == NULL) ||
  1693|         (m_continueCounterLastSync < GetProcess()->m_continueCounter))
  1694|     {
  1695|         ReadCodeBytes();
  1696|         m_continueCounterLastSync = GetProcess()->m_continueCounter;
  1697|     }
  1698|     if (*pcBufferSize == 0 && m_rgbCode != NULL)
  1699|     {
  1700|         memcpy(buffer,
  1701|                m_rgbCode+startOffset,
  1702|                endOffset - startOffset);
  1703|         *pcBufferSize = endOffset - startOffset;
  1704|     }
  1705|     return hr;
  1706| } // CordbCode::GetCode
  1707| #include "dbgipcevents.h"
  1708| HRESULT CordbCode::GetVersionNumber( ULONG32 *nVersion)
  1709| {
  1710|     PUBLIC_API_ENTRY(this);
  1711|     FAIL_IF_NEUTERED(this);
  1712|     VALIDATE_POINTER_TO_OBJECT(nVersion, ULONG32 *);
  1713|     LOG((LF_CORDB,LL_INFO10000,"R:CC:GVN:Returning 0x%x "
  1714|         "as version\n",m_nVersion));
  1715|     *nVersion = (ULONG32)m_nVersion;
  1716| #ifndef EnC_SUPPORTED
  1717|     _ASSERTE(*nVersion == 1);
  1718| #endif // EnC_SUPPORTED
  1719|     return S_OK;
  1720| }
  1721| CordbFunction * CordbCode::GetFunction()
  1722| {
  1723|     _ASSERTE(m_pFunction != NULL);
  1724|     return m_pFunction;
  1725| }
  1726| /* ------------------------------------------------------------------------- *
  1727|  * CordbILCode class
  1728|  * ------------------------------------------------------------------------- */
  1729| CordbILCode::CordbILCode(CordbFunction * pFunction,
  1730|                          TargetBuffer    codeRegionInfo,
  1731|                          SIZE_T          nVersion,
  1732|                          mdSignature     localVarSigToken,
  1733|                          UINT_PTR        id)
  1734|   : CordbCode(pFunction, id, nVersion, TRUE),
  1735| #ifdef EnC_SUPPORTED
  1736|     m_fIsOld(FALSE),
  1737| #endif
  1738|     m_codeRegionInfo(codeRegionInfo),
  1739|     m_localVarSigToken(localVarSigToken)
  1740| {
  1741| } // CordbILCode::CordbILCode
  1742| #ifdef EnC_SUPPORTED
  1743| void CordbILCode::MakeOld()
  1744| {
  1745|     m_fIsOld = TRUE;
  1746| }
  1747| #endif
  1748| HRESULT CordbILCode::GetAddress(CORDB_ADDRESS * pStart)
  1749| {
  1750|     PUBLIC_REENTRANT_API_ENTRY(this);
  1751|     FAIL_IF_NEUTERED(this);
  1752|     VALIDATE_POINTER_TO_OBJECT(pStart, CORDB_ADDRESS *);
  1753|     _ASSERTE(this != NULL);
  1754|     _ASSERTE(this->GetFunction() != NULL);
  1755|     _ASSERTE(this->GetFunction()->GetModule() != NULL);
  1756|     _ASSERTE(this->GetFunction()->GetModule()->GetProcess() == GetProcess());
  1757|     *pStart = (m_codeRegionInfo.pAddress);
  1758|     return S_OK;
  1759| } // CordbILCode::GetAddress
  1760| HRESULT CordbILCode::ReadCodeBytes()
  1761| {
  1762|     HRESULT hr = S_OK;
  1763|     EX_TRY
  1764|     {
  1765|         CORDB_ADDRESS pStart = m_codeRegionInfo.pAddress;
  1766|         ULONG32 cbSize = (ULONG32) m_codeRegionInfo.cbSize;
  1767|         delete [] m_rgbCode;
  1768|         m_rgbCode = new BYTE[cbSize];    // throws
  1769|         SIZE_T cbRead;
  1770|         hr = GetProcess()->ReadMemory(pStart, cbSize, m_rgbCode, &cbRead);
  1771|         IfFailThrow(hr);
  1772|         SIMPLIFYING_ASSUMPTION(cbRead == cbSize);
  1773|     }
  1774|     EX_CATCH_HRESULT(hr);
  1775|     return hr;
  1776| } // CordbILCode::ReadCodeBytes
  1777| HRESULT CordbILCode::GetILToNativeMapping(ULONG32                    cMap,
  1778|                                           ULONG32 *                  pcMap,
  1779|                                           COR_DEBUG_IL_TO_NATIVE_MAP map[])
  1780| {
  1781|     PUBLIC_API_ENTRY(this);
  1782|     FAIL_IF_NEUTERED(this);
  1783|     VALIDATE_POINTER_TO_OBJECT_OR_NULL(pcMap, ULONG32 *);
  1784|     VALIDATE_POINTER_TO_OBJECT_ARRAY_OR_NULL(map, COR_DEBUG_IL_TO_NATIVE_MAP *, cMap, true, true);
  1785|     ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
  1786|     return CORDBG_E_NON_NATIVE_FRAME;
  1787| } // CordbILCode::GetILToNativeMapping
  1788| /*
  1789| * CordbILCode::GetLocalVarSig
  1790| *
  1791| * Get the method's local variable metadata signature. This may be cached, but for dynamic modules we'll always
  1792| * read it from the metadata. This function also returns the count of local variables in the method.
  1793| *
  1794| * Parameters:
  1795| *    pLocalSigParser - OUT: the local variable signature for the method.
  1796| *    pLocalCount - OUT: the number of locals the method has.
  1797| *
  1798| * Returns:
  1799| *    HRESULT for success or failure.
  1800| *
  1801| */
  1802| HRESULT CordbILCode::GetLocalVarSig(SigParser *pLocalSigParser,
  1803|     ULONG *pLocalVarCount)
  1804| {
  1805|     INTERNAL_SYNC_API_ENTRY(GetProcess());
  1806|     CONTRACTL  // @dbgtodo  exceptions - convert to throws...
  1807|     {
  1808|         NOTHROW;
  1809|     }
  1810|     CONTRACTL_END;
  1811|     FAIL_IF_NEUTERED(this);
  1812|     HRESULT hr = S_OK;
  1813|     if (m_localVarSigToken != mdSignatureNil)
  1814|     {
  1815|         PCCOR_SIGNATURE localSignature = NULL;
  1816|         ULONG size = 0;
  1817|         EX_TRY // // @dbgtodo  exceptions  - push this up
  1818|         {
  1819|             GetFunction()->GetModule()->UpdateMetaDataCacheIfNeeded(m_localVarSigToken);
  1820|             hr = GetFunction()->GetModule()->GetMetaDataImporter()->GetSigFromToken(m_localVarSigToken,
  1821|                 &localSignature,
  1822|                 &size);
  1823|         }
  1824|         EX_CATCH_HRESULT(hr);
  1825|         if (FAILED(hr))
  1826|         {
  1827|             LOG((LF_CORDB, LL_WARNING, "CICF::GLVS caught hr=0x%x\n", hr));
  1828|         }
  1829|         IfFailRet(hr);
  1830|         LOG((LF_CORDB, LL_INFO100000, "CIC::GLVS creating sig parser sig=0x%x size=0x%x\n", localSignature, size));
  1831|         SigParser sigParser = SigParser(localSignature, size);
  1832|         uint32_t data;
  1833|         IfFailRet(sigParser.GetCallingConvInfo(&data));
  1834|         _ASSERTE(data == IMAGE_CEE_CS_CALLCONV_LOCAL_SIG);
  1835|         uint32_t localCount;
  1836|         IfFailRet(sigParser.GetData(&localCount));
  1837|         LOG((LF_CORDB, LL_INFO100000, "CIC::GLVS localCount=0x%x\n", localCount));
  1838|         if (pLocalSigParser != NULL)
  1839|         {
  1840|             *pLocalSigParser = sigParser;
  1841|         }
  1842|         if (pLocalVarCount != NULL)
  1843|         {
  1844|             *pLocalVarCount = localCount;
  1845|         }
  1846|     }
  1847|     else
  1848|     {
  1849|         if (pLocalSigParser != NULL)
  1850|         {
  1851|             *pLocalSigParser = SigParser(NULL, 0);
  1852|         }
  1853|         if (pLocalVarCount != NULL)
  1854|         {
  1855|             *pLocalVarCount = 0;
  1856|         }
  1857|     }
  1858|     LOG((LF_CORDB, LL_INFO100000, "CIC::GLVS returning hr=0x%x\n", hr));
  1859|     return hr;
  1860| }
  1861| HRESULT CordbILCode::GetLocalVariableType(DWORD dwIndex,
  1862|     const Instantiation * pInst,
  1863|     CordbType ** ppResultType)
  1864| {
  1865|     ATT_ALLOW_LIVE_DO_STOPGO(GetProcess());
  1866|     LOG((LF_CORDB, LL_INFO10000, "CIC::GLVT dwIndex=0x%x pInst=0x%p\n", dwIndex, pInst));
  1867|     HRESULT hr = S_OK;
  1868|     EX_TRY
  1869|     {
  1870|         SigParser sigParser;
  1871|         ULONG cLocals;
  1872|         IfFailThrow(GetLocalVarSig(&sigParser, &cLocals));
  1873|         if (dwIndex >= cLocals)
  1874|         {
  1875|             ThrowHR(E_INVALIDARG);
  1876|         }
  1877|         for (unsigned int i = 0; i < dwIndex; i++)
  1878|         {
  1879|             LOG((LF_CORDB, LL_INFO10000, "CIC::GLVT scanning index 0x%x\n", dwIndex));
  1880|             IfFailThrow(sigParser.SkipExactlyOne());
  1881|         }
  1882|         hr = CordbType::SigToType(GetFunction()->GetModule(), &sigParser, pInst, ppResultType);
  1883|         LOG((LF_CORDB, LL_INFO10000, "CIC::GLVT CT::SigToType returned hr=0x%x\n", hr));
  1884|         IfFailThrow(hr);
  1885|     } EX_CATCH_HRESULT(hr);
  1886|     return hr;
  1887| }
  1888| mdSignature CordbILCode::GetLocalVarSigToken()
  1889| {
  1890|     return m_localVarSigToken;
  1891| }
  1892| HRESULT CordbILCode::CreateNativeBreakpoint(ICorDebugFunctionBreakpoint **ppBreakpoint)
  1893| {
  1894|     FAIL_IF_NEUTERED(this);
  1895|     VALIDATE_POINTER_TO_OBJECT(ppBreakpoint, ICorDebugFunctionBreakpoint **);
  1896|     HRESULT hr;
  1897|     ULONG32 size = GetSize();
  1898|     LOG((LF_CORDB, LL_INFO10000, "CordbILCode::CreateNativeBreakpoint, size=%d, this=0x%p\n",
  1899|         size, this));
  1900|     ULONG32 offset = 0;
  1901|     CordbFunctionBreakpoint *bp = new (nothrow) CordbFunctionBreakpoint(this, offset, FALSE);
  1902|     if (bp == NULL)
  1903|     {
  1904|         return E_OUTOFMEMORY;
  1905|     }
  1906|     hr = bp->Activate(TRUE);
  1907|     if (SUCCEEDED(hr))
  1908|     {
  1909|         *ppBreakpoint = static_cast<ICorDebugFunctionBreakpoint*> (bp);
  1910|         bp->ExternalAddRef();
  1911|         return S_OK;
  1912|     }
  1913|     else
  1914|     {
  1915|         delete bp;
  1916|         return hr;
  1917|     }
  1918| }
  1919| CordbReJitILCode::CordbReJitILCode(CordbFunction *pFunction, SIZE_T encVersion, VMPTR_ILCodeVersionNode vmILCodeVersionNode) :
  1920| CordbILCode(pFunction, TargetBuffer(), encVersion, mdSignatureNil, VmPtrToCookie(vmILCodeVersionNode)),
  1921| m_cClauses(0),
  1922| m_cbLocalIL(0),
  1923| m_cILMap(0)
  1924| {
  1925|     _ASSERTE(!vmILCodeVersionNode.IsNull());
  1926|     DacSharedReJitInfo data = { 0 };
  1927|     IfFailThrow(GetProcess()->GetDAC()->GetILCodeVersionNodeData(vmILCodeVersionNode, &data));
  1928|     IfFailThrow(Init(&data));
  1929| }
  1930| HRESULT CordbReJitILCode::Init(DacSharedReJitInfo* pSharedReJitInfo)
  1931| {
  1932|     HRESULT hr = S_OK;
  1933|     if (pSharedReJitInfo->m_cInstrumentedMapEntries)
  1934|     {
  1935|         if (pSharedReJitInfo->m_cInstrumentedMapEntries > 100000)
  1936|             return CORDBG_E_TARGET_INCONSISTENT;
  1937|         m_cILMap = pSharedReJitInfo->m_cInstrumentedMapEntries;
  1938|         m_pILMap = new (nothrow)COR_IL_MAP[m_cILMap];
  1939|         TargetBuffer mapBuffer(pSharedReJitInfo->m_rgInstrumentedMapEntries, m_cILMap*sizeof(COR_IL_MAP));
  1940|         IfFailRet(GetProcess()->SafeReadBuffer(mapBuffer, (BYTE*)m_pILMap.GetValue(), FALSE /* bThrowOnError */));
  1941|     }
  1942|     CORDB_ADDRESS pIlHeader = pSharedReJitInfo->m_pbIL;
  1943|     IMAGE_COR_ILMETHOD_FAT header = { 0 };
  1944|     bool headerMustBeTiny = false;
  1945|     ULONG32 headerSize = 0;
  1946|     hr = GetProcess()->SafeReadStruct(pIlHeader, &header);
  1947|     if (hr != S_OK)
  1948|     {
  1949|         headerMustBeTiny = true;
  1950|         IfFailRet(GetProcess()->SafeReadStruct(pIlHeader, (IMAGE_COR_ILMETHOD_TINY *)&header));
  1951|     }
  1952|     ULONG32 ilCodeSize = 0;
  1953|     IMAGE_COR_ILMETHOD_TINY *pMethodTinyHeader = (IMAGE_COR_ILMETHOD_TINY *)&header;
  1954|     bool isTinyHeader = ((pMethodTinyHeader->Flags_CodeSize & (CorILMethod_FormatMask >> 1)) == CorILMethod_TinyFormat);
  1955|     if (isTinyHeader)
  1956|     {
  1957|         ilCodeSize = (((unsigned)pMethodTinyHeader->Flags_CodeSize) >> (CorILMethod_FormatShift - 1));
  1958|         headerSize = sizeof(IMAGE_COR_ILMETHOD_TINY);
  1959|         m_localVarSigToken = mdSignatureNil;
  1960|     }
  1961|     else if (headerMustBeTiny)
  1962|     {
  1963|         return CORDBG_E_READVIRTUAL_FAILURE;
  1964|     }
  1965|     else
  1966|     {
  1967|         ilCodeSize = header.CodeSize;
  1968|         headerSize = header.Size * 4;
  1969|         m_localVarSigToken = header.LocalVarSigTok;
  1970|     }
  1971|     if (ilCodeSize == 0 || ilCodeSize > 100000)
  1972|     {
  1973|         return CORDBG_E_TARGET_INCONSISTENT;
  1974|     }
  1975|     m_codeRegionInfo.Init(pIlHeader + headerSize, ilCodeSize);
  1976|     m_pLocalIL = new (nothrow) BYTE[ilCodeSize];
  1977|     if (m_pLocalIL == NULL)
  1978|         return E_OUTOFMEMORY;
  1979|     m_cbLocalIL = ilCodeSize;
  1980|     IfFailRet(GetProcess()->SafeReadBuffer(m_codeRegionInfo, m_pLocalIL, FALSE /*throwOnError*/));
  1981|     if ((pMethodTinyHeader->Flags_CodeSize & CorILMethod_MoreSects) == 0)
  1982|     {
  1983|         return S_OK; // no EH, done initing
  1984|     }
  1985|     CORDB_ADDRESS ehClauseHeader = ((pIlHeader + headerSize + ilCodeSize - 1) & ~3) + 4;
  1986|     BYTE kind = 0;
  1987|     IfFailRet(GetProcess()->SafeReadStruct(ehClauseHeader, &kind));
  1988|     if ((kind & CorILMethod_Sect_KindMask) != CorILMethod_Sect_EHTable)
  1989|     {
  1990|         return S_OK;
  1991|     }
  1992|     if (kind & CorILMethod_Sect_FatFormat)
  1993|     {
  1994|         IMAGE_COR_ILMETHOD_SECT_FAT sectionHeader = { 0 };
  1995|         IfFailRet(GetProcess()->SafeReadStruct(ehClauseHeader, &sectionHeader));
  1996|         m_cClauses = (sectionHeader.DataSize - 4) / sizeof(IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT);
  1997|         if (m_cClauses > 10000) // sanity check the data before allocating
  1998|         {
  1999|             return CORDBG_E_TARGET_INCONSISTENT;
  2000|         }
  2001|         TargetBuffer buffer(ehClauseHeader + sizeof(IMAGE_COR_ILMETHOD_SECT_FAT), m_cClauses*sizeof(IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT));
  2002|         NewArrayHolder<IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT> pClauses = new (nothrow)IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT[m_cClauses];
  2003|         if (pClauses == NULL)
  2004|             return E_OUTOFMEMORY;
  2005|         IfFailRet(GetProcess()->SafeReadBuffer(buffer, (BYTE*)pClauses.GetValue(), FALSE /*throwOnError*/));
  2006|         m_pClauses = new (nothrow)CorDebugEHClause[m_cClauses];
  2007|         if (m_pClauses == NULL)
  2008|             return E_OUTOFMEMORY;
  2009|         for (ULONG32 i = 0; i < m_cClauses; i++)
  2010|         {
  2011|             BOOL isFilter = ((pClauses[i].Flags & COR_ILEXCEPTION_CLAUSE_FILTER) != 0);
  2012|             m_pClauses[i].Flags = pClauses[i].Flags;
  2013|             m_pClauses[i].TryOffset = pClauses[i].TryOffset;
  2014|             m_pClauses[i].TryLength = pClauses[i].TryLength;
  2015|             m_pClauses[i].HandlerOffset = pClauses[i].HandlerOffset;
  2016|             m_pClauses[i].HandlerLength = pClauses[i].HandlerLength;
  2017|             m_pClauses[i].ClassToken = isFilter ? 0 : pClauses[i].ClassToken;
  2018|             m_pClauses[i].FilterOffset = isFilter ? pClauses[i].FilterOffset : 0;
  2019|         }
  2020|     }
  2021|     else
  2022|     {
  2023|         IMAGE_COR_ILMETHOD_SECT_SMALL sectionHeader = { 0 };
  2024|         IfFailRet(GetProcess()->SafeReadStruct(ehClauseHeader, &sectionHeader));
  2025|         ULONG32 m_cClauses = (sectionHeader.DataSize - 4) / sizeof(IMAGE_COR_ILMETHOD_SECT_SMALL);
  2026|         if (m_cClauses > 10000) // sanity check the data before allocating
  2027|         {
  2028|             return CORDBG_E_TARGET_INCONSISTENT;
  2029|         }
  2030|         TargetBuffer buffer(ehClauseHeader + sizeof(IMAGE_COR_ILMETHOD_SECT_SMALL), m_cClauses*sizeof(IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_SMALL));
  2031|         NewArrayHolder<IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_SMALL> pClauses = new (nothrow)IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_SMALL[m_cClauses];
  2032|         if (pClauses == NULL)
  2033|             return E_OUTOFMEMORY;
  2034|         IfFailRet(GetProcess()->SafeReadBuffer(buffer, (BYTE*)pClauses.GetValue(), FALSE /*throwOnError*/));
  2035|         m_pClauses = new (nothrow)CorDebugEHClause[m_cClauses];
  2036|         if (m_pClauses == NULL)
  2037|             return E_OUTOFMEMORY;
  2038|         for (ULONG32 i = 0; i < m_cClauses; i++)
  2039|         {
  2040|             BOOL isFilter = ((pClauses[i].Flags & COR_ILEXCEPTION_CLAUSE_FILTER) != 0);
  2041|             m_pClauses[i].Flags = pClauses[i].Flags;
  2042|             m_pClauses[i].TryOffset = pClauses[i].TryOffset;
  2043|             m_pClauses[i].TryLength = pClauses[i].TryLength;
  2044|             m_pClauses[i].HandlerOffset = pClauses[i].HandlerOffset;
  2045|             m_pClauses[i].HandlerLength = pClauses[i].HandlerLength;
  2046|             m_pClauses[i].ClassToken = isFilter ? 0 : pClauses[i].ClassToken;
  2047|             m_pClauses[i].FilterOffset = isFilter ? pClauses[i].FilterOffset : 0;
  2048|         }
  2049|     }
  2050|     return S_OK;
  2051| }
  2052| #ifndef MIN
  2053| #define MIN(a,b) ((a) < (b) ? (a) : (b))
  2054| #endif
  2055| HRESULT CordbReJitILCode::GetEHClauses(ULONG32 cClauses, ULONG32 * pcClauses, CorDebugEHClause clauses[])
  2056| {
  2057|     PUBLIC_API_ENTRY(this);
  2058|     FAIL_IF_NEUTERED(this);
  2059|     VALIDATE_POINTER_TO_OBJECT_OR_NULL(pcClauses, ULONG32 *);
  2060|     VALIDATE_POINTER_TO_OBJECT_ARRAY_OR_NULL(clauses, CorDebugEHClause *, cClauses, true, true);
  2061|     ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
  2062|     if (cClauses != 0 && clauses == NULL)
  2063|     {
  2064|         return E_INVALIDARG;
  2065|     }
  2066|     if (pcClauses != NULL)
  2067|     {
  2068|         if (cClauses == 0)
  2069|         {
  2070|             *pcClauses = m_cClauses;
  2071|         }
  2072|         else
  2073|         {
  2074|             *pcClauses = MIN(cClauses, m_cClauses);
  2075|         }
  2076|     }
  2077|     if (clauses != NULL)
  2078|     {
  2079|         memcpy_s(clauses, sizeof(CorDebugEHClause)*cClauses, m_pClauses, sizeof(CorDebugEHClause)*MIN(cClauses, m_cClauses));
  2080|     }
  2081|     return S_OK;
  2082| }
  2083| ULONG CordbReJitILCode::AddRef()
  2084| {
  2085|     return CordbCode::AddRef();
  2086| }
  2087| ULONG CordbReJitILCode::Release()
  2088| {
  2089|     return CordbCode::Release();
  2090| }
  2091| HRESULT CordbReJitILCode::QueryInterface(REFIID riid, void** ppInterface)
  2092| {
  2093|     if (riid == IID_ICorDebugILCode)
  2094|     {
  2095|         *ppInterface = static_cast<ICorDebugILCode*>(this);
  2096|     }
  2097|     else if (riid == IID_ICorDebugILCode2)
  2098|     {
  2099|         *ppInterface = static_cast<ICorDebugILCode2*>(this);
  2100|     }
  2101|     else
  2102|     {
  2103|         return CordbILCode::QueryInterface(riid, ppInterface);
  2104|     }
  2105|     AddRef();
  2106|     return S_OK;
  2107| }
  2108| HRESULT CordbReJitILCode::GetLocalVarSigToken(mdSignature *pmdSig)
  2109| {
  2110|     PUBLIC_API_ENTRY(this);
  2111|     FAIL_IF_NEUTERED(this);
  2112|     VALIDATE_POINTER_TO_OBJECT(pmdSig, mdSignature *);
  2113|     ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
  2114|     *pmdSig = m_localVarSigToken;
  2115|     return S_OK;
  2116| }
  2117| HRESULT CordbReJitILCode::GetInstrumentedILMap(ULONG32 cMap, ULONG32 *pcMap, COR_IL_MAP map[])
  2118| {
  2119|     PUBLIC_API_ENTRY(this);
  2120|     FAIL_IF_NEUTERED(this);
  2121|     VALIDATE_POINTER_TO_OBJECT_OR_NULL(pcClauses, ULONG32 *);
  2122|     VALIDATE_POINTER_TO_OBJECT_ARRAY_OR_NULL(map, COR_IL_MAP *, cMap, true, true);
  2123|     ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
  2124|     if (cMap != 0 && map == NULL)
  2125|     {
  2126|         return E_INVALIDARG;
  2127|     }
  2128|     if (pcMap != NULL)
  2129|     {
  2130|         if (cMap == 0)
  2131|         {
  2132|             *pcMap = m_cILMap;
  2133|         }
  2134|         else
  2135|         {
  2136|             *pcMap = MIN(cMap, m_cILMap);
  2137|         }
  2138|     }
  2139|     if (map != NULL)
  2140|     {
  2141|         memcpy_s(map, sizeof(COR_IL_MAP)*cMap, m_pILMap, sizeof(COR_IL_MAP)*MIN(cMap, m_cILMap));
  2142|     }
  2143|     return S_OK;
  2144| }
  2145| HRESULT FindNativeInfoInILVariableArray(DWORD                                               dwIndex,
  2146|                                         SIZE_T                                              ip,
  2147|                                         const DacDbiArrayList<ICorDebugInfo::NativeVarInfo> * nativeInfoList,
  2148|                                         const ICorDebugInfo::NativeVarInfo **                 ppNativeInfo)
  2149| {
  2150|     _ASSERTE(ppNativeInfo != NULL);
  2151|     *ppNativeInfo = NULL;
  2152|     int lastGoodOne = -1;
  2153|     for (unsigned int i = 0; i < (unsigned)nativeInfoList->Count(); i++)
  2154|     {
  2155|         if ((*nativeInfoList)[i].varNumber == dwIndex)
  2156|         {
  2157|             if ( (lastGoodOne == -1) ||
  2158|                  ((*nativeInfoList)[lastGoodOne].startOffset < (*nativeInfoList)[i].startOffset) )
  2159|             {
  2160|                 lastGoodOne = i;
  2161|             }
  2162|             if (((*nativeInfoList)[i].startOffset <= ip) &&
  2163|                 ((*nativeInfoList)[i].endOffset > ip))
  2164|             {
  2165|                 *ppNativeInfo = &((*nativeInfoList)[i]);
  2166|                 return S_OK;
  2167|             }
  2168|         }
  2169|     }
  2170|     if ((lastGoodOne > -1) && ((*nativeInfoList)[lastGoodOne].endOffset == ip))
  2171|     {
  2172|         *ppNativeInfo = &((*nativeInfoList)[lastGoodOne]);
  2173|         return S_OK;
  2174|     }
  2175|     return CORDBG_E_IL_VAR_NOT_AVAILABLE;
  2176| } // FindNativeInfoInILVariableArray
  2177| CordbVariableHome::CordbVariableHome(CordbNativeCode *pCode,
  2178|                                      const ICorDebugInfo::NativeVarInfo nativeVarInfo,
  2179|                                      BOOL isLocal,
  2180|                                      ULONG index) :
  2181|     CordbBase(pCode->GetModule()->GetProcess(), 0)
  2182| {
  2183|     _ASSERTE(pCode != NULL);
  2184|     m_pCode.Assign(pCode);
  2185|     m_nativeVarInfo = nativeVarInfo;
  2186|     m_isLocal = isLocal;
  2187|     m_index = index;
  2188| }
  2189| CordbVariableHome::~CordbVariableHome()
  2190| {
  2191|     _ASSERTE(this->IsNeutered());
  2192| }
  2193| void CordbVariableHome::Neuter()
  2194| {
  2195|     m_pCode.Clear();
  2196|     CordbBase::Neuter();
  2197| }
  2198| HRESULT CordbVariableHome::QueryInterface(REFIID id, void **pInterface)
  2199| {
  2200|     if (id == IID_ICorDebugVariableHome)
  2201|     {
  2202|         *pInterface = static_cast<ICorDebugVariableHome *>(this);
  2203|     }
  2204|     else if (id == IID_IUnknown)
  2205|     {
  2206|         *pInterface = static_cast<IUnknown *>(static_cast<ICorDebugVariableHome *>(this));
  2207|     }
  2208|     else
  2209|     {
  2210|         *pInterface = NULL;
  2211|         return E_NOINTERFACE;
  2212|     }
  2213|     ExternalAddRef();
  2214|     return S_OK;
  2215| }
  2216| HRESULT CordbVariableHome::GetCode(ICorDebugCode **ppCode)
  2217| {
  2218|     PUBLIC_REENTRANT_API_ENTRY(this);
  2219|     FAIL_IF_NEUTERED(this);
  2220|     VALIDATE_POINTER_TO_OBJECT(ppCode, ICorDebugCode **);
  2221|     ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
  2222|     HRESULT hr = m_pCode->QueryInterface(IID_ICorDebugCode, (LPVOID*)ppCode);
  2223|     return hr;
  2224| }
  2225| HRESULT CordbVariableHome::GetSlotIndex(ULONG32 *pSlotIndex)
  2226| {
  2227|     PUBLIC_REENTRANT_API_ENTRY(this);
  2228|     FAIL_IF_NEUTERED(this);
  2229|     VALIDATE_POINTER_TO_OBJECT(pSlotIndex, ULONG32 *);
  2230|     ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
  2231|     if (!m_isLocal)
  2232|     {
  2233|         return E_FAIL;
  2234|     }
  2235|     *pSlotIndex = m_index;
  2236|     return S_OK;
  2237| }
  2238| HRESULT CordbVariableHome::GetArgumentIndex(ULONG32 *pArgumentIndex)
  2239| {
  2240|     PUBLIC_REENTRANT_API_ENTRY(this);
  2241|     FAIL_IF_NEUTERED(this);
  2242|     VALIDATE_POINTER_TO_OBJECT(pArgumentIndex, ULONG32 *);
  2243|     ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
  2244|     if (m_isLocal)
  2245|     {
  2246|         return E_FAIL;
  2247|     }
  2248|     *pArgumentIndex = m_index;
  2249|     return S_OK;
  2250| }
  2251| HRESULT CordbVariableHome::GetLiveRange(ULONG32 *pStartOffset,
  2252|                                         ULONG32 *pEndOffset)
  2253| {
  2254|     PUBLIC_REENTRANT_API_ENTRY(this);
  2255|     FAIL_IF_NEUTERED(this);
  2256|     VALIDATE_POINTER_TO_OBJECT(pStartOffset, ULONG32 *);
  2257|     VALIDATE_POINTER_TO_OBJECT(pEndOffset, ULONG32 *);
  2258|     ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
  2259|     *pStartOffset = m_nativeVarInfo.startOffset;
  2260|     *pEndOffset = m_nativeVarInfo.endOffset;
  2261|     return S_OK;
  2262| }
  2263| HRESULT CordbVariableHome::GetLocationType(VariableLocationType *pLocationType)
  2264| {
  2265|     PUBLIC_REENTRANT_API_ENTRY(this);
  2266|     FAIL_IF_NEUTERED(this);
  2267|     VALIDATE_POINTER_TO_OBJECT(pLocationType, VariableLocationType *);
  2268|     ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
  2269|     switch (m_nativeVarInfo.loc.vlType)
  2270|     {
  2271|     case ICorDebugInfo::VLT_REG:
  2272|         *pLocationType = VLT_REGISTER;
  2273|         break;
  2274|     case ICorDebugInfo::VLT_STK:
  2275|         *pLocationType = VLT_REGISTER_RELATIVE;
  2276|         break;
  2277|     default:
  2278|         *pLocationType = VLT_INVALID;
  2279|     }
  2280|     return S_OK;
  2281| }
  2282| HRESULT CordbVariableHome::GetRegister(CorDebugRegister *pRegister)
  2283| {
  2284|     PUBLIC_REENTRANT_API_ENTRY(this);
  2285|     FAIL_IF_NEUTERED(this);
  2286|     VALIDATE_POINTER_TO_OBJECT(pRegister, CorDebugRegister *);
  2287|     ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
  2288|     switch (m_nativeVarInfo.loc.vlType)
  2289|     {
  2290|     case ICorDebugInfo::VLT_REG:
  2291|         *pRegister = ConvertRegNumToCorDebugRegister(m_nativeVarInfo.loc.vlReg.vlrReg);
  2292|         break;
  2293|     case ICorDebugInfo::VLT_STK:
  2294|         *pRegister = ConvertRegNumToCorDebugRegister(m_nativeVarInfo.loc.vlStk.vlsBaseReg);
  2295|         break;
  2296|     default:
  2297|         return E_FAIL;
  2298|     }
  2299|     return S_OK;
  2300| }
  2301| HRESULT CordbVariableHome::GetOffset(LONG *pOffset)
  2302| {
  2303|     PUBLIC_REENTRANT_API_ENTRY(this);
  2304|     FAIL_IF_NEUTERED(this);
  2305|     VALIDATE_POINTER_TO_OBJECT(pOffset, LONG *);
  2306|     ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
  2307|     switch (m_nativeVarInfo.loc.vlType)
  2308|     {
  2309|     case ICorDebugInfo::VLT_STK:
  2310|         *pOffset = m_nativeVarInfo.loc.vlStk.vlsOffset;
  2311|         break;
  2312|     default:
  2313|         return E_FAIL;
  2314|     }
  2315|     return S_OK;
  2316| }
  2317| CordbNativeCode::CordbNativeCode(CordbFunction *                pFunction,
  2318|                                  const NativeCodeFunctionData * pJitData,
  2319|                                  BOOL                           fIsInstantiatedGeneric)
  2320|   : CordbCode(pFunction, (UINT_PTR)pJitData->m_rgCodeRegions[kHot].pAddress, pJitData->encVersion, FALSE),
  2321|     m_vmNativeCodeMethodDescToken(pJitData->vmNativeCodeMethodDescToken),
  2322|     m_fCodeAvailable(TRUE),
  2323|     m_fIsInstantiatedGeneric(fIsInstantiatedGeneric != FALSE)
  2324| {
  2325|     _ASSERTE(GetVersion() >= CorDB_DEFAULT_ENC_FUNCTION_VERSION);
  2326|     for (CodeBlobRegion region = kHot; region < MAX_REGIONS; ++region)
  2327|     {
  2328|         m_rgCodeRegions[region] = pJitData->m_rgCodeRegions[region];
  2329|     }
  2330| } //CordbNativeCode::CordbNativeCode
  2331| HRESULT CordbNativeCode::QueryInterface(REFIID id, void ** pInterface)
  2332| {
  2333|     if (id == IID_ICorDebugCode)
  2334|     {
  2335|         *pInterface = static_cast<ICorDebugCode *>(this);
  2336|     }
  2337|     else if (id == IID_ICorDebugCode2)
  2338|     {
  2339|         *pInterface = static_cast<ICorDebugCode2 *>(this);
  2340|     }
  2341|     else if (id == IID_ICorDebugCode3)
  2342|     {
  2343|         *pInterface = static_cast<ICorDebugCode3 *>(this);
  2344|     }
  2345|     else if (id == IID_ICorDebugCode4)
  2346|     {
  2347|         *pInterface = static_cast<ICorDebugCode4 *>(this);
  2348|     }
  2349|     else if (id == IID_IUnknown)
  2350|     {
  2351|         *pInterface = static_cast<IUnknown *>(static_cast<ICorDebugCode *>(this));
  2352|     }
  2353|     else
  2354|     {
  2355|         *pInterface = NULL;
  2356|         return E_NOINTERFACE;
  2357|     }
  2358|     ExternalAddRef();
  2359|     return S_OK;
  2360| }
  2361| HRESULT CordbNativeCode::GetAddress(CORDB_ADDRESS * pStart)
  2362| {
  2363|     PUBLIC_REENTRANT_API_ENTRY(this);
  2364|     FAIL_IF_NEUTERED(this);
  2365|     VALIDATE_POINTER_TO_OBJECT(pStart, CORDB_ADDRESS *);
  2366|     _ASSERTE(this != NULL);
  2367|     _ASSERTE(this->GetFunction() != NULL);
  2368|     _ASSERTE(this->GetFunction()->GetModule() != NULL);
  2369|     _ASSERTE(this->GetFunction()->GetModule()->GetProcess() == GetProcess());
  2370|     *pStart = (m_rgCodeRegions[kHot].pAddress);
  2371|     if (*pStart == NULL)
  2372|     {
  2373|         return CORDBG_E_CODE_NOT_AVAILABLE;
  2374|     }
  2375|     return S_OK;
  2376| } // CordbNativeCode::GetAddress
  2377| HRESULT CordbNativeCode::ReadCodeBytes()
  2378| {
  2379|     HRESULT hr = S_OK;
  2380|     EX_TRY
  2381|     {
  2382|         CORDB_ADDRESS pHotStart = m_rgCodeRegions[kHot].pAddress;
  2383|         CORDB_ADDRESS pColdStart = m_rgCodeRegions[kCold].pAddress;
  2384|         ULONG32 cbHotSize = (ULONG32) m_rgCodeRegions[kHot].cbSize;
  2385|         ULONG32 cbColdSize = GetColdSize();
  2386|         delete [] m_rgbCode;
  2387|         m_rgbCode = new BYTE[cbHotSize + cbColdSize];
  2388|         SIZE_T cbRead;
  2389|         hr = GetProcess()->ReadMemory(pHotStart, cbHotSize, m_rgbCode, &cbRead);
  2390|         IfFailThrow(hr);
  2391|         SIMPLIFYING_ASSUMPTION(cbRead == cbHotSize);
  2392|         if (HasColdRegion())
  2393|         {
  2394|             hr = GetProcess()->ReadMemory(pColdStart, cbColdSize, (BYTE *) m_rgbCode + cbHotSize, &cbRead);
  2395|             IfFailThrow(hr);
  2396|             SIMPLIFYING_ASSUMPTION(cbRead == cbColdSize);
  2397|         }
  2398|     }
  2399|     EX_CATCH_HRESULT(hr);
  2400|     return hr;
  2401| } // CordbNativeCode::ReadCodeBytes
  2402| ULONG32 CordbNativeCode::GetColdSize()
  2403| {
  2404|     ULONG32 pcBytes = 0;
  2405|     for (CodeBlobRegion index = kCold; index < MAX_REGIONS; ++index)
  2406|     {
  2407|         pcBytes += m_rgCodeRegions[index].cbSize;
  2408|     }
  2409|     return pcBytes;
  2410| } // CordbNativeCode::GetColdSize
  2411| ULONG32 CordbNativeCode::GetSize()
  2412| {
  2413|     ULONG32 pcBytes = 0;
  2414|     for (CodeBlobRegion index = kHot; index < MAX_REGIONS; ++index)
  2415|     {
  2416|         pcBytes += m_rgCodeRegions[index].cbSize;
  2417|     }
  2418|     return pcBytes;
  2419| } // CordbNativeCode::GetSize
  2420| HRESULT CordbNativeCode::GetILToNativeMapping(ULONG32                    cMap,
  2421|                                               ULONG32 *                  pcMap,
  2422|                                               COR_DEBUG_IL_TO_NATIVE_MAP map[])
  2423| {
  2424|     PUBLIC_REENTRANT_API_ENTRY(this);
  2425|     FAIL_IF_NEUTERED(this);
  2426|     VALIDATE_POINTER_TO_OBJECT_OR_NULL(pcMap, ULONG32 *);
  2427|     VALIDATE_POINTER_TO_OBJECT_ARRAY_OR_NULL(map, COR_DEBUG_IL_TO_NATIVE_MAP *,cMap,true,true);
  2428|     ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
  2429|     HRESULT hr = S_OK;
  2430|     EX_TRY
  2431|     {
  2432|         LoadNativeInfo();
  2433|         SequencePoints * pSeqPts = GetSequencePoints();
  2434|         ULONG32 cMapIntCount = pSeqPts->GetEntryCount();
  2435|         if (map != NULL && cMapIntCount != 0)
  2436|         {
  2437|             DebuggerILToNativeMap * rgMapInt = pSeqPts->GetMapAddr();
  2438|             ULONG32 cMapToCopy = min(cMap, cMapIntCount);
  2439|             ULONG32 size = GetSize();
  2440|             ExportILToNativeMap(cMapToCopy, map, rgMapInt, size);
  2441|         }
  2442|         if (pcMap)
  2443|         {
  2444|             *pcMap = cMapIntCount;
  2445|         }
  2446|     }
  2447|     EX_CATCH_HRESULT(hr);
  2448|     return hr;
  2449| } // CordbNativeCode::GetILToNativeMapping
  2450| HRESULT CordbNativeCode::GetCodeChunks(
  2451|     ULONG32 cbufSize,
  2452|     ULONG32 * pcnumChunks,
  2453|     CodeChunkInfo chunks[]
  2454| )
  2455| {
  2456|     PUBLIC_API_ENTRY(this);
  2457|     if (pcnumChunks == NULL)
  2458|     {
  2459|         return E_INVALIDARG;
  2460|     }
  2461|     if ((chunks == NULL) != (cbufSize == 0))
  2462|     {
  2463|         return E_INVALIDARG;
  2464|     }
  2465|     ULONG32 cActualChunks = HasColdRegion() ? 2 : 1;
  2466|     if (cbufSize == 0)
  2467|     {
  2468|         *pcnumChunks = cActualChunks;
  2469|         return S_OK;
  2470|     }
  2471|     for (CodeBlobRegion index = kHot; (index < MAX_REGIONS) && ((int)cbufSize > index); ++index)
  2472|     {
  2473|         chunks[index].startAddr = m_rgCodeRegions[index].pAddress;
  2474|         chunks[index].length = (ULONG32) (m_rgCodeRegions[index].cbSize);
  2475|         *pcnumChunks = cbufSize;
  2476|     }
  2477|     return S_OK;
  2478| } // CordbNativeCode::GetCodeChunks
  2479| HRESULT CordbNativeCode::GetCompilerFlags(DWORD * pdwFlags)
  2480| {
  2481|     PUBLIC_API_ENTRY(this);
  2482|     FAIL_IF_NEUTERED(this);
  2483|     VALIDATE_POINTER_TO_OBJECT(pdwFlags, DWORD *);
  2484|     *pdwFlags = 0;
  2485|     ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
  2486|     return GetFunction()->GetModule()->GetJITCompilerFlags(pdwFlags);
  2487| } // CordbNativeCode::GetCompilerFlags
  2488| HRESULT CordbNativeCode::ILVariableToNative(DWORD dwIndex,
  2489|                                             SIZE_T ip,
  2490|                                             const ICorDebugInfo::NativeVarInfo ** ppNativeInfo)
  2491| {
  2492|     _ASSERTE(m_nativeVarData.IsInitialized());
  2493|     return FindNativeInfoInILVariableArray(dwIndex,
  2494|                                            ip,
  2495|                                            m_nativeVarData.GetOffsetInfoList(),
  2496|                                            ppNativeInfo);
  2497| } // CordbNativeCode::ILVariableToNative
  2498| HRESULT CordbNativeCode::GetReturnValueLiveOffset(ULONG32 ILoffset, ULONG32 bufferSize, ULONG32 *pFetched, ULONG32 *pOffsets)
  2499| {
  2500|     HRESULT hr = S_OK;
  2501|     PUBLIC_API_ENTRY(this);
  2502|     FAIL_IF_NEUTERED(this);
  2503|     VALIDATE_POINTER_TO_OBJECT(pFetched, ULONG32 *);
  2504|     ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
  2505|     EX_TRY
  2506|     {
  2507|         hr = GetReturnValueLiveOffsetImpl(NULL, ILoffset, bufferSize, pFetched, pOffsets);
  2508|     }
  2509|     EX_CATCH_HRESULT(hr);
  2510|     return hr;
  2511| }
  2512| HRESULT CordbNativeCode::EnumerateVariableHomes(ICorDebugVariableHomeEnum **ppEnum)
  2513| {
  2514|     PUBLIC_REENTRANT_API_ENTRY(this);
  2515|     FAIL_IF_NEUTERED(this);
  2516|     VALIDATE_POINTER_TO_OBJECT(ppEnum, ICorDebugVariableHomeEnum **);
  2517|     ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
  2518|     HRESULT hr = S_OK;
  2519|     ULONG argCount = 0;
  2520|     CordbFunction *func = GetFunction();
  2521|     _ASSERTE(func != NULL);
  2522|     IfFailRet(func->GetSig(NULL, &argCount, NULL));
  2523| #ifdef _DEBUG
  2524|     ULONG localCount = 0;
  2525|     EX_TRY
  2526|     {
  2527|         GetFunction()->GetILCode()->GetLocalVarSig(NULL, &localCount);
  2528|     }
  2529|     EX_CATCH_HRESULT(hr);
  2530|     IfFailRet(hr);
  2531| #endif
  2532|     RSSmartPtr<CordbVariableHome> *rsHomes = NULL;
  2533|     EX_TRY
  2534|     {
  2535|         CordbProcess *pProcess = GetProcess();
  2536|         _ASSERTE(pProcess != NULL);
  2537|         const DacDbiArrayList<ICorDebugInfo::NativeVarInfo> *pOffsetInfoList = m_nativeVarData.GetOffsetInfoList();
  2538|         _ASSERTE(pOffsetInfoList != NULL);
  2539|         DWORD countHomes = 0;
  2540|         for (unsigned int i = 0; i < pOffsetInfoList->Count(); i++)
  2541|         {
  2542|             const ICorDebugInfo::NativeVarInfo *pNativeVarInfo = &((*pOffsetInfoList)[i]);
  2543|             _ASSERTE(pNativeVarInfo != NULL);
  2544|             if (pNativeVarInfo->varNumber < (DWORD)ICorDebugInfo::MAX_ILNUM)
  2545|             {
  2546|                 countHomes++;
  2547|             }
  2548|         }
  2549|         rsHomes = new RSSmartPtr<CordbVariableHome>[countHomes];
  2550|         DWORD varHomeInd = 0;
  2551|         for (unsigned int i = 0; i < pOffsetInfoList->Count(); i++)
  2552|         {
  2553|             const ICorDebugInfo::NativeVarInfo *pNativeVarInfo = &((*pOffsetInfoList)[i]);
  2554|             if (pNativeVarInfo->varNumber < (DWORD)ICorDebugInfo::MAX_ILNUM)
  2555|             {
  2556|                 BOOL isLocal = ((ULONG)pNativeVarInfo->varNumber >= argCount);
  2557|                 ULONG argOrSlotIndex;
  2558|                 if (isLocal) {
  2559|                     argOrSlotIndex = pNativeVarInfo->varNumber - argCount;
  2560|                     _ASSERTE(argOrSlotIndex < localCount);
  2561|                 } else {
  2562|                     argOrSlotIndex = pNativeVarInfo->varNumber;
  2563|                 }
  2564|                 RSInitHolder<CordbVariableHome> pCVH(new CordbVariableHome(this,
  2565|                                                                            (*pOffsetInfoList)[i],
  2566|                                                                            isLocal,
  2567|                                                                            argOrSlotIndex));
  2568|                 pProcess->GetContinueNeuterList()->Add(pProcess, pCVH);
  2569|                 _ASSERTE(varHomeInd < countHomes);
  2570|                 rsHomes[varHomeInd].Assign(pCVH);
  2571|                 pCVH.ClearAndMarkDontNeuter();
  2572|                 varHomeInd++;
  2573|             }
  2574|         }
  2575|         RSInitHolder<CordbVariableHomeEnumerator> pCDVHE(
  2576|             new CordbVariableHomeEnumerator(GetProcess(), &rsHomes, countHomes));
  2577|         pProcess->GetContinueNeuterList()->Add(pProcess, pCDVHE);
  2578|         pCDVHE.TransferOwnershipExternal(ppEnum);
  2579|     }
  2580|     EX_CATCH_HRESULT(hr);
  2581|     return hr;
  2582| }
  2583| int CordbNativeCode::GetCallInstructionLength(BYTE *ip, ULONG32 count)
  2584| {
  2585| #if defined(TARGET_ARM)
  2586|     if (Is32BitInstruction(*(WORD*)ip))
  2587|         return 4;
  2588|     else
  2589|         return 2;
  2590| #elif defined(TARGET_ARM64)
  2591|     return MAX_INSTRUCTION_LENGTH;
  2592| #elif defined(TARGET_LOONGARCH64)
  2593|     return MAX_INSTRUCTION_LENGTH;
  2594| #elif defined(TARGET_X86)
  2595|     if (count < 2)
  2596|         return -1;
  2597|     do
  2598|     {
  2599|         switch (*ip)
  2600|         {
  2601|         case 0x26: // ES
  2602|         case 0x2E: // CS
  2603|         case 0x36: // SS
  2604|         case 0x3E: // DS
  2605|         case 0x64: // FS
  2606|         case 0x65: // GS
  2607|         case 0x66: // Operand-Size
  2608|         case 0x67: // Address-Size
  2609|         case 0xf0:
  2610|         case 0xf1:
  2611|         case 0xf2: // REPNE/REPNZ
  2612|         case 0xf3:
  2613|             ip++;
  2614|             count--;
  2615|             continue;
  2616|         default:
  2617|             break;
  2618|         }
  2619|     } while (0);
  2620|     BYTE opcode = *ip++;
  2621|     if (opcode == 0xcc)
  2622|     {
  2623|         _ASSERTE(!"Hit break opcode!");
  2624|         return -1;
  2625|     }
  2626|     switch (opcode)
  2627|     {
  2628|     case 0xff:
  2629|     {
  2630|                  if (count < 2)
  2631|                      return -1;
  2632|                  BYTE modrm = *ip++;
  2633|                  BYTE mod = (modrm & 0xC0) >> 6;
  2634|                  BYTE reg = (modrm & 0x38) >> 3;
  2635|                  BYTE rm  = (modrm & 0x07);
  2636|                  int displace = -1;
  2637|                  if ((reg != 2) && (reg != 3) && (reg != 4) && (reg != 5))
  2638|                  {
  2639|                      _ASSERTE(!"Unhandled opcode!");
  2640|                      return -1;
  2641|                  }
  2642|                  switch (mod)
  2643|                  {
  2644|                  case 0:
  2645|                  case 1:
  2646|                  case 2:
  2647|                      if (rm == 4)
  2648|                      {
  2649|                          if (count < 3)
  2650|                              return -1;
  2651|                          BYTE ss    = (*ip & 0xC0) >> 6;
  2652|                          BYTE index = (*ip & 0x38) >> 3;
  2653|                          BYTE base  = (*ip & 0x7);
  2654|                          if (mod == 0)
  2655|                          {
  2656|                              if (base == 5)
  2657|                                  displace = 7;
  2658|                              else
  2659|                                  displace = 3;
  2660|                          }
  2661|                          else if (mod == 1)
  2662|                          {
  2663|                              displace = 4;
  2664|                          }
  2665|                          else
  2666|                          {
  2667|                              displace = 7;
  2668|                          }
  2669|                      }
  2670|                      else
  2671|                      {
  2672|                          if (mod == 0)
  2673|                          {
  2674|                              if (rm == 5)
  2675|                                  displace = 6;
  2676|                              else
  2677|                                  displace = 2;
  2678|                          }
  2679|                          else if (mod == 1)
  2680|                          {
  2681|                              displace = 3;
  2682|                          }
  2683|                          else
  2684|                          {
  2685|                              displace = 6;
  2686|                          }
  2687|                      }
  2688|                      break;
  2689|                  case 3:
  2690|                  default:
  2691|                      displace = 2;
  2692|                      break;
  2693|                  }
  2694|                  return displace;
  2695|     }  // end of 0xFF case
  2696|     case 0xe8:
  2697|         return 5;
  2698|     default:
  2699|         break;
  2700|     }
  2701|     _ASSERTE(!"Unhandled opcode!");
  2702|     return -1;
  2703| #elif defined(TARGET_AMD64)
  2704|     BYTE rex = NULL;
  2705|     BYTE prefix = *ip;
  2706|     BOOL fContainsPrefix = FALSE;
  2707|     if (prefix == 0xcc)
  2708|         return -1;
  2709|     do
  2710|     {
  2711|         switch (prefix)
  2712|         {
  2713|         case 0x26: // ES
  2714|         case 0x2E: // CS
  2715|         case 0x36: // SS
  2716|         case 0x3E: // DS
  2717|         case 0x64: // FS
  2718|         case 0x65: // GS
  2719|         case 0x66: // Operand-Size
  2720|         case 0x67: // Address-Size
  2721|         case 0xf0:
  2722|         case 0xf2: // REPNE/REPNZ
  2723|         case 0xf3:
  2724|             ip++;
  2725|             fContainsPrefix = TRUE;
  2726|             continue;
  2727|         case 0x40:
  2728|         case 0x41:
  2729|         case 0x42:
  2730|         case 0x43:
  2731|         case 0x44:
  2732|         case 0x45:
  2733|         case 0x46:
  2734|         case 0x47:
  2735|         case 0x48:
  2736|         case 0x49:
  2737|         case 0x4a:
  2738|         case 0x4b:
  2739|         case 0x4c:
  2740|         case 0x4d:
  2741|         case 0x4e:
  2742|         case 0x4f:
  2743|             rex = prefix;
  2744|             ip++;
  2745|             fContainsPrefix = TRUE;
  2746|             continue;
  2747|         default:
  2748|             break;
  2749|         }
  2750|     } while (0);
  2751|     BYTE opcode = *ip++;
  2752|     if (opcode == 0xcc)
  2753|         return -1;
  2754|     BYTE rex_b = 0;
  2755|     BYTE rex_x = 0;
  2756|     BYTE rex_r = 0;
  2757|     if (rex != NULL)
  2758|     {
  2759|         rex_b = (rex & 0x1);       // high bit to modrm r/m field or SIB base field or OPCODE reg field    -- Hmm, when which?
  2760|         rex_x = (rex & 0x2) >> 1;  // high bit to sib index field
  2761|         rex_r = (rex & 0x4) >> 2;  // high bit to modrm reg field
  2762|     }
  2763|     switch (opcode)
  2764|     {
  2765|     case 0xff:
  2766|     {
  2767|                  BYTE modrm = *ip++;
  2768|                  _ASSERT(modrm != NULL);
  2769|                  BYTE mod = (modrm & 0xC0) >> 6;
  2770|                  BYTE reg = (modrm & 0x38) >> 3;
  2771|                  BYTE rm  = (modrm & 0x07);
  2772|                  reg   |= (rex_r << 3);
  2773|                  rm    |= (rex_b << 3);
  2774|                  if ((reg < 2) || (reg > 5 && reg < 8) || (reg > 15)) {
  2775|                      _ASSERTE(!"Invalid opcode!");
  2776|                      return -1;
  2777|                  }
  2778|                  SHORT displace = -1;
  2779|                  switch (mod)
  2780|                  {
  2781|                  case 0:
  2782|                  case 1:
  2783|                  case 2:
  2784|                      if ((rm & 0x07) == 4) // we have an SIB byte following
  2785|                      {
  2786|                          BYTE sib   = *ip;
  2787|                          _ASSERT(sib != NULL);
  2788|                          BYTE base  = (sib & 0x07);
  2789|                          base  |= (rex_b << 3);
  2790|                          ip++;
  2791|                          if (mod == 0)
  2792|                          {
  2793|                              if ((base & 0x07) == 5)
  2794|                                  displace = 7;
  2795|                              else
  2796|                                  displace = 3;
  2797|                          }
  2798|                          else if (mod == 1)
  2799|                          {
  2800|                              displace = 4;
  2801|                          }
  2802|                          else // mod == 2
  2803|                          {
  2804|                              displace = 7;
  2805|                          }
  2806|                      }
  2807|                      else
  2808|                      {
  2809|                          if ((mod == 0) && ((rm & 0x07) == 5))
  2810|                          {
  2811|                              displace = 6;   // 1 byte opcode + 1 byte modrm + 4 byte displacement (signed)
  2812|                          }
  2813|                          else
  2814|                          {
  2815|                              if (mod == 0)
  2816|                                  displace = 2;
  2817|                              else if (mod == 1)
  2818|                                  displace = 3;
  2819|                              else // mod == 2
  2820|                                  displace = 6;
  2821|                          }
  2822|                      }
  2823|                      break;
  2824|                  case 3:
  2825|                  default:
  2826|                      displace = 2;
  2827|                  }
  2828|                  if (displace == -1)
  2829|                  {
  2830|                      _ASSERTE(!"GetCallInstructionLength() encountered unexpected call instruction");
  2831|                      return -1;
  2832|                  }
  2833|                  if (fContainsPrefix)
  2834|                      displace++;
  2835|                  if ((reg != 4) && (reg != 5))
  2836|                      return displace;
  2837|                  break;
  2838|     }
  2839|     case 0xe8:
  2840|     {
  2841|                  return 5 + (fContainsPrefix ? 1 : 0);
  2842|     }
  2843|     default:
  2844|         break;
  2845|     }
  2846|     _ASSERTE(!"Invalid opcode!");
  2847|     return -1;
  2848| #elif defined(TARGET_RISCV64)
  2849|     return MAX_INSTRUCTION_LENGTH;
  2850| #else
  2851| #error Platform not implemented
  2852| #endif
  2853| }
  2854| HRESULT CordbNativeCode::GetSigParserFromFunction(mdToken mdFunction, mdToken *pClass, SigParser &parser, SigParser &methodGenerics)
  2855| {
  2856|     HRESULT hr = S_OK;
  2857|     IMetaDataImport* pImport = m_pFunction->GetModule()->GetMetaDataImporter();
  2858|     RSExtSmartPtr<IMetaDataImport2> pImport2;
  2859|     IfFailRet(pImport->QueryInterface(IID_IMetaDataImport2, (void**)&pImport2));
  2860|     if (TypeFromToken(mdFunction) == mdtMemberRef)
  2861|     {
  2862|         PCCOR_SIGNATURE sig = 0;
  2863|         ULONG sigSize = 0;
  2864|         IfFailRet(pImport->GetMemberRefProps(mdFunction, pClass, NULL, 0, 0, &sig, &sigSize));
  2865|         parser = SigParser(sig, sigSize);
  2866|     }
  2867|     else if (TypeFromToken(mdFunction) == mdtMethodDef)
  2868|     {
  2869|         PCCOR_SIGNATURE sig = 0;
  2870|         ULONG sigSize = 0;
  2871|         IfFailRet(pImport->GetMethodProps(mdFunction, pClass, NULL, 0, NULL, NULL, &sig, &sigSize, NULL, NULL));
  2872|         parser = SigParser(sig, sigSize);
  2873|     }
  2874|     else if (TypeFromToken(mdFunction) == mdtMethodSpec)
  2875|     {
  2876|         PCCOR_SIGNATURE sig = 0;
  2877|         ULONG sigSize = 0;
  2878|         mdToken parentToken = 0;
  2879|         IfFailRet(pImport2->GetMethodSpecProps(mdFunction, &parentToken, &sig, &sigSize));
  2880|         methodGenerics = SigParser(sig, sigSize);
  2881|         if (pClass)
  2882|             *pClass = parentToken;
  2883|         return GetSigParserFromFunction(parentToken, pClass, parser, methodGenerics);
  2884|     }
  2885|     else
  2886|     {
  2887|         return E_UNEXPECTED;
  2888|     }
  2889|     return S_OK;
  2890| }
  2891| HRESULT CordbNativeCode::EnsureReturnValueAllowed(Instantiation *currentInstantiation, mdToken targetClass, SigParser &parser, SigParser &methodGenerics)
  2892| {
  2893|     HRESULT hr = S_OK;
  2894|     uint32_t genCount = 0;
  2895|     IfFailRet(SkipToReturn(parser, &genCount));
  2896|     return EnsureReturnValueAllowedWorker(currentInstantiation, targetClass, parser, methodGenerics, genCount);
  2897| }
  2898| HRESULT CordbNativeCode::EnsureReturnValueAllowedWorker(Instantiation *currentInstantiation, mdToken targetClass, SigParser &parser, SigParser &methodGenerics, ULONG genCount)
  2899| {
  2900|     SigParser original(parser);
  2901|     HRESULT hr = S_OK;
  2902|     CorElementType returnType;
  2903|     IfFailRet(parser.GetElemType(&returnType));
  2904|     if (returnType == ELEMENT_TYPE_GENERICINST)
  2905|     {
  2906|         IfFailRet(parser.GetElemType(&returnType));
  2907|         if (returnType == ELEMENT_TYPE_CLASS)
  2908|             return S_OK;
  2909|         if (returnType != ELEMENT_TYPE_VALUETYPE)
  2910|             return META_E_BAD_SIGNATURE;
  2911|         if (currentInstantiation == NULL)
  2912|             return S_OK;  // We will check again when we have the instantiation.
  2913|         NewArrayHolder<CordbType*> types;
  2914|         Instantiation inst;
  2915|         IfFailRet(CordbJITILFrame::BuildInstantiationForCallsite(GetModule(), types, inst, currentInstantiation, targetClass, SigParser(methodGenerics)));
  2916|         CordbType *pType = 0;
  2917|         IfFailRet(CordbType::SigToType(GetModule(), &original, &inst, &pType));
  2918|         IfFailRet(pType->ReturnedByValue());
  2919|         if (hr == S_OK) // not S_FALSE
  2920|             return S_OK;
  2921|         return CORDBG_E_UNSUPPORTED;
  2922|     }
  2923|     if (returnType == ELEMENT_TYPE_VALUETYPE)
  2924|     {
  2925|         Instantiation inst;
  2926|         CordbType *pType = 0;
  2927|         IfFailRet(CordbType::SigToType(GetModule(), &original, &inst, &pType));
  2928|         IfFailRet(pType->ReturnedByValue());
  2929|         if (hr == S_OK) // not S_FALSE
  2930|             return S_OK;
  2931|         return CORDBG_E_UNSUPPORTED;
  2932|     }
  2933|     if (returnType == ELEMENT_TYPE_TYPEDBYREF)
  2934|         return CORDBG_E_UNSUPPORTED;
  2935|     if (returnType == ELEMENT_TYPE_VOID)
  2936|         return E_UNEXPECTED;
  2937|     if (returnType == ELEMENT_TYPE_MVAR)
  2938|     {
  2939|         uint32_t genParam = 0;
  2940|         IfFailRet(parser.GetData(&genParam));
  2941|         uint32_t callingConv = 0;
  2942|         IfFailRet(methodGenerics.GetCallingConvInfo(&callingConv));
  2943|         if (callingConv != IMAGE_CEE_CS_CALLCONV_GENERICINST)
  2944|             return META_E_BAD_SIGNATURE;
  2945|         SigParser generics(methodGenerics);     // Make a copy since operations are destructive.
  2946|         uint32_t maxCount = 0;
  2947|         IfFailRet(generics.GetData(&maxCount));
  2948|         if (maxCount <= genParam || genParam > 1024)
  2949|             return META_E_BAD_SIGNATURE;
  2950|         while (genParam--)
  2951|             IfFailRet(generics.SkipExactlyOne());
  2952|         return EnsureReturnValueAllowedWorker(currentInstantiation, targetClass, generics, methodGenerics, genCount);
  2953|     }
  2954|     if (returnType == ELEMENT_TYPE_VAR)
  2955|     {
  2956|         uint32_t typeParam = 0;
  2957|         parser.GetData(&typeParam);
  2958|         if (typeParam > 1024)
  2959|             return META_E_BAD_SIGNATURE;
  2960|         IMetaDataImport *pImport = m_pFunction->GetModule()->GetMetaDataImporter();
  2961|         PCCOR_SIGNATURE sig;
  2962|         ULONG countSig;
  2963|         IfFailRet(pImport->GetTypeSpecFromToken(targetClass, &sig, &countSig));
  2964|         SigParser typeParser(sig, countSig);
  2965|         CorElementType et;
  2966|         IfFailRet(typeParser.GetElemType(&et));
  2967|         if (et != ELEMENT_TYPE_GENERICINST)
  2968|             return META_E_BAD_SIGNATURE;
  2969|         IfFailRet(typeParser.GetElemType(&et));
  2970|         if (et != ELEMENT_TYPE_VALUETYPE && et != ELEMENT_TYPE_CLASS)
  2971|             return META_E_BAD_SIGNATURE;
  2972|         IfFailRet(typeParser.GetToken(NULL));
  2973|         uint32_t totalTypeCount = 0;
  2974|         IfFailRet(typeParser.GetData(&totalTypeCount));
  2975|         if (totalTypeCount < typeParam)
  2976|             return META_E_BAD_SIGNATURE;
  2977|         while (typeParam--)
  2978|             IfFailRet(typeParser.SkipExactlyOne());
  2979|         IfFailRet(typeParser.PeekElemType(&et));
  2980|         if (et == ELEMENT_TYPE_VAR)
  2981|             return E_FAIL;
  2982|         return EnsureReturnValueAllowedWorker(currentInstantiation, targetClass, typeParser, methodGenerics, genCount);
  2983|     }
  2984|     return S_OK;
  2985| }
  2986| HRESULT CordbNativeCode::SkipToReturn(SigParser &parser, uint32_t *genCount)
  2987| {
  2988|     HRESULT hr = S_OK;
  2989|     uint32_t uCallConv;
  2990|     IfFailRet(parser.GetCallingConvInfo(&uCallConv));
  2991|     if ((uCallConv == IMAGE_CEE_CS_CALLCONV_FIELD) || (uCallConv == IMAGE_CEE_CS_CALLCONV_LOCAL_SIG))
  2992|         return META_E_BAD_SIGNATURE;
  2993|     if (uCallConv & IMAGE_CEE_CS_CALLCONV_GENERIC)
  2994|         IfFailRet(parser.GetData(genCount));
  2995|     IfFailRet(parser.GetData(NULL));
  2996|     return S_OK;
  2997| }
  2998| HRESULT CordbNativeCode::GetCallSignature(ULONG32 ILoffset, mdToken *pClass, mdToken *pFunction, SigParser &parser, SigParser &generics)
  2999| {
  3000|     CordbILCode *pCode = this->m_pFunction->GetILCode();
  3001|     BYTE buffer[3];
  3002|     ULONG32 fetched = 0;
  3003|     HRESULT hr = pCode->GetCode(ILoffset, ILoffset+ARRAY_SIZE(buffer), ARRAY_SIZE(buffer), buffer, &fetched);
  3004|     if (FAILED(hr))
  3005|         return hr;
  3006|     else if (fetched != ARRAY_SIZE(buffer))
  3007|         return CORDBG_E_INVALID_OPCODE;
  3008|     BYTE instruction = buffer[0];
  3009|     if (buffer[0] == 0xfe && buffer[1] == 0x14)
  3010|     {
  3011|         return CORDBG_E_INVALID_OPCODE;
  3012|     }
  3013|     if (instruction != 0x28 && instruction != 0x6f)
  3014|         return CORDBG_E_INVALID_OPCODE;
  3015|     mdToken mdFunction = 0;
  3016|     const ULONG32 offset = ILoffset + 1;
  3017|     hr = pCode->GetCode(offset, offset+sizeof(mdToken), sizeof(mdToken), (BYTE*)&mdFunction, &fetched);
  3018|     if (FAILED(hr) || fetched != sizeof(mdToken))
  3019|         return CORDBG_E_INVALID_OPCODE;
  3020|     if (pFunction)
  3021|         *pFunction = mdFunction;
  3022|     return GetSigParserFromFunction(mdFunction, pClass, parser, generics);
  3023| }
  3024| HRESULT CordbNativeCode::GetReturnValueLiveOffsetImpl(Instantiation *currentInstantiation, ULONG32 ILoffset, ULONG32 bufferSize, ULONG32 *pFetched, ULONG32 *pOffsets)
  3025| {
  3026|     if (pFetched == NULL)
  3027|         return E_INVALIDARG;
  3028|     HRESULT hr = S_OK;
  3029|     ULONG32 found = 0;
  3030|     SigParser signature, generics;
  3031|     mdToken mdClass = 0;
  3032|     IfFailRet(GetCallSignature(ILoffset, &mdClass, NULL, signature, generics));
  3033|     IfFailRet(EnsureReturnValueAllowed(currentInstantiation, mdClass, signature, generics));
  3034|     SequencePoints *pSP = GetSequencePoints();
  3035|     DebuggerILToNativeMap *pMap = pSP->GetCallsiteMapAddr();
  3036|     for (ULONG32 i = 0; i < pSP->GetCallsiteEntryCount() && pMap; ++i, pMap++)
  3037|     {
  3038|         if (pMap->ilOffset == ILoffset && (pMap->source & ICorDebugInfo::CALL_INSTRUCTION) == ICorDebugInfo::CALL_INSTRUCTION)
  3039|         {
  3040|             if (pOffsets && found < bufferSize)
  3041|             {
  3042|                 BYTE nativeBuffer[8];
  3043|                 ULONG32 fetched = 0;
  3044|                 IfFailRet(GetCode(pMap->nativeStartOffset, pMap->nativeStartOffset+ARRAY_SIZE(nativeBuffer), ARRAY_SIZE(nativeBuffer), nativeBuffer, &fetched));
  3045|                 int skipBytes = 0;
  3046| #if defined(PSEUDORANDOM_NOP_INSERTION)
  3047|                 const BYTE nop_opcode = 0x90;
  3048|                 while (fetched && nativeBuffer[0] == nop_opcode)
  3049|                 {
  3050|                     skipBytes++;
  3051|                     for (int j = 1; j < ARRAY_SIZE(nativeBuffer) && nativeBuffer[j] == nop_opcode; ++j)
  3052|                         skipBytes++;
  3053|                     IfFailRet(GetCode(pMap->nativeStartOffset+skipBytes, pMap->nativeStartOffset+skipBytes+ARRAY_SIZE(nativeBuffer), ARRAY_SIZE(nativeBuffer), nativeBuffer, &fetched));
  3054|                 }
  3055| #endif
  3056|                 int offset = GetCallInstructionLength(nativeBuffer, fetched);
  3057|                 if (offset == -1)
  3058|                     return E_UNEXPECTED; // Could not decode instruction, this should never happen.
  3059|                 pOffsets[found] = pMap->nativeStartOffset + offset + skipBytes;
  3060|             }
  3061|             found++;
  3062|         }
  3063|     }
  3064|     if (pOffsets)
  3065|         *pFetched = found < bufferSize ? found : bufferSize;
  3066|     else
  3067|         *pFetched = found;
  3068|     if (found == 0)
  3069|         return E_FAIL;
  3070|     if (pOffsets && found > bufferSize)
  3071|         return S_FALSE;
  3072|     return S_OK;
  3073| }
  3074| CordbNativeCode * CordbModule::LookupOrCreateNativeCode(mdMethodDef methodToken,
  3075|                                                         VMPTR_MethodDesc methodDesc,
  3076|                                                         CORDB_ADDRESS startAddress)
  3077| {
  3078|     INTERNAL_SYNC_API_ENTRY(GetProcess());
  3079|     _ASSERTE(startAddress != NULL);
  3080|     _ASSERTE(methodDesc != VMPTR_MethodDesc::NullPtr());
  3081|     CordbNativeCode * pNativeCode = NULL;
  3082|     NativeCodeFunctionData codeInfo;
  3083|     RSLockHolder lockHolder(GetProcess()->GetProcessLock());
  3084|     pNativeCode = m_nativeCodeTable.GetBase((UINT_PTR) startAddress);
  3085|     if (pNativeCode == NULL)
  3086|     {
  3087|         GetProcess()->GetDAC()->GetNativeCodeInfoForAddr(methodDesc, startAddress, &codeInfo);
  3088|         LOG((LF_CORDB,
  3089|              LL_INFO10000,
  3090|              "R:CT::RSCreating code w/ ver:0x%x, md:0x%x, nativeStart=0x%08x, nativeSize=0x%08x\n",
  3091|              codeInfo.encVersion,
  3092|              VmPtrToCookie(codeInfo.vmNativeCodeMethodDescToken),
  3093|              codeInfo.m_rgCodeRegions[kHot].pAddress,
  3094|              codeInfo.m_rgCodeRegions[kHot].cbSize));
  3095|         CordbFunction* pFunction = CordbModule::LookupOrCreateFunction(methodToken, codeInfo.encVersion);
  3096|         _ASSERTE(pFunction != NULL);
  3097|         pFunction->InitParentClassOfFunction();
  3098|         pNativeCode = new (nothrow)CordbNativeCode(pFunction, &codeInfo, codeInfo.isInstantiatedGeneric != 0);
  3099|         _ASSERTE(pNativeCode != NULL);
  3100|         m_nativeCodeTable.AddBaseOrThrow(pNativeCode);
  3101|     }
  3102|     return pNativeCode;
  3103| } // CordbNativeCode::LookupOrCreateFromJITData
  3104| void CordbNativeCode::LoadNativeInfo()
  3105| {
  3106|     THROW_IF_NEUTERED(this);
  3107|     INTERNAL_API_ENTRY(this->GetProcess());
  3108|     if(m_nativeVarData.IsInitialized())
  3109|     {
  3110|         return;
  3111|     }
  3112|     if (GetFunction()->IsNativeImpl() == CordbFunction::kNativeOnly)
  3113|     {
  3114|         ThrowHR(CORDBG_E_FUNCTION_NOT_IL);
  3115|     }
  3116|      CordbProcess *pProcess = GetProcess();
  3117|     if (m_fCodeAvailable)
  3118|     {
  3119|         RSLockHolder lockHolder(pProcess->GetProcessLock());
  3120|         pProcess->GetDAC()->GetNativeCodeSequencePointsAndVarInfo(GetVMNativeCodeMethodDescToken(),
  3121|                                                                   GetAddress(),
  3122|                                                                   m_fCodeAvailable,
  3123|                                                                   &m_nativeVarData,
  3124|                                                                   &m_sequencePoints);
  3125|     }
  3126| } // CordbNativeCode::LoadNativeInfo


# ====================================================================
# FILE: src/coreclr/gc/gc.cpp
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-42652 ---
     1| #include "gcpriv.h"
     2| #if defined(TARGET_AMD64) && defined(TARGET_WINDOWS)
     3| #define USE_VXSORT
     4| #else
     5| #define USE_INTROSORT
     6| #endif
     7| #ifdef DACCESS_COMPILE
     8| #error this source file should not be compiled with DACCESS_COMPILE!
     9| #endif //DACCESS_COMPILE
    10| class gc_rand
    11| {
    12| public:
    13|     static uint64_t x;
    14|     static uint64_t get_rand()
    15|     {
    16|         x = (314159269*x+278281) & 0x7FFFFFFF;
    17|         return x;
    18|     }
    19|     static uint64_t get_rand(uint64_t r) {
    20|         uint64_t x = (uint64_t)((get_rand() * r) >> 31);
    21|         return x;
    22|     }
    23| };
    24| uint64_t gc_rand::x = 0;
    25| #if defined(BACKGROUND_GC) && defined(FEATURE_EVENT_TRACE)
    26| BOOL bgc_heap_walk_for_etw_p = FALSE;
    27| #endif //BACKGROUND_GC && FEATURE_EVENT_TRACE
    28| #define MAX_PTR ((uint8_t*)(~(ptrdiff_t)0))
    29| #define commit_min_th (16*OS_PAGE_SIZE)
    30| #define MIN_SOH_CROSS_GEN_REFS (400)
    31| #define MIN_LOH_CROSS_GEN_REFS (800)
    32| #ifdef SERVER_GC
    33| #define partial_size_th 100
    34| #define num_partial_refs 64
    35| #else //SERVER_GC
    36| #define partial_size_th 100
    37| #define num_partial_refs 32
    38| #endif //SERVER_GC
    39| #ifdef USE_REGIONS
    40| #define demotion_pinned_ratio_th (1)
    41| #define sip_surv_ratio_th (90)
    42| #define sip_old_card_surv_ratio_th (90)
    43| #else
    44| #define demotion_plug_len_th (6*1024*1024)
    45| #endif //USE_REGIONS
    46| #ifdef HOST_64BIT
    47| #define MARK_STACK_INITIAL_LENGTH 1024
    48| #else
    49| #define MARK_STACK_INITIAL_LENGTH 128
    50| #endif // HOST_64BIT
    51| #define LOH_PIN_QUEUE_LENGTH 100
    52| #define LOH_PIN_DECAY 10
    53| #define UOH_ALLOCATION_RETRY_MAX_COUNT 2
    54| #define MAX_YP_SPIN_COUNT_UNIT 32768
    55| uint32_t yp_spin_count_unit = 0;
    56| uint32_t original_spin_count_unit = 0;
    57| size_t loh_size_threshold = LARGE_OBJECT_SIZE;
    58| #ifdef GC_CONFIG_DRIVEN
    59| int compact_ratio = 0;
    60| #endif //GC_CONFIG_DRIVEN
    61| #ifdef FEATURE_SVR_GC
    62| bool g_built_with_svr_gc = true;
    63| #else
    64| bool g_built_with_svr_gc = false;
    65| #endif // FEATURE_SVR_GC
    66| #if defined(BUILDENV_DEBUG)
    67| uint8_t g_build_variant = 0;
    68| #elif defined(BUILDENV_CHECKED)
    69| uint8_t g_build_variant = 1;
    70| #else
    71| uint8_t g_build_variant = 2;
    72| #endif //BUILDENV_DEBUG
    73| VOLATILE(int32_t) g_no_gc_lock = -1;
    74| #ifdef TRACE_GC
    75| const char * const allocation_state_str[] = {
    76|     "start",
    77|     "can_allocate",
    78|     "cant_allocate",
    79|     "retry_allocate",
    80|     "try_fit",
    81|     "try_fit_new_seg",
    82|     "try_fit_after_cg",
    83|     "try_fit_after_bgc",
    84|     "try_free_full_seg_in_bgc",
    85|     "try_free_after_bgc",
    86|     "try_seg_end",
    87|     "acquire_seg",
    88|     "acquire_seg_after_cg",
    89|     "acquire_seg_after_bgc",
    90|     "check_and_wait_for_bgc",
    91|     "trigger_full_compact_gc",
    92|     "trigger_ephemeral_gc",
    93|     "trigger_2nd_ephemeral_gc",
    94|     "check_retry_seg"
    95| };
    96| const char * const msl_take_state_str[] = {
    97|     "get_large_seg",
    98|     "bgc_loh_sweep",
    99|     "wait_bgc",
   100|     "block_gc",
   101|     "clr_mem",
   102|     "clr_large_mem",
   103|     "t_eph_gc",
   104|     "t_full_gc",
   105|     "alloc_small",
   106|     "alloc_large",
   107|     "alloc_small_cant",
   108|     "alloc_large_cant",
   109|     "try_alloc",
   110|     "try_budget"
   111| };
   112| #endif //TRACE_GC
   113| #if (defined(DT_LOG) || defined(TRACE_GC))
   114| static const char* const str_gc_reasons[] =
   115| {
   116|     "alloc_soh",
   117|     "induced",
   118|     "lowmem",
   119|     "empty",
   120|     "alloc_loh",
   121|     "oos_soh",
   122|     "oos_loh",
   123|     "induced_noforce",
   124|     "gcstress",
   125|     "induced_lowmem",
   126|     "induced_compacting",
   127|     "lowmemory_host",
   128|     "pm_full_gc",
   129|     "lowmemory_host_blocking"
   130| };
   131| static const char* const str_gc_pause_modes[] =
   132| {
   133|     "batch",
   134|     "interactive",
   135|     "low_latency",
   136|     "sustained_low_latency",
   137|     "no_gc"
   138| };
   139| static const char* const str_root_kinds[] = {
   140|     "Stack",
   141|     "FinalizeQueue",
   142|     "Handles",
   143|     "OlderGen",
   144|     "SizedRef",
   145|     "Overflow",
   146|     "DependentHandles",
   147|     "NewFQ",
   148|     "Steal",
   149|     "BGC"
   150| };
   151| #endif //DT_LOG || TRACE_GC
   152| inline
   153| BOOL is_induced (gc_reason reason)
   154| {
   155|     return ((reason == reason_induced) ||
   156|             (reason == reason_induced_noforce) ||
   157|             (reason == reason_lowmemory) ||
   158|             (reason == reason_lowmemory_blocking) ||
   159|             (reason == reason_induced_compacting) ||
   160|             (reason == reason_induced_aggressive) ||
   161|             (reason == reason_lowmemory_host) ||
   162|             (reason == reason_lowmemory_host_blocking));
   163| }
   164| inline
   165| BOOL is_induced_blocking (gc_reason reason)
   166| {
   167|     return ((reason == reason_induced) ||
   168|             (reason == reason_lowmemory_blocking) ||
   169|             (reason == reason_induced_compacting) ||
   170|             (reason == reason_induced_aggressive) ||
   171|             (reason == reason_lowmemory_host_blocking));
   172| }
   173| gc_oh_num gen_to_oh(int gen)
   174| {
   175|     switch (gen)
   176|     {
   177|         case soh_gen0:
   178|             return gc_oh_num::soh;
   179|         case soh_gen1:
   180|             return gc_oh_num::soh;
   181|         case soh_gen2:
   182|             return gc_oh_num::soh;
   183|         case loh_generation:
   184|             return gc_oh_num::loh;
   185|         case poh_generation:
   186|             return gc_oh_num::poh;
   187|         default:
   188|             assert(false);
   189|             return gc_oh_num::unknown;
   190|     }
   191| }
   192| uint64_t qpf;
   193| double qpf_ms;
   194| double qpf_us;
   195| uint64_t GetHighPrecisionTimeStamp()
   196| {
   197|     int64_t ts = GCToOSInterface::QueryPerformanceCounter();
   198|     return (uint64_t)((double)ts * qpf_us);
   199| }
   200| uint64_t RawGetHighPrecisionTimeStamp()
   201| {
   202|     return (uint64_t)GCToOSInterface::QueryPerformanceCounter();
   203| }
   204| #ifdef BGC_SERVO_TUNING
   205| bool gc_heap::bgc_tuning::enable_fl_tuning = false;
   206| uint32_t gc_heap::bgc_tuning::memory_load_goal = 0;
   207| uint32_t gc_heap::bgc_tuning::memory_load_goal_slack = 0;
   208| uint64_t gc_heap::bgc_tuning::available_memory_goal = 0;
   209| bool gc_heap::bgc_tuning::panic_activated_p = false;
   210| double gc_heap::bgc_tuning::accu_error_panic = 0.0;
   211| double gc_heap::bgc_tuning::above_goal_kp = 0.0;
   212| double gc_heap::bgc_tuning::above_goal_ki = 0.0;
   213| bool gc_heap::bgc_tuning::enable_kd = false;
   214| bool gc_heap::bgc_tuning::enable_ki = false;
   215| bool gc_heap::bgc_tuning::enable_smooth = false;
   216| bool gc_heap::bgc_tuning::enable_tbh = false;
   217| bool gc_heap::bgc_tuning::enable_ff = false;
   218| bool gc_heap::bgc_tuning::enable_gradual_d = false;
   219| double gc_heap::bgc_tuning::above_goal_kd = 0.0;
   220| double gc_heap::bgc_tuning::above_goal_ff = 0.0;
   221| double gc_heap::bgc_tuning::num_gen1s_smooth_factor = 0.0;
   222| double gc_heap::bgc_tuning::ml_kp = 0.0;
   223| double gc_heap::bgc_tuning::ml_ki = 0.0;
   224| double gc_heap::bgc_tuning::accu_error = 0.0;
   225| bool gc_heap::bgc_tuning::fl_tuning_triggered = false;
   226| size_t gc_heap::bgc_tuning::num_bgcs_since_tuning_trigger = 0;
   227| bool gc_heap::bgc_tuning::next_bgc_p = false;
   228| size_t gc_heap::bgc_tuning::gen1_index_last_bgc_end;
   229| size_t gc_heap::bgc_tuning::gen1_index_last_bgc_start;
   230| size_t gc_heap::bgc_tuning::gen1_index_last_bgc_sweep;
   231| size_t gc_heap::bgc_tuning::actual_num_gen1s_to_trigger;
   232| gc_heap::bgc_tuning::tuning_calculation gc_heap::bgc_tuning::gen_calc[2];
   233| gc_heap::bgc_tuning::tuning_stats gc_heap::bgc_tuning::gen_stats[2];
   234| gc_heap::bgc_tuning::bgc_size_data gc_heap::bgc_tuning::current_bgc_end_data[2];
   235| size_t gc_heap::bgc_tuning::last_stepping_bgc_count = 0;
   236| uint32_t gc_heap::bgc_tuning::last_stepping_mem_load = 0;
   237| uint32_t gc_heap::bgc_tuning::stepping_interval = 0;
   238| bool gc_heap::bgc_tuning::use_stepping_trigger_p = true;
   239| double gc_heap::bgc_tuning::gen2_ratio_correction = 0.0;
   240| double gc_heap::bgc_tuning::ratio_correction_step = 0.0;
   241| int gc_heap::saved_bgc_tuning_reason = -1;
   242| #endif //BGC_SERVO_TUNING
   243| inline
   244| size_t round_up_power2 (size_t size)
   245| {
   246|     DWORD highest_set_bit_index;
   247|     if (0 ==
   248| #ifdef HOST_64BIT
   249|         BitScanReverse64(
   250| #else
   251|         BitScanReverse(
   252| #endif
   253|             &highest_set_bit_index, size - 1)) { return 1; }
   254|     return static_cast<size_t>(2) << highest_set_bit_index;
   255| }
   256| inline
   257| size_t round_down_power2 (size_t size)
   258| {
   259|     DWORD highest_set_bit_index;
   260|     if (0 ==
   261| #ifdef HOST_64BIT
   262|         BitScanReverse64(
   263| #else
   264|         BitScanReverse(
   265| #endif
   266|             &highest_set_bit_index, size)) { return 0; }
   267|     return static_cast<size_t>(1) << highest_set_bit_index;
   268| }
   269| inline
   270| int index_of_highest_set_bit (size_t value)
   271| {
   272|     DWORD highest_set_bit_index;
   273|     return (0 ==
   274| #ifdef HOST_64BIT
   275|         BitScanReverse64(
   276| #else
   277|         BitScanReverse(
   278| #endif
   279|             &highest_set_bit_index, value)) ? -1 : static_cast<int>(highest_set_bit_index);
   280| }
   281| inline
   282| int relative_index_power2_plug (size_t power2)
   283| {
   284|     int index = index_of_highest_set_bit (power2);
   285|     assert (index <= MAX_INDEX_POWER2);
   286|     return ((index < MIN_INDEX_POWER2) ? 0 : (index - MIN_INDEX_POWER2));
   287| }
   288| inline
   289| int relative_index_power2_free_space (size_t power2)
   290| {
   291|     int index = index_of_highest_set_bit (power2);
   292|     assert (index <= MAX_INDEX_POWER2);
   293|     return ((index < MIN_INDEX_POWER2) ? -1 : (index - MIN_INDEX_POWER2));
   294| }
   295| #ifdef BACKGROUND_GC
   296| uint32_t bgc_alloc_spin_count = 140;
   297| uint32_t bgc_alloc_spin_count_loh = 16;
   298| uint32_t bgc_alloc_spin = 2;
   299| inline
   300| void c_write (uint32_t& place, uint32_t value)
   301| {
   302|     Interlocked::Exchange (&place, value);
   303| }
   304| const size_t bgc_min_per_heap = 4*1024*1024;
   305| int gc_heap::gchist_index = 0;
   306| gc_mechanisms_store gc_heap::gchist[max_history_count];
   307| #ifndef MULTIPLE_HEAPS
   308| VOLATILE(bgc_state) gc_heap::current_bgc_state = bgc_not_in_process;
   309| int gc_heap::gchist_index_per_heap = 0;
   310| gc_heap::gc_history gc_heap::gchist_per_heap[max_history_count];
   311| #endif //MULTIPLE_HEAPS
   312| #endif //BACKGROUND_GC
   313| void gc_heap::add_to_history_per_heap()
   314| {
   315| #if defined(GC_HISTORY) && defined(BACKGROUND_GC)
   316|     gc_history* current_hist = &gchist_per_heap[gchist_index_per_heap];
   317|     current_hist->gc_index = settings.gc_index;
   318|     current_hist->current_bgc_state = current_bgc_state;
   319|     size_t elapsed = dd_gc_elapsed_time (dynamic_data_of (0));
   320|     current_hist->gc_time_ms = (uint32_t)(elapsed / 1000);
   321|     current_hist->gc_efficiency = (elapsed ? (total_promoted_bytes / elapsed) : total_promoted_bytes);
   322| #ifndef USE_REGIONS
   323|     current_hist->eph_low = generation_allocation_start (generation_of (max_generation - 1));
   324|     current_hist->gen0_start = generation_allocation_start (generation_of (0));
   325|     current_hist->eph_high = heap_segment_allocated (ephemeral_heap_segment);
   326| #endif //!USE_REGIONS
   327| #ifdef BACKGROUND_GC
   328|     current_hist->bgc_lowest = background_saved_lowest_address;
   329|     current_hist->bgc_highest = background_saved_highest_address;
   330| #endif //BACKGROUND_GC
   331|     current_hist->fgc_lowest = lowest_address;
   332|     current_hist->fgc_highest = highest_address;
   333|     current_hist->g_lowest = g_gc_lowest_address;
   334|     current_hist->g_highest = g_gc_highest_address;
   335|     gchist_index_per_heap++;
   336|     if (gchist_index_per_heap == max_history_count)
   337|     {
   338|         gchist_index_per_heap = 0;
   339|     }
   340| #endif //GC_HISTORY && BACKGROUND_GC
   341| }
   342| void gc_heap::add_to_history()
   343| {
   344| #if defined(GC_HISTORY) && defined(BACKGROUND_GC)
   345|     gc_mechanisms_store* current_settings = &gchist[gchist_index];
   346|     current_settings->store (&settings);
   347|     gchist_index++;
   348|     if (gchist_index == max_history_count)
   349|     {
   350|         gchist_index = 0;
   351|     }
   352| #endif //GC_HISTORY && BACKGROUND_GC
   353| }
   354| #if defined(TRACE_GC) && defined(SIMPLE_DPRINTF)
   355| BOOL   gc_log_on = TRUE;
   356| FILE* gc_log = NULL;
   357| size_t gc_log_file_size = 0;
   358| size_t gc_buffer_index = 0;
   359| size_t max_gc_buffers = 0;
   360| static CLRCriticalSection gc_log_lock;
   361| #define gc_log_buffer_size (1024*1024)
   362| uint8_t* gc_log_buffer = 0;
   363| size_t gc_log_buffer_offset = 0;
   364| void flush_gc_log (bool close)
   365| {
   366|     if (gc_log_on && (gc_log != NULL))
   367|     {
   368|         fwrite(gc_log_buffer, gc_log_buffer_offset, 1, gc_log);
   369|         fflush(gc_log);
   370|         if (close)
   371|         {
   372|             fclose(gc_log);
   373|             gc_log_on = false;
   374|             gc_log = NULL;
   375|         }
   376|         gc_log_buffer_offset = 0;
   377|     }
   378| }
   379| void log_va_msg(const char *fmt, va_list args)
   380| {
   381|     gc_log_lock.Enter();
   382|     const int BUFFERSIZE = 4096;
   383|     static char rgchBuffer[BUFFERSIZE];
   384|     char *  pBuffer  = &rgchBuffer[0];
   385|     pBuffer[0] = '\n';
   386|     int buffer_start = 1;
   387|     int pid_len = sprintf_s (&pBuffer[buffer_start], BUFFERSIZE - buffer_start,
   388|         "[%5d]", (uint32_t)GCToOSInterface::GetCurrentThreadIdForLogging());
   389|     buffer_start += pid_len;
   390|     memset(&pBuffer[buffer_start], '-', BUFFERSIZE - buffer_start);
   391|     int msg_len = _vsnprintf_s (&pBuffer[buffer_start], BUFFERSIZE - buffer_start, _TRUNCATE, fmt, args);
   392|     if (msg_len == -1)
   393|     {
   394|         msg_len = BUFFERSIZE - buffer_start;
   395|     }
   396|     msg_len += buffer_start;
   397|     if ((gc_log_buffer_offset + msg_len) > (gc_log_buffer_size - 12))
   398|     {
   399|         char index_str[8];
   400|         memset (index_str, '-', 8);
   401|         sprintf_s (index_str, ARRAY_SIZE(index_str), "%d", (int)gc_buffer_index);
   402|         gc_log_buffer[gc_log_buffer_offset] = '\n';
   403|         memcpy (gc_log_buffer + (gc_log_buffer_offset + 1), index_str, 8);
   404|         gc_buffer_index++;
   405|         if (gc_buffer_index > max_gc_buffers)
   406|         {
   407|             fseek (gc_log, 0, SEEK_SET);
   408|             gc_buffer_index = 0;
   409|         }
   410|         fwrite(gc_log_buffer, gc_log_buffer_size, 1, gc_log);
   411|         fflush(gc_log);
   412|         memset (gc_log_buffer, '*', gc_log_buffer_size);
   413|         gc_log_buffer_offset = 0;
   414|     }
   415|     memcpy (gc_log_buffer + gc_log_buffer_offset, pBuffer, msg_len);
   416|     gc_log_buffer_offset += msg_len;
   417|     gc_log_lock.Leave();
   418| }
   419| void GCLog (const char *fmt, ... )
   420| {
   421|     if (gc_log_on && (gc_log != NULL))
   422|     {
   423|         va_list     args;
   424|         va_start(args, fmt);
   425|         log_va_msg (fmt, args);
   426|         va_end(args);
   427|     }
   428| }
   429| #endif //TRACE_GC && SIMPLE_DPRINTF
   430| #ifdef GC_CONFIG_DRIVEN
   431| BOOL   gc_config_log_on = FALSE;
   432| FILE* gc_config_log = NULL;
   433| #define gc_config_log_buffer_size (1*1024) // TEMP
   434| uint8_t* gc_config_log_buffer = 0;
   435| size_t gc_config_log_buffer_offset = 0;
   436| void log_va_msg_config(const char *fmt, va_list args)
   437| {
   438|     const int BUFFERSIZE = 256;
   439|     static char rgchBuffer[BUFFERSIZE];
   440|     char *  pBuffer  = &rgchBuffer[0];
   441|     pBuffer[0] = '\n';
   442|     int buffer_start = 1;
   443|     int msg_len = _vsnprintf_s (&pBuffer[buffer_start], BUFFERSIZE - buffer_start, _TRUNCATE, fmt, args );
   444|     assert (msg_len != -1);
   445|     msg_len += buffer_start;
   446|     if ((gc_config_log_buffer_offset + msg_len) > gc_config_log_buffer_size)
   447|     {
   448|         fwrite(gc_config_log_buffer, gc_config_log_buffer_offset, 1, gc_config_log);
   449|         fflush(gc_config_log);
   450|         gc_config_log_buffer_offset = 0;
   451|     }
   452|     memcpy (gc_config_log_buffer + gc_config_log_buffer_offset, pBuffer, msg_len);
   453|     gc_config_log_buffer_offset += msg_len;
   454| }
   455| void GCLogConfig (const char *fmt, ... )
   456| {
   457|     if (gc_config_log_on && (gc_config_log != NULL))
   458|     {
   459|         va_list     args;
   460|         va_start( args, fmt );
   461|         log_va_msg_config (fmt, args);
   462|     }
   463| }
   464| #endif // GC_CONFIG_DRIVEN
   465| void GCHeap::Shutdown()
   466| {
   467| #if defined(TRACE_GC) && defined(SIMPLE_DPRINTF) && !defined(BUILD_AS_STANDALONE)
   468|     flush_gc_log (true);
   469| #endif //TRACE_GC && SIMPLE_DPRINTF && !BUILD_AS_STANDALONE
   470| }
   471| #ifdef SYNCHRONIZATION_STATS
   472| static unsigned int         gc_count_during_log;
   473| static const unsigned int   log_interval = 5000;
   474| static uint64_t             log_start_tick;
   475| static unsigned int         gc_lock_contended;
   476| static int64_t              log_start_hires;
   477| static uint64_t             suspend_ee_during_log;
   478| static uint64_t             restart_ee_during_log;
   479| static uint64_t             gc_during_log;
   480| #endif //SYNCHRONIZATION_STATS
   481| void
   482| init_sync_log_stats()
   483| {
   484| #ifdef SYNCHRONIZATION_STATS
   485|     if (gc_count_during_log == 0)
   486|     {
   487|         gc_heap::init_sync_stats();
   488|         suspend_ee_during_log = 0;
   489|         restart_ee_during_log = 0;
   490|         gc_during_log = 0;
   491|         gc_lock_contended = 0;
   492|         log_start_tick = GCToOSInterface::GetLowPrecisionTimeStamp();
   493|         log_start_hires = GCToOSInterface::QueryPerformanceCounter();
   494|     }
   495|     gc_count_during_log++;
   496| #endif //SYNCHRONIZATION_STATS
   497| }
   498| void
   499| process_sync_log_stats()
   500| {
   501| #ifdef SYNCHRONIZATION_STATS
   502|     uint64_t log_elapsed = GCToOSInterface::GetLowPrecisionTimeStamp() - log_start_tick;
   503|     if (log_elapsed > log_interval)
   504|     {
   505|         uint64_t total = GCToOSInterface::QueryPerformanceCounter() - log_start_hires;
   506|         printf("\n_________________________________________________________________________________\n"
   507|             "Past %d(s): #%3d GCs; Total gc_lock contended: %8u; GC: %12u\n"
   508|             "SuspendEE: %8u; RestartEE: %8u GC %.3f%%\n",
   509|             log_interval / 1000,
   510|             gc_count_during_log,
   511|             gc_lock_contended,
   512|             (unsigned int)(gc_during_log / gc_count_during_log),
   513|             (unsigned int)(suspend_ee_during_log / gc_count_during_log),
   514|             (unsigned int)(restart_ee_during_log / gc_count_during_log),
   515|             (double)(100.0f * gc_during_log / total));
   516|         gc_heap::print_sync_stats(gc_count_during_log);
   517|         gc_count_during_log = 0;
   518|     }
   519| #endif //SYNCHRONIZATION_STATS
   520| }
   521| #ifdef MULTIPLE_HEAPS
   522| uint32_t g_num_active_processors = 0;
   523| enum gc_join_stage
   524| {
   525|     gc_join_init_cpu_mapping = 0,
   526|     gc_join_done = 1,
   527|     gc_join_generation_determined = 2,
   528|     gc_join_begin_mark_phase = 3,
   529|     gc_join_scan_dependent_handles = 4,
   530|     gc_join_rescan_dependent_handles = 5,
   531|     gc_join_scan_sizedref_done = 6,
   532|     gc_join_null_dead_short_weak = 7,
   533|     gc_join_scan_finalization = 8,
   534|     gc_join_null_dead_long_weak = 9,
   535|     gc_join_null_dead_syncblk = 10,
   536|     gc_join_decide_on_compaction = 11,
   537|     gc_join_rearrange_segs_compaction = 12,
   538|     gc_join_adjust_handle_age_compact = 13,
   539|     gc_join_adjust_handle_age_sweep = 14,
   540|     gc_join_begin_relocate_phase = 15,
   541|     gc_join_relocate_phase_done = 16,
   542|     gc_join_verify_objects_done = 17,
   543|     gc_join_start_bgc = 18,
   544|     gc_join_restart_ee = 19,
   545|     gc_join_concurrent_overflow = 20,
   546|     gc_join_suspend_ee = 21,
   547|     gc_join_bgc_after_ephemeral = 22,
   548|     gc_join_allow_fgc = 23,
   549|     gc_join_bgc_sweep = 24,
   550|     gc_join_suspend_ee_verify = 25,
   551|     gc_join_restart_ee_verify = 26,
   552|     gc_join_set_state_free = 27,
   553|     gc_r_join_update_card_bundle = 28,
   554|     gc_join_after_absorb = 29,
   555|     gc_join_verify_copy_table = 30,
   556|     gc_join_after_reset = 31,
   557|     gc_join_after_ephemeral_sweep = 32,
   558|     gc_join_after_profiler_heap_walk = 33,
   559|     gc_join_minimal_gc = 34,
   560|     gc_join_after_commit_soh_no_gc = 35,
   561|     gc_join_expand_loh_no_gc = 36,
   562|     gc_join_final_no_gc = 37,
   563|     gc_join_disable_software_write_watch = 38,
   564|     gc_join_merge_temp_fl = 39,
   565|     gc_join_max = 40
   566| };
   567| enum gc_join_flavor
   568| {
   569|     join_flavor_server_gc = 0,
   570|     join_flavor_bgc = 1
   571| };
   572| #define first_thread_arrived 2
   573| #pragma warning(push)
   574| #pragma warning(disable:4324) // don't complain if DECLSPEC_ALIGN actually pads
   575| struct DECLSPEC_ALIGN(HS_CACHE_LINE_SIZE) join_structure
   576| {
   577|     int n_threads;
   578|     DECLSPEC_ALIGN(HS_CACHE_LINE_SIZE)
   579|     GCEvent joined_event[3]; // the last event in the array is only used for first_thread_arrived.
   580|     Volatile<int> lock_color;
   581|     VOLATILE(BOOL) wait_done;
   582|     VOLATILE(BOOL) joined_p;
   583|     DECLSPEC_ALIGN(HS_CACHE_LINE_SIZE)
   584|     VOLATILE(int) join_lock;
   585|     VOLATILE(int) r_join_lock;
   586| };
   587| #pragma warning(pop)
   588| enum join_type
   589| {
   590|     type_last_join = 0,
   591|     type_join = 1,
   592|     type_restart = 2,
   593|     type_first_r_join = 3,
   594|     type_r_join = 4
   595| };
   596| enum join_time
   597| {
   598|     time_start = 0,
   599|     time_end = 1
   600| };
   601| enum join_heap_index
   602| {
   603|     join_heap_restart = 100,
   604|     join_heap_r_restart = 200
   605| };
   606| class t_join
   607| {
   608|     join_structure join_struct;
   609|     int id;
   610|     gc_join_flavor flavor;
   611| #ifdef JOIN_STATS
   612|     uint64_t start[MAX_SUPPORTED_CPUS], end[MAX_SUPPORTED_CPUS], start_seq;
   613|     int thd;
   614|     uint64_t start_tick;
   615|     uint64_t elapsed_total[gc_join_max], wake_total[gc_join_max], seq_loss_total[gc_join_max], par_loss_total[gc_join_max], in_join_total[gc_join_max];
   616| #endif //JOIN_STATS
   617| public:
   618|     BOOL init (int n_th, gc_join_flavor f)
   619|     {
   620|         dprintf (JOIN_LOG, ("Initializing join structure"));
   621|         join_struct.n_threads = n_th;
   622|         join_struct.lock_color = 0;
   623|         for (int i = 0; i < 3; i++)
   624|         {
   625|             if (!join_struct.joined_event[i].IsValid())
   626|             {
   627|                 join_struct.joined_p = FALSE;
   628|                 dprintf (JOIN_LOG, ("Creating join event %d", i));
   629|                 if (!join_struct.joined_event[i].CreateManualEventNoThrow(FALSE))
   630|                     return FALSE;
   631|             }
   632|         }
   633|         join_struct.join_lock = join_struct.n_threads;
   634|         join_struct.r_join_lock = join_struct.n_threads;
   635|         join_struct.wait_done = FALSE;
   636|         flavor = f;
   637| #ifdef JOIN_STATS
   638|         start_tick = GCToOSInterface::GetLowPrecisionTimeStamp();
   639| #endif //JOIN_STATS
   640|         return TRUE;
   641|     }
   642|     void update_n_threads(int n_th)
   643|     {
   644|         join_struct.n_threads = n_th;
   645|         join_struct.join_lock = n_th;
   646|         join_struct.r_join_lock = n_th;
   647|     }
   648|     int get_num_threads()
   649|     {
   650|         return join_struct.n_threads;
   651|     }
   652|     void destroy ()
   653|     {
   654|         dprintf (JOIN_LOG, ("Destroying join structure"));
   655|         for (int i = 0; i < 3; i++)
   656|         {
   657|             if (join_struct.joined_event[i].IsValid())
   658|                 join_struct.joined_event[i].CloseEvent();
   659|         }
   660|     }
   661|     inline void fire_event (int heap, join_time time, join_type type, int join_id)
   662|     {
   663|         FIRE_EVENT(GCJoin_V2, heap, time, type, join_id);
   664|     }
   665|     void join (gc_heap* gch, int join_id)
   666|     {
   667| #ifdef JOIN_STATS
   668|         end[gch->heap_number] = get_ts();
   669| #endif //JOIN_STATS
   670|         assert (!join_struct.joined_p);
   671|         int color = join_struct.lock_color.LoadWithoutBarrier();
   672|         if (Interlocked::Decrement(&join_struct.join_lock) != 0)
   673|         {
   674|             dprintf (JOIN_LOG, ("join%d(%d): Join() Waiting...join_lock is now %d",
   675|                 flavor, join_id, (int32_t)(join_struct.join_lock)));
   676|             fire_event (gch->heap_number, time_start, type_join, join_id);
   677|             if (color == join_struct.lock_color.LoadWithoutBarrier())
   678|             {
   679| respin:
   680|                 int spin_count = 128 * yp_spin_count_unit;
   681|                 for (int j = 0; j < spin_count; j++)
   682|                 {
   683|                     if (color != join_struct.lock_color.LoadWithoutBarrier())
   684|                     {
   685|                         break;
   686|                     }
   687|                     YieldProcessor();           // indicate to the processor that we are spinning
   688|                 }
   689|                 if (color == join_struct.lock_color.LoadWithoutBarrier())
   690|                 {
   691|                     dprintf (JOIN_LOG, ("join%d(%d): Join() hard wait on reset event %d, join_lock is now %d",
   692|                         flavor, join_id, color, (int32_t)(join_struct.join_lock)));
   693|                     uint32_t dwJoinWait = join_struct.joined_event[color].Wait(INFINITE, FALSE);
   694|                     if (dwJoinWait != WAIT_OBJECT_0)
   695|                     {
   696|                         STRESS_LOG1 (LF_GC, LL_FATALERROR, "joined event wait failed with code: %zx", dwJoinWait);
   697|                         FATAL_GC_ERROR ();
   698|                     }
   699|                 }
   700|                 if (color == join_struct.lock_color.LoadWithoutBarrier())
   701|                 {
   702|                     dprintf (9999, ("---h%d %d j%d %d - respin!!! (c:%d-%d)",
   703|                         gch->heap_number, join_id, join_struct.n_threads, color, join_struct.lock_color.LoadWithoutBarrier()));
   704|                     goto respin;
   705|                 }
   706|                 dprintf (JOIN_LOG, ("join%d(%d): Join() done, join_lock is %d",
   707|                     flavor, join_id, (int32_t)(join_struct.join_lock)));
   708|             }
   709|             fire_event (gch->heap_number, time_end, type_join, join_id);
   710| #ifdef JOIN_STATS
   711|             start[gch->heap_number] = get_ts();
   712|             Interlocked::ExchangeAdd(&in_join_total[join_id], (start[gch->heap_number] - end[gch->heap_number]));
   713| #endif //JOIN_STATS
   714|         }
   715|         else
   716|         {
   717|             fire_event (gch->heap_number, time_start, type_last_join, join_id);
   718|             join_struct.joined_p = TRUE;
   719|             dprintf (JOIN_LOG, ("join%d(%d): Last thread to complete the join, setting id", flavor, join_id));
   720|             join_struct.joined_event[!color].Reset();
   721|             id = join_id;
   722| #ifdef JOIN_STATS
   723|             thd = gch->heap_number;
   724|             start_seq = get_ts();
   725|             Interlocked::ExchangeAdd(&in_join_total[join_id], (start_seq - end[gch->heap_number]));
   726| #endif //JOIN_STATS
   727|         }
   728|     }
   729|     BOOL r_join (gc_heap* gch, int join_id)
   730|     {
   731|         if (join_struct.n_threads == 1)
   732|         {
   733|             return TRUE;
   734|         }
   735|         if (Interlocked::CompareExchange(&join_struct.r_join_lock, 0, join_struct.n_threads) == 0)
   736|         {
   737|             fire_event (gch->heap_number, time_start, type_join, join_id);
   738|             dprintf (JOIN_LOG, ("r_join() Waiting..."));
   739| respin:
   740|             int spin_count = 256 * yp_spin_count_unit;
   741|             for (int j = 0; j < spin_count; j++)
   742|             {
   743|                 if (join_struct.wait_done)
   744|                 {
   745|                     break;
   746|                 }
   747|                 YieldProcessor();           // indicate to the processor that we are spinning
   748|             }
   749|             if (!join_struct.wait_done)
   750|             {
   751|                 dprintf (JOIN_LOG, ("Join() hard wait on reset event %d", first_thread_arrived));
   752|                 uint32_t dwJoinWait = join_struct.joined_event[first_thread_arrived].Wait(INFINITE, FALSE);
   753|                 if (dwJoinWait != WAIT_OBJECT_0)
   754|                 {
   755|                     STRESS_LOG1 (LF_GC, LL_FATALERROR, "joined event wait failed with code: %zx", dwJoinWait);
   756|                     FATAL_GC_ERROR ();
   757|                 }
   758|             }
   759|             if (!join_struct.wait_done)
   760|             {
   761|                 goto respin;
   762|             }
   763|             dprintf (JOIN_LOG, ("r_join() done"));
   764|             fire_event (gch->heap_number, time_end, type_join, join_id);
   765|             return FALSE;
   766|         }
   767|         else
   768|         {
   769|             fire_event (gch->heap_number, time_start, type_first_r_join, join_id);
   770|             return TRUE;
   771|         }
   772|     }
   773| #ifdef JOIN_STATS
   774|     uint64_t get_ts()
   775|     {
   776|         return GCToOSInterface::QueryPerformanceCounter();
   777|     }
   778|     void start_ts (gc_heap* gch)
   779|     {
   780|         start[gch->heap_number] = get_ts();
   781|     }
   782| #endif //JOIN_STATS
   783|     void restart()
   784|     {
   785| #ifdef JOIN_STATS
   786|         uint64_t elapsed_seq = get_ts() - start_seq;
   787|         uint64_t max = 0, sum = 0, wake = 0;
   788|         uint64_t min_ts = start[0];
   789|         for (int i = 1; i < join_struct.n_threads; i++)
   790|         {
   791|             if(min_ts > start[i]) min_ts = start[i];
   792|         }
   793|         for (int i = 0; i < join_struct.n_threads; i++)
   794|         {
   795|             uint64_t wake_delay = start[i] - min_ts;
   796|             uint64_t elapsed = end[i] - start[i];
   797|             if (max < elapsed)
   798|                 max = elapsed;
   799|             sum += elapsed;
   800|             wake += wake_delay;
   801|         }
   802|         uint64_t seq_loss = (join_struct.n_threads - 1)*elapsed_seq;
   803|         uint64_t par_loss = join_struct.n_threads*max - sum;
   804|         double efficiency = 0.0;
   805|         if (max > 0)
   806|             efficiency = sum*100.0/(join_struct.n_threads*max);
   807|         const double ts_scale = 1e-6;
   808|         elapsed_total[id] += sum;
   809|         wake_total[id] += wake;
   810|         seq_loss_total[id] += seq_loss;
   811|         par_loss_total[id] += par_loss;
   812|         if (GCToOSInterface::GetLowPrecisionTimeStamp() - start_tick > 10*1000)
   813|         {
   814|             printf("**** summary *****\n");
   815|             for (int i = 0; i < 16; i++)
   816|             {
   817|                 printf("join #%3d  elapsed_total = %8g wake_loss = %8g seq_loss = %8g  par_loss = %8g  in_join_total = %8g\n",
   818|                    i,
   819|                    ts_scale*elapsed_total[i],
   820|                    ts_scale*wake_total[i],
   821|                    ts_scale*seq_loss_total[i],
   822|                    ts_scale*par_loss_total[i],
   823|                    ts_scale*in_join_total[i]);
   824|                 elapsed_total[i] = wake_total[i] = seq_loss_total[i] = par_loss_total[i] = in_join_total[i] = 0;
   825|             }
   826|             start_tick = GCToOSInterface::GetLowPrecisionTimeStamp();
   827|         }
   828| #endif //JOIN_STATS
   829|         fire_event (join_heap_restart, time_start, type_restart, -1);
   830|         assert (join_struct.joined_p);
   831|         join_struct.joined_p = FALSE;
   832|         join_struct.join_lock = join_struct.n_threads;
   833|         dprintf (JOIN_LOG, ("join%d(%d): Restarting from join: join_lock is %d", flavor, id, (int32_t)(join_struct.join_lock)));
   834|         int color = join_struct.lock_color.LoadWithoutBarrier();
   835|         join_struct.lock_color = !color;
   836|         join_struct.joined_event[color].Set();
   837|         fire_event (join_heap_restart, time_end, type_restart, -1);
   838| #ifdef JOIN_STATS
   839|         start[thd] = get_ts();
   840| #endif //JOIN_STATS
   841|     }
   842|     BOOL joined()
   843|     {
   844|         dprintf (JOIN_LOG, ("join%d(%d): joined, join_lock is %d", flavor, id, (int32_t)(join_struct.join_lock)));
   845|         return join_struct.joined_p;
   846|     }
   847|     void r_restart()
   848|     {
   849|         if (join_struct.n_threads != 1)
   850|         {
   851|             fire_event (join_heap_r_restart, time_start, type_restart, -1);
   852|             join_struct.wait_done = TRUE;
   853|             join_struct.joined_event[first_thread_arrived].Set();
   854|             fire_event (join_heap_r_restart, time_end, type_restart, -1);
   855|         }
   856|     }
   857|     void r_init()
   858|     {
   859|         if (join_struct.n_threads != 1)
   860|         {
   861|             join_struct.r_join_lock = join_struct.n_threads;
   862|             join_struct.wait_done = FALSE;
   863|             join_struct.joined_event[first_thread_arrived].Reset();
   864|         }
   865|     }
   866| };
   867| t_join gc_t_join;
   868| #ifdef BACKGROUND_GC
   869| t_join bgc_t_join;
   870| #endif //BACKGROUND_GC
   871| #endif //MULTIPLE_HEAPS
   872| #define spin_and_switch(count_to_spin, expr) \
   873| { \
   874|     for (int j = 0; j < count_to_spin; j++) \
   875|     { \
   876|         if (expr) \
   877|         { \
   878|             break;\
   879|         } \
   880|         YieldProcessor(); \
   881|     } \
   882|     if (!(expr)) \
   883|     { \
   884|         GCToOSInterface::YieldThread(0); \
   885|     } \
   886| }
   887| #define spin_and_wait(count_to_spin, expr) \
   888| { \
   889|     while (!expr) \
   890|     { \
   891|         for (int j = 0; j < count_to_spin; j++) \
   892|         { \
   893|             if (expr) \
   894|             { \
   895|                 break; \
   896|             } \
   897|                 YieldProcessor (); \
   898|         } \
   899|         if (!(expr)) \
   900|         { \
   901|             GCToOSInterface::YieldThread (0); \
   902|         } \
   903|     } \
   904| }
   905| #ifdef BACKGROUND_GC
   906| #define max_pending_allocs 64
   907| class exclusive_sync
   908| {
   909|     VOLATILE(uint8_t*) rwp_object;
   910|     VOLATILE(int32_t) needs_checking;
   911|     int spin_count;
   912|     uint8_t cache_separator[HS_CACHE_LINE_SIZE - (sizeof (spin_count) + sizeof (needs_checking) + sizeof (rwp_object))];
   913|     VOLATILE(uint8_t*) alloc_objects[max_pending_allocs];
   914|     int find_free_index ()
   915|     {
   916|         for (int i = 0; i < max_pending_allocs; i++)
   917|         {
   918|             if (alloc_objects [i] == (uint8_t*)0)
   919|             {
   920|                 return i;
   921|             }
   922|         }
   923|         return -1;
   924|     }
   925| public:
   926|     void init()
   927|     {
   928|         spin_count = 32 * (g_num_processors - 1);
   929|         rwp_object = 0;
   930|         needs_checking = 0;
   931|         for (int i = 0; i < max_pending_allocs; i++)
   932|         {
   933|             alloc_objects [i] = (uint8_t*)0;
   934|         }
   935|     }
   936|     void check()
   937|     {
   938|         for (int i = 0; i < max_pending_allocs; i++)
   939|         {
   940|             if (alloc_objects [i] != (uint8_t*)0)
   941|             {
   942|                 FATAL_GC_ERROR();
   943|             }
   944|         }
   945|     }
   946|     void bgc_mark_set (uint8_t* obj)
   947|     {
   948|         dprintf (3, ("cm: probing %p", obj));
   949| retry:
   950|         if (Interlocked::CompareExchange(&needs_checking, 1, 0) == 0)
   951|         {
   952|             for (int i = 0; i < max_pending_allocs; i++)
   953|             {
   954|                 if (obj == alloc_objects[i])
   955|                 {
   956|                     needs_checking = 0;
   957|                     dprintf (3, ("cm: will spin"));
   958|                     spin_and_switch (spin_count, (obj != alloc_objects[i]));
   959|                     goto retry;
   960|                 }
   961|             }
   962|             rwp_object = obj;
   963|             needs_checking = 0;
   964|             dprintf (3, ("cm: set %p", obj));
   965|             return;
   966|         }
   967|         else
   968|         {
   969|             spin_and_switch (spin_count, (needs_checking == 0));
   970|             goto retry;
   971|         }
   972|     }
   973|     int uoh_alloc_set (uint8_t* obj)
   974|     {
   975|         if (!gc_heap::cm_in_progress)
   976|         {
   977|             return -1;
   978|         }
   979| retry:
   980|         dprintf (3, ("uoh alloc: probing %p", obj));
   981|         if (Interlocked::CompareExchange(&needs_checking, 1, 0) == 0)
   982|         {
   983|             if (obj == rwp_object)
   984|             {
   985|                 needs_checking = 0;
   986|                 spin_and_switch (spin_count, (obj != rwp_object));
   987|                 goto retry;
   988|             }
   989|             else
   990|             {
   991|                 int cookie = find_free_index();
   992|                 if (cookie != -1)
   993|                 {
   994|                     alloc_objects[cookie] = obj;
   995|                     needs_checking = 0;
   996|                     dprintf (3, ("uoh alloc: set %p at %d", obj, cookie));
   997|                     return cookie;
   998|                 }
   999|                 else
  1000|                 {
  1001|                     needs_checking = 0;
  1002|                     dprintf (3, ("uoh alloc: setting %p will spin to acquire a free index", obj));
  1003|                     spin_and_switch (spin_count, (find_free_index () != -1));
  1004|                     goto retry;
  1005|                 }
  1006|             }
  1007|         }
  1008|         else
  1009|         {
  1010|             dprintf (3, ("uoh alloc: will spin on checking %p", obj));
  1011|             spin_and_switch (spin_count, (needs_checking == 0));
  1012|             goto retry;
  1013|         }
  1014|     }
  1015|     void bgc_mark_done ()
  1016|     {
  1017|         dprintf (3, ("cm: release lock on %p", (uint8_t *)rwp_object));
  1018|         rwp_object = 0;
  1019|     }
  1020|     void uoh_alloc_done_with_index (int index)
  1021|     {
  1022|         dprintf (3, ("uoh alloc: release lock on %p based on %d", (uint8_t *)alloc_objects[index], index));
  1023|         assert ((index >= 0) && (index < max_pending_allocs));
  1024|         alloc_objects[index] = (uint8_t*)0;
  1025|     }
  1026|     void uoh_alloc_done (uint8_t* obj)
  1027|     {
  1028|         if (!gc_heap::cm_in_progress)
  1029|         {
  1030|             return;
  1031|         }
  1032|         for (int i = 0; i < max_pending_allocs; i++)
  1033|         {
  1034|             if (alloc_objects [i] == obj)
  1035|             {
  1036|                 uoh_alloc_done_with_index(i);
  1037|                 return;
  1038|             }
  1039|         }
  1040|         dprintf (3, ("uoh alloc: could not release lock on %p", obj));
  1041|     }
  1042| };
  1043| #endif //BACKGROUND_GC
  1044| void reset_memory (uint8_t* o, size_t sizeo);
  1045| #ifdef WRITE_WATCH
  1046| #ifndef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  1047| static bool virtual_alloc_hardware_write_watch = false;
  1048| #endif // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  1049| static bool hardware_write_watch_capability = false;
  1050| void hardware_write_watch_api_supported()
  1051| {
  1052|     if (GCToOSInterface::SupportsWriteWatch())
  1053|     {
  1054|         hardware_write_watch_capability = true;
  1055|         dprintf (2, ("WriteWatch supported"));
  1056|     }
  1057|     else
  1058|     {
  1059|         dprintf (2,("WriteWatch not supported"));
  1060|     }
  1061| }
  1062| inline bool can_use_hardware_write_watch()
  1063| {
  1064|     return hardware_write_watch_capability;
  1065| }
  1066| inline bool can_use_write_watch_for_gc_heap()
  1067| {
  1068| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  1069|     return true;
  1070| #else // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  1071|     return can_use_hardware_write_watch();
  1072| #endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  1073| }
  1074| inline bool can_use_write_watch_for_card_table()
  1075| {
  1076| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  1077|     return true;
  1078| #else
  1079|     return can_use_hardware_write_watch();
  1080| #endif
  1081| }
  1082| #else
  1083| #define mem_reserve (MEM_RESERVE)
  1084| #endif //WRITE_WATCH
  1085| void WaitLongerNoInstru (int i)
  1086| {
  1087|     bool bToggleGC = GCToEEInterface::EnablePreemptiveGC();
  1088|     if (g_fSuspensionPending == 0)
  1089|     {
  1090|         if  (g_num_processors > 1)
  1091|         {
  1092|             YieldProcessor();           // indicate to the processor that we are spinning
  1093|             if  (i & 0x01f)
  1094|                 GCToOSInterface::YieldThread (0);
  1095|             else
  1096|                 GCToOSInterface::Sleep (5);
  1097|         }
  1098|         else
  1099|             GCToOSInterface::Sleep (5);
  1100|     }
  1101|     if (bToggleGC)
  1102|     {
  1103| #ifdef _DEBUG
  1104|         if (gc_heap::gc_started)
  1105|         {
  1106|             gc_heap::wait_for_gc_done();
  1107|         }
  1108| #endif // _DEBUG
  1109|         GCToEEInterface::DisablePreemptiveGC();
  1110|     }
  1111|     else if (g_fSuspensionPending > 0)
  1112|     {
  1113|         g_theGCHeap->WaitUntilGCComplete();
  1114|     }
  1115| }
  1116| inline
  1117| static void safe_switch_to_thread()
  1118| {
  1119|     bool cooperative_mode = gc_heap::enable_preemptive();
  1120|     GCToOSInterface::YieldThread(0);
  1121|     gc_heap::disable_preemptive(cooperative_mode);
  1122| }
  1123| #define check_msl_status(msg, size) if (msl_status == msl_retry_different_heap) \
  1124|     { \
  1125|         dprintf (5555, ("h%d RETRY %s(%Id)", heap_number, msg, size)); \
  1126|         return a_state_retry_allocate; \
  1127|     }
  1128| static const int32_t lock_free = -1;
  1129| static const int32_t lock_taken = 0;
  1130| static const int32_t lock_decommissioned = 1;
  1131| bool gc_heap::should_move_heap (GCSpinLock* msl)
  1132| {
  1133| #ifdef MULTIPLE_HEAPS
  1134|     if (msl->lock == lock_decommissioned)
  1135|     {
  1136|         dprintf (5555, ("heap#%d got decommissioned! need to retry", heap_number));
  1137|     }
  1138|     return (msl->lock == lock_decommissioned);
  1139| #else //MULTIPLE_HEAPS
  1140|     return false;
  1141| #endif //MULTIPLE_HEAPS
  1142| }
  1143| enter_msl_status gc_heap::enter_spin_lock_msl_helper (GCSpinLock* msl)
  1144| {
  1145|     do
  1146|     {
  1147| #ifdef DYNAMIC_HEAP_COUNT
  1148|         uint64_t start = GetHighPrecisionTimeStamp();
  1149| #endif //DYNAMIC_HEAP_COUNT
  1150|         unsigned int i = 0;
  1151|         while (VolatileLoad (&msl->lock) != lock_free)
  1152|         {
  1153|             if (should_move_heap (msl))
  1154|             {
  1155|                 return msl_retry_different_heap;
  1156|             }
  1157|             if ((++i & 7) && !IsGCInProgress ())
  1158|             {
  1159|                 if (g_num_processors > 1)
  1160|                 {
  1161| #ifndef MULTIPLE_HEAPS
  1162|                     int spin_count = 32 * yp_spin_count_unit;
  1163| #else //!MULTIPLE_HEAPS
  1164|                     int spin_count = yp_spin_count_unit;
  1165| #endif //!MULTIPLE_HEAPS
  1166|                     for (int j = 0; j < spin_count; j++)
  1167|                     {
  1168|                         if (VolatileLoad (&msl->lock) == lock_free || IsGCInProgress ())
  1169|                             break;
  1170|                         YieldProcessor ();           // indicate to the processor that we are spinning
  1171|                     }
  1172|                     if (VolatileLoad (&msl->lock) != lock_free && !IsGCInProgress ())
  1173|                     {
  1174| #ifdef DYNAMIC_HEAP_COUNT
  1175|                         start -= GetHighPrecisionTimeStamp();
  1176| #endif //DYNAMIC_HEAP_COUNT
  1177|                         safe_switch_to_thread ();
  1178| #ifdef DYNAMIC_HEAP_COUNT
  1179|                         start += GetHighPrecisionTimeStamp();
  1180| #endif //DYNAMIC_HEAP_COUNT
  1181|                     }
  1182|                 }
  1183|                 else
  1184|                 {
  1185|                     safe_switch_to_thread ();
  1186|                 }
  1187|             }
  1188|             else
  1189|             {
  1190| #ifdef DYNAMIC_HEAP_COUNT
  1191|                 start -= GetHighPrecisionTimeStamp();
  1192| #endif //DYNAMIC_HEAP_COUNT
  1193|                 WaitLongerNoInstru (i);
  1194| #ifdef DYNAMIC_HEAP_COUNT
  1195|                 start += GetHighPrecisionTimeStamp();
  1196| #endif //DYNAMIC_HEAP_COUNT
  1197|             }
  1198|         }
  1199| #ifdef DYNAMIC_HEAP_COUNT
  1200|         uint64_t end = GetHighPrecisionTimeStamp();
  1201|         Interlocked::ExchangeAdd64 (&msl->msl_wait_time, end - start);
  1202|         dprintf (3, ("h%d wait for msl lock wait time %zd, total wait time: %zd", heap_number, (end - start), msl->msl_wait_time));
  1203| #endif //DYNAMIC_HEAP_COUNT
  1204|     }
  1205|     while (Interlocked::CompareExchange (&msl->lock, lock_taken, lock_free) != lock_free);
  1206|     return msl_entered;
  1207| }
  1208| inline
  1209| enter_msl_status gc_heap::enter_spin_lock_msl (GCSpinLock* msl)
  1210| {
  1211|     if (Interlocked::CompareExchange (&msl->lock, lock_taken, lock_free) == lock_free)
  1212|         return msl_entered;
  1213|     return enter_spin_lock_msl_helper (msl);
  1214| }
  1215| inline
  1216| static void enter_spin_lock_noinstru (RAW_KEYWORD(volatile) int32_t* lock)
  1217| {
  1218| retry:
  1219|     if (Interlocked::CompareExchange(lock, lock_taken, lock_free) != lock_free)
  1220|     {
  1221|         unsigned int i = 0;
  1222|         while (VolatileLoad(lock) != lock_free)
  1223|         {
  1224|             assert (VolatileLoad(lock) != lock_decommissioned);
  1225|             if ((++i & 7) && !IsGCInProgress())
  1226|             {
  1227|                 if  (g_num_processors > 1)
  1228|                 {
  1229| #ifndef MULTIPLE_HEAPS
  1230|                     int spin_count = 32 * yp_spin_count_unit;
  1231| #else //!MULTIPLE_HEAPS
  1232|                     int spin_count = yp_spin_count_unit;
  1233| #endif //!MULTIPLE_HEAPS
  1234|                     for (int j = 0; j < spin_count; j++)
  1235|                     {
  1236|                         if  (VolatileLoad(lock) == lock_free || IsGCInProgress())
  1237|                             break;
  1238|                         YieldProcessor();           // indicate to the processor that we are spinning
  1239|                     }
  1240|                     if  (VolatileLoad(lock) != lock_free && !IsGCInProgress())
  1241|                     {
  1242|                         safe_switch_to_thread();
  1243|                     }
  1244|                 }
  1245|                 else
  1246|                 {
  1247|                     safe_switch_to_thread();
  1248|                 }
  1249|             }
  1250|             else
  1251|             {
  1252|                 WaitLongerNoInstru(i);
  1253|             }
  1254|         }
  1255|         goto retry;
  1256|     }
  1257| }
  1258| inline
  1259| static BOOL try_enter_spin_lock_noinstru(RAW_KEYWORD(volatile) int32_t* lock)
  1260| {
  1261|     return (Interlocked::CompareExchange(&*lock, lock_taken, lock_free) == lock_free);
  1262| }
  1263| inline
  1264| static void leave_spin_lock_noinstru (RAW_KEYWORD(volatile) int32_t* lock)
  1265| {
  1266|     VolatileStore<int32_t>((int32_t*)lock, lock_free);
  1267| }
  1268| #ifdef _DEBUG
  1269| inline
  1270| static void enter_spin_lock (GCSpinLock *pSpinLock)
  1271| {
  1272|     enter_spin_lock_noinstru (&pSpinLock->lock);
  1273|     assert (pSpinLock->holding_thread == (Thread*)-1);
  1274|     pSpinLock->holding_thread = GCToEEInterface::GetThread();
  1275| }
  1276| inline
  1277| static BOOL try_enter_spin_lock(GCSpinLock *pSpinLock)
  1278| {
  1279|     BOOL ret = try_enter_spin_lock_noinstru(&pSpinLock->lock);
  1280|     if (ret)
  1281|         pSpinLock->holding_thread = GCToEEInterface::GetThread();
  1282|     return ret;
  1283| }
  1284| inline
  1285| static void leave_spin_lock(GCSpinLock *pSpinLock)
  1286| {
  1287|     bool gc_thread_p = GCToEEInterface::WasCurrentThreadCreatedByGC();
  1288|     pSpinLock->released_by_gc_p = gc_thread_p;
  1289|     pSpinLock->holding_thread = (Thread*) -1;
  1290|     if (pSpinLock->lock != lock_free)
  1291|         leave_spin_lock_noinstru(&pSpinLock->lock);
  1292| }
  1293| #define ASSERT_HOLDING_SPIN_LOCK(pSpinLock) \
  1294|     _ASSERTE((pSpinLock)->holding_thread == GCToEEInterface::GetThread());
  1295| #define ASSERT_NOT_HOLDING_SPIN_LOCK(pSpinLock) \
  1296|     _ASSERTE((pSpinLock)->holding_thread != GCToEEInterface::GetThread());
  1297| #else //_DEBUG
  1298| void WaitLonger (int i
  1299| #ifdef SYNCHRONIZATION_STATS
  1300|     , GCSpinLock* spin_lock
  1301| #endif //SYNCHRONIZATION_STATS
  1302|     )
  1303| {
  1304| #ifdef SYNCHRONIZATION_STATS
  1305|     (spin_lock->num_wait_longer)++;
  1306| #endif //SYNCHRONIZATION_STATS
  1307|     bool bToggleGC = GCToEEInterface::EnablePreemptiveGC();
  1308|     assert (bToggleGC);
  1309|     if (!gc_heap::gc_started)
  1310|     {
  1311| #ifdef SYNCHRONIZATION_STATS
  1312|         (spin_lock->num_switch_thread_w)++;
  1313| #endif //SYNCHRONIZATION_STATS
  1314|         if  (g_num_processors > 1)
  1315|         {
  1316|             YieldProcessor();           // indicate to the processor that we are spinning
  1317|             if  (i & 0x01f)
  1318|                 GCToOSInterface::YieldThread (0);
  1319|             else
  1320|                 GCToOSInterface::Sleep (5);
  1321|         }
  1322|         else
  1323|             GCToOSInterface::Sleep (5);
  1324|     }
  1325|     if (gc_heap::gc_started)
  1326|     {
  1327|         gc_heap::wait_for_gc_done();
  1328|     }
  1329|     if (bToggleGC)
  1330|     {
  1331| #ifdef SYNCHRONIZATION_STATS
  1332|         (spin_lock->num_disable_preemptive_w)++;
  1333| #endif //SYNCHRONIZATION_STATS
  1334|         GCToEEInterface::DisablePreemptiveGC();
  1335|     }
  1336| }
  1337| inline
  1338| static void enter_spin_lock (GCSpinLock* spin_lock)
  1339| {
  1340| retry:
  1341|     if (Interlocked::CompareExchange(&spin_lock->lock, lock_taken, lock_free) != lock_free)
  1342|     {
  1343|         unsigned int i = 0;
  1344|         while (spin_lock->lock != lock_free)
  1345|         {
  1346|             assert (spin_lock->lock != lock_decommissioned);
  1347|             if ((++i & 7) && !gc_heap::gc_started)
  1348|             {
  1349|                 if  (g_num_processors > 1)
  1350|                 {
  1351| #ifndef MULTIPLE_HEAPS
  1352|                     int spin_count = 32 * yp_spin_count_unit;
  1353| #else //!MULTIPLE_HEAPS
  1354|                     int spin_count = yp_spin_count_unit;
  1355| #endif //!MULTIPLE_HEAPS
  1356|                     for (int j = 0; j < spin_count; j++)
  1357|                     {
  1358|                         if  (spin_lock->lock == lock_free || gc_heap::gc_started)
  1359|                             break;
  1360|                         YieldProcessor();           // indicate to the processor that we are spinning
  1361|                     }
  1362|                     if  (spin_lock->lock != lock_free && !gc_heap::gc_started)
  1363|                     {
  1364| #ifdef SYNCHRONIZATION_STATS
  1365|                         (spin_lock->num_switch_thread)++;
  1366| #endif //SYNCHRONIZATION_STATS
  1367|                         bool cooperative_mode = gc_heap::enable_preemptive ();
  1368|                         GCToOSInterface::YieldThread(0);
  1369|                         gc_heap::disable_preemptive (cooperative_mode);
  1370|                     }
  1371|                 }
  1372|                 else
  1373|                     GCToOSInterface::YieldThread(0);
  1374|             }
  1375|             else
  1376|             {
  1377|                 WaitLonger(i
  1378| #ifdef SYNCHRONIZATION_STATS
  1379|                         , spin_lock
  1380| #endif //SYNCHRONIZATION_STATS
  1381|                     );
  1382|             }
  1383|         }
  1384|         goto retry;
  1385|     }
  1386| }
  1387| inline
  1388| static BOOL try_enter_spin_lock(GCSpinLock* spin_lock)
  1389| {
  1390|     return (Interlocked::CompareExchange(&spin_lock->lock, lock_taken, lock_free) == lock_free);
  1391| }
  1392| inline
  1393| static void leave_spin_lock (GCSpinLock * spin_lock)
  1394| {
  1395|     spin_lock->lock = lock_free;
  1396| }
  1397| #define ASSERT_HOLDING_SPIN_LOCK(pSpinLock)
  1398| #endif //_DEBUG
  1399| bool gc_heap::enable_preemptive ()
  1400| {
  1401|     return GCToEEInterface::EnablePreemptiveGC();
  1402| }
  1403| void gc_heap::disable_preemptive (bool restore_cooperative)
  1404| {
  1405|     if (restore_cooperative)
  1406|     {
  1407|         GCToEEInterface::DisablePreemptiveGC();
  1408|     }
  1409| }
  1410| typedef void **  PTR_PTR;
  1411| inline
  1412| void memclr ( uint8_t* mem, size_t size)
  1413| {
  1414|     dprintf (3, ("MEMCLR: %p, %zd", mem, size));
  1415|     assert ((size & (sizeof(PTR_PTR)-1)) == 0);
  1416|     assert (sizeof(PTR_PTR) == DATA_ALIGNMENT);
  1417|     memset (mem, 0, size);
  1418| }
  1419| void memcopy (uint8_t* dmem, uint8_t* smem, size_t size)
  1420| {
  1421|     const size_t sz4ptr = sizeof(PTR_PTR)*4;
  1422|     const size_t sz2ptr = sizeof(PTR_PTR)*2;
  1423|     const size_t sz1ptr = sizeof(PTR_PTR)*1;
  1424|     assert ((size & (sizeof (PTR_PTR)-1)) == 0);
  1425|     assert (sizeof(PTR_PTR) == DATA_ALIGNMENT);
  1426|     if (size >= sz4ptr)
  1427|     {
  1428|         do
  1429|         {
  1430|             ((PTR_PTR)dmem)[0] = ((PTR_PTR)smem)[0];
  1431|             ((PTR_PTR)dmem)[1] = ((PTR_PTR)smem)[1];
  1432|             ((PTR_PTR)dmem)[2] = ((PTR_PTR)smem)[2];
  1433|             ((PTR_PTR)dmem)[3] = ((PTR_PTR)smem)[3];
  1434|             dmem += sz4ptr;
  1435|             smem += sz4ptr;
  1436|         }
  1437|         while ((size -= sz4ptr) >= sz4ptr);
  1438|     }
  1439|     if (size & sz2ptr)
  1440|     {
  1441|         ((PTR_PTR)dmem)[0] = ((PTR_PTR)smem)[0];
  1442|         ((PTR_PTR)dmem)[1] = ((PTR_PTR)smem)[1];
  1443|         dmem += sz2ptr;
  1444|         smem += sz2ptr;
  1445|     }
  1446|     if (size & sz1ptr)
  1447|     {
  1448|         ((PTR_PTR)dmem)[0] = ((PTR_PTR)smem)[0];
  1449|     }
  1450| }
  1451| inline
  1452| ptrdiff_t round_down (ptrdiff_t add, int pitch)
  1453| {
  1454|     return ((add / pitch) * pitch);
  1455| }
  1456| #if defined(FEATURE_STRUCTALIGN) && defined(RESPECT_LARGE_ALIGNMENT)
  1457| #error FEATURE_STRUCTALIGN should imply !RESPECT_LARGE_ALIGNMENT
  1458| #endif
  1459| #if defined(FEATURE_STRUCTALIGN) && defined(FEATURE_LOH_COMPACTION)
  1460| #error FEATURE_STRUCTALIGN and FEATURE_LOH_COMPACTION are mutually exclusive
  1461| #endif
  1462| inline
  1463| BOOL same_large_alignment_p (uint8_t* p1, uint8_t* p2)
  1464| {
  1465| #ifdef RESPECT_LARGE_ALIGNMENT
  1466|     const size_t LARGE_ALIGNMENT_MASK = 2 * DATA_ALIGNMENT - 1;
  1467|     return ((((size_t)p1 ^ (size_t)p2) & LARGE_ALIGNMENT_MASK) == 0);
  1468| #else
  1469|     UNREFERENCED_PARAMETER(p1);
  1470|     UNREFERENCED_PARAMETER(p2);
  1471|     return TRUE;
  1472| #endif // RESPECT_LARGE_ALIGNMENT
  1473| }
  1474| inline
  1475| size_t switch_alignment_size (BOOL already_padded_p)
  1476| {
  1477| #ifndef RESPECT_LARGE_ALIGNMENT
  1478|     assert (!"Should not be called");
  1479| #endif // RESPECT_LARGE_ALIGNMENT
  1480|     if (already_padded_p)
  1481|         return DATA_ALIGNMENT;
  1482|     else
  1483|         return Align (min_obj_size) | DATA_ALIGNMENT;
  1484| }
  1485| #ifdef FEATURE_STRUCTALIGN
  1486| void set_node_aligninfo (uint8_t *node, int requiredAlignment, ptrdiff_t pad);
  1487| void clear_node_aligninfo (uint8_t *node);
  1488| #else // FEATURE_STRUCTALIGN
  1489| #define node_realigned(node)    (((plug_and_reloc*)(node))[-1].reloc & 1)
  1490| void set_node_realigned (uint8_t* node);
  1491| void clear_node_realigned(uint8_t* node);
  1492| #endif // FEATURE_STRUCTALIGN
  1493| inline
  1494| size_t AlignQword (size_t nbytes)
  1495| {
  1496| #ifdef FEATURE_STRUCTALIGN
  1497|     return Align (nbytes);
  1498| #else // FEATURE_STRUCTALIGN
  1499|     return (nbytes + 7) & ~7;
  1500| #endif // FEATURE_STRUCTALIGN
  1501| }
  1502| inline
  1503| BOOL Aligned (size_t n)
  1504| {
  1505|     return (n & ALIGNCONST) == 0;
  1506| }
  1507| #define OBJECT_ALIGNMENT_OFFSET (sizeof(MethodTable *))
  1508| #ifdef FEATURE_STRUCTALIGN
  1509| #define MAX_STRUCTALIGN OS_PAGE_SIZE
  1510| #else // FEATURE_STRUCTALIGN
  1511| #define MAX_STRUCTALIGN 0
  1512| #endif // FEATURE_STRUCTALIGN
  1513| #ifdef FEATURE_STRUCTALIGN
  1514| inline
  1515| ptrdiff_t AdjustmentForMinPadSize(ptrdiff_t pad, int requiredAlignment)
  1516| {
  1517|     if ((size_t)(pad - DATA_ALIGNMENT) < Align (min_obj_size) - DATA_ALIGNMENT)
  1518|     {
  1519|         return requiredAlignment;
  1520|     }
  1521|     return 0;
  1522| }
  1523| inline
  1524| uint8_t* StructAlign (uint8_t* origPtr, int requiredAlignment, ptrdiff_t alignmentOffset=OBJECT_ALIGNMENT_OFFSET)
  1525| {
  1526|     _ASSERTE(((size_t)origPtr & ALIGNCONST) == 0);
  1527|     _ASSERTE(((requiredAlignment - 1) & requiredAlignment) == 0);
  1528|     _ASSERTE(requiredAlignment >= sizeof(void *));
  1529|     _ASSERTE(requiredAlignment <= MAX_STRUCTALIGN);
  1530|     uint8_t* result = (uint8_t*)Align ((size_t)origPtr + alignmentOffset, requiredAlignment-1) - alignmentOffset;
  1531|     ptrdiff_t alignpad = result - origPtr;
  1532|     return result + AdjustmentForMinPadSize (alignpad, requiredAlignment);
  1533| }
  1534| inline
  1535| ptrdiff_t ComputeStructAlignPad (uint8_t* plug, int requiredAlignment, size_t alignmentOffset=OBJECT_ALIGNMENT_OFFSET)
  1536| {
  1537|     return StructAlign (plug, requiredAlignment, alignmentOffset) - plug;
  1538| }
  1539| BOOL IsStructAligned (uint8_t *ptr, int requiredAlignment)
  1540| {
  1541|     return StructAlign (ptr, requiredAlignment) == ptr;
  1542| }
  1543| inline
  1544| ptrdiff_t ComputeMaxStructAlignPad (int requiredAlignment)
  1545| {
  1546|     if (requiredAlignment == DATA_ALIGNMENT)
  1547|         return 0;
  1548|     return requiredAlignment + Align (min_obj_size) - DATA_ALIGNMENT;
  1549| }
  1550| inline
  1551| ptrdiff_t ComputeMaxStructAlignPadLarge (int requiredAlignment)
  1552| {
  1553|     if (requiredAlignment <= get_alignment_constant (TRUE)+1)
  1554|         return 0;
  1555|     return requiredAlignment + Align (min_obj_size) * 2 - DATA_ALIGNMENT;
  1556| }
  1557| uint8_t* gc_heap::pad_for_alignment (uint8_t* newAlloc, int requiredAlignment, size_t size, alloc_context* acontext)
  1558| {
  1559|     uint8_t* alignedPtr = StructAlign (newAlloc, requiredAlignment);
  1560|     if (alignedPtr != newAlloc) {
  1561|         make_unused_array (newAlloc, alignedPtr - newAlloc);
  1562|     }
  1563|     acontext->alloc_ptr = alignedPtr + Align (size);
  1564|     return alignedPtr;
  1565| }
  1566| uint8_t* gc_heap::pad_for_alignment_large (uint8_t* newAlloc, int requiredAlignment, size_t size)
  1567| {
  1568|     uint8_t* alignedPtr = StructAlign (newAlloc, requiredAlignment);
  1569|     if (alignedPtr != newAlloc) {
  1570|         make_unused_array (newAlloc, alignedPtr - newAlloc);
  1571|     }
  1572|     if (alignedPtr < newAlloc + ComputeMaxStructAlignPadLarge (requiredAlignment)) {
  1573|         make_unused_array (alignedPtr + AlignQword (size), newAlloc + ComputeMaxStructAlignPadLarge (requiredAlignment) - alignedPtr);
  1574|     }
  1575|     return alignedPtr;
  1576| }
  1577| #else // FEATURE_STRUCTALIGN
  1578| #define ComputeMaxStructAlignPad(requiredAlignment) 0
  1579| #define ComputeMaxStructAlignPadLarge(requiredAlignment) 0
  1580| #endif // FEATURE_STRUCTALIGN
  1581| #ifdef SERVER_GC
  1582| #define CLR_SIZE ((size_t)(8*1024+32))
  1583| #else //SERVER_GC
  1584| #define CLR_SIZE ((size_t)(8*1024+32))
  1585| #endif //SERVER_GC
  1586| #define END_SPACE_AFTER_GC (loh_size_threshold + MAX_STRUCTALIGN)
  1587| #define END_SPACE_AFTER_GC_FL (END_SPACE_AFTER_GC + Align (min_obj_size))
  1588| #if defined(BACKGROUND_GC) && !defined(USE_REGIONS)
  1589| #define SEGMENT_INITIAL_COMMIT (2*OS_PAGE_SIZE)
  1590| #else
  1591| #define SEGMENT_INITIAL_COMMIT (OS_PAGE_SIZE)
  1592| #endif //BACKGROUND_GC && !USE_REGIONS
  1593| const size_t min_segment_size_hard_limit = 1024*1024*16;
  1594| inline
  1595| size_t align_on_segment_hard_limit (size_t add)
  1596| {
  1597|     return ((size_t)(add + (min_segment_size_hard_limit - 1)) & ~(min_segment_size_hard_limit - 1));
  1598| }
  1599| #ifdef SERVER_GC
  1600| #ifdef HOST_64BIT
  1601| #define INITIAL_ALLOC ((size_t)((size_t)4*1024*1024*1024))
  1602| #define LHEAP_ALLOC   ((size_t)(1024*1024*256))
  1603| #else
  1604| #define INITIAL_ALLOC ((size_t)(1024*1024*64))
  1605| #define LHEAP_ALLOC   ((size_t)(1024*1024*32))
  1606| #endif  // HOST_64BIT
  1607| #else //SERVER_GC
  1608| #ifdef HOST_64BIT
  1609| #define INITIAL_ALLOC ((size_t)(1024*1024*256))
  1610| #define LHEAP_ALLOC   ((size_t)(1024*1024*128))
  1611| #else
  1612| #define INITIAL_ALLOC ((size_t)(1024*1024*16))
  1613| #define LHEAP_ALLOC   ((size_t)(1024*1024*16))
  1614| #endif  // HOST_64BIT
  1615| #endif //SERVER_GC
  1616| const size_t etw_allocation_tick = 100*1024;
  1617| const size_t low_latency_alloc = 256*1024;
  1618| const size_t fgn_check_quantum = 2*1024*1024;
  1619| #ifdef MH_SC_MARK
  1620| const int max_snoop_level = 128;
  1621| #endif //MH_SC_MARK
  1622| #ifdef CARD_BUNDLE
  1623| #define SH_TH_CARD_BUNDLE  (40*1024*1024)
  1624| #define MH_TH_CARD_BUNDLE  (180*1024*1024)
  1625| #endif //CARD_BUNDLE
  1626| #define MIN_DECOMMIT_SIZE  (100*OS_PAGE_SIZE)
  1627| #define DECOMMIT_SIZE_PER_MILLISECOND (160*1024)
  1628| #define DECOMMIT_TIME_STEP_MILLISECONDS (100)
  1629| inline
  1630| size_t align_on_page (size_t add)
  1631| {
  1632|     return ((add + OS_PAGE_SIZE - 1) & ~((size_t)OS_PAGE_SIZE - 1));
  1633| }
  1634| inline
  1635| uint8_t* align_on_page (uint8_t* add)
  1636| {
  1637|     return (uint8_t*)align_on_page ((size_t) add);
  1638| }
  1639| inline
  1640| size_t align_lower_page (size_t add)
  1641| {
  1642|     return (add & ~((size_t)OS_PAGE_SIZE - 1));
  1643| }
  1644| inline
  1645| uint8_t* align_lower_page (uint8_t* add)
  1646| {
  1647|     return (uint8_t*)align_lower_page ((size_t)add);
  1648| }
  1649| inline
  1650| size_t align_write_watch_lower_page (size_t add)
  1651| {
  1652|     return (add & ~(WRITE_WATCH_UNIT_SIZE - 1));
  1653| }
  1654| inline
  1655| uint8_t* align_write_watch_lower_page (uint8_t* add)
  1656| {
  1657|     return (uint8_t*)align_lower_page ((size_t)add);
  1658| }
  1659| inline
  1660| BOOL power_of_two_p (size_t integer)
  1661| {
  1662|     return !(integer & (integer-1));
  1663| }
  1664| inline
  1665| BOOL oddp (size_t integer)
  1666| {
  1667|     return (integer & 1) != 0;
  1668| }
  1669| size_t logcount (size_t word)
  1670| {
  1671|     assert (word < 0x10000);
  1672|     size_t count;
  1673|     count = (word & 0x5555) + ( (word >> 1 ) & 0x5555);
  1674|     count = (count & 0x3333) + ( (count >> 2) & 0x3333);
  1675|     count = (count & 0x0F0F) + ( (count >> 4) & 0x0F0F);
  1676|     count = (count & 0x00FF) + ( (count >> 8) & 0x00FF);
  1677|     return count;
  1678| }
  1679| void stomp_write_barrier_resize(bool is_runtime_suspended, bool requires_upper_bounds_check)
  1680| {
  1681|     WriteBarrierParameters args = {};
  1682|     args.operation = WriteBarrierOp::StompResize;
  1683|     args.is_runtime_suspended = is_runtime_suspended;
  1684|     args.requires_upper_bounds_check = requires_upper_bounds_check;
  1685|     args.card_table = g_gc_card_table;
  1686| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  1687|     args.card_bundle_table = g_gc_card_bundle_table;
  1688| #endif
  1689|     args.lowest_address = g_gc_lowest_address;
  1690|     args.highest_address = g_gc_highest_address;
  1691| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  1692|     if (SoftwareWriteWatch::IsEnabledForGCHeap())
  1693|     {
  1694|         args.write_watch_table = g_gc_sw_ww_table;
  1695|     }
  1696| #endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  1697|     GCToEEInterface::StompWriteBarrier(&args);
  1698| }
  1699| #ifdef USE_REGIONS
  1700| void region_write_barrier_settings (WriteBarrierParameters* args,
  1701|                                     gc_heap::region_info* map_region_to_generation_skewed,
  1702|                                     uint8_t region_shr)
  1703| {
  1704|     switch (GCConfig::GetGCWriteBarrier())
  1705|     {
  1706|     default:
  1707|     case GCConfig::WRITE_BARRIER_DEFAULT:
  1708|     case GCConfig::WRITE_BARRIER_REGION_BIT:
  1709|         args->region_to_generation_table = (uint8_t*)map_region_to_generation_skewed;
  1710|         args->region_shr = region_shr;
  1711|         args->region_use_bitwise_write_barrier = true;
  1712|         break;
  1713|     case GCConfig::WRITE_BARRIER_REGION_BYTE:
  1714|         args->region_to_generation_table = (uint8_t*)map_region_to_generation_skewed;
  1715|         args->region_shr = region_shr;
  1716|         assert (args->region_use_bitwise_write_barrier == false);
  1717|         break;
  1718|     case GCConfig::WRITE_BARRIER_SERVER:
  1719|         assert (args->region_use_bitwise_write_barrier == false);
  1720|         assert (args->region_to_generation_table == nullptr);
  1721|         assert (args->region_shr == 0);
  1722|         break;
  1723|     }
  1724| }
  1725| #endif //USE_REGIONS
  1726| void stomp_write_barrier_ephemeral (uint8_t* ephemeral_low, uint8_t* ephemeral_high
  1727| #ifdef USE_REGIONS
  1728|                                    , gc_heap::region_info* map_region_to_generation_skewed
  1729|                                    , uint8_t region_shr
  1730| #endif //USE_REGIONS
  1731|                                    )
  1732| {
  1733| #ifndef USE_REGIONS
  1734|     initGCShadow();
  1735| #endif
  1736|     WriteBarrierParameters args = {};
  1737|     args.operation = WriteBarrierOp::StompEphemeral;
  1738|     args.is_runtime_suspended = true;
  1739|     args.ephemeral_low = ephemeral_low;
  1740|     args.ephemeral_high = ephemeral_high;
  1741| #ifdef USE_REGIONS
  1742|     region_write_barrier_settings (&args, map_region_to_generation_skewed, region_shr);
  1743| #endif //USE_REGIONS
  1744|     GCToEEInterface::StompWriteBarrier(&args);
  1745| }
  1746| void stomp_write_barrier_initialize(uint8_t* ephemeral_low, uint8_t* ephemeral_high
  1747| #ifdef USE_REGIONS
  1748|                                    , gc_heap::region_info* map_region_to_generation_skewed
  1749|                                    , uint8_t region_shr
  1750| #endif //USE_REGIONS
  1751|                                    )
  1752| {
  1753|     WriteBarrierParameters args = {};
  1754|     args.operation = WriteBarrierOp::Initialize;
  1755|     args.is_runtime_suspended = true;
  1756|     args.requires_upper_bounds_check = false;
  1757|     args.card_table = g_gc_card_table;
  1758| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  1759|     args.card_bundle_table = g_gc_card_bundle_table;
  1760| #endif
  1761|     args.lowest_address = g_gc_lowest_address;
  1762|     args.highest_address = g_gc_highest_address;
  1763|     args.ephemeral_low = ephemeral_low;
  1764|     args.ephemeral_high = ephemeral_high;
  1765| #ifdef USE_REGIONS
  1766|     region_write_barrier_settings (&args, map_region_to_generation_skewed, region_shr);
  1767| #endif //USE_REGIONS
  1768|     GCToEEInterface::StompWriteBarrier(&args);
  1769| }
  1770| #define lowbits(wrd, bits) ((wrd) & ((1 << (bits))-1))
  1771| #define highbits(wrd, bits) ((wrd) & ~((1 << (bits))-1))
  1772| static static_data static_data_table[latency_level_last - latency_level_first + 1][total_generation_count] =
  1773| {
  1774|     {
  1775|         {0, 0, 40000, 0.5f, 9.0f, 20.0f, (1000 * 1000), 1},
  1776|         {160*1024, 0, 80000, 0.5f, 2.0f, 7.0f, (10 * 1000 * 1000), 10},
  1777|         {256*1024, SSIZE_T_MAX, 200000, 0.25f, 1.2f, 1.8f, (100 * 1000 * 1000), 100},
  1778|         {3*1024*1024, SSIZE_T_MAX, 0, 0.0f, 1.25f, 4.5f, 0, 0},
  1779|         {3*1024*1024, SSIZE_T_MAX, 0, 0.0f, 1.25f, 4.5f, 0, 0},
  1780|     },
  1781|     {
  1782|         {0, 0, 40000, 0.5f,
  1783| #ifdef MULTIPLE_HEAPS
  1784|             20.0f, 40.0f,
  1785| #else
  1786|             9.0f, 20.0f,
  1787| #endif //MULTIPLE_HEAPS
  1788|             (1000 * 1000), 1},
  1789|         {256*1024, 0, 80000, 0.5f, 2.0f, 7.0f, (10 * 1000 * 1000), 10},
  1790|         {256*1024, SSIZE_T_MAX, 200000, 0.25f, 1.2f, 1.8f, (100 * 1000 * 1000), 100},
  1791|         {3*1024*1024, SSIZE_T_MAX, 0, 0.0f, 1.25f, 4.5f, 0, 0},
  1792|         {3*1024*1024, SSIZE_T_MAX, 0, 0.0f, 1.25f, 4.5f, 0, 0}
  1793|     },
  1794| };
  1795| class mark;
  1796| class generation;
  1797| class heap_segment;
  1798| class CObjectHeader;
  1799| class dynamic_data;
  1800| class l_heap;
  1801| class sorted_table;
  1802| class c_synchronize;
  1803| #ifdef FEATURE_PREMORTEM_FINALIZATION
  1804| static
  1805| HRESULT AllocateCFinalize(CFinalize **pCFinalize);
  1806| #endif // FEATURE_PREMORTEM_FINALIZATION
  1807| uint8_t* tree_search (uint8_t* tree, uint8_t* old_address);
  1808| #ifdef USE_INTROSORT
  1809| #define _sort introsort::sort
  1810| #elif defined(USE_VXSORT)
  1811| #else //USE_INTROSORT
  1812| #define _sort qsort1
  1813| void qsort1(uint8_t** low, uint8_t** high, unsigned int depth);
  1814| #endif //USE_INTROSORT
  1815| void* virtual_alloc (size_t size);
  1816| void* virtual_alloc (size_t size, bool use_large_pages_p, uint16_t numa_node = NUMA_NODE_UNDEFINED);
  1817| /* per heap static initialization */
  1818| #if defined(BACKGROUND_GC) && !defined(MULTIPLE_HEAPS)
  1819| uint32_t*   gc_heap::mark_array;
  1820| #endif //BACKGROUND_GC && !MULTIPLE_HEAPS
  1821| uint8_t**   gc_heap::g_mark_list;
  1822| uint8_t**   gc_heap::g_mark_list_copy;
  1823| size_t      gc_heap::mark_list_size;
  1824| size_t      gc_heap::g_mark_list_total_size;
  1825| bool        gc_heap::mark_list_overflow;
  1826| #ifdef USE_REGIONS
  1827| uint8_t***  gc_heap::g_mark_list_piece;
  1828| size_t      gc_heap::g_mark_list_piece_size;
  1829| size_t      gc_heap::g_mark_list_piece_total_size;
  1830| #endif //USE_REGIONS
  1831| seg_mapping* seg_mapping_table;
  1832| #ifdef FEATURE_BASICFREEZE
  1833| sorted_table* gc_heap::seg_table;
  1834| #endif //FEATURE_BASICFREEZE
  1835| #ifdef MULTIPLE_HEAPS
  1836| GCEvent     gc_heap::ee_suspend_event;
  1837| size_t      gc_heap::min_gen0_balance_delta = 0;
  1838| size_t      gc_heap::min_balance_threshold = 0;
  1839| #endif //MULTIPLE_HEAPS
  1840| VOLATILE(BOOL) gc_heap::gc_started;
  1841| #ifdef MULTIPLE_HEAPS
  1842| GCEvent     gc_heap::gc_start_event;
  1843| bool        gc_heap::gc_thread_no_affinitize_p = false;
  1844| uintptr_t   process_mask = 0;
  1845| int         gc_heap::n_heaps;       // current number of heaps
  1846| int         gc_heap::n_max_heaps;   // maximum number of heaps
  1847| gc_heap**   gc_heap::g_heaps;
  1848| #if !defined(USE_REGIONS) || defined(_DEBUG)
  1849| size_t*     gc_heap::g_promoted;
  1850| #endif //!USE_REGIONS || _DEBUG
  1851| #ifdef MH_SC_MARK
  1852| int*        gc_heap::g_mark_stack_busy;
  1853| #endif //MH_SC_MARK
  1854| #ifdef BACKGROUND_GC
  1855| size_t*     gc_heap::g_bpromoted;
  1856| #endif //BACKGROUND_GC
  1857| BOOL        gc_heap::gradual_decommit_in_progress_p = FALSE;
  1858| size_t      gc_heap::max_decommit_step_size = 0;
  1859| #else  //MULTIPLE_HEAPS
  1860| #if !defined(USE_REGIONS) || defined(_DEBUG)
  1861| size_t      gc_heap::g_promoted;
  1862| #endif //!USE_REGIONS || _DEBUG
  1863| #ifdef BACKGROUND_GC
  1864| size_t      gc_heap::g_bpromoted;
  1865| #endif //BACKGROUND_GC
  1866| const int n_heaps = 1;
  1867| #endif //MULTIPLE_HEAPS
  1868| size_t      gc_heap::card_table_element_layout[total_bookkeeping_elements + 1];
  1869| uint8_t*    gc_heap::bookkeeping_start = nullptr;
  1870| #ifdef USE_REGIONS
  1871| uint8_t*    gc_heap::bookkeeping_covered_committed = nullptr;
  1872| size_t      gc_heap::bookkeeping_sizes[total_bookkeeping_elements];
  1873| #endif //USE_REGIONS
  1874| size_t      gc_heap::reserved_memory = 0;
  1875| size_t      gc_heap::reserved_memory_limit = 0;
  1876| BOOL        gc_heap::g_low_memory_status;
  1877| static gc_reason gc_trigger_reason = reason_empty;
  1878| gc_latency_level gc_heap::latency_level = latency_level_default;
  1879| gc_mechanisms  gc_heap::settings;
  1880| gc_history_global gc_heap::gc_data_global;
  1881| uint64_t    gc_heap::gc_last_ephemeral_decommit_time = 0;
  1882| CLRCriticalSection gc_heap::check_commit_cs;
  1883| CLRCriticalSection gc_heap::decommit_lock;
  1884| size_t      gc_heap::current_total_committed = 0;
  1885| size_t      gc_heap::committed_by_oh[recorded_committed_bucket_counts];
  1886| size_t      gc_heap::current_total_committed_bookkeeping = 0;
  1887| BOOL        gc_heap::reset_mm_p = TRUE;
  1888| #ifdef FEATURE_EVENT_TRACE
  1889| bool gc_heap::informational_event_enabled_p = false;
  1890| uint64_t*   gc_heap::gc_time_info = 0;
  1891| #ifdef BACKGROUND_GC
  1892| uint64_t*   gc_heap::bgc_time_info = 0;
  1893| #endif //BACKGROUND_GC
  1894| size_t      gc_heap::physical_memory_from_config = 0;
  1895| size_t      gc_heap::gen0_min_budget_from_config = 0;
  1896| size_t      gc_heap::gen0_max_budget_from_config = 0;
  1897| int         gc_heap::high_mem_percent_from_config = 0;
  1898| bool        gc_heap::use_frozen_segments_p = false;
  1899| #ifdef FEATURE_LOH_COMPACTION
  1900| gc_heap::etw_loh_compact_info* gc_heap::loh_compact_info;
  1901| #endif //FEATURE_LOH_COMPACTION
  1902| #endif //FEATURE_EVENT_TRACE
  1903| bool        gc_heap::hard_limit_config_p = false;
  1904| #if defined(SHORT_PLUGS) && !defined(USE_REGIONS)
  1905| double      gc_heap::short_plugs_pad_ratio = 0;
  1906| #endif //SHORT_PLUGS && !USE_REGIONS
  1907| int         gc_heap::generation_skip_ratio_threshold = 0;
  1908| int         gc_heap::conserve_mem_setting = 0;
  1909| bool        gc_heap::spin_count_unit_config_p = false;
  1910| uint64_t    gc_heap::suspended_start_time = 0;
  1911| uint64_t    gc_heap::end_gc_time = 0;
  1912| uint64_t    gc_heap::total_suspended_time = 0;
  1913| uint64_t    gc_heap::process_start_time = 0;
  1914| last_recorded_gc_info gc_heap::last_ephemeral_gc_info;
  1915| last_recorded_gc_info gc_heap::last_full_blocking_gc_info;
  1916| #ifdef BACKGROUND_GC
  1917| last_recorded_gc_info gc_heap::last_bgc_info[2];
  1918| VOLATILE(bool)        gc_heap::is_last_recorded_bgc = false;
  1919| VOLATILE(int)         gc_heap::last_bgc_info_index = 0;
  1920| #endif //BACKGROUND_GC
  1921| #if defined(HOST_64BIT)
  1922| #define MAX_ALLOWED_MEM_LOAD 85
  1923| #define MIN_YOUNGEST_GEN_DESIRED (16*1024*1024)
  1924| size_t      gc_heap::youngest_gen_desired_th;
  1925| #endif //HOST_64BIT
  1926| uint64_t    gc_heap::mem_one_percent = 0;
  1927| uint32_t    gc_heap::high_memory_load_th = 0;
  1928| uint32_t    gc_heap::m_high_memory_load_th;
  1929| uint32_t    gc_heap::v_high_memory_load_th;
  1930| bool        gc_heap::is_restricted_physical_mem;
  1931| uint64_t    gc_heap::total_physical_mem = 0;
  1932| uint64_t    gc_heap::entry_available_physical_mem = 0;
  1933| size_t      gc_heap::heap_hard_limit = 0;
  1934| size_t      gc_heap::heap_hard_limit_oh[total_oh_count];
  1935| #ifdef USE_REGIONS
  1936| size_t      gc_heap::regions_range = 0;
  1937| #endif //USE_REGIONS
  1938| bool        affinity_config_specified_p = false;
  1939| #ifdef USE_REGIONS
  1940| region_allocator global_region_allocator;
  1941| uint8_t*(*initial_regions)[total_generation_count][2] = nullptr;
  1942| size_t      gc_heap::region_count = 0;
  1943| gc_heap::region_info* gc_heap::map_region_to_generation = nullptr;
  1944| gc_heap::region_info* gc_heap::map_region_to_generation_skewed = nullptr;
  1945| #endif //USE_REGIONS
  1946| #ifdef BACKGROUND_GC
  1947| GCEvent     gc_heap::bgc_start_event;
  1948| gc_mechanisms gc_heap::saved_bgc_settings;
  1949| gc_history_global gc_heap::bgc_data_global;
  1950| GCEvent     gc_heap::background_gc_done_event;
  1951| GCEvent     gc_heap::ee_proceed_event;
  1952| bool        gc_heap::gc_can_use_concurrent = false;
  1953| bool        gc_heap::temp_disable_concurrent_p = false;
  1954| uint32_t    gc_heap::cm_in_progress = FALSE;
  1955| BOOL        gc_heap::dont_restart_ee_p = FALSE;
  1956| BOOL        gc_heap::keep_bgc_threads_p = FALSE;
  1957| GCEvent     gc_heap::bgc_threads_sync_event;
  1958| BOOL        gc_heap::do_ephemeral_gc_p = FALSE;
  1959| BOOL        gc_heap::do_concurrent_p = FALSE;
  1960| size_t      gc_heap::ephemeral_fgc_counts[max_generation];
  1961| VOLATILE(c_gc_state) gc_heap::current_c_gc_state = c_gc_state_free;
  1962| VOLATILE(BOOL) gc_heap::gc_background_running = FALSE;
  1963| #endif //BACKGROUND_GC
  1964| #ifdef USE_REGIONS
  1965| #ifdef MULTIPLE_HEAPS
  1966| uint8_t*    gc_heap::gc_low;
  1967| uint8_t*    gc_heap::gc_high;
  1968| #endif //MULTIPLE_HEAPS
  1969| VOLATILE(uint8_t*)    gc_heap::ephemeral_low;
  1970| VOLATILE(uint8_t*)    gc_heap::ephemeral_high;
  1971| #endif //USE_REGIONS
  1972| #ifndef MULTIPLE_HEAPS
  1973| #ifdef SPINLOCK_HISTORY
  1974| int         gc_heap::spinlock_info_index = 0;
  1975| spinlock_info gc_heap::last_spinlock_info[max_saved_spinlock_info];
  1976| allocation_state gc_heap::current_uoh_alloc_state = (allocation_state)-1;
  1977| #endif //SPINLOCK_HISTORY
  1978| uint32_t    gc_heap::fgn_maxgen_percent = 0;
  1979| size_t      gc_heap::fgn_last_alloc = 0;
  1980| int         gc_heap::generation_skip_ratio = 100;
  1981| #ifdef FEATURE_CARD_MARKING_STEALING
  1982| VOLATILE(size_t) gc_heap::n_eph_soh = 0;
  1983| VOLATILE(size_t) gc_heap::n_gen_soh = 0;
  1984| VOLATILE(size_t) gc_heap::n_eph_loh = 0;
  1985| VOLATILE(size_t) gc_heap::n_gen_loh = 0;
  1986| #endif //FEATURE_CARD_MARKING_STEALING
  1987| uint64_t    gc_heap::loh_alloc_since_cg = 0;
  1988| BOOL        gc_heap::elevation_requested = FALSE;
  1989| BOOL        gc_heap::last_gc_before_oom = FALSE;
  1990| BOOL        gc_heap::sufficient_gen0_space_p = FALSE;
  1991| #ifdef BACKGROUND_GC
  1992| uint8_t*    gc_heap::background_saved_lowest_address = 0;
  1993| uint8_t*    gc_heap::background_saved_highest_address = 0;
  1994| uint8_t*    gc_heap::next_sweep_obj = 0;
  1995| uint8_t*    gc_heap::current_sweep_pos = 0;
  1996| #ifdef DOUBLY_LINKED_FL
  1997| heap_segment* gc_heap::current_sweep_seg = 0;
  1998| #endif //DOUBLY_LINKED_FL
  1999| exclusive_sync* gc_heap::bgc_alloc_lock;
  2000| #endif //BACKGROUND_GC
  2001| oom_history gc_heap::oom_info;
  2002| int         gc_heap::oomhist_index_per_heap = 0;
  2003| oom_history gc_heap::oomhist_per_heap[max_oom_history_count];
  2004| fgm_history gc_heap::fgm_result;
  2005| size_t      gc_heap::allocated_since_last_gc[total_oh_count];
  2006| BOOL        gc_heap::ro_segments_in_range;
  2007| #ifndef USE_REGIONS
  2008| uint8_t*    gc_heap::ephemeral_low;
  2009| uint8_t*    gc_heap::ephemeral_high;
  2010| BOOL        gc_heap::ephemeral_promotion;
  2011| uint8_t*    gc_heap::saved_ephemeral_plan_start[ephemeral_generation_count];
  2012| size_t      gc_heap::saved_ephemeral_plan_start_size[ephemeral_generation_count];
  2013| #endif //!USE_REGIONS
  2014| uint8_t*    gc_heap::lowest_address;
  2015| uint8_t*    gc_heap::highest_address;
  2016| short*      gc_heap::brick_table;
  2017| uint32_t*   gc_heap::card_table;
  2018| #ifdef CARD_BUNDLE
  2019| uint32_t*   gc_heap::card_bundle_table;
  2020| #endif //CARD_BUNDLE
  2021| uint8_t*    gc_heap::gc_low = 0;
  2022| uint8_t*    gc_heap::gc_high = 0;
  2023| #ifndef USE_REGIONS
  2024| uint8_t*    gc_heap::demotion_low;
  2025| uint8_t*    gc_heap::demotion_high;
  2026| BOOL        gc_heap::demote_gen1_p = TRUE;
  2027| uint8_t*    gc_heap::last_gen1_pin_end;
  2028| #endif //!USE_REGIONS
  2029| gen_to_condemn_tuning gc_heap::gen_to_condemn_reasons;
  2030| size_t      gc_heap::etw_allocation_running_amount[total_oh_count];
  2031| uint64_t    gc_heap::total_alloc_bytes_soh = 0;
  2032| uint64_t    gc_heap::total_alloc_bytes_uoh = 0;
  2033| int         gc_heap::gc_policy = 0;
  2034| uint64_t    gc_heap::allocation_running_time;
  2035| size_t      gc_heap::allocation_running_amount;
  2036| heap_segment* gc_heap::ephemeral_heap_segment = 0;
  2037| #ifdef USE_REGIONS
  2038| #ifdef STRESS_REGIONS
  2039| OBJECTHANDLE* gc_heap::pinning_handles_for_alloc = 0;
  2040| int         gc_heap::ph_index_per_heap = 0;
  2041| int         gc_heap::pinning_seg_interval = 2;
  2042| size_t      gc_heap::num_gen0_regions = 0;
  2043| int         gc_heap::sip_seg_interval = 0;
  2044| int         gc_heap::sip_seg_maxgen_interval = 0;
  2045| size_t      gc_heap::num_condemned_regions = 0;
  2046| #endif //STRESS_REGIONS
  2047| region_free_list gc_heap::free_regions[count_free_region_kinds];
  2048| int         gc_heap::num_regions_freed_in_sweep = 0;
  2049| int         gc_heap::regions_per_gen[max_generation + 1];
  2050| int         gc_heap::planned_regions_per_gen[max_generation + 1];
  2051| int         gc_heap::sip_maxgen_regions_per_gen[max_generation + 1];
  2052| heap_segment* gc_heap::reserved_free_regions_sip[max_generation];
  2053| int         gc_heap::new_gen0_regions_in_plns = 0;
  2054| int         gc_heap::new_regions_in_prr = 0;
  2055| int         gc_heap::new_regions_in_threading = 0;
  2056| size_t      gc_heap::end_gen0_region_space = 0;
  2057| size_t      gc_heap::end_gen0_region_committed_space = 0;
  2058| size_t      gc_heap::gen0_pinned_free_space = 0;
  2059| bool        gc_heap::gen0_large_chunk_found = false;
  2060| size_t*     gc_heap::survived_per_region = nullptr;
  2061| size_t*     gc_heap::old_card_survived_per_region = nullptr;
  2062| #endif //USE_REGIONS
  2063| BOOL        gc_heap::blocking_collection = FALSE;
  2064| heap_segment* gc_heap::freeable_uoh_segment = 0;
  2065| uint64_t    gc_heap::time_bgc_last = 0;
  2066| size_t      gc_heap::mark_stack_tos = 0;
  2067| size_t      gc_heap::mark_stack_bos = 0;
  2068| size_t      gc_heap::mark_stack_array_length = 0;
  2069| mark*       gc_heap::mark_stack_array = 0;
  2070| #if defined (_DEBUG) && defined (VERIFY_HEAP)
  2071| BOOL        gc_heap::verify_pinned_queue_p = FALSE;
  2072| #endif //_DEBUG && VERIFY_HEAP
  2073| uint8_t*    gc_heap::oldest_pinned_plug = 0;
  2074| size_t      gc_heap::num_pinned_objects = 0;
  2075| #ifdef FEATURE_LOH_COMPACTION
  2076| size_t      gc_heap::loh_pinned_queue_tos = 0;
  2077| size_t      gc_heap::loh_pinned_queue_bos = 0;
  2078| size_t      gc_heap::loh_pinned_queue_length = 0;
  2079| mark*       gc_heap::loh_pinned_queue = 0;
  2080| BOOL        gc_heap::loh_compacted_p = FALSE;
  2081| #endif //FEATURE_LOH_COMPACTION
  2082| #ifdef BACKGROUND_GC
  2083| EEThreadId  gc_heap::bgc_thread_id;
  2084| uint8_t*    gc_heap::background_written_addresses [array_size+2];
  2085| heap_segment* gc_heap::freeable_soh_segment = 0;
  2086| size_t      gc_heap::bgc_overflow_count = 0;
  2087| size_t      gc_heap::bgc_begin_loh_size = 0;
  2088| size_t      gc_heap::end_loh_size = 0;
  2089| size_t      gc_heap::bgc_begin_poh_size = 0;
  2090| size_t      gc_heap::end_poh_size = 0;
  2091| #ifdef BGC_SERVO_TUNING
  2092| uint64_t    gc_heap::loh_a_no_bgc = 0;
  2093| uint64_t    gc_heap::loh_a_bgc_marking = 0;
  2094| uint64_t    gc_heap::loh_a_bgc_planning = 0;
  2095| size_t      gc_heap::bgc_maxgen_end_fl_size = 0;
  2096| #endif //BGC_SERVO_TUNING
  2097| size_t      gc_heap::bgc_loh_size_increased = 0;
  2098| size_t      gc_heap::bgc_poh_size_increased = 0;
  2099| size_t      gc_heap::background_soh_size_end_mark = 0;
  2100| size_t      gc_heap::background_soh_alloc_count = 0;
  2101| size_t      gc_heap::background_uoh_alloc_count = 0;
  2102| uint8_t**   gc_heap::background_mark_stack_tos = 0;
  2103| uint8_t**   gc_heap::background_mark_stack_array = 0;
  2104| size_t      gc_heap::background_mark_stack_array_length = 0;
  2105| BOOL        gc_heap::processed_eph_overflow_p = FALSE;
  2106| #ifdef USE_REGIONS
  2107| BOOL        gc_heap::background_overflow_p = FALSE;
  2108| #else //USE_REGIONS
  2109| uint8_t*    gc_heap::background_min_overflow_address =0;
  2110| uint8_t*    gc_heap::background_max_overflow_address =0;
  2111| uint8_t*    gc_heap::background_min_soh_overflow_address =0;
  2112| uint8_t*    gc_heap::background_max_soh_overflow_address =0;
  2113| heap_segment* gc_heap::saved_overflow_ephemeral_seg = 0;
  2114| heap_segment* gc_heap::saved_sweep_ephemeral_seg = 0;
  2115| uint8_t*    gc_heap::saved_sweep_ephemeral_start = 0;
  2116| #endif //USE_REGIONS
  2117| Thread*     gc_heap::bgc_thread = 0;
  2118| uint8_t**   gc_heap::c_mark_list = 0;
  2119| size_t      gc_heap::c_mark_list_length = 0;
  2120| size_t      gc_heap::c_mark_list_index = 0;
  2121| gc_history_per_heap gc_heap::bgc_data_per_heap;
  2122| BOOL    gc_heap::bgc_thread_running;
  2123| CLRCriticalSection gc_heap::bgc_threads_timeout_cs;
  2124| #endif //BACKGROUND_GC
  2125| uint8_t**   gc_heap::mark_list;
  2126| uint8_t**   gc_heap::mark_list_index;
  2127| uint8_t**   gc_heap::mark_list_end;
  2128| #ifdef SNOOP_STATS
  2129| snoop_stats_data gc_heap::snoop_stat;
  2130| #endif //SNOOP_STATS
  2131| uint8_t*    gc_heap::min_overflow_address = MAX_PTR;
  2132| uint8_t*    gc_heap::max_overflow_address = 0;
  2133| uint8_t*    gc_heap::shigh = 0;
  2134| uint8_t*    gc_heap::slow = MAX_PTR;
  2135| #ifndef USE_REGIONS
  2136| size_t      gc_heap::ordered_free_space_indices[MAX_NUM_BUCKETS];
  2137| size_t      gc_heap::saved_ordered_free_space_indices[MAX_NUM_BUCKETS];
  2138| size_t      gc_heap::ordered_plug_indices[MAX_NUM_BUCKETS];
  2139| size_t      gc_heap::saved_ordered_plug_indices[MAX_NUM_BUCKETS];
  2140| BOOL        gc_heap::ordered_plug_indices_init = FALSE;
  2141| BOOL        gc_heap::use_bestfit = FALSE;
  2142| uint8_t*    gc_heap::bestfit_first_pin = 0;
  2143| BOOL        gc_heap::commit_end_of_seg = FALSE;
  2144| size_t      gc_heap::max_free_space_items = 0;
  2145| size_t      gc_heap::free_space_buckets = 0;
  2146| size_t      gc_heap::free_space_items = 0;
  2147| int         gc_heap::trimmed_free_space_index = 0;
  2148| size_t      gc_heap::total_ephemeral_plugs = 0;
  2149| seg_free_spaces* gc_heap::bestfit_seg = 0;
  2150| size_t      gc_heap::total_ephemeral_size = 0;
  2151| #endif //!USE_REGIONS
  2152| #ifdef HEAP_ANALYZE
  2153| size_t      gc_heap::internal_root_array_length = initial_internal_roots;
  2154| uint8_t**   gc_heap::internal_root_array = 0;
  2155| size_t      gc_heap::internal_root_array_index = 0;
  2156| BOOL        gc_heap::heap_analyze_success = TRUE;
  2157| uint8_t*    gc_heap::current_obj = 0;
  2158| size_t      gc_heap::current_obj_size = 0;
  2159| #endif //HEAP_ANALYZE
  2160| #ifdef GC_CONFIG_DRIVEN
  2161| size_t gc_heap::interesting_data_per_gc[max_idp_count];
  2162| #endif //GC_CONFIG_DRIVEN
  2163| #endif //MULTIPLE_HEAPS
  2164| no_gc_region_info gc_heap::current_no_gc_region_info;
  2165| FinalizerWorkItem* gc_heap::finalizer_work;
  2166| BOOL gc_heap::proceed_with_gc_p = FALSE;
  2167| GCSpinLock gc_heap::gc_lock;
  2168| #ifdef BGC_SERVO_TUNING
  2169| uint64_t gc_heap::total_loh_a_last_bgc = 0;
  2170| #endif //BGC_SERVO_TUNING
  2171| #ifdef USE_REGIONS
  2172| region_free_list gc_heap::global_regions_to_decommit[count_free_region_kinds];
  2173| region_free_list gc_heap::global_free_huge_regions;
  2174| #else //USE_REGIONS
  2175| size_t gc_heap::eph_gen_starts_size = 0;
  2176| heap_segment* gc_heap::segment_standby_list;
  2177| #endif //USE_REGIONS
  2178| bool          gc_heap::use_large_pages_p = 0;
  2179| #ifdef HEAP_BALANCE_INSTRUMENTATION
  2180| size_t        gc_heap::last_gc_end_time_us = 0;
  2181| #endif //HEAP_BALANCE_INSTRUMENTATION
  2182| #ifdef USE_REGIONS
  2183| bool          gc_heap::enable_special_regions_p = false;
  2184| #else //USE_REGIONS
  2185| size_t        gc_heap::min_segment_size = 0;
  2186| size_t        gc_heap::min_uoh_segment_size = 0;
  2187| #endif //!USE_REGIONS
  2188| size_t        gc_heap::min_segment_size_shr = 0;
  2189| size_t        gc_heap::soh_segment_size = 0;
  2190| size_t        gc_heap::segment_info_size = 0;
  2191| #ifdef GC_CONFIG_DRIVEN
  2192| size_t gc_heap::compact_or_sweep_gcs[2];
  2193| #endif //GC_CONFIG_DRIVEN
  2194| #ifdef FEATURE_LOH_COMPACTION
  2195| BOOL                   gc_heap::loh_compaction_always_p = FALSE;
  2196| gc_loh_compaction_mode gc_heap::loh_compaction_mode = loh_compaction_default;
  2197| #endif //FEATURE_LOH_COMPACTION
  2198| GCEvent gc_heap::full_gc_approach_event;
  2199| GCEvent gc_heap::full_gc_end_event;
  2200| uint32_t gc_heap::fgn_loh_percent = 0;
  2201| #ifdef BACKGROUND_GC
  2202| BOOL gc_heap::fgn_last_gc_was_concurrent = FALSE;
  2203| #endif //BACKGROUND_GC
  2204| VOLATILE(bool) gc_heap::full_gc_approach_event_set;
  2205| size_t gc_heap::full_gc_counts[gc_type_max];
  2206| bool gc_heap::maxgen_size_inc_p = false;
  2207| #ifndef USE_REGIONS
  2208| BOOL gc_heap::should_expand_in_full_gc = FALSE;
  2209| #endif //!USE_REGIONS
  2210| #ifdef DYNAMIC_HEAP_COUNT
  2211| int gc_heap::dynamic_adaptation_mode = dynamic_adaptation_default;
  2212| gc_heap::dynamic_heap_count_data_t SVR::gc_heap::dynamic_heap_count_data;
  2213| uint64_t gc_heap::last_suspended_end_time = 0;
  2214| size_t gc_heap::gc_index_full_gc_end = 0;
  2215| #ifdef STRESS_DYNAMIC_HEAP_COUNT
  2216| int gc_heap::heaps_in_this_gc = 0;
  2217| #endif //STRESS_DYNAMIC_HEAP_COUNT
  2218| #endif // DYNAMIC_HEAP_COUNT
  2219| bool gc_heap::provisional_mode_triggered = false;
  2220| bool gc_heap::pm_trigger_full_gc = false;
  2221| size_t gc_heap::provisional_triggered_gc_count = 0;
  2222| size_t gc_heap::provisional_off_gc_count = 0;
  2223| size_t gc_heap::num_provisional_triggered = 0;
  2224| bool   gc_heap::pm_stress_on = false;
  2225| #ifdef HEAP_ANALYZE
  2226| BOOL        gc_heap::heap_analyze_enabled = FALSE;
  2227| #endif //HEAP_ANALYZE
  2228| #ifndef MULTIPLE_HEAPS
  2229| alloc_list gc_heap::loh_alloc_list [NUM_LOH_ALIST-1];
  2230| alloc_list gc_heap::gen2_alloc_list[NUM_GEN2_ALIST-1];
  2231| alloc_list gc_heap::poh_alloc_list [NUM_POH_ALIST-1];
  2232| #ifdef DOUBLY_LINKED_FL
  2233| size_t gc_heap::gen2_removed_no_undo = 0;
  2234| size_t gc_heap::saved_pinned_plug_index = INVALID_SAVED_PINNED_PLUG_INDEX;
  2235| #endif //DOUBLY_LINKED_FL
  2236| #ifdef FEATURE_EVENT_TRACE
  2237| etw_bucket_info gc_heap::bucket_info[NUM_GEN2_ALIST];
  2238| #endif //FEATURE_EVENT_TRACE
  2239| dynamic_data gc_heap::dynamic_data_table [total_generation_count];
  2240| gc_history_per_heap gc_heap::gc_data_per_heap;
  2241| size_t gc_heap::total_promoted_bytes = 0;
  2242| size_t gc_heap::finalization_promoted_bytes = 0;
  2243| size_t gc_heap::maxgen_pinned_compact_before_advance = 0;
  2244| uint8_t* gc_heap::alloc_allocated = 0;
  2245| size_t gc_heap::allocation_quantum = CLR_SIZE;
  2246| GCSpinLock gc_heap::more_space_lock_soh;
  2247| GCSpinLock gc_heap::more_space_lock_uoh;
  2248| #ifdef BACKGROUND_GC
  2249| VOLATILE(int32_t) gc_heap::uoh_alloc_thread_count = 0;
  2250| #endif //BACKGROUND_GC
  2251| #ifdef SYNCHRONIZATION_STATS
  2252| unsigned int gc_heap::good_suspension = 0;
  2253| unsigned int gc_heap::bad_suspension = 0;
  2254| uint64_t     gc_heap::total_msl_acquire = 0;
  2255| unsigned int gc_heap::num_msl_acquired = 0;
  2256| unsigned int gc_heap::num_high_msl_acquire = 0;
  2257| unsigned int gc_heap::num_low_msl_acquire = 0;
  2258| #endif //SYNCHRONIZATION_STATS
  2259| size_t   gc_heap::alloc_contexts_used = 0;
  2260| size_t   gc_heap::soh_allocation_no_gc = 0;
  2261| size_t   gc_heap::loh_allocation_no_gc = 0;
  2262| bool     gc_heap::no_gc_oom_p = false;
  2263| heap_segment* gc_heap::saved_loh_segment_no_gc = 0;
  2264| #endif //MULTIPLE_HEAPS
  2265| #ifndef MULTIPLE_HEAPS
  2266| BOOL        gc_heap::gen0_bricks_cleared = FALSE;
  2267| int         gc_heap::gen0_must_clear_bricks = 0;
  2268| #ifdef FEATURE_PREMORTEM_FINALIZATION
  2269| CFinalize*  gc_heap::finalize_queue = 0;
  2270| #endif // FEATURE_PREMORTEM_FINALIZATION
  2271| #ifdef FEATURE_CARD_MARKING_STEALING
  2272| VOLATILE(uint32_t) gc_heap::card_mark_chunk_index_soh;
  2273| VOLATILE(bool) gc_heap::card_mark_done_soh;
  2274| VOLATILE(uint32_t) gc_heap::card_mark_chunk_index_loh;
  2275| VOLATILE(uint32_t) gc_heap::card_mark_chunk_index_poh;
  2276| VOLATILE(bool) gc_heap::card_mark_done_uoh;
  2277| #endif // FEATURE_CARD_MARKING_STEALING
  2278| generation gc_heap::generation_table [total_generation_count];
  2279| size_t     gc_heap::interesting_data_per_heap[max_idp_count];
  2280| size_t     gc_heap::compact_reasons_per_heap[max_compact_reasons_count];
  2281| size_t     gc_heap::expand_mechanisms_per_heap[max_expand_mechanisms_count];
  2282| size_t     gc_heap::interesting_mechanism_bits_per_heap[max_gc_mechanism_bits_count];
  2283| mark_queue_t gc_heap::mark_queue;
  2284| #ifdef USE_REGIONS
  2285| bool gc_heap::special_sweep_p = false;
  2286| #endif //USE_REGIONS
  2287| int gc_heap::loh_pinned_queue_decay = LOH_PIN_DECAY;
  2288| #endif // MULTIPLE_HEAPS
  2289| /* end of per heap static initialization */
  2290| #ifdef USE_REGIONS
  2291| const size_t uninitialized_end_gen0_region_space = (size_t)(-1);
  2292| #endif //USE_REGIONS
  2293| size_t     gc_heap::smoothed_desired_total[total_generation_count];
  2294| /* end of static initialization */
  2295| inline
  2296| int get_start_generation_index()
  2297| {
  2298| #ifdef USE_REGIONS
  2299|     return 0;
  2300| #else
  2301|     return max_generation;
  2302| #endif //USE_REGIONS
  2303| }
  2304| inline
  2305| int get_stop_generation_index (int condemned_gen_number)
  2306| {
  2307| #ifdef USE_REGIONS
  2308|     return 0;
  2309| #else
  2310|     return condemned_gen_number;
  2311| #endif //USE_REGIONS
  2312| }
  2313| void gen_to_condemn_tuning::print (int heap_num)
  2314| {
  2315| #ifdef DT_LOG
  2316|     dprintf (DT_LOG_0, ("condemned reasons (%d %d)", condemn_reasons_gen, condemn_reasons_condition));
  2317|     dprintf (DT_LOG_0, ("%s", record_condemn_reasons_gen_header));
  2318|     gc_condemn_reason_gen r_gen;
  2319|     for (int i = 0; i < gcrg_max; i++)
  2320|     {
  2321|         r_gen = (gc_condemn_reason_gen)(i);
  2322|         str_reasons_gen[i * 2] = get_gen_char (get_gen (r_gen));
  2323|     }
  2324|     dprintf (DT_LOG_0, ("[%2d]%s", heap_num, str_reasons_gen));
  2325|     dprintf (DT_LOG_0, ("%s", record_condemn_reasons_condition_header));
  2326|     gc_condemn_reason_condition r_condition;
  2327|     for (int i = 0; i < gcrc_max; i++)
  2328|     {
  2329|         r_condition = (gc_condemn_reason_condition)(i);
  2330|         str_reasons_condition[i * 2] = get_condition_char (get_condition (r_condition));
  2331|     }
  2332|     dprintf (DT_LOG_0, ("[%2d]%s", heap_num, str_reasons_condition));
  2333| #else
  2334|     UNREFERENCED_PARAMETER(heap_num);
  2335| #endif //DT_LOG
  2336| }
  2337| void gc_generation_data::print (int heap_num, int gen_num)
  2338| {
  2339| #if defined(SIMPLE_DPRINTF) && defined(DT_LOG)
  2340|     dprintf (DT_LOG_0, ("[%2d]gen%d beg %zd fl %zd fo %zd end %zd fl %zd fo %zd in %zd p %zd np %zd alloc %zd",
  2341|                 heap_num, gen_num,
  2342|                 size_before,
  2343|                 free_list_space_before, free_obj_space_before,
  2344|                 size_after,
  2345|                 free_list_space_after, free_obj_space_after,
  2346|                 in, pinned_surv, npinned_surv,
  2347|                 new_allocation));
  2348| #else
  2349|     UNREFERENCED_PARAMETER(heap_num);
  2350|     UNREFERENCED_PARAMETER(gen_num);
  2351| #endif //SIMPLE_DPRINTF && DT_LOG
  2352| }
  2353| void gc_history_per_heap::set_mechanism (gc_mechanism_per_heap mechanism_per_heap, uint32_t value)
  2354| {
  2355|     uint32_t* mechanism = &mechanisms[mechanism_per_heap];
  2356|     *mechanism = 0;
  2357|     *mechanism |= mechanism_mask;
  2358|     *mechanism |= (1 << value);
  2359| #ifdef DT_LOG
  2360|     gc_mechanism_descr* descr = &gc_mechanisms_descr[mechanism_per_heap];
  2361|     dprintf (DT_LOG_0, ("setting %s: %s",
  2362|             descr->name,
  2363|             (descr->descr)[value]));
  2364| #endif //DT_LOG
  2365| }
  2366| void gc_history_per_heap::print()
  2367| {
  2368| #if defined(SIMPLE_DPRINTF) && defined(DT_LOG)
  2369|     for (int i = 0; i < (sizeof (gen_data)/sizeof (gc_generation_data)); i++)
  2370|     {
  2371|         gen_data[i].print (heap_index, i);
  2372|     }
  2373|     dprintf (DT_LOG_0, ("fla %zd flr %zd esa %zd ca %zd pa %zd paa %zd, rfle %d, ec %zd",
  2374|                     maxgen_size_info.free_list_allocated,
  2375|                     maxgen_size_info.free_list_rejected,
  2376|                     maxgen_size_info.end_seg_allocated,
  2377|                     maxgen_size_info.condemned_allocated,
  2378|                     maxgen_size_info.pinned_allocated,
  2379|                     maxgen_size_info.pinned_allocated_advance,
  2380|                     maxgen_size_info.running_free_list_efficiency,
  2381|                     extra_gen0_committed));
  2382|     int mechanism = 0;
  2383|     gc_mechanism_descr* descr = 0;
  2384|     for (int i = 0; i < max_mechanism_per_heap; i++)
  2385|     {
  2386|         mechanism = get_mechanism ((gc_mechanism_per_heap)i);
  2387|         if (mechanism >= 0)
  2388|         {
  2389|             descr = &gc_mechanisms_descr[(gc_mechanism_per_heap)i];
  2390|             dprintf (DT_LOG_0, ("[%2d]%s%s",
  2391|                         heap_index,
  2392|                         descr->name,
  2393|                         (descr->descr)[mechanism]));
  2394|         }
  2395|     }
  2396| #endif //SIMPLE_DPRINTF && DT_LOG
  2397| }
  2398| void gc_history_global::print()
  2399| {
  2400| #ifdef DT_LOG
  2401|     char str_settings[64];
  2402|     memset (str_settings, '|', sizeof (char) * 64);
  2403|     str_settings[max_global_mechanisms_count*2] = 0;
  2404|     for (int i = 0; i < max_global_mechanisms_count; i++)
  2405|     {
  2406|         str_settings[i * 2] = (get_mechanism_p ((gc_global_mechanism_p)i) ? 'Y' : 'N');
  2407|     }
  2408|     dprintf (DT_LOG_0, ("[hp]|c|p|o|d|b|e|"));
  2409|     dprintf (DT_LOG_0, ("%4d|%s", num_heaps, str_settings));
  2410|     dprintf (DT_LOG_0, ("Condemned gen%d(reason: %s; mode: %s), youngest budget %zd(%d), memload %d",
  2411|                         condemned_generation,
  2412|                         str_gc_reasons[reason],
  2413|                         str_gc_pause_modes[pause_mode],
  2414|                         final_youngest_desired,
  2415|                         gen0_reduction_count,
  2416|                         mem_pressure));
  2417| #endif //DT_LOG
  2418| }
  2419| uint32_t limit_time_to_uint32 (uint64_t time)
  2420| {
  2421|     time = min (time, UINT32_MAX);
  2422|     return (uint32_t)time;
  2423| }
  2424| void gc_heap::fire_per_heap_hist_event (gc_history_per_heap* current_gc_data_per_heap, int heap_num)
  2425| {
  2426|     maxgen_size_increase* maxgen_size_info = &(current_gc_data_per_heap->maxgen_size_info);
  2427|     FIRE_EVENT(GCPerHeapHistory_V3,
  2428|                (void *)(maxgen_size_info->free_list_allocated),
  2429|                (void *)(maxgen_size_info->free_list_rejected),
  2430|                (void *)(maxgen_size_info->end_seg_allocated),
  2431|                (void *)(maxgen_size_info->condemned_allocated),
  2432|                (void *)(maxgen_size_info->pinned_allocated),
  2433|                (void *)(maxgen_size_info->pinned_allocated_advance),
  2434|                maxgen_size_info->running_free_list_efficiency,
  2435|                current_gc_data_per_heap->gen_to_condemn_reasons.get_reasons0(),
  2436|                current_gc_data_per_heap->gen_to_condemn_reasons.get_reasons1(),
  2437|                current_gc_data_per_heap->mechanisms[gc_heap_compact],
  2438|                current_gc_data_per_heap->mechanisms[gc_heap_expand],
  2439|                current_gc_data_per_heap->heap_index,
  2440|                (void *)(current_gc_data_per_heap->extra_gen0_committed),
  2441|                total_generation_count,
  2442|                (uint32_t)(sizeof (gc_generation_data)),
  2443|                (void *)&(current_gc_data_per_heap->gen_data[0]));
  2444|     current_gc_data_per_heap->print();
  2445|     current_gc_data_per_heap->gen_to_condemn_reasons.print (heap_num);
  2446| }
  2447| void gc_heap::fire_pevents()
  2448| {
  2449|     gc_history_global* current_gc_data_global = get_gc_data_global();
  2450|     settings.record (current_gc_data_global);
  2451|     current_gc_data_global->print();
  2452| #ifdef FEATURE_EVENT_TRACE
  2453|     if (!informational_event_enabled_p) return;
  2454|     uint32_t count_time_info = (settings.concurrent ? max_bgc_time_type :
  2455|                                 (settings.compaction ? max_compact_time_type : max_sweep_time_type));
  2456| #ifdef BACKGROUND_GC
  2457|     uint64_t* time_info = (settings.concurrent ? bgc_time_info : gc_time_info);
  2458| #else
  2459|     uint64_t* time_info = gc_time_info;
  2460| #endif //BACKGROUND_GC
  2461|     uint32_t* time_info_32 = (uint32_t*)time_info;
  2462|     for (uint32_t i = 0; i < count_time_info; i++)
  2463|     {
  2464|         time_info_32[i] = limit_time_to_uint32 (time_info[i]);
  2465|     }
  2466|     FIRE_EVENT(GCGlobalHeapHistory_V4,
  2467|                current_gc_data_global->final_youngest_desired,
  2468|                current_gc_data_global->num_heaps,
  2469|                current_gc_data_global->condemned_generation,
  2470|                current_gc_data_global->gen0_reduction_count,
  2471|                current_gc_data_global->reason,
  2472|                current_gc_data_global->global_mechanisms_p,
  2473|                current_gc_data_global->pause_mode,
  2474|                current_gc_data_global->mem_pressure,
  2475|                current_gc_data_global->gen_to_condemn_reasons.get_reasons0(),
  2476|                current_gc_data_global->gen_to_condemn_reasons.get_reasons1(),
  2477|                count_time_info,
  2478|                (uint32_t)(sizeof (uint32_t)),
  2479|                (void*)time_info_32);
  2480| #ifdef MULTIPLE_HEAPS
  2481|     for (int i = 0; i < gc_heap::n_heaps; i++)
  2482|     {
  2483|         gc_heap* hp = gc_heap::g_heaps[i];
  2484|         gc_history_per_heap* current_gc_data_per_heap = hp->get_gc_data_per_heap();
  2485|         fire_per_heap_hist_event (current_gc_data_per_heap, hp->heap_number);
  2486|     }
  2487| #else
  2488|     gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
  2489|     fire_per_heap_hist_event (current_gc_data_per_heap, heap_number);
  2490| #endif //MULTIPLE_HEAPS
  2491| #ifdef FEATURE_LOH_COMPACTION
  2492|     if (!settings.concurrent && settings.loh_compaction)
  2493|     {
  2494|         FIRE_EVENT(GCLOHCompact,
  2495|                    (uint16_t)get_num_heaps(),
  2496|                    (uint32_t)(sizeof (etw_loh_compact_info)),
  2497|                    (void *)loh_compact_info);
  2498|     }
  2499| #endif //FEATURE_LOH_COMPACTION
  2500| #endif //FEATURE_EVENT_TRACE
  2501| }
  2502| void gc_heap::fire_committed_usage_event()
  2503| {
  2504| #if defined(FEATURE_EVENT_TRACE) && defined(USE_REGIONS)
  2505|     if (!EVENT_ENABLED (GCMarkWithType)) return;
  2506|     size_t total_committed = 0;
  2507|     size_t committed_decommit = 0;
  2508|     size_t committed_free = 0;
  2509|     size_t committed_bookkeeping = 0;
  2510|     size_t new_current_total_committed;
  2511|     size_t new_current_total_committed_bookkeeping;
  2512|     size_t new_committed_by_oh[recorded_committed_bucket_counts];
  2513|     compute_committed_bytes(total_committed, committed_decommit, committed_free,
  2514|                             committed_bookkeeping, new_current_total_committed, new_current_total_committed_bookkeeping,
  2515|                             new_committed_by_oh);
  2516|     size_t total_committed_in_use = new_committed_by_oh[soh] + new_committed_by_oh[loh] + new_committed_by_oh[poh];
  2517|     size_t total_committed_in_global_decommit = committed_decommit;
  2518|     size_t total_committed_in_free = committed_free;
  2519|     size_t total_committed_in_global_free = new_committed_by_oh[recorded_committed_free_bucket] - total_committed_in_free - total_committed_in_global_decommit;
  2520|     size_t total_bookkeeping_committed = committed_bookkeeping;
  2521|     GCEventFireCommittedUsage_V1 (
  2522|         (uint64_t)total_committed_in_use,
  2523|         (uint64_t)total_committed_in_global_decommit,
  2524|         (uint64_t)total_committed_in_free,
  2525|         (uint64_t)total_committed_in_global_free,
  2526|         (uint64_t)total_bookkeeping_committed
  2527|     );
  2528| #endif //FEATURE_EVENT_TRACE && USE_REGIONS
  2529| }
  2530| inline BOOL
  2531| gc_heap::dt_low_ephemeral_space_p (gc_tuning_point tp)
  2532| {
  2533|     BOOL ret = FALSE;
  2534|     switch (tp)
  2535|     {
  2536|         case tuning_deciding_condemned_gen:
  2537| #ifndef USE_REGIONS
  2538|         case tuning_deciding_compaction:
  2539|         case tuning_deciding_expansion:
  2540| #endif //USE_REGIONS
  2541|         case tuning_deciding_full_gc:
  2542|         {
  2543|             ret = (!ephemeral_gen_fit_p (tp));
  2544|             break;
  2545|         }
  2546| #ifndef USE_REGIONS
  2547|         case tuning_deciding_promote_ephemeral:
  2548|         {
  2549|             size_t new_gen0size = approximate_new_allocation();
  2550|             ptrdiff_t plan_ephemeral_size = total_ephemeral_size;
  2551|             dprintf (GTC_LOG, ("h%d: plan eph size is %zd, new gen0 is %zd",
  2552|                 heap_number, plan_ephemeral_size, new_gen0size));
  2553|             ret = ((soh_segment_size - segment_info_size) < (plan_ephemeral_size + new_gen0size));
  2554|             break;
  2555|         }
  2556| #endif //USE_REGIONS
  2557|         default:
  2558|         {
  2559|             assert (!"invalid tuning reason");
  2560|             break;
  2561|         }
  2562|     }
  2563|     return ret;
  2564| }
  2565| BOOL
  2566| gc_heap::dt_high_frag_p (gc_tuning_point tp,
  2567|                          int gen_number,
  2568|                          BOOL elevate_p)
  2569| {
  2570|     BOOL ret = FALSE;
  2571|     switch (tp)
  2572|     {
  2573|         case tuning_deciding_condemned_gen:
  2574|         {
  2575|             dynamic_data* dd = dynamic_data_of (gen_number);
  2576|             float fragmentation_burden = 0;
  2577|             if (elevate_p)
  2578|             {
  2579|                 ret = (dd_fragmentation (dynamic_data_of (max_generation)) >= dd_max_size(dd));
  2580|                 dprintf (GTC_LOG, ("h%d: frag is %zd, max size is %zd",
  2581|                     heap_number, dd_fragmentation (dd), dd_max_size(dd)));
  2582|             }
  2583|             else
  2584|             {
  2585| #ifndef MULTIPLE_HEAPS
  2586|                 if (gen_number == max_generation)
  2587|                 {
  2588|                     size_t maxgen_size = generation_size (max_generation);
  2589|                     float frag_ratio = (maxgen_size ? ((float)dd_fragmentation (dynamic_data_of (max_generation)) / (float)maxgen_size) : 0.0f);
  2590|                     if (frag_ratio > 0.65)
  2591|                     {
  2592|                         dprintf (GTC_LOG, ("g2 FR: %d%%", (int)(frag_ratio*100)));
  2593|                         return TRUE;
  2594|                     }
  2595|                 }
  2596| #endif //!MULTIPLE_HEAPS
  2597|                 size_t fr = generation_unusable_fragmentation (generation_of (gen_number));
  2598|                 ret = (fr > dd_fragmentation_limit(dd));
  2599|                 if (ret)
  2600|                 {
  2601|                     size_t gen_size = generation_size (gen_number);
  2602|                     fragmentation_burden = (gen_size ? ((float)fr / (float)gen_size) : 0.0f);
  2603|                     ret = (fragmentation_burden > dd_v_fragmentation_burden_limit (dd));
  2604|                 }
  2605|                 dprintf (GTC_LOG, ("h%d: gen%d, frag is %zd, alloc effi: %d%%, unusable frag is %zd, ratio is %d",
  2606|                     heap_number, gen_number, dd_fragmentation (dd),
  2607|                     (int)(100*generation_allocator_efficiency (generation_of (gen_number))),
  2608|                     fr, (int)(fragmentation_burden*100)));
  2609|             }
  2610|             break;
  2611|         }
  2612|         default:
  2613|             break;
  2614|     }
  2615|     return ret;
  2616| }
  2617| inline BOOL
  2618| gc_heap::dt_estimate_reclaim_space_p (gc_tuning_point tp, int gen_number)
  2619| {
  2620|     BOOL ret = FALSE;
  2621|     switch (tp)
  2622|     {
  2623|         case tuning_deciding_condemned_gen:
  2624|         {
  2625|             if (gen_number == max_generation)
  2626|             {
  2627|                 size_t est_maxgen_free = estimated_reclaim (gen_number);
  2628|                 uint32_t num_heaps = 1;
  2629| #ifdef MULTIPLE_HEAPS
  2630|                 num_heaps = gc_heap::n_heaps;
  2631| #endif //MULTIPLE_HEAPS
  2632|                 size_t min_frag_th = min_reclaim_fragmentation_threshold (num_heaps);
  2633|                 dprintf (GTC_LOG, ("h%d, min frag is %zd", heap_number, min_frag_th));
  2634|                 ret = (est_maxgen_free >= min_frag_th);
  2635|             }
  2636|             else
  2637|             {
  2638|                 assert (0);
  2639|             }
  2640|             break;
  2641|         }
  2642|         default:
  2643|             break;
  2644|     }
  2645|     return ret;
  2646| }
  2647| inline BOOL
  2648| gc_heap::dt_estimate_high_frag_p (gc_tuning_point tp, int gen_number, uint64_t available_mem)
  2649| {
  2650|     BOOL ret = FALSE;
  2651|     switch (tp)
  2652|     {
  2653|         case tuning_deciding_condemned_gen:
  2654|         {
  2655|             if (gen_number == max_generation)
  2656|             {
  2657|                 dynamic_data* dd = dynamic_data_of (gen_number);
  2658|                 float est_frag_ratio = 0;
  2659|                 if (dd_current_size (dd) == 0)
  2660|                 {
  2661|                     est_frag_ratio = 1;
  2662|                 }
  2663|                 else if ((dd_fragmentation (dd) == 0) || (dd_fragmentation (dd) + dd_current_size (dd) == 0))
  2664|                 {
  2665|                     est_frag_ratio = 0;
  2666|                 }
  2667|                 else
  2668|                 {
  2669|                     est_frag_ratio = (float)dd_fragmentation (dd) / (float)(dd_fragmentation (dd) + dd_current_size (dd));
  2670|                 }
  2671|                 size_t est_frag = (dd_fragmentation (dd) + (size_t)((dd_desired_allocation (dd) - dd_new_allocation (dd)) * est_frag_ratio));
  2672|                 dprintf (GTC_LOG, ("h%d: gen%d: current_size is %zd, frag is %zd, est_frag_ratio is %d%%, estimated frag is %zd",
  2673|                     heap_number,
  2674|                     gen_number,
  2675|                     dd_current_size (dd),
  2676|                     dd_fragmentation (dd),
  2677|                     (int)(est_frag_ratio*100),
  2678|                     est_frag));
  2679|                 uint32_t num_heaps = 1;
  2680| #ifdef MULTIPLE_HEAPS
  2681|                 num_heaps = gc_heap::n_heaps;
  2682| #endif //MULTIPLE_HEAPS
  2683|                 uint64_t min_frag_th = min_high_fragmentation_threshold(available_mem, num_heaps);
  2684|                 ret = (est_frag >= min_frag_th);
  2685|             }
  2686|             else
  2687|             {
  2688|                 assert (0);
  2689|             }
  2690|             break;
  2691|         }
  2692|         default:
  2693|             break;
  2694|     }
  2695|     return ret;
  2696| }
  2697| inline BOOL
  2698| gc_heap::dt_low_card_table_efficiency_p (gc_tuning_point tp)
  2699| {
  2700|     BOOL ret = FALSE;
  2701|     switch (tp)
  2702|     {
  2703|     case tuning_deciding_condemned_gen:
  2704|     {
  2705|         /* promote into max-generation if the card table has too many
  2706|         * generation faults besides the n -> 0
  2707|         */
  2708|         ret = (generation_skip_ratio < generation_skip_ratio_threshold);
  2709|         break;
  2710|     }
  2711|     default:
  2712|         break;
  2713|     }
  2714|     return ret;
  2715| }
  2716| inline BOOL
  2717| gc_heap::dt_high_memory_load_p()
  2718| {
  2719|     return ((settings.entry_memory_load >= high_memory_load_th) || g_low_memory_status);
  2720| }
  2721| inline BOOL
  2722| in_range_for_segment(uint8_t* add, heap_segment* seg)
  2723| {
  2724|     return ((add >= heap_segment_mem (seg)) && (add < heap_segment_reserved (seg)));
  2725| }
  2726| #ifdef FEATURE_BASICFREEZE
  2727| struct bk
  2728| {
  2729|     uint8_t* add;
  2730|     size_t val;
  2731| };
  2732| class sorted_table
  2733| {
  2734| private:
  2735|     ptrdiff_t size;
  2736|     ptrdiff_t count;
  2737|     bk* slots;
  2738|     bk* buckets() { return (slots + 1); }
  2739|     uint8_t*& last_slot (bk* arr) { return arr[0].add; }
  2740|     bk* old_slots;
  2741| public:
  2742|     static  sorted_table* make_sorted_table ();
  2743|     BOOL    insert (uint8_t* add, size_t val);;
  2744|     size_t  lookup (uint8_t*& add);
  2745|     void    remove (uint8_t* add);
  2746|     void    clear ();
  2747|     void    delete_sorted_table();
  2748|     void    delete_old_slots();
  2749|     void    enqueue_old_slot(bk* sl);
  2750|     BOOL    ensure_space_for_insert();
  2751| };
  2752| sorted_table*
  2753| sorted_table::make_sorted_table ()
  2754| {
  2755|     size_t size = 400;
  2756|     sorted_table* res = (sorted_table*)new (nothrow) char [sizeof (sorted_table) + (size + 1) * sizeof (bk)];
  2757|     if (!res)
  2758|         return 0;
  2759|     res->size = size;
  2760|     res->slots = (bk*)(res + 1);
  2761|     res->old_slots = 0;
  2762|     res->clear();
  2763|     return res;
  2764| }
  2765| void
  2766| sorted_table::delete_sorted_table()
  2767| {
  2768|     if (slots != (bk*)(this+1))
  2769|     {
  2770|         delete slots;
  2771|     }
  2772|     delete_old_slots();
  2773|     delete this;
  2774| }
  2775| void
  2776| sorted_table::delete_old_slots()
  2777| {
  2778|     uint8_t* sl = (uint8_t*)old_slots;
  2779|     while (sl)
  2780|     {
  2781|         uint8_t* dsl = sl;
  2782|         sl = last_slot ((bk*)sl);
  2783|         delete dsl;
  2784|     }
  2785|     old_slots = 0;
  2786| }
  2787| void
  2788| sorted_table::enqueue_old_slot(bk* sl)
  2789| {
  2790|     last_slot (sl) = (uint8_t*)old_slots;
  2791|     old_slots = sl;
  2792| }
  2793| inline
  2794| size_t
  2795| sorted_table::lookup (uint8_t*& add)
  2796| {
  2797|     ptrdiff_t high = (count-1);
  2798|     ptrdiff_t low = 0;
  2799|     ptrdiff_t ti;
  2800|     ptrdiff_t mid;
  2801|     bk* buck = buckets();
  2802|     while (low <= high)
  2803|     {
  2804|         mid = ((low + high)/2);
  2805|         ti = mid;
  2806|         if (buck[ti].add > add)
  2807|         {
  2808|             if ((ti > 0) && (buck[ti-1].add <= add))
  2809|             {
  2810|                 add = buck[ti-1].add;
  2811|                 return buck[ti - 1].val;
  2812|             }
  2813|             high = mid - 1;
  2814|         }
  2815|         else
  2816|         {
  2817|             if (buck[ti+1].add > add)
  2818|             {
  2819|                 add = buck[ti].add;
  2820|                 return buck[ti].val;
  2821|             }
  2822|             low = mid + 1;
  2823|         }
  2824|     }
  2825|     add = 0;
  2826|     return 0;
  2827| }
  2828| BOOL
  2829| sorted_table::ensure_space_for_insert()
  2830| {
  2831|     if (count == size)
  2832|     {
  2833|         size = (size * 3)/2;
  2834|         assert((size * sizeof (bk)) > 0);
  2835|         bk* res = (bk*)new (nothrow) char [(size + 1) * sizeof (bk)];
  2836|         assert (res);
  2837|         if (!res)
  2838|             return FALSE;
  2839|         last_slot (res) = 0;
  2840|         memcpy (((bk*)res + 1), buckets(), count * sizeof (bk));
  2841|         bk* last_old_slots = slots;
  2842|         slots = res;
  2843|         if (last_old_slots != (bk*)(this + 1))
  2844|             enqueue_old_slot (last_old_slots);
  2845|     }
  2846|     return TRUE;
  2847| }
  2848| BOOL
  2849| sorted_table::insert (uint8_t* add, size_t val)
  2850| {
  2851|     assert (count < size);
  2852|     ptrdiff_t high = (count-1);
  2853|     ptrdiff_t low = 0;
  2854|     ptrdiff_t ti;
  2855|     ptrdiff_t mid;
  2856|     bk* buck = buckets();
  2857|     while (low <= high)
  2858|     {
  2859|         mid = ((low + high)/2);
  2860|         ti = mid;
  2861|         if (buck[ti].add > add)
  2862|         {
  2863|             if ((ti == 0) || (buck[ti-1].add <= add))
  2864|             {
  2865|                 for (ptrdiff_t k = count; k > ti;k--)
  2866|                 {
  2867|                     buck [k] = buck [k-1];
  2868|                 }
  2869|                 buck[ti].add = add;
  2870|                 buck[ti].val = val;
  2871|                 count++;
  2872|                 return TRUE;
  2873|             }
  2874|             high = mid - 1;
  2875|         }
  2876|         else
  2877|         {
  2878|             if (buck[ti+1].add > add)
  2879|             {
  2880|                 for (ptrdiff_t k = count; k > ti+1;k--)
  2881|                 {
  2882|                     buck [k] = buck [k-1];
  2883|                 }
  2884|                 buck[ti+1].add = add;
  2885|                 buck[ti+1].val = val;
  2886|                 count++;
  2887|                 return TRUE;
  2888|             }
  2889|             low = mid + 1;
  2890|         }
  2891|     }
  2892|     assert (0);
  2893|     return TRUE;
  2894| }
  2895| void
  2896| sorted_table::remove (uint8_t* add)
  2897| {
  2898|     ptrdiff_t high = (count-1);
  2899|     ptrdiff_t low = 0;
  2900|     ptrdiff_t ti;
  2901|     ptrdiff_t mid;
  2902|     bk* buck = buckets();
  2903|     while (low <= high)
  2904|     {
  2905|         mid = ((low + high)/2);
  2906|         ti = mid;
  2907|         if (buck[ti].add > add)
  2908|         {
  2909|             if (buck[ti-1].add <= add)
  2910|             {
  2911|                 for (ptrdiff_t k = ti; k < count; k++)
  2912|                     buck[k-1] = buck[k];
  2913|                 count--;
  2914|                 return;
  2915|             }
  2916|             high = mid - 1;
  2917|         }
  2918|         else
  2919|         {
  2920|             if (buck[ti+1].add > add)
  2921|             {
  2922|                 for (ptrdiff_t k = ti+1; k < count; k++)
  2923|                     buck[k-1] = buck[k];
  2924|                 count--;
  2925|                 return;
  2926|             }
  2927|             low = mid + 1;
  2928|         }
  2929|     }
  2930|     assert (0);
  2931| }
  2932| void
  2933| sorted_table::clear()
  2934| {
  2935|     count = 1;
  2936|     buckets()[0].add = MAX_PTR;
  2937| }
  2938| #endif //FEATURE_BASICFREEZE
  2939| #ifdef USE_REGIONS
  2940| inline
  2941| size_t get_skewed_basic_region_index_for_address (uint8_t* address)
  2942| {
  2943|     assert ((g_gc_lowest_address <= address) && (address <= g_gc_highest_address));
  2944|     size_t skewed_basic_region_index = (size_t)address >> gc_heap::min_segment_size_shr;
  2945|     return skewed_basic_region_index;
  2946| }
  2947| inline
  2948| size_t get_basic_region_index_for_address (uint8_t* address)
  2949| {
  2950|     size_t skewed_basic_region_index = get_skewed_basic_region_index_for_address (address);
  2951|     return (skewed_basic_region_index - get_skewed_basic_region_index_for_address (g_gc_lowest_address));
  2952| }
  2953| inline
  2954| heap_segment* get_region_info_for_address (uint8_t* address)
  2955| {
  2956|     size_t basic_region_index = (size_t)address >> gc_heap::min_segment_size_shr;
  2957|     heap_segment* basic_region_info_entry = (heap_segment*)&seg_mapping_table[basic_region_index];
  2958|     ptrdiff_t first_field = (ptrdiff_t)heap_segment_allocated (basic_region_info_entry);
  2959|     if (first_field < 0)
  2960|     {
  2961|         basic_region_index += first_field;
  2962|     }
  2963|     return ((heap_segment*)(&seg_mapping_table[basic_region_index]));
  2964| }
  2965| inline
  2966| heap_segment* get_region_info (uint8_t* region_start)
  2967| {
  2968|     size_t region_index = (size_t)region_start >> gc_heap::min_segment_size_shr;
  2969|     heap_segment* region_info_entry = (heap_segment*)&seg_mapping_table[region_index];
  2970|     dprintf (REGIONS_LOG, ("region info for region %p is at %zd, %zx (alloc: %p)",
  2971|         region_start, region_index, (size_t)region_info_entry, heap_segment_allocated (region_info_entry)));
  2972|     return (heap_segment*)&seg_mapping_table[region_index];
  2973| }
  2974| inline
  2975| uint8_t* get_region_start (heap_segment* region_info)
  2976| {
  2977|     uint8_t* obj_start = heap_segment_mem (region_info);
  2978|     return (obj_start - sizeof (aligned_plug_and_gap));
  2979| }
  2980| inline
  2981| size_t get_region_size (heap_segment* region_info)
  2982| {
  2983|     return (size_t)(heap_segment_reserved (region_info) - get_region_start (region_info));
  2984| }
  2985| inline
  2986| size_t get_region_committed_size (heap_segment* region)
  2987| {
  2988|     uint8_t* start = get_region_start (region);
  2989|     uint8_t* committed = heap_segment_committed (region);
  2990|     return committed - start;
  2991| }
  2992| inline bool is_free_region (heap_segment* region)
  2993| {
  2994|     return (heap_segment_allocated (region) == nullptr);
  2995| }
  2996| bool region_allocator::init (uint8_t* start, uint8_t* end, size_t alignment, uint8_t** lowest, uint8_t** highest)
  2997| {
  2998|     uint8_t* actual_start = start;
  2999|     region_alignment = alignment;
  3000|     large_region_alignment = LARGE_REGION_FACTOR * alignment;
  3001|     global_region_start = (uint8_t*)align_region_up ((size_t)actual_start);
  3002|     uint8_t* actual_end = end;
  3003|     global_region_end = (uint8_t*)align_region_down ((size_t)actual_end);
  3004|     global_region_left_used = global_region_start;
  3005|     global_region_right_used = global_region_end;
  3006|     num_left_used_free_units = 0;
  3007|     num_right_used_free_units = 0;
  3008|     size_t total_num_units = (global_region_end - global_region_start) / region_alignment;
  3009|     total_free_units = (uint32_t)total_num_units;
  3010|     uint32_t* unit_map = new (nothrow) uint32_t[total_num_units];
  3011|     if (unit_map)
  3012|     {
  3013|         memset (unit_map, 0, sizeof (uint32_t) * total_num_units);
  3014|         region_map_left_start = unit_map;
  3015|         region_map_left_end = region_map_left_start;
  3016|         region_map_right_start = unit_map + total_num_units;
  3017|         region_map_right_end = region_map_right_start;
  3018|         dprintf (REGIONS_LOG, ("start: %zx, end: %zx, total %zdmb(alignment: %zdmb), map units %zd",
  3019|             (size_t)start, (size_t)end,
  3020|             (size_t)((end - start) / 1024 / 1024),
  3021|             (alignment / 1024 / 1024),
  3022|             total_num_units));
  3023|         *lowest = global_region_start;
  3024|         *highest = global_region_end;
  3025|     }
  3026|     return (unit_map != 0);
  3027| }
  3028| inline
  3029| uint8_t* region_allocator::region_address_of (uint32_t* map_index)
  3030| {
  3031|     return (global_region_start + ((map_index - region_map_left_start) * region_alignment));
  3032| }
  3033| inline
  3034| uint32_t* region_allocator::region_map_index_of (uint8_t* address)
  3035| {
  3036|     return (region_map_left_start + ((address - global_region_start) / region_alignment));
  3037| }
  3038| void region_allocator::make_busy_block (uint32_t* index_start, uint32_t num_units)
  3039| {
  3040| #ifdef _DEBUG
  3041|     dprintf (REGIONS_LOG, ("MBB[B: %zd] %d->%d", (size_t)num_units, (int)(index_start - region_map_left_start), (int)(index_start - region_map_left_start + num_units)));
  3042| #endif //_DEBUG
  3043|     ASSERT_HOLDING_SPIN_LOCK (&region_allocator_lock);
  3044|     uint32_t* index_end = index_start + (num_units - 1);
  3045|     *index_start = *index_end = num_units;
  3046| }
  3047| void region_allocator::make_free_block (uint32_t* index_start, uint32_t num_units)
  3048| {
  3049| #ifdef _DEBUG
  3050|     dprintf (REGIONS_LOG, ("MFB[F: %zd] %d->%d", (size_t)num_units, (int)(index_start - region_map_left_start), (int)(index_start - region_map_left_start + num_units)));
  3051| #endif //_DEBUG
  3052|     ASSERT_HOLDING_SPIN_LOCK (&region_allocator_lock);
  3053|     uint32_t* index_end = index_start + (num_units - 1);
  3054|     *index_start = *index_end = region_alloc_free_bit | num_units;
  3055| }
  3056| void region_allocator::print_map (const char* msg)
  3057| {
  3058|     ASSERT_HOLDING_SPIN_LOCK (&region_allocator_lock);
  3059| #ifdef _DEBUG
  3060|     const char* heap_type = "UH";
  3061|     dprintf (REGIONS_LOG, ("[%s]-----printing----%s", heap_type, msg));
  3062|     uint32_t* current_index = region_map_left_start;
  3063|     uint32_t* end_index = region_map_left_end;
  3064|     uint32_t  count_free_units = 0;
  3065|     for (int i = 0; i < 2; i++)
  3066|     {
  3067|         while (current_index < end_index)
  3068|         {
  3069|             uint32_t current_val = *current_index;
  3070|             uint32_t current_num_units = get_num_units (current_val);
  3071|             bool free_p = is_unit_memory_free (current_val);
  3072|             dprintf (REGIONS_LOG, ("[%s][%s: %zd]%d->%d", heap_type, (free_p ? "F" : "B"), (size_t)current_num_units,
  3073|                 (int)(current_index - region_map_left_start),
  3074|                 (int)(current_index - region_map_left_start + current_num_units)));
  3075|             if (free_p)
  3076|             {
  3077|                 count_free_units += current_num_units;
  3078|             }
  3079|             current_index += current_num_units;
  3080|         }
  3081|         current_index = region_map_right_start;
  3082|         end_index = region_map_right_end;
  3083|         if (i == 0)
  3084|         {
  3085|             assert (count_free_units == num_left_used_free_units);
  3086|         }
  3087|         else
  3088|         {
  3089|             assert (count_free_units == num_left_used_free_units + num_right_used_free_units);
  3090|         }
  3091|     }
  3092|     count_free_units += (uint32_t)(region_map_right_start - region_map_left_end);
  3093|     assert(count_free_units == total_free_units);
  3094|     uint32_t total_regions = (uint32_t)((global_region_end - global_region_start) / region_alignment);
  3095|     dprintf (REGIONS_LOG, ("[%s]-----end printing----[%d total, left used %zd (free: %d), right used %zd (free: %d)]\n", heap_type, total_regions,
  3096|         (region_map_left_end - region_map_left_start), num_left_used_free_units, (region_map_right_end - region_map_right_start), num_right_used_free_units));
  3097| #endif //_DEBUG
  3098| }
  3099| uint8_t* region_allocator::allocate_end (uint32_t num_units, allocate_direction direction)
  3100| {
  3101|     uint8_t* alloc = NULL;
  3102|     ASSERT_HOLDING_SPIN_LOCK (&region_allocator_lock);
  3103|     if (global_region_left_used < global_region_right_used)
  3104|     {
  3105|         size_t end_remaining = global_region_right_used - global_region_left_used;
  3106|         if ((end_remaining / region_alignment) >= num_units)
  3107|         {
  3108|             if (direction == allocate_forward)
  3109|             {
  3110|                 make_busy_block (region_map_left_end, num_units);
  3111|                 region_map_left_end += num_units;
  3112|                 alloc = global_region_left_used;
  3113|                 global_region_left_used += num_units * region_alignment;
  3114|             }
  3115|             else
  3116|             {
  3117|                 assert(direction == allocate_backward);
  3118|                 region_map_right_start -= num_units;
  3119|                 make_busy_block (region_map_right_start, num_units);
  3120|                 global_region_right_used -= num_units * region_alignment;
  3121|                 alloc = global_region_right_used;
  3122|             }
  3123|         }
  3124|     }
  3125|     return alloc;
  3126| }
  3127| void region_allocator::enter_spin_lock()
  3128| {
  3129|     while (true)
  3130|     {
  3131|         if (Interlocked::CompareExchange(&region_allocator_lock.lock, 0, -1) < 0)
  3132|             break;
  3133|         while (region_allocator_lock.lock >= 0)
  3134|         {
  3135|             YieldProcessor();           // indicate to the processor that we are spinning
  3136|         }
  3137|     }
  3138| #ifdef _DEBUG
  3139|     region_allocator_lock.holding_thread = GCToEEInterface::GetThread();
  3140| #endif //_DEBUG
  3141| }
  3142| void region_allocator::leave_spin_lock()
  3143| {
  3144| #ifdef _DEBUG
  3145|     region_allocator_lock.holding_thread = (Thread*)-1;
  3146| #endif //_DEBUG
  3147|     region_allocator_lock.lock = -1;
  3148| }
  3149| uint8_t* region_allocator::allocate (uint32_t num_units, allocate_direction direction, region_allocator_callback_fn fn)
  3150| {
  3151|     enter_spin_lock();
  3152|     uint32_t* current_index;
  3153|     uint32_t* end_index;
  3154|     if (direction == allocate_forward)
  3155|     {
  3156|         current_index = region_map_left_start;
  3157|         end_index = region_map_left_end;
  3158|     }
  3159|     else
  3160|     {
  3161|         assert(direction == allocate_backward);
  3162|         current_index = region_map_right_end;
  3163|         end_index = region_map_right_start;
  3164|     }
  3165|     dprintf (REGIONS_LOG, ("searching %d->%d", (int)(current_index - region_map_left_start), (int)(end_index - region_map_left_start)));
  3166|     print_map ("before alloc");
  3167|     if (((direction == allocate_forward) && (num_left_used_free_units >= num_units)) ||
  3168|         ((direction == allocate_backward) && (num_right_used_free_units >= num_units)))
  3169|     {
  3170|         while (((direction == allocate_forward) && (current_index < end_index)) ||
  3171|             ((direction == allocate_backward) && (current_index > end_index)))
  3172|         {
  3173|             uint32_t current_val = *(current_index - ((direction == allocate_backward) ? 1 : 0));
  3174|             uint32_t current_num_units = get_num_units (current_val);
  3175|             bool free_p = is_unit_memory_free (current_val);
  3176|             dprintf (REGIONS_LOG, ("ALLOC[%s: %zd]%d->%d", (free_p ? "F" : "B"), (size_t)current_num_units,
  3177|                 (int)(current_index - region_map_left_start), (int)(current_index + current_num_units - region_map_left_start)));
  3178|             if (free_p)
  3179|             {
  3180|                 if (current_num_units >= num_units)
  3181|                 {
  3182|                     dprintf (REGIONS_LOG, ("found %zd contiguous free units(%d->%d), sufficient",
  3183|                         (size_t)current_num_units,
  3184|                         (int)(current_index - region_map_left_start),
  3185|                         (int)(current_index - region_map_left_start + current_num_units)));
  3186|                     if (direction == allocate_forward)
  3187|                     {
  3188|                         assert (num_left_used_free_units >= num_units);
  3189|                         num_left_used_free_units -= num_units;
  3190|                     }
  3191|                     else
  3192|                     {
  3193|                         assert (direction == allocate_backward);
  3194|                         assert (num_right_used_free_units >= num_units);
  3195|                         num_right_used_free_units -= num_units;
  3196|                     }
  3197|                     uint32_t* busy_block;
  3198|                     uint32_t* free_block;
  3199|                     if (direction == 1)
  3200|                     {
  3201|                         busy_block = current_index;
  3202|                         free_block = current_index + num_units;
  3203|                     }
  3204|                     else
  3205|                     {
  3206|                         busy_block = current_index - num_units;
  3207|                         free_block = current_index - current_num_units;
  3208|                     }
  3209|                     make_busy_block (busy_block, num_units);
  3210|                     if ((current_num_units - num_units) > 0)
  3211|                     {
  3212|                         make_free_block (free_block, (current_num_units - num_units));
  3213|                     }
  3214|                     total_free_units -= num_units;
  3215|                     print_map ("alloc: found in free");
  3216|                     leave_spin_lock();
  3217|                     return region_address_of (busy_block);
  3218|                 }
  3219|             }
  3220|             if (direction == allocate_forward)
  3221|             {
  3222|                 current_index += current_num_units;
  3223|             }
  3224|             else
  3225|             {
  3226|                 current_index -= current_num_units;
  3227|             }
  3228|         }
  3229|     }
  3230|     uint8_t* alloc = allocate_end (num_units, direction);
  3231|     if (alloc)
  3232|     {
  3233|         total_free_units -= num_units;
  3234|         if (fn != nullptr)
  3235|         {
  3236|             if (!fn (global_region_left_used))
  3237|             {
  3238|                 delete_region_impl (alloc);
  3239|                 alloc = nullptr;
  3240|             }
  3241|         }
  3242|         if (alloc)
  3243|         {
  3244|             print_map ("alloc: found at the end");
  3245|         }
  3246|     }
  3247|     else
  3248|     {
  3249|         dprintf (REGIONS_LOG, ("couldn't find memory at the end! only %zd bytes left", (global_region_right_used - global_region_left_used)));
  3250|     }
  3251|     leave_spin_lock();
  3252|     return alloc;
  3253| }
  3254| bool region_allocator::allocate_region (int gen_num, size_t size, uint8_t** start, uint8_t** end, allocate_direction direction, region_allocator_callback_fn fn)
  3255| {
  3256|     size_t alignment = region_alignment;
  3257|     size_t alloc_size = align_region_up (size);
  3258|     uint32_t num_units = (uint32_t)(alloc_size / alignment);
  3259|     bool ret = false;
  3260|     uint8_t* alloc = NULL;
  3261|     dprintf (REGIONS_LOG, ("----GET %u-----", num_units));
  3262|     alloc = allocate (num_units, direction, fn);
  3263|     *start = alloc;
  3264|     *end = alloc + alloc_size;
  3265|     ret = (alloc != NULL);
  3266|     gc_etw_segment_type segment_type;
  3267|     if (gen_num == loh_generation)
  3268|     {
  3269|         segment_type = gc_etw_segment_large_object_heap;
  3270|     }
  3271|     else if (gen_num == poh_generation)
  3272|     {
  3273|         segment_type = gc_etw_segment_pinned_object_heap;
  3274|     }
  3275|     else
  3276|     {
  3277|         segment_type = gc_etw_segment_small_object_heap;
  3278|     }
  3279|     FIRE_EVENT(GCCreateSegment_V1, (alloc + sizeof (aligned_plug_and_gap)),
  3280|                                   size - sizeof (aligned_plug_and_gap),
  3281|                                   segment_type);
  3282|     return ret;
  3283| }
  3284| bool region_allocator::allocate_basic_region (int gen_num, uint8_t** start, uint8_t** end, region_allocator_callback_fn fn)
  3285| {
  3286|     return allocate_region (gen_num, region_alignment, start, end, allocate_forward, fn);
  3287| }
  3288| bool region_allocator::allocate_large_region (int gen_num, uint8_t** start, uint8_t** end, allocate_direction direction, size_t size, region_allocator_callback_fn fn)
  3289| {
  3290|     if (size == 0)
  3291|         size = large_region_alignment;
  3292|     else
  3293|     {
  3294|         assert (round_up_power2(large_region_alignment) == large_region_alignment);
  3295|         size = (size + (large_region_alignment - 1)) & ~(large_region_alignment - 1);
  3296|     }
  3297|     return allocate_region (gen_num, size, start, end, direction, fn);
  3298| }
  3299| void region_allocator::delete_region (uint8_t* region_start)
  3300| {
  3301|     enter_spin_lock();
  3302|     delete_region_impl (region_start);
  3303|     leave_spin_lock();
  3304| }
  3305| void region_allocator::delete_region_impl (uint8_t* region_start)
  3306| {
  3307|     ASSERT_HOLDING_SPIN_LOCK (&region_allocator_lock);
  3308|     assert (is_region_aligned (region_start));
  3309|     print_map ("before delete");
  3310|     uint32_t* current_index = region_map_index_of (region_start);
  3311|     uint32_t current_val = *current_index;
  3312|     assert (!is_unit_memory_free (current_val));
  3313|     dprintf (REGIONS_LOG, ("----DEL %d (%u units)-----", (*current_index - *region_map_left_start), current_val));
  3314|     uint32_t* region_end_index = current_index + current_val;
  3315|     uint8_t* region_end = region_address_of (region_end_index);
  3316|     int free_block_size = current_val;
  3317|     uint32_t* free_index = current_index;
  3318|     if (free_index <= region_map_left_end)
  3319|     {
  3320|         num_left_used_free_units += free_block_size;
  3321|     }
  3322|     else
  3323|     {
  3324|         assert (free_index >= region_map_right_start);
  3325|         num_right_used_free_units += free_block_size;
  3326|     }
  3327|     if ((current_index != region_map_left_start) && (current_index != region_map_right_start))
  3328|     {
  3329|         uint32_t previous_val = *(current_index - 1);
  3330|         if (is_unit_memory_free(previous_val))
  3331|         {
  3332|             uint32_t previous_size = get_num_units (previous_val);
  3333|             free_index -= previous_size;
  3334|             free_block_size += previous_size;
  3335|         }
  3336|     }
  3337|     if ((region_end != global_region_left_used) && (region_end != global_region_end))
  3338|     {
  3339|         uint32_t next_val = *region_end_index;
  3340|         if (is_unit_memory_free(next_val))
  3341|         {
  3342|             uint32_t next_size = get_num_units (next_val);
  3343|             free_block_size += next_size;
  3344|             region_end += next_size;
  3345|         }
  3346|     }
  3347|     if (region_end == global_region_left_used)
  3348|     {
  3349|         num_left_used_free_units -= free_block_size;
  3350|         region_map_left_end = free_index;
  3351|         dprintf (REGIONS_LOG, ("adjust global left used from %p to %p",
  3352|             global_region_left_used, region_address_of (free_index)));
  3353|         global_region_left_used = region_address_of (free_index);
  3354|     }
  3355|     else if (region_start == global_region_right_used)
  3356|     {
  3357|         num_right_used_free_units -= free_block_size;
  3358|         region_map_right_start = free_index + free_block_size;
  3359|         dprintf (REGIONS_LOG, ("adjust global right used from %p to %p",
  3360|             global_region_right_used, region_address_of (free_index + free_block_size)));
  3361|         global_region_right_used = region_address_of (free_index + free_block_size);
  3362|     }
  3363|     else
  3364|     {
  3365|         make_free_block (free_index, free_block_size);
  3366|     }
  3367|     total_free_units += current_val;
  3368|     print_map ("after delete");
  3369| }
  3370| void region_allocator::move_highest_free_regions (int64_t n, bool small_region_p, region_free_list to_free_list[count_free_region_kinds])
  3371| {
  3372|     assert (n > 0);
  3373|     uint32_t* current_index = region_map_left_end - 1;
  3374|     uint32_t* lowest_index = region_map_left_start;
  3375|     while (current_index >= lowest_index)
  3376|     {
  3377|         uint32_t current_val = *current_index;
  3378|         uint32_t current_num_units = get_num_units (current_val);
  3379|         bool free_p = is_unit_memory_free (current_val);
  3380|         if (!free_p && ((current_num_units == 1) == small_region_p))
  3381|         {
  3382|             uint32_t* index = current_index - (current_num_units - 1);
  3383|             heap_segment* region = get_region_info (region_address_of (index));
  3384|             if (is_free_region (region) && !region_free_list::is_on_free_list (region, to_free_list))
  3385|             {
  3386|                 if (n >= current_num_units)
  3387|                 {
  3388|                     n -= current_num_units;
  3389|                     region_free_list::unlink_region (region);
  3390|                     region_free_list::add_region (region, to_free_list);
  3391|                 }
  3392|                 else
  3393|                 {
  3394|                     break;
  3395|                 }
  3396|             }
  3397|         }
  3398|         current_index -= current_num_units;
  3399|     }
  3400| }
  3401| #endif //USE_REGIONS
  3402| inline
  3403| uint8_t* align_on_segment (uint8_t* add)
  3404| {
  3405|     return (uint8_t*)((size_t)(add + (((size_t)1 << gc_heap::min_segment_size_shr) - 1)) & ~(((size_t)1 << gc_heap::min_segment_size_shr) - 1));
  3406| }
  3407| inline
  3408| uint8_t* align_lower_segment (uint8_t* add)
  3409| {
  3410|     return (uint8_t*)((size_t)(add) & ~(((size_t)1 << gc_heap::min_segment_size_shr) - 1));
  3411| }
  3412| size_t size_seg_mapping_table_of (uint8_t* from, uint8_t* end)
  3413| {
  3414|     from = align_lower_segment (from);
  3415|     end = align_on_segment (end);
  3416|     dprintf (1, ("from: %p, end: %p, size: %zx", from, end,
  3417|         sizeof (seg_mapping)*(((size_t)(end - from) >> gc_heap::min_segment_size_shr))));
  3418|     return sizeof (seg_mapping)*((size_t)(end - from) >> gc_heap::min_segment_size_shr);
  3419| }
  3420| size_t size_region_to_generation_table_of (uint8_t* from, uint8_t* end)
  3421| {
  3422|     dprintf (1, ("from: %p, end: %p, size: %zx", from, end,
  3423|         sizeof (uint8_t)*(((size_t)(end - from) >> gc_heap::min_segment_size_shr))));
  3424|     return sizeof (uint8_t)*((size_t)(end - from) >> gc_heap::min_segment_size_shr);
  3425| }
  3426| inline
  3427| size_t seg_mapping_word_of (uint8_t* add)
  3428| {
  3429|     return (size_t)add >> gc_heap::min_segment_size_shr;
  3430| }
  3431| #ifdef FEATURE_BASICFREEZE
  3432| inline
  3433| size_t ro_seg_begin_index (heap_segment* seg)
  3434| {
  3435| #ifdef USE_REGIONS
  3436|     size_t begin_index = (size_t)heap_segment_mem (seg) >> gc_heap::min_segment_size_shr;
  3437| #else
  3438|     size_t begin_index = (size_t)seg >> gc_heap::min_segment_size_shr;
  3439| #endif //USE_REGIONS
  3440|     begin_index = max (begin_index, (size_t)g_gc_lowest_address >> gc_heap::min_segment_size_shr);
  3441|     return begin_index;
  3442| }
  3443| inline
  3444| size_t ro_seg_end_index (heap_segment* seg)
  3445| {
  3446|     size_t end_index = (size_t)(heap_segment_reserved (seg) - 1) >> gc_heap::min_segment_size_shr;
  3447|     end_index = min (end_index, (size_t)g_gc_highest_address >> gc_heap::min_segment_size_shr);
  3448|     return end_index;
  3449| }
  3450| void seg_mapping_table_add_ro_segment (heap_segment* seg)
  3451| {
  3452|     if ((heap_segment_reserved (seg) <= g_gc_lowest_address) || (heap_segment_mem (seg) >= g_gc_highest_address))
  3453|         return;
  3454|     for (size_t entry_index = ro_seg_begin_index (seg); entry_index <= ro_seg_end_index (seg); entry_index++)
  3455|     {
  3456| #ifdef USE_REGIONS
  3457|         heap_segment* region = (heap_segment*)&seg_mapping_table[entry_index];
  3458|         heap_segment_allocated (region) = (uint8_t*)ro_in_entry;
  3459| #else
  3460|         seg_mapping_table[entry_index].seg1 = (heap_segment*)((size_t)seg_mapping_table[entry_index].seg1 | ro_in_entry);
  3461| #endif //USE_REGIONS
  3462|     }
  3463| }
  3464| void seg_mapping_table_remove_ro_segment (heap_segment* seg)
  3465| {
  3466|     UNREFERENCED_PARAMETER(seg);
  3467| #if 0
  3468| #endif //0
  3469| }
  3470| heap_segment* ro_segment_lookup (uint8_t* o)
  3471| {
  3472|     uint8_t* ro_seg_start = o;
  3473|     heap_segment* seg = (heap_segment*)gc_heap::seg_table->lookup (ro_seg_start);
  3474|     if (ro_seg_start && in_range_for_segment (o, seg))
  3475|         return seg;
  3476|     else
  3477|         return 0;
  3478| }
  3479| #endif //FEATURE_BASICFREEZE
  3480| void gc_heap::seg_mapping_table_add_segment (heap_segment* seg, gc_heap* hp)
  3481| {
  3482| #ifndef USE_REGIONS
  3483|     size_t seg_end = (size_t)(heap_segment_reserved (seg) - 1);
  3484|     size_t begin_index = (size_t)seg >> gc_heap::min_segment_size_shr;
  3485|     seg_mapping* begin_entry = &seg_mapping_table[begin_index];
  3486|     size_t end_index = seg_end >> gc_heap::min_segment_size_shr;
  3487|     seg_mapping* end_entry = &seg_mapping_table[end_index];
  3488|     dprintf (2, ("adding seg %p(%zd)-%p(%zd)",
  3489|         seg, begin_index, heap_segment_reserved (seg), end_index));
  3490|     dprintf (2, ("before add: begin entry%zd: boundary: %p; end entry: %zd: boundary: %p",
  3491|         begin_index, (seg_mapping_table[begin_index].boundary + 1),
  3492|         end_index, (seg_mapping_table[end_index].boundary + 1)));
  3493| #ifdef MULTIPLE_HEAPS
  3494| #ifdef SIMPLE_DPRINTF
  3495|     dprintf (2, ("begin %zd: h0: %p(%d), h1: %p(%d); end %zd: h0: %p(%d), h1: %p(%d)",
  3496|         begin_index, (uint8_t*)(begin_entry->h0), (begin_entry->h0 ? begin_entry->h0->heap_number : -1),
  3497|         (uint8_t*)(begin_entry->h1), (begin_entry->h1 ? begin_entry->h1->heap_number : -1),
  3498|         end_index, (uint8_t*)(end_entry->h0), (end_entry->h0 ? end_entry->h0->heap_number : -1),
  3499|         (uint8_t*)(end_entry->h1), (end_entry->h1 ? end_entry->h1->heap_number : -1)));
  3500| #endif //SIMPLE_DPRINTF
  3501|     assert (end_entry->boundary == 0);
  3502|     assert (end_entry->h0 == 0);
  3503|     end_entry->h0 = hp;
  3504|     assert (begin_entry->h1 == 0);
  3505|     begin_entry->h1 = hp;
  3506| #else
  3507|     UNREFERENCED_PARAMETER(hp);
  3508| #endif //MULTIPLE_HEAPS
  3509|     end_entry->boundary = (uint8_t*)seg_end;
  3510|     dprintf (2, ("set entry %zd seg1 and %zd seg0 to %p", begin_index, end_index, seg));
  3511|     assert ((begin_entry->seg1 == 0) || ((size_t)(begin_entry->seg1) == ro_in_entry));
  3512|     begin_entry->seg1 = (heap_segment*)((size_t)(begin_entry->seg1) | (size_t)seg);
  3513|     end_entry->seg0 = seg;
  3514|     for (size_t entry_index = (begin_index + 1); entry_index <= (end_index - 1); entry_index++)
  3515|     {
  3516|         assert (seg_mapping_table[entry_index].boundary == 0);
  3517| #ifdef MULTIPLE_HEAPS
  3518|         assert (seg_mapping_table[entry_index].h0 == 0);
  3519|         seg_mapping_table[entry_index].h1 = hp;
  3520| #endif //MULTIPLE_HEAPS
  3521|         seg_mapping_table[entry_index].seg1 = seg;
  3522|     }
  3523|     dprintf (2, ("after add: begin entry%zd: boundary: %p; end entry: %zd: boundary: %p",
  3524|         begin_index, (seg_mapping_table[begin_index].boundary + 1),
  3525|         end_index, (seg_mapping_table[end_index].boundary + 1)));
  3526| #if defined(MULTIPLE_HEAPS) && defined(SIMPLE_DPRINTF)
  3527|     dprintf (2, ("begin %zd: h0: %p(%d), h1: %p(%d); end: %zd h0: %p(%d), h1: %p(%d)",
  3528|         begin_index, (uint8_t*)(begin_entry->h0), (begin_entry->h0 ? begin_entry->h0->heap_number : -1),
  3529|         (uint8_t*)(begin_entry->h1), (begin_entry->h1 ? begin_entry->h1->heap_number : -1),
  3530|         end_index, (uint8_t*)(end_entry->h0), (end_entry->h0 ? end_entry->h0->heap_number : -1),
  3531|         (uint8_t*)(end_entry->h1), (end_entry->h1 ? end_entry->h1->heap_number : -1)));
  3532| #endif //MULTIPLE_HEAPS && SIMPLE_DPRINTF
  3533| #endif //!USE_REGIONS
  3534| }
  3535| void gc_heap::seg_mapping_table_remove_segment (heap_segment* seg)
  3536| {
  3537| #ifndef USE_REGIONS
  3538|     size_t seg_end = (size_t)(heap_segment_reserved (seg) - 1);
  3539|     size_t begin_index = (size_t)seg >> gc_heap::min_segment_size_shr;
  3540|     seg_mapping* begin_entry = &seg_mapping_table[begin_index];
  3541|     size_t end_index = seg_end >> gc_heap::min_segment_size_shr;
  3542|     seg_mapping* end_entry = &seg_mapping_table[end_index];
  3543|     dprintf (2, ("removing seg %p(%zd)-%p(%zd)",
  3544|         seg, begin_index, heap_segment_reserved (seg), end_index));
  3545|     assert (end_entry->boundary == (uint8_t*)seg_end);
  3546|     end_entry->boundary = 0;
  3547| #ifdef MULTIPLE_HEAPS
  3548|     gc_heap* hp = heap_segment_heap (seg);
  3549|     assert (end_entry->h0 == hp);
  3550|     end_entry->h0 = 0;
  3551|     assert (begin_entry->h1 == hp);
  3552|     begin_entry->h1 = 0;
  3553| #endif //MULTIPLE_HEAPS
  3554|     assert (begin_entry->seg1 != 0);
  3555|     begin_entry->seg1 = (heap_segment*)((size_t)(begin_entry->seg1) & ro_in_entry);
  3556|     end_entry->seg0 = 0;
  3557|     for (size_t entry_index = (begin_index + 1); entry_index <= (end_index - 1); entry_index++)
  3558|     {
  3559|         assert (seg_mapping_table[entry_index].boundary == 0);
  3560| #ifdef MULTIPLE_HEAPS
  3561|         assert (seg_mapping_table[entry_index].h0 == 0);
  3562|         assert (seg_mapping_table[entry_index].h1 == hp);
  3563|         seg_mapping_table[entry_index].h1 = 0;
  3564| #endif //MULTIPLE_HEAPS
  3565|         seg_mapping_table[entry_index].seg1 = 0;
  3566|     }
  3567|     dprintf (2, ("after remove: begin entry%zd: boundary: %p; end entry: %zd: boundary: %p",
  3568|         begin_index, (seg_mapping_table[begin_index].boundary + 1),
  3569|         end_index, (seg_mapping_table[end_index].boundary + 1)));
  3570| #ifdef MULTIPLE_HEAPS
  3571|     dprintf (2, ("begin %zd: h0: %p, h1: %p; end: %zd h0: %p, h1: %p",
  3572|         begin_index, (uint8_t*)(begin_entry->h0), (uint8_t*)(begin_entry->h1),
  3573|         end_index, (uint8_t*)(end_entry->h0), (uint8_t*)(end_entry->h1)));
  3574| #endif //MULTIPLE_HEAPS
  3575| #endif //!USE_REGIONS
  3576| }
  3577| #ifdef MULTIPLE_HEAPS
  3578| inline
  3579| gc_heap* seg_mapping_table_heap_of_worker (uint8_t* o)
  3580| {
  3581|     size_t index = (size_t)o >> gc_heap::min_segment_size_shr;
  3582|     seg_mapping* entry = &seg_mapping_table[index];
  3583| #ifdef USE_REGIONS
  3584|     gc_heap* hp = heap_segment_heap ((heap_segment*)entry);
  3585| #else
  3586|     gc_heap* hp = ((o > entry->boundary) ? entry->h1 : entry->h0);
  3587|     dprintf (2, ("checking obj %p, index is %zd, entry: boundary: %p, h0: %p, seg0: %p, h1: %p, seg1: %p",
  3588|         o, index, (entry->boundary + 1),
  3589|         (uint8_t*)(entry->h0), (uint8_t*)(entry->seg0),
  3590|         (uint8_t*)(entry->h1), (uint8_t*)(entry->seg1)));
  3591| #ifdef _DEBUG
  3592|     heap_segment* seg = ((o > entry->boundary) ? entry->seg1 : entry->seg0);
  3593| #ifdef FEATURE_BASICFREEZE
  3594|     if ((size_t)seg & ro_in_entry)
  3595|         seg = (heap_segment*)((size_t)seg & ~ro_in_entry);
  3596| #endif //FEATURE_BASICFREEZE
  3597| #ifdef TRACE_GC
  3598|     if (seg)
  3599|     {
  3600|         if (in_range_for_segment (o, seg))
  3601|         {
  3602|             dprintf (2, ("obj %p belongs to segment %p(-%p)", o, seg, (uint8_t*)heap_segment_allocated (seg)));
  3603|         }
  3604|         else
  3605|         {
  3606|             dprintf (2, ("found seg %p(-%p) for obj %p, but it's not on the seg",
  3607|                 seg, (uint8_t*)heap_segment_allocated (seg), o));
  3608|         }
  3609|     }
  3610|     else
  3611|     {
  3612|         dprintf (2, ("could not find obj %p in any existing segments", o));
  3613|     }
  3614| #endif //TRACE_GC
  3615| #endif //_DEBUG
  3616| #endif //USE_REGIONS
  3617|     return hp;
  3618| }
  3619| gc_heap* seg_mapping_table_heap_of (uint8_t* o)
  3620| {
  3621|     if ((o < g_gc_lowest_address) || (o >= g_gc_highest_address))
  3622|         return 0;
  3623|     return seg_mapping_table_heap_of_worker (o);
  3624| }
  3625| gc_heap* seg_mapping_table_heap_of_gc (uint8_t* o)
  3626| {
  3627| #ifdef FEATURE_BASICFREEZE
  3628|     if ((o < g_gc_lowest_address) || (o >= g_gc_highest_address))
  3629|         return 0;
  3630| #endif //FEATURE_BASICFREEZE
  3631|     return seg_mapping_table_heap_of_worker (o);
  3632| }
  3633| #endif //MULTIPLE_HEAPS
  3634| heap_segment* seg_mapping_table_segment_of (uint8_t* o)
  3635| {
  3636| #ifdef FEATURE_BASICFREEZE
  3637|     if ((o < g_gc_lowest_address) || (o >= g_gc_highest_address))
  3638|         return ro_segment_lookup (o);
  3639| #endif //FEATURE_BASICFREEZE
  3640|     size_t index = (size_t)o >> gc_heap::min_segment_size_shr;
  3641|     seg_mapping* entry = &seg_mapping_table[index];
  3642| #ifdef USE_REGIONS
  3643|     ptrdiff_t first_field = (ptrdiff_t)heap_segment_allocated ((heap_segment*)entry);
  3644|     if (first_field == 0)
  3645|     {
  3646|         dprintf (REGIONS_LOG, ("asked for seg for %p, in a freed region mem: %p, committed %p",
  3647|             o, heap_segment_mem ((heap_segment*)entry),
  3648|             heap_segment_committed ((heap_segment*)entry)));
  3649|         return 0;
  3650|     }
  3651|     assert (first_field != 0);
  3652|     assert (first_field != ro_in_entry);
  3653|     if (first_field < 0)
  3654|     {
  3655|         index += first_field;
  3656|     }
  3657|     heap_segment* seg = (heap_segment*)&seg_mapping_table[index];
  3658| #else //USE_REGIONS
  3659|     dprintf (2, ("checking obj %p, index is %zd, entry: boundary: %p, seg0: %p, seg1: %p",
  3660|         o, index, (entry->boundary + 1),
  3661|         (uint8_t*)(entry->seg0), (uint8_t*)(entry->seg1)));
  3662|     heap_segment* seg = ((o > entry->boundary) ? entry->seg1 : entry->seg0);
  3663| #ifdef FEATURE_BASICFREEZE
  3664|     if ((size_t)seg & ro_in_entry)
  3665|         seg = (heap_segment*)((size_t)seg & ~ro_in_entry);
  3666| #endif //FEATURE_BASICFREEZE
  3667| #endif //USE_REGIONS
  3668|     if (seg)
  3669|     {
  3670|         if (in_range_for_segment (o, seg))
  3671|         {
  3672|             dprintf (2, ("obj %p belongs to segment %p(-%p)", o, (uint8_t*)heap_segment_mem(seg), (uint8_t*)heap_segment_reserved(seg)));
  3673|         }
  3674|         else
  3675|         {
  3676|             dprintf (2, ("found seg %p(-%p) for obj %p, but it's not on the seg, setting it to 0",
  3677|                 (uint8_t*)heap_segment_mem(seg), (uint8_t*)heap_segment_reserved(seg), o));
  3678|             seg = 0;
  3679|         }
  3680|     }
  3681|     else
  3682|     {
  3683|         dprintf (2, ("could not find obj %p in any existing segments", o));
  3684|     }
  3685| #ifdef FEATURE_BASICFREEZE
  3686|     if (!seg)
  3687|     {
  3688|         seg = ro_segment_lookup (o);
  3689|         if (seg && !in_range_for_segment (o, seg))
  3690|             seg = 0;
  3691|     }
  3692| #endif //FEATURE_BASICFREEZE
  3693|     return seg;
  3694| }
  3695| size_t gcard_of ( uint8_t*);
  3696| #define GC_MARKED       (size_t)0x1
  3697| #ifdef DOUBLY_LINKED_FL
  3698| #define BGC_MARKED_BY_FGC (size_t)0x2
  3699| #define MAKE_FREE_OBJ_IN_COMPACT (size_t)0x4
  3700| #define ALLOWED_SPECIAL_HEADER_BITS (GC_MARKED|BGC_MARKED_BY_FGC|MAKE_FREE_OBJ_IN_COMPACT)
  3701| #else //DOUBLY_LINKED_FL
  3702| #define ALLOWED_SPECIAL_HEADER_BITS (GC_MARKED)
  3703| #endif //!DOUBLY_LINKED_FL
  3704| #ifdef HOST_64BIT
  3705| #define SPECIAL_HEADER_BITS (0x7)
  3706| #else
  3707| #define SPECIAL_HEADER_BITS (0x3)
  3708| #endif
  3709| #define slot(i, j) ((uint8_t**)(i))[(j)+1]
  3710| #define free_object_base_size (plug_skew + sizeof(ArrayBase))
  3711| #define free_list_slot(x) ((uint8_t**)(x))[2]
  3712| #define free_list_undo(x) ((uint8_t**)(x))[-1]
  3713| #define UNDO_EMPTY ((uint8_t*)1)
  3714| #ifdef DOUBLY_LINKED_FL
  3715| #define free_list_prev(x) ((uint8_t**)(x))[3]
  3716| #define PREV_EMPTY ((uint8_t*)1)
  3717| void check_and_clear_in_free_list (uint8_t* o, size_t size)
  3718| {
  3719|     if (size >= min_free_list)
  3720|     {
  3721|         free_list_prev (o) = PREV_EMPTY;
  3722|     }
  3723| }
  3724| void clear_prev_bit (uint8_t* o, size_t size)
  3725| {
  3726|     if (size >= min_free_list)
  3727|     {
  3728|         free_list_prev (o) = 0;
  3729|     }
  3730| }
  3731| #endif //DOUBLY_LINKED_FL
  3732| class CObjectHeader : public Object
  3733| {
  3734| public:
  3735| #if defined(FEATURE_NATIVEAOT) || defined(BUILD_AS_STANDALONE)
  3736|     uint32_t GetNumComponents()
  3737|     {
  3738|         return ((ArrayBase *)this)->GetNumComponents();
  3739|     }
  3740|     void Validate(BOOL bDeep=TRUE, BOOL bVerifyNextHeader = FALSE, BOOL bVerifySyncBlock = FALSE)
  3741|     {
  3742|         UNREFERENCED_PARAMETER(bVerifyNextHeader);
  3743|         UNREFERENCED_PARAMETER(bVerifySyncBlock);
  3744|         MethodTable * pMT = GetMethodTable();
  3745|         _ASSERTE(pMT->SanityCheck());
  3746|         bool noRangeChecks =
  3747|             (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_NO_RANGE_CHECKS) == GCConfig::HEAPVERIFY_NO_RANGE_CHECKS;
  3748|         BOOL fSmallObjectHeapPtr = FALSE, fLargeObjectHeapPtr = FALSE;
  3749|         if (!noRangeChecks)
  3750|         {
  3751|             fSmallObjectHeapPtr = g_theGCHeap->IsHeapPointer(this, TRUE);
  3752|             if (!fSmallObjectHeapPtr)
  3753|                 fLargeObjectHeapPtr = g_theGCHeap->IsHeapPointer(this);
  3754|             _ASSERTE(fSmallObjectHeapPtr || fLargeObjectHeapPtr);
  3755|         }
  3756| #ifdef FEATURE_STRUCTALIGN
  3757|         _ASSERTE(IsStructAligned((uint8_t *)this, GetMethodTable()->GetBaseAlignment()));
  3758| #endif // FEATURE_STRUCTALIGN
  3759| #if defined(FEATURE_64BIT_ALIGNMENT) && !defined(FEATURE_NATIVEAOT)
  3760|         if (pMT->RequiresAlign8())
  3761|         {
  3762|             _ASSERTE((((size_t)this) & 0x7) == (pMT->IsValueType() ? 4U : 0U));
  3763|         }
  3764| #endif // FEATURE_64BIT_ALIGNMENT
  3765| #ifdef VERIFY_HEAP
  3766|         if (bDeep && (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC))
  3767|             g_theGCHeap->ValidateObjectMember(this);
  3768| #endif
  3769|         if (fSmallObjectHeapPtr)
  3770|         {
  3771| #ifdef FEATURE_BASICFREEZE
  3772|             _ASSERTE(!g_theGCHeap->IsLargeObject(this) || g_theGCHeap->IsInFrozenSegment(this));
  3773| #else
  3774|             _ASSERTE(!g_theGCHeap->IsLargeObject(this));
  3775| #endif
  3776|         }
  3777|     }
  3778|     void ValidateHeap(BOOL bDeep)
  3779|     {
  3780|         Validate(bDeep);
  3781|     }
  3782| #endif //FEATURE_NATIVEAOT || BUILD_AS_STANDALONE
  3783|     MethodTable    *GetMethodTable() const
  3784|     {
  3785|         return( (MethodTable *) (((size_t) RawGetMethodTable()) & (~SPECIAL_HEADER_BITS)));
  3786|     }
  3787|     void SetMarked()
  3788|     {
  3789|         _ASSERTE(RawGetMethodTable());
  3790|         RawSetMethodTable((MethodTable *) (((size_t) RawGetMethodTable()) | GC_MARKED));
  3791|     }
  3792|     BOOL IsMarked() const
  3793|     {
  3794|         return !!(((size_t)RawGetMethodTable()) & GC_MARKED);
  3795|     }
  3796|     void SetPinned()
  3797|     {
  3798|         assert (!(gc_heap::settings.concurrent));
  3799|         GetHeader()->SetGCBit();
  3800|     }
  3801|     BOOL IsPinned() const
  3802|     {
  3803|         return !!((((CObjectHeader*)this)->GetHeader()->GetBits()) & BIT_SBLK_GC_RESERVE);
  3804|     }
  3805|     void ClearMarked()
  3806|     {
  3807| #ifdef DOUBLY_LINKED_FL
  3808|         RawSetMethodTable ((MethodTable *)(((size_t) RawGetMethodTable()) & (~GC_MARKED)));
  3809| #else
  3810|         RawSetMethodTable (GetMethodTable());
  3811| #endif //DOUBLY_LINKED_FL
  3812|     }
  3813| #ifdef DOUBLY_LINKED_FL
  3814|     void SetBGCMarkBit()
  3815|     {
  3816|         RawSetMethodTable((MethodTable *) (((size_t) RawGetMethodTable()) | BGC_MARKED_BY_FGC));
  3817|     }
  3818|     BOOL IsBGCMarkBitSet() const
  3819|     {
  3820|         return !!(((size_t)RawGetMethodTable()) & BGC_MARKED_BY_FGC);
  3821|     }
  3822|     void ClearBGCMarkBit()
  3823|     {
  3824|         RawSetMethodTable((MethodTable *)(((size_t) RawGetMethodTable()) & (~BGC_MARKED_BY_FGC)));
  3825|     }
  3826|     void SetFreeObjInCompactBit()
  3827|     {
  3828|         RawSetMethodTable((MethodTable *) (((size_t) RawGetMethodTable()) | MAKE_FREE_OBJ_IN_COMPACT));
  3829|     }
  3830|     BOOL IsFreeObjInCompactBitSet() const
  3831|     {
  3832|         return !!(((size_t)RawGetMethodTable()) & MAKE_FREE_OBJ_IN_COMPACT);
  3833|     }
  3834|     void ClearFreeObjInCompactBit()
  3835|     {
  3836| #ifdef _DEBUG
  3837|         Validate(FALSE);
  3838| #endif //_DEBUG
  3839|         RawSetMethodTable((MethodTable *)(((size_t) RawGetMethodTable()) & (~MAKE_FREE_OBJ_IN_COMPACT)));
  3840|     }
  3841| #endif //DOUBLY_LINKED_FL
  3842|     size_t ClearSpecialBits()
  3843|     {
  3844|         size_t special_bits = ((size_t)RawGetMethodTable()) & SPECIAL_HEADER_BITS;
  3845|         if (special_bits != 0)
  3846|         {
  3847|             assert ((special_bits & (~ALLOWED_SPECIAL_HEADER_BITS)) == 0);
  3848|             RawSetMethodTable ((MethodTable*)(((size_t)RawGetMethodTable()) & ~(SPECIAL_HEADER_BITS)));
  3849|         }
  3850|         return special_bits;
  3851|     }
  3852|     void SetSpecialBits (size_t special_bits)
  3853|     {
  3854|         assert ((special_bits & (~ALLOWED_SPECIAL_HEADER_BITS)) == 0);
  3855|         if (special_bits != 0)
  3856|         {
  3857|             RawSetMethodTable ((MethodTable*)(((size_t)RawGetMethodTable()) | special_bits));
  3858|         }
  3859|     }
  3860|     CGCDesc *GetSlotMap ()
  3861|     {
  3862|         assert (GetMethodTable()->ContainsPointers());
  3863|         return CGCDesc::GetCGCDescFromMT(GetMethodTable());
  3864|     }
  3865|     void SetFree(size_t size)
  3866|     {
  3867|         assert (size >= free_object_base_size);
  3868|         assert (g_gc_pFreeObjectMethodTable->GetBaseSize() == free_object_base_size);
  3869|         assert (g_gc_pFreeObjectMethodTable->RawGetComponentSize() == 1);
  3870|         RawSetMethodTable( g_gc_pFreeObjectMethodTable );
  3871|         size_t* numComponentsPtr = (size_t*) &((uint8_t*) this)[ArrayBase::GetOffsetOfNumComponents()];
  3872|         *numComponentsPtr = size - free_object_base_size;
  3873| #ifdef VERIFY_HEAP
  3874|         assert (*numComponentsPtr >= 0);
  3875|         if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
  3876|         {
  3877|             memset (((uint8_t*)this)+sizeof(ArrayBase), 0xcc, *numComponentsPtr);
  3878| #ifdef DOUBLY_LINKED_FL
  3879|             if (*numComponentsPtr > 0)
  3880|             {
  3881|                 free_list_slot (this) = 0;
  3882|             }
  3883| #endif //DOUBLY_LINKED_FL
  3884|         }
  3885| #endif //VERIFY_HEAP
  3886| #ifdef DOUBLY_LINKED_FL
  3887|         check_and_clear_in_free_list ((uint8_t*)this, size);
  3888| #endif //DOUBLY_LINKED_FL
  3889|     }
  3890|     void UnsetFree()
  3891|     {
  3892|         size_t size = free_object_base_size - plug_skew;
  3893|         PTR_PTR m = (PTR_PTR) this;
  3894|         for (size_t i = 0; i < size / sizeof(PTR_PTR); i++)
  3895|             *(m++) = 0;
  3896|     }
  3897|     BOOL IsFree () const
  3898|     {
  3899|         return (GetMethodTable() == g_gc_pFreeObjectMethodTable);
  3900|     }
  3901| #ifdef FEATURE_STRUCTALIGN
  3902|     int GetRequiredAlignment () const
  3903|     {
  3904|         return GetMethodTable()->GetRequiredAlignment();
  3905|     }
  3906| #endif // FEATURE_STRUCTALIGN
  3907|     BOOL ContainsPointers() const
  3908|     {
  3909|         return GetMethodTable()->ContainsPointers();
  3910|     }
  3911| #ifdef COLLECTIBLE_CLASS
  3912|     BOOL Collectible() const
  3913|     {
  3914|         return GetMethodTable()->Collectible();
  3915|     }
  3916|     FORCEINLINE BOOL ContainsPointersOrCollectible() const
  3917|     {
  3918|         MethodTable *pMethodTable = GetMethodTable();
  3919|         return (pMethodTable->ContainsPointers() || pMethodTable->Collectible());
  3920|     }
  3921| #endif //COLLECTIBLE_CLASS
  3922|     Object* GetObjectBase() const
  3923|     {
  3924|         return (Object*) this;
  3925|     }
  3926| };
  3927| #define header(i) ((CObjectHeader*)(i))
  3928| #define method_table(o) ((CObjectHeader*)(o))->GetMethodTable()
  3929| #ifdef DOUBLY_LINKED_FL
  3930| inline
  3931| BOOL is_on_free_list (uint8_t* o, size_t size)
  3932| {
  3933|     if (size >= min_free_list)
  3934|     {
  3935|         if (header(o)->GetMethodTable() == g_gc_pFreeObjectMethodTable)
  3936|         {
  3937|             return (free_list_prev (o) != PREV_EMPTY);
  3938|         }
  3939|     }
  3940|     return FALSE;
  3941| }
  3942| inline
  3943| void set_plug_bgc_mark_bit (uint8_t* node)
  3944| {
  3945|     header(node)->SetBGCMarkBit();
  3946| }
  3947| inline
  3948| BOOL is_plug_bgc_mark_bit_set (uint8_t* node)
  3949| {
  3950|     return header(node)->IsBGCMarkBitSet();
  3951| }
  3952| inline
  3953| void clear_plug_bgc_mark_bit (uint8_t* node)
  3954| {
  3955|     header(node)->ClearBGCMarkBit();
  3956| }
  3957| inline
  3958| void set_free_obj_in_compact_bit (uint8_t* node)
  3959| {
  3960|     header(node)->SetFreeObjInCompactBit();
  3961| }
  3962| inline
  3963| BOOL is_free_obj_in_compact_bit_set (uint8_t* node)
  3964| {
  3965|     return header(node)->IsFreeObjInCompactBitSet();
  3966| }
  3967| inline
  3968| void clear_free_obj_in_compact_bit (uint8_t* node)
  3969| {
  3970|     header(node)->ClearFreeObjInCompactBit();
  3971| }
  3972| #endif //DOUBLY_LINKED_FL
  3973| #ifdef SHORT_PLUGS
  3974| inline
  3975| void set_plug_padded (uint8_t* node)
  3976| {
  3977|     header(node)->SetMarked();
  3978| }
  3979| inline
  3980| void clear_plug_padded (uint8_t* node)
  3981| {
  3982|     header(node)->ClearMarked();
  3983| }
  3984| inline
  3985| BOOL is_plug_padded (uint8_t* node)
  3986| {
  3987|     return header(node)->IsMarked();
  3988| }
  3989| #else //SHORT_PLUGS
  3990| inline void set_plug_padded (uint8_t* node){}
  3991| inline void clear_plug_padded (uint8_t* node){}
  3992| inline
  3993| BOOL is_plug_padded (uint8_t* node){return FALSE;}
  3994| #endif //SHORT_PLUGS
  3995| inline
  3996| size_t clear_special_bits (uint8_t* node)
  3997| {
  3998|     return header(node)->ClearSpecialBits();
  3999| }
  4000| inline
  4001| void set_special_bits (uint8_t* node, size_t special_bits)
  4002| {
  4003|     header(node)->SetSpecialBits (special_bits);
  4004| }
  4005| inline size_t unused_array_size(uint8_t * p)
  4006| {
  4007|     assert(((CObjectHeader*)p)->IsFree());
  4008|     size_t* numComponentsPtr = (size_t*)(p + ArrayBase::GetOffsetOfNumComponents());
  4009|     return free_object_base_size + *numComponentsPtr;
  4010| }
  4011| inline
  4012| heap_segment* heap_segment_non_sip (heap_segment* ns)
  4013| {
  4014| #ifdef USE_REGIONS
  4015|     if ((ns == 0) || !heap_segment_swept_in_plan (ns))
  4016|     {
  4017|         return ns;
  4018|     }
  4019|     else
  4020|     {
  4021|         do
  4022|         {
  4023|             if (heap_segment_swept_in_plan (ns))
  4024|             {
  4025|                 dprintf (REGIONS_LOG, ("region %p->%p SIP",
  4026|                     heap_segment_mem (ns), heap_segment_allocated (ns)));
  4027|             }
  4028|             ns = heap_segment_next (ns);
  4029|         } while ((ns != 0) && heap_segment_swept_in_plan (ns));
  4030|         return ns;
  4031|     }
  4032| #else //USE_REGIONS
  4033|     return ns;
  4034| #endif //USE_REGIONS
  4035| }
  4036| inline
  4037| heap_segment* heap_segment_next_non_sip (heap_segment* seg)
  4038| {
  4039|     heap_segment* ns = heap_segment_next (seg);
  4040| #ifdef USE_REGIONS
  4041|     return heap_segment_non_sip (ns);
  4042| #else
  4043|     return ns;
  4044| #endif //USE_REGIONS
  4045| }
  4046| heap_segment* heap_segment_rw (heap_segment* ns)
  4047| {
  4048|     if ((ns == 0) || !heap_segment_read_only_p (ns))
  4049|     {
  4050|         return ns;
  4051|     }
  4052|     else
  4053|     {
  4054|         do
  4055|         {
  4056|             ns = heap_segment_next (ns);
  4057|         } while ((ns != 0) && heap_segment_read_only_p (ns));
  4058|         return ns;
  4059|     }
  4060| }
  4061| heap_segment* heap_segment_next_rw (heap_segment* seg)
  4062| {
  4063|     heap_segment* ns = heap_segment_next (seg);
  4064|     return heap_segment_rw (ns);
  4065| }
  4066| heap_segment* heap_segment_prev_rw (heap_segment* begin, heap_segment* seg)
  4067| {
  4068|     assert (begin != 0);
  4069|     heap_segment* prev = begin;
  4070|     heap_segment* current = heap_segment_next_rw (begin);
  4071|     while (current && current != seg)
  4072|     {
  4073|         prev = current;
  4074|         current = heap_segment_next_rw (current);
  4075|     }
  4076|     if (current == seg)
  4077|     {
  4078|         return prev;
  4079|     }
  4080|     else
  4081|     {
  4082|         return 0;
  4083|     }
  4084| }
  4085| heap_segment* heap_segment_prev (heap_segment* begin, heap_segment* seg)
  4086| {
  4087|     assert (begin != 0);
  4088|     heap_segment* prev = begin;
  4089|     heap_segment* current = heap_segment_next (begin);
  4090|     while (current && current != seg)
  4091|     {
  4092|         prev = current;
  4093|         current = heap_segment_next (current);
  4094|     }
  4095|     if (current == seg)
  4096|     {
  4097|         return prev;
  4098|     }
  4099|     else
  4100|     {
  4101|         return 0;
  4102|     }
  4103| }
  4104| heap_segment* heap_segment_in_range (heap_segment* ns)
  4105| {
  4106|     if ((ns == 0) || heap_segment_in_range_p (ns))
  4107|     {
  4108|         return ns;
  4109|     }
  4110|     else
  4111|     {
  4112|         do
  4113|         {
  4114|             ns = heap_segment_next (ns);
  4115|         } while ((ns != 0) && !heap_segment_in_range_p (ns));
  4116|         return ns;
  4117|     }
  4118| }
  4119| heap_segment* heap_segment_next_in_range (heap_segment* seg)
  4120| {
  4121|     heap_segment* ns = heap_segment_next (seg);
  4122|     return heap_segment_in_range (ns);
  4123| }
  4124| struct imemory_data
  4125| {
  4126|     uint8_t* memory_base;
  4127| };
  4128| struct numa_reserved_block
  4129| {
  4130|     uint8_t*        memory_base;
  4131|     size_t          block_size;
  4132|     numa_reserved_block() : memory_base(nullptr), block_size(0) { }
  4133| };
  4134| struct initial_memory_details
  4135| {
  4136|     imemory_data *initial_memory;
  4137|     imemory_data *initial_normal_heap; // points into initial_memory_array
  4138|     imemory_data *initial_large_heap;  // points into initial_memory_array
  4139|     imemory_data *initial_pinned_heap; // points into initial_memory_array
  4140|     size_t block_size_normal;
  4141|     size_t block_size_large;
  4142|     size_t block_size_pinned;
  4143|     int block_count;                // # of blocks in each
  4144|     int current_block_normal;
  4145|     int current_block_large;
  4146|     int current_block_pinned;
  4147|     enum
  4148|     {
  4149|         ALLATONCE = 1,
  4150|         EACH_GENERATION,
  4151|         EACH_BLOCK,
  4152|         ALLATONCE_SEPARATED_POH,
  4153|         EACH_NUMA_NODE
  4154|     };
  4155|     size_t allocation_pattern;
  4156|     size_t block_size(int i)
  4157|     {
  4158|         switch (i / block_count)
  4159|         {
  4160|             case 0: return block_size_normal;
  4161|             case 1: return block_size_large;
  4162|             case 2: return block_size_pinned;
  4163|             default: __UNREACHABLE();
  4164|         }
  4165|     };
  4166|     void* get_initial_memory (int gen, int h_number)
  4167|     {
  4168|         switch (gen)
  4169|         {
  4170|             case soh_gen0:
  4171|             case soh_gen1:
  4172|             case soh_gen2: return initial_normal_heap[h_number].memory_base;
  4173|             case loh_generation: return initial_large_heap[h_number].memory_base;
  4174|             case poh_generation: return initial_pinned_heap[h_number].memory_base;
  4175|             default: __UNREACHABLE();
  4176|         }
  4177|     };
  4178|     size_t get_initial_size (int gen)
  4179|     {
  4180|         switch (gen)
  4181|         {
  4182|             case soh_gen0:
  4183|             case soh_gen1:
  4184|             case soh_gen2: return block_size_normal;
  4185|             case loh_generation: return block_size_large;
  4186|             case poh_generation: return block_size_pinned;
  4187|             default: __UNREACHABLE();
  4188|         }
  4189|     };
  4190|     int numa_reserved_block_count;
  4191|     numa_reserved_block* numa_reserved_block_table;
  4192| };
  4193| initial_memory_details memory_details;
  4194| BOOL gc_heap::reserve_initial_memory (size_t normal_size, size_t large_size, size_t pinned_size, int num_heaps, bool use_large_pages_p, bool separated_poh_p, uint16_t* heap_no_to_numa_node)
  4195| {
  4196|     BOOL reserve_success = FALSE;
  4197|     assert (memory_details.initial_memory == 0);
  4198|     memory_details.initial_memory = new (nothrow) imemory_data[num_heaps * (total_generation_count - ephemeral_generation_count)];
  4199|     if (memory_details.initial_memory == 0)
  4200|     {
  4201|         dprintf (2, ("failed to reserve %zd bytes for imemory_data",
  4202|             num_heaps * (total_generation_count - ephemeral_generation_count) * sizeof (imemory_data)));
  4203|         return FALSE;
  4204|     }
  4205|     memory_details.initial_normal_heap = memory_details.initial_memory;
  4206|     memory_details.initial_large_heap = memory_details.initial_normal_heap + num_heaps;
  4207|     memory_details.initial_pinned_heap = memory_details.initial_large_heap + num_heaps;
  4208|     memory_details.block_size_normal = normal_size;
  4209|     memory_details.block_size_large = large_size;
  4210|     memory_details.block_size_pinned = pinned_size;
  4211|     memory_details.block_count = num_heaps;
  4212|     memory_details.current_block_normal = 0;
  4213|     memory_details.current_block_large = 0;
  4214|     memory_details.current_block_pinned = 0;
  4215|     g_gc_lowest_address = MAX_PTR;
  4216|     g_gc_highest_address = 0;
  4217|     if (((size_t)MAX_PTR - large_size) < normal_size)
  4218|     {
  4219|         dprintf (2, ("0x%zx + 0x%zx already overflow", normal_size, large_size));
  4220|         return FALSE;
  4221|     }
  4222|     if (((size_t)MAX_PTR / memory_details.block_count) < (normal_size + large_size + pinned_size))
  4223|     {
  4224|         dprintf (2, ("(0x%zx + 0x%zx)*0x%x overflow", normal_size, large_size, memory_details.block_count));
  4225|         return FALSE;
  4226|     }
  4227|     memory_details.numa_reserved_block_count = 0;
  4228|     memory_details.numa_reserved_block_table = nullptr;
  4229|     int numa_node_count = 0;
  4230|     if (heap_no_to_numa_node != nullptr)
  4231|     {
  4232|         uint16_t highest_numa_node = 0;
  4233|         for (int heap_no = 0; heap_no < num_heaps; heap_no++)
  4234|         {
  4235|             uint16_t heap_numa_node = heap_no_to_numa_node[heap_no];
  4236|             highest_numa_node = max (highest_numa_node, heap_numa_node);
  4237|         }
  4238|         assert (highest_numa_node < MAX_SUPPORTED_CPUS);
  4239|         numa_node_count = highest_numa_node + 1;
  4240|         memory_details.numa_reserved_block_count = numa_node_count * (1 + separated_poh_p);
  4241|         memory_details.numa_reserved_block_table = new (nothrow) numa_reserved_block[memory_details.numa_reserved_block_count];
  4242|         if (memory_details.numa_reserved_block_table == nullptr)
  4243|         {
  4244|             dprintf(2, ("failed to reserve %zd bytes for numa_reserved_block data", memory_details.numa_reserved_block_count * sizeof(numa_reserved_block)));
  4245|             memory_details.numa_reserved_block_count = 0;
  4246|         }
  4247|     }
  4248|     if (memory_details.numa_reserved_block_table != nullptr)
  4249|     {
  4250|         size_t merged_pinned_size = separated_poh_p ? 0 : pinned_size;
  4251|         for (int heap_no = 0; heap_no < num_heaps; heap_no++)
  4252|         {
  4253|             uint16_t heap_numa_node = heap_no_to_numa_node[heap_no];
  4254|             numa_reserved_block * block = &memory_details.numa_reserved_block_table[heap_numa_node];
  4255|             block->block_size += normal_size + large_size + merged_pinned_size;
  4256|             if (separated_poh_p)
  4257|             {
  4258|                 numa_reserved_block* pinned_block = &memory_details.numa_reserved_block_table[numa_node_count + heap_numa_node];
  4259|                 pinned_block->block_size += pinned_size;
  4260|             }
  4261|         }
  4262|         bool failure = false;
  4263|         for (int block_index = 0; block_index < memory_details.numa_reserved_block_count; block_index++)
  4264|         {
  4265|             numa_reserved_block * block = &memory_details.numa_reserved_block_table[block_index];
  4266|             if (block->block_size == 0)
  4267|                 continue;
  4268|             int numa_node = block_index % numa_node_count;
  4269|             bool pinned_block = block_index >= numa_node_count;
  4270|             block->memory_base = (uint8_t*)virtual_alloc (block->block_size, use_large_pages_p && !pinned_block, (uint16_t)numa_node);
  4271|             if (block->memory_base == nullptr)
  4272|             {
  4273|                 dprintf(2, ("failed to reserve %zd bytes for on NUMA node %u", block->block_size, numa_node));
  4274|                 failure = true;
  4275|                 break;
  4276|             }
  4277|             else
  4278|             {
  4279|                 g_gc_lowest_address = min(g_gc_lowest_address, block->memory_base);
  4280|                 g_gc_highest_address = max(g_gc_highest_address, block->memory_base + block->block_size);
  4281|             }
  4282|         }
  4283|         if (failure)
  4284|         {
  4285|             for (int block_index = 0; block_index < memory_details.numa_reserved_block_count; block_index++)
  4286|             {
  4287|                 numa_reserved_block * block = &memory_details.numa_reserved_block_table[block_index];
  4288|                 if (block->memory_base != nullptr)
  4289|                 {
  4290|                     virtual_free(block->memory_base, block->block_size);
  4291|                     block->memory_base = nullptr;
  4292|                 }
  4293|             }
  4294|             delete [] memory_details.numa_reserved_block_table;
  4295|             memory_details.numa_reserved_block_table = nullptr;
  4296|             memory_details.numa_reserved_block_count = 0;
  4297|         }
  4298|         else
  4299|         {
  4300|             for (uint16_t numa_node = 0; numa_node < numa_node_count; numa_node++)
  4301|             {
  4302|                 numa_reserved_block * block = &memory_details.numa_reserved_block_table[numa_node];
  4303|                 numa_reserved_block* pinned_block = separated_poh_p ?
  4304|                     &memory_details.numa_reserved_block_table[numa_node_count + numa_node] : nullptr;
  4305|                 if (block->block_size == 0)
  4306|                 {
  4307|                     assert((pinned_block == nullptr) || (pinned_block->block_size == 0));
  4308|                     continue;
  4309|                 }
  4310|                 uint8_t* memory_base = block->memory_base;
  4311|                 uint8_t* pinned_memory_base = ((pinned_block == nullptr) ? nullptr : pinned_block->memory_base);
  4312|                 for (int heap_no = 0; heap_no < num_heaps; heap_no++)
  4313|                 {
  4314|                     uint16_t heap_numa_node = heap_no_to_numa_node[heap_no];
  4315|                     if (heap_numa_node != numa_node)
  4316|                     {
  4317|                         continue;
  4318|                     }
  4319|                     memory_details.initial_normal_heap[heap_no].memory_base = memory_base;
  4320|                     memory_base += normal_size;
  4321|                     memory_details.initial_large_heap[heap_no].memory_base = memory_base;
  4322|                     memory_base += large_size;
  4323|                     if (separated_poh_p)
  4324|                     {
  4325|                         memory_details.initial_pinned_heap[heap_no].memory_base = pinned_memory_base;
  4326|                         pinned_memory_base += pinned_size;
  4327|                     }
  4328|                     else
  4329|                     {
  4330|                         memory_details.initial_pinned_heap[heap_no].memory_base = memory_base;
  4331|                         memory_base += pinned_size;
  4332|                     }
  4333|                 }
  4334|                 assert (memory_base == block->memory_base + block->block_size);
  4335|                 assert ((pinned_block == nullptr) || (pinned_memory_base == pinned_block->memory_base + pinned_block->block_size));
  4336|             }
  4337|             memory_details.allocation_pattern = initial_memory_details::EACH_NUMA_NODE;
  4338|             reserve_success = TRUE;
  4339|         }
  4340|     }
  4341|     if (!reserve_success)
  4342|     {
  4343|         size_t temp_pinned_size = (separated_poh_p ? 0 : pinned_size);
  4344|         size_t separate_pinned_size = memory_details.block_count * pinned_size;
  4345|         size_t requestedMemory = memory_details.block_count * (normal_size + large_size + temp_pinned_size);
  4346|         uint8_t* allatonce_block = (uint8_t*)virtual_alloc(requestedMemory, use_large_pages_p);
  4347|         uint8_t* separated_poh_block = nullptr;
  4348|         if (allatonce_block && separated_poh_p)
  4349|         {
  4350|             separated_poh_block = (uint8_t*)virtual_alloc(separate_pinned_size, false);
  4351|             if (!separated_poh_block)
  4352|             {
  4353|                 virtual_free(allatonce_block, requestedMemory);
  4354|                 allatonce_block = nullptr;
  4355|             }
  4356|         }
  4357|         if (allatonce_block)
  4358|         {
  4359|             if (separated_poh_p)
  4360|             {
  4361|                 g_gc_lowest_address = min(allatonce_block, separated_poh_block);
  4362|                 g_gc_highest_address = max((allatonce_block + requestedMemory),
  4363|                     (separated_poh_block + separate_pinned_size));
  4364|                 memory_details.allocation_pattern = initial_memory_details::ALLATONCE_SEPARATED_POH;
  4365|             }
  4366|             else
  4367|             {
  4368|                 g_gc_lowest_address = allatonce_block;
  4369|                 g_gc_highest_address = allatonce_block + requestedMemory;
  4370|                 memory_details.allocation_pattern = initial_memory_details::ALLATONCE;
  4371|             }
  4372|             for (int i = 0; i < memory_details.block_count; i++)
  4373|             {
  4374|                 memory_details.initial_normal_heap[i].memory_base = allatonce_block +
  4375|                     (i * normal_size);
  4376|                 memory_details.initial_large_heap[i].memory_base = allatonce_block +
  4377|                     (memory_details.block_count * normal_size) + (i * large_size);
  4378|                 if (separated_poh_p)
  4379|                 {
  4380|                     memory_details.initial_pinned_heap[i].memory_base = separated_poh_block +
  4381|                         (i * pinned_size);
  4382|                 }
  4383|                 else
  4384|                 {
  4385|                     memory_details.initial_pinned_heap[i].memory_base = allatonce_block +
  4386|                         (memory_details.block_count * (normal_size + large_size)) + (i * pinned_size);
  4387|                 }
  4388|             }
  4389|             reserve_success = TRUE;
  4390|         }
  4391|         else
  4392|         {
  4393|             uint8_t* b1 = (uint8_t*)virtual_alloc(memory_details.block_count * normal_size, use_large_pages_p);
  4394|             uint8_t* b2 = (uint8_t*)virtual_alloc(memory_details.block_count * large_size, use_large_pages_p);
  4395|             uint8_t* b3 = (uint8_t*)virtual_alloc(memory_details.block_count * pinned_size, use_large_pages_p && !separated_poh_p);
  4396|             if (b1 && b2 && b3)
  4397|             {
  4398|                 memory_details.allocation_pattern = initial_memory_details::EACH_GENERATION;
  4399|                 g_gc_lowest_address = min(b1, min(b2, b3));
  4400|                 g_gc_highest_address = max(b1 + memory_details.block_count * normal_size,
  4401|                     max(b2 + memory_details.block_count * large_size,
  4402|                         b3 + memory_details.block_count * pinned_size));
  4403|                 for (int i = 0; i < memory_details.block_count; i++)
  4404|                 {
  4405|                     memory_details.initial_normal_heap[i].memory_base = b1 + (i * normal_size);
  4406|                     memory_details.initial_large_heap[i].memory_base = b2 + (i * large_size);
  4407|                     memory_details.initial_pinned_heap[i].memory_base = b3 + (i * pinned_size);
  4408|                 }
  4409|                 reserve_success = TRUE;
  4410|             }
  4411|             else
  4412|             {
  4413|                 if (b1)
  4414|                     virtual_free(b1, memory_details.block_count * normal_size);
  4415|                 if (b2)
  4416|                     virtual_free(b2, memory_details.block_count * large_size);
  4417|                 if (b3)
  4418|                     virtual_free(b3, memory_details.block_count * pinned_size);
  4419|             }
  4420|             if ((b2 == NULL) && (memory_details.block_count > 1))
  4421|             {
  4422|                 memory_details.allocation_pattern = initial_memory_details::EACH_BLOCK;
  4423|                 imemory_data* current_block = memory_details.initial_memory;
  4424|                 for (int i = 0; i < (memory_details.block_count * (total_generation_count - ephemeral_generation_count)); i++, current_block++)
  4425|                 {
  4426|                     size_t block_size = memory_details.block_size(i);
  4427|                     uint16_t numa_node = NUMA_NODE_UNDEFINED;
  4428|                     if (heap_no_to_numa_node != nullptr)
  4429|                     {
  4430|                         int heap_no = i % memory_details.block_count;
  4431|                         numa_node = heap_no_to_numa_node[heap_no];
  4432|                     }
  4433|                     current_block->memory_base =
  4434|                         (uint8_t*)virtual_alloc(block_size, use_large_pages_p, numa_node);
  4435|                     if (current_block->memory_base == 0)
  4436|                     {
  4437|                         current_block = memory_details.initial_memory;
  4438|                         for (int j = 0; j < i; j++, current_block++) {
  4439|                             if (current_block->memory_base != 0) {
  4440|                                 block_size = memory_details.block_size(i);
  4441|                                 virtual_free(current_block->memory_base, block_size);
  4442|                             }
  4443|                         }
  4444|                         reserve_success = FALSE;
  4445|                         break;
  4446|                     }
  4447|                     else
  4448|                     {
  4449|                         if (current_block->memory_base < g_gc_lowest_address)
  4450|                             g_gc_lowest_address = current_block->memory_base;
  4451|                         if (((uint8_t*)current_block->memory_base + block_size) > g_gc_highest_address)
  4452|                             g_gc_highest_address = (current_block->memory_base + block_size);
  4453|                     }
  4454|                     reserve_success = TRUE;
  4455|                 }
  4456|             }
  4457|         }
  4458|     }
  4459|     if (reserve_success && separated_poh_p)
  4460|     {
  4461|         for (int heap_no = 0; (reserve_success && (heap_no < num_heaps)); heap_no++)
  4462|         {
  4463|             if (!GCToOSInterface::VirtualCommit(memory_details.initial_pinned_heap[heap_no].memory_base, pinned_size))
  4464|             {
  4465|                 reserve_success = FALSE;
  4466|             }
  4467|         }
  4468|     }
  4469|     return reserve_success;
  4470| }
  4471| void gc_heap::destroy_initial_memory()
  4472| {
  4473|     if (memory_details.initial_memory != NULL)
  4474|     {
  4475|         switch (memory_details.allocation_pattern)
  4476|         {
  4477|             case initial_memory_details::ALLATONCE:
  4478|                 virtual_free (memory_details.initial_memory[0].memory_base,
  4479|                     memory_details.block_count*(memory_details.block_size_normal +
  4480|                     memory_details.block_size_large + memory_details.block_size_pinned));
  4481|                 break;
  4482|             case initial_memory_details::ALLATONCE_SEPARATED_POH:
  4483|                 virtual_free(memory_details.initial_memory[0].memory_base,
  4484|                     memory_details.block_count * (memory_details.block_size_normal +
  4485|                         memory_details.block_size_large));
  4486|                 virtual_free(memory_details.initial_pinned_heap[0].memory_base,
  4487|                     memory_details.block_count * (memory_details.block_size_pinned));
  4488|                 break;
  4489|             case initial_memory_details::EACH_GENERATION:
  4490|                 virtual_free (memory_details.initial_normal_heap[0].memory_base,
  4491|                     memory_details.block_count*memory_details.block_size_normal);
  4492|                 virtual_free (memory_details.initial_large_heap[0].memory_base,
  4493|                     memory_details.block_count*memory_details.block_size_large);
  4494|                 virtual_free (memory_details.initial_pinned_heap[0].memory_base,
  4495|                     memory_details.block_count*memory_details.block_size_pinned);
  4496|                 break;
  4497|             case initial_memory_details::EACH_BLOCK:
  4498|             {
  4499|                 imemory_data* current_block = memory_details.initial_memory;
  4500|                 int total_block_count = memory_details.block_count *
  4501|                     (total_generation_count - ephemeral_generation_count);
  4502|                 for (int i = 0; i < total_block_count; i++, current_block++)
  4503|                 {
  4504|                     size_t block_size = memory_details.block_size (i);
  4505|                     if (current_block->memory_base != NULL)
  4506|                     {
  4507|                         virtual_free (current_block->memory_base, block_size);
  4508|                     }
  4509|                 }
  4510|                 break;
  4511|             }
  4512|             case initial_memory_details::EACH_NUMA_NODE:
  4513|                 for (int block_index = 0; block_index < memory_details.numa_reserved_block_count; block_index++)
  4514|                 {
  4515|                     numa_reserved_block * block = &memory_details.numa_reserved_block_table[block_index];
  4516|                     if (block->memory_base != nullptr)
  4517|                     {
  4518|                         virtual_free (block->memory_base, block->block_size);
  4519|                     }
  4520|                 }
  4521|                 delete [] memory_details.numa_reserved_block_table;
  4522|                 break;
  4523|             default:
  4524|                 assert (!"unexpected allocation_pattern");
  4525|                 break;
  4526|         }
  4527|         delete [] memory_details.initial_memory;
  4528|         memory_details.initial_memory = NULL;
  4529|         memory_details.initial_normal_heap = NULL;
  4530|         memory_details.initial_large_heap = NULL;
  4531|         memory_details.initial_pinned_heap = NULL;
  4532|     }
  4533| }
  4534| heap_segment* make_initial_segment (int gen, int h_number, gc_heap* hp)
  4535| {
  4536|     void* mem = memory_details.get_initial_memory (gen, h_number);
  4537|     size_t size = memory_details.get_initial_size (gen);
  4538|     heap_segment* res = gc_heap::make_heap_segment ((uint8_t*)mem, size, hp, gen);
  4539|     return res;
  4540| }
  4541| void* virtual_alloc (size_t size)
  4542| {
  4543|     return virtual_alloc(size, false);
  4544| }
  4545| void* virtual_alloc (size_t size, bool use_large_pages_p, uint16_t numa_node)
  4546| {
  4547|     size_t requested_size = size;
  4548|     if ((gc_heap::reserved_memory_limit - gc_heap::reserved_memory) < requested_size)
  4549|     {
  4550|         gc_heap::reserved_memory_limit =
  4551|             GCScan::AskForMoreReservedMemory (gc_heap::reserved_memory_limit, requested_size);
  4552|         if ((gc_heap::reserved_memory_limit - gc_heap::reserved_memory) < requested_size)
  4553|         {
  4554|             return 0;
  4555|         }
  4556|     }
  4557|     uint32_t flags = VirtualReserveFlags::None;
  4558| #ifndef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  4559|     if (virtual_alloc_hardware_write_watch)
  4560|     {
  4561|         flags = VirtualReserveFlags::WriteWatch;
  4562|     }
  4563| #endif // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  4564|     void* prgmem = use_large_pages_p ?
  4565|         GCToOSInterface::VirtualReserveAndCommitLargePages(requested_size, numa_node) :
  4566|         GCToOSInterface::VirtualReserve(requested_size, card_size * card_word_width, flags, numa_node);
  4567|     void *aligned_mem = prgmem;
  4568|     if (prgmem)
  4569|     {
  4570|         uint8_t* end_mem = (uint8_t*)prgmem + requested_size;
  4571|         if ((end_mem == 0) || ((size_t)(MAX_PTR - end_mem) <= END_SPACE_AFTER_GC))
  4572|         {
  4573|             GCToOSInterface::VirtualRelease (prgmem, requested_size);
  4574|             dprintf (2, ("Virtual Alloc size %zd returned memory right against 4GB [%zx, %zx[ - discarding",
  4575|                         requested_size, (size_t)prgmem, (size_t)((uint8_t*)prgmem+requested_size)));
  4576|             prgmem = 0;
  4577|             aligned_mem = 0;
  4578|         }
  4579|     }
  4580|     if (prgmem)
  4581|     {
  4582|         gc_heap::reserved_memory += requested_size;
  4583|     }
  4584|     dprintf (2, ("Virtual Alloc size %zd: [%zx, %zx[",
  4585|                  requested_size, (size_t)prgmem, (size_t)((uint8_t*)prgmem+requested_size)));
  4586|     return aligned_mem;
  4587| }
  4588| static size_t get_valid_segment_size (BOOL large_seg=FALSE)
  4589| {
  4590|     size_t seg_size, initial_seg_size;
  4591|     if (!large_seg)
  4592|     {
  4593|         initial_seg_size = INITIAL_ALLOC;
  4594|         seg_size = static_cast<size_t>(GCConfig::GetSegmentSize());
  4595|     }
  4596|     else
  4597|     {
  4598|         initial_seg_size = LHEAP_ALLOC;
  4599|         seg_size = static_cast<size_t>(GCConfig::GetSegmentSize()) / 2;
  4600|     }
  4601| #ifdef MULTIPLE_HEAPS
  4602| #ifdef HOST_64BIT
  4603|     if (!large_seg)
  4604| #endif // HOST_64BIT
  4605|     {
  4606|         if (g_num_processors > 4)
  4607|             initial_seg_size /= 2;
  4608|         if (g_num_processors > 8)
  4609|             initial_seg_size /= 2;
  4610|     }
  4611| #endif //MULTIPLE_HEAPS
  4612|     if (!g_theGCHeap->IsValidSegmentSize(seg_size))
  4613|     {
  4614|         if ((seg_size >> 1) && !(seg_size >> 22))
  4615|             seg_size = 1024*1024*4;
  4616|         else
  4617|             seg_size = initial_seg_size;
  4618|     }
  4619| #ifdef HOST_64BIT
  4620|     seg_size = round_up_power2 (seg_size);
  4621| #else
  4622|     seg_size = round_down_power2 (seg_size);
  4623| #endif // HOST_64BIT
  4624|     return (seg_size);
  4625| }
  4626| #ifndef USE_REGIONS
  4627| void
  4628| gc_heap::compute_new_ephemeral_size()
  4629| {
  4630|     int eph_gen_max = max_generation - 1 - (settings.promotion ? 1 : 0);
  4631|     size_t padding_size = 0;
  4632|     for (int i = 0; i <= eph_gen_max; i++)
  4633|     {
  4634|         dynamic_data* dd = dynamic_data_of (i);
  4635|         total_ephemeral_size += (dd_survived_size (dd) - dd_pinned_survived_size (dd));
  4636| #ifdef RESPECT_LARGE_ALIGNMENT
  4637|         total_ephemeral_size += dd_num_npinned_plugs (dd) * switch_alignment_size (FALSE);
  4638| #endif //RESPECT_LARGE_ALIGNMENT
  4639| #ifdef FEATURE_STRUCTALIGN
  4640|         total_ephemeral_size += dd_num_npinned_plugs (dd) * MAX_STRUCTALIGN;
  4641| #endif //FEATURE_STRUCTALIGN
  4642| #ifdef SHORT_PLUGS
  4643|         padding_size += dd_padding_size (dd);
  4644| #endif //SHORT_PLUGS
  4645|     }
  4646|     total_ephemeral_size += eph_gen_starts_size;
  4647| #ifdef RESPECT_LARGE_ALIGNMENT
  4648|     size_t planned_ephemeral_size = heap_segment_plan_allocated (ephemeral_heap_segment) -
  4649|                                        generation_plan_allocation_start (generation_of (max_generation-1));
  4650|     total_ephemeral_size = min (total_ephemeral_size, planned_ephemeral_size);
  4651| #endif //RESPECT_LARGE_ALIGNMENT
  4652| #ifdef SHORT_PLUGS
  4653|     total_ephemeral_size = Align ((size_t)((double)total_ephemeral_size * short_plugs_pad_ratio) + 1);
  4654|     total_ephemeral_size += Align (DESIRED_PLUG_LENGTH);
  4655| #endif //SHORT_PLUGS
  4656|     dprintf (3, ("total ephemeral size is %zx, padding %zx(%zx)",
  4657|         total_ephemeral_size,
  4658|         padding_size, (total_ephemeral_size - padding_size)));
  4659| }
  4660| heap_segment*
  4661| gc_heap::soh_get_segment_to_expand()
  4662| {
  4663|     size_t size = soh_segment_size;
  4664|     ordered_plug_indices_init = FALSE;
  4665|     use_bestfit = FALSE;
  4666|     compute_new_ephemeral_size();
  4667|     if ((settings.pause_mode != pause_low_latency) &&
  4668|         (settings.pause_mode != pause_no_gc)
  4669| #ifdef BACKGROUND_GC
  4670|         && (!gc_heap::background_running_p())
  4671| #endif //BACKGROUND_GC
  4672|         )
  4673|     {
  4674|         assert (settings.condemned_generation <= max_generation);
  4675|         allocator*  gen_alloc = ((settings.condemned_generation == max_generation) ? nullptr :
  4676|                               generation_allocator (generation_of (max_generation)));
  4677|         dprintf (2, ("(gen%d)soh_get_segment_to_expand", settings.condemned_generation));
  4678|         heap_segment* fseg = heap_segment_rw (generation_start_segment (generation_of (max_generation)));
  4679|         PREFIX_ASSUME(fseg != NULL);
  4680| #ifdef SEG_REUSE_STATS
  4681|         int try_reuse = 0;
  4682| #endif //SEG_REUSE_STATS
  4683|         heap_segment* seg = ephemeral_heap_segment;
  4684|         while ((seg = heap_segment_prev_rw (fseg, seg)) && (seg != fseg))
  4685|         {
  4686| #ifdef SEG_REUSE_STATS
  4687|         try_reuse++;
  4688| #endif //SEG_REUSE_STATS
  4689|             if (can_expand_into_p (seg, size/3, total_ephemeral_size, gen_alloc))
  4690|             {
  4691|                 get_gc_data_per_heap()->set_mechanism (gc_heap_expand,
  4692|                     (use_bestfit ? expand_reuse_bestfit : expand_reuse_normal));
  4693|                 if (settings.condemned_generation == max_generation)
  4694|                 {
  4695|                     if (use_bestfit)
  4696|                     {
  4697|                         build_ordered_free_spaces (seg);
  4698|                         dprintf (GTC_LOG, ("can use best fit"));
  4699|                     }
  4700| #ifdef SEG_REUSE_STATS
  4701|                     dprintf (SEG_REUSE_LOG_0, ("(gen%d)soh_get_segment_to_expand: found seg #%d to reuse",
  4702|                         settings.condemned_generation, try_reuse));
  4703| #endif //SEG_REUSE_STATS
  4704|                     dprintf (GTC_LOG, ("max_gen: Found existing segment to expand into %zx", (size_t)seg));
  4705|                     return seg;
  4706|                 }
  4707|                 else
  4708|                 {
  4709| #ifdef SEG_REUSE_STATS
  4710|                     dprintf (SEG_REUSE_LOG_0, ("(gen%d)soh_get_segment_to_expand: found seg #%d to reuse - returning",
  4711|                         settings.condemned_generation, try_reuse));
  4712| #endif //SEG_REUSE_STATS
  4713|                     dprintf (GTC_LOG, ("max_gen-1: Found existing segment to expand into %zx", (size_t)seg));
  4714|                     if (settings.pause_mode != pause_sustained_low_latency)
  4715|                     {
  4716|                         dprintf (GTC_LOG, ("max_gen-1: SustainedLowLatency is set, acquire a new seg"));
  4717|                         get_gc_data_per_heap()->set_mechanism (gc_heap_expand, expand_next_full_gc);
  4718|                         return 0;
  4719|                     }
  4720|                 }
  4721|             }
  4722|         }
  4723|     }
  4724|     heap_segment* result = get_segment (size, gc_oh_num::soh);
  4725|     if(result)
  4726|     {
  4727| #ifdef BACKGROUND_GC
  4728|         if (current_c_gc_state == c_gc_state_planning)
  4729|         {
  4730|             result->flags |= heap_segment_flags_swept;
  4731|         }
  4732| #endif //BACKGROUND_GC
  4733|         FIRE_EVENT(GCCreateSegment_V1, heap_segment_mem(result),
  4734|                                   (size_t)(heap_segment_reserved (result) - heap_segment_mem(result)),
  4735|                                   gc_etw_segment_small_object_heap);
  4736|     }
  4737|     get_gc_data_per_heap()->set_mechanism (gc_heap_expand, (result ? expand_new_seg : expand_no_memory));
  4738|     if (result == 0)
  4739|     {
  4740|         dprintf (2, ("h%d: failed to allocate a new segment!", heap_number));
  4741|     }
  4742|     else
  4743|     {
  4744| #ifdef MULTIPLE_HEAPS
  4745|         heap_segment_heap (result) = this;
  4746| #endif //MULTIPLE_HEAPS
  4747|     }
  4748|     dprintf (GTC_LOG, ("(gen%d)creating new segment %p", settings.condemned_generation, result));
  4749|     return result;
  4750| }
  4751| heap_segment*
  4752| gc_heap::get_segment (size_t size, gc_oh_num oh)
  4753| {
  4754|     assert(oh != gc_oh_num::unknown);
  4755|     BOOL uoh_p = (oh == gc_oh_num::loh) || (oh == gc_oh_num::poh);
  4756|     if (heap_hard_limit)
  4757|         return NULL;
  4758|     heap_segment* result = 0;
  4759|     if (segment_standby_list != 0)
  4760|     {
  4761|         result = segment_standby_list;
  4762|         heap_segment* last = 0;
  4763|         while (result)
  4764|         {
  4765|             size_t hs = (size_t)(heap_segment_reserved (result) - (uint8_t*)result);
  4766|             if ((hs >= size) && ((hs / 2) < size))
  4767|             {
  4768|                 dprintf (2, ("Hoarded segment %zx found", (size_t) result));
  4769|                 if (last)
  4770|                 {
  4771|                     heap_segment_next (last) = heap_segment_next (result);
  4772|                 }
  4773|                 else
  4774|                 {
  4775|                     segment_standby_list = heap_segment_next (result);
  4776|                 }
  4777|                 break;
  4778|             }
  4779|             else
  4780|             {
  4781|                 last = result;
  4782|                 result = heap_segment_next (result);
  4783|             }
  4784|         }
  4785|     }
  4786|     if (result)
  4787|     {
  4788|         init_heap_segment (result, __this);
  4789| #ifdef BACKGROUND_GC
  4790|         if (is_bgc_in_progress())
  4791|         {
  4792|             dprintf (GC_TABLE_LOG, ("hoarded seg %p, mark_array is %p", result, mark_array));
  4793|             if (!commit_mark_array_new_seg (__this, result))
  4794|             {
  4795|                 dprintf (GC_TABLE_LOG, ("failed to commit mark array for hoarded seg"));
  4796|                 if (segment_standby_list != 0)
  4797|                 {
  4798|                     heap_segment_next (result) = segment_standby_list;
  4799|                     segment_standby_list = result;
  4800|                 }
  4801|                 else
  4802|                 {
  4803|                     segment_standby_list = result;
  4804|                 }
  4805|                 result = 0;
  4806|             }
  4807|         }
  4808| #endif //BACKGROUND_GC
  4809|         if (result)
  4810|             seg_mapping_table_add_segment (result, __this);
  4811|     }
  4812|     if (!result)
  4813|     {
  4814|         void* mem = virtual_alloc (size);
  4815|         if (!mem)
  4816|         {
  4817|             fgm_result.set_fgm (fgm_reserve_segment, size, uoh_p);
  4818|             return 0;
  4819|         }
  4820|         result = make_heap_segment ((uint8_t*)mem, size, __this, (uoh_p ? max_generation : 0));
  4821|         if (result)
  4822|         {
  4823|             uint8_t* start;
  4824|             uint8_t* end;
  4825|             if (mem < g_gc_lowest_address)
  4826|             {
  4827|                 start =  (uint8_t*)mem;
  4828|             }
  4829|             else
  4830|             {
  4831|                 start = (uint8_t*)g_gc_lowest_address;
  4832|             }
  4833|             if (((uint8_t*)mem + size) > g_gc_highest_address)
  4834|             {
  4835|                 end = (uint8_t*)mem + size;
  4836|             }
  4837|             else
  4838|             {
  4839|                 end = (uint8_t*)g_gc_highest_address;
  4840|             }
  4841|             if (gc_heap::grow_brick_card_tables (start, end, size, result, __this, uoh_p) != 0)
  4842|             {
  4843|                 virtual_free (mem, size);
  4844|                 return 0;
  4845|             }
  4846|         }
  4847|         else
  4848|         {
  4849|             fgm_result.set_fgm (fgm_commit_segment_beg, SEGMENT_INITIAL_COMMIT, uoh_p);
  4850|             virtual_free (mem, size);
  4851|         }
  4852|         if (result)
  4853|         {
  4854|             seg_mapping_table_add_segment (result, __this);
  4855|         }
  4856|     }
  4857| #ifdef BACKGROUND_GC
  4858|     if (result)
  4859|     {
  4860|         ::record_changed_seg ((uint8_t*)result, heap_segment_reserved (result),
  4861|                             settings.gc_index, current_bgc_state,
  4862|                             seg_added);
  4863|         bgc_verify_mark_array_cleared (result);
  4864|     }
  4865| #endif //BACKGROUND_GC
  4866|     dprintf (GC_TABLE_LOG, ("h%d: new seg: %p-%p (%zd)", heap_number, result, ((uint8_t*)result + size), size));
  4867|     return result;
  4868| }
  4869| void gc_heap::release_segment (heap_segment* sg)
  4870| {
  4871|     ptrdiff_t delta = 0;
  4872|     FIRE_EVENT(GCFreeSegment_V1, heap_segment_mem(sg));
  4873|     virtual_free (sg, (uint8_t*)heap_segment_reserved (sg)-(uint8_t*)sg, sg);
  4874| }
  4875| #endif //!USE_REGIONS
  4876| heap_segment* gc_heap::get_segment_for_uoh (int gen_number, size_t size
  4877| #ifdef MULTIPLE_HEAPS
  4878|                                            , gc_heap* hp
  4879| #endif //MULTIPLE_HEAPS
  4880|                                            )
  4881| {
  4882| #ifndef MULTIPLE_HEAPS
  4883|     gc_heap* hp = 0;
  4884| #endif //MULTIPLE_HEAPS
  4885| #ifdef USE_REGIONS
  4886|     heap_segment* res = hp->get_new_region (gen_number, size);
  4887| #else //USE_REGIONS
  4888|     gc_oh_num oh = gen_to_oh (gen_number);
  4889|     heap_segment* res = hp->get_segment (size, oh);
  4890| #endif //USE_REGIONS
  4891|     if (res != 0)
  4892|     {
  4893| #ifdef MULTIPLE_HEAPS
  4894|         heap_segment_heap (res) = hp;
  4895| #endif //MULTIPLE_HEAPS
  4896|         size_t flags = (gen_number == poh_generation) ?
  4897|             heap_segment_flags_poh :
  4898|             heap_segment_flags_loh;
  4899| #ifdef USE_REGIONS
  4900|         assert ((res->flags & (heap_segment_flags_loh | heap_segment_flags_poh)) == flags);
  4901| #else //USE_REGIONS
  4902|         res->flags |= flags;
  4903|         FIRE_EVENT(GCCreateSegment_V1,
  4904|             heap_segment_mem(res),
  4905|             (size_t)(heap_segment_reserved (res) - heap_segment_mem(res)),
  4906|             (gen_number == poh_generation) ?
  4907|                 gc_etw_segment_pinned_object_heap :
  4908|                 gc_etw_segment_large_object_heap);
  4909| #ifdef MULTIPLE_HEAPS
  4910|         hp->thread_uoh_segment (gen_number, res);
  4911| #else
  4912|         thread_uoh_segment (gen_number, res);
  4913| #endif //MULTIPLE_HEAPS
  4914| #endif //USE_REGIONS
  4915|         GCToEEInterface::DiagAddNewRegion(
  4916|                             gen_number,
  4917|                             heap_segment_mem (res),
  4918|                             heap_segment_allocated (res),
  4919|                             heap_segment_reserved (res)
  4920|                         );
  4921|     }
  4922|     return res;
  4923| }
  4924| void gc_heap::thread_uoh_segment (int gen_number, heap_segment* new_seg)
  4925| {
  4926|     heap_segment* seg = generation_allocation_segment (generation_of (gen_number));
  4927|     while (heap_segment_next_rw (seg))
  4928|         seg = heap_segment_next_rw (seg);
  4929|     heap_segment_next (seg) = new_seg;
  4930| }
  4931| heap_segment*
  4932| gc_heap::get_uoh_segment (int gen_number, size_t size, BOOL* did_full_compact_gc, enter_msl_status* msl_status)
  4933| {
  4934|     *did_full_compact_gc = FALSE;
  4935|     size_t last_full_compact_gc_count = get_full_compact_gc_count();
  4936|     add_saved_spinlock_info (true, me_release, mt_get_large_seg, msl_entered);
  4937|     leave_spin_lock (&more_space_lock_uoh);
  4938|     enter_spin_lock (&gc_heap::gc_lock);
  4939|     dprintf (SPINLOCK_LOG, ("[%d]Seg: Egc", heap_number));
  4940|     size_t current_full_compact_gc_count = get_full_compact_gc_count();
  4941|     if (current_full_compact_gc_count > last_full_compact_gc_count)
  4942|     {
  4943|         *did_full_compact_gc = TRUE;
  4944|     }
  4945|     if (should_move_heap (&more_space_lock_uoh))
  4946|     {
  4947|         *msl_status = msl_retry_different_heap;
  4948|         leave_spin_lock (&gc_heap::gc_lock);
  4949|         return NULL;
  4950|     }
  4951|     heap_segment* res = get_segment_for_uoh (gen_number, size
  4952| #ifdef MULTIPLE_HEAPS
  4953|                                             , this
  4954| #endif //MULTIPLE_HEAPS
  4955|                                             );
  4956|     dprintf (SPINLOCK_LOG, ("[%d]Seg: A Lgc", heap_number));
  4957|     leave_spin_lock (&gc_heap::gc_lock);
  4958|     *msl_status = enter_spin_lock_msl (&more_space_lock_uoh);
  4959|     if (*msl_status == msl_retry_different_heap)
  4960|         return NULL;
  4961|     add_saved_spinlock_info (true, me_acquire, mt_get_large_seg, *msl_status);
  4962|     return res;
  4963| }
  4964| #ifdef MULTIPLE_HEAPS
  4965| #ifdef HOST_X86
  4966| #ifdef _MSC_VER
  4967| #pragma warning(disable:4035)
  4968|     static ptrdiff_t  get_cycle_count()
  4969|     {
  4970|         __asm   rdtsc
  4971|     }
  4972| #pragma warning(default:4035)
  4973| #elif defined(__GNUC__)
  4974|     static ptrdiff_t  get_cycle_count()
  4975|     {
  4976|         ptrdiff_t cycles;
  4977|         ptrdiff_t cyclesHi;
  4978|         __asm__ __volatile__
  4979|         ("rdtsc":"=a" (cycles), "=d" (cyclesHi));
  4980|         return cycles;
  4981|     }
  4982| #else //_MSC_VER
  4983| #error Unknown compiler
  4984| #endif //_MSC_VER
  4985| #elif defined(TARGET_AMD64)
  4986| #ifdef _MSC_VER
  4987| extern "C" uint64_t __rdtsc();
  4988| #pragma intrinsic(__rdtsc)
  4989|     static ptrdiff_t get_cycle_count()
  4990|     {
  4991|         return (ptrdiff_t)__rdtsc();
  4992|     }
  4993| #elif defined(__GNUC__)
  4994|     static ptrdiff_t get_cycle_count()
  4995|     {
  4996|         ptrdiff_t cycles;
  4997|         ptrdiff_t cyclesHi;
  4998|         __asm__ __volatile__
  4999|         ("rdtsc":"=a" (cycles), "=d" (cyclesHi));
  5000|         return (cyclesHi << 32) | cycles;
  5001|     }
  5002| #else // _MSC_VER
  5003|     extern "C" ptrdiff_t get_cycle_count(void);
  5004| #endif // _MSC_VER
  5005| #elif defined(TARGET_LOONGARCH64)
  5006|     static ptrdiff_t get_cycle_count()
  5007|     {
  5008|         __asm__ volatile ("break 0 \n");
  5009|         return 0;
  5010|     }
  5011| #else
  5012|     static ptrdiff_t get_cycle_count()
  5013|     {
  5014|         return 0;
  5015|     }
  5016| #endif //TARGET_X86
  5017| struct node_heap_count
  5018| {
  5019|     int node_no;
  5020|     int heap_count;
  5021| };
  5022| class heap_select
  5023| {
  5024|     heap_select() {}
  5025| public:
  5026|     static uint8_t* sniff_buffer;
  5027|     static unsigned n_sniff_buffers;
  5028|     static unsigned cur_sniff_index;
  5029|     static uint16_t proc_no_to_heap_no[MAX_SUPPORTED_CPUS];
  5030|     static uint16_t heap_no_to_proc_no[MAX_SUPPORTED_CPUS];
  5031|     static uint16_t heap_no_to_numa_node[MAX_SUPPORTED_CPUS];
  5032|     static uint16_t proc_no_to_numa_node[MAX_SUPPORTED_CPUS];
  5033|     static uint16_t numa_node_to_heap_map[MAX_SUPPORTED_CPUS+4];
  5034|     static uint16_t total_numa_nodes;
  5035|     static node_heap_count heaps_on_node[MAX_SUPPORTED_NODES];
  5036|     static int access_time(uint8_t *sniff_buffer, int heap_number, unsigned sniff_index, unsigned n_sniff_buffers)
  5037|     {
  5038|         ptrdiff_t start_cycles = get_cycle_count();
  5039|         uint8_t sniff = sniff_buffer[(1 + heap_number*n_sniff_buffers + sniff_index)*HS_CACHE_LINE_SIZE];
  5040|         assert (sniff == 0);
  5041|         ptrdiff_t elapsed_cycles = get_cycle_count() - start_cycles;
  5042|         elapsed_cycles += sniff;
  5043|         return (int) elapsed_cycles;
  5044|     }
  5045| public:
  5046|     static BOOL init(int n_heaps)
  5047|     {
  5048|         assert (sniff_buffer == NULL && n_sniff_buffers == 0);
  5049|         if (!GCToOSInterface::CanGetCurrentProcessorNumber())
  5050|         {
  5051|             n_sniff_buffers = n_heaps*2+1;
  5052|             size_t n_cache_lines = 1 + n_heaps * n_sniff_buffers + 1;
  5053|             size_t sniff_buf_size = n_cache_lines * HS_CACHE_LINE_SIZE;
  5054|             if (sniff_buf_size / HS_CACHE_LINE_SIZE != n_cache_lines) // check for overlow
  5055|             {
  5056|                 return FALSE;
  5057|             }
  5058|             sniff_buffer = new (nothrow) uint8_t[sniff_buf_size];
  5059|             if (sniff_buffer == 0)
  5060|                 return FALSE;
  5061|             memset(sniff_buffer, 0, sniff_buf_size*sizeof(uint8_t));
  5062|         }
  5063|         bool do_numa = GCToOSInterface::CanEnableGCNumaAware();
  5064|         uint16_t proc_no[MAX_SUPPORTED_CPUS];
  5065|         uint16_t node_no[MAX_SUPPORTED_CPUS];
  5066|         uint16_t max_node_no = 0;
  5067|         uint16_t heap_num;
  5068|         for (heap_num = 0; heap_num < n_heaps; heap_num++)
  5069|         {
  5070|             if (!GCToOSInterface::GetProcessorForHeap (heap_num, &proc_no[heap_num], &node_no[heap_num]))
  5071|                 break;
  5072|             assert(proc_no[heap_num] < MAX_SUPPORTED_CPUS);
  5073|             if (!do_numa || node_no[heap_num] == NUMA_NODE_UNDEFINED)
  5074|                 node_no[heap_num] = 0;
  5075|             max_node_no = max(max_node_no, node_no[heap_num]);
  5076|         }
  5077|         int cur_heap_no = 0;
  5078|         for (uint16_t cur_node_no = 0; cur_node_no <= max_node_no; cur_node_no++)
  5079|         {
  5080|             for (int i = 0; i < heap_num; i++)
  5081|             {
  5082|                 if (node_no[i] != cur_node_no)
  5083|                     continue;
  5084|                 heap_no_to_proc_no[cur_heap_no] = proc_no[i];
  5085|                 heap_no_to_numa_node[cur_heap_no] = cur_node_no;
  5086|                 proc_no_to_numa_node[proc_no[i]] = cur_node_no;
  5087|                 cur_heap_no++;
  5088|             }
  5089|         }
  5090|         return TRUE;
  5091|     }
  5092|     static void init_cpu_mapping(int heap_number)
  5093|     {
  5094|         if (GCToOSInterface::CanGetCurrentProcessorNumber())
  5095|         {
  5096|             uint32_t proc_no = GCToOSInterface::GetCurrentProcessorNumber();
  5097|             proc_no_to_heap_no[proc_no % MAX_SUPPORTED_CPUS] = (uint16_t)heap_number;
  5098|         }
  5099|     }
  5100|     static void mark_heap(int heap_number)
  5101|     {
  5102|         if (GCToOSInterface::CanGetCurrentProcessorNumber())
  5103|             return;
  5104|         for (unsigned sniff_index = 0; sniff_index < n_sniff_buffers; sniff_index++)
  5105|             sniff_buffer[(1 + heap_number*n_sniff_buffers + sniff_index)*HS_CACHE_LINE_SIZE] &= 1;
  5106|     }
  5107|     static int select_heap(alloc_context* acontext)
  5108|     {
  5109| #ifndef TRACE_GC
  5110|         UNREFERENCED_PARAMETER(acontext); // only referenced by dprintf
  5111| #endif //TRACE_GC
  5112|         if (GCToOSInterface::CanGetCurrentProcessorNumber())
  5113|         {
  5114|             uint32_t proc_no = GCToOSInterface::GetCurrentProcessorNumber();
  5115|             int adjusted_heap = proc_no_to_heap_no[proc_no % MAX_SUPPORTED_CPUS];
  5116|             if (adjusted_heap >= gc_heap::n_heaps)
  5117|             {
  5118|                 adjusted_heap %= gc_heap::n_heaps;
  5119|             }
  5120|             return adjusted_heap;
  5121|         }
  5122|         unsigned sniff_index = Interlocked::Increment(&cur_sniff_index);
  5123|         sniff_index %= n_sniff_buffers;
  5124|         int best_heap = 0;
  5125|         int best_access_time = 1000*1000*1000;
  5126|         int second_best_access_time = best_access_time;
  5127|         uint8_t *l_sniff_buffer = sniff_buffer;
  5128|         unsigned l_n_sniff_buffers = n_sniff_buffers;
  5129|         for (int heap_number = 0; heap_number < gc_heap::n_heaps; heap_number++)
  5130|         {
  5131|             int this_access_time = access_time(l_sniff_buffer, heap_number, sniff_index, l_n_sniff_buffers);
  5132|             if (this_access_time < best_access_time)
  5133|             {
  5134|                 second_best_access_time = best_access_time;
  5135|                 best_access_time = this_access_time;
  5136|                 best_heap = heap_number;
  5137|             }
  5138|             else if (this_access_time < second_best_access_time)
  5139|             {
  5140|                 second_best_access_time = this_access_time;
  5141|             }
  5142|         }
  5143|         if (best_access_time*2 < second_best_access_time)
  5144|         {
  5145|             sniff_buffer[(1 + best_heap*n_sniff_buffers + sniff_index)*HS_CACHE_LINE_SIZE] &= 1;
  5146|             dprintf (3, ("select_heap yields crisp %d for context %p\n", best_heap, (void *)acontext));
  5147|         }
  5148|         else
  5149|         {
  5150|             dprintf (3, ("select_heap yields vague %d for context %p\n", best_heap, (void *)acontext ));
  5151|         }
  5152|         return best_heap;
  5153|     }
  5154|     static bool can_find_heap_fast()
  5155|     {
  5156|         return GCToOSInterface::CanGetCurrentProcessorNumber();
  5157|     }
  5158|     static uint16_t find_heap_no_from_proc_no(uint16_t proc_no)
  5159|     {
  5160|         return proc_no_to_heap_no[proc_no];
  5161|     }
  5162|     static uint16_t find_proc_no_from_heap_no(int heap_number)
  5163|     {
  5164|         return heap_no_to_proc_no[heap_number];
  5165|     }
  5166|     static void set_proc_no_for_heap(int heap_number, uint16_t proc_no)
  5167|     {
  5168|         heap_no_to_proc_no[heap_number] = proc_no;
  5169|     }
  5170|     static uint16_t find_numa_node_from_heap_no(int heap_number)
  5171|     {
  5172|         return heap_no_to_numa_node[heap_number];
  5173|     }
  5174|     static uint16_t find_numa_node_from_proc_no (uint16_t proc_no)
  5175|     {
  5176|         return proc_no_to_numa_node[proc_no];
  5177|     }
  5178|     static void set_numa_node_for_heap_and_proc(int heap_number, uint16_t proc_no, uint16_t numa_node)
  5179|     {
  5180|         heap_no_to_numa_node[heap_number] = numa_node;
  5181|         proc_no_to_numa_node[proc_no] = numa_node;
  5182|     }
  5183|     static void init_numa_node_to_heap_map(int nheaps)
  5184|     {
  5185|         numa_node_to_heap_map[heap_no_to_numa_node[0]] = 0;
  5186|         total_numa_nodes = 0;
  5187|         memset (heaps_on_node, 0, sizeof (heaps_on_node));
  5188|         heaps_on_node[0].node_no = heap_no_to_numa_node[0];
  5189|         heaps_on_node[0].heap_count = 1;
  5190|         for (int i=1; i < nheaps; i++)
  5191|         {
  5192|             if (heap_no_to_numa_node[i] != heap_no_to_numa_node[i-1])
  5193|             {
  5194|                 total_numa_nodes++;
  5195|                 heaps_on_node[total_numa_nodes].node_no = heap_no_to_numa_node[i];
  5196|                 numa_node_to_heap_map[heap_no_to_numa_node[i-1] + 1] =
  5197|                 numa_node_to_heap_map[heap_no_to_numa_node[i]] = (uint16_t)i;
  5198|             }
  5199|             (heaps_on_node[total_numa_nodes].heap_count)++;
  5200|         }
  5201|         numa_node_to_heap_map[heap_no_to_numa_node[nheaps-1] + 1] = (uint16_t)nheaps; //mark the end with nheaps
  5202|         total_numa_nodes++;
  5203|     }
  5204|     static void distribute_other_procs()
  5205|     {
  5206|         if (affinity_config_specified_p)
  5207|             return;
  5208|         uint16_t proc_no = 0;
  5209|         uint16_t node_no = 0;
  5210|         bool res = false;
  5211|         int start_heap = -1;
  5212|         int end_heap = -1;
  5213|         int current_node_no = -1;
  5214|         int current_heap_on_node = -1;
  5215|         for (int i = gc_heap::n_heaps; i < (int)g_num_active_processors; i++)
  5216|         {
  5217|             if (!GCToOSInterface::GetProcessorForHeap ((uint16_t)i, &proc_no, &node_no))
  5218|                 break;
  5219|             if (node_no == NUMA_NODE_UNDEFINED)
  5220|                 node_no = 0;
  5221|             int start_heap = (int)numa_node_to_heap_map[node_no];
  5222|             int end_heap = (int)(numa_node_to_heap_map[node_no + 1]);
  5223|             if ((end_heap - start_heap) > 0)
  5224|             {
  5225|                 if (node_no == current_node_no)
  5226|                 {
  5227|                     if (current_heap_on_node >= end_heap)
  5228|                     {
  5229|                         continue;
  5230|                     }
  5231|                 }
  5232|                 else
  5233|                 {
  5234|                     current_node_no = node_no;
  5235|                     current_heap_on_node = start_heap;
  5236|                 }
  5237|                 proc_no_to_heap_no[proc_no] = (uint16_t)current_heap_on_node;
  5238|                 proc_no_to_numa_node[proc_no] = (uint16_t)node_no;
  5239|                 current_heap_on_node++;
  5240|             }
  5241|         }
  5242|     }
  5243|     static void get_heap_range_for_heap(int hn, int* start, int* end)
  5244|     {
  5245|         uint16_t numa_node = heap_no_to_numa_node[hn];
  5246|         *start = (int)numa_node_to_heap_map[numa_node];
  5247|         *end   = (int)(numa_node_to_heap_map[numa_node+1]);
  5248| #ifdef HEAP_BALANCE_INSTRUMENTATION
  5249|         dprintf(HEAP_BALANCE_TEMP_LOG, ("TEMPget_heap_range: %d is in numa node %d, start = %d, end = %d", hn, numa_node, *start, *end));
  5250| #endif //HEAP_BALANCE_INSTRUMENTATION
  5251|     }
  5252|     static uint16_t get_next_numa_node (uint16_t current_index, int* start, int* end)
  5253|     {
  5254|         int start_index = current_index + 1;
  5255|         int nheaps = gc_heap::n_heaps;
  5256|         bool found_node_with_heaps_p = false;
  5257|         do
  5258|         {
  5259|             int start_heap = (int)numa_node_to_heap_map[start_index];
  5260|             int end_heap = (int)numa_node_to_heap_map[start_index + 1];
  5261|             if (start_heap == nheaps)
  5262|             {
  5263|                 start_index = 0;
  5264|                 continue;
  5265|             }
  5266|             if ((end_heap - start_heap) == 0)
  5267|             {
  5268|                 start_index++;
  5269|             }
  5270|             else
  5271|             {
  5272|                 found_node_with_heaps_p = true;
  5273|                 *start = start_heap;
  5274|                 *end = end_heap;
  5275|             }
  5276|         } while (!found_node_with_heaps_p);
  5277|         return (uint16_t)start_index;
  5278|     }
  5279| };
  5280| uint8_t* heap_select::sniff_buffer;
  5281| unsigned heap_select::n_sniff_buffers;
  5282| unsigned heap_select::cur_sniff_index;
  5283| uint16_t heap_select::proc_no_to_heap_no[MAX_SUPPORTED_CPUS];
  5284| uint16_t heap_select::heap_no_to_proc_no[MAX_SUPPORTED_CPUS];
  5285| uint16_t heap_select::heap_no_to_numa_node[MAX_SUPPORTED_CPUS];
  5286| uint16_t heap_select::proc_no_to_numa_node[MAX_SUPPORTED_CPUS];
  5287| uint16_t heap_select::numa_node_to_heap_map[MAX_SUPPORTED_CPUS+4];
  5288| uint16_t  heap_select::total_numa_nodes;
  5289| node_heap_count heap_select::heaps_on_node[MAX_SUPPORTED_NODES];
  5290| #ifdef HEAP_BALANCE_INSTRUMENTATION
  5291| struct heap_balance_info
  5292| {
  5293|     uint64_t timestamp;
  5294|     int tid;
  5295|     int alloc_heap;
  5296|     int ideal_proc_no;
  5297| };
  5298| #define default_max_hb_heap_balance_info 4096
  5299| struct heap_balance_info_proc
  5300| {
  5301|     int count;
  5302|     int index;
  5303|     heap_balance_info hb_info[default_max_hb_heap_balance_info];
  5304| };
  5305| struct heap_balance_info_numa
  5306| {
  5307|     heap_balance_info_proc* hb_info_procs;
  5308| };
  5309| uint64_t start_raw_ts = 0;
  5310| bool cpu_group_enabled_p = false;
  5311| uint32_t procs_per_numa_node = 0;
  5312| uint16_t total_numa_nodes_on_machine = 0;
  5313| uint32_t procs_per_cpu_group = 0;
  5314| uint16_t total_cpu_groups_on_machine = 0;
  5315| heap_balance_info_numa* hb_info_numa_nodes = NULL;
  5316| int get_proc_index_numa (int proc_no, int* numa_no)
  5317| {
  5318|     if (total_numa_nodes_on_machine == 1)
  5319|     {
  5320|         *numa_no = 0;
  5321|         return proc_no;
  5322|     }
  5323|     else
  5324|     {
  5325|         if (cpu_group_enabled_p)
  5326|         {
  5327|             *numa_no = proc_no >> 6;
  5328|             return (proc_no % 64);
  5329|         }
  5330|         else
  5331|         {
  5332|             *numa_no = proc_no / procs_per_numa_node;
  5333|             return (proc_no % procs_per_numa_node);
  5334|         }
  5335|     }
  5336| }
  5337| void add_to_hb_numa (
  5338|     int proc_no,
  5339|     int ideal_proc_no,
  5340|     int alloc_heap,
  5341|     bool multiple_procs_p,
  5342|     bool alloc_count_p,
  5343|     bool set_ideal_p)
  5344| {
  5345|     int tid = (int)GCToOSInterface::GetCurrentThreadIdForLogging ();
  5346|     uint64_t timestamp = RawGetHighPrecisionTimeStamp ();
  5347|     int saved_proc_no = proc_no;
  5348|     int numa_no = -1;
  5349|     proc_no = get_proc_index_numa (proc_no, &numa_no);
  5350|     heap_balance_info_numa* hb_info_numa_node = &hb_info_numa_nodes[numa_no];
  5351|     heap_balance_info_proc* hb_info_proc = &(hb_info_numa_node->hb_info_procs[proc_no]);
  5352|     int index = hb_info_proc->index;
  5353|     int count = hb_info_proc->count;
  5354|     if (index == count)
  5355|     {
  5356|         dprintf (HEAP_BALANCE_LOG, ("too much info between GCs, already logged %d entries", index));
  5357|         GCToOSInterface::DebugBreak ();
  5358|     }
  5359|     heap_balance_info* hb_info = &(hb_info_proc->hb_info[index]);
  5360|     dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMP[p%3d->%3d(i:%3d), N%d] #%4d: %zd, tid %d, ah: %d, m: %d, p: %d, i: %d",
  5361|         saved_proc_no, proc_no, ideal_proc_no, numa_no, index,
  5362|         (timestamp - start_raw_ts) / 1000, tid, alloc_heap, (int)multiple_procs_p, (int)(!alloc_count_p), (int)set_ideal_p));
  5363|     if (multiple_procs_p)
  5364|     {
  5365|         tid |= (1 << (sizeof (tid) * 8 - 1));
  5366|     }
  5367|     if (!alloc_count_p)
  5368|     {
  5369|         alloc_heap |= (1 << (sizeof (alloc_heap) * 8 - 1));
  5370|     }
  5371|     if (set_ideal_p)
  5372|     {
  5373|         alloc_heap |= (1 << (sizeof (alloc_heap) * 8 - 2));
  5374|     }
  5375|     hb_info->timestamp = timestamp;
  5376|     hb_info->tid = tid;
  5377|     hb_info->alloc_heap = alloc_heap;
  5378|     hb_info->ideal_proc_no = ideal_proc_no;
  5379|     (hb_info_proc->index)++;
  5380| }
  5381| const int hb_log_buffer_size = 4096;
  5382| static char hb_log_buffer[hb_log_buffer_size];
  5383| int last_hb_recorded_gc_index = -1;
  5384| #endif //HEAP_BALANCE_INSTRUMENTATION
  5385| void gc_heap::hb_log_balance_activities()
  5386| {
  5387| #ifdef HEAP_BALANCE_INSTRUMENTATION
  5388|     char* log_buffer = hb_log_buffer;
  5389|     uint64_t now = GetHighPrecisionTimeStamp();
  5390|     size_t time_since_last_gc_ms = (size_t)((now - last_gc_end_time_us) / 1000);
  5391|     dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMP%zd - %zd = %zd", now, last_gc_end_time_ms, time_since_last_gc_ms));
  5392|     uint64_t min_timestamp = 0xffffffffffffffff;
  5393|     uint64_t max_timestamp = 0;
  5394|     for (int numa_node_index = 0; numa_node_index < total_numa_nodes_on_machine; numa_node_index++)
  5395|     {
  5396|         heap_balance_info_proc* hb_info_procs = hb_info_numa_nodes[numa_node_index].hb_info_procs;
  5397|         for (int proc_index = 0; proc_index < (int)procs_per_numa_node; proc_index++)
  5398|         {
  5399|             heap_balance_info_proc* hb_info_proc = &hb_info_procs[proc_index];
  5400|             int total_entries_on_proc = hb_info_proc->index;
  5401|             if (total_entries_on_proc > 0)
  5402|             {
  5403|                 min_timestamp = min (min_timestamp, hb_info_proc->hb_info[0].timestamp);
  5404|                 max_timestamp = max (max_timestamp, hb_info_proc->hb_info[total_entries_on_proc - 1].timestamp);
  5405|             }
  5406|         }
  5407|     }
  5408|     dprintf (HEAP_BALANCE_LOG, ("[GCA#%zd %zd-%zd-%zd]",
  5409|         settings.gc_index, time_since_last_gc_ms, (min_timestamp - start_raw_ts), (max_timestamp - start_raw_ts)));
  5410|     if (last_hb_recorded_gc_index == (int)settings.gc_index)
  5411|     {
  5412|         GCToOSInterface::DebugBreak ();
  5413|     }
  5414|     last_hb_recorded_gc_index = (int)settings.gc_index;
  5415|     for (int numa_node_index = 0; numa_node_index < total_numa_nodes_on_machine; numa_node_index++)
  5416|     {
  5417|         heap_balance_info_proc* hb_info_procs = hb_info_numa_nodes[numa_node_index].hb_info_procs;
  5418|         for (int proc_index = 0; proc_index < (int)procs_per_numa_node; proc_index++)
  5419|         {
  5420|             heap_balance_info_proc* hb_info_proc = &hb_info_procs[proc_index];
  5421|             int total_entries_on_proc = hb_info_proc->index;
  5422|             if (total_entries_on_proc > 0)
  5423|             {
  5424|                 int total_exec_time_ms =
  5425|                     (int)((double)(hb_info_proc->hb_info[total_entries_on_proc - 1].timestamp -
  5426|                                    hb_info_proc->hb_info[0].timestamp) * qpf_ms);
  5427|                 dprintf (HEAP_BALANCE_LOG, ("[p%d]-%d-%dms",
  5428|                     (proc_index + numa_node_index * procs_per_numa_node),
  5429|                     total_entries_on_proc, total_exec_time_ms));
  5430|             }
  5431|             for (int i = 0; i < hb_info_proc->index; i++)
  5432|             {
  5433|                 heap_balance_info* hb_info = &hb_info_proc->hb_info[i];
  5434|                 bool multiple_procs_p = false;
  5435|                 bool alloc_count_p = true;
  5436|                 bool set_ideal_p = false;
  5437|                 int tid = hb_info->tid;
  5438|                 int alloc_heap = hb_info->alloc_heap;
  5439|                 if (tid & (1 << (sizeof (tid) * 8 - 1)))
  5440|                 {
  5441|                     multiple_procs_p = true;
  5442|                     tid &= ~(1 << (sizeof (tid) * 8 - 1));
  5443|                 }
  5444|                 if (alloc_heap & (1 << (sizeof (alloc_heap) * 8 - 1)))
  5445|                 {
  5446|                     alloc_count_p = false;
  5447|                     alloc_heap &= ~(1 << (sizeof (alloc_heap) * 8 - 1));
  5448|                 }
  5449|                 if (alloc_heap & (1 << (sizeof (alloc_heap) * 8 - 2)))
  5450|                 {
  5451|                     set_ideal_p = true;
  5452|                     alloc_heap &= ~(1 << (sizeof (alloc_heap) * 8 - 2));
  5453|                 }
  5454|                 int ideal_proc_no = hb_info->ideal_proc_no;
  5455|                 int ideal_node_no = -1;
  5456|                 ideal_proc_no = get_proc_index_numa (ideal_proc_no, &ideal_node_no);
  5457|                 ideal_proc_no = ideal_proc_no + ideal_node_no * procs_per_numa_node;
  5458|                 dprintf (HEAP_BALANCE_LOG, ("%zd,%d,%d,%d%s%s%s",
  5459|                     (hb_info->timestamp - start_raw_ts),
  5460|                     tid,
  5461|                     ideal_proc_no,
  5462|                     (int)alloc_heap,
  5463|                     (multiple_procs_p ? "|m" : ""), (!alloc_count_p ? "|p" : ""), (set_ideal_p ? "|i" : "")));
  5464|             }
  5465|         }
  5466|     }
  5467|     for (int numa_node_index = 0; numa_node_index < total_numa_nodes_on_machine; numa_node_index++)
  5468|     {
  5469|         heap_balance_info_proc* hb_info_procs = hb_info_numa_nodes[numa_node_index].hb_info_procs;
  5470|         for (int proc_index = 0; proc_index < (int)procs_per_numa_node; proc_index++)
  5471|         {
  5472|             heap_balance_info_proc* hb_info_proc = &hb_info_procs[proc_index];
  5473|             hb_info_proc->index = 0;
  5474|         }
  5475|     }
  5476| #endif //HEAP_BALANCE_INSTRUMENTATION
  5477| }
  5478| void gc_heap::hb_log_new_allocation()
  5479| {
  5480| #ifdef HEAP_BALANCE_INSTRUMENTATION
  5481|     char* log_buffer = hb_log_buffer;
  5482|     int desired_alloc_mb = (int)(dd_desired_allocation (g_heaps[0]->dynamic_data_of (0)) / 1024 / 1024);
  5483|     int buffer_pos = sprintf_s (hb_log_buffer, hb_log_buffer_size, "[GC_alloc_mb]\n");
  5484|     for (int numa_node_index = 0; numa_node_index < heap_select::total_numa_nodes; numa_node_index++)
  5485|     {
  5486|         int node_allocated_mb = 0;
  5487|         buffer_pos += sprintf_s (hb_log_buffer + buffer_pos, hb_log_buffer_size - buffer_pos, "[N#%3d]",
  5488|             desired_alloc_mb);
  5489|         int heaps_on_node = heap_select::heaps_on_node[numa_node_index].heap_count;
  5490|         for (int heap_index = 0; heap_index < heaps_on_node; heap_index++)
  5491|         {
  5492|             int actual_heap_index = heap_index + numa_node_index * heaps_on_node;
  5493|             gc_heap* hp = g_heaps[actual_heap_index];
  5494|             dynamic_data* dd0 = hp->dynamic_data_of (0);
  5495|             int allocated_mb = (int)((dd_desired_allocation (dd0) - dd_new_allocation (dd0)) / 1024 / 1024);
  5496|             node_allocated_mb += allocated_mb;
  5497|             buffer_pos += sprintf_s (hb_log_buffer + buffer_pos, hb_log_buffer_size - buffer_pos, "%d,",
  5498|                 allocated_mb);
  5499|         }
  5500|         dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMPN#%d a %dmb(%dmb)",
  5501|             numa_node_index, node_allocated_mb, desired_alloc_mb));
  5502|         buffer_pos += sprintf_s (hb_log_buffer + buffer_pos, hb_log_buffer_size - buffer_pos, "\n");
  5503|     }
  5504|     dprintf (HEAP_BALANCE_LOG, ("%s", hb_log_buffer));
  5505| #endif //HEAP_BALANCE_INSTRUMENTATION
  5506| }
  5507| BOOL gc_heap::create_thread_support (int number_of_heaps)
  5508| {
  5509|     BOOL ret = FALSE;
  5510|     if (!gc_start_event.CreateOSManualEventNoThrow (FALSE))
  5511|     {
  5512|         goto cleanup;
  5513|     }
  5514|     if (!ee_suspend_event.CreateOSAutoEventNoThrow (FALSE))
  5515|     {
  5516|         goto cleanup;
  5517|     }
  5518|     if (!gc_t_join.init (number_of_heaps, join_flavor_server_gc))
  5519|     {
  5520|         goto cleanup;
  5521|     }
  5522|     ret = TRUE;
  5523| cleanup:
  5524|     if (!ret)
  5525|     {
  5526|         destroy_thread_support();
  5527|     }
  5528|     return ret;
  5529| }
  5530| void gc_heap::destroy_thread_support ()
  5531| {
  5532|     if (ee_suspend_event.IsValid())
  5533|     {
  5534|         ee_suspend_event.CloseEvent();
  5535|     }
  5536|     if (gc_start_event.IsValid())
  5537|     {
  5538|         gc_start_event.CloseEvent();
  5539|     }
  5540| }
  5541| void set_thread_affinity_for_heap (int heap_number, uint16_t proc_no)
  5542| {
  5543|     if (!GCToOSInterface::SetThreadAffinity (proc_no))
  5544|     {
  5545|         dprintf (1, ("Failed to set thread affinity for GC thread %d on proc #%d", heap_number, proc_no));
  5546|     }
  5547| }
  5548| bool gc_heap::create_gc_thread ()
  5549| {
  5550|     dprintf (3, ("Creating gc thread\n"));
  5551|     return GCToEEInterface::CreateThread(gc_thread_stub, this, false, ".NET Server GC");
  5552| }
  5553| #ifdef _MSC_VER
  5554| #pragma warning(disable:4715) //IA64 xcompiler recognizes that without the 'break;' the while(1) will never end and therefore not return a value for that code path
  5555| #endif //_MSC_VER
  5556| void gc_heap::gc_thread_function ()
  5557| {
  5558|     assert (gc_done_event.IsValid());
  5559|     assert (gc_start_event.IsValid());
  5560|     dprintf (3, ("gc thread started"));
  5561|     heap_select::init_cpu_mapping(heap_number);
  5562|     while (1)
  5563|     {
  5564|         assert ((n_heaps <= heap_number) || !gc_t_join.joined());
  5565|         if (heap_number == 0)
  5566|         {
  5567|             bool wait_on_time_out_p = gradual_decommit_in_progress_p;
  5568|             uint32_t wait_time = DECOMMIT_TIME_STEP_MILLISECONDS;
  5569| #ifdef DYNAMIC_HEAP_COUNT
  5570|             if (!gc_heap::background_running_p () && dynamic_heap_count_data.should_change_heap_count)
  5571|             {
  5572|                 assert (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes);
  5573|                 dynamic_heap_count_data_t::sample& sample = dynamic_heap_count_data.samples[dynamic_heap_count_data.sample_index];
  5574|                 wait_time = min (wait_time, (uint32_t)(sample.elapsed_between_gcs / 1000 / 3));
  5575|                 wait_time = max (wait_time, 1);
  5576|                 dprintf (6666, ("gc#0 thread waiting for %d ms (betwen GCs %I64d)", wait_time, sample.elapsed_between_gcs));
  5577|             }
  5578| #endif //DYNAMIC_HEAP_COUNT
  5579|             uint32_t wait_result = gc_heap::ee_suspend_event.Wait(wait_on_time_out_p ? wait_time : INFINITE, FALSE);
  5580|             dprintf (9999, ("waiting for ee done res %d (timeout %d, %I64d ms since last suspend end)(should_change_heap_count is %d) (gradual_decommit_in_progress_p %d)",
  5581|                 wait_result, wait_time, ((GetHighPrecisionTimeStamp() - last_suspended_end_time) / 1000),
  5582|                 dynamic_heap_count_data.should_change_heap_count, gradual_decommit_in_progress_p));
  5583|             if (wait_result == WAIT_TIMEOUT)
  5584|             {
  5585| #ifdef DYNAMIC_HEAP_COUNT
  5586|                 if (dynamic_heap_count_data.should_change_heap_count)
  5587|                 {
  5588| #ifdef BACKGROUND_GC
  5589|                     if (!gc_heap::background_running_p ())
  5590| #endif //BACKGROUND_GC
  5591|                     {
  5592|                         dprintf (6666, ("changing heap count due to timeout"));
  5593|                         check_heap_count();
  5594|                     }
  5595|                 }
  5596| #endif //DYNAMIC_HEAP_COUNT
  5597|                 if (gradual_decommit_in_progress_p)
  5598|                 {
  5599|                     decommit_lock.Enter ();
  5600|                     gradual_decommit_in_progress_p = decommit_step (DECOMMIT_TIME_STEP_MILLISECONDS);
  5601|                     decommit_lock.Leave ();
  5602|                 }
  5603|                 continue;
  5604|             }
  5605| #ifdef DYNAMIC_HEAP_COUNT
  5606|             if (dynamic_heap_count_data.should_change_heap_count)
  5607|             {
  5608| #ifdef BACKGROUND_GC
  5609|                 if (!gc_heap::background_running_p ())
  5610| #endif //BACKGROUND_GC
  5611|                 {
  5612|                     dprintf (6666, ("changing heap count at a GC start"));
  5613|                     check_heap_count ();
  5614|                 }
  5615|             }
  5616|             if ((gc_heap::dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes) && 
  5617|                 (n_heaps != dynamic_heap_count_data.last_n_heaps))
  5618|             {
  5619|                 int spin_count = 1024;
  5620|                 int idle_thread_count = n_max_heaps - n_heaps;
  5621|                 dprintf (9999, ("heap count changed %d->%d, idle should be %d and is %d", dynamic_heap_count_data.last_n_heaps, n_heaps,
  5622|                     idle_thread_count, VolatileLoadWithoutBarrier (&dynamic_heap_count_data.idle_thread_count)));
  5623|                 if (idle_thread_count != dynamic_heap_count_data.idle_thread_count)
  5624|                 {
  5625|                     spin_and_wait (spin_count, (idle_thread_count == dynamic_heap_count_data.idle_thread_count));
  5626|                     dprintf (9999, ("heap count changed %d->%d, now idle is %d", dynamic_heap_count_data.last_n_heaps, n_heaps,
  5627|                         VolatileLoadWithoutBarrier (&dynamic_heap_count_data.idle_thread_count)));
  5628|                 }
  5629|                 dynamic_heap_count_data.last_n_heaps = n_heaps;
  5630|             }
  5631| #endif //DYNAMIC_HEAP_COUNT
  5632|             suspended_start_time = GetHighPrecisionTimeStamp();
  5633|             BEGIN_TIMING(suspend_ee_during_log);
  5634|             dprintf (9999, ("h0 suspending EE in GC!"));
  5635|             GCToEEInterface::SuspendEE(SUSPEND_FOR_GC);
  5636|             dprintf (9999, ("h0 suspended EE in GC!"));
  5637|             END_TIMING(suspend_ee_during_log);
  5638|             proceed_with_gc_p = TRUE;
  5639|             if (!should_proceed_with_gc())
  5640|             {
  5641|                 update_collection_counts_for_no_gc();
  5642|                 proceed_with_gc_p = FALSE;
  5643|             }
  5644|             else
  5645|             {
  5646|                 settings.init_mechanisms();
  5647| #ifdef DYNAMIC_HEAP_COUNT
  5648|                 if (gc_heap::dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes)
  5649|                 {
  5650|                     assert (dynamic_heap_count_data.new_n_heaps == n_heaps);
  5651|                 }
  5652| #endif //DYNAMIC_HEAP_COUNT
  5653|                 dprintf (9999, ("GC thread %d setting_gc_start_in_gc(h%d)", heap_number, n_heaps));
  5654|                 gc_start_event.Set();
  5655|             }
  5656|             dprintf (3, (ThreadStressLog::gcServerThread0StartMsg(), heap_number));
  5657|         }
  5658|         else
  5659|         {
  5660|             dprintf (9999, ("GC thread %d waiting_for_gc_start(%d)(gc%Id)", heap_number, n_heaps, VolatileLoadWithoutBarrier(&settings.gc_index)));
  5661|             gc_start_event.Wait(INFINITE, FALSE);
  5662| #ifdef DYNAMIC_HEAP_COUNT
  5663|             dprintf (9999, ("GC thread %d waiting_done_gc_start(%d-%d)(i: %d)(gc%Id)",
  5664|                 heap_number, n_heaps, dynamic_heap_count_data.new_n_heaps, dynamic_heap_count_data.init_only_p, VolatileLoadWithoutBarrier (&settings.gc_index)));
  5665|             if ((gc_heap::dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes) &&
  5666|                 (dynamic_heap_count_data.new_n_heaps != n_heaps))
  5667|             {
  5668|                 int old_n_heaps = n_heaps;
  5669|                 int new_n_heaps = dynamic_heap_count_data.new_n_heaps;
  5670|                 int num_threads_to_wake = max (new_n_heaps, old_n_heaps);
  5671|                 if (heap_number < num_threads_to_wake)
  5672|                 {
  5673|                     dprintf (9999, ("h%d < %d, calling change", heap_number, num_threads_to_wake));
  5674|                     change_heap_count (dynamic_heap_count_data.new_n_heaps);
  5675|                     if (new_n_heaps < old_n_heaps)
  5676|                     {
  5677|                         dprintf (9999, ("h%d after change", heap_number));
  5678|                         if (heap_number < new_n_heaps)
  5679|                         {
  5680|                             dprintf (9999, ("h%d < %d participating (dec)", heap_number, new_n_heaps));
  5681|                         }
  5682|                         else
  5683|                         {
  5684|                             Interlocked::Increment (&dynamic_heap_count_data.idle_thread_count);
  5685|                             dprintf (9999, ("GC thread %d wait_on_idle(%d < %d)(gc%Id), total idle %d", heap_number, old_n_heaps, new_n_heaps,
  5686|                                 VolatileLoadWithoutBarrier (&settings.gc_index), VolatileLoadWithoutBarrier (&dynamic_heap_count_data.idle_thread_count)));
  5687|                             gc_idle_thread_event.Wait (INFINITE, FALSE);
  5688|                             dprintf (9999, ("GC thread %d waking_from_idle(%d)(gc%Id) after doing change", heap_number, n_heaps, VolatileLoadWithoutBarrier (&settings.gc_index)));
  5689|                         }
  5690|                     }
  5691|                     else
  5692|                     {
  5693|                         dprintf (9999, ("h%d < %d participating (inc)", heap_number, new_n_heaps));
  5694|                     }
  5695|                 }
  5696|                 else
  5697|                 {
  5698|                     Interlocked::Increment (&dynamic_heap_count_data.idle_thread_count);
  5699|                     dprintf (9999, ("GC thread %d wait_on_idle(< max %d)(gc%Id), total  idle %d", heap_number, num_threads_to_wake,
  5700|                         VolatileLoadWithoutBarrier (&settings.gc_index), VolatileLoadWithoutBarrier (&dynamic_heap_count_data.idle_thread_count)));
  5701|                     gc_idle_thread_event.Wait (INFINITE, FALSE);
  5702|                     dprintf (9999, ("GC thread %d waking_from_idle(%d)(gc%Id)", heap_number, n_heaps, VolatileLoadWithoutBarrier (&settings.gc_index)));
  5703|                 }
  5704|                 continue;
  5705|             }
  5706| #endif //DYNAMIC_HEAP_COUNT
  5707|             dprintf (3, (ThreadStressLog::gcServerThreadNStartMsg(), heap_number));
  5708|         }
  5709|         assert ((heap_number == 0) || proceed_with_gc_p);
  5710|         if (proceed_with_gc_p)
  5711|         {
  5712|             garbage_collect (GCHeap::GcCondemnedGeneration);
  5713|             if (pm_trigger_full_gc)
  5714|             {
  5715|                 garbage_collect_pm_full_gc();
  5716|             }
  5717|         }
  5718|         if (heap_number == 0)
  5719|         {
  5720|             if (proceed_with_gc_p && (!settings.concurrent))
  5721|             {
  5722|                 do_post_gc();
  5723|             }
  5724| #ifdef BACKGROUND_GC
  5725|             recover_bgc_settings();
  5726| #endif //BACKGROUND_GC
  5727| #ifdef MULTIPLE_HEAPS
  5728| #ifdef STRESS_DYNAMIC_HEAP_COUNT
  5729|             dynamic_heap_count_data.lowest_heap_with_msl_uoh = -1;
  5730| #endif //STRESS_DYNAMIC_HEAP_COUNT
  5731|             for (int i = 0; i < gc_heap::n_heaps; i++)
  5732|             {
  5733|                 gc_heap* hp = gc_heap::g_heaps[i];
  5734|                 leave_spin_lock(&hp->more_space_lock_soh);
  5735| #ifdef STRESS_DYNAMIC_HEAP_COUNT
  5736|                 if ((dynamic_heap_count_data.lowest_heap_with_msl_uoh == -1) && (hp->uoh_msl_before_gc_p))
  5737|                 {
  5738|                     dynamic_heap_count_data.lowest_heap_with_msl_uoh = i;
  5739|                 }
  5740|                 if (hp->uoh_msl_before_gc_p)
  5741|                 {
  5742|                     dprintf (5555, ("h%d uoh msl was taken before GC", i));
  5743|                     hp->uoh_msl_before_gc_p = false;
  5744|                 }
  5745| #endif //STRESS_DYNAMIC_HEAP_COUNT
  5746|             }
  5747| #endif //MULTIPLE_HEAPS
  5748|             gc_heap::gc_started = FALSE;
  5749| #ifdef BACKGROUND_GC
  5750|             gc_heap::add_bgc_pause_duration_0();
  5751| #endif //BACKGROUND_GC
  5752|             BEGIN_TIMING(restart_ee_during_log);
  5753|             GCToEEInterface::RestartEE(TRUE);
  5754|             END_TIMING(restart_ee_during_log);
  5755|             process_sync_log_stats();
  5756|             dprintf (SPINLOCK_LOG, ("GC Lgc"));
  5757|             leave_spin_lock (&gc_heap::gc_lock);
  5758|             gc_heap::internal_gc_done = true;
  5759|             if (proceed_with_gc_p)
  5760|                 set_gc_done();
  5761|             else
  5762|             {
  5763|                 for (int i = 0; i < gc_heap::n_heaps; i++)
  5764|                 {
  5765|                     gc_heap* hp = gc_heap::g_heaps[i];
  5766|                     hp->set_gc_done();
  5767|                 }
  5768|             }
  5769|             if (gradual_decommit_in_progress_p)
  5770|             {
  5771|                 gradual_decommit_in_progress_p = decommit_step (DECOMMIT_TIME_STEP_MILLISECONDS);
  5772|             }
  5773|         }
  5774|         else
  5775|         {
  5776|             int spin_count = 32 * (gc_heap::n_heaps - 1);
  5777|             while (!gc_heap::internal_gc_done && !GCHeap::SafeToRestartManagedThreads())
  5778|             {
  5779|                 spin_and_switch (spin_count, (gc_heap::internal_gc_done || GCHeap::SafeToRestartManagedThreads()));
  5780|             }
  5781|             set_gc_done();
  5782|         }
  5783|     }
  5784| }
  5785| #ifdef _MSC_VER
  5786| #pragma warning(default:4715) //IA64 xcompiler recognizes that without the 'break;' the while(1) will never end and therefore not return a value for that code path
  5787| #endif //_MSC_VER
  5788| #endif //MULTIPLE_HEAPS
  5789| bool gc_heap::virtual_alloc_commit_for_heap (void* addr, size_t size, int h_number)
  5790| {
  5791| #if defined(MULTIPLE_HEAPS) && !defined(FEATURE_NATIVEAOT)
  5792| #if !defined(FEATURE_CORECLR) && !defined(BUILD_AS_STANDALONE)
  5793|     if (!CLRMemoryHosted())
  5794| #endif
  5795|     {
  5796|         if (GCToOSInterface::CanEnableGCNumaAware())
  5797|         {
  5798|             uint16_t numa_node = heap_select::find_numa_node_from_heap_no(h_number);
  5799|             if (GCToOSInterface::VirtualCommit (addr, size, numa_node))
  5800|                 return true;
  5801|         }
  5802|     }
  5803| #else //MULTIPLE_HEAPS && !FEATURE_NATIVEAOT
  5804|     UNREFERENCED_PARAMETER(h_number);
  5805| #endif //MULTIPLE_HEAPS && !FEATURE_NATIVEAOT
  5806|     return GCToOSInterface::VirtualCommit(addr, size);
  5807| }
  5808| bool gc_heap::virtual_commit (void* address, size_t size, int bucket, int h_number, bool* hard_limit_exceeded_p)
  5809| {
  5810|     /**
  5811|      * Here are all the possible cases for the commits:
  5812|      *
  5813|      * Case 1: This is for a particular generation - the bucket will be one of the gc_oh_num != unknown, and the h_number will be the right heap
  5814|      * Case 2: This is for bookkeeping - the bucket will be recorded_committed_bookkeeping_bucket, and the h_number will be -1
  5815|      *
  5816|      * Note  : We never commit into free directly, so bucket != recorded_committed_free_bucket
  5817|      */
  5818| #ifndef HOST_64BIT
  5819|     assert (heap_hard_limit == 0);
  5820| #endif //!HOST_64BIT
  5821|     assert(0 <= bucket && bucket < recorded_committed_bucket_counts);
  5822|     assert(bucket < total_oh_count || h_number == -1);
  5823|     assert(bucket != recorded_committed_free_bucket);
  5824|     dprintf(3, ("commit-accounting:  commit in %d [%p, %p) for heap %d", bucket, address, ((uint8_t*)address + size), h_number));
  5825| #ifndef COMMITTED_BYTES_SHADOW
  5826|     if (heap_hard_limit)
  5827| #endif //!COMMITTED_BYTES_SHADOW
  5828|     {
  5829|         check_commit_cs.Enter();
  5830|         bool exceeded_p = false;
  5831|         if (heap_hard_limit_oh[soh] != 0)
  5832|         {
  5833|             if ((bucket < total_oh_count) && (committed_by_oh[bucket] + size) > heap_hard_limit_oh[bucket])
  5834|             {
  5835|                 exceeded_p = true;
  5836|             }
  5837|         }
  5838|         else
  5839|         {
  5840|             size_t base = current_total_committed;
  5841|             size_t limit = heap_hard_limit;
  5842|             if ((base + size) > limit)
  5843|             {
  5844|                 dprintf (1, ("%zd + %zd = %zd > limit %zd ", base, size, (base + size), limit));
  5845|                 exceeded_p = true;
  5846|             }
  5847|         }
  5848| #ifdef COMMITTED_BYTES_SHADOW
  5849|         if (!heap_hard_limit) {
  5850|             exceeded_p = false;
  5851|         }
  5852| #endif //COMMITTED_BYTES_SHADOW
  5853|         if (!exceeded_p)
  5854|         {
  5855| #if defined(_DEBUG) && defined(MULTIPLE_HEAPS)
  5856|             if ((h_number != -1) && (bucket < total_oh_count))
  5857|             {
  5858|                 g_heaps[h_number]->committed_by_oh_per_heap[bucket] += size;
  5859|             }
  5860| #endif // _DEBUG && MULTIPLE_HEAPS
  5861|             committed_by_oh[bucket] += size;
  5862|             current_total_committed += size;
  5863|             if (h_number < 0)
  5864|                 current_total_committed_bookkeeping += size;
  5865|         }
  5866|         check_commit_cs.Leave();
  5867|         if (hard_limit_exceeded_p)
  5868|             *hard_limit_exceeded_p = exceeded_p;
  5869|         if (exceeded_p)
  5870|         {
  5871|             dprintf (1, ("can't commit %zx for %zd bytes > HARD LIMIT %zd", (size_t)address, size, heap_hard_limit));
  5872|             return false;
  5873|         }
  5874|     }
  5875|     bool commit_succeeded_p = ((h_number >= 0) ? (use_large_pages_p ? true :
  5876|                               virtual_alloc_commit_for_heap (address, size, h_number)) :
  5877|                               GCToOSInterface::VirtualCommit(address, size));
  5878|     if (!commit_succeeded_p && heap_hard_limit)
  5879|     {
  5880|         check_commit_cs.Enter();
  5881|         committed_by_oh[bucket] -= size;
  5882| #if defined(_DEBUG) && defined(MULTIPLE_HEAPS)
  5883|         if ((h_number != -1) && (bucket < total_oh_count))
  5884|         {
  5885|             assert (g_heaps[h_number]->committed_by_oh_per_heap[bucket] >= size);
  5886|             g_heaps[h_number]->committed_by_oh_per_heap[bucket] -= size;
  5887|         }
  5888| #endif // _DEBUG && MULTIPLE_HEAPS
  5889|         dprintf (1, ("commit failed, updating %zd to %zd",
  5890|                 current_total_committed, (current_total_committed - size)));
  5891|         current_total_committed -= size;
  5892|         if (h_number < 0)
  5893|         {
  5894|             assert (current_total_committed_bookkeeping >= size);
  5895|             current_total_committed_bookkeeping -= size;
  5896|         }
  5897|         check_commit_cs.Leave();
  5898|     }
  5899|     return commit_succeeded_p;
  5900| }
  5901| bool gc_heap::virtual_decommit (void* address, size_t size, int bucket, int h_number)
  5902| {
  5903|     /**
  5904|      * Here are all possible cases for the decommits:
  5905|      *
  5906|      * Case 1: This is for a particular generation - the bucket will be one of the gc_oh_num != unknown, and the h_number will be the right heap
  5907|      * Case 2: This is for bookkeeping - the bucket will be recorded_committed_bookkeeping_bucket, and the h_number will be -1
  5908|      * Case 3: This is for free - the bucket will be recorded_committed_free_bucket, and the h_number will be -1
  5909|      */
  5910| #ifndef HOST_64BIT
  5911|     assert (heap_hard_limit == 0);
  5912| #endif //!HOST_64BIT
  5913|     assert(0 <= bucket && bucket < recorded_committed_bucket_counts);
  5914|     assert(bucket < total_oh_count || h_number == -1);
  5915|     bool decommit_succeeded_p = ((bucket != recorded_committed_bookkeeping_bucket) && use_large_pages_p) ? true : GCToOSInterface::VirtualDecommit (address, size);
  5916|     dprintf(3, ("commit-accounting:  decommit in %d [%p, %p) for heap %d", bucket, address, ((uint8_t*)address + size), h_number));
  5917|     if (decommit_succeeded_p)
  5918| #ifndef COMMITTED_BYTES_SHADOW
  5919|     if (heap_hard_limit)
  5920| #endif //!COMMITTED_BYTES_SHADOW
  5921|     {
  5922|         check_commit_cs.Enter();
  5923|         assert (committed_by_oh[bucket] >= size);
  5924|         committed_by_oh[bucket] -= size;
  5925| #if defined(_DEBUG) && defined(MULTIPLE_HEAPS)
  5926|         if ((h_number != -1) && (bucket < total_oh_count))
  5927|         {
  5928|             assert (g_heaps[h_number]->committed_by_oh_per_heap[bucket] >= size);
  5929|             g_heaps[h_number]->committed_by_oh_per_heap[bucket] -= size;
  5930|         }
  5931| #endif // _DEBUG && MULTIPLE_HEAPS
  5932|         assert (current_total_committed >= size);
  5933|         current_total_committed -= size;
  5934|         if (bucket == recorded_committed_bookkeeping_bucket)
  5935|         {
  5936|             assert (current_total_committed_bookkeeping >= size);
  5937|             current_total_committed_bookkeeping -= size;
  5938|         }
  5939|         check_commit_cs.Leave();
  5940|     }
  5941|     return decommit_succeeded_p;
  5942| }
  5943| void gc_heap::virtual_free (void* add, size_t allocated_size, heap_segment* sg)
  5944| {
  5945|     bool release_succeeded_p = GCToOSInterface::VirtualRelease (add, allocated_size);
  5946|     if (release_succeeded_p)
  5947|     {
  5948|         reserved_memory -= allocated_size;
  5949|         dprintf (2, ("Virtual Free size %zd: [%zx, %zx[",
  5950|                     allocated_size, (size_t)add, (size_t)((uint8_t*)add + allocated_size)));
  5951|     }
  5952| }
  5953| class mark
  5954| {
  5955| public:
  5956|     uint8_t* first;
  5957|     size_t len;
  5958|     gap_reloc_pair saved_pre_plug;
  5959|     gap_reloc_pair saved_pre_plug_reloc;
  5960|     gap_reloc_pair saved_post_plug;
  5961|     gap_reloc_pair saved_post_plug_reloc;
  5962|     uint8_t* saved_pre_plug_info_reloc_start;
  5963|     uint8_t* saved_post_plug_info_start;
  5964| #ifdef SHORT_PLUGS
  5965|     uint8_t* allocation_context_start_region;
  5966| #endif //SHORT_PLUGS
  5967|     BOOL saved_pre_p;
  5968|     BOOL saved_post_p;
  5969| #ifdef _DEBUG
  5970|     gap_reloc_pair saved_post_plug_debug;
  5971| #endif //_DEBUG
  5972|     size_t get_max_short_bits()
  5973|     {
  5974|         return (sizeof (gap_reloc_pair) / sizeof (uint8_t*));
  5975|     }
  5976|     size_t get_pre_short_start_bit ()
  5977|     {
  5978|         return (sizeof (saved_pre_p) * 8 - 1 - (sizeof (gap_reloc_pair) / sizeof (uint8_t*)));
  5979|     }
  5980|     BOOL pre_short_p()
  5981|     {
  5982|         return (saved_pre_p & (1 << (sizeof (saved_pre_p) * 8 - 1)));
  5983|     }
  5984|     void set_pre_short()
  5985|     {
  5986|         saved_pre_p |= (1 << (sizeof (saved_pre_p) * 8 - 1));
  5987|     }
  5988|     void set_pre_short_bit (size_t bit)
  5989|     {
  5990|         saved_pre_p |= 1 << (get_pre_short_start_bit() + bit);
  5991|     }
  5992|     BOOL pre_short_bit_p (size_t bit)
  5993|     {
  5994|         return (saved_pre_p & (1 << (get_pre_short_start_bit() + bit)));
  5995|     }
  5996| #ifdef COLLECTIBLE_CLASS
  5997|     void set_pre_short_collectible()
  5998|     {
  5999|         saved_pre_p |= 2;
  6000|     }
  6001|     BOOL pre_short_collectible_p()
  6002|     {
  6003|         return (saved_pre_p & 2);
  6004|     }
  6005| #endif //COLLECTIBLE_CLASS
  6006|     size_t get_post_short_start_bit ()
  6007|     {
  6008|         return (sizeof (saved_post_p) * 8 - 1 - (sizeof (gap_reloc_pair) / sizeof (uint8_t*)));
  6009|     }
  6010|     BOOL post_short_p()
  6011|     {
  6012|         return (saved_post_p & (1 << (sizeof (saved_post_p) * 8 - 1)));
  6013|     }
  6014|     void set_post_short()
  6015|     {
  6016|         saved_post_p |= (1 << (sizeof (saved_post_p) * 8 - 1));
  6017|     }
  6018|     void set_post_short_bit (size_t bit)
  6019|     {
  6020|         saved_post_p |= 1 << (get_post_short_start_bit() + bit);
  6021|     }
  6022|     BOOL post_short_bit_p (size_t bit)
  6023|     {
  6024|         return (saved_post_p & (1 << (get_post_short_start_bit() + bit)));
  6025|     }
  6026| #ifdef COLLECTIBLE_CLASS
  6027|     void set_post_short_collectible()
  6028|     {
  6029|         saved_post_p |= 2;
  6030|     }
  6031|     BOOL post_short_collectible_p()
  6032|     {
  6033|         return (saved_post_p & 2);
  6034|     }
  6035| #endif //COLLECTIBLE_CLASS
  6036|     uint8_t* get_plug_address() { return first; }
  6037|     BOOL has_pre_plug_info() { return saved_pre_p; }
  6038|     BOOL has_post_plug_info() { return saved_post_p; }
  6039|     gap_reloc_pair* get_pre_plug_reloc_info() { return &saved_pre_plug_reloc; }
  6040|     gap_reloc_pair* get_post_plug_reloc_info() { return &saved_post_plug_reloc; }
  6041|     void set_pre_plug_info_reloc_start (uint8_t* reloc) { saved_pre_plug_info_reloc_start = reloc; }
  6042|     uint8_t* get_post_plug_info_start() { return saved_post_plug_info_start; }
  6043|     void swap_pre_plug_and_saved()
  6044|     {
  6045|         gap_reloc_pair temp;
  6046|         memcpy (&temp, (first - sizeof (plug_and_gap)), sizeof (temp));
  6047|         memcpy ((first - sizeof (plug_and_gap)), &saved_pre_plug_reloc, sizeof (saved_pre_plug_reloc));
  6048|         saved_pre_plug_reloc = temp;
  6049|     }
  6050|     void swap_post_plug_and_saved()
  6051|     {
  6052|         gap_reloc_pair temp;
  6053|         memcpy (&temp, saved_post_plug_info_start, sizeof (temp));
  6054|         memcpy (saved_post_plug_info_start, &saved_post_plug_reloc, sizeof (saved_post_plug_reloc));
  6055|         saved_post_plug_reloc = temp;
  6056|     }
  6057|     void swap_pre_plug_and_saved_for_profiler()
  6058|     {
  6059|         gap_reloc_pair temp;
  6060|         memcpy (&temp, (first - sizeof (plug_and_gap)), sizeof (temp));
  6061|         memcpy ((first - sizeof (plug_and_gap)), &saved_pre_plug, sizeof (saved_pre_plug));
  6062|         saved_pre_plug = temp;
  6063|     }
  6064|     void swap_post_plug_and_saved_for_profiler()
  6065|     {
  6066|         gap_reloc_pair temp;
  6067|         memcpy (&temp, saved_post_plug_info_start, sizeof (temp));
  6068|         memcpy (saved_post_plug_info_start, &saved_post_plug, sizeof (saved_post_plug));
  6069|         saved_post_plug = temp;
  6070|     }
  6071|     size_t recover_plug_info()
  6072|     {
  6073|         size_t recovered_sweep_size = 0;
  6074|         if (saved_pre_p)
  6075|         {
  6076|             if (gc_heap::settings.compaction)
  6077|             {
  6078|                 dprintf (3, ("%p: REC Pre: %p-%p",
  6079|                     first,
  6080|                     &saved_pre_plug_reloc,
  6081|                     saved_pre_plug_info_reloc_start));
  6082|                 memcpy (saved_pre_plug_info_reloc_start, &saved_pre_plug_reloc, sizeof (saved_pre_plug_reloc));
  6083|             }
  6084|             else
  6085|             {
  6086|                 dprintf (3, ("%p: REC Pre: %p-%p",
  6087|                     first,
  6088|                     &saved_pre_plug,
  6089|                     (first - sizeof (plug_and_gap))));
  6090|                 memcpy ((first - sizeof (plug_and_gap)), &saved_pre_plug, sizeof (saved_pre_plug));
  6091|                 recovered_sweep_size += sizeof (saved_pre_plug);
  6092|             }
  6093|         }
  6094|         if (saved_post_p)
  6095|         {
  6096|             if (gc_heap::settings.compaction)
  6097|             {
  6098|                 dprintf (3, ("%p: REC Post: %p-%p",
  6099|                     first,
  6100|                     &saved_post_plug_reloc,
  6101|                     saved_post_plug_info_start));
  6102|                 memcpy (saved_post_plug_info_start, &saved_post_plug_reloc, sizeof (saved_post_plug_reloc));
  6103|             }
  6104|             else
  6105|             {
  6106|                 dprintf (3, ("%p: REC Post: %p-%p",
  6107|                     first,
  6108|                     &saved_post_plug,
  6109|                     saved_post_plug_info_start));
  6110|                 memcpy (saved_post_plug_info_start, &saved_post_plug, sizeof (saved_post_plug));
  6111|                 recovered_sweep_size += sizeof (saved_post_plug);
  6112|             }
  6113|         }
  6114|         return recovered_sweep_size;
  6115|     }
  6116| };
  6117| void gc_mechanisms::init_mechanisms()
  6118| {
  6119|     condemned_generation = 0;
  6120|     promotion = FALSE;//TRUE;
  6121|     compaction = TRUE;
  6122| #ifdef FEATURE_LOH_COMPACTION
  6123|     loh_compaction = gc_heap::loh_compaction_requested();
  6124| #else
  6125|     loh_compaction = FALSE;
  6126| #endif //FEATURE_LOH_COMPACTION
  6127|     heap_expansion = FALSE;
  6128|     concurrent = FALSE;
  6129|     demotion = FALSE;
  6130|     elevation_reduced = FALSE;
  6131|     found_finalizers = FALSE;
  6132| #ifdef BACKGROUND_GC
  6133|     background_p = gc_heap::background_running_p() != FALSE;
  6134| #endif //BACKGROUND_GC
  6135|     entry_memory_load = 0;
  6136|     entry_available_physical_mem = 0;
  6137|     exit_memory_load = 0;
  6138| #ifdef STRESS_HEAP
  6139|     stress_induced = FALSE;
  6140| #endif // STRESS_HEAP
  6141| }
  6142| void gc_mechanisms::first_init()
  6143| {
  6144|     gc_index = 0;
  6145|     gen0_reduction_count = 0;
  6146|     should_lock_elevation = FALSE;
  6147|     elevation_locked_count = 0;
  6148|     reason = reason_empty;
  6149| #ifdef BACKGROUND_GC
  6150|     pause_mode = gc_heap::gc_can_use_concurrent ? pause_interactive : pause_batch;
  6151| #ifdef _DEBUG
  6152|     int debug_pause_mode = static_cast<int>(GCConfig::GetLatencyMode());
  6153|     if (debug_pause_mode >= 0)
  6154|     {
  6155|         assert (debug_pause_mode <= pause_sustained_low_latency);
  6156|         pause_mode = (gc_pause_mode)debug_pause_mode;
  6157|     }
  6158| #endif //_DEBUG
  6159| #else //BACKGROUND_GC
  6160|     pause_mode = pause_batch;
  6161| #endif //BACKGROUND_GC
  6162|     init_mechanisms();
  6163| }
  6164| void gc_mechanisms::record (gc_history_global* history)
  6165| {
  6166| #ifdef MULTIPLE_HEAPS
  6167|     history->num_heaps = gc_heap::n_heaps;
  6168| #else
  6169|     history->num_heaps = 1;
  6170| #endif //MULTIPLE_HEAPS
  6171|     history->condemned_generation = condemned_generation;
  6172|     history->gen0_reduction_count = gen0_reduction_count;
  6173|     history->reason = reason;
  6174|     history->pause_mode = (int)pause_mode;
  6175|     history->mem_pressure = entry_memory_load;
  6176|     history->global_mechanisms_p = 0;
  6177|     if (concurrent)
  6178|         history->set_mechanism_p (global_concurrent);
  6179|     if (compaction)
  6180|         history->set_mechanism_p (global_compaction);
  6181|     if (promotion)
  6182|         history->set_mechanism_p (global_promotion);
  6183|     if (demotion)
  6184|         history->set_mechanism_p (global_demotion);
  6185|     if (card_bundles)
  6186|         history->set_mechanism_p (global_card_bundles);
  6187|     if (elevation_reduced)
  6188|         history->set_mechanism_p (global_elevation);
  6189| }
  6190| /**********************************
  6191|    called at the beginning of GC to fix the allocated size to
  6192|    what is really allocated, or to turn the free area into an unused object
  6193|    It needs to be called after all of the other allocation contexts have been
  6194|    fixed since it relies on alloc_allocated.
  6195|  ********************************/
  6196| void gc_heap::fix_youngest_allocation_area()
  6197| {
  6198|     assert (generation_allocation_pointer (youngest_generation) == nullptr);
  6199|     assert (generation_allocation_limit (youngest_generation) == nullptr);
  6200|     heap_segment_allocated (ephemeral_heap_segment) = alloc_allocated;
  6201|     assert (heap_segment_mem (ephemeral_heap_segment) <= heap_segment_allocated (ephemeral_heap_segment));
  6202|     assert (heap_segment_allocated (ephemeral_heap_segment) <= heap_segment_reserved (ephemeral_heap_segment));
  6203| }
  6204| void gc_heap::fix_allocation_context (alloc_context* acontext, BOOL for_gc_p,
  6205|                                       BOOL record_ac_p)
  6206| {
  6207|     dprintf (3, ("Fixing allocation context %zx: ptr: %zx, limit: %zx",
  6208|                  (size_t)acontext,
  6209|                  (size_t)acontext->alloc_ptr, (size_t)acontext->alloc_limit));
  6210|     if (acontext->alloc_ptr == 0)
  6211|     {
  6212|         return;
  6213|     }
  6214|     int align_const = get_alignment_constant (TRUE);
  6215| #ifdef USE_REGIONS
  6216|     bool is_ephemeral_heap_segment = in_range_for_segment (acontext->alloc_limit, ephemeral_heap_segment);
  6217| #else // USE_REGIONS
  6218|     bool is_ephemeral_heap_segment = true;
  6219| #endif // USE_REGIONS
  6220|     if ((!is_ephemeral_heap_segment) || ((size_t)(alloc_allocated - acontext->alloc_limit) > Align (min_obj_size, align_const)) ||
  6221|         !for_gc_p)
  6222|     {
  6223|         uint8_t*  point = acontext->alloc_ptr;
  6224|         size_t  size = (acontext->alloc_limit - acontext->alloc_ptr);
  6225|         size += Align (min_obj_size, align_const);
  6226|         assert ((size >= Align (min_obj_size)));
  6227|         dprintf(3,("Making unused area [%zx, %zx[", (size_t)point,
  6228|                     (size_t)point + size ));
  6229|         make_unused_array (point, size);
  6230|         if (for_gc_p)
  6231|         {
  6232|             generation_free_obj_space (generation_of (0)) += size;
  6233|             if (record_ac_p)
  6234|                 alloc_contexts_used ++;
  6235|         }
  6236|     }
  6237|     else if (for_gc_p)
  6238|     {
  6239|         assert (is_ephemeral_heap_segment);
  6240|         alloc_allocated = acontext->alloc_ptr;
  6241|         assert (heap_segment_allocated (ephemeral_heap_segment) <=
  6242|                 heap_segment_committed (ephemeral_heap_segment));
  6243|         if (record_ac_p)
  6244|             alloc_contexts_used ++;
  6245|     }
  6246|     if (for_gc_p)
  6247|     {
  6248|         acontext->alloc_bytes -= (acontext->alloc_limit - acontext->alloc_ptr);
  6249|         total_alloc_bytes_soh -= (acontext->alloc_limit - acontext->alloc_ptr);
  6250|         acontext->alloc_ptr = 0;
  6251|         acontext->alloc_limit = acontext->alloc_ptr;
  6252|     }
  6253| }
  6254| void repair_allocation (gc_alloc_context* acontext, void*)
  6255| {
  6256|     uint8_t*  point = acontext->alloc_ptr;
  6257|     if (point != 0)
  6258|     {
  6259|         dprintf (3, ("Clearing [%zx, %zx[", (size_t)acontext->alloc_ptr,
  6260|                      (size_t)acontext->alloc_limit+Align(min_obj_size)));
  6261|         memclr (acontext->alloc_ptr - plug_skew,
  6262|                 (acontext->alloc_limit - acontext->alloc_ptr)+Align (min_obj_size));
  6263|     }
  6264| }
  6265| void void_allocation (gc_alloc_context* acontext, void*)
  6266| {
  6267|     uint8_t*  point = acontext->alloc_ptr;
  6268|     if (point != 0)
  6269|     {
  6270|         dprintf (3, ("Void [%zx, %zx[", (size_t)acontext->alloc_ptr,
  6271|                      (size_t)acontext->alloc_limit+Align(min_obj_size)));
  6272|         acontext->alloc_ptr = 0;
  6273|         acontext->alloc_limit = acontext->alloc_ptr;
  6274|     }
  6275| }
  6276| void gc_heap::repair_allocation_contexts (BOOL repair_p)
  6277| {
  6278|     GCToEEInterface::GcEnumAllocContexts (repair_p ? repair_allocation : void_allocation, NULL);
  6279| }
  6280| struct fix_alloc_context_args
  6281| {
  6282|     BOOL for_gc_p;
  6283|     void* heap;
  6284| };
  6285| void fix_alloc_context (gc_alloc_context* acontext, void* param)
  6286| {
  6287|     fix_alloc_context_args* args = (fix_alloc_context_args*)param;
  6288|     g_theGCHeap->FixAllocContext(acontext, (void*)(size_t)(args->for_gc_p), args->heap);
  6289| }
  6290| void gc_heap::fix_allocation_contexts (BOOL for_gc_p)
  6291| {
  6292|     fix_alloc_context_args args;
  6293|     args.for_gc_p = for_gc_p;
  6294|     args.heap = __this;
  6295|     GCToEEInterface::GcEnumAllocContexts(fix_alloc_context, &args);
  6296|     fix_youngest_allocation_area();
  6297| }
  6298| void gc_heap::fix_older_allocation_area (generation* older_gen)
  6299| {
  6300|     heap_segment* older_gen_seg = generation_allocation_segment (older_gen);
  6301|     if (generation_allocation_limit (older_gen) !=
  6302|         heap_segment_plan_allocated (older_gen_seg))
  6303|     {
  6304|         uint8_t*  point = generation_allocation_pointer (older_gen);
  6305|         size_t  size = (generation_allocation_limit (older_gen) -
  6306|                                generation_allocation_pointer (older_gen));
  6307|         if (size != 0)
  6308|         {
  6309|             assert ((size >= Align (min_obj_size)));
  6310|             dprintf(3,("Making unused area [%zx, %zx[", (size_t)point, (size_t)point+size));
  6311|             make_unused_array (point, size);
  6312|             if (size >= min_free_list)
  6313|             {
  6314|                 generation_allocator (older_gen)->thread_item_front (point, size);
  6315|                 add_gen_free (older_gen->gen_num, size);
  6316|                 generation_free_list_space (older_gen) += size;
  6317|             }
  6318|             else
  6319|             {
  6320|                 generation_free_obj_space (older_gen) += size;
  6321|             }
  6322|         }
  6323|     }
  6324|     else
  6325|     {
  6326|         assert (older_gen_seg != ephemeral_heap_segment);
  6327|         heap_segment_plan_allocated (older_gen_seg) =
  6328|             generation_allocation_pointer (older_gen);
  6329|         generation_allocation_limit (older_gen) =
  6330|             generation_allocation_pointer (older_gen);
  6331|     }
  6332|     generation_allocation_pointer (older_gen) = 0;
  6333|     generation_allocation_limit (older_gen) = 0;
  6334| }
  6335| #ifdef MULTIPLE_HEAPS
  6336| void gc_heap::fix_allocation_context_heaps (gc_alloc_context* gc_context, void*)
  6337| {
  6338|     alloc_context* acontext = (alloc_context*)gc_context;
  6339|     GCHeap* pHomeHeap = acontext->get_home_heap ();
  6340|     int home_hp_num = pHomeHeap ? pHomeHeap->pGenGCHeap->heap_number : 0;
  6341|     if (home_hp_num >= gc_heap::n_heaps)
  6342|     {
  6343|         home_hp_num %= gc_heap::n_heaps;
  6344|         acontext->set_home_heap (GCHeap::GetHeap (home_hp_num));
  6345|     }
  6346|     GCHeap* pAllocHeap = acontext->get_alloc_heap ();
  6347|     int alloc_hp_num = pAllocHeap ? pAllocHeap->pGenGCHeap->heap_number : 0;
  6348|     if (alloc_hp_num >= gc_heap::n_heaps)
  6349|     {
  6350|         alloc_hp_num %= gc_heap::n_heaps;
  6351|         acontext->set_alloc_heap (GCHeap::GetHeap (alloc_hp_num));
  6352|         gc_heap* hp = acontext->get_alloc_heap ()->pGenGCHeap;
  6353|         hp->alloc_context_count++;
  6354|     }
  6355| }
  6356| void gc_heap::fix_allocation_contexts_heaps()
  6357| {
  6358|     GCToEEInterface::GcEnumAllocContexts (fix_allocation_context_heaps, nullptr);
  6359| }
  6360| #endif //MULTIPLE_HEAPS
  6361| void gc_heap::set_allocation_heap_segment (generation* gen)
  6362| {
  6363| #ifdef USE_REGIONS
  6364|     heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
  6365|     dprintf (REGIONS_LOG, ("set gen%d alloc seg to start seg %p", gen->gen_num, heap_segment_mem (seg)));
  6366| #else
  6367|     uint8_t* p = generation_allocation_start (gen);
  6368|     assert (p);
  6369|     heap_segment* seg = generation_allocation_segment (gen);
  6370|     if (in_range_for_segment (p, seg))
  6371|         return;
  6372|     seg = ephemeral_heap_segment;
  6373|     if (!in_range_for_segment (p, seg))
  6374|     {
  6375|         seg = heap_segment_rw (generation_start_segment (gen));
  6376|         PREFIX_ASSUME(seg != NULL);
  6377|         while (!in_range_for_segment (p, seg))
  6378|         {
  6379|             seg = heap_segment_next_rw (seg);
  6380|             PREFIX_ASSUME(seg != NULL);
  6381|         }
  6382|     }
  6383| #endif //USE_REGIONS
  6384|     generation_allocation_segment (gen) = seg;
  6385| }
  6386| void gc_heap::reset_allocation_pointers (generation* gen, uint8_t* start)
  6387| {
  6388|     assert (start);
  6389|     assert (Align ((size_t)start) == (size_t)start);
  6390| #ifndef USE_REGIONS
  6391|     generation_allocation_start (gen) = start;
  6392| #endif //!USE_REGIONS
  6393|     generation_allocation_pointer (gen) =  0;//start + Align (min_obj_size);
  6394|     generation_allocation_limit (gen) = 0;//generation_allocation_pointer (gen);
  6395|     set_allocation_heap_segment (gen);
  6396| }
  6397| bool gc_heap::new_allocation_allowed (int gen_number)
  6398| {
  6399|     if (dd_new_allocation (dynamic_data_of (gen_number)) < 0)
  6400|     {
  6401|         if (gen_number != 0)
  6402|         {
  6403|             if (settings.concurrent)
  6404|             {
  6405|                 dynamic_data* dd2 = dynamic_data_of (gen_number);
  6406|                 if (dd_new_allocation (dd2) <= (ptrdiff_t)(-2 * dd_desired_allocation (dd2)))
  6407|                 {
  6408|                     return TRUE;
  6409|                 }
  6410|             }
  6411|         }
  6412|         return FALSE;
  6413|     }
  6414| #ifndef MULTIPLE_HEAPS
  6415|     else if ((settings.pause_mode != pause_no_gc) && (gen_number == 0))
  6416|     {
  6417|         dynamic_data* dd0 = dynamic_data_of (0);
  6418|         dprintf (3, ("evaluating, running amount %zd - new %zd = %zd",
  6419|             allocation_running_amount, dd_new_allocation (dd0),
  6420|             (allocation_running_amount - dd_new_allocation (dd0))));
  6421|         if ((allocation_running_amount - dd_new_allocation (dd0)) >
  6422|             dd_min_size (dd0))
  6423|         {
  6424|             uint64_t ctime = GCToOSInterface::GetLowPrecisionTimeStamp();
  6425|             if ((ctime - allocation_running_time) > 1000)
  6426|             {
  6427|                 dprintf (2, (">1s since last gen0 gc"));
  6428|                 return FALSE;
  6429|             }
  6430|             else
  6431|             {
  6432|                 allocation_running_amount = dd_new_allocation (dd0);
  6433|             }
  6434|         }
  6435|     }
  6436| #endif //MULTIPLE_HEAPS
  6437|     return TRUE;
  6438| }
  6439| inline
  6440| ptrdiff_t gc_heap::get_desired_allocation (int gen_number)
  6441| {
  6442|     return dd_desired_allocation (dynamic_data_of (gen_number));
  6443| }
  6444| inline
  6445| ptrdiff_t  gc_heap::get_new_allocation (int gen_number)
  6446| {
  6447|     return dd_new_allocation (dynamic_data_of (gen_number));
  6448| }
  6449| inline
  6450| ptrdiff_t  gc_heap::get_allocation (int gen_number)
  6451| {
  6452|     dynamic_data* dd = dynamic_data_of (gen_number);
  6453|     return dd_desired_allocation (dd) - dd_new_allocation (dd);
  6454| }
  6455| inline
  6456| BOOL grow_mark_stack (mark*& m, size_t& len, size_t init_len)
  6457| {
  6458|     size_t new_size = max (init_len, 2*len);
  6459|     mark* tmp = new (nothrow) mark [new_size];
  6460|     if (tmp)
  6461|     {
  6462|         memcpy (tmp, m, len * sizeof (mark));
  6463|         delete[] m;
  6464|         m = tmp;
  6465|         len = new_size;
  6466|         return TRUE;
  6467|     }
  6468|     else
  6469|     {
  6470|         dprintf (1, ("Failed to allocate %zd bytes for mark stack", (len * sizeof (mark))));
  6471|         return FALSE;
  6472|     }
  6473| }
  6474| inline
  6475| uint8_t* pinned_plug (mark* m)
  6476| {
  6477|    return m->first;
  6478| }
  6479| inline
  6480| size_t& pinned_len (mark* m)
  6481| {
  6482|     return m->len;
  6483| }
  6484| inline
  6485| void set_new_pin_info (mark* m, uint8_t* pin_free_space_start)
  6486| {
  6487|     m->len = pinned_plug (m) - pin_free_space_start;
  6488| #ifdef SHORT_PLUGS
  6489|     m->allocation_context_start_region = pin_free_space_start;
  6490| #endif //SHORT_PLUGS
  6491| }
  6492| #ifdef SHORT_PLUGS
  6493| inline
  6494| uint8_t*& pin_allocation_context_start_region (mark* m)
  6495| {
  6496|     return m->allocation_context_start_region;
  6497| }
  6498| uint8_t* get_plug_start_in_saved (uint8_t* old_loc, mark* pinned_plug_entry)
  6499| {
  6500|     uint8_t* saved_pre_plug_info = (uint8_t*)(pinned_plug_entry->get_pre_plug_reloc_info());
  6501|     uint8_t* plug_start_in_saved = saved_pre_plug_info + (old_loc - (pinned_plug (pinned_plug_entry) - sizeof (plug_and_gap)));
  6502|     dprintf (1, ("EP: %p(%p), %p", old_loc, pinned_plug (pinned_plug_entry), plug_start_in_saved));
  6503|     return plug_start_in_saved;
  6504| }
  6505| inline
  6506| void set_padding_in_expand (uint8_t* old_loc,
  6507|                             BOOL set_padding_on_saved_p,
  6508|                             mark* pinned_plug_entry)
  6509| {
  6510|     if (set_padding_on_saved_p)
  6511|     {
  6512|         set_plug_padded (get_plug_start_in_saved (old_loc, pinned_plug_entry));
  6513|     }
  6514|     else
  6515|     {
  6516|         set_plug_padded (old_loc);
  6517|     }
  6518| }
  6519| inline
  6520| void clear_padding_in_expand (uint8_t* old_loc,
  6521|                               BOOL set_padding_on_saved_p,
  6522|                               mark* pinned_plug_entry)
  6523| {
  6524|     if (set_padding_on_saved_p)
  6525|     {
  6526|         clear_plug_padded (get_plug_start_in_saved (old_loc, pinned_plug_entry));
  6527|     }
  6528|     else
  6529|     {
  6530|         clear_plug_padded (old_loc);
  6531|     }
  6532| }
  6533| #endif //SHORT_PLUGS
  6534| void gc_heap::reset_pinned_queue()
  6535| {
  6536|     mark_stack_tos = 0;
  6537|     mark_stack_bos = 0;
  6538| }
  6539| void gc_heap::reset_pinned_queue_bos()
  6540| {
  6541|     mark_stack_bos = 0;
  6542| }
  6543| void gc_heap::merge_with_last_pinned_plug (uint8_t* last_pinned_plug, size_t plug_size)
  6544| {
  6545|     if (last_pinned_plug)
  6546|     {
  6547|         mark& last_m = mark_stack_array[mark_stack_tos - 1];
  6548|         assert (last_pinned_plug == last_m.first);
  6549|         if (last_m.saved_post_p)
  6550|         {
  6551|             last_m.saved_post_p = FALSE;
  6552|             dprintf (3, ("setting last plug %p post to false", last_m.first));
  6553|             memcpy ((last_m.first + last_m.len - sizeof (plug_and_gap)), &(last_m.saved_post_plug), sizeof (gap_reloc_pair));
  6554|         }
  6555|         last_m.len += plug_size;
  6556|         dprintf (3, ("recovered the last part of plug %p, setting its plug size to %zx", last_m.first, last_m.len));
  6557|     }
  6558| }
  6559| void gc_heap::set_allocator_next_pin (generation* gen)
  6560| {
  6561|     dprintf (3, ("SANP: gen%d, ptr; %p, limit: %p", gen->gen_num, generation_allocation_pointer (gen), generation_allocation_limit (gen)));
  6562|     if (!(pinned_plug_que_empty_p()))
  6563|     {
  6564|         mark*  oldest_entry = oldest_pin();
  6565|         uint8_t* plug = pinned_plug (oldest_entry);
  6566|         if ((plug >= generation_allocation_pointer (gen)) &&
  6567|             (plug <  generation_allocation_limit (gen)))
  6568|         {
  6569| #ifdef USE_REGIONS
  6570|             assert (region_of (generation_allocation_pointer (gen)) ==
  6571|                     region_of (generation_allocation_limit (gen) - 1));
  6572| #endif //USE_REGIONS
  6573|             generation_allocation_limit (gen) = pinned_plug (oldest_entry);
  6574|             dprintf (3, ("SANP: get next pin free space in gen%d for alloc: %p->%p(%zd)",
  6575|                 gen->gen_num,
  6576|                 generation_allocation_pointer (gen), generation_allocation_limit (gen),
  6577|                 (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
  6578|         }
  6579|         else
  6580|             assert (!((plug < generation_allocation_pointer (gen)) &&
  6581|                       (plug >= heap_segment_mem (generation_allocation_segment (gen)))));
  6582|     }
  6583| }
  6584| void gc_heap::set_pinned_info (uint8_t* last_pinned_plug, size_t plug_len, generation* gen)
  6585| {
  6586| #ifndef _DEBUG
  6587|     UNREFERENCED_PARAMETER(last_pinned_plug);
  6588| #endif //_DEBUG
  6589|     mark& m = mark_stack_array[mark_stack_tos];
  6590|     assert (m.first == last_pinned_plug);
  6591|     m.len = plug_len;
  6592|     mark_stack_tos++;
  6593|     assert (gen != 0);
  6594|     if (gen != 0)
  6595|     {
  6596|         set_allocator_next_pin (gen);
  6597|     }
  6598| }
  6599| size_t gc_heap::deque_pinned_plug ()
  6600| {
  6601|     size_t m = mark_stack_bos;
  6602|     dprintf (3, ("deque: %zd->%p", mark_stack_bos, pinned_plug (pinned_plug_of (m))));
  6603|     mark_stack_bos++;
  6604|     return m;
  6605| }
  6606| inline
  6607| mark* gc_heap::pinned_plug_of (size_t bos)
  6608| {
  6609|     return &mark_stack_array [ bos ];
  6610| }
  6611| inline
  6612| mark* gc_heap::oldest_pin ()
  6613| {
  6614|     return pinned_plug_of (mark_stack_bos);
  6615| }
  6616| inline
  6617| BOOL gc_heap::pinned_plug_que_empty_p ()
  6618| {
  6619|     return (mark_stack_bos == mark_stack_tos);
  6620| }
  6621| inline
  6622| mark* gc_heap::before_oldest_pin()
  6623| {
  6624|     if (mark_stack_bos >= 1)
  6625|         return pinned_plug_of (mark_stack_bos-1);
  6626|     else
  6627|         return 0;
  6628| }
  6629| inline
  6630| BOOL gc_heap::ephemeral_pointer_p (uint8_t* o)
  6631| {
  6632| #ifdef USE_REGIONS
  6633|     int gen_num = object_gennum ((uint8_t*)o);
  6634|     assert (gen_num >= 0);
  6635|     return (gen_num < max_generation);
  6636| #else
  6637|     return ((o >= ephemeral_low) && (o < ephemeral_high));
  6638| #endif //USE_REGIONS
  6639| }
  6640| inline
  6641| bool gc_heap::is_in_find_object_range (uint8_t* o)
  6642| {
  6643|     if (o == nullptr)
  6644|     {
  6645|         return false;
  6646|     }
  6647| #if defined(USE_REGIONS) && defined(FEATURE_CONSERVATIVE_GC)
  6648|     return ((o >= g_gc_lowest_address) && (o < bookkeeping_covered_committed));
  6649| #else //USE_REGIONS && FEATURE_CONSERVATIVE_GC
  6650|     if ((o >= g_gc_lowest_address) && (o < g_gc_highest_address))
  6651|     {
  6652| #ifdef USE_REGIONS
  6653|         assert ((o >= g_gc_lowest_address) && (o < bookkeeping_covered_committed));
  6654| #endif //USE_REGIONS
  6655|         return true;
  6656|     }
  6657|     else
  6658|     {
  6659|         return false;
  6660|     }
  6661| #endif //USE_REGIONS && FEATURE_CONSERVATIVE_GC
  6662| }
  6663| #ifdef USE_REGIONS
  6664| inline
  6665| bool gc_heap::is_in_condemned_gc (uint8_t* o)
  6666| {
  6667|     assert ((o >= g_gc_lowest_address) && (o < g_gc_highest_address));
  6668|     int condemned_gen = settings.condemned_generation;
  6669|     if (condemned_gen < max_generation)
  6670|     {
  6671|         int gen = get_region_gen_num (o);
  6672|         if (gen > condemned_gen)
  6673|         {
  6674|             return false;
  6675|         }
  6676|     }
  6677|     return true;
  6678| }
  6679| inline
  6680| bool gc_heap::should_check_brick_for_reloc (uint8_t* o)
  6681| {
  6682|     assert ((o >= g_gc_lowest_address) && (o < g_gc_highest_address));
  6683|     size_t skewed_basic_region_index = get_skewed_basic_region_index_for_address (o);
  6684|     return (map_region_to_generation_skewed[skewed_basic_region_index] & (RI_SIP|RI_GEN_MASK)) <= settings.condemned_generation;
  6685| }
  6686| #endif //USE_REGIONS
  6687| #ifdef MH_SC_MARK
  6688| inline
  6689| int& gc_heap::mark_stack_busy()
  6690| {
  6691|     return  g_mark_stack_busy [(heap_number+2)*HS_CACHE_LINE_SIZE/sizeof(int)];
  6692| }
  6693| #endif //MH_SC_MARK
  6694| void gc_heap::make_mark_stack (mark* arr)
  6695| {
  6696|     reset_pinned_queue();
  6697|     mark_stack_array = arr;
  6698|     mark_stack_array_length = MARK_STACK_INITIAL_LENGTH;
  6699| #ifdef MH_SC_MARK
  6700|     mark_stack_busy() = 0;
  6701| #endif //MH_SC_MARK
  6702| }
  6703| #ifdef BACKGROUND_GC
  6704| inline
  6705| size_t& gc_heap::bpromoted_bytes(int thread)
  6706| {
  6707| #ifdef MULTIPLE_HEAPS
  6708|     return g_bpromoted [thread*16];
  6709| #else //MULTIPLE_HEAPS
  6710|     UNREFERENCED_PARAMETER(thread);
  6711|     return g_bpromoted;
  6712| #endif //MULTIPLE_HEAPS
  6713| }
  6714| void gc_heap::make_background_mark_stack (uint8_t** arr)
  6715| {
  6716|     background_mark_stack_array = arr;
  6717|     background_mark_stack_array_length = MARK_STACK_INITIAL_LENGTH;
  6718|     background_mark_stack_tos = arr;
  6719| }
  6720| void gc_heap::make_c_mark_list (uint8_t** arr)
  6721| {
  6722|     c_mark_list = arr;
  6723|     c_mark_list_index = 0;
  6724|     c_mark_list_length = 1 + (OS_PAGE_SIZE / MIN_OBJECT_SIZE);
  6725| }
  6726| #endif //BACKGROUND_GC
  6727| #ifdef CARD_BUNDLE
  6728| static const size_t card_bundle_word_width = 32;
  6729| static const size_t card_bundle_size = (size_t)(GC_PAGE_SIZE / (sizeof(uint32_t)*card_bundle_word_width));
  6730| inline
  6731| size_t card_bundle_word (size_t cardb)
  6732| {
  6733|     return cardb / card_bundle_word_width;
  6734| }
  6735| inline
  6736| uint32_t card_bundle_bit (size_t cardb)
  6737| {
  6738|     return (uint32_t)(cardb % card_bundle_word_width);
  6739| }
  6740| size_t align_cardw_on_bundle (size_t cardw)
  6741| {
  6742|     return ((size_t)(cardw + card_bundle_size - 1) & ~(card_bundle_size - 1 ));
  6743| }
  6744| size_t cardw_card_bundle (size_t cardw)
  6745| {
  6746|     return cardw / card_bundle_size;
  6747| }
  6748| size_t card_bundle_cardw (size_t cardb)
  6749| {
  6750|     return cardb * card_bundle_size;
  6751| }
  6752| void gc_heap::card_bundle_clear (size_t cardb)
  6753| {
  6754|     uint32_t bit = (uint32_t)(1 << card_bundle_bit (cardb));
  6755|     uint32_t* bundle = &card_bundle_table[card_bundle_word (cardb)];
  6756| #ifdef MULTIPLE_HEAPS
  6757|     if ((*bundle & bit) != 0)
  6758|     {
  6759|         Interlocked::And (bundle, ~bit);
  6760|     }
  6761| #else
  6762|     *bundle &= ~bit;
  6763| #endif
  6764|     assert ((*bundle & bit) == 0);
  6765|     dprintf (2, ("Cleared card bundle %zx [%zx, %zx[", cardb, (size_t)card_bundle_cardw (cardb),
  6766|               (size_t)card_bundle_cardw (cardb+1)));
  6767| }
  6768| inline void set_bundle_bits (uint32_t* bundle, uint32_t bits)
  6769| {
  6770| #ifdef MULTIPLE_HEAPS
  6771|     if ((*bundle & bits) != bits)
  6772|     {
  6773|         Interlocked::Or (bundle, bits);
  6774|     }
  6775| #else
  6776|     *bundle |= bits;
  6777| #endif
  6778|     assert ((*bundle & bits) == bits);
  6779| }
  6780| void gc_heap::card_bundle_set (size_t cardb)
  6781| {
  6782|     uint32_t bits = (1 << card_bundle_bit (cardb));
  6783|     set_bundle_bits (&card_bundle_table [card_bundle_word (cardb)], bits);
  6784| }
  6785| void gc_heap::card_bundles_set (size_t start_cardb, size_t end_cardb)
  6786| {
  6787|     if (start_cardb == end_cardb)
  6788|     {
  6789|         card_bundle_set(start_cardb);
  6790|         return;
  6791|     }
  6792|     size_t start_word = card_bundle_word (start_cardb);
  6793|     size_t end_word = card_bundle_word (end_cardb);
  6794|     if (start_word < end_word)
  6795|     {
  6796|         uint32_t bits = highbits (~0u, card_bundle_bit (start_cardb));
  6797|         set_bundle_bits (&card_bundle_table [start_word], bits);
  6798|         if (card_bundle_bit (end_cardb))
  6799|         {
  6800|             bits = lowbits (~0u, card_bundle_bit (end_cardb));
  6801|             set_bundle_bits (&card_bundle_table [end_word], bits);
  6802|         }
  6803|         for (size_t i = start_word + 1; i < end_word; i++)
  6804|         {
  6805|             card_bundle_table [i] = ~0u;
  6806|         }
  6807|     }
  6808|     else
  6809|     {
  6810|         uint32_t bits = (highbits (~0u, card_bundle_bit (start_cardb)) &
  6811|                           lowbits (~0u, card_bundle_bit (end_cardb)));
  6812|         set_bundle_bits (&card_bundle_table [start_word], bits);
  6813|     }
  6814| }
  6815| BOOL gc_heap::card_bundle_set_p (size_t cardb)
  6816| {
  6817|     return (card_bundle_table[card_bundle_word(cardb)] & (1 << card_bundle_bit (cardb)));
  6818| }
  6819| size_t size_card_bundle_of (uint8_t* from, uint8_t* end)
  6820| {
  6821|     size_t cbw_span = card_size * card_word_width * card_bundle_size * card_bundle_word_width;
  6822|     from = (uint8_t*)((size_t)from & ~(cbw_span - 1));
  6823|     end = (uint8_t*)((size_t)(end + (cbw_span - 1)) & ~(cbw_span - 1));
  6824|     assert (((size_t)from & (cbw_span - 1)) == 0);
  6825|     assert (((size_t)end  & (cbw_span - 1)) == 0);
  6826|     return ((end - from) / cbw_span) * sizeof (uint32_t);
  6827| }
  6828| uint32_t* translate_card_bundle_table (uint32_t* cb, uint8_t* lowest_address)
  6829| {
  6830|     const size_t heap_bytes_for_bundle_word = card_size * card_word_width * card_bundle_size * card_bundle_word_width;
  6831|     return (uint32_t*)((uint8_t*)cb - (((size_t)lowest_address / heap_bytes_for_bundle_word) * sizeof (uint32_t)));
  6832| }
  6833| void gc_heap::enable_card_bundles ()
  6834| {
  6835|     if (can_use_write_watch_for_card_table() && (!card_bundles_enabled()))
  6836|     {
  6837|         dprintf (1, ("Enabling card bundles"));
  6838|         card_bundles_set (cardw_card_bundle (card_word (card_of (lowest_address))),
  6839|                           cardw_card_bundle (align_cardw_on_bundle (card_word (card_of (highest_address)))));
  6840|         settings.card_bundles = TRUE;
  6841|     }
  6842| }
  6843| BOOL gc_heap::card_bundles_enabled ()
  6844| {
  6845|     return settings.card_bundles;
  6846| }
  6847| #endif // CARD_BUNDLE
  6848| #if defined (HOST_64BIT)
  6849| #define brick_size ((size_t)4096)
  6850| #else
  6851| #define brick_size ((size_t)2048)
  6852| #endif //HOST_64BIT
  6853| inline
  6854| size_t gc_heap::brick_of (uint8_t* add)
  6855| {
  6856|     return (size_t)(add - lowest_address) / brick_size;
  6857| }
  6858| inline
  6859| uint8_t* gc_heap::brick_address (size_t brick)
  6860| {
  6861|     return lowest_address + (brick_size * brick);
  6862| }
  6863| void gc_heap::clear_brick_table (uint8_t* from, uint8_t* end)
  6864| {
  6865|     size_t from_brick = brick_of (from);
  6866|     size_t end_brick = brick_of (end);
  6867|     memset (&brick_table[from_brick], 0, sizeof(brick_table[from_brick])*(end_brick-from_brick));
  6868| }
  6869| inline
  6870| void gc_heap::set_brick (size_t index, ptrdiff_t val)
  6871| {
  6872|     if (val < -32767)
  6873|     {
  6874|         val = -32767;
  6875|     }
  6876|     assert (val < 32767);
  6877|     if (val >= 0)
  6878|         brick_table [index] = (short)val+1;
  6879|     else
  6880|         brick_table [index] = (short)val;
  6881|     dprintf (3, ("set brick[%zx] to %d\n", index, (short)val));
  6882| }
  6883| inline
  6884| int gc_heap::get_brick_entry (size_t index)
  6885| {
  6886| #ifdef MULTIPLE_HEAPS
  6887|     return VolatileLoadWithoutBarrier(&brick_table [index]);
  6888| #else
  6889|     return brick_table[index];
  6890| #endif
  6891| }
  6892| inline
  6893| uint8_t* align_on_brick (uint8_t* add)
  6894| {
  6895|     return (uint8_t*)((size_t)(add + brick_size - 1) & ~(brick_size - 1));
  6896| }
  6897| inline
  6898| uint8_t* align_lower_brick (uint8_t* add)
  6899| {
  6900|     return (uint8_t*)(((size_t)add) & ~(brick_size - 1));
  6901| }
  6902| size_t size_brick_of (uint8_t* from, uint8_t* end)
  6903| {
  6904|     assert (((size_t)from & (brick_size-1)) == 0);
  6905|     assert (((size_t)end  & (brick_size-1)) == 0);
  6906|     return ((end - from) / brick_size) * sizeof (short);
  6907| }
  6908| inline
  6909| uint8_t* gc_heap::card_address (size_t card)
  6910| {
  6911|     return  (uint8_t*) (card_size * card);
  6912| }
  6913| inline
  6914| size_t gc_heap::card_of ( uint8_t* object)
  6915| {
  6916|     return (size_t)(object) / card_size;
  6917| }
  6918| inline
  6919| uint8_t* align_on_card (uint8_t* add)
  6920| {
  6921|     return (uint8_t*)((size_t)(add + card_size - 1) & ~(card_size - 1 ));
  6922| }
  6923| inline
  6924| uint8_t* align_on_card_word (uint8_t* add)
  6925| {
  6926|     return (uint8_t*) ((size_t)(add + (card_size*card_word_width)-1) & ~(card_size*card_word_width - 1));
  6927| }
  6928| inline
  6929| uint8_t* align_lower_card (uint8_t* add)
  6930| {
  6931|     return (uint8_t*)((size_t)add & ~(card_size-1));
  6932| }
  6933| inline
  6934| void gc_heap::clear_card (size_t card)
  6935| {
  6936|     card_table [card_word (card)] =
  6937|         (card_table [card_word (card)] & ~(1 << card_bit (card)));
  6938|     dprintf (3,("Cleared card %zx [%zx, %zx[", card, (size_t)card_address (card),
  6939|               (size_t)card_address (card+1)));
  6940| }
  6941| inline
  6942| void gc_heap::set_card (size_t card)
  6943| {
  6944|     size_t word = card_word (card);
  6945|     card_table[word] = (card_table [word] | (1 << card_bit (card)));
  6946| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  6947|     size_t bundle_to_set = cardw_card_bundle(word);
  6948|     card_bundle_set(bundle_to_set);
  6949|     dprintf (3,("Set card %zx [%zx, %zx[ and bundle %zx", card, (size_t)card_address (card), (size_t)card_address (card+1), bundle_to_set));
  6950| #endif
  6951| }
  6952| inline
  6953| BOOL  gc_heap::card_set_p (size_t card)
  6954| {
  6955|     return ( card_table [ card_word (card) ] & (1 << card_bit (card)));
  6956| }
  6957| size_t count_card_of (uint8_t* from, uint8_t* end)
  6958| {
  6959|     return card_word (gcard_of (end - 1)) - card_word (gcard_of (from)) + 1;
  6960| }
  6961| size_t size_card_of (uint8_t* from, uint8_t* end)
  6962| {
  6963|     return count_card_of (from, end) * sizeof(uint32_t);
  6964| }
  6965| class card_table_info
  6966| {
  6967| public:
  6968|     unsigned    recount;
  6969|     size_t      size;
  6970|     uint32_t*   next_card_table;
  6971|     uint8_t*    lowest_address;
  6972|     uint8_t*    highest_address;
  6973|     short*      brick_table;
  6974| #ifdef CARD_BUNDLE
  6975|     uint32_t*   card_bundle_table;
  6976| #endif //CARD_BUNDLE
  6977| #ifdef BACKGROUND_GC
  6978|     uint32_t*   mark_array;
  6979| #endif //BACKGROUND_GC
  6980| };
  6981| static_assert(offsetof(dac_card_table_info, size) == offsetof(card_table_info, size), "DAC card_table_info layout mismatch");
  6982| static_assert(offsetof(dac_card_table_info, next_card_table) == offsetof(card_table_info, next_card_table), "DAC card_table_info layout mismatch");
  6983| inline
  6984| unsigned& card_table_refcount (uint32_t* c_table)
  6985| {
  6986|     return *(unsigned*)((char*)c_table - sizeof (card_table_info));
  6987| }
  6988| inline
  6989| uint8_t*& card_table_lowest_address (uint32_t* c_table)
  6990| {
  6991|     return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->lowest_address;
  6992| }
  6993| uint32_t* translate_card_table (uint32_t* ct)
  6994| {
  6995|     return (uint32_t*)((uint8_t*)ct - card_word (gcard_of (card_table_lowest_address (ct))) * sizeof(uint32_t));
  6996| }
  6997| inline
  6998| uint8_t*& card_table_highest_address (uint32_t* c_table)
  6999| {
  7000|     return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->highest_address;
  7001| }
  7002| inline
  7003| short*& card_table_brick_table (uint32_t* c_table)
  7004| {
  7005|     return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->brick_table;
  7006| }
  7007| #ifdef CARD_BUNDLE
  7008| inline
  7009| uint32_t*& card_table_card_bundle_table (uint32_t* c_table)
  7010| {
  7011|     return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->card_bundle_table;
  7012| }
  7013| #endif //CARD_BUNDLE
  7014| #ifdef BACKGROUND_GC
  7015| inline
  7016| uint32_t*& card_table_mark_array (uint32_t* c_table)
  7017| {
  7018|     return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->mark_array;
  7019| }
  7020| #ifdef HOST_64BIT
  7021| #define mark_bit_pitch ((size_t)16)
  7022| #else
  7023| #define mark_bit_pitch ((size_t)8)
  7024| #endif // HOST_64BIT
  7025| #define mark_word_width ((size_t)32)
  7026| #define mark_word_size (mark_word_width * mark_bit_pitch)
  7027| inline
  7028| uint8_t* align_on_mark_bit (uint8_t* add)
  7029| {
  7030|     return (uint8_t*)((size_t)(add + (mark_bit_pitch - 1)) & ~(mark_bit_pitch - 1));
  7031| }
  7032| inline
  7033| uint8_t* align_lower_mark_bit (uint8_t* add)
  7034| {
  7035|     return (uint8_t*)((size_t)(add) & ~(mark_bit_pitch - 1));
  7036| }
  7037| inline
  7038| BOOL is_aligned_on_mark_word (uint8_t* add)
  7039| {
  7040|     return ((size_t)add == ((size_t)(add) & ~(mark_word_size - 1)));
  7041| }
  7042| inline
  7043| uint8_t* align_on_mark_word (uint8_t* add)
  7044| {
  7045|     return (uint8_t*)((size_t)(add + mark_word_size - 1) & ~(mark_word_size - 1));
  7046| }
  7047| inline
  7048| uint8_t* align_lower_mark_word (uint8_t* add)
  7049| {
  7050|     return (uint8_t*)((size_t)(add) & ~(mark_word_size - 1));
  7051| }
  7052| inline
  7053| size_t mark_bit_of (uint8_t* add)
  7054| {
  7055|     return ((size_t)add / mark_bit_pitch);
  7056| }
  7057| inline
  7058| unsigned int mark_bit_bit (size_t mark_bit)
  7059| {
  7060|     return (unsigned int)(mark_bit % mark_word_width);
  7061| }
  7062| inline
  7063| size_t mark_bit_word (size_t mark_bit)
  7064| {
  7065|     return (mark_bit / mark_word_width);
  7066| }
  7067| inline
  7068| size_t mark_word_of (uint8_t* add)
  7069| {
  7070|     return ((size_t)add) / mark_word_size;
  7071| }
  7072| uint8_t* mark_word_address (size_t wd)
  7073| {
  7074|     return (uint8_t*)(wd*mark_word_size);
  7075| }
  7076| uint8_t* mark_bit_address (size_t mark_bit)
  7077| {
  7078|     return (uint8_t*)(mark_bit*mark_bit_pitch);
  7079| }
  7080| inline
  7081| size_t mark_bit_bit_of (uint8_t* add)
  7082| {
  7083|     return  (((size_t)add / mark_bit_pitch) % mark_word_width);
  7084| }
  7085| inline
  7086| unsigned int gc_heap::mark_array_marked(uint8_t* add)
  7087| {
  7088|     return mark_array [mark_word_of (add)] & (1 << mark_bit_bit_of (add));
  7089| }
  7090| inline
  7091| BOOL gc_heap::is_mark_bit_set (uint8_t* add)
  7092| {
  7093|     return (mark_array [mark_word_of (add)] & (1 << mark_bit_bit_of (add)));
  7094| }
  7095| inline
  7096| void gc_heap::mark_array_set_marked (uint8_t* add)
  7097| {
  7098|     size_t index = mark_word_of (add);
  7099|     uint32_t val = (1 << mark_bit_bit_of (add));
  7100| #ifdef MULTIPLE_HEAPS
  7101|     Interlocked::Or (&(mark_array [index]), val);
  7102| #else
  7103|     mark_array [index] |= val;
  7104| #endif
  7105| }
  7106| inline
  7107| void gc_heap::mark_array_clear_marked (uint8_t* add)
  7108| {
  7109|     mark_array [mark_word_of (add)] &= ~(1 << mark_bit_bit_of (add));
  7110| }
  7111| size_t size_mark_array_of (uint8_t* from, uint8_t* end)
  7112| {
  7113|     assert (((size_t)from & ((mark_word_size)-1)) == 0);
  7114|     assert (((size_t)end  & ((mark_word_size)-1)) == 0);
  7115|     return sizeof (uint32_t)*(((end - from) / mark_word_size));
  7116| }
  7117| uint32_t* translate_mark_array (uint32_t* ma)
  7118| {
  7119|     return (uint32_t*)((uint8_t*)ma - size_mark_array_of (0, g_gc_lowest_address));
  7120| }
  7121| #ifdef FEATURE_BASICFREEZE
  7122| void gc_heap::clear_mark_array (uint8_t* from, uint8_t* end)
  7123| {
  7124|     assert (gc_can_use_concurrent);
  7125|     assert (end == align_on_mark_word (end));
  7126|     uint8_t* current_lowest_address = background_saved_lowest_address;
  7127|     uint8_t* current_highest_address = background_saved_highest_address;
  7128|     if ((end <= current_highest_address) && (from >= current_lowest_address))
  7129|     {
  7130|         size_t beg_word = mark_word_of (align_on_mark_word (from));
  7131|         size_t end_word = mark_word_of (align_on_mark_word (end));
  7132|         dprintf (3, ("Calling clearing mark array [%zx, %zx[ for addresses [%zx, %zx[",
  7133|                      (size_t)mark_word_address (beg_word),
  7134|                      (size_t)mark_word_address (end_word),
  7135|                      (size_t)from, (size_t)end));
  7136|         uint8_t* op = from;
  7137|         while (op < mark_word_address (beg_word))
  7138|         {
  7139|             mark_array_clear_marked (op);
  7140|             op += mark_bit_pitch;
  7141|         }
  7142|         memset (&mark_array[beg_word], 0, (end_word - beg_word)*sizeof (uint32_t));
  7143| #ifdef _DEBUG
  7144|         size_t  markw = mark_word_of (align_on_mark_word (from));
  7145|         size_t  markw_end = mark_word_of (align_on_mark_word (end));
  7146|         while (markw < markw_end)
  7147|         {
  7148|             assert (!(mark_array [markw]));
  7149|             markw++;
  7150|         }
  7151|         uint8_t* p = mark_word_address (markw_end);
  7152|         while (p < end)
  7153|         {
  7154|             assert (!(mark_array_marked (p)));
  7155|             p++;
  7156|         }
  7157| #endif //_DEBUG
  7158|     }
  7159| }
  7160| #endif // FEATURE_BASICFREEZE
  7161| #endif //BACKGROUND_GC
  7162| inline
  7163| uint32_t*& card_table_next (uint32_t* c_table)
  7164| {
  7165|     return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->next_card_table;
  7166| }
  7167| inline
  7168| size_t& card_table_size (uint32_t* c_table)
  7169| {
  7170|     return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->size;
  7171| }
  7172| void own_card_table (uint32_t* c_table)
  7173| {
  7174|     card_table_refcount (c_table) += 1;
  7175| }
  7176| void destroy_card_table (uint32_t* c_table);
  7177| void delete_next_card_table (uint32_t* c_table)
  7178| {
  7179|     uint32_t* n_table = card_table_next (c_table);
  7180|     if (n_table)
  7181|     {
  7182|         if (card_table_next (n_table))
  7183|         {
  7184|             delete_next_card_table (n_table);
  7185|         }
  7186|         if (card_table_refcount (n_table) == 0)
  7187|         {
  7188|             destroy_card_table (n_table);
  7189|             card_table_next (c_table) = 0;
  7190|         }
  7191|     }
  7192| }
  7193| void release_card_table (uint32_t* c_table)
  7194| {
  7195|     assert (card_table_refcount (c_table) >0);
  7196|     card_table_refcount (c_table) -= 1;
  7197|     if (card_table_refcount (c_table) == 0)
  7198|     {
  7199|         delete_next_card_table (c_table);
  7200|         if (card_table_next (c_table) == 0)
  7201|         {
  7202|             destroy_card_table (c_table);
  7203|             if (&g_gc_card_table[card_word (gcard_of(g_gc_lowest_address))] == c_table)
  7204|             {
  7205|                 g_gc_card_table = 0;
  7206| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  7207|                 g_gc_card_bundle_table = 0;
  7208| #endif
  7209| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  7210|                 SoftwareWriteWatch::StaticClose();
  7211| #endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  7212|             }
  7213|             else
  7214|             {
  7215|                 uint32_t* p_table = &g_gc_card_table[card_word (gcard_of(g_gc_lowest_address))];
  7216|                 if (p_table)
  7217|                 {
  7218|                     while (p_table && (card_table_next (p_table) != c_table))
  7219|                         p_table = card_table_next (p_table);
  7220|                     card_table_next (p_table) = 0;
  7221|                 }
  7222|             }
  7223|         }
  7224|     }
  7225| }
  7226| void destroy_card_table (uint32_t* c_table)
  7227| {
  7228|     GCToOSInterface::VirtualRelease (&card_table_refcount(c_table), card_table_size(c_table));
  7229|     dprintf (2, ("Table Virtual Free : %zx", (size_t)&card_table_refcount(c_table)));
  7230| }
  7231| void gc_heap::get_card_table_element_sizes (uint8_t* start, uint8_t* end, size_t sizes[total_bookkeeping_elements])
  7232| {
  7233|     memset (sizes, 0, sizeof(size_t) * total_bookkeeping_elements);
  7234|     sizes[card_table_element] = size_card_of (start, end);
  7235|     sizes[brick_table_element] = size_brick_of (start, end);
  7236| #ifdef CARD_BUNDLE
  7237|     if (can_use_write_watch_for_card_table())
  7238|     {
  7239|         sizes[card_bundle_table_element] = size_card_bundle_of (start, end);
  7240|     }
  7241| #endif //CARD_BUNDLE
  7242| #if defined(FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP) && defined (BACKGROUND_GC)
  7243|     if (gc_can_use_concurrent)
  7244|     {
  7245|         sizes[software_write_watch_table_element] = SoftwareWriteWatch::GetTableByteSize(start, end);
  7246|     }
  7247| #endif //FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP && BACKGROUND_GC
  7248| #ifdef USE_REGIONS
  7249|     sizes[region_to_generation_table_element] = size_region_to_generation_table_of (start, end);
  7250| #endif //USE_REGIONS
  7251|     sizes[seg_mapping_table_element] = size_seg_mapping_table_of (start, end);
  7252| #ifdef BACKGROUND_GC
  7253|     if (gc_can_use_concurrent)
  7254|     {
  7255|         sizes[mark_array_element] = size_mark_array_of (start, end);
  7256|     }
  7257| #endif //BACKGROUND_GC
  7258| }
  7259| void gc_heap::get_card_table_element_layout (uint8_t* start, uint8_t* end, size_t layout[total_bookkeeping_elements + 1])
  7260| {
  7261|     size_t sizes[total_bookkeeping_elements];
  7262|     get_card_table_element_sizes(start, end, sizes);
  7263|     const size_t alignment[total_bookkeeping_elements + 1] =
  7264|     {
  7265|         sizeof (uint32_t), // card_table_element
  7266|         sizeof (short),    // brick_table_element
  7267| #ifdef CARD_BUNDLE
  7268|         sizeof (uint32_t), // card_bundle_table_element
  7269| #endif //CARD_BUNDLE
  7270| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  7271|         sizeof(size_t),    // software_write_watch_table_element
  7272| #endif //FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
  7273| #ifdef USE_REGIONS
  7274|         sizeof (uint8_t),  // region_to_generation_table_element
  7275| #endif //USE_REGIONS
  7276|         sizeof (uint8_t*), // seg_mapping_table_element
  7277| #ifdef BACKGROUND_GC
  7278|         OS_PAGE_SIZE,      // mark_array_element
  7279| #endif //BACKGROUND_GC
  7280|         OS_PAGE_SIZE       // total_bookkeeping_elements
  7281|     };
  7282|     layout[card_table_element] = ALIGN_UP(sizeof(card_table_info), alignment[card_table_element]);
  7283|     for (int element = brick_table_element; element <= total_bookkeeping_elements; element++)
  7284|     {
  7285|         layout[element] = layout[element - 1] + sizes[element - 1];
  7286|         if ((element != total_bookkeeping_elements) && (sizes[element] != 0))
  7287|         {
  7288|             layout[element] = ALIGN_UP(layout[element], alignment[element]);
  7289|         }
  7290|     }
  7291| }
  7292| #ifdef USE_REGIONS
  7293| bool gc_heap::on_used_changed (uint8_t* new_used)
  7294| {
  7295| #if defined(WRITE_BARRIER_CHECK) && !defined (SERVER_GC)
  7296|     if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_BARRIERCHECK)
  7297|     {
  7298|         size_t shadow_covered = g_GCShadowEnd - g_GCShadow;
  7299|         size_t used_heap_range = new_used - g_gc_lowest_address;
  7300|         if (used_heap_range > shadow_covered)
  7301|         {
  7302|             size_t extra = used_heap_range - shadow_covered;
  7303|             if (!GCToOSInterface::VirtualCommit (g_GCShadowEnd, extra))
  7304|             {
  7305|                 _ASSERTE(!"Not enough memory to run HeapVerify level 2");
  7306|                 deleteGCShadow();
  7307|             }
  7308|             else
  7309|             {
  7310|                 g_GCShadowEnd += extra;
  7311|             }
  7312|         }
  7313|     }
  7314| #endif //WRITE_BARRIER_CHECK && !SERVER_GC
  7315|     if (new_used > bookkeeping_covered_committed)
  7316|     {
  7317|         bool speculative_commit_tried = false;
  7318| #ifdef STRESS_REGIONS
  7319|         if (gc_rand::get_rand(10) > 3)
  7320|         {
  7321|             dprintf (REGIONS_LOG, ("skipping speculative commit under stress regions"));
  7322|             speculative_commit_tried = true;
  7323|         }
  7324| #endif
  7325|         while (true)
  7326|         {
  7327|             uint8_t* new_bookkeeping_covered_committed = nullptr;
  7328|             if (speculative_commit_tried)
  7329|             {
  7330|                 new_bookkeeping_covered_committed = new_used;
  7331|             }
  7332|             else
  7333|             {
  7334|                 uint64_t committed_size = (uint64_t)(bookkeeping_covered_committed - g_gc_lowest_address);
  7335|                 uint64_t total_size = (uint64_t)(g_gc_highest_address - g_gc_lowest_address);
  7336|                 assert (committed_size <= total_size);
  7337|                 assert (committed_size < (UINT64_MAX / 2));
  7338|                 uint64_t new_committed_size = min(committed_size * 2, total_size);
  7339|                 assert ((UINT64_MAX - new_committed_size) > (uint64_t)g_gc_lowest_address);
  7340|                 uint8_t* double_commit = g_gc_lowest_address + new_committed_size;
  7341|                 new_bookkeeping_covered_committed = max(double_commit, new_used);
  7342|                 dprintf (REGIONS_LOG, ("committed_size                           = %zd", committed_size));
  7343|                 dprintf (REGIONS_LOG, ("total_size                               = %zd", total_size));
  7344|                 dprintf (REGIONS_LOG, ("new_committed_size                       = %zd", new_committed_size));
  7345|                 dprintf (REGIONS_LOG, ("double_commit                            = %p", double_commit));
  7346|             }
  7347|             dprintf (REGIONS_LOG, ("bookkeeping_covered_committed     = %p", bookkeeping_covered_committed));
  7348|             dprintf (REGIONS_LOG, ("new_bookkeeping_covered_committed = %p", new_bookkeeping_covered_committed));
  7349|             if (inplace_commit_card_table (bookkeeping_covered_committed, new_bookkeeping_covered_committed))
  7350|             {
  7351|                 bookkeeping_covered_committed = new_bookkeeping_covered_committed;
  7352|                 break;
  7353|             }
  7354|             else
  7355|             {
  7356|                 if (new_bookkeeping_covered_committed == new_used)
  7357|                 {
  7358|                     dprintf (REGIONS_LOG, ("The minimal commit for the GC bookkeeping data structure failed, giving up"));
  7359|                     return false;
  7360|                 }
  7361|                 dprintf (REGIONS_LOG, ("The speculative commit for the GC bookkeeping data structure failed, retry for minimal commit"));
  7362|                 speculative_commit_tried = true;
  7363|             }
  7364|         }
  7365|     }
  7366|     return true;
  7367| }
  7368| bool gc_heap::get_card_table_commit_layout (uint8_t* from, uint8_t* to,
  7369|                     uint8_t* commit_begins[total_bookkeeping_elements],
  7370|                     size_t commit_sizes[total_bookkeeping_elements],
  7371|                     size_t new_sizes[total_bookkeeping_elements])
  7372| {
  7373|     uint8_t* start = g_gc_lowest_address;
  7374|     uint8_t* end = g_gc_highest_address;
  7375|     bool initial_commit = (from == start);
  7376|     bool additional_commit = !initial_commit && (to > from);
  7377|     if (!initial_commit && !additional_commit)
  7378|     {
  7379|         return false;
  7380|     }
  7381| #ifdef DEBUG
  7382|     size_t offsets[total_bookkeeping_elements + 1];
  7383|     get_card_table_element_layout(start, end, offsets);
  7384|     dprintf (REGIONS_LOG, ("layout"));
  7385|     for (int i = card_table_element; i <= total_bookkeeping_elements; i++)
  7386|     {
  7387|         assert (offsets[i] == card_table_element_layout[i]);
  7388|         dprintf (REGIONS_LOG, ("%zd", card_table_element_layout[i]));
  7389|     }
  7390| #endif //DEBUG
  7391|     get_card_table_element_sizes (start, to, new_sizes);
  7392| #ifdef DEBUG
  7393|     dprintf (REGIONS_LOG, ("new_sizes"));
  7394|     for (int i = card_table_element; i < total_bookkeeping_elements; i++)
  7395|     {
  7396|         dprintf (REGIONS_LOG, ("%zd", new_sizes[i]));
  7397|     }
  7398|     if (additional_commit)
  7399|     {
  7400|         size_t current_sizes[total_bookkeeping_elements];
  7401|         get_card_table_element_sizes (start, from, current_sizes);
  7402|         dprintf (REGIONS_LOG, ("old_sizes"));
  7403|         for (int i = card_table_element; i < total_bookkeeping_elements; i++)
  7404|         {
  7405|             assert (current_sizes[i] == bookkeeping_sizes[i]);
  7406|             dprintf (REGIONS_LOG, ("%zd", bookkeeping_sizes[i]));
  7407|         }
  7408|     }
  7409| #endif //DEBUG
  7410|     for (int i = card_table_element; i <= seg_mapping_table_element; i++)
  7411|     {
  7412|         uint8_t* required_begin = nullptr;
  7413|         uint8_t* required_end = nullptr;
  7414|         uint8_t* commit_begin = nullptr;
  7415|         uint8_t* commit_end = nullptr;
  7416|         if (initial_commit)
  7417|         {
  7418|             required_begin = bookkeeping_start + ((i == card_table_element) ? 0 : card_table_element_layout[i]);
  7419|             required_end = bookkeeping_start + card_table_element_layout[i] + new_sizes[i];
  7420|             commit_begin = align_lower_page(required_begin);
  7421|         }
  7422|         else
  7423|         {
  7424|             assert (additional_commit);
  7425|             required_begin = bookkeeping_start + card_table_element_layout[i] + bookkeeping_sizes[i];
  7426|             required_end = required_begin + new_sizes[i] - bookkeeping_sizes[i];
  7427|             commit_begin = align_on_page(required_begin);
  7428|         }
  7429|         assert (required_begin <= required_end);
  7430|         commit_end = align_on_page(required_end);
  7431|         commit_end = min (commit_end, align_lower_page(bookkeeping_start + card_table_element_layout[i + 1]));
  7432|         commit_begin = min (commit_begin, commit_end);
  7433|         assert (commit_begin <= commit_end);
  7434|         dprintf (REGIONS_LOG, ("required = [%p, %p), size = %zd", required_begin, required_end, required_end - required_begin));
  7435|         dprintf (REGIONS_LOG, ("commit   = [%p, %p), size = %zd", commit_begin, commit_end, commit_end - commit_begin));
  7436|         commit_begins[i] = commit_begin;
  7437|         commit_sizes[i] = (size_t)(commit_end - commit_begin);
  7438|     }
  7439|     dprintf (REGIONS_LOG, ("---------------------------------------"));
  7440|     return true;
  7441| }
  7442| bool gc_heap::inplace_commit_card_table (uint8_t* from, uint8_t* to)
  7443| {
  7444|     dprintf (REGIONS_LOG, ("inplace_commit_card_table(%p, %p), size = %zd", from, to, to - from));
  7445|     uint8_t* start = g_gc_lowest_address;
  7446|     uint8_t* end = g_gc_highest_address;
  7447|     uint8_t* commit_begins[total_bookkeeping_elements];
  7448|     size_t commit_sizes[total_bookkeeping_elements];
  7449|     size_t new_sizes[total_bookkeeping_elements];
  7450|     if (!get_card_table_commit_layout(from, to, commit_begins, commit_sizes, new_sizes))
  7451|     {
  7452|         return true;
  7453|     }
  7454|     int failed_commit = -1;
  7455|     for (int i = card_table_element; i <= seg_mapping_table_element; i++)
  7456|     {
  7457|         bool succeed;
  7458|         if (commit_sizes[i] > 0)
  7459|         {
  7460|             succeed = virtual_commit (commit_begins[i], commit_sizes[i], recorded_committed_bookkeeping_bucket);
  7461|             if (!succeed)
  7462|             {
  7463|                 failed_commit = i;
  7464|                 break;
  7465|             }
  7466|         }
  7467|     }
  7468|     if (failed_commit == -1)
  7469|     {
  7470|         for (int i = card_table_element; i < total_bookkeeping_elements; i++)
  7471|         {
  7472|             bookkeeping_sizes[i] = new_sizes[i];
  7473|         }
  7474|     }
  7475|     else
  7476|     {
  7477|         for (int i = card_table_element; i < failed_commit; i++)
  7478|         {
  7479|             bool succeed;
  7480|             if (commit_sizes[i] > 0)
  7481|             {
  7482|                 succeed = virtual_decommit (commit_begins[i], commit_sizes[i], recorded_committed_bookkeeping_bucket);
  7483|                 assert (succeed);
  7484|             }
  7485|         }
  7486|         return false;
  7487|     }
  7488|     return true;
  7489| }
  7490| #endif //USE_REGIONS
  7491| uint32_t* gc_heap::make_card_table (uint8_t* start, uint8_t* end)
  7492| {
  7493|     assert (g_gc_lowest_address == start);
  7494|     assert (g_gc_highest_address == end);
  7495|     uint32_t virtual_reserve_flags = VirtualReserveFlags::None;
  7496| #ifdef CARD_BUNDLE
  7497|     if (can_use_write_watch_for_card_table())
  7498|     {
  7499| #ifndef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  7500|         virtual_reserve_flags |= VirtualReserveFlags::WriteWatch;
  7501| #endif
  7502|     }
  7503| #endif //CARD_BUNDLE
  7504|     get_card_table_element_layout(start, end, card_table_element_layout);
  7505|     size_t alloc_size = card_table_element_layout[total_bookkeeping_elements];
  7506|     uint8_t* mem = (uint8_t*)GCToOSInterface::VirtualReserve (alloc_size, 0, virtual_reserve_flags);
  7507|     bookkeeping_start = mem;
  7508|     if (!mem)
  7509|         return 0;
  7510|     dprintf (2, ("Init - Card table alloc for %zd bytes: [%zx, %zx[",
  7511|                  alloc_size, (size_t)mem, (size_t)(mem+alloc_size)));
  7512| #ifdef USE_REGIONS
  7513|     if (!inplace_commit_card_table (g_gc_lowest_address, global_region_allocator.get_left_used_unsafe()))
  7514|     {
  7515|         dprintf (1, ("Card table commit failed"));
  7516|         GCToOSInterface::VirtualRelease (mem, alloc_size);
  7517|         return 0;
  7518|     }
  7519|     bookkeeping_covered_committed = global_region_allocator.get_left_used_unsafe();
  7520| #else
  7521|     size_t commit_size = card_table_element_layout[seg_mapping_table_element + 1];
  7522|     if (!virtual_commit (mem, commit_size, recorded_committed_bookkeeping_bucket))
  7523|     {
  7524|         dprintf (1, ("Card table commit failed"));
  7525|         GCToOSInterface::VirtualRelease (mem, alloc_size);
  7526|         return 0;
  7527|     }
  7528| #endif //USE_REGIONS
  7529|     uint32_t* ct = (uint32_t*)(mem + card_table_element_layout[card_table_element]);
  7530|     card_table_refcount (ct) = 0;
  7531|     card_table_lowest_address (ct) = start;
  7532|     card_table_highest_address (ct) = end;
  7533|     card_table_brick_table (ct) = (short*)(mem + card_table_element_layout[brick_table_element]);
  7534|     card_table_size (ct) = alloc_size;
  7535|     card_table_next (ct) = 0;
  7536| #ifdef CARD_BUNDLE
  7537|     card_table_card_bundle_table (ct) = (uint32_t*)(mem + card_table_element_layout[card_bundle_table_element]);
  7538| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  7539|     g_gc_card_bundle_table = translate_card_bundle_table(card_table_card_bundle_table(ct), g_gc_lowest_address);
  7540| #endif
  7541| #endif //CARD_BUNDLE
  7542| #if defined(FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP) && defined (BACKGROUND_GC)
  7543|     if (gc_can_use_concurrent)
  7544|     {
  7545|         SoftwareWriteWatch::InitializeUntranslatedTable(mem + card_table_element_layout[software_write_watch_table_element], start);
  7546|     }
  7547| #endif //FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP && BACKGROUND_GC
  7548| #ifdef USE_REGIONS
  7549|     map_region_to_generation = (region_info*)(mem + card_table_element_layout[region_to_generation_table_element]);
  7550|     map_region_to_generation_skewed = map_region_to_generation - size_region_to_generation_table_of (0, g_gc_lowest_address);
  7551| #endif //USE_REGIONS
  7552|     seg_mapping_table = (seg_mapping*)(mem + card_table_element_layout[seg_mapping_table_element]);
  7553|     seg_mapping_table = (seg_mapping*)((uint8_t*)seg_mapping_table -
  7554|                                         size_seg_mapping_table_of (0, (align_lower_segment (g_gc_lowest_address))));
  7555| #ifdef BACKGROUND_GC
  7556|     if (gc_can_use_concurrent)
  7557|         card_table_mark_array (ct) = (uint32_t*)(mem + card_table_element_layout[mark_array_element]);
  7558|     else
  7559|         card_table_mark_array (ct) = NULL;
  7560| #endif //BACKGROUND_GC
  7561|     return translate_card_table(ct);
  7562| }
  7563| void gc_heap::set_fgm_result (failure_get_memory f, size_t s, BOOL loh_p)
  7564| {
  7565| #ifdef MULTIPLE_HEAPS
  7566|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
  7567|     {
  7568|         gc_heap* hp = gc_heap::g_heaps [hn];
  7569|         hp->fgm_result.set_fgm (f, s, loh_p);
  7570|     }
  7571| #else //MULTIPLE_HEAPS
  7572|     fgm_result.set_fgm (f, s, loh_p);
  7573| #endif //MULTIPLE_HEAPS
  7574| }
  7575| #ifndef USE_REGIONS
  7576| int gc_heap::grow_brick_card_tables (uint8_t* start,
  7577|                                      uint8_t* end,
  7578|                                      size_t size,
  7579|                                      heap_segment* new_seg,
  7580|                                      gc_heap* hp,
  7581|                                      BOOL uoh_p)
  7582| {
  7583|     uint8_t* la = g_gc_lowest_address;
  7584|     uint8_t* ha = g_gc_highest_address;
  7585|     uint8_t* saved_g_lowest_address = min (start, g_gc_lowest_address);
  7586|     uint8_t* saved_g_highest_address = max (end, g_gc_highest_address);
  7587|     seg_mapping* new_seg_mapping_table = nullptr;
  7588| #ifdef BACKGROUND_GC
  7589|     size_t logging_ma_commit_size = size_mark_array_of (0, (uint8_t*)size);
  7590| #endif //BACKGROUND_GC
  7591|     if ((la != saved_g_lowest_address ) || (ha != saved_g_highest_address))
  7592|     {
  7593|         {
  7594|             uint8_t* top = (uint8_t*)0 + Align (GCToOSInterface::GetVirtualMemoryLimit());
  7595|             if (top < saved_g_highest_address)
  7596|             {
  7597|                 top = saved_g_highest_address;
  7598|             }
  7599|             size_t ps = ha-la;
  7600| #ifdef HOST_64BIT
  7601|             if (ps > (uint64_t)200*1024*1024*1024)
  7602|                 ps += (uint64_t)100*1024*1024*1024;
  7603|             else
  7604| #endif // HOST_64BIT
  7605|                 ps *= 2;
  7606|             if (saved_g_lowest_address < g_gc_lowest_address)
  7607|             {
  7608|                 if (ps > (size_t)g_gc_lowest_address)
  7609|                     saved_g_lowest_address = (uint8_t*)(size_t)OS_PAGE_SIZE;
  7610|                 else
  7611|                 {
  7612|                     assert (((size_t)g_gc_lowest_address - ps) >= OS_PAGE_SIZE);
  7613|                     saved_g_lowest_address = min (saved_g_lowest_address, (g_gc_lowest_address - ps));
  7614|                 }
  7615|             }
  7616|             if (saved_g_highest_address > g_gc_highest_address)
  7617|             {
  7618|                 saved_g_highest_address = max ((saved_g_lowest_address + ps), saved_g_highest_address);
  7619|                 if (saved_g_highest_address > top)
  7620|                     saved_g_highest_address = top;
  7621|             }
  7622|         }
  7623|         dprintf (GC_TABLE_LOG, ("Growing card table [%zx, %zx[",
  7624|                                 (size_t)saved_g_lowest_address,
  7625|                                 (size_t)saved_g_highest_address));
  7626|         bool write_barrier_updated = false;
  7627|         uint32_t virtual_reserve_flags = VirtualReserveFlags::None;
  7628|         uint32_t* saved_g_card_table = g_gc_card_table;
  7629| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  7630|         uint32_t* saved_g_card_bundle_table = g_gc_card_bundle_table;
  7631| #endif
  7632|         get_card_table_element_layout(saved_g_lowest_address, saved_g_highest_address, card_table_element_layout);
  7633|         size_t cb = 0;
  7634|         uint32_t* ct = 0;
  7635|         uint32_t* translated_ct = 0;
  7636| #ifdef CARD_BUNDLE
  7637|         if (can_use_write_watch_for_card_table())
  7638|         {
  7639|             cb = size_card_bundle_of (saved_g_lowest_address, saved_g_highest_address);
  7640| #ifndef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  7641|             virtual_reserve_flags |= VirtualReserveFlags::WriteWatch;
  7642| #endif
  7643|         }
  7644| #endif //CARD_BUNDLE
  7645|         size_t alloc_size = card_table_element_layout[total_bookkeeping_elements];
  7646|         uint8_t* mem = (uint8_t*)GCToOSInterface::VirtualReserve (alloc_size, 0, virtual_reserve_flags);
  7647|         if (!mem)
  7648|         {
  7649|             set_fgm_result (fgm_grow_table, alloc_size, uoh_p);
  7650|             goto fail;
  7651|         }
  7652|         dprintf (GC_TABLE_LOG, ("Table alloc for %zd bytes: [%zx, %zx[",
  7653|                                  alloc_size, (size_t)mem, (size_t)((uint8_t*)mem+alloc_size)));
  7654|         {
  7655|             size_t commit_size = card_table_element_layout[seg_mapping_table_element + 1];
  7656|             if (!virtual_commit (mem, commit_size, recorded_committed_bookkeeping_bucket))
  7657|             {
  7658|                 dprintf (GC_TABLE_LOG, ("Table commit failed"));
  7659|                 set_fgm_result (fgm_commit_table, commit_size, uoh_p);
  7660|                 goto fail;
  7661|             }
  7662|         }
  7663|         ct = (uint32_t*)(mem + card_table_element_layout[card_table_element]);
  7664|         card_table_refcount (ct) = 0;
  7665|         card_table_lowest_address (ct) = saved_g_lowest_address;
  7666|         card_table_highest_address (ct) = saved_g_highest_address;
  7667|         card_table_next (ct) = &g_gc_card_table[card_word (gcard_of (la))];
  7668| /*
  7669|         memclr ((uint8_t*)ct,
  7670|                 (((saved_g_highest_address - saved_g_lowest_address)*sizeof (uint32_t) /
  7671|                   (card_size * card_word_width))
  7672|                  + sizeof (uint32_t)));
  7673| */
  7674|         card_table_brick_table (ct) = (short*)(mem + card_table_element_layout[brick_table_element]);
  7675| #ifdef CARD_BUNDLE
  7676|         card_table_card_bundle_table (ct) = (uint32_t*)(mem + card_table_element_layout[card_bundle_table_element]);
  7677|         memset(card_table_card_bundle_table (ct), 0xFF, cb);
  7678| #endif //CARD_BUNDLE
  7679|         new_seg_mapping_table = (seg_mapping*)(mem + card_table_element_layout[seg_mapping_table_element]);
  7680|         new_seg_mapping_table = (seg_mapping*)((uint8_t*)new_seg_mapping_table -
  7681|                                             size_seg_mapping_table_of (0, (align_lower_segment (saved_g_lowest_address))));
  7682|         memcpy(&new_seg_mapping_table[seg_mapping_word_of(g_gc_lowest_address)],
  7683|             &seg_mapping_table[seg_mapping_word_of(g_gc_lowest_address)],
  7684|             size_seg_mapping_table_of(g_gc_lowest_address, g_gc_highest_address));
  7685| #ifdef BACKGROUND_GC
  7686|         if(gc_can_use_concurrent)
  7687|             card_table_mark_array (ct) = (uint32_t*)(mem + card_table_element_layout[mark_array_element]);
  7688|         else
  7689|             card_table_mark_array (ct) = NULL;
  7690| #endif //BACKGROUND_GC
  7691|         translated_ct = translate_card_table (ct);
  7692|         dprintf (GC_TABLE_LOG, ("card table: %zx(translated: %zx), seg map: %zx, mark array: %zx",
  7693|             (size_t)ct, (size_t)translated_ct, (size_t)new_seg_mapping_table, (size_t)card_table_mark_array (ct)));
  7694| #ifdef BACKGROUND_GC
  7695|         if (hp->is_bgc_in_progress())
  7696|         {
  7697|             dprintf (GC_TABLE_LOG, ("new low: %p, new high: %p, latest mark array is %p(translate: %p)",
  7698|                                     saved_g_lowest_address, saved_g_highest_address,
  7699|                                     card_table_mark_array (ct),
  7700|                                     translate_mark_array (card_table_mark_array (ct))));
  7701|             uint32_t* new_mark_array = (uint32_t*)((uint8_t*)card_table_mark_array (ct) - size_mark_array_of (0, saved_g_lowest_address));
  7702|             if (!commit_new_mark_array_global (new_mark_array))
  7703|             {
  7704|                 dprintf (GC_TABLE_LOG, ("failed to commit portions in the mark array for existing segments"));
  7705|                 set_fgm_result (fgm_commit_table, logging_ma_commit_size, uoh_p);
  7706|                 goto fail;
  7707|             }
  7708|             if (!commit_mark_array_new_seg (hp, new_seg, translated_ct, saved_g_lowest_address))
  7709|             {
  7710|                 dprintf (GC_TABLE_LOG, ("failed to commit mark array for the new seg"));
  7711|                 set_fgm_result (fgm_commit_table, logging_ma_commit_size, uoh_p);
  7712|                 goto fail;
  7713|             }
  7714|         }
  7715|         else
  7716|         {
  7717|             clear_commit_flag_global();
  7718|         }
  7719| #endif //BACKGROUND_GC
  7720| #if defined(FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP) && defined(BACKGROUND_GC)
  7721|         if (gc_can_use_concurrent)
  7722|         {
  7723|             bool is_runtime_suspended = GCToEEInterface::IsGCThread();
  7724|             if (!is_runtime_suspended)
  7725|             {
  7726|                 suspend_EE();
  7727|             }
  7728|             g_gc_card_table = translated_ct;
  7729| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  7730|             g_gc_card_bundle_table = translate_card_bundle_table(card_table_card_bundle_table(ct), saved_g_lowest_address);
  7731| #endif
  7732|             SoftwareWriteWatch::SetResizedUntranslatedTable(
  7733|                 mem + card_table_element_layout[software_write_watch_table_element],
  7734|                 saved_g_lowest_address,
  7735|                 saved_g_highest_address);
  7736|             seg_mapping_table = new_seg_mapping_table;
  7737|             g_gc_lowest_address = saved_g_lowest_address;
  7738|             g_gc_highest_address = saved_g_highest_address;
  7739|             stomp_write_barrier_resize(true, la != saved_g_lowest_address);
  7740|             write_barrier_updated = true;
  7741|             if (!is_runtime_suspended)
  7742|             {
  7743|                 restart_EE();
  7744|             }
  7745|         }
  7746|         else
  7747| #endif //FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP && BACKGROUND_GC
  7748|         {
  7749|             g_gc_card_table = translated_ct;
  7750| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  7751|             g_gc_card_bundle_table = translate_card_bundle_table(card_table_card_bundle_table(ct), saved_g_lowest_address);
  7752| #endif
  7753|         }
  7754|         if (!write_barrier_updated)
  7755|         {
  7756|             seg_mapping_table = new_seg_mapping_table;
  7757|             GCToOSInterface::FlushProcessWriteBuffers();
  7758|             g_gc_lowest_address = saved_g_lowest_address;
  7759|             g_gc_highest_address = saved_g_highest_address;
  7760|             stomp_write_barrier_resize(GCToEEInterface::IsGCThread(), la != saved_g_lowest_address);
  7761|         }
  7762|         return 0;
  7763| fail:
  7764|         if (mem)
  7765|         {
  7766|             assert(g_gc_card_table == saved_g_card_table);
  7767| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  7768|             assert(g_gc_card_bundle_table  == saved_g_card_bundle_table);
  7769| #endif
  7770|             if (!GCToOSInterface::VirtualRelease (mem, alloc_size))
  7771|             {
  7772|                 dprintf (GC_TABLE_LOG, ("GCToOSInterface::VirtualRelease failed"));
  7773|                 assert (!"release failed");
  7774|             }
  7775|         }
  7776|         return -1;
  7777|     }
  7778|     else
  7779|     {
  7780| #ifdef BACKGROUND_GC
  7781|         if (hp->is_bgc_in_progress())
  7782|         {
  7783|             dprintf (GC_TABLE_LOG, ("in range new seg %p, mark_array is %p", new_seg, hp->mark_array));
  7784|             if (!commit_mark_array_new_seg (hp, new_seg))
  7785|             {
  7786|                 dprintf (GC_TABLE_LOG, ("failed to commit mark array for the new seg in range"));
  7787|                 set_fgm_result (fgm_commit_table, logging_ma_commit_size, uoh_p);
  7788|                 return -1;
  7789|             }
  7790|         }
  7791| #endif //BACKGROUND_GC
  7792|     }
  7793|     return 0;
  7794| }
  7795| #endif //!USE_REGIONS
  7796| void gc_heap::copy_brick_card_range (uint8_t* la, uint32_t* old_card_table,
  7797|                                      short* old_brick_table,
  7798|                                      uint8_t* start, uint8_t* end)
  7799| {
  7800|     ptrdiff_t brick_offset = brick_of (start) - brick_of (la);
  7801|     dprintf (2, ("copying tables for range [%zx %zx[", (size_t)start, (size_t)end));
  7802|     short* brick_start = &brick_table [brick_of (start)];
  7803|     if (old_brick_table)
  7804|     {
  7805|         memcpy (brick_start, &old_brick_table[brick_offset],
  7806|                 size_brick_of (start, end));
  7807|     }
  7808|     uint32_t* old_ct = &old_card_table[card_word (card_of (la))];
  7809| #ifdef BACKGROUND_GC
  7810|     if (gc_heap::background_running_p())
  7811|     {
  7812|         uint32_t* old_mark_array = card_table_mark_array (old_ct);
  7813|         if ((card_table_highest_address (old_ct) >= start) &&
  7814|             (card_table_lowest_address (old_ct) <= end))
  7815|         {
  7816|             if ((background_saved_highest_address >= start) &&
  7817|                 (background_saved_lowest_address <= end))
  7818|             {
  7819|                 uint8_t* m_start = max (background_saved_lowest_address, start);
  7820|                 uint8_t* m_end = min (background_saved_highest_address, end);
  7821|                 memcpy (&mark_array[mark_word_of (m_start)],
  7822|                         &old_mark_array[mark_word_of (m_start) - mark_word_of (la)],
  7823|                         size_mark_array_of (m_start, m_end));
  7824|             }
  7825|         }
  7826|         else
  7827|         {
  7828|             assert (old_brick_table == 0);
  7829|         }
  7830|     }
  7831| #endif //BACKGROUND_GC
  7832|     uint32_t* ct = card_table_next (&card_table[card_word (card_of(lowest_address))]);
  7833|     assert (ct);
  7834|     while (card_table_next (old_ct) != ct)
  7835|     {
  7836|         if ((card_table_highest_address (ct) >= end) &&
  7837|             (card_table_lowest_address (ct) <= start))
  7838|         {
  7839|             size_t start_word = card_word (card_of (start));
  7840|             uint32_t* dest = &card_table[start_word];
  7841|             uint32_t* src = &((translate_card_table (ct))[start_word]);
  7842|             ptrdiff_t count = count_card_of (start, end);
  7843|             for (int x = 0; x < count; x++)
  7844|             {
  7845|                 *dest |= *src;
  7846| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
  7847|                 if (*src != 0)
  7848|                 {
  7849|                     card_bundle_set(cardw_card_bundle(start_word+x));
  7850|                 }
  7851| #endif
  7852|                 dest++;
  7853|                 src++;
  7854|             }
  7855|         }
  7856|         ct = card_table_next (ct);
  7857|     }
  7858| }
  7859| void gc_heap::copy_brick_card_table()
  7860| {
  7861|     uint32_t* old_card_table = card_table;
  7862|     short* old_brick_table = brick_table;
  7863|     uint8_t* la = lowest_address;
  7864| #ifdef _DEBUG
  7865|     uint8_t* ha = highest_address;
  7866|     assert (la == card_table_lowest_address (&old_card_table[card_word (card_of (la))]));
  7867|     assert (ha == card_table_highest_address (&old_card_table[card_word (card_of (la))]));
  7868| #endif //_DEBUG
  7869|     /* todo: Need a global lock for this */
  7870|     uint32_t* ct = &g_gc_card_table[card_word (gcard_of (g_gc_lowest_address))];
  7871|     own_card_table (ct);
  7872|     card_table = translate_card_table (ct);
  7873|     bookkeeping_start = (uint8_t*)ct - sizeof(card_table_info);
  7874|     card_table_size(ct) = card_table_element_layout[total_bookkeeping_elements];
  7875|     /* End of global lock */
  7876|     highest_address = card_table_highest_address (ct);
  7877|     lowest_address = card_table_lowest_address (ct);
  7878|     brick_table = card_table_brick_table (ct);
  7879| #ifdef BACKGROUND_GC
  7880|     if (gc_can_use_concurrent)
  7881|     {
  7882|         mark_array = translate_mark_array (card_table_mark_array (ct));
  7883|         assert (mark_word_of (g_gc_highest_address) ==
  7884|             mark_word_of (align_on_mark_word (g_gc_highest_address)));
  7885|     }
  7886|     else
  7887|         mark_array = NULL;
  7888| #endif //BACKGROUND_GC
  7889| #ifdef CARD_BUNDLE
  7890|     card_bundle_table = translate_card_bundle_table (card_table_card_bundle_table (ct), g_gc_lowest_address);
  7891|     assert (&card_bundle_table [card_bundle_word (cardw_card_bundle (card_word (card_of (g_gc_lowest_address))))] ==
  7892|             card_table_card_bundle_table (ct));
  7893|     if (card_bundles_enabled())
  7894|     {
  7895|         card_bundles_set (cardw_card_bundle (card_word (card_of (lowest_address))),
  7896|                           cardw_card_bundle (align_cardw_on_bundle (card_word (card_of (highest_address)))));
  7897|     }
  7898| #ifdef MULTIPLE_HEAPS
  7899|     uint64_t th = (uint64_t)MH_TH_CARD_BUNDLE*gc_heap::n_heaps;
  7900| #else
  7901|     uint64_t th = (uint64_t)SH_TH_CARD_BUNDLE;
  7902| #endif //MULTIPLE_HEAPS
  7903|     if (reserved_memory >= th)
  7904|     {
  7905|         enable_card_bundles();
  7906|     }
  7907| #endif //CARD_BUNDLE
  7908|     for (int i = get_start_generation_index(); i < total_generation_count; i++)
  7909|     {
  7910|         heap_segment* seg = generation_start_segment (generation_of (i));
  7911|         while (seg)
  7912|         {
  7913|             if (heap_segment_read_only_p (seg) && !heap_segment_in_range_p (seg))
  7914|             {
  7915|                 if ((heap_segment_reserved (seg) > lowest_address) &&
  7916|                     (heap_segment_mem (seg) < highest_address))
  7917|                 {
  7918|                     set_ro_segment_in_range (seg);
  7919|                 }
  7920|             }
  7921|             else
  7922|             {
  7923|                 uint8_t* end = align_on_page (heap_segment_allocated (seg));
  7924|                 copy_brick_card_range (la, old_card_table,
  7925|                     (i < uoh_start_generation) ? old_brick_table : NULL,
  7926|                     align_lower_page (heap_segment_mem (seg)),
  7927|                     end);
  7928|             }
  7929|             seg = heap_segment_next (seg);
  7930|         }
  7931|     }
  7932|     release_card_table (&old_card_table[card_word (card_of(la))]);
  7933| }
  7934| #ifdef FEATURE_BASICFREEZE
  7935| BOOL gc_heap::insert_ro_segment (heap_segment* seg)
  7936| {
  7937| #ifdef FEATURE_EVENT_TRACE
  7938|     if (!use_frozen_segments_p)
  7939|         use_frozen_segments_p = true;
  7940| #endif //FEATURE_EVENT_TRACE
  7941|     enter_spin_lock (&gc_heap::gc_lock);
  7942|     if (!gc_heap::seg_table->ensure_space_for_insert ()
  7943| #ifdef BACKGROUND_GC
  7944|         || (is_bgc_in_progress() && !commit_mark_array_new_seg(__this, seg))
  7945| #endif //BACKGROUND_GC
  7946|         )
  7947|     {
  7948|         leave_spin_lock(&gc_heap::gc_lock);
  7949|         return FALSE;
  7950|     }
  7951|     generation* gen2 = generation_of (max_generation);
  7952|     heap_segment* oldhead = generation_start_segment (gen2);
  7953|     heap_segment_next (seg) = oldhead;
  7954|     generation_start_segment (gen2) = seg;
  7955| #ifdef USE_REGIONS
  7956|     dprintf (REGIONS_LOG, ("setting gen2 start seg to %zx(%p)->%p",
  7957|         (size_t)seg, heap_segment_mem (seg), heap_segment_mem (oldhead)));
  7958|     if (generation_tail_ro_region (gen2) == 0)
  7959|     {
  7960|         dprintf (REGIONS_LOG, ("setting gen2 tail ro -> %p", heap_segment_mem (seg)));
  7961|         generation_tail_ro_region (gen2) = seg;
  7962|     }
  7963| #endif //USE_REGIONS
  7964|     seg_table->insert (heap_segment_mem(seg), (size_t)seg);
  7965|     seg_mapping_table_add_ro_segment (seg);
  7966|     if ((heap_segment_reserved (seg) > lowest_address) &&
  7967|         (heap_segment_mem (seg) < highest_address))
  7968|     {
  7969|         set_ro_segment_in_range (seg);
  7970|     }
  7971|     FIRE_EVENT(GCCreateSegment_V1, heap_segment_mem(seg), (size_t)(heap_segment_reserved (seg) - heap_segment_mem(seg)), gc_etw_segment_read_only_heap);
  7972|     leave_spin_lock (&gc_heap::gc_lock);
  7973|     return TRUE;
  7974| }
  7975| void gc_heap::update_ro_segment (heap_segment* seg, uint8_t* allocated, uint8_t* committed)
  7976| {
  7977|     enter_spin_lock (&gc_heap::gc_lock);
  7978|     assert (heap_segment_read_only_p (seg));
  7979|     assert (allocated <= committed);
  7980|     assert (committed <= heap_segment_reserved (seg));
  7981|     heap_segment_allocated (seg) = allocated;
  7982|     heap_segment_committed (seg) = committed;
  7983|     leave_spin_lock (&gc_heap::gc_lock);
  7984| }
  7985| void gc_heap::remove_ro_segment (heap_segment* seg)
  7986| {
  7987| #ifdef BACKGROUND_GC
  7988|     if (gc_can_use_concurrent)
  7989|     {
  7990|         if ((seg->flags & heap_segment_flags_ma_committed) || (seg->flags & heap_segment_flags_ma_pcommitted))
  7991|         {
  7992|             seg_clear_mark_array_bits_soh (seg);
  7993|         }
  7994|     }
  7995| #endif //BACKGROUND_GC
  7996|     enter_spin_lock (&gc_heap::gc_lock);
  7997|     seg_table->remove (heap_segment_mem (seg));
  7998|     seg_mapping_table_remove_ro_segment (seg);
  7999|     generation* gen2 = generation_of (max_generation);
  8000| #ifdef USE_REGIONS
  8001|     if (generation_tail_ro_region (gen2) == seg)
  8002|     {
  8003|         generation_tail_ro_region (gen2) = 0;
  8004|     }
  8005| #endif //USE_REGIONS
  8006|     heap_segment* curr_seg = generation_start_segment (gen2);
  8007|     heap_segment* prev_seg = NULL;
  8008|     while (curr_seg && curr_seg != seg)
  8009|     {
  8010|         prev_seg = curr_seg;
  8011|         curr_seg = heap_segment_next (curr_seg);
  8012|     }
  8013|     assert (curr_seg == seg);
  8014|     if (prev_seg)
  8015|         heap_segment_next (prev_seg) = heap_segment_next (curr_seg);
  8016|     else
  8017|         generation_start_segment (gen2) = heap_segment_next (curr_seg);
  8018|     leave_spin_lock (&gc_heap::gc_lock);
  8019| }
  8020| #endif //FEATURE_BASICFREEZE
  8021| BOOL gc_heap::set_ro_segment_in_range (heap_segment* seg)
  8022| {
  8023|     seg->flags |= heap_segment_flags_inrange;
  8024|     ro_segments_in_range = TRUE;
  8025|     return TRUE;
  8026| }
  8027| uint8_t** make_mark_list (size_t size)
  8028| {
  8029|     uint8_t** mark_list = new (nothrow) uint8_t* [size];
  8030|     return mark_list;
  8031| }
  8032| #define swap(a,b){uint8_t* t; t = a; a = b; b = t;}
  8033| void verify_qsort_array (uint8_t* *low, uint8_t* *high)
  8034| {
  8035|     uint8_t **i = 0;
  8036|     for (i = low+1; i <= high; i++)
  8037|     {
  8038|         if (*i < *(i-1))
  8039|         {
  8040|             FATAL_GC_ERROR();
  8041|         }
  8042|     }
  8043| }
  8044| #ifndef USE_INTROSORT
  8045| void qsort1( uint8_t* *low, uint8_t* *high, unsigned int depth)
  8046| {
  8047|     if (((low + 16) >= high) || (depth > 100))
  8048|     {
  8049|         uint8_t **i, **j;
  8050|         for (i = low+1; i <= high; i++)
  8051|         {
  8052|             uint8_t* val = *i;
  8053|             for (j=i;j >low && val<*(j-1);j--)
  8054|             {
  8055|                 *j=*(j-1);
  8056|             }
  8057|             *j=val;
  8058|         }
  8059|     }
  8060|     else
  8061|     {
  8062|         uint8_t *pivot, **left, **right;
  8063|         if (*(low+((high-low)/2)) < *low)
  8064|             swap (*(low+((high-low)/2)), *low);
  8065|         if (*high < *low)
  8066|             swap (*low, *high);
  8067|         if (*high < *(low+((high-low)/2)))
  8068|             swap (*(low+((high-low)/2)), *high);
  8069|         swap (*(low+((high-low)/2)), *(high-1));
  8070|         pivot =  *(high-1);
  8071|         left = low; right = high-1;
  8072|         while (1) {
  8073|             while (*(--right) > pivot);
  8074|             while (*(++left)  < pivot);
  8075|             if (left < right)
  8076|             {
  8077|                 swap(*left, *right);
  8078|             }
  8079|             else
  8080|                 break;
  8081|         }
  8082|         swap (*left, *(high-1));
  8083|         qsort1(low, left-1, depth+1);
  8084|         qsort1(left+1, high, depth+1);
  8085|     }
  8086| }
  8087| #endif //USE_INTROSORT
  8088| void rqsort1( uint8_t* *low, uint8_t* *high)
  8089| {
  8090|     if ((low + 16) >= high)
  8091|     {
  8092|         uint8_t **i, **j;
  8093|         for (i = low+1; i <= high; i++)
  8094|         {
  8095|             uint8_t* val = *i;
  8096|             for (j=i;j >low && val>*(j-1);j--)
  8097|             {
  8098|                 *j=*(j-1);
  8099|             }
  8100|             *j=val;
  8101|         }
  8102|     }
  8103|     else
  8104|     {
  8105|         uint8_t *pivot, **left, **right;
  8106|         if (*(low+((high-low)/2)) > *low)
  8107|             swap (*(low+((high-low)/2)), *low);
  8108|         if (*high > *low)
  8109|             swap (*low, *high);
  8110|         if (*high > *(low+((high-low)/2)))
  8111|             swap (*(low+((high-low)/2)), *high);
  8112|         swap (*(low+((high-low)/2)), *(high-1));
  8113|         pivot =  *(high-1);
  8114|         left = low; right = high-1;
  8115|         while (1) {
  8116|             while (*(--right) < pivot);
  8117|             while (*(++left)  > pivot);
  8118|             if (left < right)
  8119|             {
  8120|                 swap(*left, *right);
  8121|             }
  8122|             else
  8123|                 break;
  8124|         }
  8125|         swap (*left, *(high-1));
  8126|         rqsort1(low, left-1);
  8127|         rqsort1(left+1, high);
  8128|     }
  8129| }
  8130| #if defined(USE_INTROSORT) || defined(USE_VXSORT)
  8131| class introsort
  8132| {
  8133| private:
  8134|     static const int size_threshold = 64;
  8135|     static const int max_depth = 100;
  8136| inline static void swap_elements(uint8_t** i,uint8_t** j)
  8137|     {
  8138|         uint8_t* t=*i;
  8139|         *i=*j;
  8140|         *j=t;
  8141|     }
  8142| public:
  8143|     static void sort (uint8_t** begin, uint8_t** end, int ignored)
  8144|     {
  8145|         ignored = 0;
  8146|         introsort_loop (begin, end, max_depth);
  8147|         insertionsort (begin, end);
  8148|     }
  8149| private:
  8150|     static void introsort_loop (uint8_t** lo, uint8_t** hi, int depth_limit)
  8151|     {
  8152|         while (hi-lo >= size_threshold)
  8153|         {
  8154|             if (depth_limit == 0)
  8155|             {
  8156|                 heapsort (lo, hi);
  8157|                 return;
  8158|             }
  8159|             uint8_t** p=median_partition (lo, hi);
  8160|             depth_limit=depth_limit-1;
  8161|             introsort_loop (p, hi, depth_limit);
  8162|             hi=p-1;
  8163|         }
  8164|     }
  8165|     static uint8_t** median_partition (uint8_t** low, uint8_t** high)
  8166|     {
  8167|         uint8_t *pivot, **left, **right;
  8168|         if (*(low+((high-low)/2)) < *low)
  8169|             swap_elements ((low+((high-low)/2)), low);
  8170|         if (*high < *low)
  8171|             swap_elements (low, high);
  8172|         if (*high < *(low+((high-low)/2)))
  8173|             swap_elements ((low+((high-low)/2)), high);
  8174|         swap_elements ((low+((high-low)/2)), (high-1));
  8175|         pivot =  *(high-1);
  8176|         left = low; right = high-1;
  8177|         while (1) {
  8178|             while (*(--right) > pivot);
  8179|             while (*(++left)  < pivot);
  8180|             if (left < right)
  8181|             {
  8182|                 swap_elements(left, right);
  8183|             }
  8184|             else
  8185|                 break;
  8186|         }
  8187|         swap_elements (left, (high-1));
  8188|         return left;
  8189|     }
  8190|     static void insertionsort (uint8_t** lo, uint8_t** hi)
  8191|     {
  8192|         for (uint8_t** i=lo+1; i <= hi; i++)
  8193|         {
  8194|             uint8_t** j = i;
  8195|             uint8_t* t = *i;
  8196|             while((j > lo) && (t <*(j-1)))
  8197|             {
  8198|                 *j = *(j-1);
  8199|                 j--;
  8200|             }
  8201|             *j = t;
  8202|         }
  8203|     }
  8204|     static void heapsort (uint8_t** lo, uint8_t** hi)
  8205|     {
  8206|         size_t n = hi - lo + 1;
  8207|         for (size_t i=n / 2; i >= 1; i--)
  8208|         {
  8209|             downheap (i,n,lo);
  8210|         }
  8211|         for (size_t i = n; i > 1; i--)
  8212|         {
  8213|             swap_elements (lo, lo + i - 1);
  8214|             downheap(1, i - 1,  lo);
  8215|         }
  8216|     }
  8217|     static void downheap (size_t i, size_t n, uint8_t** lo)
  8218|     {
  8219|         uint8_t* d = *(lo + i - 1);
  8220|         size_t child;
  8221|         while (i <= n / 2)
  8222|         {
  8223|             child = 2*i;
  8224|             if (child < n && *(lo + child - 1)<(*(lo + child)))
  8225|             {
  8226|                 child++;
  8227|             }
  8228|             if (!(d<*(lo + child - 1)))
  8229|             {
  8230|                 break;
  8231|             }
  8232|             *(lo + i - 1) = *(lo + child - 1);
  8233|             i = child;
  8234|         }
  8235|         *(lo + i - 1) = d;
  8236|     }
  8237| };
  8238| #endif //defined(USE_INTROSORT) || defined(USE_VXSORT)
  8239| #ifdef USE_VXSORT
  8240| static void do_vxsort (uint8_t** item_array, ptrdiff_t item_count, uint8_t* range_low, uint8_t* range_high)
  8241| {
  8242|     const size_t AVX2_THRESHOLD_SIZE = 8 * 1024;
  8243|     const size_t AVX512F_THRESHOLD_SIZE = 128 * 1024;
  8244|     if (item_count <= 1)
  8245|         return;
  8246|     if (IsSupportedInstructionSet (InstructionSet::AVX2) && (item_count > AVX2_THRESHOLD_SIZE))
  8247|     {
  8248|         dprintf(3, ("Sorting mark lists"));
  8249|         if (IsSupportedInstructionSet (InstructionSet::AVX512F) && (item_count > AVX512F_THRESHOLD_SIZE))
  8250|         {
  8251|             do_vxsort_avx512 (item_array, &item_array[item_count - 1], range_low, range_high);
  8252|         }
  8253|         else
  8254|         {
  8255|             do_vxsort_avx2 (item_array, &item_array[item_count - 1], range_low, range_high);
  8256|         }
  8257|     }
  8258|     else
  8259|     {
  8260|         dprintf (3, ("Sorting mark lists"));
  8261|         introsort::sort (item_array, &item_array[item_count - 1], 0);
  8262|     }
  8263| #ifdef _DEBUG
  8264|     for (ptrdiff_t i = 0; i < item_count - 1; i++)
  8265|     {
  8266|         assert (item_array[i] <= item_array[i + 1]);
  8267|     }
  8268|     assert ((range_low <= item_array[0]) && (item_array[item_count - 1] <= range_high));
  8269| #endif
  8270| }
  8271| #endif //USE_VXSORT
  8272| #ifdef MULTIPLE_HEAPS
  8273| static size_t target_mark_count_for_heap (size_t total_mark_count, int heap_count, int heap_number)
  8274| {
  8275|     size_t average_mark_count = total_mark_count / heap_count;
  8276|     size_t remaining_mark_count = total_mark_count - (average_mark_count * heap_count);
  8277|     if (heap_number == (heap_count - 1))
  8278|         return (average_mark_count + remaining_mark_count);
  8279|     else
  8280|         return average_mark_count;
  8281| }
  8282| NOINLINE
  8283| uint8_t** gc_heap::equalize_mark_lists (size_t total_mark_list_size)
  8284| {
  8285|     size_t local_mark_count[MAX_SUPPORTED_CPUS];
  8286|     size_t total_mark_count = 0;
  8287|     for (int i = 0; i < n_heaps; i++)
  8288|     {
  8289|         gc_heap* hp = g_heaps[i];
  8290|         size_t mark_count = hp->mark_list_index - hp->mark_list;
  8291|         local_mark_count[i] = mark_count;
  8292|         total_mark_count += mark_count;
  8293|     }
  8294|     assert(total_mark_count == total_mark_list_size);
  8295|     size_t this_target_mark_count = target_mark_count_for_heap (total_mark_count, n_heaps, heap_number);
  8296|     if (local_mark_count[heap_number] >= this_target_mark_count)
  8297|         return (mark_list + this_target_mark_count);
  8298|     int surplus_heap_index = 0;
  8299|     for (int deficit_heap_index = 0; deficit_heap_index <= heap_number; deficit_heap_index++)
  8300|     {
  8301|         size_t deficit_target_mark_count = target_mark_count_for_heap (total_mark_count, n_heaps, deficit_heap_index);
  8302|         if (local_mark_count[deficit_heap_index] >= deficit_target_mark_count)
  8303|             continue;
  8304|         while ((surplus_heap_index < n_heaps) && (local_mark_count[deficit_heap_index] < deficit_target_mark_count))
  8305|         {
  8306|             size_t deficit = deficit_target_mark_count - local_mark_count[deficit_heap_index];
  8307|             size_t surplus_target_mark_count = target_mark_count_for_heap(total_mark_count, n_heaps, surplus_heap_index);
  8308|             if (local_mark_count[surplus_heap_index] > surplus_target_mark_count)
  8309|             {
  8310|                 size_t surplus = local_mark_count[surplus_heap_index] - surplus_target_mark_count;
  8311|                 size_t amount_to_transfer = min(deficit, surplus);
  8312|                 local_mark_count[surplus_heap_index] -= amount_to_transfer;
  8313|                 if (deficit_heap_index == heap_number)
  8314|                 {
  8315|                     memcpy(&g_heaps[deficit_heap_index]->mark_list[local_mark_count[deficit_heap_index]],
  8316|                            &g_heaps[surplus_heap_index]->mark_list[local_mark_count[surplus_heap_index]],
  8317|                            (amount_to_transfer*sizeof(mark_list[0])));
  8318|                 }
  8319|                 local_mark_count[deficit_heap_index] += amount_to_transfer;
  8320|             }
  8321|             else
  8322|             {
  8323|                 surplus_heap_index++;
  8324|             }
  8325|         }
  8326|     }
  8327|     return (mark_list + local_mark_count[heap_number]);
  8328| }
  8329| NOINLINE
  8330| size_t gc_heap::sort_mark_list()
  8331| {
  8332|     if ((settings.condemned_generation >= max_generation)
  8333| #ifdef USE_REGIONS
  8334|       || (g_mark_list_piece == nullptr)
  8335| #endif //USE_REGIONS
  8336|         )
  8337|     {
  8338|         mark_list_index = mark_list_end + 1;
  8339|         return 0;
  8340|     }
  8341|     if (mark_list_index > mark_list_end)
  8342|     {
  8343|         dprintf (2, ("h%d sort_mark_list overflow", heap_number));
  8344|         mark_list_overflow = true;
  8345|         return 0;
  8346|     }
  8347|     for (int i = 0; i < n_heaps; i++)
  8348|     {
  8349|         if (g_heaps[i]->mark_list_index > g_heaps[i]->mark_list_end)
  8350|         {
  8351|             mark_list_index = mark_list_end + 1;
  8352|             dprintf (2, ("h%d sort_mark_list: detected overflow on heap %d", heap_number, i));
  8353|             return 0;
  8354|         }
  8355|     }
  8356|     size_t total_mark_list_size = 0;
  8357|     size_t total_ephemeral_size = 0;
  8358|     uint8_t* low = (uint8_t*)~0;
  8359|     uint8_t* high = 0;
  8360|     for (int i = 0; i < n_heaps; i++)
  8361|     {
  8362|         gc_heap* hp = g_heaps[i];
  8363|         total_mark_list_size += (hp->mark_list_index - hp->mark_list);
  8364| #ifdef USE_REGIONS
  8365|         for (int gen_num = settings.condemned_generation; gen_num >= 0; gen_num--)
  8366|         {
  8367|             generation* gen = hp->generation_of (gen_num);
  8368|             for (heap_segment* seg = generation_start_segment (gen); seg != nullptr; seg = heap_segment_next (seg))
  8369|             {
  8370|                 size_t ephemeral_size = heap_segment_allocated (seg) - heap_segment_mem (seg);
  8371|                 total_ephemeral_size += ephemeral_size;
  8372|                 low = min (low, heap_segment_mem (seg));
  8373|                 high = max (high, heap_segment_allocated (seg));
  8374|             }
  8375|         }
  8376| #else //USE_REGIONS
  8377|         size_t ephemeral_size = heap_segment_allocated (hp->ephemeral_heap_segment) - hp->gc_low;
  8378|         total_ephemeral_size += ephemeral_size;
  8379|         low = min (low, hp->gc_low);
  8380|         high = max (high, heap_segment_allocated (hp->ephemeral_heap_segment));
  8381| #endif //USE_REGIONS
  8382|     }
  8383|     if (total_mark_list_size > (total_ephemeral_size / 256))
  8384|     {
  8385|         mark_list_index = mark_list_end + 1;
  8386|         dprintf (2, ("h%d total mark list %zd is too large > (%zd / 256), don't use",
  8387|             heap_number, total_mark_list_size, total_ephemeral_size));
  8388|         mark_list_overflow = false;
  8389|         return 0;
  8390|     }
  8391|     uint8_t **local_mark_list_index = equalize_mark_lists (total_mark_list_size);
  8392| #ifdef USE_VXSORT
  8393|     ptrdiff_t item_count = local_mark_list_index - mark_list;
  8394| #if defined(_DEBUG) || defined(WRITE_SORT_DATA)
  8395|     uint8_t** mark_list_copy = &g_mark_list_copy[heap_number * mark_list_size];
  8396|     uint8_t** mark_list_copy_index = &mark_list_copy[item_count];
  8397|     for (ptrdiff_t i = 0; i < item_count; i++)
  8398|     {
  8399|         uint8_t* item = mark_list[i];
  8400|         assert ((low <= item) && (item < high));
  8401|         mark_list_copy[i] = item;
  8402|     }
  8403| #endif // _DEBUG || WRITE_SORT_DATA
  8404|     do_vxsort (mark_list, item_count, low, high);
  8405| #ifdef WRITE_SORT_DATA
  8406|     char file_name[256];
  8407|     sprintf_s (file_name, ARRAY_SIZE(file_name), "sort_data_gc%d_heap%d", settings.gc_index, heap_number);
  8408|     FILE* f;
  8409|     errno_t err = fopen_s (&f, file_name, "wb");
  8410|     if (err == 0)
  8411|     {
  8412|         size_t magic = 'SDAT';
  8413|         if (fwrite (&magic, sizeof(magic), 1, f) != 1)
  8414|             dprintf (3, ("fwrite failed\n"));
  8415|         if (fwrite (&elapsed_cycles, sizeof(elapsed_cycles), 1, f) != 1)
  8416|             dprintf (3, ("fwrite failed\n"));
  8417|         if (fwrite (&low, sizeof(low), 1, f) != 1)
  8418|             dprintf (3, ("fwrite failed\n"));
  8419|         if (fwrite (&item_count, sizeof(item_count), 1, f) != 1)
  8420|             dprintf (3, ("fwrite failed\n"));
  8421|         if (fwrite (mark_list_copy, sizeof(mark_list_copy[0]), item_count, f) != item_count)
  8422|             dprintf (3, ("fwrite failed\n"));
  8423|         if (fwrite (&magic, sizeof(magic), 1, f) != 1)
  8424|             dprintf (3, ("fwrite failed\n"));
  8425|         if (fclose (f) != 0)
  8426|             dprintf (3, ("fclose failed\n"));
  8427|     }
  8428| #endif
  8429| #ifdef _DEBUG
  8430|     if (mark_list_copy_index > mark_list_copy)
  8431|     {
  8432|         introsort::sort (mark_list_copy, mark_list_copy_index - 1, 0);
  8433|     }
  8434|     for (ptrdiff_t i = 0; i < item_count; i++)
  8435|     {
  8436|         uint8_t* item = mark_list[i];
  8437|         assert (mark_list_copy[i] == item);
  8438|     }
  8439| #endif //_DEBUG
  8440| #else //USE_VXSORT
  8441|     dprintf (3, ("Sorting mark lists"));
  8442|     if (local_mark_list_index > mark_list)
  8443|     {
  8444|         introsort::sort (mark_list, local_mark_list_index - 1, 0);
  8445|     }
  8446| #endif //USE_VXSORT
  8447|     uint8_t** x = mark_list;
  8448| #ifdef USE_REGIONS
  8449|     assert (g_mark_list_piece_size >= region_count);
  8450|     assert (g_mark_list_piece_total_size >= region_count*n_heaps);
  8451|     for (size_t region_index = 0; region_index < region_count; region_index++)
  8452|     {
  8453|         mark_list_piece_start[region_index] = NULL;
  8454|         mark_list_piece_end[region_index] = NULL;
  8455|     }
  8456| #define predicate(x) (((x) < local_mark_list_index) && (*(x) < region_limit))
  8457|     while (x < local_mark_list_index)
  8458|     {
  8459|         heap_segment* region = get_region_info_for_address (*x);
  8460|         assert ((heap_segment_mem (region) <= *x) && (*x < heap_segment_allocated (region)));
  8461|         size_t region_index = get_basic_region_index_for_address (heap_segment_mem (region));
  8462|         uint8_t* region_limit = heap_segment_allocated (region);
  8463|         uint8_t*** mark_list_piece_start_ptr = &mark_list_piece_start[region_index];
  8464|         uint8_t*** mark_list_piece_end_ptr = &mark_list_piece_end[region_index];
  8465| #else // USE_REGIONS
  8466| #define predicate(x) (((x) < local_mark_list_index) && (*(x) < heap->ephemeral_high))
  8467|     int heap_num;
  8468|     for (heap_num = 0; heap_num < n_heaps; heap_num++)
  8469|     {
  8470|         mark_list_piece_start[heap_num] = NULL;
  8471|         mark_list_piece_end[heap_num] = NULL;
  8472|     }
  8473|     heap_num = -1;
  8474|     while (x < local_mark_list_index)
  8475|     {
  8476|         gc_heap* heap;
  8477| #ifdef _DEBUG
  8478|         int last_heap_num = heap_num;
  8479| #endif //_DEBUG
  8480|         do
  8481|         {
  8482|             heap_num++;
  8483|             if (heap_num >= n_heaps)
  8484|                 heap_num = 0;
  8485|             assert(heap_num != last_heap_num); // we should always find the heap - infinite loop if not!
  8486|             heap = g_heaps[heap_num];
  8487|         }
  8488|         while (!(*x >= heap->ephemeral_low && *x < heap->ephemeral_high));
  8489|         uint8_t*** mark_list_piece_start_ptr = &mark_list_piece_start[heap_num];
  8490|         uint8_t*** mark_list_piece_end_ptr = &mark_list_piece_end[heap_num];
  8491| #endif // USE_REGIONS
  8492|         *mark_list_piece_start_ptr = x;
  8493|         if (predicate(x))
  8494|         {
  8495|             if (predicate(local_mark_list_index -1))
  8496|             {
  8497|                 x = local_mark_list_index;
  8498|                 *mark_list_piece_end_ptr = x;
  8499|                 break;
  8500|             }
  8501|             unsigned inc = 1;
  8502|             do
  8503|             {
  8504|                 inc *= 2;
  8505|                 uint8_t** temp_x = x;
  8506|                 x += inc;
  8507|                 if (temp_x > x)
  8508|                 {
  8509|                     break;
  8510|                 }
  8511|             }
  8512|             while (predicate(x));
  8513|             x -= inc;
  8514|             do
  8515|             {
  8516|                 assert (predicate(x) && !(((x + inc) > x) && predicate(x + inc)));
  8517|                 inc /= 2;
  8518|                 if (((x + inc) > x) && predicate(x + inc))
  8519|                 {
  8520|                     x += inc;
  8521|                 }
  8522|             }
  8523|             while (inc > 1);
  8524|             assert(predicate(x) && !predicate(x + inc) && (inc == 1));
  8525|             x += 1;
  8526|         }
  8527|         *mark_list_piece_end_ptr = x;
  8528|     }
  8529| #undef predicate
  8530|     return total_mark_list_size;
  8531| }
  8532| void gc_heap::append_to_mark_list (uint8_t **start, uint8_t **end)
  8533| {
  8534|     size_t slots_needed = end - start;
  8535|     size_t slots_available = mark_list_end + 1 - mark_list_index;
  8536|     size_t slots_to_copy = min(slots_needed, slots_available);
  8537|     memcpy(mark_list_index, start, slots_to_copy*sizeof(*start));
  8538|     mark_list_index += slots_to_copy;
  8539|     dprintf (3, ("h%d: appended %zd slots to mark_list\n", heap_number, slots_to_copy));
  8540| }
  8541| #ifdef _DEBUG
  8542| #if !defined(_MSC_VER)
  8543| #if !defined(__cdecl)
  8544| #if defined(__i386__)
  8545| #define __cdecl __attribute__((cdecl))
  8546| #else
  8547| #define __cdecl
  8548| #endif
  8549| #endif
  8550| #endif
  8551| static int __cdecl cmp_mark_list_item (const void* vkey, const void* vdatum)
  8552| {
  8553|     uint8_t** key = (uint8_t**)vkey;
  8554|     uint8_t** datum = (uint8_t**)vdatum;
  8555|     if (*key < *datum)
  8556|         return -1;
  8557|     else if (*key > *datum)
  8558|         return 1;
  8559|     else
  8560|         return 0;
  8561| }
  8562| #endif // _DEBUG
  8563| #ifdef USE_REGIONS
  8564| uint8_t** gc_heap::get_region_mark_list (BOOL& use_mark_list, uint8_t* start, uint8_t* end, uint8_t*** mark_list_end_ptr)
  8565| {
  8566|     size_t region_number = get_basic_region_index_for_address (start);
  8567|     size_t source_number = region_number;
  8568| #else //USE_REGIONS
  8569| void gc_heap::merge_mark_lists (size_t total_mark_list_size)
  8570| {
  8571|     if (total_mark_list_size == 0)
  8572|     {
  8573|         return;
  8574|     }
  8575| #ifdef _DEBUG
  8576|     size_t this_mark_list_size = target_mark_count_for_heap (total_mark_list_size, n_heaps, heap_number);
  8577|     for (uint8_t** p = mark_list + this_mark_list_size; p < mark_list_index; p++)
  8578|     {
  8579|         uint8_t* item = *p;
  8580|         uint8_t** found_slot = nullptr;
  8581|         for (int i = 0; i < n_heaps; i++)
  8582|         {
  8583|             uint8_t** heap_mark_list = &g_mark_list[i * mark_list_size];
  8584|             size_t heap_mark_list_size = target_mark_count_for_heap (total_mark_list_size, n_heaps, i);
  8585|             found_slot = (uint8_t**)bsearch (&item, heap_mark_list, heap_mark_list_size, sizeof(item), cmp_mark_list_item);
  8586|             if (found_slot != nullptr)
  8587|                 break;
  8588|         }
  8589|         assert ((found_slot != nullptr) && (*found_slot == item));
  8590|     }
  8591| #endif
  8592|     dprintf(3, ("merge_mark_lists: heap_number = %d  starts out with %zd entries",
  8593|         heap_number, (mark_list_index - mark_list)));
  8594|     int source_number = (size_t)heap_number;
  8595| #endif //USE_REGIONS
  8596|     uint8_t** source[MAX_SUPPORTED_CPUS];
  8597|     uint8_t** source_end[MAX_SUPPORTED_CPUS];
  8598|     int source_heap[MAX_SUPPORTED_CPUS];
  8599|     int source_count = 0;
  8600|     for (int i = 0; i < n_heaps; i++)
  8601|     {
  8602|         gc_heap* heap = g_heaps[i];
  8603|         if (heap->mark_list_piece_start[source_number] < heap->mark_list_piece_end[source_number])
  8604|         {
  8605|             source[source_count] = heap->mark_list_piece_start[source_number];
  8606|             source_end[source_count] = heap->mark_list_piece_end[source_number];
  8607|             source_heap[source_count] = i;
  8608|             if (source_count < MAX_SUPPORTED_CPUS)
  8609|                 source_count++;
  8610|         }
  8611|     }
  8612|     dprintf(3, ("source_number = %zd  has %d sources\n", (size_t)source_number, source_count));
  8613| #if defined(_DEBUG) || defined(TRACE_GC)
  8614|     for (int j = 0; j < source_count; j++)
  8615|     {
  8616|         dprintf(3, ("source_number = %zd  ", (size_t)source_number));
  8617|         dprintf(3, (" source from heap %zd = %zx .. %zx (%zd entries)",
  8618|             (size_t)(source_heap[j]), (size_t)(source[j][0]),
  8619|             (size_t)(source_end[j][-1]), (size_t)(source_end[j] - source[j])));
  8620|         for (uint8_t **x = source[j]; x < source_end[j] - 1; x++)
  8621|         {
  8622|             if (x[0] > x[1])
  8623|             {
  8624|                 dprintf(3, ("oops, mark_list from source %d for heap %zd isn't sorted\n", j,  (size_t)source_number));
  8625|                 assert (0);
  8626|             }
  8627|         }
  8628|     }
  8629| #endif //_DEBUG || TRACE_GC
  8630|     mark_list = &g_mark_list_copy [heap_number*mark_list_size];
  8631|     mark_list_index = mark_list;
  8632|     mark_list_end = &mark_list [mark_list_size-1];
  8633|     int piece_count = 0;
  8634|     if (source_count == 0)
  8635|     {
  8636|         ; // nothing to do
  8637|     }
  8638|     else if (source_count == 1)
  8639|     {
  8640|         mark_list = source[0];
  8641|         mark_list_index = source_end[0];
  8642|         mark_list_end = mark_list_index;
  8643|         piece_count++;
  8644|     }
  8645|     else
  8646|     {
  8647|         while (source_count > 1)
  8648|         {
  8649|             int lowest_source = 0;
  8650|             uint8_t *lowest = *source[0];
  8651|             uint8_t *second_lowest = *source[1];
  8652|             for (int i = 1; i < source_count; i++)
  8653|             {
  8654|                 if (lowest > *source[i])
  8655|                 {
  8656|                     second_lowest = lowest;
  8657|                     lowest = *source[i];
  8658|                     lowest_source = i;
  8659|                 }
  8660|                 else if (second_lowest > *source[i])
  8661|                 {
  8662|                     second_lowest = *source[i];
  8663|                 }
  8664|             }
  8665|             uint8_t **x;
  8666|             if (source_end[lowest_source][-1] <= second_lowest)
  8667|                 x = source_end[lowest_source];
  8668|             else
  8669|             {
  8670|                 for (x = source[lowest_source]; x < source_end[lowest_source] && *x <= second_lowest; x++)
  8671|                     ;
  8672|             }
  8673|             append_to_mark_list(source[lowest_source], x);
  8674| #ifdef USE_REGIONS
  8675|             if (mark_list_index > mark_list_end)
  8676|             {
  8677|                 use_mark_list = false;
  8678|                 return nullptr;
  8679|             }
  8680| #endif //USE_REGIONS
  8681|             piece_count++;
  8682|             source[lowest_source] = x;
  8683|             if (x >= source_end[lowest_source])
  8684|             {
  8685|                 if (lowest_source < source_count-1)
  8686|                 {
  8687|                     source[lowest_source] = source[source_count-1];
  8688|                     source_end[lowest_source] = source_end[source_count-1];
  8689|                 }
  8690|                 source_count--;
  8691|             }
  8692|         }
  8693|         append_to_mark_list(source[0], source_end[0]);
  8694| #ifdef USE_REGIONS
  8695|         if (mark_list_index > mark_list_end)
  8696|         {
  8697|             use_mark_list = false;
  8698|             return nullptr;
  8699|         }
  8700| #endif //USE_REGIONS
  8701|         piece_count++;
  8702|     }
  8703| #if defined(_DEBUG) || defined(TRACE_GC)
  8704|     for (uint8_t **x = mark_list; x < mark_list_index - 1; x++)
  8705|     {
  8706|         if (x[0] > x[1])
  8707|         {
  8708|             dprintf(3, ("oops, mark_list for heap %d isn't sorted at the end of merge_mark_lists", heap_number));
  8709|             assert (0);
  8710|         }
  8711|     }
  8712| #endif //_DEBUG || TRACE_GC
  8713| #ifdef USE_REGIONS
  8714|     *mark_list_end_ptr = mark_list_index;
  8715|     return mark_list;
  8716| #endif // USE_REGIONS
  8717| }
  8718| #else
  8719| #ifdef USE_REGIONS
  8720| static uint8_t** binary_search (uint8_t** left, uint8_t** right, uint8_t* e)
  8721| {
  8722|     if (left == right)
  8723|         return left;
  8724|     assert (left < right);
  8725|     uint8_t** a = left;
  8726|     size_t l = 0;
  8727|     size_t r = (size_t)(right - left);
  8728|     while ((r - l) >= 2)
  8729|     {
  8730|         size_t m = l + (r - l) / 2;
  8731|         assert ((l < m) && (m < r));
  8732|         if (a[m] < e)
  8733|         {
  8734|             l = m;
  8735|         }
  8736|         else
  8737|         {
  8738|             r = m;
  8739|         }
  8740|     }
  8741|     if (a[l] < e)
  8742|         return a + l + 1;
  8743|     else
  8744|         return a + l;
  8745| }
  8746| uint8_t** gc_heap::get_region_mark_list (BOOL& use_mark_list, uint8_t* start, uint8_t* end, uint8_t*** mark_list_end_ptr)
  8747| {
  8748|     *mark_list_end_ptr = binary_search (mark_list, mark_list_index, end);
  8749|     return binary_search (mark_list, *mark_list_end_ptr, start);
  8750| }
  8751| #endif //USE_REGIONS
  8752| #endif //MULTIPLE_HEAPS
  8753| void gc_heap::grow_mark_list ()
  8754| {
  8755| #ifdef USE_VXSORT
  8756| #ifdef MULTIPLE_HEAPS
  8757|     const size_t MAX_MARK_LIST_SIZE = IsSupportedInstructionSet (InstructionSet::AVX2) ?
  8758|         (1000 * 1024) : (200 * 1024);
  8759| #else //MULTIPLE_HEAPS
  8760|     const size_t MAX_MARK_LIST_SIZE = IsSupportedInstructionSet (InstructionSet::AVX2) ?
  8761|         (32 * 1024) : (16 * 1024);
  8762| #endif //MULTIPLE_HEAPS
  8763| #else //USE_VXSORT
  8764| #ifdef MULTIPLE_HEAPS
  8765|     const size_t MAX_MARK_LIST_SIZE = 200 * 1024;
  8766| #else //MULTIPLE_HEAPS
  8767|     const size_t MAX_MARK_LIST_SIZE = 16 * 1024;
  8768| #endif //MULTIPLE_HEAPS
  8769| #endif //USE_VXSORT
  8770|     size_t new_mark_list_size = min (mark_list_size * 2, MAX_MARK_LIST_SIZE);
  8771|     size_t new_mark_list_total_size = new_mark_list_size*n_heaps;
  8772|     if (new_mark_list_total_size == g_mark_list_total_size)
  8773|         return;
  8774| #ifdef MULTIPLE_HEAPS
  8775|     uint8_t** new_mark_list = make_mark_list (new_mark_list_total_size);
  8776|     uint8_t** new_mark_list_copy = make_mark_list (new_mark_list_total_size);
  8777|     if ((new_mark_list != nullptr) && (new_mark_list_copy != nullptr))
  8778|     {
  8779|         delete[] g_mark_list;
  8780|         g_mark_list = new_mark_list;
  8781|         delete[] g_mark_list_copy;
  8782|         g_mark_list_copy = new_mark_list_copy;
  8783|         mark_list_size = new_mark_list_size;
  8784|         g_mark_list_total_size = new_mark_list_total_size;
  8785|     }
  8786|     else
  8787|     {
  8788|         delete[] new_mark_list;
  8789|         delete[] new_mark_list_copy;
  8790|     }
  8791| #else //MULTIPLE_HEAPS
  8792|     uint8_t** new_mark_list = make_mark_list (new_mark_list_size);
  8793|     if (new_mark_list != nullptr)
  8794|     {
  8795|         delete[] mark_list;
  8796|         g_mark_list = new_mark_list;
  8797|         mark_list_size = new_mark_list_size;
  8798|         g_mark_list_total_size = new_mark_list_size;
  8799|     }
  8800| #endif //MULTIPLE_HEAPS
  8801| }
  8802| #ifndef USE_REGIONS
  8803| class seg_free_spaces
  8804| {
  8805|     struct seg_free_space
  8806|     {
  8807|         BOOL is_plug;
  8808|         void* start;
  8809|     };
  8810|     struct free_space_bucket
  8811|     {
  8812|         seg_free_space* free_space;
  8813|         ptrdiff_t count_add; // Assigned when we first construct the array.
  8814|         ptrdiff_t count_fit; // How many items left when we are fitting plugs.
  8815|     };
  8816|     void move_bucket (int old_power2, int new_power2)
  8817|     {
  8818|         assert (old_power2 >= 0);
  8819|         assert (old_power2 >= new_power2);
  8820|         if (old_power2 == new_power2)
  8821|         {
  8822|             return;
  8823|         }
  8824|         seg_free_space* src_index = free_space_buckets[old_power2].free_space;
  8825|         for (int i = old_power2; i > new_power2; i--)
  8826|         {
  8827|             seg_free_space** dest = &(free_space_buckets[i].free_space);
  8828|             (*dest)++;
  8829|             seg_free_space* dest_index = free_space_buckets[i - 1].free_space;
  8830|             if (i > (new_power2 + 1))
  8831|             {
  8832|                 seg_free_space temp = *src_index;
  8833|                 *src_index = *dest_index;
  8834|                 *dest_index = temp;
  8835|             }
  8836|             src_index = dest_index;
  8837|         }
  8838|         free_space_buckets[old_power2].count_fit--;
  8839|         free_space_buckets[new_power2].count_fit++;
  8840|     }
  8841| #ifdef _DEBUG
  8842|     void dump_free_space (seg_free_space* item)
  8843|     {
  8844|         uint8_t* addr = 0;
  8845|         size_t len = 0;
  8846|         if (item->is_plug)
  8847|         {
  8848|             mark* m = (mark*)(item->start);
  8849|             len = pinned_len (m);
  8850|             addr = pinned_plug (m) - len;
  8851|         }
  8852|         else
  8853|         {
  8854|             heap_segment* seg = (heap_segment*)(item->start);
  8855|             addr = heap_segment_plan_allocated (seg);
  8856|             len = heap_segment_committed (seg) - addr;
  8857|         }
  8858|         dprintf (SEG_REUSE_LOG_1, ("[%d]0x%p %zd", heap_num, addr, len));
  8859|     }
  8860|     void dump()
  8861|     {
  8862|         seg_free_space* item = NULL;
  8863|         int i = 0;
  8864|         dprintf (SEG_REUSE_LOG_1, ("[%d]----------------------------------\nnow the free spaces look like:", heap_num));
  8865|         for (i = 0; i < (free_space_bucket_count - 1); i++)
  8866|         {
  8867|             dprintf (SEG_REUSE_LOG_1, ("[%d]Free spaces for 2^%d bucket:", heap_num, (base_power2 + i)));
  8868|             dprintf (SEG_REUSE_LOG_1, ("[%d]%s %s", heap_num, "start", "len"));
  8869|             item = free_space_buckets[i].free_space;
  8870|             while (item < free_space_buckets[i + 1].free_space)
  8871|             {
  8872|                 dump_free_space (item);
  8873|                 item++;
  8874|             }
  8875|             dprintf (SEG_REUSE_LOG_1, ("[%d]----------------------------------", heap_num));
  8876|         }
  8877|         dprintf (SEG_REUSE_LOG_1, ("[%d]Free spaces for 2^%d bucket:", heap_num, (base_power2 + i)));
  8878|         dprintf (SEG_REUSE_LOG_1, ("[%d]%s %s", heap_num, "start", "len"));
  8879|         item = free_space_buckets[i].free_space;
  8880|         while (item <= &seg_free_space_array[free_space_item_count - 1])
  8881|         {
  8882|             dump_free_space (item);
  8883|             item++;
  8884|         }
  8885|         dprintf (SEG_REUSE_LOG_1, ("[%d]----------------------------------", heap_num));
  8886|     }
  8887| #endif //_DEBUG
  8888|     free_space_bucket* free_space_buckets;
  8889|     seg_free_space* seg_free_space_array;
  8890|     ptrdiff_t free_space_bucket_count;
  8891|     ptrdiff_t free_space_item_count;
  8892|     int base_power2;
  8893|     int heap_num;
  8894| #ifdef _DEBUG
  8895|     BOOL has_end_of_seg;
  8896| #endif //_DEBUG
  8897| public:
  8898|     seg_free_spaces (int h_number)
  8899|     {
  8900|         heap_num = h_number;
  8901|     }
  8902|     BOOL alloc ()
  8903|     {
  8904|         size_t total_prealloc_size =
  8905|             MAX_NUM_BUCKETS * sizeof (free_space_bucket) +
  8906|             MAX_NUM_FREE_SPACES * sizeof (seg_free_space);
  8907|         free_space_buckets = (free_space_bucket*) new (nothrow) uint8_t[total_prealloc_size];
  8908|         return (!!free_space_buckets);
  8909|     }
  8910|     void add_buckets (int base, size_t* ordered_free_spaces, int bucket_count, size_t item_count)
  8911|     {
  8912|         assert (free_space_buckets);
  8913|         assert (item_count <= (size_t)MAX_PTR);
  8914|         free_space_bucket_count = bucket_count;
  8915|         free_space_item_count = item_count;
  8916|         base_power2 = base;
  8917| #ifdef _DEBUG
  8918|         has_end_of_seg = FALSE;
  8919| #endif //_DEBUG
  8920|         ptrdiff_t total_item_count = 0;
  8921|         ptrdiff_t i = 0;
  8922|         seg_free_space_array = (seg_free_space*)(free_space_buckets + free_space_bucket_count);
  8923|         for (i = 0; i < (ptrdiff_t)item_count; i++)
  8924|         {
  8925|             seg_free_space_array[i].start = 0;
  8926|             seg_free_space_array[i].is_plug = FALSE;
  8927|         }
  8928|         for (i = 0; i < bucket_count; i++)
  8929|         {
  8930|             free_space_buckets[i].count_add = ordered_free_spaces[i];
  8931|             free_space_buckets[i].count_fit = ordered_free_spaces[i];
  8932|             free_space_buckets[i].free_space = &seg_free_space_array[total_item_count];
  8933|             total_item_count += free_space_buckets[i].count_add;
  8934|         }
  8935|         assert (total_item_count == (ptrdiff_t)item_count);
  8936|     }
  8937|     void add (void* start, BOOL plug_p, BOOL first_p)
  8938|     {
  8939|         size_t size = (plug_p ?
  8940|                        pinned_len ((mark*)start) :
  8941|                        (heap_segment_committed ((heap_segment*)start) -
  8942|                            heap_segment_plan_allocated ((heap_segment*)start)));
  8943|         if (plug_p)
  8944|         {
  8945|             dprintf (SEG_REUSE_LOG_1, ("[%d]Adding a free space before plug: %zd", heap_num, size));
  8946|         }
  8947|         else
  8948|         {
  8949|             dprintf (SEG_REUSE_LOG_1, ("[%d]Adding a free space at end of seg: %zd", heap_num, size));
  8950| #ifdef _DEBUG
  8951|             has_end_of_seg = TRUE;
  8952| #endif //_DEBUG
  8953|         }
  8954|         if (first_p)
  8955|         {
  8956|             size_t eph_gen_starts = gc_heap::eph_gen_starts_size;
  8957|             size -= eph_gen_starts;
  8958|             if (plug_p)
  8959|             {
  8960|                 mark* m = (mark*)(start);
  8961|                 pinned_len (m) -= eph_gen_starts;
  8962|             }
  8963|             else
  8964|             {
  8965|                 heap_segment* seg = (heap_segment*)start;
  8966|                 heap_segment_plan_allocated (seg) += eph_gen_starts;
  8967|             }
  8968|         }
  8969|         int bucket_power2 = index_of_highest_set_bit (size);
  8970|         if (bucket_power2 < base_power2)
  8971|         {
  8972|             return;
  8973|         }
  8974|         free_space_bucket* bucket = &free_space_buckets[bucket_power2 - base_power2];
  8975|         seg_free_space* bucket_free_space = bucket->free_space;
  8976|         assert (plug_p || (!plug_p && bucket->count_add));
  8977|         if (bucket->count_add == 0)
  8978|         {
  8979|             dprintf (SEG_REUSE_LOG_1, ("[%d]Already have enough of 2^%d", heap_num, bucket_power2));
  8980|             return;
  8981|         }
  8982|         ptrdiff_t index = bucket->count_add - 1;
  8983|         dprintf (SEG_REUSE_LOG_1, ("[%d]Building free spaces: adding %p; len: %zd (2^%d)",
  8984|                     heap_num,
  8985|                     (plug_p ?
  8986|                         (pinned_plug ((mark*)start) - pinned_len ((mark*)start)) :
  8987|                         heap_segment_plan_allocated ((heap_segment*)start)),
  8988|                     size,
  8989|                     bucket_power2));
  8990|         if (plug_p)
  8991|         {
  8992|             bucket_free_space[index].is_plug = TRUE;
  8993|         }
  8994|         bucket_free_space[index].start = start;
  8995|         bucket->count_add--;
  8996|     }
  8997| #ifdef _DEBUG
  8998|     void check()
  8999|     {
  9000|         ptrdiff_t i = 0;
  9001|         int end_of_seg_count = 0;
  9002|         for (i = 0; i < free_space_item_count; i++)
  9003|         {
  9004|             assert (seg_free_space_array[i].start);
  9005|             if (!(seg_free_space_array[i].is_plug))
  9006|             {
  9007|                 end_of_seg_count++;
  9008|             }
  9009|         }
  9010|         if (has_end_of_seg)
  9011|         {
  9012|             assert (end_of_seg_count == 1);
  9013|         }
  9014|         else
  9015|         {
  9016|             assert (end_of_seg_count == 0);
  9017|         }
  9018|         for (i = 0; i < free_space_bucket_count; i++)
  9019|         {
  9020|             assert (free_space_buckets[i].count_add == 0);
  9021|         }
  9022|     }
  9023| #endif //_DEBUG
  9024|     uint8_t* fit (uint8_t* old_loc,
  9025|                size_t plug_size
  9026|                REQD_ALIGN_AND_OFFSET_DCL)
  9027|     {
  9028|         if (old_loc)
  9029|         {
  9030| #ifdef SHORT_PLUGS
  9031|             assert (!is_plug_padded (old_loc));
  9032| #endif //SHORT_PLUGS
  9033|             assert (!node_realigned (old_loc));
  9034|         }
  9035|         size_t saved_plug_size = plug_size;
  9036| #ifdef FEATURE_STRUCTALIGN
  9037|         _ASSERTE(requiredAlignment == DATA_ALIGNMENT && false);
  9038| #endif // FEATURE_STRUCTALIGN
  9039|         size_t plug_size_to_fit = plug_size;
  9040| #ifdef RESPECT_LARGE_ALIGNMENT
  9041|         plug_size_to_fit += switch_alignment_size(FALSE);
  9042| #endif //RESPECT_LARGE_ALIGNMENT
  9043|         int plug_power2 = index_of_highest_set_bit (round_up_power2 (plug_size_to_fit + Align(min_obj_size)));
  9044|         ptrdiff_t i;
  9045|         uint8_t* new_address = 0;
  9046|         if (plug_power2 < base_power2)
  9047|         {
  9048|             plug_power2 = base_power2;
  9049|         }
  9050|         int chosen_power2 = plug_power2 - base_power2;
  9051| retry:
  9052|         for (i = chosen_power2; i < free_space_bucket_count; i++)
  9053|         {
  9054|             if (free_space_buckets[i].count_fit != 0)
  9055|             {
  9056|                 break;
  9057|             }
  9058|             chosen_power2++;
  9059|         }
  9060|         dprintf (SEG_REUSE_LOG_1, ("[%d]Fitting plug len %zd (2^%d) using 2^%d free space",
  9061|             heap_num,
  9062|             plug_size,
  9063|             plug_power2,
  9064|             (chosen_power2 + base_power2)));
  9065|         assert (i < free_space_bucket_count);
  9066|         seg_free_space* bucket_free_space = free_space_buckets[chosen_power2].free_space;
  9067|         ptrdiff_t free_space_count = free_space_buckets[chosen_power2].count_fit;
  9068|         size_t new_free_space_size = 0;
  9069|         BOOL can_fit = FALSE;
  9070|         size_t pad = 0;
  9071|         for (i = 0; i < free_space_count; i++)
  9072|         {
  9073|             size_t free_space_size = 0;
  9074|             pad = 0;
  9075|             if (bucket_free_space[i].is_plug)
  9076|             {
  9077|                 mark* m = (mark*)(bucket_free_space[i].start);
  9078|                 uint8_t* plug_free_space_start = pinned_plug (m) - pinned_len (m);
  9079|                 if (!((old_loc == 0) || same_large_alignment_p (old_loc, plug_free_space_start)))
  9080|                 {
  9081|                     pad = switch_alignment_size (FALSE);
  9082|                 }
  9083|                 plug_size = saved_plug_size + pad;
  9084|                 free_space_size = pinned_len (m);
  9085|                 new_address = pinned_plug (m) - pinned_len (m);
  9086|                 if (free_space_size >= (plug_size + Align (min_obj_size)) ||
  9087|                     free_space_size == plug_size)
  9088|                 {
  9089|                     new_free_space_size = free_space_size - plug_size;
  9090|                     pinned_len (m) = new_free_space_size;
  9091| #ifdef SIMPLE_DPRINTF
  9092|                     dprintf (SEG_REUSE_LOG_0, ("[%d]FP: 0x%p->0x%p(%zx)(%zx), [0x%p (2^%d) -> [0x%p (2^%d)",
  9093|                                 heap_num,
  9094|                                 old_loc,
  9095|                                 new_address,
  9096|                                 (plug_size - pad),
  9097|                                 pad,
  9098|                                 pinned_plug (m),
  9099|                                 index_of_highest_set_bit (free_space_size),
  9100|                                 (pinned_plug (m) - pinned_len (m)),
  9101|                                 index_of_highest_set_bit (new_free_space_size)));
  9102| #endif //SIMPLE_DPRINTF
  9103|                     if (pad != 0)
  9104|                     {
  9105|                         set_node_realigned (old_loc);
  9106|                     }
  9107|                     can_fit = TRUE;
  9108|                 }
  9109|             }
  9110|             else
  9111|             {
  9112|                 heap_segment* seg = (heap_segment*)(bucket_free_space[i].start);
  9113|                 free_space_size = heap_segment_committed (seg) - heap_segment_plan_allocated (seg);
  9114|                 if (!((old_loc == 0) || same_large_alignment_p (old_loc, heap_segment_plan_allocated (seg))))
  9115|                 {
  9116|                     pad = switch_alignment_size (FALSE);
  9117|                 }
  9118|                 plug_size = saved_plug_size + pad;
  9119|                 if (free_space_size >= (plug_size + Align (min_obj_size)) ||
  9120|                     free_space_size == plug_size)
  9121|                 {
  9122|                     new_address = heap_segment_plan_allocated (seg);
  9123|                     new_free_space_size = free_space_size - plug_size;
  9124|                     heap_segment_plan_allocated (seg) = new_address + plug_size;
  9125| #ifdef SIMPLE_DPRINTF
  9126|                     dprintf (SEG_REUSE_LOG_0, ("[%d]FS: 0x%p-> 0x%p(%zd) (2^%d) -> 0x%p (2^%d)",
  9127|                                 heap_num,
  9128|                                 old_loc,
  9129|                                 new_address,
  9130|                                 (plug_size - pad),
  9131|                                 index_of_highest_set_bit (free_space_size),
  9132|                                 heap_segment_plan_allocated (seg),
  9133|                                 index_of_highest_set_bit (new_free_space_size)));
  9134| #endif //SIMPLE_DPRINTF
  9135|                     if (pad != 0)
  9136|                         set_node_realigned (old_loc);
  9137|                     can_fit = TRUE;
  9138|                 }
  9139|             }
  9140|             if (can_fit)
  9141|             {
  9142|                 break;
  9143|             }
  9144|         }
  9145|         if (!can_fit)
  9146|         {
  9147|             assert (chosen_power2 == 0);
  9148|             chosen_power2 = 1;
  9149|             goto retry;
  9150|         }
  9151|         new_address += pad;
  9152|         assert ((chosen_power2 && (i == 0)) ||
  9153|                 ((!chosen_power2) && (i < free_space_count)));
  9154|         int new_bucket_power2 = index_of_highest_set_bit (new_free_space_size);
  9155|         if (new_bucket_power2 < base_power2)
  9156|         {
  9157|             new_bucket_power2 = base_power2;
  9158|         }
  9159|         move_bucket (chosen_power2, new_bucket_power2 - base_power2);
  9160|         return new_address;
  9161|     }
  9162|     void cleanup ()
  9163|     {
  9164|         if (free_space_buckets)
  9165|         {
  9166|             delete [] free_space_buckets;
  9167|         }
  9168|         if (seg_free_space_array)
  9169|         {
  9170|             delete [] seg_free_space_array;
  9171|         }
  9172|     }
  9173| };
  9174| #endif //!USE_REGIONS
  9175| #define marked(i) header(i)->IsMarked()
  9176| #define set_marked(i) header(i)->SetMarked()
  9177| #define clear_marked(i) header(i)->ClearMarked()
  9178| #define pinned(i) header(i)->IsPinned()
  9179| #define set_pinned(i) header(i)->SetPinned()
  9180| #define clear_pinned(i) header(i)->GetHeader()->ClrGCBit();
  9181| inline size_t my_get_size (Object* ob)
  9182| {
  9183|     MethodTable* mT = header(ob)->GetMethodTable();
  9184|     return (mT->GetBaseSize() +
  9185|             (mT->HasComponentSize() ?
  9186|              ((size_t)((CObjectHeader*)ob)->GetNumComponents() * mT->RawGetComponentSize()) : 0));
  9187| }
  9188| #define size(i) my_get_size (header(i))
  9189| #define contain_pointers(i) header(i)->ContainsPointers()
  9190| #ifdef COLLECTIBLE_CLASS
  9191| #define contain_pointers_or_collectible(i) header(i)->ContainsPointersOrCollectible()
  9192| #define get_class_object(i) GCToEEInterface::GetLoaderAllocatorObjectForGC((Object *)i)
  9193| #define is_collectible(i) method_table(i)->Collectible()
  9194| #else //COLLECTIBLE_CLASS
  9195| #define contain_pointers_or_collectible(i) header(i)->ContainsPointers()
  9196| #endif //COLLECTIBLE_CLASS
  9197| #ifdef BACKGROUND_GC
  9198| #ifdef FEATURE_BASICFREEZE
  9199| inline
  9200| void gc_heap::seg_clear_mark_array_bits_soh (heap_segment* seg)
  9201| {
  9202|     uint8_t* range_beg = 0;
  9203|     uint8_t* range_end = 0;
  9204|     if (bgc_mark_array_range (seg, FALSE, &range_beg, &range_end))
  9205|     {
  9206|         clear_mark_array (range_beg, align_on_mark_word (range_end));
  9207|     }
  9208| }
  9209| inline
  9210| void gc_heap::seg_set_mark_array_bits_soh (heap_segment* seg)
  9211| {
  9212|     uint8_t* range_beg = 0;
  9213|     uint8_t* range_end = 0;
  9214|     if (bgc_mark_array_range (seg, FALSE, &range_beg, &range_end))
  9215|     {
  9216|         size_t beg_word = mark_word_of (align_on_mark_word (range_beg));
  9217|         size_t end_word = mark_word_of (align_on_mark_word (range_end));
  9218|         uint8_t* op = range_beg;
  9219|         while (op < mark_word_address (beg_word))
  9220|         {
  9221|             mark_array_set_marked (op);
  9222|             op += mark_bit_pitch;
  9223|         }
  9224|         memset (&mark_array[beg_word], 0xFF, (end_word - beg_word)*sizeof (uint32_t));
  9225|     }
  9226| }
  9227| #endif //FEATURE_BASICFREEZE
  9228| void gc_heap::bgc_clear_batch_mark_array_bits (uint8_t* start, uint8_t* end)
  9229| {
  9230|     if ((start < background_saved_highest_address) &&
  9231|         (end > background_saved_lowest_address))
  9232|     {
  9233|         start = max (start, background_saved_lowest_address);
  9234|         end = min (end, background_saved_highest_address);
  9235|         size_t start_mark_bit = mark_bit_of (start);
  9236|         size_t end_mark_bit = mark_bit_of (end);
  9237|         unsigned int startbit = mark_bit_bit (start_mark_bit);
  9238|         unsigned int endbit = mark_bit_bit (end_mark_bit);
  9239|         size_t startwrd = mark_bit_word (start_mark_bit);
  9240|         size_t endwrd = mark_bit_word (end_mark_bit);
  9241|         dprintf (3, ("Clearing all mark array bits between [%zx:%zx-[%zx:%zx",
  9242|             (size_t)start, (size_t)start_mark_bit,
  9243|             (size_t)end, (size_t)end_mark_bit));
  9244|         unsigned int firstwrd = lowbits (~0, startbit);
  9245|         unsigned int lastwrd = highbits (~0, endbit);
  9246|         if (startwrd == endwrd)
  9247|         {
  9248|             if (startbit != endbit)
  9249|             {
  9250|                 unsigned int wrd = firstwrd | lastwrd;
  9251|                 mark_array[startwrd] &= wrd;
  9252|             }
  9253|             else
  9254|             {
  9255|                 assert (start == end);
  9256|             }
  9257|             return;
  9258|         }
  9259|         if (startbit)
  9260|         {
  9261|             mark_array[startwrd] &= firstwrd;
  9262|             startwrd++;
  9263|         }
  9264|         for (size_t wrdtmp = startwrd; wrdtmp < endwrd; wrdtmp++)
  9265|         {
  9266|             mark_array[wrdtmp] = 0;
  9267|         }
  9268|         if (endbit)
  9269|         {
  9270|             mark_array[endwrd] &= lastwrd;
  9271|         }
  9272|     }
  9273| }
  9274| #endif //BACKGROUND_GC
  9275| inline
  9276| BOOL gc_heap::is_mark_set (uint8_t* o)
  9277| {
  9278|     return marked (o);
  9279| }
  9280| #if defined (_MSC_VER) && defined (TARGET_X86)
  9281| #pragma optimize("y", on)        // Small critical routines, don't put in EBP frame
  9282| #endif //_MSC_VER && TARGET_X86
  9283| int gc_heap::object_gennum (uint8_t* o)
  9284| {
  9285| #ifdef USE_REGIONS
  9286|     return get_region_gen_num (o);
  9287| #else
  9288|     if (in_range_for_segment (o, ephemeral_heap_segment) &&
  9289|         (o >= generation_allocation_start (generation_of (max_generation - 1))))
  9290|     {
  9291|         for ( int i = 0; i < max_generation-1; i++)
  9292|         {
  9293|             if ((o >= generation_allocation_start (generation_of (i))))
  9294|                 return i;
  9295|         }
  9296|         return max_generation-1;
  9297|     }
  9298|     else
  9299|     {
  9300|         return max_generation;
  9301|     }
  9302| #endif //USE_REGIONS
  9303| }
  9304| int gc_heap::object_gennum_plan (uint8_t* o)
  9305| {
  9306| #ifdef USE_REGIONS
  9307|     return get_region_plan_gen_num (o);
  9308| #else
  9309|     if (in_range_for_segment (o, ephemeral_heap_segment))
  9310|     {
  9311|         for (int i = 0; i < ephemeral_generation_count; i++)
  9312|         {
  9313|             uint8_t* plan_start = generation_plan_allocation_start (generation_of (i));
  9314|             if (plan_start && (o >= plan_start))
  9315|             {
  9316|                 return i;
  9317|             }
  9318|         }
  9319|     }
  9320|     return max_generation;
  9321| #endif //USE_REGIONS
  9322| }
  9323| #if defined(_MSC_VER) && defined(TARGET_X86)
  9324| #pragma optimize("", on)        // Go back to command line default optimizations
  9325| #endif //_MSC_VER && TARGET_X86
  9326| #ifdef USE_REGIONS
  9327| void get_initial_region(int gen, int hn, uint8_t** region_start, uint8_t** region_end)
  9328| {
  9329|     *region_start = initial_regions[hn][gen][0];
  9330|     *region_end = initial_regions[hn][gen][1];
  9331| }
  9332| bool gc_heap::initial_make_soh_regions (gc_heap* hp)
  9333| {
  9334|     uint8_t* region_start;
  9335|     uint8_t* region_end;
  9336|     uint32_t hn = 0;
  9337| #ifdef MULTIPLE_HEAPS
  9338|     hn = hp->heap_number;
  9339| #endif //MULTIPLE_HEAPS
  9340|     for (int i = max_generation; i >= 0; i--)
  9341|     {
  9342|         get_initial_region(i, hn, &region_start, &region_end);
  9343|         size_t region_size = region_end - region_start;
  9344|         heap_segment* current_region = make_heap_segment (region_start, region_size, hp, i);
  9345|         if (current_region == nullptr)
  9346|         {
  9347|             return false;
  9348|         }
  9349|         uint8_t* gen_start = heap_segment_mem (current_region);
  9350|         make_generation (i, current_region, gen_start);
  9351|         if (i == 0)
  9352|         {
  9353|             ephemeral_heap_segment = current_region;
  9354|             alloc_allocated = heap_segment_allocated (current_region);
  9355|         }
  9356|     }
  9357|     for (int i = max_generation; i >= 0; i--)
  9358|     {
  9359|         dprintf (REGIONS_LOG, ("h%d gen%d alloc seg is %p, start seg is %p (%p-%p)",
  9360|             heap_number, i, generation_allocation_segment (generation_of (i)),
  9361|             generation_start_segment (generation_of (i)),
  9362|             heap_segment_mem (generation_start_segment (generation_of (i))),
  9363|             heap_segment_allocated (generation_start_segment (generation_of (i)))));
  9364|     }
  9365|     return true;
  9366| }
  9367| bool gc_heap::initial_make_uoh_regions (int gen, gc_heap* hp)
  9368| {
  9369|     uint8_t* region_start;
  9370|     uint8_t* region_end;
  9371|     uint32_t hn = 0;
  9372| #ifdef MULTIPLE_HEAPS
  9373|     hn = hp->heap_number;
  9374| #endif //MULTIPLE_HEAPS
  9375|     get_initial_region(gen, hn, &region_start, &region_end);
  9376|     size_t region_size = region_end - region_start;
  9377|     heap_segment* uoh_region = make_heap_segment (region_start, region_size, hp, gen);
  9378|     if (uoh_region == nullptr)
  9379|     {
  9380|         return false;
  9381|     }
  9382|     uoh_region->flags |=
  9383|         (gen == loh_generation) ? heap_segment_flags_loh : heap_segment_flags_poh;
  9384|     uint8_t* gen_start = heap_segment_mem (uoh_region);
  9385|     make_generation (gen, uoh_region, gen_start);
  9386|     return true;
  9387| }
  9388| void gc_heap::clear_region_info (heap_segment* region)
  9389| {
  9390|     if (!heap_segment_uoh_p (region))
  9391|     {
  9392|         clear_brick_table (heap_segment_mem (region), heap_segment_reserved (region));
  9393|     }
  9394|     clear_card_for_addresses (get_region_start (region), heap_segment_reserved (region));
  9395| #ifdef BACKGROUND_GC
  9396|     ::record_changed_seg ((uint8_t*)region, heap_segment_reserved (region),
  9397|                         settings.gc_index, current_bgc_state,
  9398|                         seg_deleted);
  9399|     bgc_verify_mark_array_cleared (region);
  9400| #endif //BACKGROUND_GC
  9401| }
  9402| void gc_heap::return_free_region (heap_segment* region)
  9403| {
  9404|     gc_oh_num oh = heap_segment_oh (region);
  9405|     dprintf(3, ("commit-accounting:  from %d to free [%p, %p) for heap %d", oh, get_region_start (region), heap_segment_committed (region), heap_number));
  9406| #ifndef COMMITTED_BYTES_SHADOW
  9407|     if (heap_hard_limit)
  9408| #endif //!COMMITTED_BYTES_SHADOW
  9409|     {
  9410|         size_t committed = heap_segment_committed (region) - get_region_start (region);
  9411|         if (committed > 0)
  9412|         {
  9413|             check_commit_cs.Enter();
  9414|             assert (committed_by_oh[oh] >= committed);
  9415|             committed_by_oh[oh] -= committed;
  9416|             committed_by_oh[recorded_committed_free_bucket] += committed;
  9417| #if defined(_DEBUG) && defined(MULTIPLE_HEAPS)
  9418|             assert (committed_by_oh_per_heap[oh] >= committed);
  9419|             committed_by_oh_per_heap[oh] -= committed;
  9420| #endif // _DEBUG && MULTIPLE_HEAPS
  9421|             check_commit_cs.Leave();
  9422|         }
  9423|     }
  9424|     clear_region_info (region);
  9425|     region_free_list::add_region_descending (region, free_regions);
  9426|     uint8_t* region_start = get_region_start (region);
  9427|     uint8_t* region_end = heap_segment_reserved (region);
  9428|     int num_basic_regions = (int)((region_end - region_start) >> min_segment_size_shr);
  9429|     dprintf (REGIONS_LOG, ("RETURNING region %p (%d basic regions) to free",
  9430|         heap_segment_mem (region), num_basic_regions));
  9431|     for (int i = 0; i < num_basic_regions; i++)
  9432|     {
  9433|         uint8_t* basic_region_start = region_start + ((size_t)i << min_segment_size_shr);
  9434|         heap_segment* basic_region = get_region_info (basic_region_start);
  9435|         heap_segment_allocated (basic_region) = 0;
  9436| #ifdef MULTIPLE_HEAPS
  9437|         heap_segment_heap (basic_region) = 0;
  9438| #endif //MULTIPLE_HEAPS
  9439|     }
  9440| }
  9441| heap_segment* gc_heap::get_free_region (int gen_number, size_t size)
  9442| {
  9443|     heap_segment* region = 0;
  9444|     if (gen_number <= max_generation)
  9445|     {
  9446|         assert (size == 0);
  9447|         region = free_regions[basic_free_region].unlink_region_front();
  9448|     }
  9449|     else
  9450|     {
  9451|         const size_t LARGE_REGION_SIZE = global_region_allocator.get_large_region_alignment();
  9452|         assert (size >= LARGE_REGION_SIZE);
  9453|         if (size == LARGE_REGION_SIZE)
  9454|         {
  9455|             region = free_regions[large_free_region].unlink_region_front();
  9456|         }
  9457|         else
  9458|         {
  9459|             region = free_regions[huge_free_region].unlink_smallest_region (size);
  9460|             if (region == nullptr)
  9461|             {
  9462|                 if (settings.pause_mode == pause_no_gc)
  9463|                 {
  9464|                     assert (gc_lock.holding_thread != (Thread*)-1);
  9465|                 }
  9466|                 else
  9467|                 {
  9468|                     ASSERT_HOLDING_SPIN_LOCK(&gc_lock);
  9469|                 }
  9470|                 region = global_free_huge_regions.unlink_smallest_region (size);
  9471|             }
  9472|         }
  9473|     }
  9474|     if (region)
  9475|     {
  9476|         uint8_t* region_start = get_region_start (region);
  9477|         uint8_t* region_end = heap_segment_reserved (region);
  9478|         init_heap_segment (region, __this, region_start,
  9479|                            (region_end - region_start),
  9480|                            gen_number, true);
  9481|         gc_oh_num oh = gen_to_oh (gen_number);
  9482|         dprintf(3, ("commit-accounting:  from free to %d [%p, %p) for heap %d", oh, get_region_start (region), heap_segment_committed (region), heap_number));
  9483| #ifndef COMMITTED_BYTES_SHADOW
  9484|         if (heap_hard_limit)
  9485| #endif //!COMMITTED_BYTES_SHADOW
  9486|         {
  9487|             size_t committed = heap_segment_committed (region) - get_region_start (region);
  9488|             if (committed > 0)
  9489|             {
  9490|                 check_commit_cs.Enter();
  9491|                 committed_by_oh[oh] += committed;
  9492|                 assert (committed_by_oh[recorded_committed_free_bucket] >= committed);
  9493|                 committed_by_oh[recorded_committed_free_bucket] -= committed;
  9494| #if defined(_DEBUG) && defined(MULTIPLE_HEAPS)
  9495|                 committed_by_oh_per_heap[oh] += committed;
  9496| #endif // _DEBUG && MULTIPLE_HEAPS
  9497|                 check_commit_cs.Leave();
  9498|             }
  9499|         }
  9500|         dprintf (REGIONS_LOG, ("h%d GFR get region %zx (%p-%p) for gen%d",
  9501|             heap_number, (size_t)region,
  9502|             region_start, region_end,
  9503|             gen_number));
  9504|         assert (heap_segment_allocated(region) == heap_segment_mem (region));
  9505|     }
  9506|     else
  9507|     {
  9508|         region = allocate_new_region (__this, gen_number, (gen_number > max_generation), size);
  9509|     }
  9510|     if (region)
  9511|     {
  9512|         if (!init_table_for_region (gen_number, region))
  9513|         {
  9514|             region = 0;
  9515|         }
  9516|     }
  9517|     return region;
  9518| }
  9519| heap_segment* gc_heap::region_of (uint8_t* obj)
  9520| {
  9521|     size_t index = (size_t)obj >> gc_heap::min_segment_size_shr;
  9522|     seg_mapping* entry = &seg_mapping_table[index];
  9523|     return (heap_segment*)entry;
  9524| }
  9525| heap_segment* gc_heap::get_region_at_index (size_t index)
  9526| {
  9527|     index += (size_t)g_gc_lowest_address >> gc_heap::min_segment_size_shr;
  9528|     return (heap_segment*)(&seg_mapping_table[index]);
  9529| }
  9530| void gc_heap::check_seg_gen_num (heap_segment* seg)
  9531| {
  9532| #ifdef _DEBUG
  9533|     uint8_t* mem = heap_segment_mem (seg);
  9534|     if ((mem < g_gc_lowest_address) || (mem >= g_gc_highest_address))
  9535|     {
  9536|         GCToOSInterface::DebugBreak();
  9537|     }
  9538|     int alloc_seg_gen_num = get_region_gen_num (mem);
  9539|     int alloc_seg_plan_gen_num = get_region_plan_gen_num (mem);
  9540|     dprintf (3, ("seg %p->%p, num %d, %d",
  9541|         seg, mem, alloc_seg_gen_num, alloc_seg_plan_gen_num));
  9542| #endif //_DEBUG
  9543| }
  9544| int gc_heap::get_region_gen_num (heap_segment* region)
  9545| {
  9546|     return heap_segment_gen_num (region);
  9547| }
  9548| int gc_heap::get_region_gen_num (uint8_t* obj)
  9549| {
  9550|     size_t skewed_basic_region_index = get_skewed_basic_region_index_for_address (obj);
  9551|     int gen_num = map_region_to_generation_skewed[skewed_basic_region_index] & gc_heap::RI_GEN_MASK;
  9552|     assert ((soh_gen0 <= gen_num) && (gen_num <= soh_gen2));
  9553|     assert (gen_num == heap_segment_gen_num (region_of (obj)));
  9554|     return gen_num;
  9555| }
  9556| int gc_heap::get_region_plan_gen_num (uint8_t* obj)
  9557| {
  9558|     size_t skewed_basic_region_index = get_skewed_basic_region_index_for_address (obj);
  9559|     int plan_gen_num = map_region_to_generation_skewed[skewed_basic_region_index] >> gc_heap::RI_PLAN_GEN_SHR;
  9560|     assert ((soh_gen0 <= plan_gen_num) && (plan_gen_num <= soh_gen2));
  9561|     assert (plan_gen_num == heap_segment_plan_gen_num (region_of (obj)));
  9562|     return plan_gen_num;
  9563| }
  9564| bool gc_heap::is_region_demoted (uint8_t* obj)
  9565| {
  9566|     size_t skewed_basic_region_index = get_skewed_basic_region_index_for_address (obj);
  9567|     bool demoted_p = (map_region_to_generation_skewed[skewed_basic_region_index] & gc_heap::RI_DEMOTED) != 0;
  9568|     assert (demoted_p == heap_segment_demoted_p (region_of (obj)));
  9569|     return demoted_p;
  9570| }
  9571| static GCSpinLock write_barrier_spin_lock;
  9572| inline
  9573| void gc_heap::set_region_gen_num (heap_segment* region, int gen_num)
  9574| {
  9575|     assert (gen_num < (1 << (sizeof (uint8_t) * 8)));
  9576|     assert (gen_num >= 0);
  9577|     heap_segment_gen_num (region) = (uint8_t)gen_num;
  9578|     uint8_t* region_start = get_region_start (region);
  9579|     uint8_t* region_end = heap_segment_reserved (region);
  9580|     size_t region_index_start = get_basic_region_index_for_address (region_start);
  9581|     size_t region_index_end = get_basic_region_index_for_address (region_end);
  9582|     region_info entry = (region_info)((gen_num << RI_PLAN_GEN_SHR) | gen_num);
  9583|     for (size_t region_index = region_index_start; region_index < region_index_end; region_index++)
  9584|     {
  9585|         assert (gen_num <= max_generation);
  9586|         map_region_to_generation[region_index] = entry;
  9587|     }
  9588|     if (gen_num <= soh_gen1)
  9589|     {
  9590|         if ((region_start < ephemeral_low) || (ephemeral_high < region_end))
  9591|         {
  9592|             while (true)
  9593|             {
  9594|                 if (Interlocked::CompareExchange(&write_barrier_spin_lock.lock, 0, -1) < 0)
  9595|                     break;
  9596|                 if ((ephemeral_low <= region_start) && (region_end <= ephemeral_high))
  9597|                     return;
  9598|                 while (write_barrier_spin_lock.lock >= 0)
  9599|                 {
  9600|                     YieldProcessor();           // indicate to the processor that we are spinning
  9601|                 }
  9602|             }
  9603| #ifdef _DEBUG
  9604|             write_barrier_spin_lock.holding_thread = GCToEEInterface::GetThread();
  9605| #endif //_DEBUG
  9606|             if ((region_start < ephemeral_low) || (ephemeral_high < region_end))
  9607|             {
  9608|                 uint8_t* new_ephemeral_low = min (region_start, (uint8_t*)ephemeral_low);
  9609|                 uint8_t* new_ephemeral_high = max (region_end, (uint8_t*)ephemeral_high);
  9610|                 dprintf (REGIONS_LOG, ("about to set ephemeral_low = %p ephemeral_high = %p", new_ephemeral_low, new_ephemeral_high));
  9611|                 stomp_write_barrier_ephemeral (new_ephemeral_low, new_ephemeral_high,
  9612|                                                map_region_to_generation_skewed, (uint8_t)min_segment_size_shr);
  9613|                 if (ephemeral_low < new_ephemeral_low)
  9614|                     GCToOSInterface::DebugBreak ();
  9615|                 if (new_ephemeral_high < ephemeral_high)
  9616|                     GCToOSInterface::DebugBreak ();
  9617|                 ephemeral_low = new_ephemeral_low;
  9618|                 ephemeral_high = new_ephemeral_high;
  9619|                 dprintf (REGIONS_LOG, ("set ephemeral_low = %p ephemeral_high = %p", new_ephemeral_low, new_ephemeral_high));
  9620|             }
  9621|             else
  9622|             {
  9623|                 dprintf (REGIONS_LOG, ("leaving lock - no need to update ephemeral range [%p,%p[ for region [%p,%p]", (uint8_t*)ephemeral_low, (uint8_t*)ephemeral_high, region_start, region_end));
  9624|             }
  9625| #ifdef _DEBUG
  9626|             write_barrier_spin_lock.holding_thread = (Thread*)-1;
  9627| #endif //_DEBUG
  9628|             write_barrier_spin_lock.lock = -1;
  9629|         }
  9630|         else
  9631|         {
  9632|             dprintf (REGIONS_LOG, ("no need to update ephemeral range [%p,%p[ for region [%p,%p]", (uint8_t*)ephemeral_low, (uint8_t*)ephemeral_high, region_start, region_end));
  9633|         }
  9634|     }
  9635| }
  9636| inline
  9637| void gc_heap::set_region_plan_gen_num (heap_segment* region, int plan_gen_num, bool replace_p)
  9638| {
  9639|     int gen_num = heap_segment_gen_num (region);
  9640|     int supposed_plan_gen_num = get_plan_gen_num (gen_num);
  9641|     dprintf (REGIONS_LOG, ("h%d setting plan gen on %p->%p(was gen%d) to %d(should be: %d) %s",
  9642|         heap_number, region,
  9643|         heap_segment_mem (region),
  9644|         gen_num, plan_gen_num,
  9645|         supposed_plan_gen_num,
  9646|         ((plan_gen_num < supposed_plan_gen_num) ? "DEMOTED" : "ND")));
  9647|     region_info region_info_bits_to_set = (region_info)(plan_gen_num << RI_PLAN_GEN_SHR);
  9648|     if ((plan_gen_num < supposed_plan_gen_num) && (heap_segment_pinned_survived (region) != 0))
  9649|     {
  9650|         if (!settings.demotion)
  9651|         {
  9652|             settings.demotion = TRUE;
  9653|         }
  9654|         get_gc_data_per_heap()->set_mechanism_bit (gc_demotion_bit);
  9655|         region->flags |= heap_segment_flags_demoted;
  9656|         region_info_bits_to_set = (region_info)(region_info_bits_to_set | RI_DEMOTED);
  9657|     }
  9658|     else
  9659|     {
  9660|         region->flags &= ~heap_segment_flags_demoted;
  9661|     }
  9662|     if (replace_p)
  9663|     {
  9664|         int original_plan_gen_num = heap_segment_plan_gen_num (region);
  9665|         planned_regions_per_gen[original_plan_gen_num]--;
  9666|     }
  9667|     planned_regions_per_gen[plan_gen_num]++;
  9668|     dprintf (REGIONS_LOG, ("h%d g%d %zx(%zx) -> g%d (total %d region planned in g%d)",
  9669|         heap_number, heap_segment_gen_num (region), (size_t)region, heap_segment_mem (region), plan_gen_num, planned_regions_per_gen[plan_gen_num], plan_gen_num));
  9670|     heap_segment_plan_gen_num (region) = plan_gen_num;
  9671|     uint8_t* region_start = get_region_start (region);
  9672|     uint8_t* region_end = heap_segment_reserved (region);
  9673|     size_t region_index_start = get_basic_region_index_for_address (region_start);
  9674|     size_t region_index_end = get_basic_region_index_for_address (region_end);
  9675|     for (size_t region_index = region_index_start; region_index < region_index_end; region_index++)
  9676|     {
  9677|         assert (plan_gen_num <= max_generation);
  9678|         map_region_to_generation[region_index] = (region_info)(region_info_bits_to_set | (map_region_to_generation[region_index] & ~(RI_PLAN_GEN_MASK|RI_DEMOTED)));
  9679|     }
  9680| }
  9681| inline
  9682| void gc_heap::set_region_plan_gen_num_sip (heap_segment* region, int plan_gen_num)
  9683| {
  9684|     if (!heap_segment_swept_in_plan (region))
  9685|     {
  9686|         set_region_plan_gen_num (region, plan_gen_num);
  9687|     }
  9688| }
  9689| void gc_heap::set_region_sweep_in_plan (heap_segment*region)
  9690| {
  9691|     heap_segment_swept_in_plan (region) = true;
  9692|     assert (get_region_size (region) == global_region_allocator.get_region_alignment());
  9693|     uint8_t* region_start = get_region_start (region);
  9694|     size_t region_index = get_basic_region_index_for_address (region_start);
  9695|     map_region_to_generation[region_index] = (region_info)(map_region_to_generation[region_index] | RI_SIP);
  9696| }
  9697| void gc_heap::clear_region_sweep_in_plan (heap_segment*region)
  9698| {
  9699|     heap_segment_swept_in_plan (region) = false;
  9700|     assert (get_region_size (region) == global_region_allocator.get_region_alignment());
  9701|     uint8_t* region_start = get_region_start (region);
  9702|     size_t region_index = get_basic_region_index_for_address (region_start);
  9703|     map_region_to_generation[region_index] = (region_info)(map_region_to_generation[region_index] & ~RI_SIP);
  9704| }
  9705| void gc_heap::clear_region_demoted (heap_segment* region)
  9706| {
  9707|     region->flags &= ~heap_segment_flags_demoted;
  9708|     assert (get_region_size (region) == global_region_allocator.get_region_alignment());
  9709|     uint8_t* region_start = get_region_start (region);
  9710|     size_t region_index = get_basic_region_index_for_address (region_start);
  9711|     map_region_to_generation[region_index] = (region_info)(map_region_to_generation[region_index] & ~RI_DEMOTED);
  9712| }
  9713| #endif //USE_REGIONS
  9714| int gc_heap::get_plan_gen_num (int gen_number)
  9715| {
  9716|     return ((settings.promotion) ? min ((gen_number + 1), max_generation) : gen_number);
  9717| }
  9718| uint8_t* gc_heap::get_uoh_start_object (heap_segment* region, generation* gen)
  9719| {
  9720| #ifdef USE_REGIONS
  9721|     uint8_t* o = heap_segment_mem (region);
  9722| #else
  9723|     uint8_t* o = generation_allocation_start (gen);
  9724|     assert(((CObjectHeader*)o)->IsFree());
  9725|     size_t s = Align (size (o), get_alignment_constant (FALSE));
  9726|     assert (s == AlignQword (min_obj_size));
  9727|     o += s;
  9728| #endif //USE_REGIONS
  9729|     return o;
  9730| }
  9731| uint8_t* gc_heap::get_soh_start_object (heap_segment* region, generation* gen)
  9732| {
  9733| #ifdef USE_REGIONS
  9734|     uint8_t* o             = heap_segment_mem (region);
  9735| #else
  9736|     uint8_t* o             = generation_allocation_start (gen);
  9737| #endif //USE_REGIONS
  9738|     return o;
  9739| }
  9740| size_t gc_heap::get_soh_start_obj_len (uint8_t* start_obj)
  9741| {
  9742| #ifdef USE_REGIONS
  9743|     return 0;
  9744| #else
  9745|     return Align (size (start_obj));
  9746| #endif //USE_REGIONS
  9747| }
  9748| void gc_heap::clear_gen1_cards()
  9749| {
  9750| #if defined(_DEBUG) && !defined(USE_REGIONS)
  9751|     for (int x = 0; x <= max_generation; x++)
  9752|     {
  9753|         assert (generation_allocation_start (generation_of (x)));
  9754|     }
  9755| #endif //_DEBUG && !USE_REGIONS
  9756|     if (!settings.demotion && settings.promotion)
  9757|     {
  9758| #ifdef USE_REGIONS
  9759|         heap_segment* region = generation_start_segment (generation_of (1));
  9760|         while (region)
  9761|         {
  9762|             clear_card_for_addresses (get_region_start (region), heap_segment_reserved (region));
  9763|             region = heap_segment_next (region);
  9764|         }
  9765| #else //USE_REGIONS
  9766|         clear_card_for_addresses (
  9767|             generation_allocation_start (generation_of (1)),
  9768|             generation_allocation_start (generation_of (0)));
  9769| #endif //USE_REGIONS
  9770| #ifdef _DEBUG
  9771|         uint8_t* start = get_soh_start_object (ephemeral_heap_segment, youngest_generation);
  9772|         assert (heap_segment_allocated (ephemeral_heap_segment) ==
  9773|                 (start + get_soh_start_obj_len (start)));
  9774| #endif //_DEBUG
  9775|     }
  9776| }
  9777| heap_segment* gc_heap::make_heap_segment (uint8_t* new_pages, size_t size, gc_heap* hp, int gen_num)
  9778| {
  9779|     gc_oh_num oh = gen_to_oh (gen_num);
  9780|     size_t initial_commit = use_large_pages_p ? size : SEGMENT_INITIAL_COMMIT;
  9781|     int h_number =
  9782| #ifdef MULTIPLE_HEAPS
  9783|         hp->heap_number;
  9784| #else
  9785|         0;
  9786| #endif //MULTIPLE_HEAPS
  9787|     if (!virtual_commit (new_pages, initial_commit, oh, h_number))
  9788|     {
  9789|         return 0;
  9790|     }
  9791| #ifdef USE_REGIONS
  9792|     dprintf (REGIONS_LOG, ("Making region %p->%p(%zdmb)",
  9793|         new_pages, (new_pages + size), (size / 1024 / 1024)));
  9794|     heap_segment* new_segment = get_region_info (new_pages);
  9795|     uint8_t* start = new_pages + sizeof (aligned_plug_and_gap);
  9796| #else
  9797|     heap_segment* new_segment = (heap_segment*)new_pages;
  9798|     uint8_t* start = new_pages + segment_info_size;
  9799| #endif //USE_REGIONS
  9800|     heap_segment_mem (new_segment) = start;
  9801|     heap_segment_used (new_segment) = start;
  9802|     heap_segment_reserved (new_segment) = new_pages + size;
  9803|     heap_segment_committed (new_segment) = new_pages + initial_commit;
  9804|     init_heap_segment (new_segment, hp
  9805| #ifdef USE_REGIONS
  9806|                        , new_pages, size, gen_num
  9807| #endif //USE_REGIONS
  9808|                        );
  9809|     dprintf (2, ("Creating heap segment %zx", (size_t)new_segment));
  9810|     return new_segment;
  9811| }
  9812| void gc_heap::init_heap_segment (heap_segment* seg, gc_heap* hp
  9813| #ifdef USE_REGIONS
  9814|                                  , uint8_t* start, size_t size, int gen_num, bool existing_region_p
  9815| #endif //USE_REGIONS
  9816|     )
  9817| {
  9818| #ifndef USE_REGIONS
  9819|     bool existing_region_p = false;
  9820| #endif //!USE_REGIONS
  9821| #ifdef BACKGROUND_GC
  9822|     seg->flags = existing_region_p ? (seg->flags & heap_segment_flags_ma_committed) : 0;
  9823| #else
  9824|     seg->flags = 0;
  9825| #endif
  9826|     heap_segment_next (seg) = 0;
  9827|     heap_segment_plan_allocated (seg) = heap_segment_mem (seg);
  9828|     heap_segment_allocated (seg) = heap_segment_mem (seg);
  9829|     heap_segment_saved_allocated (seg) = heap_segment_mem (seg);
  9830|     heap_segment_decommit_target (seg) = heap_segment_reserved (seg);
  9831| #ifdef BACKGROUND_GC
  9832|     heap_segment_background_allocated (seg) = 0;
  9833|     heap_segment_saved_bg_allocated (seg) = 0;
  9834| #endif //BACKGROUND_GC
  9835| #ifdef MULTIPLE_HEAPS
  9836|     heap_segment_heap (seg) = hp;
  9837| #endif //MULTIPLE_HEAPS
  9838| #ifdef USE_REGIONS
  9839|     int gen_num_for_region = min (gen_num, max_generation);
  9840|     set_region_gen_num (seg, gen_num_for_region);
  9841|     heap_segment_plan_gen_num (seg) = gen_num_for_region;
  9842|     heap_segment_swept_in_plan (seg) = false;
  9843| #endif //USE_REGIONS
  9844| #ifdef USE_REGIONS
  9845|     int num_basic_regions = (int)(size >> min_segment_size_shr);
  9846|     size_t basic_region_size = (size_t)1 << min_segment_size_shr;
  9847|     dprintf (REGIONS_LOG, ("this region contains %d basic regions", num_basic_regions));
  9848|     if (num_basic_regions > 1)
  9849|     {
  9850|         for (int i = 1; i < num_basic_regions; i++)
  9851|         {
  9852|             uint8_t* basic_region_start = start + (i * basic_region_size);
  9853|             heap_segment* basic_region = get_region_info (basic_region_start);
  9854|             heap_segment_allocated (basic_region) = (uint8_t*)(ptrdiff_t)-i;
  9855|             dprintf (REGIONS_LOG, ("Initing basic region %p->%p(%zdmb) alloc to %p",
  9856|                 basic_region_start, (basic_region_start + basic_region_size),
  9857|                 (size_t)(basic_region_size / 1024 / 1024),
  9858|                 heap_segment_allocated (basic_region)));
  9859|             heap_segment_gen_num (basic_region) = (uint8_t)gen_num_for_region;
  9860|             heap_segment_plan_gen_num (basic_region) = gen_num_for_region;
  9861| #ifdef MULTIPLE_HEAPS
  9862|             heap_segment_heap (basic_region) = hp;
  9863| #endif //MULTIPLE_HEAPS
  9864|         }
  9865|     }
  9866| #endif //USE_REGIONS
  9867| }
  9868| void gc_heap::delete_heap_segment (heap_segment* seg, BOOL consider_hoarding)
  9869| {
  9870|     if (!heap_segment_uoh_p (seg))
  9871|     {
  9872|         clear_brick_table (heap_segment_mem (seg), heap_segment_reserved (seg));
  9873|     }
  9874| #ifdef USE_REGIONS
  9875|     return_free_region (seg);
  9876| #else // USE_REGIONS
  9877|     if (consider_hoarding)
  9878|     {
  9879|         assert ((heap_segment_mem (seg) - (uint8_t*)seg) <= ptrdiff_t(2*OS_PAGE_SIZE));
  9880|         size_t ss = (size_t) (heap_segment_reserved (seg) - (uint8_t*)seg);
  9881|         if (ss <= INITIAL_ALLOC)
  9882|         {
  9883|             dprintf (2, ("Hoarding segment %zx", (size_t)seg));
  9884| #ifdef BACKGROUND_GC
  9885|             if (!heap_segment_decommitted_p (seg))
  9886| #endif //BACKGROUND_GC
  9887|             {
  9888|                 decommit_heap_segment (seg);
  9889|             }
  9890|             seg_mapping_table_remove_segment (seg);
  9891|             heap_segment_next (seg) = segment_standby_list;
  9892|             segment_standby_list = seg;
  9893|             seg = 0;
  9894|         }
  9895|     }
  9896|     if (seg != 0)
  9897|     {
  9898|         dprintf (2, ("h%d: del seg: [%zx, %zx[",
  9899|                      heap_number, (size_t)seg,
  9900|                      (size_t)(heap_segment_reserved (seg))));
  9901| #ifdef BACKGROUND_GC
  9902|         ::record_changed_seg ((uint8_t*)seg, heap_segment_reserved (seg),
  9903|                             settings.gc_index, current_bgc_state,
  9904|                             seg_deleted);
  9905|         bgc_verify_mark_array_cleared (seg);
  9906|         decommit_mark_array_by_seg (seg);
  9907| #endif //BACKGROUND_GC
  9908|         seg_mapping_table_remove_segment (seg);
  9909|         release_segment (seg);
  9910|     }
  9911| #endif //USE_REGIONS
  9912| }
  9913| void gc_heap::reset_heap_segment_pages (heap_segment* seg)
  9914| {
  9915|     size_t page_start = align_on_page ((size_t)heap_segment_allocated (seg));
  9916|     size_t size = (size_t)heap_segment_committed (seg) - page_start;
  9917|     if (size != 0)
  9918|         GCToOSInterface::VirtualReset((void*)page_start, size, false /* unlock */);
  9919| }
  9920| void gc_heap::decommit_heap_segment_pages (heap_segment* seg,
  9921|                                            size_t extra_space)
  9922| {
  9923|     if (use_large_pages_p)
  9924|         return;
  9925|     uint8_t*  page_start = align_on_page (heap_segment_allocated(seg));
  9926|     assert (heap_segment_committed (seg) >= page_start);
  9927|     size_t size = heap_segment_committed (seg) - page_start;
  9928|     extra_space = align_on_page (extra_space);
  9929|     if (size >= max ((extra_space + 2*OS_PAGE_SIZE), MIN_DECOMMIT_SIZE))
  9930|     {
  9931|         page_start += max(extra_space, 32*OS_PAGE_SIZE);
  9932|         decommit_heap_segment_pages_worker (seg, page_start);
  9933|     }
  9934| }
  9935| size_t gc_heap::decommit_heap_segment_pages_worker (heap_segment* seg,
  9936|                                                     uint8_t* new_committed)
  9937| {
  9938|     assert (!use_large_pages_p);
  9939|     uint8_t* page_start = align_on_page (new_committed);
  9940|     ptrdiff_t size = heap_segment_committed (seg) - page_start;
  9941|     if (size > 0)
  9942|     {
  9943|         bool decommit_succeeded_p = virtual_decommit (page_start, (size_t)size, heap_segment_oh (seg), heap_number);
  9944|         if (decommit_succeeded_p)
  9945|         {
  9946|             dprintf (3, ("Decommitting heap segment [%zx, %zx[(%zd)",
  9947|                 (size_t)page_start,
  9948|                 (size_t)(page_start + size),
  9949|                 size));
  9950|             heap_segment_committed (seg) = page_start;
  9951|             if (heap_segment_used (seg) > heap_segment_committed (seg))
  9952|             {
  9953|                 heap_segment_used (seg) = heap_segment_committed (seg);
  9954|             }
  9955|         }
  9956|         else
  9957|         {
  9958|             dprintf (3, ("Decommitting heap segment failed"));
  9959|         }
  9960|     }
  9961|     return size;
  9962| }
  9963| void gc_heap::decommit_heap_segment (heap_segment* seg)
  9964| {
  9965| #ifdef USE_REGIONS
  9966|     if (!dt_high_memory_load_p())
  9967|     {
  9968|         return;
  9969|     }
  9970| #endif
  9971|     uint8_t*  page_start = align_on_page (heap_segment_mem (seg));
  9972|     dprintf (3, ("Decommitting heap segment %zx(%p)", (size_t)seg, heap_segment_mem (seg)));
  9973| #if defined(BACKGROUND_GC) && !defined(USE_REGIONS)
  9974|     page_start += OS_PAGE_SIZE;
  9975| #endif //BACKGROUND_GC && !USE_REGIONS
  9976|     assert (heap_segment_committed (seg) >= page_start);
  9977|     size_t size = heap_segment_committed (seg) - page_start;
  9978|     bool decommit_succeeded_p = virtual_decommit (page_start, size, heap_segment_oh (seg), heap_number);
  9979|     if (decommit_succeeded_p)
  9980|     {
  9981|         heap_segment_committed (seg) = page_start;
  9982|         if (heap_segment_used (seg) > heap_segment_committed (seg))
  9983|         {
  9984|             heap_segment_used (seg) = heap_segment_committed (seg);
  9985|         }
  9986|     }
  9987| }
  9988| void gc_heap::clear_gen0_bricks()
  9989| {
  9990|     if (!gen0_bricks_cleared)
  9991|     {
  9992|         gen0_bricks_cleared = TRUE;
  9993| #ifdef USE_REGIONS
  9994|         heap_segment* gen0_region = generation_start_segment (generation_of (0));
  9995|         while (gen0_region)
  9996|         {
  9997|             uint8_t* clear_start = heap_segment_mem (gen0_region);
  9998| #else
  9999|         heap_segment* gen0_region = ephemeral_heap_segment;
 10000|         uint8_t* clear_start = generation_allocation_start (generation_of (0));
 10001|         {
 10002| #endif //USE_REGIONS
 10003|             for (size_t b = brick_of (clear_start);
 10004|                     b < brick_of (align_on_brick
 10005|                                 (heap_segment_allocated (gen0_region)));
 10006|                     b++)
 10007|             {
 10008|                 set_brick (b, -1);
 10009|             }
 10010| #ifdef USE_REGIONS
 10011|             gen0_region = heap_segment_next (gen0_region);
 10012| #endif //USE_REGIONS
 10013|         }
 10014|     }
 10015| }
 10016| void gc_heap::check_gen0_bricks()
 10017| {
 10018|     if (gen0_bricks_cleared)
 10019|     {
 10020| #ifdef USE_REGIONS
 10021|         heap_segment* gen0_region = generation_start_segment (generation_of (0));
 10022|         while (gen0_region)
 10023|         {
 10024|             uint8_t* start = heap_segment_mem (gen0_region);
 10025| #else
 10026|         heap_segment* gen0_region = ephemeral_heap_segment;
 10027|         uint8_t* start = generation_allocation_start (generation_of (0));
 10028|         {
 10029| #endif //USE_REGIONS
 10030|             size_t end_b = brick_of (heap_segment_allocated (gen0_region));
 10031|             for (size_t b = brick_of (start); b < end_b; b++)
 10032|             {
 10033|                 assert (brick_table[b] != 0);
 10034|                 if (brick_table[b] == 0)
 10035|                 {
 10036|                     GCToOSInterface::DebugBreak();
 10037|                 }
 10038|             }
 10039| #ifdef USE_REGIONS
 10040|             gen0_region = heap_segment_next (gen0_region);
 10041| #endif //USE_REGIONS
 10042|         }
 10043|     }
 10044| }
 10045| #ifdef BACKGROUND_GC
 10046| void gc_heap::rearrange_small_heap_segments()
 10047| {
 10048|     heap_segment* seg = freeable_soh_segment;
 10049|     while (seg)
 10050|     {
 10051|         heap_segment* next_seg = heap_segment_next (seg);
 10052|         delete_heap_segment (seg, FALSE);
 10053|         seg = next_seg;
 10054|     }
 10055|     freeable_soh_segment = 0;
 10056| }
 10057| #endif //BACKGROUND_GC
 10058| void gc_heap::rearrange_uoh_segments()
 10059| {
 10060|     dprintf (2, ("deleting empty large segments"));
 10061|     heap_segment* seg = freeable_uoh_segment;
 10062|     while (seg)
 10063|     {
 10064|         heap_segment* next_seg = heap_segment_next (seg);
 10065|         delete_heap_segment (seg, GCConfig::GetRetainVM());
 10066|         seg = next_seg;
 10067|     }
 10068|     freeable_uoh_segment = 0;
 10069| }
 10070| void gc_heap::delay_free_segments()
 10071| {
 10072|     rearrange_uoh_segments();
 10073| #ifdef BACKGROUND_GC
 10074|     background_delay_delete_uoh_segments();
 10075|     if (!gc_heap::background_running_p())
 10076|         rearrange_small_heap_segments();
 10077| #endif //BACKGROUND_GC
 10078| }
 10079| #ifndef USE_REGIONS
 10080| void gc_heap::rearrange_heap_segments(BOOL compacting)
 10081| {
 10082|     heap_segment* seg =
 10083|         generation_start_segment (generation_of (max_generation));
 10084|     heap_segment* prev_seg = 0;
 10085|     heap_segment* next_seg = 0;
 10086|     while (seg)
 10087|     {
 10088|         next_seg = heap_segment_next (seg);
 10089|         if ((next_seg == 0) && (seg != ephemeral_heap_segment))
 10090|         {
 10091|             seg->next = ephemeral_heap_segment;
 10092|             next_seg = heap_segment_next (seg);
 10093|         }
 10094|         if ((seg == ephemeral_heap_segment) && next_seg)
 10095|         {
 10096|             heap_segment_next (prev_seg) = next_seg;
 10097|             heap_segment_next (seg) = 0;
 10098|         }
 10099|         else
 10100|         {
 10101|             uint8_t* end_segment = (compacting ?
 10102|                                  heap_segment_plan_allocated (seg) :
 10103|                                  heap_segment_allocated (seg));
 10104|             if ((end_segment == heap_segment_mem (seg))&&
 10105|                 !heap_segment_read_only_p (seg))
 10106|             {
 10107|                 assert (prev_seg);
 10108|                 assert (seg != ephemeral_heap_segment);
 10109|                 heap_segment_next (prev_seg) = next_seg;
 10110|                 delete_heap_segment (seg, GCConfig::GetRetainVM());
 10111|                 dprintf (2, ("Deleting heap segment %zx", (size_t)seg));
 10112|             }
 10113|             else
 10114|             {
 10115|                 if (!heap_segment_read_only_p (seg))
 10116|                 {
 10117|                     if (compacting)
 10118|                     {
 10119|                         heap_segment_allocated (seg) =
 10120|                             heap_segment_plan_allocated (seg);
 10121|                     }
 10122|                     if (seg != ephemeral_heap_segment)
 10123|                     {
 10124|                         decommit_heap_segment_pages (seg, 0);
 10125|                     }
 10126|                 }
 10127|                 prev_seg = seg;
 10128|             }
 10129|         }
 10130|         seg = next_seg;
 10131|     }
 10132| }
 10133| #endif //!USE_REGIONS
 10134| #if defined(USE_REGIONS)
 10135| static void remove_surplus_regions (region_free_list* free_list, region_free_list* surplus_list, size_t target_count)
 10136| {
 10137|     while (free_list->get_num_free_regions() > target_count)
 10138|     {
 10139|         heap_segment* region = free_list->unlink_region_front();
 10140|         surplus_list->add_region_front (region);
 10141|     }
 10142| }
 10143| static int64_t add_regions (region_free_list* free_list, region_free_list* surplus_list, size_t target_count)
 10144| {
 10145|     int64_t added_count = 0;
 10146|     while (free_list->get_num_free_regions() < target_count)
 10147|     {
 10148|         if (surplus_list->get_num_free_regions() == 0)
 10149|             break;
 10150|         added_count++;
 10151|         heap_segment* region = surplus_list->unlink_region_front();
 10152|         free_list->add_region_front (region);
 10153|     }
 10154|     return added_count;
 10155| }
 10156| region_free_list::region_free_list() : num_free_regions (0),
 10157|                                        size_free_regions (0),
 10158|                                        size_committed_in_free_regions (0),
 10159|                                        num_free_regions_added (0),
 10160|                                        num_free_regions_removed (0),
 10161|                                        head_free_region (nullptr),
 10162|                                        tail_free_region (nullptr)
 10163| {
 10164| }
 10165| void region_free_list::verify (bool empty_p)
 10166| {
 10167| #ifdef _DEBUG
 10168|     assert ((num_free_regions == 0) == empty_p);
 10169|     assert ((size_free_regions == 0) == empty_p);
 10170|     assert ((size_committed_in_free_regions == 0) == empty_p);
 10171|     assert ((head_free_region == nullptr) == empty_p);
 10172|     assert ((tail_free_region == nullptr) == empty_p);
 10173|     assert (num_free_regions == (num_free_regions_added - num_free_regions_removed));
 10174|     if (!empty_p)
 10175|     {
 10176|         assert (heap_segment_next (tail_free_region) == nullptr);
 10177|         assert (heap_segment_prev_free_region (head_free_region) == nullptr);
 10178|         size_t actual_count = 0;
 10179|         heap_segment* last_region = nullptr;
 10180|         for (heap_segment* region = head_free_region; region != nullptr; region = heap_segment_next(region))
 10181|         {
 10182|             last_region = region;
 10183|             actual_count++;
 10184|         }
 10185|         assert (num_free_regions == actual_count);
 10186|         assert (last_region == tail_free_region);
 10187|         heap_segment* first_region = nullptr;
 10188|         for (heap_segment* region = tail_free_region; region != nullptr; region = heap_segment_prev_free_region(region))
 10189|         {
 10190|             first_region = region;
 10191|             actual_count--;
 10192|         }
 10193|         assert (actual_count == 0);
 10194|         assert (head_free_region == first_region);
 10195|     }
 10196| #endif
 10197| }
 10198| void region_free_list::reset()
 10199| {
 10200|     num_free_regions = 0;
 10201|     size_free_regions = 0;
 10202|     size_committed_in_free_regions = 0;
 10203|     head_free_region = nullptr;
 10204|     tail_free_region = nullptr;
 10205| }
 10206| inline
 10207| void region_free_list::update_added_region_info (heap_segment* region)
 10208| {
 10209|     num_free_regions++;
 10210|     num_free_regions_added++;
 10211|     size_t region_size = get_region_size (region);
 10212|     size_free_regions += region_size;
 10213|     size_t region_committed_size = get_region_committed_size (region);
 10214|     size_committed_in_free_regions += region_committed_size;
 10215|     verify (false);
 10216| }
 10217| void region_free_list::add_region_front (heap_segment* region)
 10218| {
 10219|     assert (heap_segment_containing_free_list (region) == nullptr);
 10220|     heap_segment_containing_free_list(region) = this;
 10221|     if (head_free_region != nullptr)
 10222|     {
 10223|         heap_segment_prev_free_region(head_free_region) = region;
 10224|         assert (tail_free_region != nullptr);
 10225|     }
 10226|     else
 10227|     {
 10228|         tail_free_region = region;
 10229|     }
 10230|     heap_segment_next (region) = head_free_region;
 10231|     head_free_region = region;
 10232|     heap_segment_prev_free_region (region) = nullptr;
 10233|     update_added_region_info (region);
 10234| }
 10235| void region_free_list::add_region_in_descending_order (heap_segment* region_to_add)
 10236| {
 10237|     assert (heap_segment_containing_free_list (region_to_add) == nullptr);
 10238|     heap_segment_containing_free_list (region_to_add) = this;
 10239|     heap_segment_age_in_free (region_to_add) = 0;
 10240|     heap_segment* prev_region = nullptr;
 10241|     heap_segment* region = nullptr;
 10242|     if (heap_segment_committed (region_to_add) == heap_segment_reserved (region_to_add))
 10243|     {
 10244|         region = head_free_region;
 10245|     }
 10246|     else
 10247|     {
 10248|         size_t region_to_add_committed = get_region_committed_size (region_to_add);
 10249|         for (prev_region = tail_free_region; prev_region != nullptr; prev_region = heap_segment_prev_free_region (prev_region))
 10250|         {
 10251|             size_t prev_region_committed = get_region_committed_size (prev_region);
 10252|             if (prev_region_committed >= region_to_add_committed)
 10253|             {
 10254|                 break;
 10255|             }
 10256|             region = prev_region;
 10257|         }
 10258|     }
 10259|     if (prev_region != nullptr)
 10260|     {
 10261|         heap_segment_next (prev_region) = region_to_add;
 10262|     }
 10263|     else
 10264|     {
 10265|         assert (region == head_free_region);
 10266|         head_free_region = region_to_add;
 10267|     }
 10268|     heap_segment_prev_free_region (region_to_add) = prev_region;
 10269|     heap_segment_next (region_to_add) = region;
 10270|     if (region != nullptr)
 10271|     {
 10272|         heap_segment_prev_free_region (region) = region_to_add;
 10273|     }
 10274|     else
 10275|     {
 10276|         assert (prev_region == tail_free_region);
 10277|         tail_free_region = region_to_add;
 10278|     }
 10279|     update_added_region_info (region_to_add);
 10280| }
 10281| heap_segment* region_free_list::unlink_region_front()
 10282| {
 10283|     heap_segment* region = head_free_region;
 10284|     if (region != nullptr)
 10285|     {
 10286|         assert (heap_segment_containing_free_list (region) == this);
 10287|         unlink_region (region);
 10288|     }
 10289|     return region;
 10290| }
 10291| void region_free_list::unlink_region (heap_segment* region)
 10292| {
 10293|     region_free_list* rfl = heap_segment_containing_free_list (region);
 10294|     rfl->verify (false);
 10295|     heap_segment* prev = heap_segment_prev_free_region (region);
 10296|     heap_segment* next = heap_segment_next (region);
 10297|     if (prev != nullptr)
 10298|     {
 10299|         assert (region != rfl->head_free_region);
 10300|         assert (heap_segment_next (prev) == region);
 10301|         heap_segment_next (prev) = next;
 10302|     }
 10303|     else
 10304|     {
 10305|         assert (region == rfl->head_free_region);
 10306|         rfl->head_free_region = next;
 10307|     }
 10308|     if (next != nullptr)
 10309|     {
 10310|         assert (region != rfl->tail_free_region);
 10311|         assert (heap_segment_prev_free_region (next) == region);
 10312|         heap_segment_prev_free_region (next) = prev;
 10313|     }
 10314|     else
 10315|     {
 10316|         assert (region == rfl->tail_free_region);
 10317|         rfl->tail_free_region = prev;
 10318|     }
 10319|     heap_segment_containing_free_list (region) = nullptr;
 10320|     rfl->num_free_regions--;
 10321|     rfl->num_free_regions_removed++;
 10322|     size_t region_size = get_region_size (region);
 10323|     assert (rfl->size_free_regions >= region_size);
 10324|     rfl->size_free_regions -= region_size;
 10325|     size_t region_committed_size = get_region_committed_size (region);
 10326|     assert (rfl->size_committed_in_free_regions >= region_committed_size);
 10327|     rfl->size_committed_in_free_regions -= region_committed_size;
 10328| }
 10329| free_region_kind region_free_list::get_region_kind (heap_segment* region)
 10330| {
 10331|     const size_t BASIC_REGION_SIZE = global_region_allocator.get_region_alignment();
 10332|     const size_t LARGE_REGION_SIZE = global_region_allocator.get_large_region_alignment();
 10333|     size_t region_size = get_region_size (region);
 10334|     if (region_size == BASIC_REGION_SIZE)
 10335|         return basic_free_region;
 10336|     else if (region_size == LARGE_REGION_SIZE)
 10337|         return large_free_region;
 10338|     else
 10339|     {
 10340|         assert(region_size > LARGE_REGION_SIZE);
 10341|         return huge_free_region;
 10342|     }
 10343| }
 10344| heap_segment* region_free_list::unlink_smallest_region (size_t minimum_size)
 10345| {
 10346|     verify (num_free_regions == 0);
 10347|     heap_segment* smallest_region = nullptr;
 10348|     size_t smallest_size = (size_t)-1;
 10349|     for (heap_segment* region = head_free_region; region != nullptr; region = heap_segment_next (region))
 10350|     {
 10351|         uint8_t* region_start = get_region_start(region);
 10352|         uint8_t* region_end = heap_segment_reserved(region);
 10353|         size_t region_size = get_region_size (region);
 10354|         const size_t LARGE_REGION_SIZE = global_region_allocator.get_large_region_alignment();
 10355|         assert (region_size >= LARGE_REGION_SIZE * 2);
 10356|         if (region_size >= minimum_size)
 10357|         {
 10358|             if (smallest_size > region_size)
 10359|             {
 10360|                 smallest_size = region_size;
 10361|                 smallest_region = region;
 10362|             }
 10363|             if (region_size == LARGE_REGION_SIZE * 2)
 10364|             {
 10365|                 assert (region == smallest_region);
 10366|                 break;
 10367|             }
 10368|         }
 10369|     }
 10370|     if (smallest_region != nullptr)
 10371|     {
 10372|         unlink_region (smallest_region);
 10373|         dprintf(REGIONS_LOG, ("get %p-%p-%p",
 10374|             heap_segment_mem(smallest_region), heap_segment_committed(smallest_region), heap_segment_used(smallest_region)));
 10375|     }
 10376|     return smallest_region;
 10377| }
 10378| void region_free_list::transfer_regions (region_free_list* from)
 10379| {
 10380|     this->verify (this->num_free_regions == 0);
 10381|     from->verify (from->num_free_regions == 0);
 10382|     if (from->num_free_regions == 0)
 10383|     {
 10384|         return;
 10385|     }
 10386|     if (num_free_regions == 0)
 10387|     {
 10388|         head_free_region = from->head_free_region;
 10389|         tail_free_region = from->tail_free_region;
 10390|     }
 10391|     else
 10392|     {
 10393|         heap_segment* this_tail = tail_free_region;
 10394|         heap_segment* from_head = from->head_free_region;
 10395|         heap_segment_next (this_tail) = from_head;
 10396|         heap_segment_prev_free_region (from_head) = this_tail;
 10397|         tail_free_region = from->tail_free_region;
 10398|     }
 10399|     for (heap_segment* region = from->head_free_region; region != nullptr; region = heap_segment_next (region))
 10400|     {
 10401|         heap_segment_containing_free_list (region) = this;
 10402|     }
 10403|     num_free_regions += from->num_free_regions;
 10404|     num_free_regions_added += from->num_free_regions;
 10405|     size_free_regions += from->size_free_regions;
 10406|     size_committed_in_free_regions += from->size_committed_in_free_regions;
 10407|     from->num_free_regions_removed += from->num_free_regions;
 10408|     from->reset();
 10409|     verify (false);
 10410| }
 10411| size_t region_free_list::get_num_free_regions()
 10412| {
 10413| #ifdef _DEBUG
 10414|     verify (num_free_regions == 0);
 10415| #endif //_DEBUG
 10416|     return num_free_regions;
 10417| }
 10418| void region_free_list::add_region (heap_segment* region, region_free_list to_free_list[count_free_region_kinds])
 10419| {
 10420|     free_region_kind kind = get_region_kind (region);
 10421|     to_free_list[kind].add_region_front (region);
 10422| }
 10423| void region_free_list::add_region_descending (heap_segment* region, region_free_list to_free_list[count_free_region_kinds])
 10424| {
 10425|     free_region_kind kind = get_region_kind (region);
 10426|     to_free_list[kind].add_region_in_descending_order (region);
 10427| }
 10428| bool region_free_list::is_on_free_list (heap_segment* region, region_free_list free_list[count_free_region_kinds])
 10429| {
 10430|     region_free_list* rfl = heap_segment_containing_free_list (region);
 10431|     free_region_kind kind = get_region_kind (region);
 10432|     return rfl == &free_list[kind];
 10433| }
 10434| void region_free_list::age_free_regions()
 10435| {
 10436|     for (heap_segment* region = head_free_region; region != nullptr; region = heap_segment_next (region))
 10437|     {
 10438|         if (heap_segment_age_in_free (region) < MAX_AGE_IN_FREE)
 10439|             heap_segment_age_in_free (region)++;
 10440|     }
 10441| }
 10442| void region_free_list::age_free_regions (region_free_list free_lists[count_free_region_kinds])
 10443| {
 10444|     for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
 10445|     {
 10446|         free_lists[kind].age_free_regions();
 10447|     }
 10448| }
 10449| void region_free_list::print (int hn, const char* msg, int* ages)
 10450| {
 10451|     dprintf (3, ("h%2d PRINTING-------------------------------", hn));
 10452|     for (heap_segment* region = head_free_region; region != nullptr; region = heap_segment_next (region))
 10453|     {
 10454|         if (ages)
 10455|         {
 10456|             ages[heap_segment_age_in_free (region)]++;
 10457|         }
 10458|         dprintf (3, ("[%s] h%2d age %d region %p (%zd)%s",
 10459|             msg, hn, (int)heap_segment_age_in_free (region),
 10460|             heap_segment_mem (region), get_region_committed_size (region),
 10461|             ((heap_segment_committed (region) == heap_segment_reserved (region)) ? "(FC)" : "")));
 10462|     }
 10463|     dprintf (3, ("h%2d PRINTING END-------------------------------", hn));
 10464| }
 10465| void region_free_list::print (region_free_list free_lists[count_free_region_kinds], int hn, const char* msg, int* ages)
 10466| {
 10467|     for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
 10468|     {
 10469|         free_lists[kind].print (hn, msg, ages);
 10470|     }
 10471| }
 10472| static int compare_by_committed_and_age (heap_segment* l, heap_segment* r)
 10473| {
 10474|     size_t l_committed = get_region_committed_size (l);
 10475|     size_t r_committed = get_region_committed_size (r);
 10476|     if (l_committed > r_committed)
 10477|         return -1;
 10478|     else if (l_committed < r_committed)
 10479|         return 1;
 10480|     int l_age = heap_segment_age_in_free (l);
 10481|     int r_age = heap_segment_age_in_free (r);
 10482|     return (l_age - r_age);
 10483| }
 10484| static heap_segment* merge_sort_by_committed_and_age (heap_segment *head, size_t count)
 10485| {
 10486|     if (count <= 1)
 10487|         return head;
 10488|     size_t half = count / 2;
 10489|     heap_segment* mid = nullptr;
 10490|     size_t i = 0;
 10491|     for (heap_segment *region = head; region != nullptr; region = heap_segment_next (region))
 10492|     {
 10493|         i++;
 10494|         if (i == half)
 10495|         {
 10496|             mid = heap_segment_next (region);
 10497|             heap_segment_next (region) = nullptr;
 10498|             break;
 10499|         }
 10500|     }
 10501|     head = merge_sort_by_committed_and_age (head, half);
 10502|     mid = merge_sort_by_committed_and_age (mid, count - half);
 10503|     heap_segment* new_head;
 10504|     if (compare_by_committed_and_age (head, mid) <= 0)
 10505|     {
 10506|         new_head = head;
 10507|         head = heap_segment_next (head);
 10508|     }
 10509|     else
 10510|     {
 10511|         new_head = mid;
 10512|         mid = heap_segment_next (mid);
 10513|     }
 10514|     heap_segment* new_tail = new_head;
 10515|     while ((head != nullptr) && (mid != nullptr))
 10516|     {
 10517|         heap_segment* region = nullptr;
 10518|         if (compare_by_committed_and_age (head, mid) <= 0)
 10519|         {
 10520|             region = head;
 10521|             head = heap_segment_next (head);
 10522|         }
 10523|         else
 10524|         {
 10525|             region = mid;
 10526|             mid = heap_segment_next (mid);
 10527|         }
 10528|         heap_segment_next (new_tail) = region;
 10529|         new_tail = region;
 10530|     }
 10531|     if (head != nullptr)
 10532|     {
 10533|         assert (mid == nullptr);
 10534|         heap_segment_next (new_tail) = head;
 10535|     }
 10536|     else
 10537|     {
 10538|         heap_segment_next (new_tail) = mid;
 10539|     }
 10540|     return new_head;
 10541| }
 10542| void region_free_list::sort_by_committed_and_age()
 10543| {
 10544|     if (num_free_regions <= 1)
 10545|         return;
 10546|     heap_segment* new_head = merge_sort_by_committed_and_age (head_free_region, num_free_regions);
 10547|     head_free_region = new_head;
 10548|     heap_segment* prev = nullptr;
 10549|     for (heap_segment* region = new_head; region != nullptr; region = heap_segment_next (region))
 10550|     {
 10551|         heap_segment_prev_free_region (region) = prev;
 10552|         assert ((prev == nullptr) || (compare_by_committed_and_age (prev, region) <= 0));
 10553|         prev = region;
 10554|     }
 10555|     tail_free_region = prev;
 10556| }
 10557| #endif //USE_REGIONS
 10558| void gc_heap::distribute_free_regions()
 10559| {
 10560| #ifdef USE_REGIONS
 10561|     const int kind_count = large_free_region + 1;
 10562| #ifdef MULTIPLE_HEAPS
 10563|     BOOL joined_last_gc_before_oom = FALSE;
 10564|     for (int i = 0; i < n_heaps; i++)
 10565|     {
 10566|         if (g_heaps[i]->last_gc_before_oom)
 10567|         {
 10568|             joined_last_gc_before_oom = TRUE;
 10569|             break;
 10570|         }
 10571|     }
 10572| #else
 10573|     BOOL joined_last_gc_before_oom = last_gc_before_oom;
 10574| #endif //MULTIPLE_HEAPS
 10575|     if (settings.reason == reason_induced_aggressive)
 10576|     {
 10577| #ifdef MULTIPLE_HEAPS
 10578|         for (int i = 0; i < n_heaps; i++)
 10579|         {
 10580|             gc_heap* hp = g_heaps[i];
 10581| #else //MULTIPLE_HEAPS
 10582|         {
 10583|             gc_heap* hp = pGenGCHeap;
 10584| #endif //MULTIPLE_HEAPS
 10585|             for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
 10586|             {
 10587|                 global_regions_to_decommit[kind].transfer_regions (&hp->free_regions[kind]);
 10588|             }
 10589|         }
 10590|         while (decommit_step(DECOMMIT_TIME_STEP_MILLISECONDS))
 10591|         {
 10592|         }
 10593| #ifdef MULTIPLE_HEAPS
 10594|         for (int i = 0; i < n_heaps; i++)
 10595|         {
 10596|             gc_heap* hp = g_heaps[i];
 10597|             int hn = i;
 10598| #else //MULTIPLE_HEAPS
 10599|         {
 10600|             gc_heap* hp = pGenGCHeap;
 10601|             int hn  = 0;
 10602| #endif //MULTIPLE_HEAPS
 10603|             for (int i = 0; i < total_generation_count; i++)
 10604|             {
 10605|                 generation* generation = hp->generation_of (i);
 10606|                 heap_segment* region = heap_segment_rw (generation_start_segment (generation));
 10607|                 while (region != nullptr)
 10608|                 {
 10609|                     uint8_t* aligned_allocated = align_on_page (heap_segment_allocated (region));
 10610|                     size_t end_space = heap_segment_committed (region) - aligned_allocated;
 10611|                     if (end_space > 0)
 10612|                     {
 10613|                         virtual_decommit (aligned_allocated, end_space, gen_to_oh (i), hn);
 10614|                         heap_segment_committed (region) = aligned_allocated;
 10615|                         heap_segment_used (region) = min (heap_segment_used (region), heap_segment_committed (region));
 10616|                         assert (heap_segment_committed (region) > heap_segment_mem (region));
 10617|                     }
 10618|                     region = heap_segment_next_rw (region);
 10619|                 }
 10620|             }
 10621|         }
 10622|         return;
 10623|     }
 10624|     size_t total_num_free_regions[kind_count] = { 0, 0 };
 10625|     size_t total_budget_in_region_units[kind_count] = { 0,  0 };
 10626|     size_t num_decommit_regions_by_time = 0;
 10627|     size_t size_decommit_regions_by_time = 0;
 10628|     size_t heap_budget_in_region_units[MAX_SUPPORTED_CPUS][kind_count];
 10629|     size_t min_heap_budget_in_region_units[MAX_SUPPORTED_CPUS];
 10630|     size_t region_size[kind_count] = { global_region_allocator.get_region_alignment(), global_region_allocator.get_large_region_alignment() };
 10631|     region_free_list surplus_regions[kind_count];
 10632|     for (int kind = basic_free_region; kind < kind_count; kind++)
 10633|     {
 10634|         surplus_regions[kind].transfer_regions (&global_regions_to_decommit[kind]);
 10635|     }
 10636| #ifdef MULTIPLE_HEAPS
 10637|     for (int i = 0; i < n_heaps; i++)
 10638|     {
 10639|         gc_heap* hp = g_heaps[i];
 10640| #else //MULTIPLE_HEAPS
 10641|     {
 10642|         gc_heap* hp = pGenGCHeap;
 10643|         const int i = 0;
 10644|         const int n_heaps = 1;
 10645| #endif //MULTIPLE_HEAPS
 10646|         for (int kind = basic_free_region; kind < kind_count; kind++)
 10647|         {
 10648|             region_free_list& region_list = hp->free_regions[kind];
 10649|             heap_segment* next_region = nullptr;
 10650|             for (heap_segment* region = region_list.get_first_free_region(); region != nullptr; region = next_region)
 10651|             {
 10652|                 next_region = heap_segment_next (region);
 10653|                 int age_in_free_to_decommit = min (max (AGE_IN_FREE_TO_DECOMMIT, n_heaps), MAX_AGE_IN_FREE);
 10654|                 if ((heap_segment_age_in_free (region) >= age_in_free_to_decommit) ||
 10655|                     ((get_region_committed_size (region) == GC_PAGE_SIZE) && joined_last_gc_before_oom))
 10656|                 {
 10657|                     num_decommit_regions_by_time++;
 10658|                     size_decommit_regions_by_time += get_region_committed_size (region);
 10659|                     dprintf (REGIONS_LOG, ("h%2d region %p age %2d, decommit",
 10660|                         i, heap_segment_mem (region), heap_segment_age_in_free (region)));
 10661|                     region_free_list::unlink_region (region);
 10662|                     region_free_list::add_region (region, global_regions_to_decommit);
 10663|                 }
 10664|             }
 10665|             total_num_free_regions[kind] += region_list.get_num_free_regions();
 10666|         }
 10667|         global_free_huge_regions.transfer_regions (&hp->free_regions[huge_free_region]);
 10668|         heap_budget_in_region_units[i][basic_free_region] = 0;
 10669|         min_heap_budget_in_region_units[i] = 0;
 10670|         heap_budget_in_region_units[i][large_free_region] = 0;
 10671|     }
 10672|     for (int gen = soh_gen0; gen < total_generation_count; gen++)
 10673|     {
 10674|         if ((gen <= soh_gen2) &&
 10675|             total_budget_in_region_units[basic_free_region] >= (total_num_free_regions[basic_free_region] +
 10676|                                                                 surplus_regions[basic_free_region].get_num_free_regions()))
 10677|         {
 10678|             dprintf (REGIONS_LOG, ("out of free regions - skipping gen %d budget = %zd >= avail %zd",
 10679|                 gen,
 10680|                 total_budget_in_region_units[basic_free_region],
 10681|                 total_num_free_regions[basic_free_region] + surplus_regions[basic_free_region].get_num_free_regions()));
 10682|             continue;
 10683|         }
 10684| #ifdef MULTIPLE_HEAPS
 10685|         for (int i = 0; i < n_heaps; i++)
 10686|         {
 10687|             gc_heap* hp = g_heaps[i];
 10688| #else //MULTIPLE_HEAPS
 10689|         {
 10690|             gc_heap* hp = pGenGCHeap;
 10691|             const int i = 0;
 10692|             const int n_heaps = 1;
 10693| #endif //MULTIPLE_HEAPS
 10694|             ptrdiff_t budget_gen = max (hp->estimate_gen_growth (gen), 0);
 10695|             int kind = gen >= loh_generation;
 10696|             size_t budget_gen_in_region_units = (budget_gen + (region_size[kind] - 1)) / region_size[kind];
 10697|             dprintf (REGIONS_LOG, ("h%2d gen %d has an estimated growth of %zd bytes (%zd regions)", i, gen, budget_gen, budget_gen_in_region_units));
 10698|             if (gen <= soh_gen2)
 10699|             {
 10700|                 min_heap_budget_in_region_units[i] = heap_budget_in_region_units[i][kind];
 10701|             }
 10702|             heap_budget_in_region_units[i][kind] += budget_gen_in_region_units;
 10703|             total_budget_in_region_units[kind] += budget_gen_in_region_units;
 10704|         }
 10705|     }
 10706|     dprintf (1, ("moved %2zd regions (%8zd) to decommit based on time", num_decommit_regions_by_time, size_decommit_regions_by_time));
 10707|     global_free_huge_regions.transfer_regions (&global_regions_to_decommit[huge_free_region]);
 10708|     size_t free_space_in_huge_regions = global_free_huge_regions.get_size_free_regions();
 10709|     ptrdiff_t num_regions_to_decommit[kind_count];
 10710|     int region_factor[kind_count] = { 1, LARGE_REGION_FACTOR };
 10711| #ifdef TRACE_GC
 10712|     const char* kind_name[count_free_region_kinds] = { "basic", "large", "huge"};
 10713| #endif // TRACE_GC
 10714| #ifndef MULTIPLE_HEAPS
 10715|     const int n_heaps = 1;
 10716| #endif //!MULTIPLE_HEAPS
 10717|     size_t num_huge_region_units_to_consider[kind_count] = { 0, free_space_in_huge_regions / region_size[large_free_region] };
 10718|     for (int kind = basic_free_region; kind < kind_count; kind++)
 10719|     {
 10720|         num_regions_to_decommit[kind] = surplus_regions[kind].get_num_free_regions();
 10721|         dprintf(REGIONS_LOG, ("%zd %s free regions, %zd regions budget, %zd regions on decommit list, %zd huge regions to consider",
 10722|             total_num_free_regions[kind],
 10723|             kind_name[kind],
 10724|             total_budget_in_region_units[kind],
 10725|             num_regions_to_decommit[kind],
 10726|             num_huge_region_units_to_consider[kind]));
 10727|         total_num_free_regions[kind] += num_regions_to_decommit[kind];
 10728|         ptrdiff_t balance = total_num_free_regions[kind] + num_huge_region_units_to_consider[kind] - total_budget_in_region_units[kind];
 10729|         if (
 10730| #ifdef BACKGROUND_GC
 10731|             background_running_p() ||
 10732| #endif
 10733|             (balance < 0))
 10734|         {
 10735|             dprintf (REGIONS_LOG, ("distributing the %zd %s regions deficit", -balance, kind_name[kind]));
 10736| #ifdef MULTIPLE_HEAPS
 10737|             if (balance != 0)
 10738|             {
 10739|                 ptrdiff_t curr_balance = 0;
 10740|                 ptrdiff_t rem_balance = 0;
 10741|                 for (int i = 0; i < n_heaps; i++)
 10742|                 {
 10743|                     curr_balance += balance;
 10744|                     ptrdiff_t adjustment_per_heap = curr_balance / n_heaps;
 10745|                     curr_balance -= adjustment_per_heap * n_heaps;
 10746|                     ptrdiff_t new_budget = (ptrdiff_t)heap_budget_in_region_units[i][kind] + adjustment_per_heap;
 10747|                     ptrdiff_t min_budget = (kind == basic_free_region) ? (ptrdiff_t)min_heap_budget_in_region_units[i] : 0;
 10748|                     dprintf (REGIONS_LOG, ("adjusting the budget for heap %d from %zd %s regions by %zd to %zd",
 10749|                         i,
 10750|                         heap_budget_in_region_units[i][kind],
 10751|                         kind_name[kind],
 10752|                         adjustment_per_heap,
 10753|                         max (min_budget, new_budget)));
 10754|                     heap_budget_in_region_units[i][kind] = max (min_budget, new_budget);
 10755|                     rem_balance += new_budget - heap_budget_in_region_units[i][kind];
 10756|                 }
 10757|                 assert (rem_balance <= 0);
 10758|                 dprintf (REGIONS_LOG, ("remaining balance: %zd %s regions", rem_balance, kind_name[kind]));
 10759|                 while (rem_balance < 0)
 10760|                 {
 10761|                     for (int i = 0; i < n_heaps; i++)
 10762|                     {
 10763|                         size_t min_budget = (kind == basic_free_region) ? min_heap_budget_in_region_units[i] : 0;
 10764|                         if (heap_budget_in_region_units[i][kind] > min_budget)
 10765|                         {
 10766|                             dprintf (REGIONS_LOG, ("adjusting the budget for heap %d from %zd %s regions by %d to %zd",
 10767|                                 i,
 10768|                                 heap_budget_in_region_units[i][kind],
 10769|                                 kind_name[kind],
 10770|                                 -1,
 10771|                                 heap_budget_in_region_units[i][kind] - 1));
 10772|                             heap_budget_in_region_units[i][kind] -= 1;
 10773|                             rem_balance += 1;
 10774|                             if (rem_balance == 0)
 10775|                                 break;
 10776|                         }
 10777|                     }
 10778|                 }
 10779|             }
 10780| #endif //MULTIPLE_HEAPS
 10781|         }
 10782|         else
 10783|         {
 10784|             num_regions_to_decommit[kind] = balance;
 10785|             dprintf(REGIONS_LOG, ("distributing the %zd %s regions, removing %zd regions",
 10786|                 total_budget_in_region_units[kind],
 10787|                 kind_name[kind],
 10788|                 num_regions_to_decommit[kind]));
 10789|             if (num_regions_to_decommit[kind] > 0)
 10790|             {
 10791|                 size_t num_regions_to_decommit_before = global_regions_to_decommit[kind].get_num_free_regions();
 10792|                 global_region_allocator.move_highest_free_regions (num_regions_to_decommit[kind]*region_factor[kind],
 10793|                                                                    kind == basic_free_region,
 10794|                                                                    global_regions_to_decommit);
 10795|                 dprintf (REGIONS_LOG, ("Moved %zd %s regions to decommit list",
 10796|                          global_regions_to_decommit[kind].get_num_free_regions(), kind_name[kind]));
 10797|                 if (kind == basic_free_region)
 10798|                 {
 10799|                     assert (global_regions_to_decommit[kind].get_num_free_regions() ==
 10800|                             num_regions_to_decommit_before + (size_t)num_regions_to_decommit[kind]);
 10801|                 }
 10802|                 else
 10803|                 {
 10804|                     dprintf (REGIONS_LOG, ("Moved %zd %s regions to decommit list",
 10805|                         global_regions_to_decommit[huge_free_region].get_num_free_regions(), kind_name[huge_free_region]));
 10806|                 }
 10807|             }
 10808|         }
 10809|     }
 10810|     for (int kind = basic_free_region; kind < kind_count; kind++)
 10811|     {
 10812| #ifdef MULTIPLE_HEAPS
 10813|         for (int i = 0; i < n_heaps; i++)
 10814|         {
 10815|             gc_heap* hp = g_heaps[i];
 10816|             if (hp->free_regions[kind].get_num_free_regions() > heap_budget_in_region_units[i][kind])
 10817|             {
 10818|                 dprintf (REGIONS_LOG, ("removing %zd %s regions from heap %d with %zd regions, budget is %zd",
 10819|                     hp->free_regions[kind].get_num_free_regions() - heap_budget_in_region_units[i][kind],
 10820|                     kind_name[kind],
 10821|                     i,
 10822|                     hp->free_regions[kind].get_num_free_regions(),
 10823|                     heap_budget_in_region_units[i][kind]));
 10824|                 remove_surplus_regions (&hp->free_regions[kind], &surplus_regions[kind], heap_budget_in_region_units[i][kind]);
 10825|             }
 10826|         }
 10827|         for (int i = 0; i < n_heaps; i++)
 10828|         {
 10829|             gc_heap* hp = g_heaps[i];
 10830| #else //MULTIPLE_HEAPS
 10831|         {
 10832|             gc_heap* hp = pGenGCHeap;
 10833|             const int i = 0;
 10834| #endif //MULTIPLE_HEAPS
 10835|             if (hp->free_regions[kind].get_num_free_regions() < heap_budget_in_region_units[i][kind])
 10836|             {
 10837|                 int64_t num_added_regions = add_regions (&hp->free_regions[kind], &surplus_regions[kind], heap_budget_in_region_units[i][kind]);
 10838|                 dprintf (REGIONS_LOG, ("added %zd %s regions to heap %d - now has %zd, budget is %zd",
 10839|                     (size_t)num_added_regions,
 10840|                     kind_name[kind],
 10841|                     i,
 10842|                     hp->free_regions[kind].get_num_free_regions(),
 10843|                     heap_budget_in_region_units[i][kind]));
 10844|             }
 10845|             hp->free_regions[kind].sort_by_committed_and_age();
 10846|         }
 10847|         if (surplus_regions[kind].get_num_free_regions() > 0)
 10848|         {
 10849|             assert (!"should have exhausted the surplus_regions");
 10850|             global_regions_to_decommit[kind].transfer_regions (&surplus_regions[kind]);
 10851|         }
 10852|     }
 10853| #ifdef MULTIPLE_HEAPS
 10854|     for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
 10855|     {
 10856|         if (global_regions_to_decommit[kind].get_num_free_regions() != 0)
 10857|         {
 10858|             gradual_decommit_in_progress_p = TRUE;
 10859|             break;
 10860|         }
 10861|     }
 10862| #else //MULTIPLE_HEAPS
 10863|     dynamic_data* dd0 = dynamic_data_of (0);
 10864|     size_t ephemeral_elapsed = (size_t)((dd_time_clock (dd0) - gc_last_ephemeral_decommit_time) / 1000);
 10865|     if (ephemeral_elapsed >= DECOMMIT_TIME_STEP_MILLISECONDS)
 10866|     {
 10867|         gc_last_ephemeral_decommit_time = dd_time_clock (dd0);
 10868|         size_t decommit_step_milliseconds = min (ephemeral_elapsed, (10*1000));
 10869|         decommit_step (decommit_step_milliseconds);
 10870|     }
 10871|     for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
 10872|     {
 10873|         if (global_regions_to_decommit[kind].get_num_free_regions() != 0)
 10874|         {
 10875|             free_regions[kind].transfer_regions (&global_regions_to_decommit[kind]);
 10876|         }
 10877|     }
 10878| #endif //MULTIPLE_HEAPS
 10879| #endif //USE_REGIONS
 10880| }
 10881| #ifdef WRITE_WATCH
 10882| uint8_t* g_addresses [array_size+2]; // to get around the bug in GetWriteWatch
 10883| #ifdef CARD_BUNDLE
 10884| inline void gc_heap::verify_card_bundle_bits_set(size_t first_card_word, size_t last_card_word)
 10885| {
 10886| #ifdef _DEBUG
 10887|     for (size_t x = cardw_card_bundle (first_card_word); x < cardw_card_bundle (last_card_word); x++)
 10888|     {
 10889|         if (!card_bundle_set_p (x))
 10890|         {
 10891|             assert (!"Card bundle not set");
 10892|             dprintf (3, ("Card bundle %zx not set", x));
 10893|         }
 10894|     }
 10895| #else
 10896|     UNREFERENCED_PARAMETER(first_card_word);
 10897|     UNREFERENCED_PARAMETER(last_card_word);
 10898| #endif
 10899| }
 10900| inline void gc_heap::verify_card_bundles()
 10901| {
 10902| #ifdef _DEBUG
 10903|     size_t lowest_card = card_word (card_of (lowest_address));
 10904| #ifdef USE_REGIONS
 10905|     size_t highest_card = card_word (card_of (global_region_allocator.get_left_used_unsafe()));
 10906| #else
 10907|     size_t highest_card = card_word (card_of (highest_address));
 10908| #endif
 10909|     size_t cardb = cardw_card_bundle (lowest_card);
 10910|     size_t end_cardb = cardw_card_bundle (align_cardw_on_bundle (highest_card));
 10911|     while (cardb < end_cardb)
 10912|     {
 10913|         uint32_t* card_word = &card_table[max(card_bundle_cardw (cardb), lowest_card)];
 10914|         uint32_t* card_word_end = &card_table[min(card_bundle_cardw (cardb+1), highest_card)];
 10915|         if (card_bundle_set_p (cardb) == 0)
 10916|         {
 10917|             while (card_word < card_word_end)
 10918|             {
 10919|                 if (*card_word != 0)
 10920|                 {
 10921|                     dprintf  (3, ("gc: %zd, Card word %zx for address %zx set, card_bundle %zx clear",
 10922|                             dd_collection_count (dynamic_data_of (0)),
 10923|                             (size_t)(card_word-&card_table[0]),
 10924|                             (size_t)(card_address ((size_t)(card_word-&card_table[0]) * card_word_width)),
 10925|                             cardb));
 10926|                 }
 10927|                 assert((*card_word)==0);
 10928|                 card_word++;
 10929|             }
 10930|         }
 10931|         cardb++;
 10932|     }
 10933| #endif
 10934| }
 10935| void gc_heap::update_card_table_bundle()
 10936| {
 10937|     if (card_bundles_enabled())
 10938|     {
 10939|         uint8_t* base_address = (uint8_t*)(&card_table[card_word (card_of (lowest_address))]);
 10940| #ifdef USE_REGIONS
 10941|         uint8_t* high_address = (uint8_t*)(&card_table[card_word (card_of (global_region_allocator.get_left_used_unsafe()))]);
 10942| #else
 10943|         uint8_t* high_address = (uint8_t*)(&card_table[card_word (card_of (highest_address))]);
 10944| #endif //USE_REGIONS
 10945|         uint8_t* saved_base_address = base_address;
 10946|         uintptr_t bcount = array_size;
 10947|         size_t saved_region_size = align_on_page (high_address) - saved_base_address;
 10948|         do
 10949|         {
 10950|             size_t region_size = align_on_page (high_address) - base_address;
 10951|             dprintf (3,("Probing card table pages [%zx, %zx[",
 10952|                 (size_t)base_address, (size_t)(base_address + region_size)));
 10953|             bool success = GCToOSInterface::GetWriteWatch(false /* resetState */,
 10954|                                                           base_address,
 10955|                                                           region_size,
 10956|                                                           (void**)g_addresses,
 10957|                                                           &bcount);
 10958|             assert (success && "GetWriteWatch failed!");
 10959|             dprintf (3,("Found %zd pages written", bcount));
 10960|             for (unsigned i = 0; i < bcount; i++)
 10961|             {
 10962|                 size_t bcardw = (uint32_t*)(max(g_addresses[i],base_address)) - &card_table[0];
 10963|                 size_t ecardw = (uint32_t*)(min(g_addresses[i]+OS_PAGE_SIZE, high_address)) - &card_table[0];
 10964|                 assert (bcardw >= card_word (card_of (g_gc_lowest_address)));
 10965|                 card_bundles_set (cardw_card_bundle (bcardw),
 10966|                                   cardw_card_bundle (align_cardw_on_bundle (ecardw)));
 10967|                 dprintf (3,("Set Card bundle [%zx, %zx[",
 10968|                     cardw_card_bundle (bcardw), cardw_card_bundle (align_cardw_on_bundle (ecardw))));
 10969|                 verify_card_bundle_bits_set(bcardw, ecardw);
 10970|             }
 10971|             if (bcount >= array_size)
 10972|             {
 10973|                 base_address = g_addresses [array_size-1] + OS_PAGE_SIZE;
 10974|                 bcount = array_size;
 10975|             }
 10976|         } while ((bcount >= array_size) && (base_address < high_address));
 10977|         GCToOSInterface::ResetWriteWatch (saved_base_address, saved_region_size);
 10978|     }
 10979| }
 10980| #endif //CARD_BUNDLE
 10981| #ifdef BACKGROUND_GC
 10982| void gc_heap::reset_write_watch_for_gc_heap(void* base_address, size_t region_size)
 10983| {
 10984| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 10985|     SoftwareWriteWatch::ClearDirty(base_address, region_size);
 10986| #else // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 10987|     GCToOSInterface::ResetWriteWatch(base_address, region_size);
 10988| #endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 10989| }
 10990| void gc_heap::get_write_watch_for_gc_heap(bool reset, void *base_address, size_t region_size,
 10991|                                           void** dirty_pages, uintptr_t* dirty_page_count_ref,
 10992|                                           bool is_runtime_suspended)
 10993| {
 10994| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 10995|     SoftwareWriteWatch::GetDirty(base_address, region_size, dirty_pages, dirty_page_count_ref,
 10996|                                  reset, is_runtime_suspended);
 10997| #else // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 10998|     UNREFERENCED_PARAMETER(is_runtime_suspended);
 10999|     bool success = GCToOSInterface::GetWriteWatch(reset, base_address, region_size, dirty_pages,
 11000|                                                   dirty_page_count_ref);
 11001|     assert(success);
 11002| #endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 11003| }
 11004| const size_t ww_reset_quantum = 128*1024*1024;
 11005| inline
 11006| void gc_heap::switch_one_quantum()
 11007| {
 11008|     enable_preemptive ();
 11009|     GCToOSInterface::Sleep (1);
 11010|     disable_preemptive (true);
 11011| }
 11012| void gc_heap::reset_ww_by_chunk (uint8_t* start_address, size_t total_reset_size)
 11013| {
 11014|     size_t reset_size = 0;
 11015|     size_t remaining_reset_size = 0;
 11016|     size_t next_reset_size = 0;
 11017|     while (reset_size != total_reset_size)
 11018|     {
 11019|         remaining_reset_size = total_reset_size - reset_size;
 11020|         next_reset_size = ((remaining_reset_size >= ww_reset_quantum) ?
 11021|             ww_reset_quantum : remaining_reset_size);
 11022|         if (next_reset_size)
 11023|         {
 11024|             reset_write_watch_for_gc_heap(start_address, next_reset_size);
 11025|             reset_size += next_reset_size;
 11026|             switch_one_quantum();
 11027|         }
 11028|     }
 11029|     assert (reset_size == total_reset_size);
 11030| }
 11031| void gc_heap::switch_on_reset (BOOL concurrent_p, size_t* current_total_reset_size, size_t last_reset_size)
 11032| {
 11033|     if (concurrent_p)
 11034|     {
 11035|         *current_total_reset_size += last_reset_size;
 11036|         dprintf (2, ("reset %zd bytes so far", *current_total_reset_size));
 11037|         if (*current_total_reset_size > ww_reset_quantum)
 11038|         {
 11039|             switch_one_quantum();
 11040|             *current_total_reset_size = 0;
 11041|         }
 11042|     }
 11043| }
 11044| void gc_heap::reset_write_watch (BOOL concurrent_p)
 11045| {
 11046| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 11047|     assert(!concurrent_p);
 11048| #endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 11049|     dprintf (2, ("bgc lowest: %p, bgc highest: %p",
 11050|         background_saved_lowest_address, background_saved_highest_address));
 11051|     size_t reset_size = 0;
 11052|     for (int i = get_start_generation_index(); i < total_generation_count; i++)
 11053|     {
 11054|         heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (i)));
 11055|         while (seg)
 11056|         {
 11057|             uint8_t* base_address = align_lower_page (heap_segment_mem (seg));
 11058|             base_address = max (base_address, background_saved_lowest_address);
 11059|             uint8_t* high_address = ((seg == ephemeral_heap_segment) ?
 11060|                 alloc_allocated : heap_segment_allocated (seg));
 11061|             high_address = min (high_address, background_saved_highest_address);
 11062|             if (base_address < high_address)
 11063|             {
 11064|                 size_t reset_size = 0;
 11065|                 size_t region_size = high_address - base_address;
 11066|                 dprintf (3, ("h%d, gen: %x, ww: [%zx(%zd)", heap_number, i, (size_t)base_address, region_size));
 11067|                 reset_write_watch_for_gc_heap(base_address, region_size);
 11068|                 switch_on_reset (concurrent_p, &reset_size, region_size);
 11069|             }
 11070|             seg = heap_segment_next_rw (seg);
 11071|             concurrent_print_time_delta (i == max_generation ? "CRWW soh": "CRWW uoh");
 11072|         }
 11073|     }
 11074| }
 11075| #endif //BACKGROUND_GC
 11076| #endif //WRITE_WATCH
 11077| #ifdef BACKGROUND_GC
 11078| void gc_heap::restart_vm()
 11079| {
 11080|     dprintf (3, ("Restarting EE"));
 11081|     STRESS_LOG0(LF_GC, LL_INFO10000, "Concurrent GC: Restarting EE\n");
 11082|     ee_proceed_event.Set();
 11083| }
 11084| inline
 11085| void fire_alloc_wait_event (alloc_wait_reason awr, BOOL begin_p)
 11086| {
 11087|     if (awr != awr_ignored)
 11088|     {
 11089|         if (begin_p)
 11090|         {
 11091|             FIRE_EVENT(BGCAllocWaitBegin, awr);
 11092|         }
 11093|         else
 11094|         {
 11095|             FIRE_EVENT(BGCAllocWaitEnd, awr);
 11096|         }
 11097|     }
 11098| }
 11099| void gc_heap::fire_alloc_wait_event_begin (alloc_wait_reason awr)
 11100| {
 11101|     fire_alloc_wait_event (awr, TRUE);
 11102| }
 11103| void gc_heap::fire_alloc_wait_event_end (alloc_wait_reason awr)
 11104| {
 11105|     fire_alloc_wait_event (awr, FALSE);
 11106| }
 11107| #endif //BACKGROUND_GC
 11108| void gc_heap::make_generation (int gen_num, heap_segment* seg, uint8_t* start)
 11109| {
 11110|     generation* gen = generation_of (gen_num);
 11111|     gen->gen_num = gen_num;
 11112| #ifndef USE_REGIONS
 11113|     gen->allocation_start = start;
 11114|     gen->plan_allocation_start = 0;
 11115| #endif //USE_REGIONS
 11116|     gen->allocation_context.alloc_ptr = 0;
 11117|     gen->allocation_context.alloc_limit = 0;
 11118|     gen->allocation_context.alloc_bytes = 0;
 11119|     gen->allocation_context.alloc_bytes_uoh = 0;
 11120|     gen->allocation_context_start_region = 0;
 11121|     gen->start_segment = seg;
 11122| #ifdef USE_REGIONS
 11123|     dprintf (REGIONS_LOG, ("g%d start seg is %zx-%p", gen_num, (size_t)seg, heap_segment_mem (seg)));
 11124|     gen->tail_region = seg;
 11125|     gen->plan_start_segment = 0;
 11126|     gen->tail_ro_region = 0;
 11127| #endif //USE_REGIONS
 11128|     gen->allocation_segment = seg;
 11129|     gen->free_list_space = 0;
 11130|     gen->free_list_allocated = 0;
 11131|     gen->end_seg_allocated = 0;
 11132|     gen->condemned_allocated = 0;
 11133|     gen->sweep_allocated = 0;
 11134|     gen->free_obj_space = 0;
 11135|     gen->allocation_size = 0;
 11136|     gen->pinned_allocation_sweep_size = 0;
 11137|     gen->pinned_allocation_compact_size = 0;
 11138|     gen->allocate_end_seg_p = FALSE;
 11139|     gen->free_list_allocator.clear();
 11140| #ifdef DOUBLY_LINKED_FL
 11141|     gen->set_bgc_mark_bit_p = FALSE;
 11142| #endif //DOUBLY_LINKED_FL
 11143| #ifdef FREE_USAGE_STATS
 11144|     memset (gen->gen_free_spaces, 0, sizeof (gen->gen_free_spaces));
 11145|     memset (gen->gen_current_pinned_free_spaces, 0, sizeof (gen->gen_current_pinned_free_spaces));
 11146|     memset (gen->gen_plugs, 0, sizeof (gen->gen_plugs));
 11147| #endif //FREE_USAGE_STATS
 11148| }
 11149| void gc_heap::adjust_ephemeral_limits ()
 11150| {
 11151| #ifndef USE_REGIONS
 11152|     ephemeral_low = generation_allocation_start (generation_of (max_generation - 1));
 11153|     ephemeral_high = heap_segment_reserved (ephemeral_heap_segment);
 11154|     dprintf (3, ("new ephemeral low: %zx new ephemeral high: %zx",
 11155|         (size_t)ephemeral_low, (size_t)ephemeral_high))
 11156| #ifndef MULTIPLE_HEAPS
 11157|     stomp_write_barrier_ephemeral(ephemeral_low, ephemeral_high);
 11158| #endif // MULTIPLE_HEAPS
 11159| #endif //USE_REGIONS
 11160| }
 11161| #if defined(TRACE_GC) || defined(GC_CONFIG_DRIVEN)
 11162| FILE* CreateLogFile(const GCConfigStringHolder& temp_logfile_name, bool is_config)
 11163| {
 11164|     FILE* logFile;
 11165|     if (!temp_logfile_name.Get())
 11166|     {
 11167|         return nullptr;
 11168|     }
 11169|     char logfile_name[MAX_LONGPATH+1];
 11170|     const char* suffix = is_config ? ".config.log" : ".log";
 11171|     _snprintf_s(logfile_name, MAX_LONGPATH+1, _TRUNCATE, "%s%s", temp_logfile_name.Get(), suffix);
 11172|     logFile = fopen(logfile_name, "wb");
 11173|     return logFile;
 11174| }
 11175| #endif //TRACE_GC || GC_CONFIG_DRIVEN
 11176| uint32_t adjust_heaps_hard_limit_worker (uint32_t nhp, size_t limit)
 11177| {
 11178|     if (!limit)
 11179|         return nhp;
 11180|     size_t aligned_limit =  align_on_segment_hard_limit (limit);
 11181|     uint32_t nhp_oh = (uint32_t)(aligned_limit / min_segment_size_hard_limit);
 11182|     nhp = min (nhp_oh, nhp);
 11183|     return (max (nhp, 1));
 11184| }
 11185| uint32_t gc_heap::adjust_heaps_hard_limit (uint32_t nhp)
 11186| {
 11187| #ifdef MULTIPLE_HEAPS
 11188|     if (heap_hard_limit_oh[soh])
 11189|     {
 11190|         for (int i = 0; i < (total_oh_count - 1); i++)
 11191|         {
 11192|             nhp = adjust_heaps_hard_limit_worker (nhp, heap_hard_limit_oh[i]);
 11193|         }
 11194|     }
 11195|     else if (heap_hard_limit)
 11196|     {
 11197|         nhp = adjust_heaps_hard_limit_worker (nhp, heap_hard_limit);
 11198|     }
 11199| #endif
 11200|     return nhp;
 11201| }
 11202| size_t gc_heap::adjust_segment_size_hard_limit_va (size_t seg_size)
 11203| {
 11204|     return (use_large_pages_p ?
 11205|             align_on_segment_hard_limit (seg_size) :
 11206|             round_up_power2 (seg_size));
 11207| }
 11208| size_t gc_heap::adjust_segment_size_hard_limit (size_t limit, uint32_t nhp)
 11209| {
 11210|     if (!limit)
 11211|     {
 11212|         limit = min_segment_size_hard_limit;
 11213|     }
 11214|     size_t seg_size = align_on_segment_hard_limit (limit) / nhp;
 11215|     return adjust_segment_size_hard_limit_va (seg_size);
 11216| }
 11217| #ifdef USE_REGIONS
 11218| bool allocate_initial_regions(int number_of_heaps)
 11219| {
 11220|     initial_regions = new (nothrow) uint8_t*[number_of_heaps][total_generation_count][2];
 11221|     if (initial_regions == nullptr)
 11222|     {
 11223|         return false;
 11224|     }
 11225|     for (int i = 0; i < number_of_heaps; i++)
 11226|     {
 11227|         bool succeed = global_region_allocator.allocate_large_region(
 11228|             poh_generation,
 11229|             &initial_regions[i][poh_generation][0],
 11230|             &initial_regions[i][poh_generation][1], allocate_forward, 0, nullptr);
 11231|         assert(succeed);
 11232|     }
 11233|     for (int i = 0; i < number_of_heaps; i++)
 11234|     {
 11235|         for (int gen_num = max_generation; gen_num >= 0; gen_num--)
 11236|         {
 11237|             bool succeed = global_region_allocator.allocate_basic_region(
 11238|                 gen_num,
 11239|                 &initial_regions[i][gen_num][0],
 11240|                 &initial_regions[i][gen_num][1], nullptr);
 11241|             assert(succeed);
 11242|         }
 11243|     }
 11244|     for (int i = 0; i < number_of_heaps; i++)
 11245|     {
 11246|         bool succeed = global_region_allocator.allocate_large_region(
 11247|             loh_generation,
 11248|             &initial_regions[i][loh_generation][0],
 11249|             &initial_regions[i][loh_generation][1], allocate_forward, 0, nullptr);
 11250|         assert(succeed);
 11251|     }
 11252|     return true;
 11253| }
 11254| #endif
 11255| HRESULT gc_heap::initialize_gc (size_t soh_segment_size,
 11256|                                 size_t loh_segment_size,
 11257|                                 size_t poh_segment_size
 11258| #ifdef MULTIPLE_HEAPS
 11259|                                 ,int number_of_heaps
 11260| #endif //MULTIPLE_HEAPS
 11261| )
 11262| {
 11263| #if defined(TRACE_GC) && defined(SIMPLE_DPRINTF)
 11264|     if (GCConfig::GetLogEnabled())
 11265|     {
 11266|         gc_log = CreateLogFile(GCConfig::GetLogFile(), false);
 11267|         if (gc_log == NULL)
 11268|         {
 11269|             GCToEEInterface::LogErrorToHost("Cannot create log file");
 11270|             return E_FAIL;
 11271|         }
 11272|         gc_log_file_size = static_cast<size_t>(GCConfig::GetLogFileSize());
 11273|         if (gc_log_file_size <= 0 || gc_log_file_size > 500)
 11274|         {
 11275|             GCToEEInterface::LogErrorToHost("Invalid log file size (valid size needs to be larger than 0 and smaller than 500)");
 11276|             fclose (gc_log);
 11277|             return E_FAIL;
 11278|         }
 11279|         gc_log_lock.Initialize();
 11280|         gc_log_buffer = new (nothrow) uint8_t [gc_log_buffer_size];
 11281|         if (!gc_log_buffer)
 11282|         {
 11283|             fclose(gc_log);
 11284|             return E_OUTOFMEMORY;
 11285|         }
 11286|         memset (gc_log_buffer, '*', gc_log_buffer_size);
 11287|         max_gc_buffers = gc_log_file_size * 1024 * 1024 / gc_log_buffer_size;
 11288|     }
 11289| #endif //TRACE_GC && SIMPLE_DPRINTF
 11290| #ifdef GC_CONFIG_DRIVEN
 11291|     if (GCConfig::GetConfigLogEnabled())
 11292|     {
 11293|         gc_config_log = CreateLogFile(GCConfig::GetConfigLogFile(), true);
 11294|         if (gc_config_log == NULL)
 11295|         {
 11296|             GCToEEInterface::LogErrorToHost("Cannot create log file");
 11297|             return E_FAIL;
 11298|         }
 11299|         gc_config_log_buffer = new (nothrow) uint8_t [gc_config_log_buffer_size];
 11300|         if (!gc_config_log_buffer)
 11301|         {
 11302|             fclose(gc_config_log);
 11303|             return E_OUTOFMEMORY;
 11304|         }
 11305|         compact_ratio = static_cast<int>(GCConfig::GetCompactRatio());
 11306|         cprintf (("%2s | %6s | %1s | %1s | %2s | %2s | %2s | %2s | %2s || %5s | %5s | %5s | %5s | %5s | %5s | %5s | %5s | %5s |",
 11307|                 "h#", // heap index
 11308|                 "GC", // GC index
 11309|                 "g", // generation
 11310|                 "C",  // compaction (empty means sweeping), 'M' means it was mandatory, 'W' means it was not
 11311|                 "EX", // heap expansion
 11312|                 "NF", // normal fit
 11313|                 "BF", // best fit (if it indicates neither NF nor BF it means it had to acquire a new seg.
 11314|                 "ML", // mark list
 11315|                 "DM", // demotion
 11316|                 "PreS", // short object before pinned plug
 11317|                 "PostS", // short object after pinned plug
 11318|                 "Merge", // merged pinned plugs
 11319|                 "Conv", // converted to pinned plug
 11320|                 "Pre", // plug before pinned plug but not after
 11321|                 "Post", // plug after pinned plug but not before
 11322|                 "PrPo", // plug both before and after pinned plug
 11323|                 "PreP", // pre short object padded
 11324|                 "PostP" // post short object padded
 11325|                 ));
 11326|     }
 11327| #endif //GC_CONFIG_DRIVEN
 11328|     HRESULT hres = S_OK;
 11329|     conserve_mem_setting = (int)GCConfig::GetGCConserveMem();
 11330| #ifdef DYNAMIC_HEAP_COUNT
 11331|     dynamic_adaptation_mode = (int)GCConfig::GetGCDynamicAdaptationMode();
 11332|     if (GCConfig::GetHeapCount() != 0)
 11333|     {
 11334|         dynamic_adaptation_mode = 0;
 11335|     }
 11336|     if ((dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes) && (conserve_mem_setting == 0))
 11337|         conserve_mem_setting = 5;
 11338| #endif //DYNAMIC_HEAP_COUNT
 11339|     if (conserve_mem_setting < 0)
 11340|         conserve_mem_setting = 0;
 11341|     if (conserve_mem_setting > 9)
 11342|         conserve_mem_setting = 9;
 11343|     dprintf (1, ("conserve_mem_setting = %d", conserve_mem_setting));
 11344| #ifdef WRITE_WATCH
 11345|     hardware_write_watch_api_supported();
 11346| #ifdef BACKGROUND_GC
 11347|     if (can_use_write_watch_for_gc_heap() && GCConfig::GetConcurrentGC())
 11348|     {
 11349|         gc_can_use_concurrent = true;
 11350| #ifndef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 11351|         virtual_alloc_hardware_write_watch = true;
 11352| #endif // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 11353|     }
 11354|     else
 11355|     {
 11356|         gc_can_use_concurrent = false;
 11357|     }
 11358|     GCConfig::SetConcurrentGC(gc_can_use_concurrent);
 11359| #else //BACKGROUND_GC
 11360|     GCConfig::SetConcurrentGC(false);
 11361| #endif //BACKGROUND_GC
 11362| #endif //WRITE_WATCH
 11363| #ifdef BACKGROUND_GC
 11364|     segment_info_size = OS_PAGE_SIZE;
 11365| #else
 11366|     segment_info_size = Align (sizeof (heap_segment), get_alignment_constant (FALSE));
 11367| #endif //BACKGROUND_GC
 11368|     reserved_memory = 0;
 11369|     size_t initial_heap_size = soh_segment_size + loh_segment_size + poh_segment_size;
 11370|     uint16_t* heap_no_to_numa_node = nullptr;
 11371| #ifdef MULTIPLE_HEAPS
 11372|     reserved_memory_limit = initial_heap_size * number_of_heaps;
 11373|     if (!heap_select::init(number_of_heaps))
 11374|         return E_OUTOFMEMORY;
 11375|     if (GCToOSInterface::CanEnableGCNumaAware())
 11376|         heap_no_to_numa_node = heap_select::heap_no_to_numa_node;
 11377| #else //MULTIPLE_HEAPS
 11378|     reserved_memory_limit = initial_heap_size;
 11379|     int number_of_heaps = 1;
 11380| #endif //MULTIPLE_HEAPS
 11381| #ifndef COMMITTED_BYTES_SHADOW
 11382|     if (heap_hard_limit)
 11383| #endif //!COMMITTED_BYTES_SHADOW
 11384|     {
 11385|         check_commit_cs.Initialize();
 11386|     }
 11387|     decommit_lock.Initialize();
 11388| #ifdef USE_REGIONS
 11389|     if (regions_range)
 11390|     {
 11391|         size_t reserve_size = regions_range;
 11392|         uint8_t* reserve_range = (uint8_t*)virtual_alloc (reserve_size, use_large_pages_p);
 11393|         if (!reserve_range)
 11394|             return E_OUTOFMEMORY;
 11395|         if (!global_region_allocator.init (reserve_range, (reserve_range + reserve_size),
 11396|                                            ((size_t)1 << min_segment_size_shr),
 11397|                                            &g_gc_lowest_address, &g_gc_highest_address))
 11398|             return E_OUTOFMEMORY;
 11399|         if (!allocate_initial_regions(number_of_heaps))
 11400|             return E_OUTOFMEMORY;
 11401|     }
 11402|     else
 11403|     {
 11404|         assert (!"cannot use regions without specifying the range!!!");
 11405|         GCToEEInterface::LogErrorToHost("Cannot use regions without specifying the range (using DOTNET_GCRegionRange)");
 11406|         return E_FAIL;
 11407|     }
 11408| #else //USE_REGIONS
 11409|     bool separated_poh_p = use_large_pages_p &&
 11410|                            heap_hard_limit_oh[soh] &&
 11411|                            (GCConfig::GetGCHeapHardLimitPOH() == 0) &&
 11412|                            (GCConfig::GetGCHeapHardLimitPOHPercent() == 0);
 11413|     if (!reserve_initial_memory (soh_segment_size, loh_segment_size, poh_segment_size, number_of_heaps,
 11414|                                  use_large_pages_p, separated_poh_p, heap_no_to_numa_node))
 11415|         return E_OUTOFMEMORY;
 11416|     if (use_large_pages_p)
 11417|     {
 11418|         if (heap_hard_limit_oh[soh])
 11419|         {
 11420|             heap_hard_limit_oh[soh] = soh_segment_size * number_of_heaps;
 11421|             heap_hard_limit_oh[loh] = loh_segment_size * number_of_heaps;
 11422|             heap_hard_limit_oh[poh] = poh_segment_size * number_of_heaps;
 11423|             heap_hard_limit = heap_hard_limit_oh[soh] + heap_hard_limit_oh[loh] + heap_hard_limit_oh[poh];
 11424|         }
 11425|         else
 11426|         {
 11427|             assert (heap_hard_limit);
 11428|             heap_hard_limit = (soh_segment_size + loh_segment_size + poh_segment_size) * number_of_heaps;
 11429|         }
 11430|     }
 11431| #endif //USE_REGIONS
 11432| #ifdef CARD_BUNDLE
 11433| #ifdef MULTIPLE_HEAPS
 11434|     uint64_t th = (uint64_t)MH_TH_CARD_BUNDLE*number_of_heaps;
 11435| #else
 11436|     uint64_t th = (uint64_t)SH_TH_CARD_BUNDLE;
 11437| #endif //MULTIPLE_HEAPS
 11438|     if (can_use_write_watch_for_card_table() && reserved_memory >= th)
 11439|     {
 11440|         settings.card_bundles = TRUE;
 11441|     }
 11442|     else
 11443|     {
 11444|         settings.card_bundles = FALSE;
 11445|     }
 11446| #endif //CARD_BUNDLE
 11447|     settings.first_init();
 11448|     int latency_level_from_config = static_cast<int>(GCConfig::GetLatencyLevel());
 11449|     if (latency_level_from_config >= latency_level_first && latency_level_from_config <= latency_level_last)
 11450|     {
 11451|         gc_heap::latency_level = static_cast<gc_latency_level>(latency_level_from_config);
 11452|     }
 11453|     init_static_data();
 11454|     g_gc_card_table = make_card_table (g_gc_lowest_address, g_gc_highest_address);
 11455|     if (!g_gc_card_table)
 11456|         return E_OUTOFMEMORY;
 11457|     gc_started = FALSE;
 11458| #ifdef MULTIPLE_HEAPS
 11459|     g_heaps = new (nothrow) gc_heap* [number_of_heaps];
 11460|     if (!g_heaps)
 11461|         return E_OUTOFMEMORY;
 11462| #ifdef _PREFAST_
 11463| #pragma warning(push)
 11464| #pragma warning(disable:22011) // Suppress PREFast warning about integer underflow/overflow
 11465| #endif // _PREFAST_
 11466| #if !defined(USE_REGIONS) || defined(_DEBUG)
 11467|     g_promoted = new (nothrow) size_t [number_of_heaps*16];
 11468|     if (!g_promoted)
 11469|         return E_OUTOFMEMORY;
 11470| #endif //!USE_REGIONS || _DEBUG
 11471| #ifdef BACKGROUND_GC
 11472|     g_bpromoted = new (nothrow) size_t [number_of_heaps*16];
 11473|     if (!g_bpromoted)
 11474|         return E_OUTOFMEMORY;
 11475| #endif
 11476| #ifdef MH_SC_MARK
 11477|     g_mark_stack_busy = new (nothrow) int[(number_of_heaps+2)*HS_CACHE_LINE_SIZE/sizeof(int)];
 11478| #endif //MH_SC_MARK
 11479| #ifdef _PREFAST_
 11480| #pragma warning(pop)
 11481| #endif // _PREFAST_
 11482| #ifdef MH_SC_MARK
 11483|     if (!g_mark_stack_busy)
 11484|         return E_OUTOFMEMORY;
 11485| #endif //MH_SC_MARK
 11486|     if (!create_thread_support (number_of_heaps))
 11487|         return E_OUTOFMEMORY;
 11488| #endif //MULTIPLE_HEAPS
 11489| #ifdef MULTIPLE_HEAPS
 11490|     yp_spin_count_unit = 32 * number_of_heaps;
 11491| #else
 11492|     yp_spin_count_unit = 32 * g_num_processors;
 11493| #endif //MULTIPLE_HEAPS
 11494|     int64_t spin_count_unit_from_config = GCConfig::GetGCSpinCountUnit();
 11495|     gc_heap::spin_count_unit_config_p = (spin_count_unit_from_config > 0) && (spin_count_unit_from_config <= MAX_YP_SPIN_COUNT_UNIT);
 11496|     if (gc_heap::spin_count_unit_config_p)
 11497|     {
 11498|         yp_spin_count_unit = static_cast<int32_t>(spin_count_unit_from_config);
 11499|     }
 11500|     original_spin_count_unit = yp_spin_count_unit;
 11501| #if defined(__linux__)
 11502|     GCToEEInterface::UpdateGCEventStatus(static_cast<int>(GCEventStatus::GetEnabledLevel(GCEventProvider_Default)),
 11503|                                          static_cast<int>(GCEventStatus::GetEnabledKeywords(GCEventProvider_Default)),
 11504|                                          static_cast<int>(GCEventStatus::GetEnabledLevel(GCEventProvider_Private)),
 11505|                                          static_cast<int>(GCEventStatus::GetEnabledKeywords(GCEventProvider_Private)));
 11506| #endif // __linux__
 11507| #ifdef USE_VXSORT
 11508|     InitSupportedInstructionSet ((int32_t)GCConfig::GetGCEnabledInstructionSets());
 11509| #endif
 11510|     if (!init_semi_shared())
 11511|     {
 11512|         GCToEEInterface::LogErrorToHost("PER_HEAP_ISOLATED data members initialization failed");
 11513|         hres = E_FAIL;
 11514|     }
 11515|     return hres;
 11516| }
 11517| int
 11518| gc_heap::init_semi_shared()
 11519| {
 11520|     int ret = 0;
 11521| #ifdef BGC_SERVO_TUNING
 11522|     uint32_t current_memory_load = 0;
 11523|     uint32_t sweep_flr_goal = 0;
 11524|     uint32_t sweep_flr_goal_loh = 0;
 11525| #endif //BGC_SERVO_TUNING
 11526| #ifndef USE_REGIONS
 11527|     eph_gen_starts_size = (Align (min_obj_size)) * max_generation;
 11528| #endif //!USE_REGIONS
 11529| #ifdef MULTIPLE_HEAPS
 11530|     mark_list_size = min (100*1024, max (8192, soh_segment_size/(2*10*32)));
 11531| #ifdef DYNAMIC_HEAP_COUNT
 11532|     if (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes)
 11533|     {
 11534|         g_mark_list_total_size = mark_list_size;
 11535|     }
 11536|     else
 11537| #endif //DYNAMIC_HEAP_COUNT
 11538|     {
 11539|         g_mark_list_total_size = mark_list_size*n_heaps;
 11540|     }
 11541|     g_mark_list = make_mark_list (g_mark_list_total_size);
 11542|     min_balance_threshold = alloc_quantum_balance_units * CLR_SIZE * 2;
 11543|     g_mark_list_copy = make_mark_list (g_mark_list_total_size);
 11544|     if (!g_mark_list_copy)
 11545|     {
 11546|         goto cleanup;
 11547|     }
 11548| #else //MULTIPLE_HEAPS
 11549|     mark_list_size = min(100*1024, max (8192, soh_segment_size/(64*32)));
 11550|     g_mark_list_total_size = mark_list_size;
 11551|     g_mark_list = make_mark_list (mark_list_size);
 11552| #endif //MULTIPLE_HEAPS
 11553|     dprintf (3, ("mark_list_size: %zd", mark_list_size));
 11554|     if (!g_mark_list)
 11555|     {
 11556|         goto cleanup;
 11557|     }
 11558| #ifdef MULTIPLE_HEAPS
 11559|     max_decommit_step_size = ((DECOMMIT_SIZE_PER_MILLISECOND * DECOMMIT_TIME_STEP_MILLISECONDS) / n_heaps);
 11560|     max_decommit_step_size = max (max_decommit_step_size, MIN_DECOMMIT_SIZE);
 11561| #endif //MULTIPLE_HEAPS
 11562| #ifdef FEATURE_BASICFREEZE
 11563|     seg_table = sorted_table::make_sorted_table();
 11564|     if (!seg_table)
 11565|         goto cleanup;
 11566| #endif //FEATURE_BASICFREEZE
 11567| #ifndef USE_REGIONS
 11568|     segment_standby_list = 0;
 11569| #endif //USE_REGIONS
 11570|     if (!full_gc_approach_event.CreateManualEventNoThrow(FALSE))
 11571|     {
 11572|         goto cleanup;
 11573|     }
 11574|     if (!full_gc_end_event.CreateManualEventNoThrow(FALSE))
 11575|     {
 11576|         goto cleanup;
 11577|     }
 11578|     fgn_loh_percent = 0;
 11579|     full_gc_approach_event_set = false;
 11580|     memset (full_gc_counts, 0, sizeof (full_gc_counts));
 11581| #ifndef USE_REGIONS
 11582|     should_expand_in_full_gc = FALSE;
 11583| #endif //!USE_REGIONS
 11584| #ifdef FEATURE_LOH_COMPACTION
 11585|     loh_compaction_always_p = GCConfig::GetLOHCompactionMode() != 0;
 11586|     loh_compaction_mode = loh_compaction_default;
 11587| #endif //FEATURE_LOH_COMPACTION
 11588|     loh_size_threshold = (size_t)GCConfig::GetLOHThreshold();
 11589|     assert (loh_size_threshold >= LARGE_OBJECT_SIZE);
 11590| #ifdef BGC_SERVO_TUNING
 11591|     memset (bgc_tuning::gen_calc, 0, sizeof (bgc_tuning::gen_calc));
 11592|     memset (bgc_tuning::gen_stats, 0, sizeof (bgc_tuning::gen_stats));
 11593|     memset (bgc_tuning::current_bgc_end_data, 0, sizeof (bgc_tuning::current_bgc_end_data));
 11594|     bgc_tuning::enable_fl_tuning = (GCConfig::GetBGCFLTuningEnabled() != 0);
 11595|     bgc_tuning::memory_load_goal = (uint32_t)GCConfig::GetBGCMemGoal();
 11596|     bgc_tuning::memory_load_goal_slack = (uint32_t)GCConfig::GetBGCMemGoalSlack();
 11597|     bgc_tuning::ml_kp = (double)GCConfig::GetBGCMLkp() / 1000.0;
 11598|     bgc_tuning::ml_ki = (double)GCConfig::GetBGCMLki() / 1000.0;
 11599|     bgc_tuning::ratio_correction_step = (double)GCConfig::GetBGCG2RatioStep() / 100.0;
 11600|     bgc_tuning::above_goal_kp = (double)GCConfig::GetBGCFLkp() / 1000000.0;
 11601|     bgc_tuning::enable_ki = (GCConfig::GetBGCFLEnableKi() != 0);
 11602|     bgc_tuning::above_goal_ki = (double)GCConfig::GetBGCFLki() / 1000000.0;
 11603|     bgc_tuning::enable_kd = (GCConfig::GetBGCFLEnableKd() != 0);
 11604|     bgc_tuning::above_goal_kd = (double)GCConfig::GetBGCFLkd() / 100.0;
 11605|     bgc_tuning::enable_smooth = (GCConfig::GetBGCFLEnableSmooth() != 0);
 11606|     bgc_tuning::num_gen1s_smooth_factor = (double)GCConfig::GetBGCFLSmoothFactor() / 100.0;
 11607|     bgc_tuning::enable_tbh = (GCConfig::GetBGCFLEnableTBH() != 0);
 11608|     bgc_tuning::enable_ff = (GCConfig::GetBGCFLEnableFF() != 0);
 11609|     bgc_tuning::above_goal_ff = (double)GCConfig::GetBGCFLff() / 100.0;
 11610|     bgc_tuning::enable_gradual_d = (GCConfig::GetBGCFLGradualD() != 0);
 11611|     sweep_flr_goal = (uint32_t)GCConfig::GetBGCFLSweepGoal();
 11612|     sweep_flr_goal_loh = (uint32_t)GCConfig::GetBGCFLSweepGoalLOH();
 11613|     bgc_tuning::gen_calc[0].sweep_flr_goal = ((sweep_flr_goal == 0) ? 20.0 : (double)sweep_flr_goal);
 11614|     bgc_tuning::gen_calc[1].sweep_flr_goal = ((sweep_flr_goal_loh == 0) ? 20.0 : (double)sweep_flr_goal_loh);
 11615|     bgc_tuning::available_memory_goal = (uint64_t)((double)gc_heap::total_physical_mem * (double)(100 - bgc_tuning::memory_load_goal) / 100);
 11616|     get_memory_info (&current_memory_load);
 11617|     dprintf (BGC_TUNING_LOG, ("BTL tuning %s!!!",
 11618|         (bgc_tuning::enable_fl_tuning ? "enabled" : "disabled")));
 11619| #ifdef SIMPLE_DPRINTF
 11620|     dprintf (BGC_TUNING_LOG, ("BTL tuning parameters: mem goal: %d%%(%zd), +/-%d%%, gen2 correction factor: %.2f, sweep flr goal: %d%%, smooth factor: %.3f(%s), TBH: %s, FF: %.3f(%s), ml: kp %.5f, ki %.10f",
 11621|         bgc_tuning::memory_load_goal,
 11622|         bgc_tuning::available_memory_goal,
 11623|         bgc_tuning::memory_load_goal_slack,
 11624|         bgc_tuning::ratio_correction_step,
 11625|         (int)bgc_tuning::gen_calc[0].sweep_flr_goal,
 11626|         bgc_tuning::num_gen1s_smooth_factor,
 11627|         (bgc_tuning::enable_smooth ? "enabled" : "disabled"),
 11628|         (bgc_tuning::enable_tbh ? "enabled" : "disabled"),
 11629|         bgc_tuning::above_goal_ff,
 11630|         (bgc_tuning::enable_ff ? "enabled" : "disabled"),
 11631|         bgc_tuning::ml_kp,
 11632|         bgc_tuning::ml_ki));
 11633|     dprintf (BGC_TUNING_LOG, ("BTL tuning parameters: kp: %.5f, ki: %.5f (%s), kd: %.3f (kd-%s, gd-%s), ff: %.3f",
 11634|         bgc_tuning::above_goal_kp,
 11635|         bgc_tuning::above_goal_ki,
 11636|         (bgc_tuning::enable_ki ? "enabled" : "disabled"),
 11637|         bgc_tuning::above_goal_kd,
 11638|         (bgc_tuning::enable_kd ? "enabled" : "disabled"),
 11639|         (bgc_tuning::enable_gradual_d ? "enabled" : "disabled"),
 11640|         bgc_tuning::above_goal_ff));
 11641| #endif //SIMPLE_DPRINTF
 11642|     if (bgc_tuning::enable_fl_tuning && (current_memory_load < bgc_tuning::memory_load_goal))
 11643|     {
 11644|         uint32_t distance_to_goal = bgc_tuning::memory_load_goal - current_memory_load;
 11645|         bgc_tuning::stepping_interval = max (distance_to_goal / 10, 1);
 11646|         bgc_tuning::last_stepping_mem_load = current_memory_load;
 11647|         bgc_tuning::last_stepping_bgc_count = 0;
 11648|         dprintf (BGC_TUNING_LOG, ("current ml: %d, %d to goal, interval: %d",
 11649|             current_memory_load, distance_to_goal, bgc_tuning::stepping_interval));
 11650|     }
 11651|     else
 11652|     {
 11653|         dprintf (BGC_TUNING_LOG, ("current ml: %d, >= goal: %d, disable stepping",
 11654|             current_memory_load, bgc_tuning::memory_load_goal));
 11655|         bgc_tuning::use_stepping_trigger_p = false;
 11656|     }
 11657| #endif //BGC_SERVO_TUNING
 11658| #ifdef BACKGROUND_GC
 11659|     memset (ephemeral_fgc_counts, 0, sizeof (ephemeral_fgc_counts));
 11660|     bgc_alloc_spin_count = static_cast<uint32_t>(GCConfig::GetBGCSpinCount());
 11661|     bgc_alloc_spin = static_cast<uint32_t>(GCConfig::GetBGCSpin());
 11662|     {
 11663|         int number_bgc_threads = get_num_heaps();
 11664|         if (!create_bgc_threads_support (number_bgc_threads))
 11665|         {
 11666|             goto cleanup;
 11667|         }
 11668|     }
 11669| #endif //BACKGROUND_GC
 11670|     memset (&current_no_gc_region_info, 0, sizeof (current_no_gc_region_info));
 11671| #ifdef GC_CONFIG_DRIVEN
 11672|     compact_or_sweep_gcs[0] = 0;
 11673|     compact_or_sweep_gcs[1] = 0;
 11674| #endif //GC_CONFIG_DRIVEN
 11675| #if defined(SHORT_PLUGS) && !defined(USE_REGIONS)
 11676|     short_plugs_pad_ratio = (double)DESIRED_PLUG_LENGTH / (double)(DESIRED_PLUG_LENGTH - Align (min_obj_size));
 11677| #endif //SHORT_PLUGS && !USE_REGIONS
 11678|     generation_skip_ratio_threshold = (int)GCConfig::GetGCLowSkipRatio();
 11679| #ifdef FEATURE_EVENT_TRACE
 11680|     gc_time_info = new (nothrow) uint64_t[max_compact_time_type];
 11681|     if (!gc_time_info)
 11682|     {
 11683|         goto cleanup;
 11684|     }
 11685| #ifdef BACKGROUND_GC
 11686|     bgc_time_info = new (nothrow) uint64_t[max_bgc_time_type];
 11687|     if (!bgc_time_info)
 11688|     {
 11689|         goto cleanup;
 11690|     }
 11691| #endif //BACKGROUND_GC
 11692| #ifdef FEATURE_LOH_COMPACTION
 11693|     loh_compact_info = new (nothrow) etw_loh_compact_info [get_num_heaps()];
 11694|     if (!loh_compact_info)
 11695|     {
 11696|         goto cleanup;
 11697|     }
 11698| #endif //FEATURE_LOH_COMPACTION
 11699| #endif //FEATURE_EVENT_TRACE
 11700|     reset_mm_p = TRUE;
 11701|     ret = 1;
 11702| cleanup:
 11703|     if (!ret)
 11704|     {
 11705|         if (full_gc_approach_event.IsValid())
 11706|         {
 11707|             full_gc_approach_event.CloseEvent();
 11708|         }
 11709|         if (full_gc_end_event.IsValid())
 11710|         {
 11711|             full_gc_end_event.CloseEvent();
 11712|         }
 11713|     }
 11714|     return ret;
 11715| }
 11716| gc_heap* gc_heap::make_gc_heap (
 11717| #ifdef MULTIPLE_HEAPS
 11718|                                 GCHeap* vm_hp,
 11719|                                 int heap_number
 11720| #endif //MULTIPLE_HEAPS
 11721|                                 )
 11722| {
 11723|     gc_heap* res = 0;
 11724| #ifdef MULTIPLE_HEAPS
 11725|     res = new (nothrow) gc_heap;
 11726|     if (!res)
 11727|         return 0;
 11728|     res->vm_heap = vm_hp;
 11729|     res->alloc_context_count = 0;
 11730| #ifndef USE_REGIONS
 11731|     res->mark_list_piece_start = new (nothrow) uint8_t**[n_heaps];
 11732|     if (!res->mark_list_piece_start)
 11733|         return 0;
 11734| #ifdef _PREFAST_
 11735| #pragma warning(push)
 11736| #pragma warning(disable:22011) // Suppress PREFast warning about integer underflow/overflow
 11737| #endif // _PREFAST_
 11738|     res->mark_list_piece_end = new (nothrow) uint8_t**[n_heaps + 32]; // +32 is padding to reduce false sharing
 11739| #ifdef _PREFAST_
 11740| #pragma warning(pop)
 11741| #endif // _PREFAST_
 11742|     if (!res->mark_list_piece_end)
 11743|         return 0;
 11744| #endif //!USE_REGIONS
 11745| #endif //MULTIPLE_HEAPS
 11746|     if (res->init_gc_heap (
 11747| #ifdef MULTIPLE_HEAPS
 11748|         heap_number
 11749| #else  //MULTIPLE_HEAPS
 11750|         0
 11751| #endif //MULTIPLE_HEAPS
 11752|         )==0)
 11753|     {
 11754|         return 0;
 11755|     }
 11756| #ifdef MULTIPLE_HEAPS
 11757|     return res;
 11758| #else
 11759|     return (gc_heap*)1;
 11760| #endif //MULTIPLE_HEAPS
 11761| }
 11762| uint32_t
 11763| gc_heap::wait_for_gc_done(int32_t timeOut)
 11764| {
 11765|     bool cooperative_mode = enable_preemptive ();
 11766|     uint32_t dwWaitResult = NOERROR;
 11767|     gc_heap* wait_heap = NULL;
 11768|     while (gc_heap::gc_started)
 11769|     {
 11770| #ifdef MULTIPLE_HEAPS
 11771|         wait_heap = GCHeap::GetHeap(heap_select::select_heap(NULL))->pGenGCHeap;
 11772|         dprintf(2, ("waiting for the gc_done_event on heap %d", wait_heap->heap_number));
 11773| #endif // MULTIPLE_HEAPS
 11774| #ifdef _PREFAST_
 11775|         PREFIX_ASSUME(wait_heap != NULL);
 11776| #endif // _PREFAST_
 11777|         dwWaitResult = wait_heap->gc_done_event.Wait(timeOut, FALSE);
 11778|     }
 11779|     disable_preemptive (cooperative_mode);
 11780|     return dwWaitResult;
 11781| }
 11782| void
 11783| gc_heap::set_gc_done()
 11784| {
 11785|     enter_gc_done_event_lock();
 11786|     if (!gc_done_event_set)
 11787|     {
 11788|         gc_done_event_set = true;
 11789|         dprintf (2, ("heap %d: setting gc_done_event", heap_number));
 11790|         gc_done_event.Set();
 11791|     }
 11792|     exit_gc_done_event_lock();
 11793| }
 11794| void
 11795| gc_heap::reset_gc_done()
 11796| {
 11797|     enter_gc_done_event_lock();
 11798|     if (gc_done_event_set)
 11799|     {
 11800|         gc_done_event_set = false;
 11801|         dprintf (2, ("heap %d: resetting gc_done_event", heap_number));
 11802|         gc_done_event.Reset();
 11803|     }
 11804|     exit_gc_done_event_lock();
 11805| }
 11806| void
 11807| gc_heap::enter_gc_done_event_lock()
 11808| {
 11809|     uint32_t dwSwitchCount = 0;
 11810| retry:
 11811|     if (Interlocked::CompareExchange(&gc_done_event_lock, 0, -1) >= 0)
 11812|     {
 11813|         while (gc_done_event_lock >= 0)
 11814|         {
 11815|             if  (g_num_processors > 1)
 11816|             {
 11817|                 int spin_count = yp_spin_count_unit;
 11818|                 for (int j = 0; j < spin_count; j++)
 11819|                 {
 11820|                     if  (gc_done_event_lock < 0)
 11821|                         break;
 11822|                     YieldProcessor();           // indicate to the processor that we are spinning
 11823|                 }
 11824|                 if  (gc_done_event_lock >= 0)
 11825|                     GCToOSInterface::YieldThread(++dwSwitchCount);
 11826|             }
 11827|             else
 11828|                 GCToOSInterface::YieldThread(++dwSwitchCount);
 11829|         }
 11830|         goto retry;
 11831|     }
 11832| }
 11833| void
 11834| gc_heap::exit_gc_done_event_lock()
 11835| {
 11836|     gc_done_event_lock = -1;
 11837| }
 11838| #ifndef MULTIPLE_HEAPS
 11839| #ifdef RECORD_LOH_STATE
 11840| int gc_heap::loh_state_index = 0;
 11841| gc_heap::loh_state_info gc_heap::last_loh_states[max_saved_loh_states];
 11842| #endif //RECORD_LOH_STATE
 11843| VOLATILE(int32_t) gc_heap::gc_done_event_lock;
 11844| VOLATILE(bool) gc_heap::gc_done_event_set;
 11845| GCEvent gc_heap::gc_done_event;
 11846| #endif //!MULTIPLE_HEAPS
 11847| VOLATILE(bool) gc_heap::internal_gc_done;
 11848| void gc_heap::add_saved_spinlock_info (
 11849|             bool loh_p,
 11850|             msl_enter_state enter_state,
 11851|             msl_take_state take_state,
 11852|             enter_msl_status msl_status)
 11853| {
 11854| #ifdef SPINLOCK_HISTORY
 11855|     if (!loh_p || (msl_status == msl_retry_different_heap))
 11856|     {
 11857|         return;
 11858|     }
 11859|     spinlock_info* current = &last_spinlock_info[spinlock_info_index];
 11860|     current->enter_state = enter_state;
 11861|     current->take_state = take_state;
 11862|     current->current_uoh_alloc_state = current_uoh_alloc_state;
 11863|     current->thread_id.SetToCurrentThread();
 11864|     current->loh_p = loh_p;
 11865|     dprintf (SPINLOCK_LOG, ("[%d]%s %s %s",
 11866|         heap_number,
 11867|         (loh_p ? "loh" : "soh"),
 11868|         ((enter_state == me_acquire) ? "E" : "L"),
 11869|         msl_take_state_str[take_state]));
 11870|     spinlock_info_index++;
 11871|     assert (spinlock_info_index <= max_saved_spinlock_info);
 11872|     if (spinlock_info_index >= max_saved_spinlock_info)
 11873|     {
 11874|         spinlock_info_index = 0;
 11875|     }
 11876| #else
 11877|     UNREFERENCED_PARAMETER(enter_state);
 11878|     UNREFERENCED_PARAMETER(take_state);
 11879| #endif //SPINLOCK_HISTORY
 11880| }
 11881| int
 11882| gc_heap::init_gc_heap (int h_number)
 11883| {
 11884| #ifdef MULTIPLE_HEAPS
 11885| #ifdef _DEBUG
 11886|     memset (committed_by_oh_per_heap, 0, sizeof (committed_by_oh_per_heap));
 11887| #endif
 11888|     g_heaps [h_number] = this;
 11889|     time_bgc_last = 0;
 11890| #ifdef SPINLOCK_HISTORY
 11891|     spinlock_info_index = 0;
 11892|     memset (last_spinlock_info, 0, sizeof(last_spinlock_info));
 11893| #endif //SPINLOCK_HISTORY
 11894| #ifndef USE_REGIONS
 11895|     ephemeral_low = (uint8_t*)1;
 11896|     ephemeral_high = MAX_PTR;
 11897| #endif //!USE_REGIONS
 11898|     gc_low = 0;
 11899|     gc_high = 0;
 11900|     ephemeral_heap_segment = 0;
 11901|     oomhist_index_per_heap = 0;
 11902|     freeable_uoh_segment = 0;
 11903|     condemned_generation_num = 0;
 11904|     blocking_collection = FALSE;
 11905|     generation_skip_ratio = 100;
 11906| #ifdef FEATURE_CARD_MARKING_STEALING
 11907|     n_eph_soh = 0;
 11908|     n_gen_soh = 0;
 11909|     n_eph_loh = 0;
 11910|     n_gen_loh = 0;
 11911| #endif //FEATURE_CARD_MARKING_STEALING
 11912|     mark_stack_tos = 0;
 11913|     mark_stack_bos = 0;
 11914|     mark_stack_array_length = 0;
 11915|     mark_stack_array = 0;
 11916| #if defined (_DEBUG) && defined (VERIFY_HEAP)
 11917|     verify_pinned_queue_p = FALSE;
 11918| #endif // _DEBUG && VERIFY_HEAP
 11919| #ifdef FEATURE_LOH_COMPACTION
 11920|     loh_pinned_queue_tos = 0;
 11921|     loh_pinned_queue_bos = 0;
 11922|     loh_pinned_queue_length = 0;
 11923|     loh_pinned_queue_decay = LOH_PIN_DECAY;
 11924|     loh_pinned_queue = 0;
 11925| #endif //FEATURE_LOH_COMPACTION
 11926|     min_overflow_address = MAX_PTR;
 11927|     max_overflow_address = 0;
 11928|     gen0_bricks_cleared = FALSE;
 11929|     gen0_must_clear_bricks = 0;
 11930|     allocation_quantum = CLR_SIZE;
 11931|     more_space_lock_soh = gc_lock;
 11932|     more_space_lock_uoh = gc_lock;
 11933|     ro_segments_in_range = FALSE;
 11934|     loh_alloc_since_cg = 0;
 11935| #ifndef USE_REGIONS
 11936|     new_heap_segment = NULL;
 11937| #endif //!USE_REGIONS
 11938|     gen0_allocated_after_gc_p = false;
 11939| #ifdef RECORD_LOH_STATE
 11940|     loh_state_index = 0;
 11941| #endif //RECORD_LOH_STATE
 11942| #ifdef USE_REGIONS
 11943|     new_gen0_regions_in_plns = 0;
 11944|     new_regions_in_prr = 0;
 11945|     new_regions_in_threading = 0;
 11946|     special_sweep_p = false;
 11947| #endif //USE_REGIONS
 11948| #endif //MULTIPLE_HEAPS
 11949| #ifdef MULTIPLE_HEAPS
 11950|     if (h_number > n_heaps)
 11951|     {
 11952|         assert (!"Number of heaps exceeded");
 11953|         return 0;
 11954|     }
 11955|     heap_number = h_number;
 11956| #endif //MULTIPLE_HEAPS
 11957|     memset (etw_allocation_running_amount, 0, sizeof (etw_allocation_running_amount));
 11958|     memset (allocated_since_last_gc, 0, sizeof (allocated_since_last_gc));
 11959|     memset (&oom_info, 0, sizeof (oom_info));
 11960|     memset (&fgm_result, 0, sizeof (fgm_result));
 11961|     memset (oomhist_per_heap, 0, sizeof (oomhist_per_heap));
 11962|     if (!gc_done_event.CreateManualEventNoThrow(FALSE))
 11963|     {
 11964|         return 0;
 11965|     }
 11966|     gc_done_event_lock = -1;
 11967|     gc_done_event_set = false;
 11968| #ifdef DYNAMIC_HEAP_COUNT
 11969|     if (h_number != 0)
 11970|     {
 11971|         if (!gc_idle_thread_event.CreateAutoEventNoThrow (FALSE))
 11972|         {
 11973|             return 0;
 11974|         }
 11975| #ifdef BACKGROUND_GC
 11976|         if (!bgc_idle_thread_event.CreateAutoEventNoThrow (FALSE))
 11977|         {
 11978|             return 0;
 11979|         }
 11980| #endif //BACKGROUND_GC
 11981|         dprintf (9999, ("creating idle events for h%d", h_number));
 11982|     }
 11983| #endif //DYNAMIC_HEAP_COUNT
 11984|     if (!init_dynamic_data())
 11985|     {
 11986|         return 0;
 11987|     }
 11988|     uint32_t* ct = &g_gc_card_table [card_word (card_of (g_gc_lowest_address))];
 11989|     own_card_table (ct);
 11990|     card_table = translate_card_table (ct);
 11991|     brick_table = card_table_brick_table (ct);
 11992|     highest_address = card_table_highest_address (ct);
 11993|     lowest_address = card_table_lowest_address (ct);
 11994| #ifdef CARD_BUNDLE
 11995|     card_bundle_table = translate_card_bundle_table (card_table_card_bundle_table (ct), g_gc_lowest_address);
 11996|     assert (&card_bundle_table [card_bundle_word (cardw_card_bundle (card_word (card_of (g_gc_lowest_address))))] ==
 11997|             card_table_card_bundle_table (ct));
 11998| #endif //CARD_BUNDLE
 11999| #ifdef BACKGROUND_GC
 12000|     background_saved_highest_address = nullptr;
 12001|     background_saved_lowest_address = nullptr;
 12002|     if (gc_can_use_concurrent)
 12003|         mark_array = translate_mark_array (card_table_mark_array (&g_gc_card_table[card_word (card_of (g_gc_lowest_address))]));
 12004|     else
 12005|         mark_array = NULL;
 12006| #endif //BACKGROUND_GC
 12007| #ifdef USE_REGIONS
 12008| #ifdef STRESS_REGIONS
 12009|     disable_preemptive (true);
 12010|     pinning_handles_for_alloc = new (nothrow) (OBJECTHANDLE[PINNING_HANDLE_INITIAL_LENGTH]);
 12011|     for (int i = 0; i < PINNING_HANDLE_INITIAL_LENGTH; i++)
 12012|     {
 12013|         pinning_handles_for_alloc[i] = g_gcGlobalHandleStore->CreateHandleOfType (0, HNDTYPE_PINNED);
 12014|     }
 12015|     enable_preemptive();
 12016|     ph_index_per_heap = 0;
 12017|     pinning_seg_interval = 2;
 12018|     num_gen0_regions = 0;
 12019|     sip_seg_interval = 2;
 12020|     sip_seg_maxgen_interval = 3;
 12021|     num_condemned_regions = 0;
 12022| #endif //STRESS_REGIONS
 12023|     end_gen0_region_space = 0;
 12024|     end_gen0_region_committed_space = 0;
 12025|     gen0_pinned_free_space = 0;
 12026|     gen0_large_chunk_found = false;
 12027|     if (!initial_make_soh_regions (__this) ||
 12028|         !initial_make_uoh_regions (loh_generation, __this) ||
 12029|         !initial_make_uoh_regions (poh_generation, __this))
 12030|     {
 12031|         return 0;
 12032|     }
 12033| #else //USE_REGIONS
 12034|     heap_segment* seg = make_initial_segment (soh_gen0, h_number, __this);
 12035|     if (!seg)
 12036|         return 0;
 12037|     FIRE_EVENT(GCCreateSegment_V1, heap_segment_mem(seg),
 12038|                               (size_t)(heap_segment_reserved (seg) - heap_segment_mem(seg)),
 12039|                               gc_etw_segment_small_object_heap);
 12040|     seg_mapping_table_add_segment (seg, __this);
 12041| #ifdef MULTIPLE_HEAPS
 12042|     assert (heap_segment_heap (seg) == __this);
 12043| #endif //MULTIPLE_HEAPS
 12044|     uint8_t*  start = heap_segment_mem (seg);
 12045|     for (int i = max_generation; i >= 0; i--)
 12046|     {
 12047|         make_generation (i, seg, start);
 12048|         start += Align (min_obj_size);
 12049|     }
 12050|     heap_segment_allocated (seg) = start;
 12051|     alloc_allocated = start;
 12052|     heap_segment_used (seg) = start - plug_skew;
 12053|     ephemeral_heap_segment = seg;
 12054|     heap_segment* lseg = make_initial_segment(loh_generation, h_number, __this);
 12055|     if (!lseg)
 12056|         return 0;
 12057|     lseg->flags |= heap_segment_flags_loh;
 12058|     FIRE_EVENT(GCCreateSegment_V1, heap_segment_mem(lseg),
 12059|                               (size_t)(heap_segment_reserved (lseg) - heap_segment_mem(lseg)),
 12060|                               gc_etw_segment_large_object_heap);
 12061|     heap_segment* pseg = make_initial_segment (poh_generation, h_number, __this);
 12062|     if (!pseg)
 12063|         return 0;
 12064|     pseg->flags |= heap_segment_flags_poh;
 12065|     FIRE_EVENT(GCCreateSegment_V1, heap_segment_mem(pseg),
 12066|                               (size_t)(heap_segment_reserved (pseg) - heap_segment_mem(pseg)),
 12067|                               gc_etw_segment_pinned_object_heap);
 12068|     seg_mapping_table_add_segment (lseg, __this);
 12069|     seg_mapping_table_add_segment (pseg, __this);
 12070|     make_generation (loh_generation, lseg, heap_segment_mem (lseg));
 12071|     make_generation (poh_generation, pseg, heap_segment_mem (pseg));
 12072|     heap_segment_allocated (lseg) = heap_segment_mem (lseg) + Align (min_obj_size, get_alignment_constant (FALSE));
 12073|     heap_segment_used (lseg) = heap_segment_allocated (lseg) - plug_skew;
 12074|     heap_segment_allocated (pseg) = heap_segment_mem (pseg) + Align (min_obj_size, get_alignment_constant (FALSE));
 12075|     heap_segment_used (pseg) = heap_segment_allocated (pseg) - plug_skew;
 12076|     for (int gen_num = 0; gen_num < total_generation_count; gen_num++)
 12077|     {
 12078|         generation*  gen = generation_of (gen_num);
 12079|         make_unused_array (generation_allocation_start (gen), Align (min_obj_size));
 12080|     }
 12081| #ifdef MULTIPLE_HEAPS
 12082|     assert (heap_segment_heap (lseg) == __this);
 12083|     assert (heap_segment_heap (pseg) == __this);
 12084| #endif //MULTIPLE_HEAPS
 12085| #endif //USE_REGIONS
 12086| #ifdef MULTIPLE_HEAPS
 12087|     generation_alloc_context (generation_of (soh_gen0))->set_alloc_heap(vm_heap);
 12088|     generation_alloc_context (generation_of (loh_generation))->set_alloc_heap(vm_heap);
 12089|     generation_alloc_context (generation_of (poh_generation))->set_alloc_heap(vm_heap);
 12090| #endif //MULTIPLE_HEAPS
 12091|     generation_of (max_generation)->free_list_allocator = allocator(NUM_GEN2_ALIST, BASE_GEN2_ALIST_BITS, gen2_alloc_list, max_generation);
 12092|     generation_of (loh_generation)->free_list_allocator = allocator(NUM_LOH_ALIST, BASE_LOH_ALIST_BITS, loh_alloc_list);
 12093|     generation_of (poh_generation)->free_list_allocator = allocator(NUM_POH_ALIST, BASE_POH_ALIST_BITS, poh_alloc_list);
 12094|     total_alloc_bytes_soh = 0;
 12095|     total_alloc_bytes_uoh = 0;
 12096| #ifdef MULTIPLE_HEAPS
 12097| #ifdef STRESS_DYNAMIC_HEAP_COUNT
 12098|     uoh_msl_before_gc_p = false;
 12099| #endif //STRESS_DYNAMIC_HEAP_COUNT
 12100| #else //MULTIPLE_HEAPS
 12101|     allocation_running_amount = dd_min_size (dynamic_data_of (0));
 12102| #endif //!MULTIPLE_HEAPS
 12103|     fgn_maxgen_percent = 0;
 12104|     fgn_last_alloc = dd_min_size (dynamic_data_of (0));
 12105|     mark* arr = new (nothrow) (mark [MARK_STACK_INITIAL_LENGTH]);
 12106|     if (!arr)
 12107|         return 0;
 12108|     make_mark_stack(arr);
 12109| #ifdef BACKGROUND_GC
 12110| #ifdef BGC_SERVO_TUNING
 12111|     loh_a_no_bgc = 0;
 12112|     loh_a_bgc_marking = 0;
 12113|     loh_a_bgc_planning = 0;
 12114|     bgc_maxgen_end_fl_size = 0;
 12115| #endif //BGC_SERVO_TUNING
 12116|     freeable_soh_segment = 0;
 12117|     gchist_index_per_heap = 0;
 12118|     if (gc_can_use_concurrent)
 12119|     {
 12120|         uint8_t** b_arr = new (nothrow) (uint8_t * [MARK_STACK_INITIAL_LENGTH]);
 12121|         if (!b_arr)
 12122|             return 0;
 12123|         make_background_mark_stack(b_arr);
 12124|     }
 12125| #endif //BACKGROUND_GC
 12126| #ifndef USE_REGIONS
 12127|     ephemeral_low = generation_allocation_start(generation_of(max_generation - 1));
 12128|     ephemeral_high = heap_segment_reserved(ephemeral_heap_segment);
 12129| #endif //!USE_REGIONS
 12130|     if (heap_number == 0)
 12131|     {
 12132|         stomp_write_barrier_initialize(
 12133| #if defined(USE_REGIONS)
 12134|             ephemeral_low, ephemeral_high,
 12135|             map_region_to_generation_skewed, (uint8_t)min_segment_size_shr
 12136| #elif defined(MULTIPLE_HEAPS)
 12137|             reinterpret_cast<uint8_t*>(1), reinterpret_cast<uint8_t*>(~0)
 12138| #else
 12139|             ephemeral_low, ephemeral_high
 12140| #endif //MULTIPLE_HEAPS || USE_REGIONS
 12141|         );
 12142|     }
 12143| #ifdef MULTIPLE_HEAPS
 12144|     if (!create_gc_thread ())
 12145|         return 0;
 12146| #endif //MULTIPLE_HEAPS
 12147| #ifdef FEATURE_PREMORTEM_FINALIZATION
 12148|     HRESULT hr = AllocateCFinalize(&finalize_queue);
 12149|     if (FAILED(hr))
 12150|         return 0;
 12151| #endif // FEATURE_PREMORTEM_FINALIZATION
 12152| #ifdef USE_REGIONS
 12153| #ifdef MULTIPLE_HEAPS
 12154|     min_fl_list = 0;
 12155|     num_fl_items_rethreaded_stage2 = 0;
 12156|     free_list_space_per_heap = nullptr;
 12157| #endif //MULTIPLE_HEAPS
 12158| #else //USE_REGIONS
 12159|     max_free_space_items = MAX_NUM_FREE_SPACES;
 12160|     bestfit_seg = new (nothrow) seg_free_spaces (heap_number);
 12161|     if (!bestfit_seg)
 12162|     {
 12163|         return 0;
 12164|     }
 12165|     if (!bestfit_seg->alloc())
 12166|     {
 12167|         return 0;
 12168|     }
 12169| #endif //USE_REGIONS
 12170|     last_gc_before_oom = FALSE;
 12171|     sufficient_gen0_space_p = FALSE;
 12172| #ifdef MULTIPLE_HEAPS
 12173| #ifdef HEAP_ANALYZE
 12174|     heap_analyze_success = TRUE;
 12175|     internal_root_array  = 0;
 12176|     internal_root_array_index = 0;
 12177|     internal_root_array_length = initial_internal_roots;
 12178|     current_obj          = 0;
 12179|     current_obj_size     = 0;
 12180| #endif //HEAP_ANALYZE
 12181| #endif // MULTIPLE_HEAPS
 12182| #ifdef BACKGROUND_GC
 12183|     bgc_thread_id.Clear();
 12184|     if (!create_bgc_thread_support())
 12185|     {
 12186|         return 0;
 12187|     }
 12188|     bgc_alloc_lock = new (nothrow) exclusive_sync;
 12189|     if (!bgc_alloc_lock)
 12190|     {
 12191|         return 0;
 12192|     }
 12193|     bgc_alloc_lock->init();
 12194|     bgc_thread_running = 0;
 12195|     bgc_thread = 0;
 12196|     bgc_threads_timeout_cs.Initialize();
 12197|     current_bgc_state = bgc_not_in_process;
 12198|     background_soh_alloc_count = 0;
 12199|     background_uoh_alloc_count = 0;
 12200|     bgc_overflow_count = 0;
 12201|     end_loh_size = dd_min_size (dynamic_data_of (loh_generation));
 12202|     end_poh_size = dd_min_size (dynamic_data_of (poh_generation));
 12203|     current_sweep_pos = 0;
 12204| #ifdef DOUBLY_LINKED_FL
 12205|     current_sweep_seg = 0;
 12206| #endif //DOUBLY_LINKED_FL
 12207| #endif //BACKGROUND_GC
 12208| #ifdef GC_CONFIG_DRIVEN
 12209|     memset(interesting_data_per_heap, 0, sizeof (interesting_data_per_heap));
 12210|     memset(compact_reasons_per_heap, 0, sizeof (compact_reasons_per_heap));
 12211|     memset(expand_mechanisms_per_heap, 0, sizeof (expand_mechanisms_per_heap));
 12212|     memset(interesting_mechanism_bits_per_heap, 0, sizeof (interesting_mechanism_bits_per_heap));
 12213| #endif //GC_CONFIG_DRIVEN
 12214|     return 1;
 12215| }
 12216| void
 12217| gc_heap::destroy_semi_shared()
 12218| {
 12219|     if (g_mark_list)
 12220|         delete g_mark_list;
 12221|     if (seg_mapping_table)
 12222|         delete seg_mapping_table;
 12223| #ifdef FEATURE_BASICFREEZE
 12224|     seg_table->delete_sorted_table();
 12225| #endif //FEATURE_BASICFREEZE
 12226| }
 12227| void
 12228| gc_heap::self_destroy()
 12229| {
 12230| #ifdef BACKGROUND_GC
 12231|     kill_gc_thread();
 12232| #endif //BACKGROUND_GC
 12233|     if (gc_done_event.IsValid())
 12234|     {
 12235|         gc_done_event.CloseEvent();
 12236|     }
 12237|     for (int i = get_start_generation_index(); i < total_generation_count; i++)
 12238|     {
 12239|         heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (i)));
 12240|         PREFIX_ASSUME(seg != NULL);
 12241|         while (seg)
 12242|         {
 12243|             heap_segment* next_seg = heap_segment_next_rw (seg);
 12244|             delete_heap_segment (seg);
 12245|             seg = next_seg;
 12246|         }
 12247|     }
 12248|     release_card_table (card_table);
 12249|     delete[] mark_stack_array;
 12250| #ifdef FEATURE_PREMORTEM_FINALIZATION
 12251|     if (finalize_queue)
 12252|         delete finalize_queue;
 12253| #endif // FEATURE_PREMORTEM_FINALIZATION
 12254| }
 12255| void
 12256| gc_heap::destroy_gc_heap(gc_heap* heap)
 12257| {
 12258|     heap->self_destroy();
 12259|     delete heap;
 12260| }
 12261| void gc_heap::shutdown_gc()
 12262| {
 12263|     destroy_semi_shared();
 12264| #ifdef MULTIPLE_HEAPS
 12265|     delete g_heaps;
 12266|     destroy_thread_support();
 12267|     n_heaps = 0;
 12268| #endif //MULTIPLE_HEAPS
 12269|     destroy_initial_memory();
 12270|     GCToOSInterface::Shutdown();
 12271| }
 12272| inline
 12273| BOOL gc_heap::size_fit_p (size_t size REQD_ALIGN_AND_OFFSET_DCL, uint8_t* alloc_pointer, uint8_t* alloc_limit,
 12274|                           uint8_t* old_loc, int use_padding)
 12275| {
 12276|     BOOL already_padded = FALSE;
 12277| #ifdef SHORT_PLUGS
 12278|     if ((old_loc != 0) && (use_padding & USE_PADDING_FRONT))
 12279|     {
 12280|         alloc_pointer = alloc_pointer + Align (min_obj_size);
 12281|         already_padded = TRUE;
 12282|     }
 12283| #endif //SHORT_PLUGS
 12284|     if (!((old_loc == 0) || same_large_alignment_p (old_loc, alloc_pointer)))
 12285|         size = size + switch_alignment_size (already_padded);
 12286| #ifdef FEATURE_STRUCTALIGN
 12287|     alloc_pointer = StructAlign(alloc_pointer, requiredAlignment, alignmentOffset);
 12288| #endif // FEATURE_STRUCTALIGN
 12289|     if (alloc_limit < alloc_pointer)
 12290|     {
 12291|         return FALSE;
 12292|     }
 12293|     if (old_loc != 0)
 12294|     {
 12295|         return (((size_t)(alloc_limit - alloc_pointer) >= (size + ((use_padding & USE_PADDING_TAIL)? Align(min_obj_size) : 0)))
 12296| #ifdef SHORT_PLUGS
 12297|                 ||((!(use_padding & USE_PADDING_FRONT)) && ((alloc_pointer + size) == alloc_limit))
 12298| #else //SHORT_PLUGS
 12299|                 ||((alloc_pointer + size) == alloc_limit)
 12300| #endif //SHORT_PLUGS
 12301|             );
 12302|     }
 12303|     else
 12304|     {
 12305|         assert (size == Align (min_obj_size));
 12306|         return ((size_t)(alloc_limit - alloc_pointer) >= size);
 12307|     }
 12308| }
 12309| inline
 12310| BOOL gc_heap::a_size_fit_p (size_t size, uint8_t* alloc_pointer, uint8_t* alloc_limit,
 12311|                             int align_const)
 12312| {
 12313|     if (alloc_limit < alloc_pointer)
 12314|     {
 12315|         return FALSE;
 12316|     }
 12317|     return ((size_t)(alloc_limit - alloc_pointer) >= (size + Align(min_obj_size, align_const)));
 12318| }
 12319| BOOL gc_heap::grow_heap_segment (heap_segment* seg, uint8_t* high_address, bool* hard_limit_exceeded_p)
 12320| {
 12321|     assert (high_address <= heap_segment_reserved (seg));
 12322|     if (hard_limit_exceeded_p)
 12323|         *hard_limit_exceeded_p = false;
 12324|     if (align_on_page (high_address) > heap_segment_reserved (seg))
 12325|         return FALSE;
 12326|     if (high_address <= heap_segment_committed (seg))
 12327|         return TRUE;
 12328|     size_t c_size = align_on_page ((size_t)(high_address - heap_segment_committed (seg)));
 12329|     c_size = max (c_size, commit_min_th);
 12330|     c_size = min (c_size, (size_t)(heap_segment_reserved (seg) - heap_segment_committed (seg)));
 12331|     if (c_size == 0)
 12332|         return FALSE;
 12333|     STRESS_LOG2(LF_GC, LL_INFO10000,
 12334|                 "Growing heap_segment: %zx high address: %zx\n",
 12335|                 (size_t)seg, (size_t)high_address);
 12336|     bool ret = virtual_commit (heap_segment_committed (seg), c_size, heap_segment_oh (seg), heap_number, hard_limit_exceeded_p);
 12337|     if (ret)
 12338|     {
 12339|         heap_segment_committed (seg) += c_size;
 12340|         STRESS_LOG1(LF_GC, LL_INFO10000, "New commit: %zx\n",
 12341|                     (size_t)heap_segment_committed (seg));
 12342|         assert (heap_segment_committed (seg) <= heap_segment_reserved (seg));
 12343|         assert (high_address <= heap_segment_committed (seg));
 12344| #if defined(MULTIPLE_HEAPS) && !defined(USE_REGIONS)
 12345|         assert (!gradual_decommit_in_progress_p ||
 12346|                 (seg != ephemeral_heap_segment) ||
 12347|                 (heap_segment_committed (seg) <= heap_segment_decommit_target (seg)));
 12348| #endif //MULTIPLE_HEAPS && !USE_REGIONS
 12349|     }
 12350|     return !!ret;
 12351| }
 12352| inline
 12353| int gc_heap::grow_heap_segment (heap_segment* seg, uint8_t* allocated, uint8_t* old_loc, size_t size,
 12354|                                 BOOL pad_front_p  REQD_ALIGN_AND_OFFSET_DCL)
 12355| {
 12356|     BOOL already_padded = FALSE;
 12357| #ifdef SHORT_PLUGS
 12358|     if ((old_loc != 0) && pad_front_p)
 12359|     {
 12360|         allocated = allocated + Align (min_obj_size);
 12361|         already_padded = TRUE;
 12362|     }
 12363| #endif //SHORT_PLUGS
 12364|     if (!((old_loc == 0) || same_large_alignment_p (old_loc, allocated)))
 12365|         size += switch_alignment_size (already_padded);
 12366| #ifdef FEATURE_STRUCTALIGN
 12367|     size_t pad = ComputeStructAlignPad(allocated, requiredAlignment, alignmentOffset);
 12368|     return grow_heap_segment (seg, allocated + pad + size);
 12369| #else // FEATURE_STRUCTALIGN
 12370|     return grow_heap_segment (seg, allocated + size);
 12371| #endif // FEATURE_STRUCTALIGN
 12372| }
 12373| void gc_heap::thread_free_item_front (generation* gen, uint8_t* free_start, size_t free_size)
 12374| {
 12375|     make_unused_array (free_start, free_size);
 12376|     generation_free_list_space (gen) += free_size;
 12377|     generation_allocator(gen)->thread_item_front (free_start, free_size);
 12378|     add_gen_free (gen->gen_num, free_size);
 12379|     if (gen->gen_num == max_generation)
 12380|     {
 12381|         dprintf (2, ("AO h%d: gen2F+: %p(%zd)->%zd, FO: %zd",
 12382|             heap_number, free_start, free_size,
 12383|             generation_free_list_space (gen), generation_free_obj_space (gen)));
 12384|     }
 12385| }
 12386| #ifdef DOUBLY_LINKED_FL
 12387| void gc_heap::thread_item_front_added (generation* gen, uint8_t* free_start, size_t free_size)
 12388| {
 12389|     make_unused_array (free_start, free_size);
 12390|     generation_free_list_space (gen) += free_size;
 12391|     int bucket_index = generation_allocator(gen)->thread_item_front_added (free_start, free_size);
 12392|     if (gen->gen_num == max_generation)
 12393|     {
 12394|         dprintf (2, ("AO [h%d] gen2FL+: %p(%zd)->%zd",
 12395|             heap_number, free_start, free_size, generation_free_list_space (gen)));
 12396|     }
 12397|     add_gen_free (gen->gen_num, free_size);
 12398| }
 12399| #endif //DOUBLY_LINKED_FL
 12400| void gc_heap::make_free_obj (generation* gen, uint8_t* free_start, size_t free_size)
 12401| {
 12402|     make_unused_array (free_start, free_size);
 12403|     generation_free_obj_space (gen) += free_size;
 12404|     if (gen->gen_num == max_generation)
 12405|     {
 12406|         dprintf (2, ("AO [h%d] gen2FO+: %p(%zd)->%zd",
 12407|             heap_number, free_start, free_size, generation_free_obj_space (gen)));
 12408|     }
 12409| }
 12410| void gc_heap::adjust_limit (uint8_t* start, size_t limit_size, generation* gen)
 12411| {
 12412|     dprintf (3, ("gc Expanding segment allocation"));
 12413|     heap_segment* seg = generation_allocation_segment (gen);
 12414|     if ((generation_allocation_limit (gen) != start) || (start != heap_segment_plan_allocated (seg)))
 12415|     {
 12416|         if (generation_allocation_limit (gen) == heap_segment_plan_allocated (seg))
 12417|         {
 12418|             assert (generation_allocation_pointer (gen) >= heap_segment_mem (seg));
 12419|             assert (generation_allocation_pointer (gen) <= heap_segment_committed (seg));
 12420|             heap_segment_plan_allocated (generation_allocation_segment (gen)) = generation_allocation_pointer (gen);
 12421|         }
 12422|         else
 12423|         {
 12424|             uint8_t*  hole = generation_allocation_pointer (gen);
 12425|             size_t  size = (generation_allocation_limit (gen) - generation_allocation_pointer (gen));
 12426|             if (size != 0)
 12427|             {
 12428|                 dprintf (3, ("filling up hole: %p, size %zx", hole, size));
 12429|                 size_t allocated_size = generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen);
 12430| #ifdef DOUBLY_LINKED_FL
 12431|                 if (gen->gen_num == max_generation)
 12432|                 {
 12433|                     if (allocated_size <= min_free_item_no_prev)
 12434|                     {
 12435|                         size_t* filler_free_obj_size_location = (size_t*)(generation_allocation_context_start_region (gen) +
 12436|                                                                           min_free_item_no_prev);
 12437|                         size_t filler_free_obj_size = 0;
 12438|                         if (size >= (Align (min_free_list) + Align (min_obj_size)))
 12439|                         {
 12440|                             filler_free_obj_size = Align (min_obj_size);
 12441|                             size_t fl_size = size - filler_free_obj_size;
 12442|                             thread_item_front_added (gen, (hole + filler_free_obj_size), fl_size);
 12443|                         }
 12444|                         else
 12445|                         {
 12446|                             filler_free_obj_size = size;
 12447|                         }
 12448|                         generation_free_obj_space (gen) += filler_free_obj_size;
 12449|                         *filler_free_obj_size_location = filler_free_obj_size;
 12450|                         uint8_t* old_loc = generation_last_free_list_allocated (gen);
 12451|                         uint8_t* saved_plug_and_gap = nullptr;
 12452|                         if (saved_pinned_plug_index != INVALID_SAVED_PINNED_PLUG_INDEX)
 12453|                         {
 12454|                             saved_plug_and_gap = pinned_plug (pinned_plug_of (saved_pinned_plug_index)) - sizeof(plug_and_gap);
 12455|                             dprintf (3333, ("[h%d] sppi: %zd mtos: %zd old_loc: %p pp: %p(%zd) offs: %zd",
 12456|                                 heap_number,
 12457|                                 saved_pinned_plug_index,
 12458|                                 mark_stack_tos,
 12459|                                 old_loc,
 12460|                                 pinned_plug (pinned_plug_of (saved_pinned_plug_index)),
 12461|                                 pinned_len (pinned_plug_of (saved_pinned_plug_index)),
 12462|                                 old_loc - saved_plug_and_gap));
 12463|                         }
 12464|                         size_t offset = old_loc - saved_plug_and_gap;
 12465|                         if (offset < sizeof(gap_reloc_pair))
 12466|                         {
 12467|                             assert (offset <= sizeof(plug_and_gap) - min_obj_size);
 12468|                             set_free_obj_in_compact_bit ((uint8_t*)(&pinned_plug_of (saved_pinned_plug_index)->saved_pre_plug_reloc) + offset);
 12469|                         }
 12470|                         else
 12471|                         {
 12472| #ifdef _DEBUG
 12473|                             header(old_loc)->Validate();
 12474| #endif //_DEBUG
 12475|                             set_free_obj_in_compact_bit (old_loc);
 12476|                         }
 12477|                         dprintf (3333, ("[h%d] ac: %p->%p((%zd < %zd), Pset %p s->%zd", heap_number,
 12478|                             generation_allocation_context_start_region (gen), generation_allocation_pointer (gen),
 12479|                             allocated_size, min_free_item_no_prev, filler_free_obj_size_location, filler_free_obj_size));
 12480|                     }
 12481|                     else
 12482|                     {
 12483|                         if (size >= Align (min_free_list))
 12484|                         {
 12485|                             thread_item_front_added (gen, hole, size);
 12486|                         }
 12487|                         else
 12488|                         {
 12489|                             make_free_obj (gen, hole, size);
 12490|                         }
 12491|                     }
 12492|                 }
 12493|                 else
 12494| #endif //DOUBLY_LINKED_FL
 12495|                 {
 12496|                     if (size >= Align (min_free_list))
 12497|                     {
 12498|                         if (allocated_size < min_free_item_no_prev)
 12499|                         {
 12500|                             if (size >= (Align (min_free_list) + Align (min_obj_size)))
 12501|                             {
 12502|                                 make_free_obj (gen, hole, min_obj_size);
 12503|                                 thread_free_item_front (gen, (hole + Align (min_obj_size)),
 12504|                                     (size - Align (min_obj_size)));
 12505|                             }
 12506|                             else
 12507|                             {
 12508|                                 dprintf (3, ("allocated size too small, can't put back rest on free list %zx",
 12509|                                     allocated_size));
 12510|                                 make_free_obj (gen, hole, size);
 12511|                             }
 12512|                         }
 12513|                         else
 12514|                         {
 12515|                             dprintf (3, ("threading hole in front of free list"));
 12516|                             thread_free_item_front (gen, hole, size);
 12517|                         }
 12518|                     }
 12519|                     else
 12520|                     {
 12521|                         make_free_obj (gen, hole, size);
 12522|                     }
 12523|                 }
 12524|             }
 12525|         }
 12526|         generation_allocation_pointer (gen) = start;
 12527|         generation_allocation_context_start_region (gen) = start;
 12528|     }
 12529|     generation_allocation_limit (gen) = (start + limit_size);
 12530| }
 12531| void verify_mem_cleared (uint8_t* start, size_t size)
 12532| {
 12533|     if (!Aligned (size))
 12534|     {
 12535|         FATAL_GC_ERROR();
 12536|     }
 12537|     PTR_PTR curr_ptr = (PTR_PTR) start;
 12538|     for (size_t i = 0; i < size / sizeof(PTR_PTR); i++)
 12539|     {
 12540|         if (*(curr_ptr++) != 0)
 12541|         {
 12542|             FATAL_GC_ERROR();
 12543|         }
 12544|     }
 12545| }
 12546| #if defined (VERIFY_HEAP) && defined (BACKGROUND_GC)
 12547| void gc_heap::set_batch_mark_array_bits (uint8_t* start, uint8_t* end)
 12548| {
 12549|     size_t start_mark_bit = mark_bit_of (start);
 12550|     size_t end_mark_bit = mark_bit_of (end);
 12551|     unsigned int startbit = mark_bit_bit (start_mark_bit);
 12552|     unsigned int endbit = mark_bit_bit (end_mark_bit);
 12553|     size_t startwrd = mark_bit_word (start_mark_bit);
 12554|     size_t endwrd = mark_bit_word (end_mark_bit);
 12555|     dprintf (3, ("Setting all mark array bits between [%zx:%zx-[%zx:%zx",
 12556|         (size_t)start, (size_t)start_mark_bit,
 12557|         (size_t)end, (size_t)end_mark_bit));
 12558|     unsigned int firstwrd = ~(lowbits (~0, startbit));
 12559|     unsigned int lastwrd = ~(highbits (~0, endbit));
 12560|     if (startwrd == endwrd)
 12561|     {
 12562|         unsigned int wrd = firstwrd & lastwrd;
 12563|         mark_array[startwrd] |= wrd;
 12564|         return;
 12565|     }
 12566|     if (startbit)
 12567|     {
 12568|         mark_array[startwrd] |= firstwrd;
 12569|         startwrd++;
 12570|     }
 12571|     for (size_t wrdtmp = startwrd; wrdtmp < endwrd; wrdtmp++)
 12572|     {
 12573|         mark_array[wrdtmp] = ~(unsigned int)0;
 12574|     }
 12575|     if (endbit)
 12576|     {
 12577|         mark_array[endwrd] |= lastwrd;
 12578|     }
 12579| }
 12580| void gc_heap::check_batch_mark_array_bits (uint8_t* start, uint8_t* end)
 12581| {
 12582|     size_t start_mark_bit = mark_bit_of (start);
 12583|     size_t end_mark_bit = mark_bit_of (end);
 12584|     unsigned int startbit = mark_bit_bit (start_mark_bit);
 12585|     unsigned int endbit = mark_bit_bit (end_mark_bit);
 12586|     size_t startwrd = mark_bit_word (start_mark_bit);
 12587|     size_t endwrd = mark_bit_word (end_mark_bit);
 12588|     unsigned int firstwrd = ~(lowbits (~0, startbit));
 12589|     unsigned int lastwrd = ~(highbits (~0, endbit));
 12590|     if (startwrd == endwrd)
 12591|     {
 12592|         unsigned int wrd = firstwrd & lastwrd;
 12593|         if (mark_array[startwrd] & wrd)
 12594|         {
 12595|             dprintf  (1, ("The %x portion of mark bits at 0x%zx:0x%x(addr: 0x%p) were not cleared",
 12596|                             wrd, startwrd,
 12597|                             mark_array [startwrd], mark_word_address (startwrd)));
 12598|             FATAL_GC_ERROR();
 12599|         }
 12600|         return;
 12601|     }
 12602|     if (startbit)
 12603|     {
 12604|         if (mark_array[startwrd] & firstwrd)
 12605|         {
 12606|             dprintf  (1, ("The %x portion of mark bits at 0x%zx:0x%x(addr: 0x%p) were not cleared",
 12607|                             firstwrd, startwrd,
 12608|                             mark_array [startwrd], mark_word_address (startwrd)));
 12609|             FATAL_GC_ERROR();
 12610|         }
 12611|         startwrd++;
 12612|     }
 12613|     for (size_t wrdtmp = startwrd; wrdtmp < endwrd; wrdtmp++)
 12614|     {
 12615|         if (mark_array[wrdtmp])
 12616|         {
 12617|             dprintf  (1, ("The mark bits at 0x%zx:0x%x(addr: 0x%p) were not cleared",
 12618|                             wrdtmp,
 12619|                             mark_array [wrdtmp], mark_word_address (wrdtmp)));
 12620|             FATAL_GC_ERROR();
 12621|         }
 12622|     }
 12623|     if (endbit)
 12624|     {
 12625|         if (mark_array[endwrd] & lastwrd)
 12626|         {
 12627|             dprintf  (1, ("The %x portion of mark bits at 0x%x:0x%x(addr: 0x%p) were not cleared",
 12628|                             lastwrd, lastwrd,
 12629|                             mark_array [lastwrd], mark_word_address (lastwrd)));
 12630|             FATAL_GC_ERROR();
 12631|         }
 12632|     }
 12633| }
 12634| #endif //VERIFY_HEAP && BACKGROUND_GC
 12635| allocator::allocator (unsigned int num_b, int fbb, alloc_list* b, int gen)
 12636| {
 12637|     assert (num_b < MAX_BUCKET_COUNT);
 12638|     num_buckets = num_b;
 12639|     first_bucket_bits = fbb;
 12640|     buckets = b;
 12641|     gen_number = gen;
 12642| }
 12643| alloc_list& allocator::alloc_list_of (unsigned int bn)
 12644| {
 12645|     assert (bn < num_buckets);
 12646|     if (bn == 0)
 12647|         return first_bucket;
 12648|     else
 12649|         return buckets [bn-1];
 12650| }
 12651| size_t& allocator::alloc_list_damage_count_of (unsigned int bn)
 12652| {
 12653|     assert (bn < num_buckets);
 12654|     if (bn == 0)
 12655|         return first_bucket.alloc_list_damage_count();
 12656|     else
 12657|         return buckets [bn-1].alloc_list_damage_count();
 12658| }
 12659| void allocator::unlink_item (unsigned int bn, uint8_t* item, uint8_t* prev_item, BOOL use_undo_p)
 12660| {
 12661|     alloc_list* al = &alloc_list_of (bn);
 12662|     uint8_t* next_item = free_list_slot(item);
 12663| #ifdef DOUBLY_LINKED_FL
 12664|     BOOL repair_list = !discard_if_no_fit_p ();
 12665| #endif //DOUBLY_LINKED_FL
 12666|     if (prev_item)
 12667|     {
 12668|         if (use_undo_p && (free_list_undo (prev_item) == UNDO_EMPTY))
 12669|         {
 12670|             assert (item == free_list_slot (prev_item));
 12671|             free_list_undo (prev_item) = item;
 12672|             alloc_list_damage_count_of (bn)++;
 12673|         }
 12674|         free_list_slot (prev_item) = next_item;
 12675|     }
 12676|     else
 12677|     {
 12678|         al->alloc_list_head() = next_item;
 12679|     }
 12680|     if (al->alloc_list_tail() == item)
 12681|     {
 12682|         al->alloc_list_tail() = prev_item;
 12683|     }
 12684| #ifdef DOUBLY_LINKED_FL
 12685|     if (repair_list)
 12686|     {
 12687|         if (!use_undo_p)
 12688|         {
 12689|             free_list_prev (item) = PREV_EMPTY;
 12690|         }
 12691|     }
 12692|     if (gen_number == max_generation)
 12693|     {
 12694|         dprintf (3, ("[g%2d, b%2d]UL: %p->%p->%p (h: %p, t: %p)",
 12695|             gen_number, bn, free_list_prev (item), item, free_list_slot (item),
 12696|             al->alloc_list_head(), al->alloc_list_tail()));
 12697|         dprintf (3, ("[g%2d, b%2d]UL: exit, h->N: %p, h->P: %p, t->N: %p, t->P: %p",
 12698|             gen_number, bn,
 12699|             (al->alloc_list_head() ? free_list_slot (al->alloc_list_head()) : 0),
 12700|             (al->alloc_list_head() ? free_list_prev (al->alloc_list_head()) : 0),
 12701|             (al->alloc_list_tail() ? free_list_slot (al->alloc_list_tail()) : 0),
 12702|             (al->alloc_list_tail() ? free_list_prev (al->alloc_list_tail()) : 0)));
 12703|     }
 12704| #endif //DOUBLY_LINKED_FL
 12705|     if (al->alloc_list_head() == 0)
 12706|     {
 12707|         assert (al->alloc_list_tail() == 0);
 12708|     }
 12709| }
 12710| #ifdef DOUBLY_LINKED_FL
 12711| void allocator::unlink_item_no_undo (unsigned int bn, uint8_t* item, size_t size)
 12712| {
 12713|     alloc_list* al = &alloc_list_of (bn);
 12714|     uint8_t* next_item = free_list_slot (item);
 12715|     uint8_t* prev_item = free_list_prev (item);
 12716| #ifdef FL_VERIFICATION
 12717|     {
 12718|         uint8_t* start = al->alloc_list_head();
 12719|         BOOL found_p = FALSE;
 12720|         while (start)
 12721|         {
 12722|             if (start == item)
 12723|             {
 12724|                 found_p = TRUE;
 12725|                 break;
 12726|             }
 12727|             start = free_list_slot (start);
 12728|         }
 12729|         if (!found_p)
 12730|         {
 12731|             dprintf (1, ("could not find %p in b%d!!!", item, a_l_number));
 12732|             FATAL_GC_ERROR();
 12733|         }
 12734|     }
 12735| #endif //FL_VERIFICATION
 12736|     if (prev_item)
 12737|     {
 12738|         free_list_slot (prev_item) = next_item;
 12739|     }
 12740|     else
 12741|     {
 12742|         al->alloc_list_head() = next_item;
 12743|     }
 12744|     if (next_item)
 12745|     {
 12746|         free_list_prev (next_item) = prev_item;
 12747|     }
 12748|     if (al->alloc_list_tail() == item)
 12749|     {
 12750|         al->alloc_list_tail() = prev_item;
 12751|     }
 12752|     free_list_prev (item) = PREV_EMPTY;
 12753|     if (gen_number == max_generation)
 12754|     {
 12755|         dprintf (3333, ("[g%2d, b%2d]ULN: %p->%p->%p (h: %p, t: %p)",
 12756|             gen_number, bn, free_list_prev (item), item, free_list_slot (item),
 12757|             al->alloc_list_head(), al->alloc_list_tail()));
 12758|         dprintf (3333, ("[g%2d, b%2d]ULN: exit: h->N: %p, h->P: %p, t->N: %p, t->P: %p",
 12759|             gen_number, bn,
 12760|             (al->alloc_list_head() ? free_list_slot (al->alloc_list_head()) : 0),
 12761|             (al->alloc_list_head() ? free_list_prev (al->alloc_list_head()) : 0),
 12762|             (al->alloc_list_tail() ? free_list_slot (al->alloc_list_tail()) : 0),
 12763|             (al->alloc_list_tail() ? free_list_prev (al->alloc_list_tail()) : 0)));
 12764|     }
 12765| }
 12766| void allocator::unlink_item_no_undo (uint8_t* item, size_t size)
 12767| {
 12768|     unsigned int bn = first_suitable_bucket (size);
 12769|     unlink_item_no_undo (bn, item, size);
 12770| }
 12771| void allocator::unlink_item_no_undo_added (unsigned int bn, uint8_t* item, uint8_t* previous_item)
 12772| {
 12773|     alloc_list* al = &alloc_list_of (bn);
 12774|     uint8_t* next_item = free_list_slot (item);
 12775|     uint8_t* prev_item = free_list_prev (item);
 12776|     assert (prev_item == previous_item);
 12777|     if (prev_item)
 12778|     {
 12779|         free_list_slot (prev_item) = next_item;
 12780|     }
 12781|     else
 12782|     {
 12783|         al->added_alloc_list_head() = next_item;
 12784|     }
 12785|     if (next_item)
 12786|     {
 12787|         free_list_prev (next_item) = prev_item;
 12788|     }
 12789|     if (al->added_alloc_list_tail() == item)
 12790|     {
 12791|         al->added_alloc_list_tail() = prev_item;
 12792|     }
 12793|     free_list_prev (item) = PREV_EMPTY;
 12794|     if (gen_number == max_generation)
 12795|     {
 12796|         dprintf (3333, ("[g%2d, b%2d]ULNA: %p->%p->%p (h: %p, t: %p)",
 12797|             gen_number, bn, free_list_prev (item), item, free_list_slot (item),
 12798|             al->added_alloc_list_head(), al->added_alloc_list_tail()));
 12799|         dprintf (3333, ("[g%2d, b%2d]ULNA: exit: h->N: %p, h->P: %p, t->N: %p, t->P: %p",
 12800|             gen_number, bn,
 12801|             (al->added_alloc_list_head() ? free_list_slot (al->added_alloc_list_head()) : 0),
 12802|             (al->added_alloc_list_head() ? free_list_prev (al->added_alloc_list_head()) : 0),
 12803|             (al->added_alloc_list_tail() ? free_list_slot (al->added_alloc_list_tail()) : 0),
 12804|             (al->added_alloc_list_tail() ? free_list_prev (al->added_alloc_list_tail()) : 0)));
 12805|     }
 12806| }
 12807| int allocator::thread_item_front_added (uint8_t* item, size_t size)
 12808| {
 12809|     unsigned int a_l_number = first_suitable_bucket (size);
 12810|     alloc_list* al = &alloc_list_of (a_l_number);
 12811|     free_list_slot (item) = al->added_alloc_list_head();
 12812|     free_list_prev (item) = 0;
 12813|     free_list_undo (item) = UNDO_EMPTY;
 12814|     if (al->added_alloc_list_head() != 0)
 12815|     {
 12816|         free_list_prev (al->added_alloc_list_head()) = item;
 12817|     }
 12818|     al->added_alloc_list_head() = item;
 12819|     if (al->added_alloc_list_tail() == 0)
 12820|     {
 12821|         al->added_alloc_list_tail() = item;
 12822|     }
 12823|     if (gen_number == max_generation)
 12824|     {
 12825|         dprintf (3333, ("[g%2d, b%2d]TFFA: exit: %p->%p->%p (h: %p, t: %p)",
 12826|             gen_number, a_l_number,
 12827|             free_list_prev (item), item, free_list_slot (item),
 12828|             al->added_alloc_list_head(), al->added_alloc_list_tail()));
 12829|         dprintf (3333, ("[g%2d, b%2d]TFFA: h->N: %p, h->P: %p, t->N: %p, t->P: %p",
 12830|             gen_number, a_l_number,
 12831|             (al->added_alloc_list_head() ? free_list_slot (al->added_alloc_list_head()) : 0),
 12832|             (al->added_alloc_list_head() ? free_list_prev (al->added_alloc_list_head()) : 0),
 12833|             (al->added_alloc_list_tail() ? free_list_slot (al->added_alloc_list_tail()) : 0),
 12834|             (al->added_alloc_list_tail() ? free_list_prev (al->added_alloc_list_tail()) : 0)));
 12835|     }
 12836|     return a_l_number;
 12837| }
 12838| #if defined(MULTIPLE_HEAPS) && defined(USE_REGIONS)
 12839| void allocator::count_items (gc_heap* this_hp, size_t* fl_items_count, size_t* fl_items_for_oh_count)
 12840| {
 12841|     uint64_t start_us = GetHighPrecisionTimeStamp();
 12842|     uint64_t end_us = 0;
 12843|     int align_const = get_alignment_constant (gen_number == max_generation);
 12844|     size_t num_fl_items = 0;
 12845|     size_t num_fl_items_for_oh = 0;
 12846|     for (unsigned int i = 0; i < num_buckets; i++)
 12847|     {
 12848|         uint8_t* free_item = alloc_list_head_of (i);
 12849|         while (free_item)
 12850|         {
 12851|             assert (((CObjectHeader*)free_item)->IsFree());
 12852|             num_fl_items++;
 12853|             heap_segment* region = gc_heap::region_of (free_item);
 12854|             dprintf (3, ("b#%2d FL %Ix region %Ix heap %d -> %d",
 12855|                 i, free_item, (size_t)region, this_hp->heap_number, region->heap->heap_number));
 12856|             if (region->heap != this_hp)
 12857|             {
 12858|                 num_fl_items_for_oh++;
 12859|             }
 12860|             free_item = free_list_slot (free_item);
 12861|         }
 12862|     }
 12863|     end_us = GetHighPrecisionTimeStamp();
 12864|     dprintf (3, ("total - %Id items out of %Id items are from a different heap in %I64d us",
 12865|         num_fl_items_for_oh, num_fl_items, (end_us - start_us)));
 12866|     *fl_items_count = num_fl_items;
 12867|     *fl_items_for_oh_count = num_fl_items_for_oh;
 12868| }
 12869| void min_fl_list_info::thread_item (uint8_t* item)
 12870| {
 12871|     free_list_slot (item) = 0;
 12872|     free_list_undo (item) = UNDO_EMPTY;
 12873|     assert (item != head);
 12874|     free_list_prev (item) = tail;
 12875|     if (head == 0)
 12876|     {
 12877|         head = item;
 12878|     }
 12879|     else
 12880|     {
 12881|         assert ((free_list_slot(head) != 0) || (tail == head));
 12882|         assert (item != tail);
 12883|         assert (free_list_slot(tail) == 0);
 12884|         free_list_slot (tail) = item;
 12885|     }
 12886|     tail = item;
 12887| }
 12888| void min_fl_list_info::thread_item_no_prev (uint8_t* item)
 12889| {
 12890|     free_list_slot (item) = 0;
 12891|     free_list_undo (item) = UNDO_EMPTY;
 12892|     assert (item != head);
 12893|     if (head == 0)
 12894|     {
 12895|         head = item;
 12896|     }
 12897|     else
 12898|     {
 12899|         assert ((free_list_slot(head) != 0) || (tail == head));
 12900|         assert (item != tail);
 12901|         assert (free_list_slot(tail) == 0);
 12902|         free_list_slot (tail) = item;
 12903|     }
 12904|     tail = item;
 12905| }
 12906| void allocator::rethread_items (size_t* num_total_fl_items, size_t* num_total_fl_items_rethreaded, gc_heap* current_heap,
 12907|                                 min_fl_list_info* min_fl_list, size_t *free_list_space_per_heap, int num_heaps)
 12908| {
 12909|     uint64_t start_us = GetHighPrecisionTimeStamp();
 12910|     uint64_t end_us = 0;
 12911|     int align_const = get_alignment_constant (gen_number == max_generation);
 12912|     size_t num_fl_items = 0;
 12913|     size_t num_fl_items_rethreaded = 0;
 12914|     assert (num_buckets <= MAX_BUCKET_COUNT);
 12915|     for (unsigned int i = 0; i < num_buckets; i++)
 12916|     {
 12917|         min_fl_list_info* current_bucket_min_fl_list = min_fl_list + (i * num_heaps);
 12918|         uint8_t* free_item = alloc_list_head_of (i);
 12919|         uint8_t* prev_item = nullptr;
 12920|         while (free_item)
 12921|         {
 12922|             assert (((CObjectHeader*)free_item)->IsFree());
 12923|             num_fl_items++;
 12924|             heap_segment* region = gc_heap::region_of (free_item);
 12925|             dprintf (3, ("b#%2d FL %Ix region %Ix heap %d -> %d",
 12926|                 i, free_item, (size_t)region, current_heap->heap_number, region->heap->heap_number));
 12927|             if (region->heap != current_heap)
 12928|             {
 12929|                 num_fl_items_rethreaded++;
 12930|                 size_t size_o = Align(size (free_item), align_const);
 12931|                 uint8_t* next_item = free_list_slot (free_item);
 12932|                 int hn = region->heap->heap_number;
 12933|                 if (is_doubly_linked_p())
 12934|                 {
 12935|                     unlink_item_no_undo (free_item, size_o);
 12936|                     current_bucket_min_fl_list[hn].thread_item (free_item);
 12937|                 }
 12938|                 else
 12939|                 {
 12940|                     unlink_item (i, free_item, prev_item, FALSE);
 12941|                     current_bucket_min_fl_list[hn].thread_item_no_prev (free_item);
 12942|                 }
 12943|                 free_list_space_per_heap[hn] += size_o;
 12944|                 free_item = next_item;
 12945|             }
 12946|             else
 12947|             {
 12948|                 prev_item = free_item;
 12949|                 free_item = free_list_slot (free_item);
 12950|             }
 12951|         }
 12952|     }
 12953|     end_us = GetHighPrecisionTimeStamp();
 12954|     dprintf (8888, ("h%d total %Id items rethreaded out of %Id items in %I64d us (%I64dms)",
 12955|         current_heap->heap_number, num_fl_items_rethreaded, num_fl_items, (end_us - start_us), ((end_us - start_us) / 1000)));
 12956|     (*num_total_fl_items) += num_fl_items;
 12957|     (*num_total_fl_items_rethreaded) += num_fl_items_rethreaded;
 12958| }
 12959| void allocator::merge_items (gc_heap* current_heap, int to_num_heaps, int from_num_heaps)
 12960| {
 12961|     int this_hn = current_heap->heap_number;
 12962|     for (unsigned int i = 0; i < num_buckets; i++)
 12963|     {
 12964|         alloc_list* al = &alloc_list_of (i);
 12965|         uint8_t*& head = al->alloc_list_head ();
 12966|         uint8_t*& tail = al->alloc_list_tail ();
 12967|         for (int other_hn = 0; other_hn < from_num_heaps; other_hn++)
 12968|         {
 12969|             min_fl_list_info* current_bucket_min_fl_list = gc_heap::g_heaps[other_hn]->min_fl_list + (i * to_num_heaps);
 12970|             min_fl_list_info* current_heap_bucket_min_fl_list = &current_bucket_min_fl_list[this_hn];
 12971|             uint8_t* head_other_heap = current_heap_bucket_min_fl_list->head;
 12972|             if (head_other_heap)
 12973|             {
 12974|                 if (is_doubly_linked_p())
 12975|                 {
 12976|                     free_list_prev (head_other_heap) = tail;
 12977|                 }
 12978|                 uint8_t* saved_head = head;
 12979|                 uint8_t* saved_tail = tail;
 12980|                 if (head)
 12981|                 {
 12982|                     free_list_slot (tail) = head_other_heap;
 12983|                 }
 12984|                 else
 12985|                 {
 12986|                     head = head_other_heap;
 12987|                 }
 12988|                 tail = current_heap_bucket_min_fl_list->tail;
 12989|             }
 12990|         }
 12991|     }
 12992| }
 12993| #endif //MULTIPLE_HEAPS && USE_REGIONS
 12994| #endif //DOUBLY_LINKED_FL
 12995| void allocator::clear()
 12996| {
 12997|     for (unsigned int i = 0; i < num_buckets; i++)
 12998|     {
 12999|         alloc_list_head_of (i) = 0;
 13000|         alloc_list_tail_of (i) = 0;
 13001|     }
 13002| }
 13003| void allocator::thread_item (uint8_t* item, size_t size)
 13004| {
 13005|     unsigned int a_l_number = first_suitable_bucket (size);
 13006|     alloc_list* al = &alloc_list_of (a_l_number);
 13007|     uint8_t*& head = al->alloc_list_head();
 13008|     uint8_t*& tail = al->alloc_list_tail();
 13009|     if (al->alloc_list_head() == 0)
 13010|     {
 13011|         assert (al->alloc_list_tail() == 0);
 13012|     }
 13013|     free_list_slot (item) = 0;
 13014|     free_list_undo (item) = UNDO_EMPTY;
 13015|     assert (item != head);
 13016| #ifdef DOUBLY_LINKED_FL
 13017|     if (gen_number == max_generation)
 13018|     {
 13019|         free_list_prev (item) = tail;
 13020|     }
 13021| #endif //DOUBLY_LINKED_FL
 13022|     if (head == 0)
 13023|     {
 13024|         head = item;
 13025|     }
 13026|     else
 13027|     {
 13028|         assert ((free_list_slot(head) != 0) || (tail == head));
 13029|         assert (item != tail);
 13030|         assert (free_list_slot(tail) == 0);
 13031|         free_list_slot (tail) = item;
 13032|     }
 13033|     tail = item;
 13034| #ifdef DOUBLY_LINKED_FL
 13035|     if (gen_number == max_generation)
 13036|     {
 13037|         dprintf (3333, ("[g%2d, b%2d]TFE: %p->%p->%p (h: %p, t: %p)",
 13038|             gen_number, a_l_number,
 13039|             free_list_prev (item), item, free_list_slot (item),
 13040|             al->alloc_list_head(), al->alloc_list_tail()));
 13041|         dprintf (3333, ("[g%2d, b%2d]TFE: exit: h->N: %p, h->P: %p, t->N: %p, t->P: %p",
 13042|             gen_number, a_l_number,
 13043|             (al->alloc_list_head() ? free_list_slot (al->alloc_list_head()) : 0),
 13044|             (al->alloc_list_head() ? free_list_prev (al->alloc_list_head()) : 0),
 13045|             (al->alloc_list_tail() ? free_list_slot (al->alloc_list_tail()) : 0),
 13046|             (al->alloc_list_tail() ? free_list_prev (al->alloc_list_tail()) : 0)));
 13047|     }
 13048| #endif //DOUBLY_LINKED_FL
 13049| }
 13050| void allocator::thread_item_front (uint8_t* item, size_t size)
 13051| {
 13052|     unsigned int a_l_number = first_suitable_bucket (size);
 13053|     alloc_list* al = &alloc_list_of (a_l_number);
 13054|     if (al->alloc_list_head() == 0)
 13055|     {
 13056|         assert (al->alloc_list_tail() == 0);
 13057|     }
 13058|     free_list_slot (item) = al->alloc_list_head();
 13059|     free_list_undo (item) = UNDO_EMPTY;
 13060|     if (al->alloc_list_tail() == 0)
 13061|     {
 13062|         assert (al->alloc_list_head() == 0);
 13063|         al->alloc_list_tail() = al->alloc_list_head();
 13064|     }
 13065| #ifdef DOUBLY_LINKED_FL
 13066|     if (gen_number == max_generation)
 13067|     {
 13068|         if (al->alloc_list_head() != 0)
 13069|         {
 13070|             free_list_prev (al->alloc_list_head()) = item;
 13071|         }
 13072|     }
 13073| #endif //DOUBLY_LINKED_FL
 13074|     al->alloc_list_head() = item;
 13075|     if (al->alloc_list_tail() == 0)
 13076|     {
 13077|         al->alloc_list_tail() = item;
 13078|     }
 13079| #ifdef DOUBLY_LINKED_FL
 13080|     if (gen_number == max_generation)
 13081|     {
 13082|         free_list_prev (item) = 0;
 13083|         dprintf (3333, ("[g%2d, b%2d]TFF: exit: %p->%p->%p (h: %p, t: %p)",
 13084|             gen_number, a_l_number,
 13085|             free_list_prev (item), item, free_list_slot (item),
 13086|             al->alloc_list_head(), al->alloc_list_tail()));
 13087|         dprintf (3333, ("[g%2d, b%2d]TFF: h->N: %p, h->P: %p, t->N: %p, t->P: %p",
 13088|             gen_number, a_l_number,
 13089|             (al->alloc_list_head() ? free_list_slot (al->alloc_list_head()) : 0),
 13090|             (al->alloc_list_head() ? free_list_prev (al->alloc_list_head()) : 0),
 13091|             (al->alloc_list_tail() ? free_list_slot (al->alloc_list_tail()) : 0),
 13092|             (al->alloc_list_tail() ? free_list_prev (al->alloc_list_tail()) : 0)));
 13093|     }
 13094| #endif //DOUBLY_LINKED_FL
 13095| }
 13096| void allocator::copy_to_alloc_list (alloc_list* toalist)
 13097| {
 13098|     for (unsigned int i = 0; i < num_buckets; i++)
 13099|     {
 13100|         toalist [i] = alloc_list_of (i);
 13101| #ifdef FL_VERIFICATION
 13102|         size_t damage_count = alloc_list_damage_count_of (i);
 13103|         assert (damage_count == 0);
 13104|         uint8_t* free_item = alloc_list_head_of (i);
 13105|         size_t count = 0;
 13106|         while (free_item)
 13107|         {
 13108|             count++;
 13109|             free_item = free_list_slot (free_item);
 13110|         }
 13111|         toalist[i].item_count = count;
 13112| #endif //FL_VERIFICATION
 13113|     }
 13114| }
 13115| void allocator::copy_from_alloc_list (alloc_list* fromalist)
 13116| {
 13117|     BOOL repair_list = !discard_if_no_fit_p ();
 13118| #ifdef DOUBLY_LINKED_FL
 13119|     BOOL bgc_repair_p = FALSE;
 13120|     if (gen_number == max_generation)
 13121|     {
 13122|         bgc_repair_p = TRUE;
 13123|         if (alloc_list_damage_count_of (0) != 0)
 13124|         {
 13125|             GCToOSInterface::DebugBreak();
 13126|         }
 13127|         uint8_t* b0_head = alloc_list_head_of (0);
 13128|         if (b0_head)
 13129|         {
 13130|             free_list_prev (b0_head) = 0;
 13131|         }
 13132|         added_alloc_list_head_of (0) = 0;
 13133|         added_alloc_list_tail_of (0) = 0;
 13134|     }
 13135|     unsigned int start_index = (bgc_repair_p ? 1 : 0);
 13136| #else
 13137|     unsigned int start_index = 0;
 13138| #endif //DOUBLY_LINKED_FL
 13139|     for (unsigned int i = start_index; i < num_buckets; i++)
 13140|     {
 13141|         size_t count = alloc_list_damage_count_of (i);
 13142|         alloc_list_of (i) = fromalist [i];
 13143|         assert (alloc_list_damage_count_of (i) == 0);
 13144|         if (repair_list)
 13145|         {
 13146|             uint8_t* free_item = alloc_list_head_of (i);
 13147|             while (free_item && count)
 13148|             {
 13149|                 assert (((CObjectHeader*)free_item)->IsFree());
 13150|                 if ((free_list_undo (free_item) != UNDO_EMPTY))
 13151|                 {
 13152|                     count--;
 13153|                     free_list_slot (free_item) = free_list_undo (free_item);
 13154|                     free_list_undo (free_item) = UNDO_EMPTY;
 13155|                 }
 13156|                 free_item = free_list_slot (free_item);
 13157|             }
 13158| #ifdef DOUBLY_LINKED_FL
 13159|             if (bgc_repair_p)
 13160|             {
 13161|                 added_alloc_list_head_of (i) = 0;
 13162|                 added_alloc_list_tail_of (i) = 0;
 13163|             }
 13164| #endif //DOUBLY_LINKED_FL
 13165| #ifdef FL_VERIFICATION
 13166|             free_item = alloc_list_head_of (i);
 13167|             size_t item_count = 0;
 13168|             while (free_item)
 13169|             {
 13170|                 item_count++;
 13171|                 free_item = free_list_slot (free_item);
 13172|             }
 13173|             assert (item_count == alloc_list_of (i).item_count);
 13174| #endif //FL_VERIFICATION
 13175|         }
 13176| #ifdef DEBUG
 13177|         uint8_t* tail_item = alloc_list_tail_of (i);
 13178|         assert ((tail_item == 0) || (free_list_slot (tail_item) == 0));
 13179| #endif
 13180|     }
 13181| }
 13182| void allocator::commit_alloc_list_changes()
 13183| {
 13184|     BOOL repair_list = !discard_if_no_fit_p ();
 13185| #ifdef DOUBLY_LINKED_FL
 13186|     BOOL bgc_repair_p = FALSE;
 13187|     if (gen_number == max_generation)
 13188|     {
 13189|         bgc_repair_p = TRUE;
 13190|     }
 13191| #endif //DOUBLY_LINKED_FL
 13192|     if (repair_list)
 13193|     {
 13194|         for (unsigned int i = 0; i < num_buckets; i++)
 13195|         {
 13196|             uint8_t* free_item = alloc_list_head_of (i);
 13197| #ifdef DOUBLY_LINKED_FL
 13198|             if (bgc_repair_p)
 13199|             {
 13200|                 dprintf (3, ("C[b%2d] ENTRY: h: %p t: %p", i,
 13201|                     alloc_list_head_of (i), alloc_list_tail_of (i)));
 13202|             }
 13203|             if (free_item && bgc_repair_p)
 13204|             {
 13205|                 if (free_list_prev (free_item) != 0)
 13206|                     free_list_prev (free_item) = 0;
 13207|             }
 13208| #endif //DOUBLY_LINKED_FL
 13209|             size_t count = alloc_list_damage_count_of (i);
 13210|             while (free_item && count)
 13211|             {
 13212|                 assert (((CObjectHeader*)free_item)->IsFree());
 13213|                 if (free_list_undo (free_item) != UNDO_EMPTY)
 13214|                 {
 13215|                     free_list_undo (free_item) = UNDO_EMPTY;
 13216| #ifdef DOUBLY_LINKED_FL
 13217|                     if (bgc_repair_p)
 13218|                     {
 13219|                         uint8_t* next_item = free_list_slot (free_item);
 13220|                         if (next_item && (free_list_prev (next_item) != free_item))
 13221|                             free_list_prev (next_item) = free_item;
 13222|                     }
 13223| #endif //DOUBLY_LINKED_FL
 13224|                     count--;
 13225|                 }
 13226|                 free_item = free_list_slot (free_item);
 13227|             }
 13228|             alloc_list_damage_count_of (i) = 0;
 13229| #ifdef DOUBLY_LINKED_FL
 13230|             if (bgc_repair_p)
 13231|             {
 13232|                 uint8_t* head = alloc_list_head_of (i);
 13233|                 uint8_t* tail_added = added_alloc_list_tail_of (i);
 13234|                 if (tail_added)
 13235|                 {
 13236|                     assert (free_list_slot (tail_added) == 0);
 13237|                     if (head)
 13238|                     {
 13239|                         free_list_slot (tail_added) = head;
 13240|                         free_list_prev (head) = tail_added;
 13241|                     }
 13242|                 }
 13243|                 uint8_t* head_added = added_alloc_list_head_of (i);
 13244|                 if (head_added)
 13245|                 {
 13246|                     alloc_list_head_of (i) = head_added;
 13247|                     uint8_t* final_head = alloc_list_head_of (i);
 13248|                     if (alloc_list_tail_of (i) == 0)
 13249|                     {
 13250|                         alloc_list_tail_of (i) = tail_added;
 13251|                     }
 13252|                 }
 13253|                 added_alloc_list_head_of (i) = 0;
 13254|                 added_alloc_list_tail_of (i) = 0;
 13255|             }
 13256| #endif //DOUBLY_LINKED_FL
 13257|         }
 13258|     }
 13259| }
 13260| #ifdef USE_REGIONS
 13261| void allocator::thread_sip_fl (heap_segment* region)
 13262| {
 13263|     uint8_t* region_fl_head = region->free_list_head;
 13264|     uint8_t* region_fl_tail = region->free_list_tail;
 13265|     if (!region_fl_head)
 13266|     {
 13267|         assert (!region_fl_tail);
 13268|         assert (region->free_list_size == 0);
 13269|         return;
 13270|     }
 13271|     if (num_buckets == 1)
 13272|     {
 13273|         dprintf (REGIONS_LOG, ("threading gen%d region %p onto gen%d FL",
 13274|             heap_segment_gen_num (region), heap_segment_mem (region), gen_number));
 13275|         alloc_list* al = &alloc_list_of (0);
 13276|         uint8_t*& head = al->alloc_list_head();
 13277|         uint8_t*& tail = al->alloc_list_tail();
 13278|         if (tail == 0)
 13279|         {
 13280|             assert (head == 0);
 13281|             head = region_fl_head;
 13282|         }
 13283|         else
 13284|         {
 13285|             free_list_slot (tail) = region_fl_head;
 13286|         }
 13287|         tail = region_fl_tail;
 13288|     }
 13289|     else
 13290|     {
 13291|         dprintf (REGIONS_LOG, ("threading gen%d region %p onto gen%d bucketed FL",
 13292|             heap_segment_gen_num (region), heap_segment_mem (region), gen_number));
 13293|         uint8_t* region_fl_item = region_fl_head;
 13294|         size_t total_free_size = 0;
 13295|         while (region_fl_item)
 13296|         {
 13297|             uint8_t* next_fl_item = free_list_slot (region_fl_item);
 13298|             size_t size_item = size (region_fl_item);
 13299|             thread_item (region_fl_item, size_item);
 13300|             total_free_size += size_item;
 13301|             region_fl_item = next_fl_item;
 13302|         }
 13303|         assert (total_free_size == region->free_list_size);
 13304|     }
 13305| }
 13306| #endif //USE_REGIONS
 13307| #ifdef FEATURE_EVENT_TRACE
 13308| uint16_t allocator::count_largest_items (etw_bucket_info* bucket_info,
 13309|                                          size_t max_size,
 13310|                                          size_t max_item_count,
 13311|                                          size_t* recorded_fl_info_size)
 13312| {
 13313|     assert (gen_number == max_generation);
 13314|     size_t size_counted_total = 0;
 13315|     size_t items_counted_total = 0;
 13316|     uint16_t bucket_info_index = 0;
 13317|     for (int i = (num_buckets - 1); i >= 0; i--)
 13318|     {
 13319|         uint32_t items_counted = 0;
 13320|         size_t size_counted = 0;
 13321|         uint8_t* free_item = alloc_list_head_of ((unsigned int)i);
 13322|         while (free_item)
 13323|         {
 13324|             assert (((CObjectHeader*)free_item)->IsFree());
 13325|             size_t free_item_size = Align (size (free_item));
 13326|             size_counted_total += free_item_size;
 13327|             size_counted += free_item_size;
 13328|             items_counted_total++;
 13329|             items_counted++;
 13330|             if ((size_counted_total > max_size) || (items_counted > max_item_count))
 13331|             {
 13332|                 bucket_info[bucket_info_index++].set ((uint16_t)i, items_counted, size_counted);
 13333|                 *recorded_fl_info_size = size_counted_total;
 13334|                 return bucket_info_index;
 13335|             }
 13336|             free_item = free_list_slot (free_item);
 13337|         }
 13338|         if (items_counted)
 13339|         {
 13340|             bucket_info[bucket_info_index++].set ((uint16_t)i, items_counted, size_counted);
 13341|         }
 13342|     }
 13343|     *recorded_fl_info_size = size_counted_total;
 13344|     return bucket_info_index;
 13345| }
 13346| #endif //FEATURE_EVENT_TRACE
 13347| void gc_heap::adjust_limit_clr (uint8_t* start, size_t limit_size, size_t size,
 13348|                                 alloc_context* acontext, uint32_t flags,
 13349|                                 heap_segment* seg, int align_const, int gen_number)
 13350| {
 13351|     bool uoh_p = (gen_number > 0);
 13352|     GCSpinLock* msl = uoh_p ? &more_space_lock_uoh : &more_space_lock_soh;
 13353|     uint64_t& total_alloc_bytes = uoh_p ? total_alloc_bytes_uoh : total_alloc_bytes_soh;
 13354|     size_t aligned_min_obj_size = Align(min_obj_size, align_const);
 13355|     if (seg)
 13356|     {
 13357|         assert (heap_segment_used (seg) <= heap_segment_committed (seg));
 13358|     }
 13359| #ifdef MULTIPLE_HEAPS
 13360|     if (gen_number == 0)
 13361|     {
 13362|         if (!gen0_allocated_after_gc_p)
 13363|         {
 13364|             gen0_allocated_after_gc_p = true;
 13365|         }
 13366|     }
 13367| #endif //MULTIPLE_HEAPS
 13368|     dprintf (3, ("Expanding segment allocation [%zx, %zx[", (size_t)start,
 13369|                (size_t)start + limit_size - aligned_min_obj_size));
 13370|     if ((acontext->alloc_limit != start) &&
 13371|         (acontext->alloc_limit + aligned_min_obj_size)!= start)
 13372|     {
 13373|         uint8_t*  hole = acontext->alloc_ptr;
 13374|         if (hole != 0)
 13375|         {
 13376|             size_t  ac_size = (acontext->alloc_limit - acontext->alloc_ptr);
 13377|             dprintf (3, ("filling up hole [%zx, %zx[", (size_t)hole, (size_t)hole + ac_size + aligned_min_obj_size));
 13378|             acontext->alloc_bytes -= ac_size;
 13379|             total_alloc_bytes -= ac_size;
 13380|             size_t free_obj_size = ac_size + aligned_min_obj_size;
 13381|             make_unused_array (hole, free_obj_size);
 13382|             generation_free_obj_space (generation_of (gen_number)) += free_obj_size;
 13383|         }
 13384|         acontext->alloc_ptr = start;
 13385|     }
 13386|     else
 13387|     {
 13388|         if (gen_number == 0)
 13389|         {
 13390| #ifdef USE_REGIONS
 13391|             if (acontext->alloc_ptr == 0)
 13392|             {
 13393|                 acontext->alloc_ptr = start;
 13394|             }
 13395|             else
 13396| #endif //USE_REGIONS
 13397|             {
 13398|                 size_t pad_size = aligned_min_obj_size;
 13399|                 dprintf (3, ("contiguous ac: making min obj gap %p->%p(%zd)",
 13400|                     acontext->alloc_ptr, (acontext->alloc_ptr + pad_size), pad_size));
 13401|                 make_unused_array (acontext->alloc_ptr, pad_size);
 13402|                 acontext->alloc_ptr += pad_size;
 13403|             }
 13404|         }
 13405|     }
 13406|     acontext->alloc_limit = (start + limit_size - aligned_min_obj_size);
 13407|     size_t added_bytes = limit_size - ((gen_number <= max_generation) ? aligned_min_obj_size : 0);
 13408|     acontext->alloc_bytes += added_bytes;
 13409|     total_alloc_bytes     += added_bytes;
 13410|     size_t etw_allocation_amount = 0;
 13411|     bool fire_event_p = update_alloc_info (gen_number, added_bytes, &etw_allocation_amount);
 13412|     uint8_t* saved_used = 0;
 13413|     if (seg)
 13414|     {
 13415|         saved_used = heap_segment_used (seg);
 13416|     }
 13417|     if (seg == ephemeral_heap_segment)
 13418|     {
 13419|         if (heap_segment_used (seg) < (alloc_allocated - plug_skew))
 13420|         {
 13421|             heap_segment_used (seg) = alloc_allocated - plug_skew;
 13422|             assert (heap_segment_mem (seg) <= heap_segment_used (seg));
 13423|             assert (heap_segment_used (seg) <= heap_segment_reserved (seg));
 13424|         }
 13425|     }
 13426| #ifdef BACKGROUND_GC
 13427|     else if (seg)
 13428|     {
 13429|         uint8_t* old_allocated = heap_segment_allocated (seg) - plug_skew - limit_size;
 13430| #ifdef FEATURE_LOH_COMPACTION
 13431|         if (gen_number == loh_generation)
 13432|         {
 13433|             old_allocated -= Align (loh_padding_obj_size, align_const);
 13434|         }
 13435| #endif //FEATURE_LOH_COMPACTION
 13436|         assert (heap_segment_used (seg) >= old_allocated);
 13437|     }
 13438| #endif //BACKGROUND_GC
 13439|     uint8_t* clear_start = start - plug_skew;
 13440|     uint8_t* clear_limit = start + limit_size - plug_skew;
 13441|     if (flags & GC_ALLOC_ZEROING_OPTIONAL)
 13442|     {
 13443|         uint8_t* obj_start = acontext->alloc_ptr;
 13444|         assert(start >= obj_start);
 13445|         uint8_t* obj_end = obj_start + size - plug_skew;
 13446|         assert(obj_end >= clear_start);
 13447|         if(obj_start == start)
 13448|         {
 13449|             *(PTR_PTR)clear_start = 0;
 13450|         }
 13451|         dprintf(3, ("zeroing optional: skipping object at %p->%p(%zd)",
 13452|             clear_start, obj_end, obj_end - clear_start));
 13453|         clear_start = obj_end;
 13454|     }
 13455|     heap_segment* gen0_segment = ephemeral_heap_segment;
 13456| #ifdef BACKGROUND_GC
 13457|     {
 13458|         if (uoh_p && gc_heap::background_running_p())
 13459|         {
 13460|             uint8_t* obj = acontext->alloc_ptr;
 13461|             uint8_t* result = obj;
 13462|             uint8_t* current_lowest_address = background_saved_lowest_address;
 13463|             uint8_t* current_highest_address = background_saved_highest_address;
 13464|             if (current_c_gc_state == c_gc_state_planning)
 13465|             {
 13466|                 dprintf (3, ("Concurrent allocation of a large object %zx",
 13467|                             (size_t)obj));
 13468|                 if ((result < current_highest_address) && (result >= current_lowest_address))
 13469|                 {
 13470| #ifdef DOUBLY_LINKED_FL
 13471|                     heap_segment* seg = seg_mapping_table_segment_of (result);
 13472|                     uint8_t* background_allocated = heap_segment_background_allocated(seg);
 13473|                     if (background_allocated != 0)
 13474| #endif //DOUBLY_LINKED_FL
 13475|                     {
 13476|                         dprintf(3, ("Setting mark bit at address %zx",
 13477|                             (size_t)(&mark_array[mark_word_of(result)])));
 13478|                         mark_array_set_marked(result);
 13479|                     }
 13480|                 }
 13481|             }
 13482|         }
 13483|     }
 13484| #endif //BACKGROUND_GC
 13485|     if ((seg == 0) || (clear_limit <= heap_segment_used (seg)))
 13486|     {
 13487|         add_saved_spinlock_info (uoh_p, me_release, mt_clr_mem, msl_entered);
 13488|         leave_spin_lock (msl);
 13489|         if (clear_start < clear_limit)
 13490|         {
 13491|             dprintf(3, ("clearing memory at %p for %zd bytes", clear_start, clear_limit - clear_start));
 13492|             memclr(clear_start, clear_limit - clear_start);
 13493|         }
 13494|     }
 13495|     else
 13496|     {
 13497|         uint8_t* used = heap_segment_used (seg);
 13498|         heap_segment_used (seg) = clear_limit;
 13499|         add_saved_spinlock_info (uoh_p, me_release, mt_clr_mem, msl_entered);
 13500|         leave_spin_lock (msl);
 13501|         if (clear_start < used)
 13502|         {
 13503|             if (used != saved_used)
 13504|             {
 13505|                 FATAL_GC_ERROR();
 13506|             }
 13507|             dprintf (2, ("clearing memory before used at %p for %zd bytes", clear_start, used - clear_start));
 13508|             memclr (clear_start, used - clear_start);
 13509|         }
 13510|     }
 13511| #ifdef FEATURE_EVENT_TRACE
 13512|     if (fire_event_p)
 13513|     {
 13514|         fire_etw_allocation_event (etw_allocation_amount, gen_number, acontext->alloc_ptr, size);
 13515|     }
 13516| #endif //FEATURE_EVENT_TRACE
 13517|     if (seg == gen0_segment ||
 13518|        ((seg == nullptr) && (gen_number == 0) && (limit_size >= CLR_SIZE / 2)))
 13519|     {
 13520|         if (gen0_must_clear_bricks > 0)
 13521|         {
 13522|             size_t b = brick_of (acontext->alloc_ptr);
 13523|             set_brick (b, acontext->alloc_ptr - brick_address (b));
 13524|             b++;
 13525|             dprintf (3, ("Allocation Clearing bricks [%zx, %zx[",
 13526|                          b, brick_of (align_on_brick (start + limit_size))));
 13527|             volatile short* x = &brick_table [b];
 13528|             short* end_x = &brick_table [brick_of (align_on_brick (start + limit_size))];
 13529|             for (;x < end_x;x++)
 13530|                 *x = -1;
 13531|         }
 13532|         else
 13533|         {
 13534|             gen0_bricks_cleared = FALSE;
 13535|         }
 13536|     }
 13537| }
 13538| size_t gc_heap::new_allocation_limit (size_t size, size_t physical_limit, int gen_number)
 13539| {
 13540|     dynamic_data* dd = dynamic_data_of (gen_number);
 13541|     ptrdiff_t new_alloc = dd_new_allocation (dd);
 13542|     assert (new_alloc == (ptrdiff_t)Align (new_alloc, get_alignment_constant (gen_number < uoh_start_generation)));
 13543|     ptrdiff_t logical_limit = max (new_alloc, (ptrdiff_t)size);
 13544|     size_t limit = min (logical_limit, (ptrdiff_t)physical_limit);
 13545|     assert (limit == Align (limit, get_alignment_constant (gen_number <= max_generation)));
 13546|     return limit;
 13547| }
 13548| size_t gc_heap::limit_from_size (size_t size, uint32_t flags, size_t physical_limit, int gen_number,
 13549|                                  int align_const)
 13550| {
 13551|     size_t padded_size = size + Align (min_obj_size, align_const);
 13552|     assert ((gen_number != 0) || (physical_limit >= padded_size));
 13553|     size_t min_size_to_allocate = ((gen_number == 0 && !(flags & GC_ALLOC_ZEROING_OPTIONAL)) ? allocation_quantum : 0);
 13554|     size_t desired_size_to_allocate  = max (padded_size, min_size_to_allocate);
 13555|     size_t new_physical_limit = min (physical_limit, desired_size_to_allocate);
 13556|     size_t new_limit = new_allocation_limit (padded_size,
 13557|                                              new_physical_limit,
 13558|                                              gen_number);
 13559|     assert (new_limit >= (size + Align (min_obj_size, align_const)));
 13560|     dprintf (3, ("h%d requested to allocate %zd bytes, actual size is %zd, phy limit: %zd",
 13561|         heap_number, size, new_limit, physical_limit));
 13562|     return new_limit;
 13563| }
 13564| void gc_heap::add_to_oom_history_per_heap()
 13565| {
 13566|     oom_history* current_hist = &oomhist_per_heap[oomhist_index_per_heap];
 13567|     memcpy (current_hist, &oom_info, sizeof (oom_info));
 13568|     oomhist_index_per_heap++;
 13569|     if (oomhist_index_per_heap == max_oom_history_count)
 13570|     {
 13571|         oomhist_index_per_heap = 0;
 13572|     }
 13573| }
 13574| void gc_heap::handle_oom (oom_reason reason, size_t alloc_size,
 13575|                           uint8_t* allocated, uint8_t* reserved)
 13576| {
 13577|     if (reason == oom_budget)
 13578|     {
 13579|         alloc_size = dd_min_size (dynamic_data_of (0)) / 2;
 13580|     }
 13581|     if ((reason == oom_budget) && ((!fgm_result.loh_p) && (fgm_result.fgm != fgm_no_failure)))
 13582|     {
 13583|         reason = oom_low_mem;
 13584|     }
 13585|     oom_info.reason = reason;
 13586|     oom_info.allocated = allocated;
 13587|     oom_info.reserved = reserved;
 13588|     oom_info.alloc_size = alloc_size;
 13589|     oom_info.gc_index = settings.gc_index;
 13590|     oom_info.fgm = fgm_result.fgm;
 13591|     oom_info.size = fgm_result.size;
 13592|     oom_info.available_pagefile_mb = fgm_result.available_pagefile_mb;
 13593|     oom_info.loh_p = fgm_result.loh_p;
 13594|     add_to_oom_history_per_heap();
 13595|     fgm_result.fgm = fgm_no_failure;
 13596|     if (GCConfig::GetBreakOnOOM())
 13597|     {
 13598|         GCToOSInterface::DebugBreak();
 13599|     }
 13600| }
 13601| #ifdef BACKGROUND_GC
 13602| BOOL gc_heap::background_allowed_p()
 13603| {
 13604|     return ( gc_can_use_concurrent && ((settings.pause_mode == pause_interactive) || (settings.pause_mode == pause_sustained_low_latency)) );
 13605| }
 13606| #endif //BACKGROUND_GC
 13607| void gc_heap::check_for_full_gc (int gen_num, size_t size)
 13608| {
 13609|     BOOL should_notify = FALSE;
 13610|     BOOL alloc_factor = TRUE;
 13611|     int n_initial = gen_num;
 13612|     BOOL local_blocking_collection = FALSE;
 13613|     BOOL local_elevation_requested = FALSE;
 13614|     int new_alloc_remain_percent = 0;
 13615|     if (full_gc_approach_event_set)
 13616|     {
 13617|         return;
 13618|     }
 13619|     if (gen_num < max_generation)
 13620|     {
 13621|         gen_num = max_generation;
 13622|     }
 13623|     dynamic_data* dd_full = dynamic_data_of (gen_num);
 13624|     ptrdiff_t new_alloc_remain = 0;
 13625|     uint32_t pct = (gen_num >= uoh_start_generation) ? fgn_loh_percent : fgn_maxgen_percent;
 13626|     for (int gen_index = 0; gen_index < total_generation_count; gen_index++)
 13627|     {
 13628|         dprintf (2, ("FGN: h#%d: gen%d: %zd(%zd)",
 13629|                      heap_number, gen_index,
 13630|                      dd_new_allocation (dynamic_data_of (gen_index)),
 13631|                      dd_desired_allocation (dynamic_data_of (gen_index))));
 13632|     }
 13633|     if (n_initial == 0)
 13634|     {
 13635|         dprintf (2, ("FGN: gen0 last recorded alloc: %zd", fgn_last_alloc));
 13636|         dynamic_data* dd_0 = dynamic_data_of (n_initial);
 13637|         if (((fgn_last_alloc - dd_new_allocation (dd_0)) < fgn_check_quantum) &&
 13638|             (dd_new_allocation (dd_0) >= 0))
 13639|         {
 13640|             return;
 13641|         }
 13642|         else
 13643|         {
 13644|             fgn_last_alloc = dd_new_allocation (dd_0);
 13645|             dprintf (2, ("FGN: gen0 last recorded alloc is now: %zd", fgn_last_alloc));
 13646|         }
 13647|         size = 0;
 13648|     }
 13649|     int n = 0;
 13650|     for (int i = 1; i <= max_generation; i++)
 13651|     {
 13652|             if (get_new_allocation (i) <= 0)
 13653|             {
 13654|                 n = i;
 13655|             }
 13656|             else
 13657|                 break;
 13658|     }
 13659|     dprintf (2, ("FGN: h#%d: gen%d budget exceeded", heap_number, n));
 13660|     if (gen_num == max_generation)
 13661|     {
 13662|         if (n < (max_generation - 1))
 13663|         {
 13664|             goto check_other_factors;
 13665|         }
 13666|     }
 13667|     new_alloc_remain = dd_new_allocation (dd_full) - size;
 13668|     new_alloc_remain_percent = (int)(((float)(new_alloc_remain) / (float)dd_desired_allocation (dd_full)) * 100);
 13669|     dprintf (2, ("FGN: alloc threshold for gen%d is %d%%, current threshold is %d%%",
 13670|                  gen_num, pct, new_alloc_remain_percent));
 13671|     if (new_alloc_remain_percent <= (int)pct)
 13672|     {
 13673| #ifdef BACKGROUND_GC
 13674|         if (background_allowed_p())
 13675|         {
 13676|             goto check_other_factors;
 13677|         }
 13678| #endif //BACKGROUND_GC
 13679|         should_notify = TRUE;
 13680|         goto done;
 13681|     }
 13682| check_other_factors:
 13683|     dprintf (2, ("FGC: checking other factors"));
 13684|     n = generation_to_condemn (n,
 13685|                                &local_blocking_collection,
 13686|                                &local_elevation_requested,
 13687|                                TRUE);
 13688|     if (local_elevation_requested && (n == max_generation))
 13689|     {
 13690|         if (settings.should_lock_elevation)
 13691|         {
 13692|             int local_elevation_locked_count = settings.elevation_locked_count + 1;
 13693|             if (local_elevation_locked_count != 6)
 13694|             {
 13695|                 dprintf (2, ("FGN: lock count is %d - Condemning max_generation-1",
 13696|                     local_elevation_locked_count));
 13697|                 n = max_generation - 1;
 13698|             }
 13699|         }
 13700|     }
 13701|     dprintf (2, ("FGN: we estimate gen%d will be collected", n));
 13702| #ifdef BACKGROUND_GC
 13703|     if ((n == max_generation) &&
 13704|         (gc_heap::background_running_p()))
 13705|     {
 13706|         n = max_generation - 1;
 13707|         dprintf (2, ("FGN: bgc - 1 instead of 2"));
 13708|     }
 13709|     if ((n == max_generation) && !local_blocking_collection)
 13710|     {
 13711|         if (!background_allowed_p())
 13712|         {
 13713|             local_blocking_collection = TRUE;
 13714|         }
 13715|     }
 13716| #endif //BACKGROUND_GC
 13717|     dprintf (2, ("FGN: we estimate gen%d will be collected: %s",
 13718|                        n,
 13719|                        (local_blocking_collection ? "blocking" : "background")));
 13720|     if ((n == max_generation) && local_blocking_collection)
 13721|     {
 13722|         alloc_factor = FALSE;
 13723|         should_notify = TRUE;
 13724|         goto done;
 13725|     }
 13726| done:
 13727|     if (should_notify)
 13728|     {
 13729|         dprintf (2, ("FGN: gen%d detecting full GC approaching(%s) (GC#%zd) (%d%% left in gen%d)",
 13730|                      n_initial,
 13731|                      (alloc_factor ? "alloc" : "other"),
 13732|                      dd_collection_count (dynamic_data_of (0)),
 13733|                      new_alloc_remain_percent,
 13734|                      gen_num));
 13735|         send_full_gc_notification (n_initial, alloc_factor);
 13736|     }
 13737| }
 13738| void gc_heap::send_full_gc_notification (int gen_num, BOOL due_to_alloc_p)
 13739| {
 13740|     if (!full_gc_approach_event_set)
 13741|     {
 13742|         assert (full_gc_approach_event.IsValid());
 13743|         FIRE_EVENT(GCFullNotify_V1, gen_num, due_to_alloc_p);
 13744|         full_gc_end_event.Reset();
 13745|         full_gc_approach_event.Set();
 13746|         full_gc_approach_event_set = true;
 13747|     }
 13748| }
 13749| wait_full_gc_status gc_heap::full_gc_wait (GCEvent *event, int time_out_ms)
 13750| {
 13751| #ifdef MULTIPLE_HEAPS
 13752|     gc_heap* hp = gc_heap::g_heaps[0];
 13753| #else
 13754|     gc_heap* hp = pGenGCHeap;
 13755| #endif //MULTIPLE_HEAPS
 13756|     if (hp->fgn_maxgen_percent == 0)
 13757|     {
 13758|         return wait_full_gc_na;
 13759|     }
 13760|     uint32_t wait_result = user_thread_wait(event, FALSE, time_out_ms);
 13761|     if ((wait_result == WAIT_OBJECT_0) || (wait_result == WAIT_TIMEOUT))
 13762|     {
 13763|         if (hp->fgn_maxgen_percent == 0)
 13764|         {
 13765|             return wait_full_gc_cancelled;
 13766|         }
 13767|         if (wait_result == WAIT_OBJECT_0)
 13768|         {
 13769| #ifdef BACKGROUND_GC
 13770|             if (fgn_last_gc_was_concurrent)
 13771|             {
 13772|                 fgn_last_gc_was_concurrent = FALSE;
 13773|                 return wait_full_gc_na;
 13774|             }
 13775|             else
 13776| #endif //BACKGROUND_GC
 13777|             {
 13778|                 return wait_full_gc_success;
 13779|             }
 13780|         }
 13781|         else
 13782|         {
 13783|             return wait_full_gc_timeout;
 13784|         }
 13785|     }
 13786|     else
 13787|     {
 13788|         return wait_full_gc_failed;
 13789|     }
 13790| }
 13791| size_t gc_heap::get_full_compact_gc_count()
 13792| {
 13793|     return full_gc_counts[gc_type_compacting];
 13794| }
 13795| inline
 13796| BOOL gc_heap::short_on_end_of_seg (heap_segment* seg)
 13797| {
 13798|     uint8_t* allocated = heap_segment_allocated (seg);
 13799| #ifdef USE_REGIONS
 13800|     assert (end_gen0_region_space != uninitialized_end_gen0_region_space);
 13801|     BOOL sufficient_p = sufficient_space_regions_for_allocation (end_gen0_region_space, end_space_after_gc());
 13802| #else
 13803|     BOOL sufficient_p = sufficient_space_end_seg (allocated,
 13804|                                                   heap_segment_committed (seg),
 13805|                                                   heap_segment_reserved (seg),
 13806|                                                   end_space_after_gc());
 13807| #endif //USE_REGIONS
 13808|     if (!sufficient_p)
 13809|     {
 13810|         if (sufficient_gen0_space_p)
 13811|         {
 13812|             dprintf (GTC_LOG, ("gen0 has enough free space"));
 13813|         }
 13814|         sufficient_p = sufficient_gen0_space_p;
 13815|     }
 13816|     return !sufficient_p;
 13817| }
 13818| inline
 13819| BOOL gc_heap::a_fit_free_list_p (int gen_number,
 13820|                                  size_t size,
 13821|                                  alloc_context* acontext,
 13822|                                  uint32_t flags,
 13823|                                  int align_const)
 13824| {
 13825|     BOOL can_fit = FALSE;
 13826|     generation* gen = generation_of (gen_number);
 13827|     allocator* gen_allocator = generation_allocator (gen);
 13828|     for (unsigned int a_l_idx = gen_allocator->first_suitable_bucket(size); a_l_idx < gen_allocator->number_of_buckets(); a_l_idx++)
 13829|     {
 13830|         uint8_t* free_list = gen_allocator->alloc_list_head_of (a_l_idx);
 13831|         uint8_t* prev_free_item = 0;
 13832|         while (free_list != 0)
 13833|         {
 13834|             dprintf (3, ("considering free list %zx", (size_t)free_list));
 13835|             size_t free_list_size = unused_array_size (free_list);
 13836|             if ((size + Align (min_obj_size, align_const)) <= free_list_size)
 13837|             {
 13838|                 dprintf (3, ("Found adequate unused area: [%zx, size: %zd",
 13839|                                 (size_t)free_list, free_list_size));
 13840|                 gen_allocator->unlink_item (a_l_idx, free_list, prev_free_item, FALSE);
 13841|                 size_t limit = limit_from_size (size, flags, free_list_size, gen_number, align_const);
 13842|                 dd_new_allocation (dynamic_data_of (gen_number)) -= limit;
 13843|                 uint8_t*  remain = (free_list + limit);
 13844|                 size_t remain_size = (free_list_size - limit);
 13845|                 if (remain_size >= Align(min_free_list, align_const))
 13846|                 {
 13847|                     make_unused_array (remain, remain_size);
 13848|                     gen_allocator->thread_item_front (remain, remain_size);
 13849|                     assert (remain_size >= Align (min_obj_size, align_const));
 13850|                 }
 13851|                 else
 13852|                 {
 13853|                     limit += remain_size;
 13854|                 }
 13855|                 generation_free_list_space (gen) -= limit;
 13856|                 assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
 13857|                 adjust_limit_clr (free_list, limit, size, acontext, flags, 0, align_const, gen_number);
 13858|                 can_fit = TRUE;
 13859|                 goto end;
 13860|             }
 13861|             else if (gen_allocator->discard_if_no_fit_p())
 13862|             {
 13863|                 assert (prev_free_item == 0);
 13864|                 dprintf (3, ("couldn't use this free area, discarding"));
 13865|                 generation_free_obj_space (gen) += free_list_size;
 13866|                 gen_allocator->unlink_item (a_l_idx, free_list, prev_free_item, FALSE);
 13867|                 generation_free_list_space (gen) -= free_list_size;
 13868|                 assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
 13869|             }
 13870|             else
 13871|             {
 13872|                 prev_free_item = free_list;
 13873|             }
 13874|             free_list = free_list_slot (free_list);
 13875|         }
 13876|     }
 13877| end:
 13878|     return can_fit;
 13879| }
 13880| #ifdef BACKGROUND_GC
 13881| void gc_heap::bgc_uoh_alloc_clr (uint8_t* alloc_start,
 13882|                                  size_t size,
 13883|                                  alloc_context* acontext,
 13884|                                  uint32_t flags,
 13885|                                  int gen_number,
 13886|                                  int align_const,
 13887|                                  int lock_index,
 13888|                                  BOOL check_used_p,
 13889|                                  heap_segment* seg)
 13890| {
 13891|     make_unused_array (alloc_start, size);
 13892| #ifdef DOUBLY_LINKED_FL
 13893|     clear_prev_bit (alloc_start, size);
 13894| #endif //DOUBLY_LINKED_FL
 13895|     size_t size_of_array_base = sizeof(ArrayBase);
 13896|     bgc_alloc_lock->uoh_alloc_done_with_index (lock_index);
 13897|     size_t size_to_skip = size_of_array_base;
 13898|     size_t size_to_clear = size - size_to_skip - plug_skew;
 13899|     size_t saved_size_to_clear = size_to_clear;
 13900|     if (check_used_p)
 13901|     {
 13902|         uint8_t* end = alloc_start + size - plug_skew;
 13903|         uint8_t* used = heap_segment_used (seg);
 13904|         if (used < end)
 13905|         {
 13906|             if ((alloc_start + size_to_skip) < used)
 13907|             {
 13908|                 size_to_clear = used - (alloc_start + size_to_skip);
 13909|             }
 13910|             else
 13911|             {
 13912|                 size_to_clear = 0;
 13913|             }
 13914|             dprintf (2, ("bgc uoh: setting used to %p", end));
 13915|             heap_segment_used (seg) = end;
 13916|         }
 13917|         dprintf (2, ("bgc uoh: used: %p, alloc: %p, end of alloc: %p, clear %zd bytes",
 13918|                      used, alloc_start, end, size_to_clear));
 13919|     }
 13920|     else
 13921|     {
 13922|         dprintf (2, ("bgc uoh: [%p-[%p(%zd)", alloc_start, alloc_start+size, size));
 13923|     }
 13924| #ifdef VERIFY_HEAP
 13925|     if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
 13926|     {
 13927|         if (size_to_clear < saved_size_to_clear)
 13928|         {
 13929|             size_to_clear = saved_size_to_clear;
 13930|         }
 13931|     }
 13932| #endif //VERIFY_HEAP
 13933|     size_t allocated_size = size - Align (min_obj_size, align_const);
 13934|     total_alloc_bytes_uoh += allocated_size;
 13935|     size_t etw_allocation_amount = 0;
 13936|     bool fire_event_p = update_alloc_info (gen_number, allocated_size, &etw_allocation_amount);
 13937|     dprintf (SPINLOCK_LOG, ("[%d]Lmsl to clear uoh obj", heap_number));
 13938|     add_saved_spinlock_info (true, me_release, mt_clr_large_mem, msl_entered);
 13939|     leave_spin_lock (&more_space_lock_uoh);
 13940| #ifdef FEATURE_EVENT_TRACE
 13941|     if (fire_event_p)
 13942|     {
 13943|         fire_etw_allocation_event (etw_allocation_amount, gen_number, alloc_start, size);
 13944|     }
 13945| #endif //FEATURE_EVENT_TRACE
 13946|     ((void**) alloc_start)[-1] = 0;     //clear the sync block
 13947|     if (!(flags & GC_ALLOC_ZEROING_OPTIONAL))
 13948|     {
 13949|         memclr(alloc_start + size_to_skip, size_to_clear);
 13950|     }
 13951| #ifdef MULTIPLE_HEAPS
 13952|     assert (heap_of (alloc_start) == this);
 13953| #endif // MULTIPLE_HEAPS
 13954|     bgc_alloc_lock->uoh_alloc_set (alloc_start);
 13955|     acontext->alloc_ptr = alloc_start;
 13956|     acontext->alloc_limit = (alloc_start + size - Align (min_obj_size, align_const));
 13957|     clear_unused_array(alloc_start, size);
 13958| }
 13959| #endif //BACKGROUND_GC
 13960| BOOL gc_heap::a_fit_free_list_uoh_p (size_t size,
 13961|                                        alloc_context* acontext,
 13962|                                        uint32_t flags,
 13963|                                        int align_const,
 13964|                                        int gen_number)
 13965| {
 13966|     BOOL can_fit = FALSE;
 13967|     generation* gen = generation_of (gen_number);
 13968|     allocator* allocator = generation_allocator (gen);
 13969| #ifdef FEATURE_LOH_COMPACTION
 13970|     size_t loh_pad = gen_number == loh_generation ? Align (loh_padding_obj_size, align_const) : 0;
 13971| #endif //FEATURE_LOH_COMPACTION
 13972| #ifdef BACKGROUND_GC
 13973|     int cookie = -1;
 13974| #endif //BACKGROUND_GC
 13975|     for (unsigned int a_l_idx = allocator->first_suitable_bucket(size); a_l_idx < allocator->number_of_buckets(); a_l_idx++)
 13976|     {
 13977|         uint8_t* free_list = allocator->alloc_list_head_of (a_l_idx);
 13978|         uint8_t* prev_free_item = 0;
 13979|         while (free_list != 0)
 13980|         {
 13981|             dprintf (3, ("considering free list %zx", (size_t)free_list));
 13982|             size_t free_list_size = unused_array_size(free_list);
 13983|             ptrdiff_t diff = free_list_size - size;
 13984| #ifdef FEATURE_LOH_COMPACTION
 13985|             diff -= loh_pad;
 13986| #endif //FEATURE_LOH_COMPACTION
 13987|             if ((diff == 0) || (diff >= (ptrdiff_t)Align (min_obj_size, align_const)))
 13988|             {
 13989| #ifdef BACKGROUND_GC
 13990| #ifdef MULTIPLE_HEAPS
 13991|                 assert (heap_of (free_list) == this);
 13992| #endif // MULTIPLE_HEAPS
 13993|                 cookie = bgc_alloc_lock->uoh_alloc_set (free_list);
 13994|                 bgc_track_uoh_alloc();
 13995| #endif //BACKGROUND_GC
 13996|                 allocator->unlink_item (a_l_idx, free_list, prev_free_item, FALSE);
 13997|                 remove_gen_free (gen_number, free_list_size);
 13998|                 size_t limit = limit_from_size (size - Align(min_obj_size, align_const), flags, free_list_size,
 13999|                                                 gen_number, align_const);
 14000|                 dd_new_allocation (dynamic_data_of (gen_number)) -= limit;
 14001|                 size_t saved_free_list_size = free_list_size;
 14002| #ifdef FEATURE_LOH_COMPACTION
 14003|                 if (loh_pad)
 14004|                 {
 14005|                     make_unused_array (free_list, loh_pad);
 14006|                     generation_free_obj_space (gen) += loh_pad;
 14007|                     limit -= loh_pad;
 14008|                     free_list += loh_pad;
 14009|                     free_list_size -= loh_pad;
 14010|                 }
 14011| #endif //FEATURE_LOH_COMPACTION
 14012|                 uint8_t*  remain = (free_list + limit);
 14013|                 size_t remain_size = (free_list_size - limit);
 14014|                 if (remain_size != 0)
 14015|                 {
 14016|                     assert (remain_size >= Align (min_obj_size, align_const));
 14017|                     make_unused_array (remain, remain_size);
 14018|                 }
 14019|                 if (remain_size >= Align(min_free_list, align_const))
 14020|                 {
 14021|                     uoh_thread_gap_front (remain, remain_size, gen);
 14022|                     add_gen_free (gen_number, remain_size);
 14023|                     assert (remain_size >= Align (min_obj_size, align_const));
 14024|                 }
 14025|                 else
 14026|                 {
 14027|                     generation_free_obj_space (gen) += remain_size;
 14028|                 }
 14029|                 generation_free_list_space (gen) -= saved_free_list_size;
 14030|                 assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
 14031|                 generation_free_list_allocated (gen) += limit;
 14032|                 dprintf (3, ("found fit on loh at %p", free_list));
 14033| #ifdef BACKGROUND_GC
 14034|                 if (cookie != -1)
 14035|                 {
 14036|                     bgc_uoh_alloc_clr (free_list, limit, acontext, flags, gen_number, align_const, cookie, FALSE, 0);
 14037|                 }
 14038|                 else
 14039| #endif //BACKGROUND_GC
 14040|                 {
 14041|                     adjust_limit_clr (free_list, limit, size, acontext, flags, 0, align_const, gen_number);
 14042|                 }
 14043|                 acontext->alloc_limit += Align (min_obj_size, align_const);
 14044|                 can_fit = TRUE;
 14045|                 goto exit;
 14046|             }
 14047|             prev_free_item = free_list;
 14048|             free_list = free_list_slot (free_list);
 14049|         }
 14050|     }
 14051| exit:
 14052|     return can_fit;
 14053| }
 14054| BOOL gc_heap::a_fit_segment_end_p (int gen_number,
 14055|                                    heap_segment* seg,
 14056|                                    size_t size,
 14057|                                    alloc_context* acontext,
 14058|                                    uint32_t flags,
 14059|                                    int align_const,
 14060|                                    BOOL* commit_failed_p)
 14061| {
 14062|     *commit_failed_p = FALSE;
 14063|     size_t limit = 0;
 14064|     bool hard_limit_short_seg_end_p = false;
 14065| #ifdef BACKGROUND_GC
 14066|     int cookie = -1;
 14067| #endif //BACKGROUND_GC
 14068|     uint8_t*& allocated = ((gen_number == 0) ?
 14069|                                     alloc_allocated :
 14070|                                     heap_segment_allocated(seg));
 14071|     size_t pad = Align (min_obj_size, align_const);
 14072| #ifdef FEATURE_LOH_COMPACTION
 14073|     size_t loh_pad = Align (loh_padding_obj_size, align_const);
 14074|     if (gen_number == loh_generation)
 14075|     {
 14076|         pad += loh_pad;
 14077|     }
 14078| #endif //FEATURE_LOH_COMPACTION
 14079|     uint8_t* end = heap_segment_committed (seg) - pad;
 14080|     if (a_size_fit_p (size, allocated, end, align_const))
 14081|     {
 14082|         limit = limit_from_size (size,
 14083|                                  flags,
 14084|                                  (end - allocated),
 14085|                                  gen_number, align_const);
 14086|         goto found_fit;
 14087|     }
 14088|     end = heap_segment_reserved (seg) - pad;
 14089|     if ((heap_segment_reserved (seg) != heap_segment_committed (seg)) && (a_size_fit_p (size, allocated, end, align_const)))
 14090|     {
 14091|         limit = limit_from_size (size,
 14092|                                  flags,
 14093|                                  (end - allocated),
 14094|                                  gen_number, align_const);
 14095|         if (grow_heap_segment (seg, (allocated + limit), &hard_limit_short_seg_end_p))
 14096|         {
 14097|             goto found_fit;
 14098|         }
 14099|         else
 14100|         {
 14101| #ifdef USE_REGIONS
 14102|             *commit_failed_p = TRUE;
 14103| #else
 14104|             if (!hard_limit_short_seg_end_p)
 14105|             {
 14106|                 dprintf (2, ("can't grow segment, doing a full gc"));
 14107|                 *commit_failed_p = TRUE;
 14108|             }
 14109|             else
 14110|             {
 14111|                 assert (heap_hard_limit);
 14112|             }
 14113| #endif // USE_REGIONS
 14114|         }
 14115|     }
 14116|     goto found_no_fit;
 14117| found_fit:
 14118|     dd_new_allocation (dynamic_data_of (gen_number)) -= limit;
 14119| #ifdef BACKGROUND_GC
 14120|     if (gen_number != 0)
 14121|     {
 14122| #ifdef MULTIPLE_HEAPS
 14123|         assert (heap_of (allocated) == this);
 14124| #endif // MULTIPLE_HEAPS
 14125|         cookie = bgc_alloc_lock->uoh_alloc_set (allocated);
 14126|         bgc_track_uoh_alloc();
 14127|     }
 14128| #endif //BACKGROUND_GC
 14129| #ifdef FEATURE_LOH_COMPACTION
 14130|     if (gen_number == loh_generation)
 14131|     {
 14132|         make_unused_array (allocated, loh_pad);
 14133|         generation_free_obj_space (generation_of (gen_number)) += loh_pad;
 14134|         allocated += loh_pad;
 14135|         limit -= loh_pad;
 14136|     }
 14137| #endif //FEATURE_LOH_COMPACTION
 14138| #if defined (VERIFY_HEAP) && defined (_DEBUG)
 14139|     ((void**)allocated)[-1] = 0;    // clear the sync block
 14140|     VOLATILE_MEMORY_BARRIER();
 14141| #endif //VERIFY_HEAP && _DEBUG
 14142|     uint8_t* old_alloc;
 14143|     old_alloc = allocated;
 14144|     dprintf (3, ("found fit at end of seg: %p", old_alloc));
 14145| #ifdef BACKGROUND_GC
 14146|     if (cookie != -1)
 14147|     {
 14148|         allocated += limit;
 14149|         bgc_uoh_alloc_clr (old_alloc, limit, acontext, flags, gen_number, align_const, cookie, TRUE, seg);
 14150|     }
 14151|     else
 14152| #endif //BACKGROUND_GC
 14153|     {
 14154|         if ((flags & GC_ALLOC_ZEROING_OPTIONAL) &&
 14155|             ((allocated == acontext->alloc_limit) ||
 14156|              (allocated == (acontext->alloc_limit + Align (min_obj_size, align_const)))))
 14157|         {
 14158|             assert(gen_number == 0);
 14159|             assert(allocated > acontext->alloc_ptr);
 14160|             size_t extra = allocated - acontext->alloc_ptr;
 14161|             limit -= extra;
 14162|             dynamic_data* dd = dynamic_data_of (0);
 14163|             dd_new_allocation (dd) += extra;
 14164|             limit += Align(min_obj_size, align_const);
 14165|         }
 14166|         allocated += limit;
 14167|         adjust_limit_clr (old_alloc, limit, size, acontext, flags, seg, align_const, gen_number);
 14168|     }
 14169|     return TRUE;
 14170| found_no_fit:
 14171|     return FALSE;
 14172| }
 14173| BOOL gc_heap::uoh_a_fit_segment_end_p (int gen_number,
 14174|                                        size_t size,
 14175|                                        alloc_context* acontext,
 14176|                                        uint32_t flags,
 14177|                                        int align_const,
 14178|                                        BOOL* commit_failed_p,
 14179|                                        oom_reason* oom_r)
 14180| {
 14181|     *commit_failed_p = FALSE;
 14182|     generation* gen = generation_of (gen_number);
 14183|     heap_segment* seg = generation_allocation_segment (gen);
 14184|     BOOL can_allocate_p = FALSE;
 14185|     while (seg)
 14186|     {
 14187| #ifdef BACKGROUND_GC
 14188|         if (seg->flags & heap_segment_flags_uoh_delete)
 14189|         {
 14190|             dprintf (3, ("h%d skipping seg %zx to be deleted", heap_number, (size_t)seg));
 14191|         }
 14192|         else
 14193| #endif //BACKGROUND_GC
 14194|         {
 14195|             if (a_fit_segment_end_p (gen_number, seg, (size - Align (min_obj_size, align_const)),
 14196|                                         acontext, flags, align_const, commit_failed_p))
 14197|             {
 14198|                 acontext->alloc_limit += Align (min_obj_size, align_const);
 14199|                 can_allocate_p = TRUE;
 14200|                 break;
 14201|             }
 14202|             if (*commit_failed_p)
 14203|             {
 14204|                 *oom_r = oom_cant_commit;
 14205|                 break;
 14206|             }
 14207|         }
 14208|         seg = heap_segment_next_rw (seg);
 14209|     }
 14210|     if (can_allocate_p)
 14211|     {
 14212|         generation_end_seg_allocated (gen) += size;
 14213|     }
 14214|     return can_allocate_p;
 14215| }
 14216| #ifdef BACKGROUND_GC
 14217| inline
 14218| enter_msl_status gc_heap::wait_for_background (alloc_wait_reason awr, bool loh_p)
 14219| {
 14220|     GCSpinLock* msl = loh_p ? &more_space_lock_uoh : &more_space_lock_soh;
 14221|     enter_msl_status msl_status = msl_entered;
 14222|     dprintf (2, ("BGC is already in progress, waiting for it to finish"));
 14223|     add_saved_spinlock_info (loh_p, me_release, mt_wait_bgc, msl_status);
 14224|     leave_spin_lock (msl);
 14225|     background_gc_wait (awr);
 14226|     msl_status = enter_spin_lock_msl (msl);
 14227|     add_saved_spinlock_info (loh_p, me_acquire, mt_wait_bgc, msl_status);
 14228|     return msl_status;
 14229| }
 14230| bool gc_heap::wait_for_bgc_high_memory (alloc_wait_reason awr, bool loh_p, enter_msl_status* msl_status)
 14231| {
 14232|     bool wait_p = false;
 14233|     if (gc_heap::background_running_p())
 14234|     {
 14235|         uint32_t memory_load;
 14236|         get_memory_info (&memory_load);
 14237|         if (memory_load >= m_high_memory_load_th)
 14238|         {
 14239|             wait_p = true;
 14240|             dprintf (GTC_LOG, ("high mem - wait for BGC to finish, wait reason: %d", awr));
 14241|             *msl_status = wait_for_background (awr, loh_p);
 14242|         }
 14243|     }
 14244|     return wait_p;
 14245| }
 14246| #endif //BACKGROUND_GC
 14247| BOOL gc_heap::trigger_ephemeral_gc (gc_reason gr, enter_msl_status* msl_status)
 14248| {
 14249| #ifdef BACKGROUND_GC
 14250|     wait_for_bgc_high_memory (awr_loh_oos_bgc, false, msl_status);
 14251|     if (*msl_status == msl_retry_different_heap) return FALSE;
 14252| #endif //BACKGROUND_GC
 14253|     BOOL did_full_compact_gc = FALSE;
 14254|     dprintf (1, ("h%d triggering a gen1 GC", heap_number));
 14255|     size_t last_full_compact_gc_count = get_full_compact_gc_count();
 14256|     vm_heap->GarbageCollectGeneration(max_generation - 1, gr);
 14257| #ifdef MULTIPLE_HEAPS
 14258|     *msl_status = enter_spin_lock_msl (&more_space_lock_soh);
 14259|     if (*msl_status == msl_retry_different_heap) return FALSE;
 14260|     add_saved_spinlock_info (false, me_acquire, mt_t_eph_gc, *msl_status);
 14261| #endif //MULTIPLE_HEAPS
 14262|     size_t current_full_compact_gc_count = get_full_compact_gc_count();
 14263|     if (current_full_compact_gc_count > last_full_compact_gc_count)
 14264|     {
 14265|         dprintf (2, ("attempted to trigger an ephemeral GC and got a full compacting GC"));
 14266|         did_full_compact_gc = TRUE;
 14267|     }
 14268|     return did_full_compact_gc;
 14269| }
 14270| BOOL gc_heap::soh_try_fit (int gen_number,
 14271|                            size_t size,
 14272|                            alloc_context* acontext,
 14273|                            uint32_t flags,
 14274|                            int align_const,
 14275|                            BOOL* commit_failed_p,
 14276|                            BOOL* short_seg_end_p)
 14277| {
 14278|     BOOL can_allocate = TRUE;
 14279|     if (short_seg_end_p)
 14280|     {
 14281|         *short_seg_end_p = FALSE;
 14282|     }
 14283|     can_allocate = a_fit_free_list_p (gen_number, size, acontext, flags, align_const);
 14284|     if (!can_allocate)
 14285|     {
 14286|         if (short_seg_end_p)
 14287|         {
 14288|             *short_seg_end_p = short_on_end_of_seg (ephemeral_heap_segment);
 14289|         }
 14290|         if (!short_seg_end_p || !(*short_seg_end_p))
 14291|         {
 14292| #ifdef USE_REGIONS
 14293|             while (ephemeral_heap_segment)
 14294| #endif //USE_REGIONS
 14295|             {
 14296|                 can_allocate = a_fit_segment_end_p (gen_number, ephemeral_heap_segment, size,
 14297|                                                     acontext, flags, align_const, commit_failed_p);
 14298| #ifdef USE_REGIONS
 14299|                 if (can_allocate)
 14300|                 {
 14301|                     break;
 14302|                 }
 14303|                 dprintf (REGIONS_LOG, ("h%d fixing region %p end to alloc ptr: %p, alloc_allocated %p",
 14304|                     heap_number, heap_segment_mem (ephemeral_heap_segment), acontext->alloc_ptr,
 14305|                     alloc_allocated));
 14306|                 fix_allocation_context (acontext, TRUE, FALSE);
 14307|                 fix_youngest_allocation_area();
 14308|                 heap_segment* next_seg = heap_segment_next (ephemeral_heap_segment);
 14309|                 bool new_seg = false;
 14310|                 if (!next_seg)
 14311|                 {
 14312|                     assert (ephemeral_heap_segment == generation_tail_region (generation_of (gen_number)));
 14313|                     next_seg = get_new_region (gen_number);
 14314|                     new_seg = true;
 14315|                 }
 14316|                 if (next_seg)
 14317|                 {
 14318|                     dprintf (REGIONS_LOG, ("eph seg %p -> next %p",
 14319|                         heap_segment_mem (ephemeral_heap_segment), heap_segment_mem (next_seg)));
 14320|                     ephemeral_heap_segment = next_seg;
 14321|                     if (new_seg)
 14322|                     {
 14323|                         GCToEEInterface::DiagAddNewRegion(
 14324|                             heap_segment_gen_num (next_seg),
 14325|                             heap_segment_mem (next_seg),
 14326|                             heap_segment_allocated (next_seg),
 14327|                             heap_segment_reserved (next_seg)
 14328|                         );
 14329|                     }
 14330|                 }
 14331|                 else
 14332|                 {
 14333|                     *commit_failed_p = TRUE;
 14334|                     dprintf (REGIONS_LOG, ("couldn't get a new ephemeral region"));
 14335|                     return FALSE;
 14336|                 }
 14337|                 alloc_allocated = heap_segment_allocated (ephemeral_heap_segment);
 14338|                 dprintf (REGIONS_LOG, ("h%d alloc_allocated is now %p", heap_number, alloc_allocated));
 14339| #endif //USE_REGIONS
 14340|             }
 14341|         }
 14342|     }
 14343|     return can_allocate;
 14344| }
 14345| allocation_state gc_heap::allocate_soh (int gen_number,
 14346|                                           size_t size,
 14347|                                           alloc_context* acontext,
 14348|                                           uint32_t flags,
 14349|                                           int align_const)
 14350| {
 14351|     enter_msl_status msl_status = msl_entered;
 14352| #if defined (BACKGROUND_GC) && !defined (MULTIPLE_HEAPS)
 14353|     if (gc_heap::background_running_p())
 14354|     {
 14355|         background_soh_alloc_count++;
 14356|         if ((background_soh_alloc_count % bgc_alloc_spin_count) == 0)
 14357|         {
 14358|             add_saved_spinlock_info (false, me_release, mt_alloc_small, msl_status);
 14359|             leave_spin_lock (&more_space_lock_soh);
 14360|             bool cooperative_mode = enable_preemptive();
 14361|             GCToOSInterface::Sleep (bgc_alloc_spin);
 14362|             disable_preemptive (cooperative_mode);
 14363|             msl_status = enter_spin_lock_msl (&more_space_lock_soh);
 14364|             if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
 14365|             add_saved_spinlock_info (false, me_acquire, mt_alloc_small, msl_status);
 14366|         }
 14367|         else
 14368|         {
 14369|         }
 14370|     }
 14371| #endif //BACKGROUND_GC && !MULTIPLE_HEAPS
 14372|     gc_reason gr = reason_oos_soh;
 14373|     oom_reason oom_r = oom_no_failure;
 14374|     allocation_state soh_alloc_state = a_state_start;
 14375|     while (1)
 14376|     {
 14377|         dprintf (3, ("[h%d]soh state is %s", heap_number, allocation_state_str[soh_alloc_state]));
 14378|         switch (soh_alloc_state)
 14379|         {
 14380|             case a_state_can_allocate:
 14381|             case a_state_cant_allocate:
 14382|             {
 14383|                 goto exit;
 14384|             }
 14385|             case a_state_start:
 14386|             {
 14387|                 soh_alloc_state = a_state_try_fit;
 14388|                 break;
 14389|             }
 14390|             case a_state_try_fit:
 14391|             {
 14392|                 BOOL commit_failed_p = FALSE;
 14393|                 BOOL can_use_existing_p = FALSE;
 14394|                 can_use_existing_p = soh_try_fit (gen_number, size, acontext, flags,
 14395|                                                   align_const, &commit_failed_p,
 14396|                                                   NULL);
 14397|                 soh_alloc_state = (can_use_existing_p ?
 14398|                                         a_state_can_allocate :
 14399|                                         (commit_failed_p ?
 14400|                                             a_state_trigger_full_compact_gc :
 14401|                                             a_state_trigger_ephemeral_gc));
 14402|                 break;
 14403|             }
 14404|             case a_state_try_fit_after_bgc:
 14405|             {
 14406|                 BOOL commit_failed_p = FALSE;
 14407|                 BOOL can_use_existing_p = FALSE;
 14408|                 BOOL short_seg_end_p = FALSE;
 14409|                 can_use_existing_p = soh_try_fit (gen_number, size, acontext, flags,
 14410|                                                   align_const, &commit_failed_p,
 14411|                                                   &short_seg_end_p);
 14412|                 soh_alloc_state = (can_use_existing_p ?
 14413|                                         a_state_can_allocate :
 14414|                                         (short_seg_end_p ?
 14415|                                             a_state_trigger_2nd_ephemeral_gc :
 14416|                                             a_state_trigger_full_compact_gc));
 14417|                 break;
 14418|             }
 14419|             case a_state_try_fit_after_cg:
 14420|             {
 14421|                 BOOL commit_failed_p = FALSE;
 14422|                 BOOL can_use_existing_p = FALSE;
 14423|                 BOOL short_seg_end_p = FALSE;
 14424|                 can_use_existing_p = soh_try_fit (gen_number, size, acontext, flags,
 14425|                                                   align_const, &commit_failed_p,
 14426|                                                   &short_seg_end_p);
 14427|                 if (can_use_existing_p)
 14428|                 {
 14429|                     soh_alloc_state = a_state_can_allocate;
 14430|                 }
 14431| #ifdef MULTIPLE_HEAPS
 14432|                 else if (gen0_allocated_after_gc_p)
 14433|                 {
 14434|                     soh_alloc_state = a_state_trigger_ephemeral_gc;
 14435|                 }
 14436| #endif //MULTIPLE_HEAPS
 14437|                 else if (short_seg_end_p)
 14438|                 {
 14439|                     soh_alloc_state = a_state_cant_allocate;
 14440|                     oom_r = oom_budget;
 14441|                 }
 14442|                 else
 14443|                 {
 14444|                     assert (commit_failed_p || heap_hard_limit);
 14445|                     soh_alloc_state = a_state_cant_allocate;
 14446|                     oom_r = oom_cant_commit;
 14447|                 }
 14448|                 break;
 14449|             }
 14450|             case a_state_check_and_wait_for_bgc:
 14451|             {
 14452|                 BOOL bgc_in_progress_p = FALSE;
 14453|                 BOOL did_full_compacting_gc = FALSE;
 14454|                 bgc_in_progress_p = check_and_wait_for_bgc (awr_gen0_oos_bgc, &did_full_compacting_gc, false, &msl_status);
 14455|                 if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
 14456|                 soh_alloc_state = (did_full_compacting_gc ?
 14457|                                         a_state_try_fit_after_cg :
 14458|                                         a_state_try_fit_after_bgc);
 14459|                 break;
 14460|             }
 14461|             case a_state_trigger_ephemeral_gc:
 14462|             {
 14463|                 BOOL commit_failed_p = FALSE;
 14464|                 BOOL can_use_existing_p = FALSE;
 14465|                 BOOL short_seg_end_p = FALSE;
 14466|                 BOOL bgc_in_progress_p = FALSE;
 14467|                 BOOL did_full_compacting_gc = FALSE;
 14468|                 did_full_compacting_gc = trigger_ephemeral_gc (gr, &msl_status);
 14469|                 if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
 14470|                 if (did_full_compacting_gc)
 14471|                 {
 14472|                     soh_alloc_state = a_state_try_fit_after_cg;
 14473|                 }
 14474|                 else
 14475|                 {
 14476|                     can_use_existing_p = soh_try_fit (gen_number, size, acontext, flags,
 14477|                                                       align_const, &commit_failed_p,
 14478|                                                       &short_seg_end_p);
 14479| #ifdef BACKGROUND_GC
 14480|                     bgc_in_progress_p = gc_heap::background_running_p();
 14481| #endif //BACKGROUND_GC
 14482|                     if (can_use_existing_p)
 14483|                     {
 14484|                         soh_alloc_state = a_state_can_allocate;
 14485|                     }
 14486|                     else
 14487|                     {
 14488|                         if (short_seg_end_p)
 14489|                         {
 14490| #ifndef USE_REGIONS
 14491|                             if (should_expand_in_full_gc)
 14492|                             {
 14493|                                 dprintf (2, ("gen1 GC wanted to expand!"));
 14494|                                 soh_alloc_state = a_state_trigger_full_compact_gc;
 14495|                             }
 14496|                             else
 14497| #endif //!USE_REGIONS
 14498|                             {
 14499|                                 soh_alloc_state = (bgc_in_progress_p ?
 14500|                                                         a_state_check_and_wait_for_bgc :
 14501|                                                         a_state_trigger_full_compact_gc);
 14502|                             }
 14503|                         }
 14504|                         else if (commit_failed_p)
 14505|                         {
 14506|                             soh_alloc_state = a_state_trigger_full_compact_gc;
 14507|                         }
 14508|                         else
 14509|                         {
 14510| #ifdef MULTIPLE_HEAPS
 14511|                             assert (gen0_allocated_after_gc_p);
 14512|                             soh_alloc_state = a_state_trigger_ephemeral_gc;
 14513| #else //MULTIPLE_HEAPS
 14514|                             assert (!"shouldn't get here");
 14515| #endif //MULTIPLE_HEAPS
 14516|                         }
 14517|                     }
 14518|                 }
 14519|                 break;
 14520|             }
 14521|             case a_state_trigger_2nd_ephemeral_gc:
 14522|             {
 14523|                 BOOL commit_failed_p = FALSE;
 14524|                 BOOL can_use_existing_p = FALSE;
 14525|                 BOOL short_seg_end_p = FALSE;
 14526|                 BOOL did_full_compacting_gc = FALSE;
 14527|                 did_full_compacting_gc = trigger_ephemeral_gc (gr, &msl_status);
 14528|                 if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
 14529|                 if (did_full_compacting_gc)
 14530|                 {
 14531|                     soh_alloc_state = a_state_try_fit_after_cg;
 14532|                 }
 14533|                 else
 14534|                 {
 14535|                     can_use_existing_p = soh_try_fit (gen_number, size, acontext, flags,
 14536|                                                       align_const, &commit_failed_p,
 14537|                                                       &short_seg_end_p);
 14538|                     if (short_seg_end_p || commit_failed_p)
 14539|                     {
 14540|                         soh_alloc_state = a_state_trigger_full_compact_gc;
 14541|                     }
 14542|                     else
 14543|                     {
 14544|                         assert (can_use_existing_p);
 14545|                         soh_alloc_state = a_state_can_allocate;
 14546|                     }
 14547|                 }
 14548|                 break;
 14549|             }
 14550|             case a_state_trigger_full_compact_gc:
 14551|             {
 14552|                 if (fgn_maxgen_percent)
 14553|                 {
 14554|                     dprintf (2, ("FGN: SOH doing last GC before we throw OOM"));
 14555|                     send_full_gc_notification (max_generation, FALSE);
 14556|                 }
 14557|                 BOOL got_full_compacting_gc = FALSE;
 14558|                 got_full_compacting_gc = trigger_full_compact_gc (gr, &oom_r, false, &msl_status);
 14559|                 if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
 14560|                 soh_alloc_state = (got_full_compacting_gc ? a_state_try_fit_after_cg : a_state_cant_allocate);
 14561|                 break;
 14562|             }
 14563|             default:
 14564|             {
 14565|                 assert (!"Invalid state!");
 14566|                 break;
 14567|             }
 14568|         }
 14569|     }
 14570| exit:
 14571|     if (soh_alloc_state == a_state_cant_allocate)
 14572|     {
 14573|         assert (oom_r != oom_no_failure);
 14574|         handle_oom (oom_r,
 14575|                     size,
 14576|                     heap_segment_allocated (ephemeral_heap_segment),
 14577|                     heap_segment_reserved (ephemeral_heap_segment));
 14578|         add_saved_spinlock_info (false, me_release, mt_alloc_small_cant, msl_entered);
 14579|         leave_spin_lock (&more_space_lock_soh);
 14580|     }
 14581|     assert ((soh_alloc_state == a_state_can_allocate) ||
 14582|             (soh_alloc_state == a_state_cant_allocate) ||
 14583|             (soh_alloc_state == a_state_retry_allocate));
 14584|     return soh_alloc_state;
 14585| }
 14586| #ifdef BACKGROUND_GC
 14587| inline
 14588| void gc_heap::bgc_track_uoh_alloc()
 14589| {
 14590|     if (current_c_gc_state == c_gc_state_planning)
 14591|     {
 14592|         Interlocked::Increment (&uoh_alloc_thread_count);
 14593|         dprintf (3, ("h%d: inc lc: %d", heap_number, (int32_t)uoh_alloc_thread_count));
 14594|     }
 14595| }
 14596| inline
 14597| void gc_heap::bgc_untrack_uoh_alloc()
 14598| {
 14599|     if (current_c_gc_state == c_gc_state_planning)
 14600|     {
 14601|         Interlocked::Decrement (&uoh_alloc_thread_count);
 14602|         dprintf (3, ("h%d: dec lc: %d", heap_number, (int32_t)uoh_alloc_thread_count));
 14603|     }
 14604| }
 14605| int bgc_allocate_spin(size_t min_gc_size, size_t bgc_begin_size, size_t bgc_size_increased, size_t end_size)
 14606| {
 14607|     if ((bgc_begin_size + bgc_size_increased) < (min_gc_size * 10))
 14608|     {
 14609|         return 0;
 14610|     }
 14611|     if ((bgc_begin_size >= (2 * end_size)) || (bgc_size_increased >= bgc_begin_size))
 14612|     {
 14613|         if (bgc_begin_size >= (2 * end_size))
 14614|         {
 14615|             dprintf (3, ("alloc-ed too much before bgc started"));
 14616|         }
 14617|         else
 14618|         {
 14619|             dprintf (3, ("alloc-ed too much after bgc started"));
 14620|         }
 14621|         return -1;
 14622|     }
 14623|     else
 14624|     {
 14625|         return (int)(((float)bgc_size_increased / (float)bgc_begin_size) * 10);
 14626|     }
 14627| }
 14628| int gc_heap::bgc_loh_allocate_spin()
 14629| {
 14630|     size_t min_gc_size = dd_min_size (dynamic_data_of (loh_generation));
 14631|     size_t bgc_begin_size = bgc_begin_loh_size;
 14632|     size_t bgc_size_increased = bgc_loh_size_increased;
 14633|     size_t end_size = end_loh_size;
 14634|     return bgc_allocate_spin(min_gc_size, bgc_begin_size, bgc_size_increased, end_size);
 14635| }
 14636| int gc_heap::bgc_poh_allocate_spin()
 14637| {
 14638|     size_t min_gc_size = dd_min_size (dynamic_data_of (poh_generation));
 14639|     size_t bgc_begin_size = bgc_begin_poh_size;
 14640|     size_t bgc_size_increased = bgc_poh_size_increased;
 14641|     size_t end_size = end_poh_size;
 14642|     return bgc_allocate_spin(min_gc_size, bgc_begin_size, bgc_size_increased, end_size);
 14643| }
 14644| #endif //BACKGROUND_GC
 14645| size_t gc_heap::get_uoh_seg_size (size_t size)
 14646| {
 14647|     size_t default_seg_size =
 14648| #ifdef USE_REGIONS
 14649|         global_region_allocator.get_large_region_alignment();
 14650| #else
 14651|         min_uoh_segment_size;
 14652| #endif //USE_REGIONS
 14653|     size_t align_size =  default_seg_size;
 14654|     int align_const = get_alignment_constant (FALSE);
 14655|     size_t large_seg_size = align_on_page (
 14656|         max (default_seg_size,
 14657|             ((size + 2 * Align(min_obj_size, align_const) + OS_PAGE_SIZE +
 14658|             align_size) / align_size * align_size)));
 14659|     return large_seg_size;
 14660| }
 14661| BOOL gc_heap::uoh_get_new_seg (int gen_number,
 14662|                                size_t size,
 14663|                                BOOL* did_full_compact_gc,
 14664|                                oom_reason* oom_r,
 14665|                                enter_msl_status* msl_status)
 14666| {
 14667|     *did_full_compact_gc = FALSE;
 14668|     size_t seg_size = get_uoh_seg_size (size);
 14669|     heap_segment* new_seg = get_uoh_segment (gen_number, seg_size, did_full_compact_gc, msl_status);
 14670|     if (*msl_status == msl_retry_different_heap) return FALSE;
 14671|     if (new_seg && (gen_number == loh_generation))
 14672|     {
 14673|         loh_alloc_since_cg += seg_size;
 14674|     }
 14675|     else
 14676|     {
 14677|         *oom_r = oom_loh;
 14678|     }
 14679|     return (new_seg != 0);
 14680| }
 14681| BOOL gc_heap::retry_full_compact_gc (size_t size)
 14682| {
 14683|     size_t seg_size = get_uoh_seg_size (size);
 14684|     if (loh_alloc_since_cg >= (2 * (uint64_t)seg_size))
 14685|     {
 14686|         return TRUE;
 14687|     }
 14688| #ifdef MULTIPLE_HEAPS
 14689|     uint64_t total_alloc_size = 0;
 14690|     for (int i = 0; i < n_heaps; i++)
 14691|     {
 14692|         total_alloc_size += g_heaps[i]->loh_alloc_since_cg;
 14693|     }
 14694|     if (total_alloc_size >= (2 * (uint64_t)seg_size))
 14695|     {
 14696|         return TRUE;
 14697|     }
 14698| #endif //MULTIPLE_HEAPS
 14699|     return FALSE;
 14700| }
 14701| BOOL gc_heap::check_and_wait_for_bgc (alloc_wait_reason awr,
 14702|                                       BOOL* did_full_compact_gc,
 14703|                                       bool loh_p,
 14704|                                       enter_msl_status* msl_status)
 14705| {
 14706|     BOOL bgc_in_progress = FALSE;
 14707|     *did_full_compact_gc = FALSE;
 14708| #ifdef BACKGROUND_GC
 14709|     if (gc_heap::background_running_p())
 14710|     {
 14711|         bgc_in_progress = TRUE;
 14712|         size_t last_full_compact_gc_count = get_full_compact_gc_count();
 14713|         *msl_status = wait_for_background (awr, loh_p);
 14714|         size_t current_full_compact_gc_count = get_full_compact_gc_count();
 14715|         if (current_full_compact_gc_count > last_full_compact_gc_count)
 14716|         {
 14717|             *did_full_compact_gc = TRUE;
 14718|         }
 14719|     }
 14720| #endif //BACKGROUND_GC
 14721|     return bgc_in_progress;
 14722| }
 14723| BOOL gc_heap::uoh_try_fit (int gen_number,
 14724|                            size_t size,
 14725|                            alloc_context* acontext,
 14726|                            uint32_t flags,
 14727|                            int align_const,
 14728|                            BOOL* commit_failed_p,
 14729|                            oom_reason* oom_r)
 14730| {
 14731|     BOOL can_allocate = TRUE;
 14732|     if (!a_fit_free_list_uoh_p (size, acontext, flags, align_const, gen_number))
 14733|     {
 14734|         can_allocate = uoh_a_fit_segment_end_p (gen_number, size,
 14735|                                                 acontext, flags, align_const,
 14736|                                                 commit_failed_p, oom_r);
 14737| #ifdef BACKGROUND_GC
 14738|         if (can_allocate && gc_heap::background_running_p())
 14739|         {
 14740|             if (gen_number == poh_generation)
 14741|             {
 14742|                 bgc_poh_size_increased += size;
 14743|             }
 14744|             else
 14745|             {
 14746|                 bgc_loh_size_increased += size;
 14747|             }
 14748|         }
 14749| #endif //BACKGROUND_GC
 14750|     }
 14751|     return can_allocate;
 14752| }
 14753| BOOL gc_heap::trigger_full_compact_gc (gc_reason gr,
 14754|                                        oom_reason* oom_r,
 14755|                                        bool loh_p,
 14756|                                        enter_msl_status* msl_status)
 14757| {
 14758|     BOOL did_full_compact_gc = FALSE;
 14759|     size_t last_full_compact_gc_count = get_full_compact_gc_count();
 14760|     if (!last_gc_before_oom)
 14761|     {
 14762|         last_gc_before_oom = TRUE;
 14763|     }
 14764| #ifdef BACKGROUND_GC
 14765|     if (gc_heap::background_running_p())
 14766|     {
 14767|         *msl_status = wait_for_background (((gr == reason_oos_soh) ? awr_gen0_oos_bgc : awr_loh_oos_bgc), loh_p);
 14768|         dprintf (2, ("waited for BGC - done"));
 14769|         if (*msl_status == msl_retry_different_heap) return FALSE;
 14770|     }
 14771| #endif //BACKGROUND_GC
 14772|     GCSpinLock* msl = loh_p ? &more_space_lock_uoh : &more_space_lock_soh;
 14773|     size_t current_full_compact_gc_count = get_full_compact_gc_count();
 14774|     if (current_full_compact_gc_count > last_full_compact_gc_count)
 14775|     {
 14776|         dprintf (3, ("a full compacting GC triggered while waiting for BGC (%zd->%zd)", last_full_compact_gc_count, current_full_compact_gc_count));
 14777|         assert (current_full_compact_gc_count > last_full_compact_gc_count);
 14778|         did_full_compact_gc = TRUE;
 14779|         goto exit;
 14780|     }
 14781|     dprintf (3, ("h%d full GC", heap_number));
 14782|     *msl_status = trigger_gc_for_alloc (max_generation, gr, msl, loh_p, mt_t_full_gc);
 14783|     current_full_compact_gc_count = get_full_compact_gc_count();
 14784|     if (current_full_compact_gc_count == last_full_compact_gc_count)
 14785|     {
 14786|         dprintf (2, ("attempted to trigger a full compacting GC but didn't get it"));
 14787|         *oom_r = oom_unproductive_full_gc;
 14788|     }
 14789|     else
 14790|     {
 14791|         dprintf (3, ("h%d: T full compacting GC (%zd->%zd)",
 14792|             heap_number,
 14793|             last_full_compact_gc_count,
 14794|             current_full_compact_gc_count));
 14795|         assert (current_full_compact_gc_count > last_full_compact_gc_count);
 14796|         did_full_compact_gc = TRUE;
 14797|     }
 14798| exit:
 14799|     return did_full_compact_gc;
 14800| }
 14801| #ifdef RECORD_LOH_STATE
 14802| void gc_heap::add_saved_loh_state (allocation_state loh_state_to_save, EEThreadId thread_id)
 14803| {
 14804|     if (loh_state_to_save != a_state_can_allocate)
 14805|     {
 14806|         last_loh_states[loh_state_index].alloc_state = loh_state_to_save;
 14807|         last_loh_states[loh_state_index].gc_index = VolatileLoadWithoutBarrier (&settings.gc_index);
 14808|         last_loh_states[loh_state_index].thread_id = thread_id;
 14809|         loh_state_index++;
 14810|         if (loh_state_index == max_saved_loh_states)
 14811|         {
 14812|             loh_state_index = 0;
 14813|         }
 14814|         assert (loh_state_index < max_saved_loh_states);
 14815|     }
 14816| }
 14817| #endif //RECORD_LOH_STATE
 14818| bool gc_heap::should_retry_other_heap (int gen_number, size_t size)
 14819| {
 14820| #ifdef MULTIPLE_HEAPS
 14821|     if (heap_hard_limit)
 14822|     {
 14823|         size_t min_size = dd_min_size (g_heaps[0]->dynamic_data_of (gen_number));
 14824|         size_t slack_space = max (commit_min_th, min_size);
 14825|         bool retry_p = ((current_total_committed + size) < (heap_hard_limit - slack_space));
 14826|         dprintf (1, ("%zd - %zd - total committed %zd - size %zd = %zd, %s",
 14827|             heap_hard_limit, slack_space, current_total_committed, size,
 14828|             (heap_hard_limit - slack_space - current_total_committed - size),
 14829|             (retry_p ? "retry" : "no retry")));
 14830|         return retry_p;
 14831|     }
 14832|     else
 14833| #endif //MULTIPLE_HEAPS
 14834|     {
 14835|         return false;
 14836|     }
 14837| }
 14838| allocation_state gc_heap::allocate_uoh (int gen_number,
 14839|                                           size_t size,
 14840|                                           alloc_context* acontext,
 14841|                                           uint32_t flags,
 14842|                                           int align_const)
 14843| {
 14844|     enter_msl_status msl_status = msl_entered;
 14845|     allocation_state uoh_alloc_state = a_state_start;
 14846| #ifdef SPINLOCK_HISTORY
 14847|     current_uoh_alloc_state = uoh_alloc_state;
 14848| #endif //SPINLOCK_HISTORY
 14849| #ifdef RECORD_LOH_STATE
 14850|     EEThreadId current_thread_id;
 14851|     current_thread_id.SetToCurrentThread ();
 14852| #endif //RECORD_LOH_STATE
 14853| #ifdef BACKGROUND_GC
 14854|     if (gc_heap::background_running_p())
 14855|     {
 14856| #ifdef BGC_SERVO_TUNING
 14857|         bool planning_p = (current_c_gc_state == c_gc_state_planning);
 14858| #endif //BGC_SERVO_TUNING
 14859|         background_uoh_alloc_count++;
 14860|         {
 14861| #ifdef BGC_SERVO_TUNING
 14862|             if (planning_p)
 14863|             {
 14864|                 loh_a_bgc_planning += size;
 14865|             }
 14866|             else
 14867|             {
 14868|                 loh_a_bgc_marking += size;
 14869|             }
 14870| #endif //BGC_SERVO_TUNING
 14871|             int spin_for_allocation = (gen_number == loh_generation) ?
 14872|                 bgc_loh_allocate_spin() :
 14873|                 bgc_poh_allocate_spin();
 14874|             if (spin_for_allocation > 0)
 14875|             {
 14876|                 add_saved_spinlock_info (true, me_release, mt_alloc_large, msl_status);
 14877|                 leave_spin_lock (&more_space_lock_uoh);
 14878|                 bool cooperative_mode = enable_preemptive();
 14879|                 GCToOSInterface::YieldThread (spin_for_allocation);
 14880|                 disable_preemptive (cooperative_mode);
 14881|                 msl_status = enter_spin_lock_msl (&more_space_lock_uoh);
 14882|                 if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
 14883|                 add_saved_spinlock_info (true, me_acquire, mt_alloc_large, msl_status);
 14884|                 dprintf (SPINLOCK_LOG, ("[%d]spin Emsl uoh", heap_number));
 14885|             }
 14886|             else if (spin_for_allocation < 0)
 14887|             {
 14888|                 msl_status = wait_for_background (awr_uoh_alloc_during_bgc, true);
 14889|                 check_msl_status ("uoh a_state_acquire_seg", size);
 14890|             }
 14891|         }
 14892|     }
 14893| #ifdef BGC_SERVO_TUNING
 14894|     else
 14895|     {
 14896|         loh_a_no_bgc += size;
 14897|     }
 14898| #endif //BGC_SERVO_TUNING
 14899| #endif //BACKGROUND_GC
 14900|     gc_reason gr = reason_oos_loh;
 14901|     generation* gen = generation_of (gen_number);
 14902|     oom_reason oom_r = oom_no_failure;
 14903|     size_t current_full_compact_gc_count = 0;
 14904|     while (1)
 14905|     {
 14906|         dprintf (3, ("[h%d]loh state is %s", heap_number, allocation_state_str[uoh_alloc_state]));
 14907| #ifdef SPINLOCK_HISTORY
 14908|         current_uoh_alloc_state = uoh_alloc_state;
 14909| #endif //SPINLOCK_HISTORY
 14910| #ifdef RECORD_LOH_STATE
 14911|         current_uoh_alloc_state = uoh_alloc_state;
 14912|         add_saved_loh_state (uoh_alloc_state, current_thread_id);
 14913| #endif //RECORD_LOH_STATE
 14914|         switch (uoh_alloc_state)
 14915|         {
 14916|             case a_state_can_allocate:
 14917|             case a_state_cant_allocate:
 14918|             {
 14919|                 goto exit;
 14920|             }
 14921|             case a_state_start:
 14922|             {
 14923|                 uoh_alloc_state = a_state_try_fit;
 14924|                 break;
 14925|             }
 14926|             case a_state_try_fit:
 14927|             {
 14928|                 BOOL commit_failed_p = FALSE;
 14929|                 BOOL can_use_existing_p = FALSE;
 14930|                 can_use_existing_p = uoh_try_fit (gen_number, size, acontext, flags,
 14931|                                                   align_const, &commit_failed_p, &oom_r);
 14932|                 uoh_alloc_state = (can_use_existing_p ?
 14933|                                         a_state_can_allocate :
 14934|                                         (commit_failed_p ?
 14935|                                             a_state_trigger_full_compact_gc :
 14936|                                             a_state_acquire_seg));
 14937|                 assert ((uoh_alloc_state == a_state_can_allocate) == (acontext->alloc_ptr != 0));
 14938|                 break;
 14939|             }
 14940|             case a_state_try_fit_new_seg:
 14941|             {
 14942|                 BOOL commit_failed_p = FALSE;
 14943|                 BOOL can_use_existing_p = FALSE;
 14944|                 can_use_existing_p = uoh_try_fit (gen_number, size, acontext, flags,
 14945|                                                   align_const, &commit_failed_p, &oom_r);
 14946|                 uoh_alloc_state = (can_use_existing_p ? a_state_can_allocate : a_state_try_fit);
 14947|                 assert ((uoh_alloc_state == a_state_can_allocate) == (acontext->alloc_ptr != 0));
 14948|                 break;
 14949|             }
 14950|             case a_state_try_fit_after_cg:
 14951|             {
 14952|                 BOOL commit_failed_p = FALSE;
 14953|                 BOOL can_use_existing_p = FALSE;
 14954|                 can_use_existing_p = uoh_try_fit (gen_number, size, acontext, flags,
 14955|                                                   align_const, &commit_failed_p, &oom_r);
 14956|                 uoh_alloc_state = (can_use_existing_p ?
 14957|                                         a_state_can_allocate :
 14958|                                         (commit_failed_p ?
 14959|                                             a_state_cant_allocate :
 14960|                                             a_state_acquire_seg_after_cg));
 14961|                 assert ((uoh_alloc_state == a_state_can_allocate) == (acontext->alloc_ptr != 0));
 14962|                 break;
 14963|             }
 14964|             case a_state_try_fit_after_bgc:
 14965|             {
 14966|                 BOOL commit_failed_p = FALSE;
 14967|                 BOOL can_use_existing_p = FALSE;
 14968|                 can_use_existing_p = uoh_try_fit (gen_number, size, acontext, flags,
 14969|                                                   align_const, &commit_failed_p, &oom_r);
 14970|                 uoh_alloc_state = (can_use_existing_p ?
 14971|                                         a_state_can_allocate :
 14972|                                         (commit_failed_p ?
 14973|                                             a_state_trigger_full_compact_gc :
 14974|                                             a_state_acquire_seg_after_bgc));
 14975|                 assert ((uoh_alloc_state == a_state_can_allocate) == (acontext->alloc_ptr != 0));
 14976|                 break;
 14977|             }
 14978|             case a_state_acquire_seg:
 14979|             {
 14980|                 BOOL can_get_new_seg_p = FALSE;
 14981|                 BOOL did_full_compacting_gc = FALSE;
 14982|                 current_full_compact_gc_count = get_full_compact_gc_count();
 14983|                 can_get_new_seg_p = uoh_get_new_seg (gen_number, size, &did_full_compacting_gc, &oom_r, &msl_status);
 14984|                 check_msl_status ("uoh a_state_acquire_seg", size);
 14985|                 uoh_alloc_state = (can_get_new_seg_p ?
 14986|                                         a_state_try_fit_new_seg :
 14987|                                         (did_full_compacting_gc ?
 14988|                                             a_state_check_retry_seg :
 14989|                                             a_state_check_and_wait_for_bgc));
 14990|                 break;
 14991|             }
 14992|             case a_state_acquire_seg_after_cg:
 14993|             {
 14994|                 BOOL can_get_new_seg_p = FALSE;
 14995|                 BOOL did_full_compacting_gc = FALSE;
 14996|                 current_full_compact_gc_count = get_full_compact_gc_count();
 14997|                 can_get_new_seg_p = uoh_get_new_seg (gen_number, size, &did_full_compacting_gc, &oom_r, &msl_status);
 14998|                 check_msl_status ("uoh a_state_acquire_seg_after_cg", size);
 14999|                 uoh_alloc_state = (can_get_new_seg_p ?
 15000|                                         a_state_try_fit_after_cg :
 15001|                                         a_state_check_retry_seg);
 15002|                 break;
 15003|             }
 15004|             case a_state_acquire_seg_after_bgc:
 15005|             {
 15006|                 BOOL can_get_new_seg_p = FALSE;
 15007|                 BOOL did_full_compacting_gc = FALSE;
 15008|                 current_full_compact_gc_count = get_full_compact_gc_count();
 15009|                 can_get_new_seg_p = uoh_get_new_seg (gen_number, size, &did_full_compacting_gc, &oom_r, &msl_status);
 15010|                 check_msl_status ("uoh a_state_acquire_seg_after_bgc", size);
 15011|                 uoh_alloc_state = (can_get_new_seg_p ?
 15012|                                         a_state_try_fit_new_seg :
 15013|                                         (did_full_compacting_gc ?
 15014|                                             a_state_check_retry_seg :
 15015|                                             a_state_trigger_full_compact_gc));
 15016|                 assert ((uoh_alloc_state != a_state_cant_allocate) || (oom_r != oom_no_failure));
 15017|                 break;
 15018|             }
 15019|             case a_state_check_and_wait_for_bgc:
 15020|             {
 15021|                 BOOL bgc_in_progress_p = FALSE;
 15022|                 BOOL did_full_compacting_gc = FALSE;
 15023|                 bgc_in_progress_p = check_and_wait_for_bgc (awr_loh_oos_bgc, &did_full_compacting_gc, true, &msl_status);
 15024|                 check_msl_status ("uoh a_state_check_and_wait_for_bgc", size);
 15025|                 uoh_alloc_state = (!bgc_in_progress_p ?
 15026|                                         a_state_trigger_full_compact_gc :
 15027|                                         (did_full_compacting_gc ?
 15028|                                             a_state_try_fit_after_cg :
 15029|                                             a_state_try_fit_after_bgc));
 15030|                 break;
 15031|             }
 15032|             case a_state_trigger_full_compact_gc:
 15033|             {
 15034|                 if (fgn_maxgen_percent)
 15035|                 {
 15036|                     dprintf (2, ("FGN: LOH doing last GC before we throw OOM"));
 15037|                     send_full_gc_notification (max_generation, FALSE);
 15038|                 }
 15039|                 BOOL got_full_compacting_gc = FALSE;
 15040|                 got_full_compacting_gc = trigger_full_compact_gc (gr, &oom_r, true, &msl_status);
 15041|                 check_msl_status ("uoh a_state_trigger_full_compact_gc", size);
 15042|                 uoh_alloc_state = (got_full_compacting_gc ? a_state_try_fit_after_cg : a_state_cant_allocate);
 15043|                 assert ((uoh_alloc_state != a_state_cant_allocate) || (oom_r != oom_no_failure));
 15044|                 break;
 15045|             }
 15046|             case a_state_check_retry_seg:
 15047|             {
 15048|                 BOOL should_retry_gc = retry_full_compact_gc (size);
 15049|                 BOOL should_retry_get_seg = FALSE;
 15050|                 if (!should_retry_gc)
 15051|                 {
 15052|                     size_t last_full_compact_gc_count = current_full_compact_gc_count;
 15053|                     current_full_compact_gc_count = get_full_compact_gc_count();
 15054|                     if (current_full_compact_gc_count > last_full_compact_gc_count)
 15055|                     {
 15056|                         should_retry_get_seg = TRUE;
 15057|                     }
 15058|                 }
 15059|                 uoh_alloc_state = (should_retry_gc ?
 15060|                                         a_state_trigger_full_compact_gc :
 15061|                                         (should_retry_get_seg ?
 15062|                                             a_state_try_fit_after_cg :
 15063|                                             a_state_cant_allocate));
 15064|                 assert ((uoh_alloc_state != a_state_cant_allocate) || (oom_r != oom_no_failure));
 15065|                 break;
 15066|             }
 15067|             default:
 15068|             {
 15069|                 assert (!"Invalid state!");
 15070|                 break;
 15071|             }
 15072|         }
 15073|     }
 15074| exit:
 15075|     if (uoh_alloc_state == a_state_cant_allocate)
 15076|     {
 15077|         assert (oom_r != oom_no_failure);
 15078|         if ((oom_r != oom_cant_commit) && should_retry_other_heap (gen_number, size))
 15079|         {
 15080|             uoh_alloc_state = a_state_retry_allocate;
 15081|         }
 15082|         else
 15083|         {
 15084|             handle_oom (oom_r,
 15085|                         size,
 15086|                         0,
 15087|                         0);
 15088|         }
 15089|         add_saved_spinlock_info (true, me_release, mt_alloc_large_cant, msl_entered);
 15090|         leave_spin_lock (&more_space_lock_uoh);
 15091|     }
 15092|     assert ((uoh_alloc_state == a_state_can_allocate) ||
 15093|             (uoh_alloc_state == a_state_cant_allocate) ||
 15094|             (uoh_alloc_state == a_state_retry_allocate));
 15095|     return uoh_alloc_state;
 15096| }
 15097| enter_msl_status gc_heap::trigger_gc_for_alloc (int gen_number, gc_reason gr,
 15098|                                     GCSpinLock* msl, bool loh_p,
 15099|                                     msl_take_state take_state)
 15100| {
 15101|     enter_msl_status msl_status = msl_entered;
 15102| #ifdef BACKGROUND_GC
 15103|     if (loh_p)
 15104|     {
 15105| #ifdef MULTIPLE_HEAPS
 15106| #ifdef STRESS_DYNAMIC_HEAP_COUNT
 15107|         uoh_msl_before_gc_p = true;
 15108| #endif //STRESS_DYNAMIC_HEAP_COUNT
 15109|         dprintf (5555, ("h%d uoh alloc before GC", heap_number));
 15110| #endif //MULTIPLE_HEAPS
 15111|         add_saved_spinlock_info (loh_p, me_release, take_state, msl_status);
 15112|         leave_spin_lock (msl);
 15113|     }
 15114| #endif //BACKGROUND_GC
 15115| #ifdef MULTIPLE_HEAPS
 15116|     if (!loh_p)
 15117|     {
 15118|         add_saved_spinlock_info (loh_p, me_release, take_state, msl_status);
 15119|         leave_spin_lock (msl);
 15120|     }
 15121| #endif //MULTIPLE_HEAPS
 15122|     vm_heap->GarbageCollectGeneration (gen_number, gr);
 15123| #ifdef MULTIPLE_HEAPS
 15124|     if (!loh_p)
 15125|     {
 15126|         msl_status = enter_spin_lock_msl (msl);
 15127|         add_saved_spinlock_info (loh_p, me_acquire, take_state, msl_status);
 15128|     }
 15129| #endif //MULTIPLE_HEAPS
 15130| #ifdef BACKGROUND_GC
 15131|     if (loh_p)
 15132|     {
 15133|         msl_status = enter_spin_lock_msl (msl);
 15134|         add_saved_spinlock_info (loh_p, me_acquire, take_state, msl_status);
 15135|     }
 15136| #endif //BACKGROUND_GC
 15137|     return msl_status;
 15138| }
 15139| inline
 15140| bool gc_heap::update_alloc_info (int gen_number, size_t allocated_size, size_t* etw_allocation_amount)
 15141| {
 15142|     bool exceeded_p = false;
 15143|     int oh_index = gen_to_oh (gen_number);
 15144|     allocated_since_last_gc[oh_index] += allocated_size;
 15145|     size_t& etw_allocated = etw_allocation_running_amount[oh_index];
 15146|     etw_allocated += allocated_size;
 15147|     if (etw_allocated > etw_allocation_tick)
 15148|     {
 15149|         *etw_allocation_amount = etw_allocated;
 15150|         exceeded_p = true;
 15151|         etw_allocated = 0;
 15152|     }
 15153|     return exceeded_p;
 15154| }
 15155| allocation_state gc_heap::try_allocate_more_space (alloc_context* acontext, size_t size,
 15156|                                     uint32_t flags, int gen_number)
 15157| {
 15158|     enter_msl_status msl_status = msl_entered;
 15159|     if (gc_heap::gc_started)
 15160|     {
 15161|         wait_for_gc_done();
 15162|         return a_state_retry_allocate;
 15163|     }
 15164|     bool loh_p = (gen_number > 0);
 15165|     GCSpinLock* msl = loh_p ? &more_space_lock_uoh : &more_space_lock_soh;
 15166| #ifdef SYNCHRONIZATION_STATS
 15167|     int64_t msl_acquire_start = GCToOSInterface::QueryPerformanceCounter();
 15168| #endif //SYNCHRONIZATION_STATS
 15169|     msl_status = enter_spin_lock_msl (msl);
 15170|     check_msl_status ("TAMS", size);
 15171|     add_saved_spinlock_info (loh_p, me_acquire, mt_try_alloc, msl_status);
 15172|     dprintf (SPINLOCK_LOG, ("[%d]Emsl for alloc", heap_number));
 15173| #ifdef SYNCHRONIZATION_STATS
 15174|     int64_t msl_acquire = GCToOSInterface::QueryPerformanceCounter() - msl_acquire_start;
 15175|     total_msl_acquire += msl_acquire;
 15176|     num_msl_acquired++;
 15177|     if (msl_acquire > 200)
 15178|     {
 15179|         num_high_msl_acquire++;
 15180|     }
 15181|     else
 15182|     {
 15183|         num_low_msl_acquire++;
 15184|     }
 15185| #endif //SYNCHRONIZATION_STATS
 15186|     /*
 15187|     if (gc_heap::gc_started)
 15188|     {
 15189| #ifdef SYNCHRONIZATION_STATS
 15190|         good_suspension++;
 15191| #endif //SYNCHRONIZATION_STATS
 15192|         BOOL fStress = (g_pConfig->GetGCStressLevel() & GCConfig::GCSTRESS_TRANSITION) != 0;
 15193|         if (!fStress)
 15194|         {
 15195|             wait_for_gc_done();
 15196| #ifdef MULTIPLE_HEAPS
 15197|             return -1;
 15198| #endif //MULTIPLE_HEAPS
 15199|         }
 15200|     }
 15201|     */
 15202|     dprintf (3, ("requested to allocate %zd bytes on gen%d", size, gen_number));
 15203|     int align_const = get_alignment_constant (gen_number <= max_generation);
 15204|     if (fgn_maxgen_percent)
 15205|     {
 15206|         check_for_full_gc (gen_number, size);
 15207|     }
 15208| #ifdef BGC_SERVO_TUNING
 15209|     if ((gen_number != 0) && bgc_tuning::should_trigger_bgc_loh())
 15210|     {
 15211|         msl_status = trigger_gc_for_alloc (max_generation, reason_bgc_tuning_loh, msl, loh_p, mt_try_servo_budget);
 15212|         if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
 15213|     }
 15214|     else
 15215| #endif //BGC_SERVO_TUNING
 15216|     {
 15217|         bool trigger_on_budget_loh_p =
 15218| #ifdef BGC_SERVO_TUNING
 15219|             !bgc_tuning::enable_fl_tuning;
 15220| #else
 15221|             true;
 15222| #endif //BGC_SERVO_TUNING
 15223|         bool check_budget_p = true;
 15224|         if (gen_number != 0)
 15225|         {
 15226|             check_budget_p = trigger_on_budget_loh_p;
 15227|         }
 15228|         if (check_budget_p && !(new_allocation_allowed (gen_number)))
 15229|         {
 15230|             if (fgn_maxgen_percent && (gen_number == 0))
 15231|             {
 15232|                 check_for_full_gc (gen_number, size);
 15233|             }
 15234| #ifdef BACKGROUND_GC
 15235|             bool recheck_p = wait_for_bgc_high_memory (awr_gen0_alloc, loh_p, &msl_status);
 15236|             if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
 15237| #endif //BACKGROUND_GC
 15238| #ifdef SYNCHRONIZATION_STATS
 15239|             bad_suspension++;
 15240| #endif //SYNCHRONIZATION_STATS
 15241|             dprintf (2, ("h%d running out of budget on gen%d, gc", heap_number, gen_number));
 15242| #ifdef BACKGROUND_GC
 15243|             bool trigger_gc_p = true;
 15244|             if (recheck_p)
 15245|                 trigger_gc_p = !(new_allocation_allowed (gen_number));
 15246|             if (trigger_gc_p)
 15247| #endif //BACKGROUND_GC
 15248|             {
 15249|                 if (!settings.concurrent || (gen_number == 0))
 15250|                 {
 15251|                     msl_status = trigger_gc_for_alloc (0, ((gen_number == 0) ? reason_alloc_soh : reason_alloc_loh),
 15252|                                                        msl, loh_p, mt_try_budget);
 15253|                     if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
 15254|                 }
 15255|             }
 15256|         }
 15257|     }
 15258|     allocation_state can_allocate = ((gen_number == 0) ?
 15259|         allocate_soh (gen_number, size, acontext, flags, align_const) :
 15260|         allocate_uoh (gen_number, size, acontext, flags, align_const));
 15261|     return can_allocate;
 15262| }
 15263| #ifdef MULTIPLE_HEAPS
 15264| void gc_heap::balance_heaps (alloc_context* acontext)
 15265| {
 15266|     if (acontext->alloc_count < 4)
 15267|     {
 15268|         if (acontext->alloc_count == 0)
 15269|         {
 15270|             int home_hp_num = heap_select::select_heap (acontext);
 15271|             acontext->set_home_heap (GCHeap::GetHeap (home_hp_num));
 15272|             gc_heap* hp = acontext->get_home_heap ()->pGenGCHeap;
 15273|             acontext->set_alloc_heap (acontext->get_home_heap ());
 15274|             hp->alloc_context_count++;
 15275| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15276|             uint16_t ideal_proc_no = 0;
 15277|             GCToOSInterface::GetCurrentThreadIdealProc (&ideal_proc_no);
 15278|             uint32_t proc_no = GCToOSInterface::GetCurrentProcessorNumber ();
 15279|             add_to_hb_numa (proc_no, ideal_proc_no,
 15280|                 home_hp_num, false, true, false);
 15281|             dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMPafter GC: 1st alloc on p%3d, h%d, ip: %d",
 15282|                 proc_no, home_hp_num, ideal_proc_no));
 15283| #endif //HEAP_BALANCE_INSTRUMENTATION
 15284|         }
 15285|     }
 15286|     else
 15287|     {
 15288|         BOOL set_home_heap = FALSE;
 15289|         gc_heap* home_hp = NULL;
 15290|         int proc_hp_num = 0;
 15291| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15292|         bool alloc_count_p = true;
 15293|         bool multiple_procs_p = false;
 15294|         bool set_ideal_p = false;
 15295|         uint32_t proc_no = GCToOSInterface::GetCurrentProcessorNumber ();
 15296|         uint32_t last_proc_no = proc_no;
 15297| #endif //HEAP_BALANCE_INSTRUMENTATION
 15298|         if (heap_select::can_find_heap_fast ())
 15299|         {
 15300|             assert (acontext->get_home_heap () != NULL);
 15301|             home_hp = acontext->get_home_heap ()->pGenGCHeap;
 15302|             proc_hp_num = heap_select::select_heap (acontext);
 15303|             if (home_hp != gc_heap::g_heaps[proc_hp_num])
 15304|             {
 15305| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15306|                 alloc_count_p = false;
 15307| #endif //HEAP_BALANCE_INSTRUMENTATION
 15308|                 set_home_heap = TRUE;
 15309|             }
 15310|             else if ((acontext->alloc_count & 15) == 0)
 15311|                 set_home_heap = TRUE;
 15312|         }
 15313|         else
 15314|         {
 15315|             if ((acontext->alloc_count & 3) == 0)
 15316|                 set_home_heap = TRUE;
 15317|         }
 15318|         if (set_home_heap)
 15319|         {
 15320|             /*
 15321|                         if (n_heaps > MAX_SUPPORTED_CPUS)
 15322|                         {
 15323|                             acontext->home_heap = GCHeap::GetHeap( heap_select::select_heap(acontext));
 15324|                             acontext->alloc_heap = acontext->home_heap;
 15325|                         }
 15326|                         else
 15327|             */
 15328|             {
 15329|                 gc_heap* org_hp = acontext->get_alloc_heap ()->pGenGCHeap;
 15330|                 int org_hp_num = org_hp->heap_number;
 15331|                 int final_alloc_hp_num = org_hp_num;
 15332|                 dynamic_data* dd = org_hp->dynamic_data_of (0);
 15333|                 ptrdiff_t org_size = dd_new_allocation (dd);
 15334|                 ptrdiff_t total_size = (ptrdiff_t)dd_desired_allocation (dd);
 15335| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15336|                 dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMP[p%3d] ph h%3d, hh: %3d, ah: %3d (%dmb-%dmb), ac: %5d(%s)",
 15337|                     proc_no, proc_hp_num, home_hp->heap_number,
 15338|                     org_hp_num, (total_size / 1024 / 1024), (org_size / 1024 / 1024),
 15339|                     acontext->alloc_count,
 15340|                     ((proc_hp_num == home_hp->heap_number) ? "AC" : "H")));
 15341| #endif //HEAP_BALANCE_INSTRUMENTATION
 15342|                 int org_alloc_context_count;
 15343|                 int max_alloc_context_count;
 15344|                 gc_heap* max_hp;
 15345|                 int max_hp_num = 0;
 15346|                 ptrdiff_t max_size;
 15347|                 size_t local_delta = max (((size_t)org_size >> 6), min_gen0_balance_delta);
 15348|                 size_t delta = local_delta;
 15349|                 if (((size_t)org_size + 2 * delta) >= (size_t)total_size)
 15350|                 {
 15351|                     acontext->alloc_count++;
 15352|                     return;
 15353|                 }
 15354| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15355|                 proc_no = GCToOSInterface::GetCurrentProcessorNumber ();
 15356|                 if (proc_no != last_proc_no)
 15357|                 {
 15358|                     dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMPSP: %d->%d", last_proc_no, proc_no));
 15359|                     multiple_procs_p = true;
 15360|                     last_proc_no = proc_no;
 15361|                 }
 15362|                 int new_home_hp_num = heap_select::proc_no_to_heap_no[proc_no];
 15363| #else
 15364|                 int new_home_hp_num = heap_select::select_heap(acontext);
 15365| #endif //HEAP_BALANCE_INSTRUMENTATION
 15366|                 gc_heap* new_home_hp = gc_heap::g_heaps[new_home_hp_num];
 15367|                 acontext->set_home_heap (new_home_hp->vm_heap);
 15368|                 int start, end, finish;
 15369|                 heap_select::get_heap_range_for_heap (new_home_hp_num, &start, &end);
 15370|                 finish = start + n_heaps;
 15371|                 do
 15372|                 {
 15373|                     max_hp = org_hp;
 15374|                     max_hp_num = org_hp_num;
 15375|                     max_size = org_size + delta;
 15376|                     org_alloc_context_count = org_hp->alloc_context_count;
 15377|                     max_alloc_context_count = org_alloc_context_count;
 15378|                     if (org_hp == new_home_hp)
 15379|                         max_size = max_size + delta;
 15380|                     if (max_alloc_context_count > 1)
 15381|                         max_size /= max_alloc_context_count;
 15382|                     if (org_hp != new_home_hp)
 15383|                     {
 15384|                         dd = new_home_hp->dynamic_data_of(0);
 15385|                         ptrdiff_t size = dd_new_allocation(dd);
 15386|                         size += delta * 2;
 15387|                         int new_home_hp_alloc_context_count = new_home_hp->alloc_context_count;
 15388|                         if (new_home_hp_alloc_context_count > 0)
 15389|                             size /= (new_home_hp_alloc_context_count + 1);
 15390|                         if (size > max_size)
 15391|                         {
 15392| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15393|                             dprintf(HEAP_BALANCE_TEMP_LOG, ("TEMPorg h%d(%dmb), m h%d(%dmb)",
 15394|                                 org_hp_num, (max_size / 1024 / 1024),
 15395|                                 new_home_hp_num, (size / 1024 / 1024)));
 15396| #endif //HEAP_BALANCE_INSTRUMENTATION
 15397|                             max_hp = new_home_hp;
 15398|                             max_size = size;
 15399|                             max_hp_num = new_home_hp_num;
 15400|                             max_alloc_context_count = new_home_hp_alloc_context_count;
 15401|                         }
 15402|                     }
 15403|                     enum
 15404|                     {
 15405|                         LOCAL_NUMA_NODE,
 15406|                         REMOTE_NUMA_NODE
 15407|                     };
 15408|                     for (int pass = LOCAL_NUMA_NODE; pass <= REMOTE_NUMA_NODE; pass++)
 15409|                     {
 15410|                         int count = end - start;
 15411|                         int max_tries = min(count, 4);
 15412|                         int heap_num = start + ((acontext->alloc_count >> 2) + new_home_hp_num) % count;
 15413| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15414|                         dprintf(HEAP_BALANCE_TEMP_LOG, ("TEMP starting at h%d (home_heap_num = %d, alloc_count = %d)", heap_num, new_home_hp_num, acontext->alloc_count));
 15415| #endif //HEAP_BALANCE_INSTRUMENTATION
 15416|                         for (int tries = max_tries; --tries >= 0; heap_num++)
 15417|                         {
 15418|                             if (heap_num >= end)
 15419|                                 heap_num -= count;
 15420|                             while (heap_num >= n_heaps)
 15421|                                 heap_num -= n_heaps;
 15422|                             assert (heap_num < n_heaps);
 15423|                             gc_heap* hp = gc_heap::g_heaps[heap_num];
 15424|                             dd = hp->dynamic_data_of(0);
 15425|                             ptrdiff_t size = dd_new_allocation(dd);
 15426| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15427|                             dprintf(HEAP_BALANCE_TEMP_LOG, ("TEMP looking at h%d(%dmb)",
 15428|                                 heap_num, (size / 1024 / 1024)));
 15429| #endif //HEAP_BALANCE_INSTRUMENTATION
 15430|                             if (size <= max_size)
 15431|                                 continue;
 15432|                             int hp_alloc_context_count = hp->alloc_context_count;
 15433|                             if (hp_alloc_context_count > 0)
 15434|                             {
 15435|                                 size /= (hp_alloc_context_count + 1);
 15436|                             }
 15437|                             if (size > max_size)
 15438|                             {
 15439| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15440|                                 dprintf(HEAP_BALANCE_TEMP_LOG, ("TEMPorg h%d(%dmb), m h%d(%dmb)",
 15441|                                     org_hp_num, (max_size / 1024 / 1024),
 15442|                                     hp->heap_number, (size / 1024 / 1024)));
 15443| #endif //HEAP_BALANCE_INSTRUMENTATION
 15444|                                 max_hp = hp;
 15445|                                 max_size = size;
 15446|                                 max_hp_num = max_hp->heap_number;
 15447|                                 max_alloc_context_count = hp_alloc_context_count;
 15448|                             }
 15449|                         }
 15450|                         if ((max_hp == org_hp) && (end < finish))
 15451|                         {
 15452|                             start = end; end = finish;
 15453|                             delta = local_delta * 2; // Make it twice as hard to balance to remote nodes on NUMA.
 15454|                         }
 15455|                         else
 15456|                         {
 15457|                             break;
 15458|                         }
 15459|                     }
 15460|                 }
 15461|                 while (org_alloc_context_count != org_hp->alloc_context_count ||
 15462|                        max_alloc_context_count != max_hp->alloc_context_count);
 15463| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15464|                 uint16_t ideal_proc_no_before_set_ideal = 0;
 15465|                 GCToOSInterface::GetCurrentThreadIdealProc (&ideal_proc_no_before_set_ideal);
 15466| #endif //HEAP_BALANCE_INSTRUMENTATION
 15467|                 if (max_hp != org_hp)
 15468|                 {
 15469|                     final_alloc_hp_num = max_hp->heap_number;
 15470|                     org_hp->alloc_context_count--;
 15471|                     max_hp->alloc_context_count++;
 15472|                     acontext->set_alloc_heap (GCHeap::GetHeap (final_alloc_hp_num));
 15473|                     if (!gc_thread_no_affinitize_p)
 15474|                     {
 15475|                         uint16_t src_proc_no = heap_select::find_proc_no_from_heap_no (org_hp->heap_number);
 15476|                         uint16_t dst_proc_no = heap_select::find_proc_no_from_heap_no (max_hp->heap_number);
 15477|                         dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMPSW! h%d(p%d)->h%d(p%d)",
 15478|                             org_hp_num, src_proc_no, final_alloc_hp_num, dst_proc_no));
 15479| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15480|                         int current_proc_no_before_set_ideal = GCToOSInterface::GetCurrentProcessorNumber ();
 15481|                         if (current_proc_no_before_set_ideal != last_proc_no)
 15482|                         {
 15483|                             dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMPSPa: %d->%d", last_proc_no, current_proc_no_before_set_ideal));
 15484|                             multiple_procs_p = true;
 15485|                         }
 15486| #endif //HEAP_BALANCE_INSTRUMENTATION
 15487|                         if (!GCToOSInterface::SetCurrentThreadIdealAffinity (src_proc_no, dst_proc_no))
 15488|                         {
 15489|                             dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMPFailed to set the ideal processor for heap %d %d->%d",
 15490|                                 org_hp->heap_number, (int)src_proc_no, (int)dst_proc_no));
 15491|                         }
 15492| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15493|                         else
 15494|                         {
 15495|                             set_ideal_p = true;
 15496|                         }
 15497| #endif //HEAP_BALANCE_INSTRUMENTATION
 15498|                     }
 15499|                 }
 15500| #ifdef HEAP_BALANCE_INSTRUMENTATION
 15501|                 add_to_hb_numa (proc_no, ideal_proc_no_before_set_ideal,
 15502|                     final_alloc_hp_num, multiple_procs_p, alloc_count_p, set_ideal_p);
 15503| #endif //HEAP_BALANCE_INSTRUMENTATION
 15504|             }
 15505|         }
 15506|     }
 15507|     acontext->alloc_count++;
 15508| }
 15509| ptrdiff_t gc_heap::get_balance_heaps_uoh_effective_budget (int generation_num)
 15510| {
 15511| #ifndef USE_REGIONS
 15512|     if (heap_hard_limit)
 15513|     {
 15514|         const ptrdiff_t free_list_space = generation_free_list_space (generation_of (generation_num));
 15515|         heap_segment* seg = generation_start_segment (generation_of (generation_num));
 15516|         assert (heap_segment_next (seg) == nullptr);
 15517|         const ptrdiff_t allocated = heap_segment_allocated (seg) - seg->mem;
 15518|         return free_list_space - allocated;
 15519|     }
 15520|     else
 15521| #endif // !USE_REGIONS
 15522|     {
 15523|         return dd_new_allocation (dynamic_data_of (generation_num));
 15524|     }
 15525| }
 15526| gc_heap* gc_heap::balance_heaps_uoh (alloc_context* acontext, size_t alloc_size, int generation_num)
 15527| {
 15528|     const int home_hp_num = heap_select::select_heap(acontext);
 15529|     dprintf (3, ("[h%d] LA: %zd", home_hp_num, alloc_size));
 15530|     gc_heap* home_hp = GCHeap::GetHeap(home_hp_num)->pGenGCHeap;
 15531|     dynamic_data* dd = home_hp->dynamic_data_of (generation_num);
 15532|     const ptrdiff_t home_hp_size = home_hp->get_balance_heaps_uoh_effective_budget (generation_num);
 15533|     size_t delta = dd_min_size (dd) / 2;
 15534|     int start, end;
 15535|     heap_select::get_heap_range_for_heap(home_hp_num, &start, &end);
 15536|     const int finish = start + n_heaps;
 15537| try_again:
 15538|     gc_heap* max_hp = home_hp;
 15539|     ptrdiff_t max_size = home_hp_size + delta;
 15540|     dprintf (3, ("home hp: %d, max size: %zd",
 15541|         home_hp_num,
 15542|         max_size));
 15543|     for (int i = start; i < end; i++)
 15544|     {
 15545|         gc_heap* hp = GCHeap::GetHeap(i%n_heaps)->pGenGCHeap;
 15546|         const ptrdiff_t size = hp->get_balance_heaps_uoh_effective_budget (generation_num);
 15547|         dprintf (3, ("hp: %d, size: %zd", hp->heap_number, size));
 15548|         if (size > max_size)
 15549|         {
 15550|             max_hp = hp;
 15551|             max_size = size;
 15552|             dprintf (3, ("max hp: %d, max size: %zd",
 15553|                 max_hp->heap_number,
 15554|                 max_size));
 15555|         }
 15556|     }
 15557|     if ((max_hp == home_hp) && (end < finish))
 15558|     {
 15559|         start = end; end = finish;
 15560|         delta = dd_min_size (dd) * 3 / 2; // Make it harder to balance to remote nodes on NUMA.
 15561|         goto try_again;
 15562|     }
 15563|     if (max_hp != home_hp)
 15564|     {
 15565|         dprintf (3, ("uoh: %d(%zd)->%d(%zd)",
 15566|             home_hp->heap_number, dd_new_allocation (home_hp->dynamic_data_of (generation_num)),
 15567|             max_hp->heap_number, dd_new_allocation (max_hp->dynamic_data_of (generation_num))));
 15568|     }
 15569|     return max_hp;
 15570| }
 15571| gc_heap* gc_heap::balance_heaps_uoh_hard_limit_retry (alloc_context* acontext, size_t alloc_size, int generation_num)
 15572| {
 15573|     assert (heap_hard_limit);
 15574| #ifdef USE_REGIONS
 15575|     return balance_heaps_uoh (acontext, alloc_size, generation_num);
 15576| #else //USE_REGIONS
 15577|     const int home_heap = heap_select::select_heap(acontext);
 15578|     dprintf (3, ("[h%d] balance_heaps_loh_hard_limit_retry alloc_size: %zd", home_heap, alloc_size));
 15579|     int start, end;
 15580|     heap_select::get_heap_range_for_heap (home_heap, &start, &end);
 15581|     const int finish = start + n_heaps;
 15582|     gc_heap* max_hp = nullptr;
 15583|     size_t max_end_of_seg_space = alloc_size; // Must be more than this much, or return NULL
 15584| try_again:
 15585|     {
 15586|         for (int i = start; i < end; i++)
 15587|         {
 15588|             gc_heap* hp = GCHeap::GetHeap (i%n_heaps)->pGenGCHeap;
 15589|             heap_segment* seg = generation_start_segment (hp->generation_of (generation_num));
 15590|             assert (heap_segment_next (seg) == nullptr);
 15591|             const size_t end_of_seg_space = heap_segment_reserved (seg) - heap_segment_allocated (seg);
 15592|             if (end_of_seg_space >= max_end_of_seg_space)
 15593|             {
 15594|                 dprintf (3, ("Switching heaps in hard_limit_retry! To: [h%d], New end_of_seg_space: %zd", hp->heap_number, end_of_seg_space));
 15595|                 max_end_of_seg_space = end_of_seg_space;
 15596|                 max_hp = hp;
 15597|             }
 15598|         }
 15599|     }
 15600|     if ((max_hp == nullptr) && (end < finish))
 15601|     {
 15602|         start = end; end = finish;
 15603|         goto try_again;
 15604|     }
 15605|     return max_hp;
 15606| #endif //USE_REGIONS
 15607| }
 15608| #endif //MULTIPLE_HEAPS
 15609| BOOL gc_heap::allocate_more_space(alloc_context* acontext, size_t size,
 15610|                                    uint32_t flags, int alloc_generation_number)
 15611| {
 15612|     allocation_state status = a_state_start;
 15613|     int retry_count = 0;
 15614|     gc_heap* saved_alloc_heap = 0;
 15615|     do
 15616|     {
 15617| #ifdef MULTIPLE_HEAPS
 15618|         if (alloc_generation_number == 0)
 15619|         {
 15620|             balance_heaps (acontext);
 15621|             status = acontext->get_alloc_heap ()->pGenGCHeap->try_allocate_more_space (acontext, size, flags, alloc_generation_number);
 15622|         }
 15623|         else
 15624|         {
 15625|             uint64_t start_us = GetHighPrecisionTimeStamp ();
 15626|             gc_heap* alloc_heap;
 15627|             if (heap_hard_limit && (status == a_state_retry_allocate))
 15628|             {
 15629|                 alloc_heap = balance_heaps_uoh_hard_limit_retry (acontext, size, alloc_generation_number);
 15630|                 if (alloc_heap == nullptr || (retry_count++ == UOH_ALLOCATION_RETRY_MAX_COUNT))
 15631|                 {
 15632|                     return false;
 15633|                 }
 15634|             }
 15635|             else
 15636|             {
 15637|                 alloc_heap = balance_heaps_uoh (acontext, size, alloc_generation_number);
 15638|                 dprintf (3, ("uoh alloc %Id on h%d", size, alloc_heap->heap_number));
 15639|                 saved_alloc_heap = alloc_heap;
 15640|             }
 15641|             bool alloced_on_retry = (status == a_state_retry_allocate);
 15642|             status = alloc_heap->try_allocate_more_space (acontext, size, flags, alloc_generation_number);
 15643|             dprintf (3, ("UOH h%d %Id returned from TAMS, s %d", alloc_heap->heap_number, size, status));
 15644|             uint64_t end_us = GetHighPrecisionTimeStamp ();
 15645|             if (status == a_state_retry_allocate)
 15646|             {
 15647|                 dprintf (5555, ("UOH h%d alloc %Id retry!", alloc_heap->heap_number, size));
 15648|             }
 15649|             else
 15650|             {
 15651|                 if (alloced_on_retry)
 15652|                 {
 15653|                     dprintf (5555, ("UOH h%d allocated %Id on retry (%I64dus)", alloc_heap->heap_number, size, (end_us - start_us)));
 15654|                 }
 15655|             }
 15656|         }
 15657| #else
 15658|         status = try_allocate_more_space (acontext, size, flags, alloc_generation_number);
 15659| #endif //MULTIPLE_HEAPS
 15660|     }
 15661|     while (status == a_state_retry_allocate);
 15662|     return (status == a_state_can_allocate);
 15663| }
 15664| inline
 15665| CObjectHeader* gc_heap::allocate (size_t jsize, alloc_context* acontext, uint32_t flags)
 15666| {
 15667|     size_t size = Align (jsize);
 15668|     assert (size >= Align (min_obj_size));
 15669|     {
 15670|     retry:
 15671|         uint8_t*  result = acontext->alloc_ptr;
 15672|         acontext->alloc_ptr+=size;
 15673|         if (acontext->alloc_ptr <= acontext->alloc_limit)
 15674|         {
 15675|             CObjectHeader* obj = (CObjectHeader*)result;
 15676|             assert (obj != 0);
 15677|             return obj;
 15678|         }
 15679|         else
 15680|         {
 15681|             acontext->alloc_ptr -= size;
 15682| #ifdef _MSC_VER
 15683| #pragma inline_depth(0)
 15684| #endif //_MSC_VER
 15685|             if (! allocate_more_space (acontext, size, flags, 0))
 15686|                 return 0;
 15687| #ifdef _MSC_VER
 15688| #pragma inline_depth(20)
 15689| #endif //_MSC_VER
 15690|             goto retry;
 15691|         }
 15692|     }
 15693| }
 15694| void  gc_heap::leave_allocation_segment (generation* gen)
 15695| {
 15696|     adjust_limit (0, 0, gen);
 15697| }
 15698| void gc_heap::init_free_and_plug()
 15699| {
 15700| #ifdef FREE_USAGE_STATS
 15701|     int i = (settings.concurrent ? max_generation : 0);
 15702|     for (; i <= settings.condemned_generation; i++)
 15703|     {
 15704|         generation* gen = generation_of (i);
 15705| #ifdef DOUBLY_LINKED_FL
 15706|         print_free_and_plug ("BGC");
 15707| #else
 15708|         memset (gen->gen_free_spaces, 0, sizeof (gen->gen_free_spaces));
 15709| #endif //DOUBLY_LINKED_FL
 15710|         memset (gen->gen_plugs, 0, sizeof (gen->gen_plugs));
 15711|         memset (gen->gen_current_pinned_free_spaces, 0, sizeof (gen->gen_current_pinned_free_spaces));
 15712|     }
 15713|     if (settings.condemned_generation != max_generation)
 15714|     {
 15715|         for (int i = (settings.condemned_generation + 1); i <= max_generation; i++)
 15716|         {
 15717|             generation* gen = generation_of (i);
 15718|             memset (gen->gen_plugs, 0, sizeof (gen->gen_plugs));
 15719|         }
 15720|     }
 15721| #endif //FREE_USAGE_STATS
 15722| }
 15723| void gc_heap::print_free_and_plug (const char* msg)
 15724| {
 15725| #ifdef FREE_USAGE_STATS
 15726|     int older_gen = ((settings.condemned_generation == max_generation) ? max_generation : (settings.condemned_generation + 1));
 15727|     for (int i = 0; i <= older_gen; i++)
 15728|     {
 15729|         generation* gen = generation_of (i);
 15730|         for (int j = 0; j < NUM_GEN_POWER2; j++)
 15731|         {
 15732|             if ((gen->gen_free_spaces[j] != 0) || (gen->gen_plugs[j] != 0))
 15733|             {
 15734|                 dprintf (2, ("[%s][h%d][%s#%d]gen%d: 2^%d: F: %zd, P: %zd",
 15735|                     msg,
 15736|                     heap_number,
 15737|                     (settings.concurrent ? "BGC" : "GC"),
 15738|                     settings.gc_index,
 15739|                     i,
 15740|                     (j + 9), gen->gen_free_spaces[j], gen->gen_plugs[j]));
 15741|             }
 15742|         }
 15743|     }
 15744| #else
 15745|     UNREFERENCED_PARAMETER(msg);
 15746| #endif //FREE_USAGE_STATS
 15747| }
 15748| int gc_heap::find_bucket (size_t size)
 15749| {
 15750|     size_t sz = BASE_GEN_SIZE;
 15751|     int i = 0;
 15752|     for (; i < (NUM_GEN_POWER2 - 1); i++)
 15753|     {
 15754|         if (size < sz)
 15755|         {
 15756|             break;
 15757|         }
 15758|         sz = sz * 2;
 15759|     }
 15760|     return i;
 15761| }
 15762| void gc_heap::add_gen_plug (int gen_number, size_t plug_size)
 15763| {
 15764| #ifdef FREE_USAGE_STATS
 15765|     dprintf (3, ("adding plug size %zd to gen%d", plug_size, gen_number));
 15766|     generation* gen = generation_of (gen_number);
 15767|     size_t sz = BASE_GEN_SIZE;
 15768|     int i = find_bucket (plug_size);
 15769|     (gen->gen_plugs[i])++;
 15770| #else
 15771|     UNREFERENCED_PARAMETER(gen_number);
 15772|     UNREFERENCED_PARAMETER(plug_size);
 15773| #endif //FREE_USAGE_STATS
 15774| }
 15775| void gc_heap::add_item_to_current_pinned_free (int gen_number, size_t free_size)
 15776| {
 15777| #ifdef FREE_USAGE_STATS
 15778|     generation* gen = generation_of (gen_number);
 15779|     size_t sz = BASE_GEN_SIZE;
 15780|     int i = find_bucket (free_size);
 15781|     (gen->gen_current_pinned_free_spaces[i])++;
 15782|     generation_pinned_free_obj_space (gen) += free_size;
 15783|     dprintf (3, ("left pin free %zd(2^%d) to gen%d, total %zd bytes (%zd)",
 15784|         free_size, (i + 10), gen_number,
 15785|         generation_pinned_free_obj_space (gen),
 15786|         gen->gen_current_pinned_free_spaces[i]));
 15787| #else
 15788|     UNREFERENCED_PARAMETER(gen_number);
 15789|     UNREFERENCED_PARAMETER(free_size);
 15790| #endif //FREE_USAGE_STATS
 15791| }
 15792| void gc_heap::add_gen_free (int gen_number, size_t free_size)
 15793| {
 15794| #ifdef FREE_USAGE_STATS
 15795|     dprintf (3, ("adding free size %zd to gen%d", free_size, gen_number));
 15796|     if (free_size < min_free_list)
 15797|         return;
 15798|     generation* gen = generation_of (gen_number);
 15799|     size_t sz = BASE_GEN_SIZE;
 15800|     int i = find_bucket (free_size);
 15801|     (gen->gen_free_spaces[i])++;
 15802|     if (gen_number == max_generation)
 15803|     {
 15804|         dprintf (3, ("Mb b%d: f+ %zd (%zd)",
 15805|             i, free_size, gen->gen_free_spaces[i]));
 15806|     }
 15807| #else
 15808|     UNREFERENCED_PARAMETER(gen_number);
 15809|     UNREFERENCED_PARAMETER(free_size);
 15810| #endif //FREE_USAGE_STATS
 15811| }
 15812| void gc_heap::remove_gen_free (int gen_number, size_t free_size)
 15813| {
 15814| #ifdef FREE_USAGE_STATS
 15815|     dprintf (3, ("removing free %zd from gen%d", free_size, gen_number));
 15816|     if (free_size < min_free_list)
 15817|         return;
 15818|     generation* gen = generation_of (gen_number);
 15819|     size_t sz = BASE_GEN_SIZE;
 15820|     int i = find_bucket (free_size);
 15821|     (gen->gen_free_spaces[i])--;
 15822|     if (gen_number == max_generation)
 15823|     {
 15824|         dprintf (3, ("Mb b%d: f- %zd (%zd)",
 15825|             i, free_size, gen->gen_free_spaces[i]));
 15826|     }
 15827| #else
 15828|     UNREFERENCED_PARAMETER(gen_number);
 15829|     UNREFERENCED_PARAMETER(free_size);
 15830| #endif //FREE_USAGE_STATS
 15831| }
 15832| #ifdef DOUBLY_LINKED_FL
 15833| BOOL gc_heap::should_set_bgc_mark_bit (uint8_t* o)
 15834| {
 15835|     if (!current_sweep_seg)
 15836|     {
 15837|         assert (current_bgc_state == bgc_not_in_process);
 15838|         return FALSE;
 15839|     }
 15840|     if (in_range_for_segment (o, current_sweep_seg))
 15841|     {
 15842|         if ((o >= current_sweep_pos) && (o < heap_segment_background_allocated (current_sweep_seg)))
 15843|         {
 15844| #ifndef USE_REGIONS
 15845|             if (current_sweep_seg == saved_sweep_ephemeral_seg)
 15846|             {
 15847|                 return (o < saved_sweep_ephemeral_start);
 15848|             }
 15849|             else
 15850| #endif //!USE_REGIONS
 15851|             {
 15852|                 return TRUE;
 15853|             }
 15854|         }
 15855|         else
 15856|             return FALSE;
 15857|     }
 15858|     else
 15859|     {
 15860|         if ((o >= background_saved_lowest_address) && (o < background_saved_highest_address))
 15861|         {
 15862|             heap_segment* seg = seg_mapping_table_segment_of (o);
 15863|             uint8_t* background_allocated = heap_segment_background_allocated (seg);
 15864|             if (background_allocated == 0)
 15865|                 return FALSE;
 15866|             else if (o >= background_allocated)
 15867|                 return FALSE;
 15868|             else
 15869|                 return (!heap_segment_swept_p (seg));
 15870|         }
 15871|         else
 15872|             return FALSE;
 15873|     }
 15874| }
 15875| #endif //DOUBLY_LINKED_FL
 15876| uint8_t* gc_heap::allocate_in_older_generation (generation* gen, size_t size,
 15877|                                                 int from_gen_number,
 15878|                                                 uint8_t* old_loc REQD_ALIGN_AND_OFFSET_DCL)
 15879| {
 15880|     size = Align (size);
 15881|     assert (size >= Align (min_obj_size));
 15882|     assert (from_gen_number < max_generation);
 15883|     assert (from_gen_number >= 0);
 15884|     assert (generation_of (from_gen_number + 1) == gen);
 15885| #ifdef DOUBLY_LINKED_FL
 15886|     BOOL consider_bgc_mark_p        = FALSE;
 15887|     BOOL check_current_sweep_p      = FALSE;
 15888|     BOOL check_saved_sweep_p        = FALSE;
 15889|     BOOL try_added_list_p       = (gen->gen_num == max_generation);
 15890|     BOOL record_free_list_allocated_p = ((gen->gen_num == max_generation) &&
 15891|                                          (current_c_gc_state == c_gc_state_planning));
 15892| #endif //DOUBLY_LINKED_FL
 15893|     allocator* gen_allocator = generation_allocator (gen);
 15894|     BOOL discard_p = gen_allocator->discard_if_no_fit_p ();
 15895| #ifdef SHORT_PLUGS
 15896|     int pad_in_front = ((old_loc != 0) && ((from_gen_number+1) != max_generation)) ? USE_PADDING_FRONT : 0;
 15897| #else //SHORT_PLUGS
 15898|     int pad_in_front = 0;
 15899| #endif //SHORT_PLUGS
 15900|     size_t real_size = size + Align (min_obj_size);
 15901|     if (pad_in_front)
 15902|         real_size += Align (min_obj_size);
 15903| #ifdef RESPECT_LARGE_ALIGNMENT
 15904|     real_size += switch_alignment_size (pad_in_front);
 15905| #endif //RESPECT_LARGE_ALIGNMENT
 15906|     if (! (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, generation_allocation_pointer (gen),
 15907|                        generation_allocation_limit (gen), old_loc, USE_PADDING_TAIL | pad_in_front)))
 15908|     {
 15909|         for (unsigned int a_l_idx = gen_allocator->first_suitable_bucket(real_size * 2);
 15910|              a_l_idx < gen_allocator->number_of_buckets(); a_l_idx++)
 15911|         {
 15912|             uint8_t* free_list = 0;
 15913|             uint8_t* prev_free_item = 0;
 15914|             BOOL use_undo_p = !discard_p;
 15915| #ifdef DOUBLY_LINKED_FL
 15916|             if (a_l_idx == 0)
 15917|             {
 15918|                 use_undo_p = FALSE;
 15919|             }
 15920|             if (try_added_list_p)
 15921|             {
 15922|                 free_list = gen_allocator->added_alloc_list_head_of (a_l_idx);
 15923|                 while (free_list != 0)
 15924|                 {
 15925|                     dprintf (3, ("considering free list in added list%zx", (size_t)free_list));
 15926|                     size_t free_list_size = unused_array_size (free_list);
 15927|                     if (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, free_list, (free_list + free_list_size),
 15928|                                     old_loc, USE_PADDING_TAIL | pad_in_front))
 15929|                     {
 15930|                         dprintf (4, ("F:%zx-%zd",
 15931|                                     (size_t)free_list, free_list_size));
 15932|                         gen_allocator->unlink_item_no_undo_added (a_l_idx, free_list, prev_free_item);
 15933|                         generation_free_list_space (gen) -= free_list_size;
 15934|                         assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
 15935|                         remove_gen_free (gen->gen_num, free_list_size);
 15936|                         if (record_free_list_allocated_p)
 15937|                         {
 15938|                             generation_set_bgc_mark_bit_p (gen) = should_set_bgc_mark_bit (free_list);
 15939|                             dprintf (3333, ("SFA: %p->%p(%d)", free_list, (free_list + free_list_size),
 15940|                                 (generation_set_bgc_mark_bit_p (gen) ? 1 : 0)));
 15941|                         }
 15942|                         adjust_limit (free_list, free_list_size, gen);
 15943|                         generation_allocate_end_seg_p (gen) = FALSE;
 15944|                         goto finished;
 15945|                     }
 15946|                     else if (a_l_idx == 0)
 15947|                     {
 15948|                         dprintf (3, ("couldn't use this free area, discarding"));
 15949|                         generation_free_obj_space (gen) += free_list_size;
 15950|                         gen_allocator->unlink_item_no_undo_added (a_l_idx, free_list, prev_free_item);
 15951|                         generation_free_list_space (gen) -= free_list_size;
 15952|                         assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
 15953|                         remove_gen_free (gen->gen_num, free_list_size);
 15954|                     }
 15955|                     else
 15956|                     {
 15957|                         prev_free_item = free_list;
 15958|                     }
 15959|                     free_list = free_list_slot (free_list);
 15960|                 }
 15961|             }
 15962| #endif //DOUBLY_LINKED_FL
 15963|             free_list = gen_allocator->alloc_list_head_of (a_l_idx);
 15964|             prev_free_item = 0;
 15965|             while (free_list != 0)
 15966|             {
 15967|                 dprintf (3, ("considering free list %zx", (size_t)free_list));
 15968|                 size_t free_list_size = unused_array_size (free_list);
 15969|                 if (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, free_list, (free_list + free_list_size),
 15970|                                 old_loc, USE_PADDING_TAIL | pad_in_front))
 15971|                 {
 15972|                     dprintf (4, ("F:%zx-%zd",
 15973|                                     (size_t)free_list, free_list_size));
 15974|                     gen_allocator->unlink_item (a_l_idx, free_list, prev_free_item, use_undo_p);
 15975|                     generation_free_list_space (gen) -= free_list_size;
 15976|                     assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
 15977|                     remove_gen_free (gen->gen_num, free_list_size);
 15978| #ifdef DOUBLY_LINKED_FL
 15979|                     if (!discard_p && !use_undo_p)
 15980|                     {
 15981|                         gen2_removed_no_undo += free_list_size;
 15982|                         dprintf (3, ("h%d: remove with no undo %zd = %zd",
 15983|                             heap_number, free_list_size, gen2_removed_no_undo));
 15984|                     }
 15985|                     if (record_free_list_allocated_p)
 15986|                     {
 15987|                         generation_set_bgc_mark_bit_p (gen) = should_set_bgc_mark_bit (free_list);
 15988|                         dprintf (3333, ("SF: %p(%d)", free_list, (generation_set_bgc_mark_bit_p (gen) ? 1 : 0)));
 15989|                     }
 15990| #endif //DOUBLY_LINKED_FL
 15991|                     adjust_limit (free_list, free_list_size, gen);
 15992|                     generation_allocate_end_seg_p (gen) = FALSE;
 15993|                     goto finished;
 15994|                 }
 15995|                 else if (discard_p || (a_l_idx == 0))
 15996|                 {
 15997|                     dprintf (3, ("couldn't use this free area, discarding"));
 15998|                     generation_free_obj_space (gen) += free_list_size;
 15999|                     gen_allocator->unlink_item (a_l_idx, free_list, prev_free_item, FALSE);
 16000|                     generation_free_list_space (gen) -= free_list_size;
 16001|                     assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
 16002|                     remove_gen_free (gen->gen_num, free_list_size);
 16003| #ifdef DOUBLY_LINKED_FL
 16004|                     if (!discard_p)
 16005|                     {
 16006|                         gen2_removed_no_undo += free_list_size;
 16007|                         dprintf (3, ("h%d: b0 remove with no undo %zd = %zd",
 16008|                             heap_number, free_list_size, gen2_removed_no_undo));
 16009|                     }
 16010| #endif //DOUBLY_LINKED_FL
 16011|                 }
 16012|                 else
 16013|                 {
 16014|                     prev_free_item = free_list;
 16015|                 }
 16016|                 free_list = free_list_slot (free_list);
 16017|             }
 16018|         }
 16019| #ifdef USE_REGIONS
 16020|         heap_segment* seg = generation_allocation_segment (gen);
 16021|         dprintf (3, ("end of seg, starting from alloc seg %p", heap_segment_mem (seg)));
 16022|         assert (seg != ephemeral_heap_segment);
 16023|         while (true)
 16024| #else
 16025|         heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 16026|         if (seg != generation_allocation_segment (gen))
 16027|         {
 16028|             leave_allocation_segment (gen);
 16029|             generation_allocation_segment (gen) = seg;
 16030|         }
 16031|         while (seg != ephemeral_heap_segment)
 16032| #endif //USE_REGIONS
 16033|         {
 16034|             if (size_fit_p(size REQD_ALIGN_AND_OFFSET_ARG, heap_segment_plan_allocated (seg),
 16035|                            heap_segment_committed (seg), old_loc, USE_PADDING_TAIL | pad_in_front))
 16036|             {
 16037|                 adjust_limit (heap_segment_plan_allocated (seg),
 16038|                               (heap_segment_committed (seg) - heap_segment_plan_allocated (seg)),
 16039|                               gen);
 16040|                 generation_allocate_end_seg_p (gen) = TRUE;
 16041|                 heap_segment_plan_allocated (seg) =
 16042|                     heap_segment_committed (seg);
 16043|                 dprintf (3, ("seg %p is used for end of seg alloc", heap_segment_mem (seg)));
 16044|                 goto finished;
 16045|             }
 16046|             else
 16047|             {
 16048|                 if (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, heap_segment_plan_allocated (seg),
 16049|                                 heap_segment_reserved (seg), old_loc, USE_PADDING_TAIL | pad_in_front) &&
 16050|                     grow_heap_segment (seg, heap_segment_plan_allocated (seg), old_loc, size, pad_in_front REQD_ALIGN_AND_OFFSET_ARG))
 16051|                 {
 16052|                     adjust_limit (heap_segment_plan_allocated (seg),
 16053|                                   (heap_segment_committed (seg) - heap_segment_plan_allocated (seg)),
 16054|                                   gen);
 16055|                     generation_allocate_end_seg_p (gen) = TRUE;
 16056|                     heap_segment_plan_allocated (seg) =
 16057|                         heap_segment_committed (seg);
 16058|                     dprintf (3, ("seg %p is used for end of seg alloc after grow, %p",
 16059|                         heap_segment_mem (seg), heap_segment_committed (seg)));
 16060|                     goto finished;
 16061|                 }
 16062|                 else
 16063|                 {
 16064|                     leave_allocation_segment (gen);
 16065|                     heap_segment*   next_seg = heap_segment_next_rw (seg);
 16066| #ifdef USE_REGIONS
 16067|                     assert (next_seg != ephemeral_heap_segment);
 16068| #endif //USE_REGIONS
 16069|                     if (next_seg)
 16070|                     {
 16071|                         generation_allocation_segment (gen) = next_seg;
 16072|                         generation_allocation_pointer (gen) = heap_segment_mem (next_seg);
 16073|                         generation_allocation_limit (gen) = generation_allocation_pointer (gen);
 16074|                         dprintf (3, ("alloc region advanced to %p", heap_segment_mem (next_seg)));
 16075|                     }
 16076|                     else
 16077|                     {
 16078|                         size = 0;
 16079|                         goto finished;
 16080|                     }
 16081|                 }
 16082|             }
 16083|             seg = generation_allocation_segment (gen);
 16084|         }
 16085|         size = 0;
 16086|         goto finished;
 16087|     }
 16088| finished:
 16089|     if (0 == size)
 16090|     {
 16091|         return 0;
 16092|     }
 16093|     else
 16094|     {
 16095|         uint8_t*  result = generation_allocation_pointer (gen);
 16096|         size_t pad = 0;
 16097| #ifdef SHORT_PLUGS
 16098|         if ((pad_in_front & USE_PADDING_FRONT) &&
 16099|             (((generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen))==0) ||
 16100|              ((generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen))>=DESIRED_PLUG_LENGTH)))
 16101|         {
 16102|             pad = Align (min_obj_size);
 16103|             set_plug_padded (old_loc);
 16104|         }
 16105| #endif //SHORT_PLUGS
 16106| #ifdef FEATURE_STRUCTALIGN
 16107|         _ASSERTE(!old_loc || alignmentOffset != 0);
 16108|         _ASSERTE(old_loc || requiredAlignment == DATA_ALIGNMENT);
 16109|         if (old_loc != 0)
 16110|         {
 16111|             size_t pad1 = ComputeStructAlignPad(result+pad, requiredAlignment, alignmentOffset);
 16112|             set_node_aligninfo (old_loc, requiredAlignment, pad1);
 16113|             pad += pad1;
 16114|         }
 16115| #else // FEATURE_STRUCTALIGN
 16116|         if (!((old_loc == 0) || same_large_alignment_p (old_loc, result+pad)))
 16117|         {
 16118|             pad += switch_alignment_size (pad != 0);
 16119|             set_node_realigned (old_loc);
 16120|             dprintf (3, ("Allocation realignment old_loc: %zx, new_loc:%zx",
 16121|                          (size_t)old_loc, (size_t)(result+pad)));
 16122|             assert (same_large_alignment_p (result + pad, old_loc));
 16123|         }
 16124| #endif // FEATURE_STRUCTALIGN
 16125|         dprintf (3, ("Allocate %zd bytes", size));
 16126|         if ((old_loc == 0) || (pad != 0))
 16127|         {
 16128|             generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
 16129|         }
 16130|         generation_allocation_pointer (gen) += size + pad;
 16131|         assert (generation_allocation_pointer (gen) <= generation_allocation_limit (gen));
 16132|         generation_free_obj_space (gen) += pad;
 16133|         if (generation_allocate_end_seg_p (gen))
 16134|         {
 16135|             generation_end_seg_allocated (gen) += size;
 16136|         }
 16137|         else
 16138|         {
 16139| #ifdef DOUBLY_LINKED_FL
 16140|             if (generation_set_bgc_mark_bit_p (gen))
 16141|             {
 16142|                 dprintf (2, ("IOM: %p(->%p(%zd) (%zx-%zx)", old_loc, result, pad,
 16143|                         (size_t)(&mark_array [mark_word_of (result)]),
 16144|                         (size_t)(mark_array [mark_word_of (result)])));
 16145|                 set_plug_bgc_mark_bit (old_loc);
 16146|             }
 16147|             generation_last_free_list_allocated (gen) = old_loc;
 16148| #endif //DOUBLY_LINKED_FL
 16149|             generation_free_list_allocated (gen) += size;
 16150|         }
 16151|         generation_allocation_size (gen) += size;
 16152|         dprintf (3, ("aio: ptr: %p, limit: %p, sr: %p",
 16153|             generation_allocation_pointer (gen), generation_allocation_limit (gen),
 16154|             generation_allocation_context_start_region (gen)));
 16155|         return (result + pad);
 16156|     }
 16157| }
 16158| #ifndef USE_REGIONS
 16159| void gc_heap::repair_allocation_in_expanded_heap (generation* consing_gen)
 16160| {
 16161|     int  gen_number = max_generation - 1;
 16162|     while (gen_number>= 0)
 16163|     {
 16164|         generation* gen = generation_of (gen_number);
 16165|         if (0 == generation_plan_allocation_start (gen))
 16166|         {
 16167|             realloc_plan_generation_start (gen, consing_gen);
 16168|             assert (generation_plan_allocation_start (gen));
 16169|         }
 16170|         gen_number--;
 16171|     }
 16172|     size_t  size = (generation_allocation_limit (consing_gen) - generation_allocation_pointer (consing_gen));
 16173|     heap_segment* seg = generation_allocation_segment (consing_gen);
 16174|     if (generation_allocation_limit (consing_gen) == heap_segment_plan_allocated (seg))
 16175|     {
 16176|         if (size != 0)
 16177|         {
 16178|             heap_segment_plan_allocated (seg) = generation_allocation_pointer (consing_gen);
 16179|         }
 16180|     }
 16181|     else
 16182|     {
 16183|         assert (settings.condemned_generation == max_generation);
 16184|         uint8_t* first_address = generation_allocation_limit (consing_gen);
 16185|         size_t mi = 0;
 16186|         mark* m = 0;
 16187|         while (mi != mark_stack_tos)
 16188|         {
 16189|             m = pinned_plug_of (mi);
 16190|             if ((pinned_plug (m) == first_address))
 16191|                 break;
 16192|             else
 16193|                 mi++;
 16194|         }
 16195|         assert (mi != mark_stack_tos);
 16196|         pinned_len (m) = size;
 16197|     }
 16198| }
 16199| uint8_t* gc_heap::allocate_in_expanded_heap (generation* gen,
 16200|                                           size_t size,
 16201|                                           BOOL& adjacentp,
 16202|                                           uint8_t* old_loc,
 16203| #ifdef SHORT_PLUGS
 16204|                                           BOOL set_padding_on_saved_p,
 16205|                                           mark* pinned_plug_entry,
 16206| #endif //SHORT_PLUGS
 16207|                                           BOOL consider_bestfit,
 16208|                                           int active_new_gen_number
 16209|                                           REQD_ALIGN_AND_OFFSET_DCL)
 16210| {
 16211|     dprintf (3, ("aie: P: %p, size: %zx", old_loc, size));
 16212|     size = Align (size);
 16213|     assert (size >= Align (min_obj_size));
 16214| #ifdef SHORT_PLUGS
 16215|     int pad_in_front = ((old_loc != 0) && (active_new_gen_number != max_generation)) ? USE_PADDING_FRONT : 0;
 16216| #else //SHORT_PLUGS
 16217|     int pad_in_front = 0;
 16218| #endif //SHORT_PLUGS
 16219|     if (consider_bestfit && use_bestfit)
 16220|     {
 16221|         assert (bestfit_seg);
 16222|         dprintf (SEG_REUSE_LOG_1, ("reallocating 0x%p in expanded heap, size: %zd",
 16223|                     old_loc, size));
 16224|         return bestfit_seg->fit (old_loc,
 16225|                                  size REQD_ALIGN_AND_OFFSET_ARG);
 16226|     }
 16227|     heap_segment* seg = generation_allocation_segment (gen);
 16228|     if (! (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, generation_allocation_pointer (gen),
 16229|                        generation_allocation_limit (gen), old_loc,
 16230|                        ((generation_allocation_limit (gen) !=
 16231|                           heap_segment_plan_allocated (seg))? USE_PADDING_TAIL : 0) | pad_in_front)))
 16232|     {
 16233|         dprintf (3, ("aie: can't fit: ptr: %p, limit: %p", generation_allocation_pointer (gen),
 16234|             generation_allocation_limit (gen)));
 16235|         adjacentp = FALSE;
 16236|         uint8_t* first_address = (generation_allocation_limit (gen) ?
 16237|                                generation_allocation_limit (gen) :
 16238|                                heap_segment_mem (seg));
 16239|         assert (in_range_for_segment (first_address, seg));
 16240|         uint8_t* end_address   = heap_segment_reserved (seg);
 16241|         dprintf (3, ("aie: first_addr: %p, gen alloc limit: %p, end_address: %p",
 16242|             first_address, generation_allocation_limit (gen), end_address));
 16243|         size_t mi = 0;
 16244|         mark* m = 0;
 16245|         if (heap_segment_allocated (seg) != heap_segment_mem (seg))
 16246|         {
 16247|             assert (settings.condemned_generation == max_generation);
 16248|             while (mi != mark_stack_tos)
 16249|             {
 16250|                 m = pinned_plug_of (mi);
 16251|                 if ((pinned_plug (m) >= first_address) && (pinned_plug (m) < end_address))
 16252|                 {
 16253|                     dprintf (3, ("aie: found pin: %p", pinned_plug (m)));
 16254|                     break;
 16255|                 }
 16256|                 else
 16257|                     mi++;
 16258|             }
 16259|             if (mi != mark_stack_tos)
 16260|             {
 16261|                 size_t  hsize = (generation_allocation_limit (gen) - generation_allocation_pointer (gen));
 16262|                 {
 16263|                     dprintf(3,("gc filling up hole"));
 16264|                     ptrdiff_t mi1 = (ptrdiff_t)mi;
 16265|                     while ((mi1 >= 0) &&
 16266|                            (pinned_plug (pinned_plug_of(mi1)) != generation_allocation_limit (gen)))
 16267|                     {
 16268|                         dprintf (3, ("aie: checking pin %p", pinned_plug (pinned_plug_of(mi1))));
 16269|                         mi1--;
 16270|                     }
 16271|                     if (mi1 >= 0)
 16272|                     {
 16273|                         size_t saved_pinned_len = pinned_len (pinned_plug_of(mi1));
 16274|                         pinned_len (pinned_plug_of(mi1)) = hsize;
 16275|                         dprintf (3, ("changing %p len %zx->%zx",
 16276|                             pinned_plug (pinned_plug_of(mi1)),
 16277|                             saved_pinned_len, pinned_len (pinned_plug_of(mi1))));
 16278|                     }
 16279|                 }
 16280|             }
 16281|         }
 16282|         else
 16283|         {
 16284|             assert (generation_allocation_limit (gen) ==
 16285|                     generation_allocation_pointer (gen));
 16286|             mi = mark_stack_tos;
 16287|         }
 16288|         while ((mi != mark_stack_tos) && in_range_for_segment (pinned_plug (m), seg))
 16289|         {
 16290|             size_t len = pinned_len (m);
 16291|             uint8_t*  free_list = (pinned_plug (m) - len);
 16292|             dprintf (3, ("aie: testing free item: %p->%p(%zx)",
 16293|                 free_list, (free_list + len), len));
 16294|             if (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, free_list, (free_list + len), old_loc, USE_PADDING_TAIL | pad_in_front))
 16295|             {
 16296|                 dprintf (3, ("aie: Found adequate unused area: %zx, size: %zd",
 16297|                             (size_t)free_list, len));
 16298|                 {
 16299|                     generation_allocation_pointer (gen) = free_list;
 16300|                     generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
 16301|                     generation_allocation_limit (gen) = (free_list + len);
 16302|                 }
 16303|                 goto allocate_in_free;
 16304|             }
 16305|             mi++;
 16306|             m = pinned_plug_of (mi);
 16307|         }
 16308|         generation_allocation_pointer (gen) = heap_segment_plan_allocated (seg);
 16309|         generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
 16310|         heap_segment_plan_allocated (seg) = heap_segment_committed (seg);
 16311|         generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
 16312|         dprintf (3, ("aie: switching to end of seg: %p->%p(%zx)",
 16313|             generation_allocation_pointer (gen), generation_allocation_limit (gen),
 16314|             (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
 16315|         if (!size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, generation_allocation_pointer (gen),
 16316|                          generation_allocation_limit (gen), old_loc, USE_PADDING_TAIL | pad_in_front))
 16317|         {
 16318|             dprintf (3, ("aie: ptr: %p, limit: %p, can't alloc", generation_allocation_pointer (gen),
 16319|                 generation_allocation_limit (gen)));
 16320|             assert (!"Can't allocate if no free space");
 16321|             return 0;
 16322|         }
 16323|     }
 16324|     else
 16325|     {
 16326|         adjacentp = TRUE;
 16327|     }
 16328| allocate_in_free:
 16329|     {
 16330|         uint8_t*  result = generation_allocation_pointer (gen);
 16331|         size_t pad = 0;
 16332| #ifdef SHORT_PLUGS
 16333|         if ((pad_in_front & USE_PADDING_FRONT) &&
 16334|             (((generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen))==0) ||
 16335|              ((generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen))>=DESIRED_PLUG_LENGTH)))
 16336|         {
 16337|             pad = Align (min_obj_size);
 16338|             set_padding_in_expand (old_loc, set_padding_on_saved_p, pinned_plug_entry);
 16339|         }
 16340| #endif //SHORT_PLUGS
 16341| #ifdef FEATURE_STRUCTALIGN
 16342|         _ASSERTE(!old_loc || alignmentOffset != 0);
 16343|         _ASSERTE(old_loc || requiredAlignment == DATA_ALIGNMENT);
 16344|         if (old_loc != 0)
 16345|         {
 16346|             size_t pad1 = ComputeStructAlignPad(result+pad, requiredAlignment, alignmentOffset);
 16347|             set_node_aligninfo (old_loc, requiredAlignment, pad1);
 16348|             pad += pad1;
 16349|             adjacentp = FALSE;
 16350|         }
 16351| #else // FEATURE_STRUCTALIGN
 16352|         if (!((old_loc == 0) || same_large_alignment_p (old_loc, result+pad)))
 16353|         {
 16354|             pad += switch_alignment_size (pad != 0);
 16355|             set_node_realigned (old_loc);
 16356|             dprintf (3, ("Allocation realignment old_loc: %zx, new_loc:%zx",
 16357|                          (size_t)old_loc, (size_t)(result+pad)));
 16358|             assert (same_large_alignment_p (result + pad, old_loc));
 16359|             adjacentp = FALSE;
 16360|         }
 16361| #endif // FEATURE_STRUCTALIGN
 16362|         if ((old_loc == 0) || (pad != 0))
 16363|         {
 16364|             generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
 16365|         }
 16366|         generation_allocation_pointer (gen) += size + pad;
 16367|         assert (generation_allocation_pointer (gen) <= generation_allocation_limit (gen));
 16368|         dprintf (3, ("Allocated in expanded heap %zx:%zd", (size_t)(result+pad), size));
 16369|         dprintf (3, ("aie: ptr: %p, limit: %p, sr: %p",
 16370|             generation_allocation_pointer (gen), generation_allocation_limit (gen),
 16371|             generation_allocation_context_start_region (gen)));
 16372|         return result + pad;
 16373|     }
 16374| }
 16375| generation*  gc_heap::ensure_ephemeral_heap_segment (generation* consing_gen)
 16376| {
 16377|     heap_segment* seg = generation_allocation_segment (consing_gen);
 16378|     if (seg != ephemeral_heap_segment)
 16379|     {
 16380|         assert (generation_allocation_pointer (consing_gen)>= heap_segment_mem (seg));
 16381|         assert (generation_allocation_pointer (consing_gen)<= heap_segment_committed (seg));
 16382|         heap_segment_plan_allocated (seg) = generation_allocation_pointer (consing_gen);
 16383|         generation* new_consing_gen = generation_of (max_generation - 1);
 16384|         generation_allocation_pointer (new_consing_gen) =
 16385|                 heap_segment_mem (ephemeral_heap_segment);
 16386|         generation_allocation_limit (new_consing_gen) =
 16387|             generation_allocation_pointer (new_consing_gen);
 16388|         generation_allocation_context_start_region (new_consing_gen) =
 16389|             generation_allocation_pointer (new_consing_gen);
 16390|         generation_allocation_segment (new_consing_gen) = ephemeral_heap_segment;
 16391|         return new_consing_gen;
 16392|     }
 16393|     else
 16394|         return consing_gen;
 16395| }
 16396| #endif //!USE_REGIONS
 16397| inline
 16398| void gc_heap::init_alloc_info (generation* gen, heap_segment* seg)
 16399| {
 16400|     generation_allocation_segment (gen) = seg;
 16401|     generation_allocation_pointer (gen) = heap_segment_mem (seg);
 16402|     generation_allocation_limit (gen) = generation_allocation_pointer (gen);
 16403|     generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
 16404| }
 16405| inline
 16406| heap_segment* gc_heap::get_next_alloc_seg (generation* gen)
 16407| {
 16408| #ifdef USE_REGIONS
 16409|     heap_segment* saved_region = generation_allocation_segment (gen);
 16410|     int gen_num = heap_segment_gen_num (saved_region);
 16411|     heap_segment* region = saved_region;
 16412|     while (1)
 16413|     {
 16414|         region = heap_segment_non_sip (region);
 16415|         if (region)
 16416|         {
 16417|             break;
 16418|         }
 16419|         else
 16420|         {
 16421|             if (gen_num > 0)
 16422|             {
 16423|                 gen_num--;
 16424|                 region = generation_start_segment (generation_of (gen_num));
 16425|                 dprintf (REGIONS_LOG, ("h%d next alloc region: switching to next gen%d start %zx(%p)",
 16426|                     heap_number, heap_segment_gen_num (region), (size_t)region,
 16427|                     heap_segment_mem (region)));
 16428|             }
 16429|             else
 16430|             {
 16431|                 assert (!"ran out regions when getting the next alloc seg!");
 16432|             }
 16433|         }
 16434|     }
 16435|     if (region != saved_region)
 16436|     {
 16437|         dprintf (REGIONS_LOG, ("init allocate region for gen%d to %p(%d)",
 16438|             gen->gen_num, heap_segment_mem (region), heap_segment_gen_num (region)));
 16439|         init_alloc_info (gen, region);
 16440|     }
 16441|     return region;
 16442| #else
 16443|     return generation_allocation_segment (gen);
 16444| #endif //USE_REGIONS
 16445| }
 16446| uint8_t* gc_heap::allocate_in_condemned_generations (generation* gen,
 16447|                                                   size_t size,
 16448|                                                   int from_gen_number,
 16449| #ifdef SHORT_PLUGS
 16450|                                                   BOOL* convert_to_pinned_p,
 16451|                                                   uint8_t* next_pinned_plug,
 16452|                                                   heap_segment* current_seg,
 16453| #endif //SHORT_PLUGS
 16454|                                                   uint8_t* old_loc
 16455|                                                   REQD_ALIGN_AND_OFFSET_DCL)
 16456| {
 16457| #ifndef USE_REGIONS
 16458|     if (settings.promotion)
 16459|     {
 16460|         assert (generation_plan_allocation_start (youngest_generation) == 0);
 16461|     }
 16462| #endif //!USE_REGIONS
 16463|     size = Align (size);
 16464|     assert (size >= Align (min_obj_size));
 16465|     int to_gen_number = from_gen_number;
 16466|     if (from_gen_number != (int)max_generation)
 16467|     {
 16468|         to_gen_number = from_gen_number + (settings.promotion ? 1 : 0);
 16469|     }
 16470|     dprintf (3, ("aic gen%d: s: %zd, ac: %p-%p", gen->gen_num, size,
 16471|             generation_allocation_pointer (gen), generation_allocation_limit (gen)));
 16472| #ifdef SHORT_PLUGS
 16473|     int pad_in_front = ((old_loc != 0) && (to_gen_number != max_generation)) ? USE_PADDING_FRONT : 0;
 16474| #else //SHORT_PLUGS
 16475|     int pad_in_front = 0;
 16476| #endif //SHORT_PLUGS
 16477|     if ((from_gen_number != -1) && (from_gen_number != (int)max_generation) && settings.promotion)
 16478|     {
 16479|         generation_condemned_allocated (generation_of (from_gen_number + (settings.promotion ? 1 : 0))) += size;
 16480|         generation_allocation_size (generation_of (from_gen_number + (settings.promotion ? 1 : 0))) += size;
 16481|     }
 16482| retry:
 16483|     {
 16484|         heap_segment* seg = get_next_alloc_seg (gen);
 16485|         if (! (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, generation_allocation_pointer (gen),
 16486|                            generation_allocation_limit (gen), old_loc,
 16487|                            ((generation_allocation_limit (gen) != heap_segment_plan_allocated (seg))?USE_PADDING_TAIL:0)|pad_in_front)))
 16488|         {
 16489|             if ((! (pinned_plug_que_empty_p()) &&
 16490|                  (generation_allocation_limit (gen) ==
 16491|                   pinned_plug (oldest_pin()))))
 16492|             {
 16493|                 size_t entry = deque_pinned_plug();
 16494|                 mark* pinned_plug_entry = pinned_plug_of (entry);
 16495|                 size_t len = pinned_len (pinned_plug_entry);
 16496|                 uint8_t* plug = pinned_plug (pinned_plug_entry);
 16497|                 set_new_pin_info (pinned_plug_entry, generation_allocation_pointer (gen));
 16498| #ifdef USE_REGIONS
 16499|                 if (to_gen_number == 0)
 16500|                 {
 16501|                     update_planned_gen0_free_space (pinned_len (pinned_plug_entry), plug);
 16502|                     dprintf (REGIONS_LOG, ("aic: not promotion, gen0 added free space %zd at %p",
 16503|                                     pinned_len (pinned_plug_entry), plug));
 16504|                 }
 16505| #endif //USE_REGIONS
 16506| #ifdef FREE_USAGE_STATS
 16507|                 generation_allocated_in_pinned_free (gen) += generation_allocated_since_last_pin (gen);
 16508|                 dprintf (3, ("allocated %zd so far within pin %zx, total->%zd",
 16509|                     generation_allocated_since_last_pin (gen),
 16510|                     plug,
 16511|                     generation_allocated_in_pinned_free (gen)));
 16512|                 generation_allocated_since_last_pin (gen) = 0;
 16513|                 add_item_to_current_pinned_free (gen->gen_num, pinned_len (pinned_plug_of (entry)));
 16514| #endif //FREE_USAGE_STATS
 16515|                 dprintf (3, ("mark stack bos: %zd, tos: %zd, aic: p %p len: %zx->%zx",
 16516|                     mark_stack_bos, mark_stack_tos, plug, len, pinned_len (pinned_plug_of (entry))));
 16517|                 assert(mark_stack_array[entry].len == 0 ||
 16518|                        mark_stack_array[entry].len >= Align(min_obj_size));
 16519|                 generation_allocation_pointer (gen) = plug + len;
 16520|                 generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
 16521|                 generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
 16522|                 set_allocator_next_pin (gen);
 16523|                 int frgn = object_gennum (plug);
 16524|                 if ((frgn != (int)max_generation) && settings.promotion)
 16525|                 {
 16526|                     generation_pinned_allocation_sweep_size (generation_of (frgn + 1)) += len;
 16527| #ifdef USE_REGIONS
 16528|                     int togn = (in_range_for_segment (plug, seg) ? to_gen_number : object_gennum_plan (plug));
 16529| #else
 16530|                     int togn = object_gennum_plan (plug);
 16531| #endif //USE_REGIONS
 16532|                     if (frgn < togn)
 16533|                     {
 16534|                         generation_pinned_allocation_compact_size (generation_of (togn)) += len;
 16535|                     }
 16536|                 }
 16537|                 goto retry;
 16538|             }
 16539|             if (generation_allocation_limit (gen) != heap_segment_plan_allocated (seg))
 16540|             {
 16541|                 generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
 16542|                 dprintf (3, ("changed limit to plan alloc: %p", generation_allocation_limit (gen)));
 16543|             }
 16544|             else
 16545|             {
 16546|                 if (heap_segment_plan_allocated (seg) != heap_segment_committed (seg))
 16547|                 {
 16548|                     heap_segment_plan_allocated (seg) = heap_segment_committed (seg);
 16549|                     generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
 16550|                     dprintf (3, ("changed limit to commit: %p", generation_allocation_limit (gen)));
 16551|                 }
 16552|                 else
 16553|                 {
 16554| #if !defined(RESPECT_LARGE_ALIGNMENT) && !defined(USE_REGIONS)
 16555|                     assert (gen != youngest_generation);
 16556| #endif //!RESPECT_LARGE_ALIGNMENT && !USE_REGIONS
 16557|                     if (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, generation_allocation_pointer (gen),
 16558|                                     heap_segment_reserved (seg), old_loc, USE_PADDING_TAIL | pad_in_front) &&
 16559|                         (grow_heap_segment (seg, generation_allocation_pointer (gen), old_loc,
 16560|                                             size, pad_in_front REQD_ALIGN_AND_OFFSET_ARG)))
 16561|                     {
 16562|                         dprintf (3, ("Expanded segment allocation by committing more memory"));
 16563|                         heap_segment_plan_allocated (seg) = heap_segment_committed (seg);
 16564|                         generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
 16565|                     }
 16566|                     else
 16567|                     {
 16568|                         heap_segment*   next_seg = heap_segment_next (seg);
 16569|                         dprintf (REGIONS_LOG, ("aic next: %p(%p,%p) -> %p(%p,%p)",
 16570|                             heap_segment_mem (seg), heap_segment_allocated (seg), heap_segment_plan_allocated (seg),
 16571|                             (next_seg ? heap_segment_mem (next_seg) : 0),
 16572|                             (next_seg ? heap_segment_allocated (next_seg) : 0),
 16573|                             (next_seg ? heap_segment_plan_allocated (next_seg) : 0)));
 16574|                         assert (generation_allocation_pointer (gen)>=
 16575|                                 heap_segment_mem (seg));
 16576|                         if (!pinned_plug_que_empty_p() &&
 16577|                             ((pinned_plug (oldest_pin()) <
 16578|                               heap_segment_allocated (seg)) &&
 16579|                              (pinned_plug (oldest_pin()) >=
 16580|                               generation_allocation_pointer (gen))))
 16581|                         {
 16582|                             LOG((LF_GC, LL_INFO10, "remaining pinned plug %zx while leaving segment on allocation",
 16583|                                          pinned_plug (oldest_pin())));
 16584|                             FATAL_GC_ERROR();
 16585|                         }
 16586|                         assert (generation_allocation_pointer (gen)>=
 16587|                                 heap_segment_mem (seg));
 16588|                         assert (generation_allocation_pointer (gen)<=
 16589|                                 heap_segment_committed (seg));
 16590|                         heap_segment_plan_allocated (seg) = generation_allocation_pointer (gen);
 16591| #ifdef USE_REGIONS
 16592|                         set_region_plan_gen_num (seg, to_gen_number);
 16593|                         if ((next_seg == 0) && (heap_segment_gen_num (seg) > 0))
 16594|                         {
 16595|                             next_seg = generation_start_segment (generation_of (heap_segment_gen_num (seg) - 1));
 16596|                             dprintf (REGIONS_LOG, ("h%d aic: switching to next gen%d start %zx(%p)",
 16597|                                 heap_number, heap_segment_gen_num (next_seg), (size_t)next_seg,
 16598|                                 heap_segment_mem (next_seg)));
 16599|                         }
 16600| #endif //USE_REGIONS
 16601|                         if (next_seg)
 16602|                         {
 16603|                             init_alloc_info (gen, next_seg);
 16604|                         }
 16605|                         else
 16606|                         {
 16607| #ifdef USE_REGIONS
 16608|                             assert (!"should not happen for regions!");
 16609| #else
 16610|                             return 0; //should only happen during allocation of generation 0 gap
 16611| #endif //USE_REGIONS
 16612|                         }
 16613|                     }
 16614|                 }
 16615|             }
 16616|             set_allocator_next_pin (gen);
 16617|             goto retry;
 16618|         }
 16619|     }
 16620|     {
 16621|         assert (generation_allocation_pointer (gen)>=
 16622|                 heap_segment_mem (generation_allocation_segment (gen)));
 16623|         uint8_t* result = generation_allocation_pointer (gen);
 16624|         size_t pad = 0;
 16625| #ifdef SHORT_PLUGS
 16626|         if ((pad_in_front & USE_PADDING_FRONT) &&
 16627|             (((generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen))==0) ||
 16628|              ((generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen))>=DESIRED_PLUG_LENGTH)))
 16629|         {
 16630|             ptrdiff_t dist = old_loc - result;
 16631|             if (dist == 0)
 16632|             {
 16633|                 dprintf (3, ("old alloc: %p, same as new alloc, not padding", old_loc));
 16634|                 pad = 0;
 16635|             }
 16636|             else
 16637|             {
 16638|                 if ((dist > 0) && (dist < (ptrdiff_t)Align (min_obj_size)))
 16639|                 {
 16640|                     dprintf (1, ("old alloc: %p, only %zd bytes > new alloc! Shouldn't happen", old_loc, dist));
 16641|                     FATAL_GC_ERROR();
 16642|                 }
 16643|                 pad = Align (min_obj_size);
 16644|                 set_plug_padded (old_loc);
 16645|             }
 16646|         }
 16647| #endif //SHORT_PLUGS
 16648| #ifdef FEATURE_STRUCTALIGN
 16649|         _ASSERTE(!old_loc || alignmentOffset != 0);
 16650|         _ASSERTE(old_loc || requiredAlignment == DATA_ALIGNMENT);
 16651|         if ((old_loc != 0))
 16652|         {
 16653|             size_t pad1 = ComputeStructAlignPad(result+pad, requiredAlignment, alignmentOffset);
 16654|             set_node_aligninfo (old_loc, requiredAlignment, pad1);
 16655|             pad += pad1;
 16656|         }
 16657| #else // FEATURE_STRUCTALIGN
 16658|         if (!((old_loc == 0) || same_large_alignment_p (old_loc, result+pad)))
 16659|         {
 16660|             pad += switch_alignment_size (pad != 0);
 16661|             set_node_realigned(old_loc);
 16662|             dprintf (3, ("Allocation realignment old_loc: %zx, new_loc:%zx",
 16663|                          (size_t)old_loc, (size_t)(result+pad)));
 16664|             assert (same_large_alignment_p (result + pad, old_loc));
 16665|         }
 16666| #endif // FEATURE_STRUCTALIGN
 16667| #ifdef SHORT_PLUGS
 16668|         if ((next_pinned_plug != 0) && (pad != 0) && (generation_allocation_segment (gen) == current_seg))
 16669|         {
 16670|             assert (old_loc != 0);
 16671|             ptrdiff_t dist_to_next_pin = (ptrdiff_t)(next_pinned_plug - (generation_allocation_pointer (gen) + size + pad));
 16672|             assert (dist_to_next_pin >= 0);
 16673|             if ((dist_to_next_pin >= 0) && (dist_to_next_pin < (ptrdiff_t)Align (min_obj_size)))
 16674|             {
 16675|                 dprintf (3, ("%p->(%p,%p),%p(%zx)(%zx),NP->PP",
 16676|                     old_loc,
 16677|                     generation_allocation_pointer (gen),
 16678|                     generation_allocation_limit (gen),
 16679|                     next_pinned_plug,
 16680|                     size,
 16681|                     dist_to_next_pin));
 16682|                 clear_plug_padded (old_loc);
 16683|                 pad = 0;
 16684|                 *convert_to_pinned_p = TRUE;
 16685|                 record_interesting_data_point (idp_converted_pin);
 16686|                 return 0;
 16687|             }
 16688|         }
 16689| #endif //SHORT_PLUGS
 16690|         if ((old_loc == 0) || (pad != 0))
 16691|         {
 16692|             generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
 16693|         }
 16694|         generation_allocation_pointer (gen) += size + pad;
 16695|         assert (generation_allocation_pointer (gen) <= generation_allocation_limit (gen));
 16696|         if ((pad > 0) && (to_gen_number >= 0))
 16697|         {
 16698|             generation_free_obj_space (generation_of (to_gen_number)) += pad;
 16699|         }
 16700| #ifdef FREE_USAGE_STATS
 16701|         generation_allocated_since_last_pin (gen) += size;
 16702| #endif //FREE_USAGE_STATS
 16703|         dprintf (3, ("aic: old: %p ptr: %p, limit: %p, sr: %p, res: %p, pad: %zd",
 16704|             old_loc,
 16705|             generation_allocation_pointer (gen), generation_allocation_limit (gen),
 16706|             generation_allocation_context_start_region (gen),
 16707|             result, (size_t)pad));
 16708|         assert (result + pad);
 16709|         return result + pad;
 16710|     }
 16711| }
 16712| int gc_heap::joined_generation_to_condemn (BOOL should_evaluate_elevation,
 16713|                                            int initial_gen,
 16714|                                            int current_gen,
 16715|                                            BOOL* blocking_collection_p
 16716|                                            STRESS_HEAP_ARG(int n_original))
 16717| {
 16718|     gc_data_global.gen_to_condemn_reasons.init();
 16719| #ifdef BGC_SERVO_TUNING
 16720|     if (settings.entry_memory_load == 0)
 16721|     {
 16722|         uint32_t current_memory_load = 0;
 16723|         uint64_t current_available_physical = 0;
 16724|         get_memory_info (&current_memory_load, &current_available_physical);
 16725|         settings.entry_memory_load = current_memory_load;
 16726|         settings.entry_available_physical_mem = current_available_physical;
 16727|     }
 16728| #endif //BGC_SERVO_TUNING
 16729|     int n = current_gen;
 16730| #ifdef MULTIPLE_HEAPS
 16731|     BOOL joined_last_gc_before_oom = FALSE;
 16732|     for (int i = 0; i < n_heaps; i++)
 16733|     {
 16734|         if (g_heaps[i]->last_gc_before_oom)
 16735|         {
 16736|             dprintf (GTC_LOG, ("h%d is setting blocking to TRUE", i));
 16737|             joined_last_gc_before_oom = TRUE;
 16738|             break;
 16739|         }
 16740|     }
 16741| #else
 16742|     BOOL joined_last_gc_before_oom = last_gc_before_oom;
 16743| #endif //MULTIPLE_HEAPS
 16744|     if (joined_last_gc_before_oom && settings.pause_mode != pause_low_latency)
 16745|     {
 16746|         assert (*blocking_collection_p);
 16747|     }
 16748|     if (should_evaluate_elevation && (n == max_generation))
 16749|     {
 16750|         dprintf (GTC_LOG, ("lock: %d(%d)",
 16751|             (settings.should_lock_elevation ? 1 : 0),
 16752|             settings.elevation_locked_count));
 16753|         if (settings.should_lock_elevation)
 16754|         {
 16755|             settings.elevation_locked_count++;
 16756|             if (settings.elevation_locked_count == 6)
 16757|             {
 16758|                 settings.elevation_locked_count = 0;
 16759|             }
 16760|             else
 16761|             {
 16762|                 n = max_generation - 1;
 16763|                 gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_avoid_unproductive);
 16764|                 settings.elevation_reduced = TRUE;
 16765|             }
 16766|         }
 16767|         else
 16768|         {
 16769|             settings.elevation_locked_count = 0;
 16770|         }
 16771|     }
 16772|     else
 16773|     {
 16774|         settings.should_lock_elevation = FALSE;
 16775|         settings.elevation_locked_count = 0;
 16776|     }
 16777|     if (provisional_mode_triggered && (n == max_generation))
 16778|     {
 16779|         if ((initial_gen == max_generation) || (settings.reason == reason_alloc_loh))
 16780|         {
 16781|             dprintf (GTC_LOG, ("full GC induced, not reducing gen"));
 16782|             if (initial_gen == max_generation)
 16783|             {
 16784|                 gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_pm_induced_fullgc_p);
 16785|             }
 16786|             else
 16787|             {
 16788|                 gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_pm_alloc_loh);
 16789|             }
 16790|             *blocking_collection_p = TRUE;
 16791|         }
 16792|         else if (
 16793| #ifndef USE_REGIONS
 16794|                  should_expand_in_full_gc ||
 16795| #endif //!USE_REGIONS
 16796|                  joined_last_gc_before_oom)
 16797|         {
 16798|             dprintf (GTC_LOG, ("need full blocking GCs to expand heap or avoid OOM, not reducing gen"));
 16799|             assert (*blocking_collection_p);
 16800|         }
 16801|         else
 16802|         {
 16803|             dprintf (GTC_LOG, ("reducing gen in PM: %d->%d->%d", initial_gen, n, (max_generation - 1)));
 16804|             gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_gen1_in_pm);
 16805|             n = max_generation - 1;
 16806|         }
 16807|     }
 16808| #ifndef USE_REGIONS
 16809|     if (should_expand_in_full_gc)
 16810|     {
 16811|         should_expand_in_full_gc = FALSE;
 16812|     }
 16813| #endif //!USE_REGIONS
 16814|     if (heap_hard_limit)
 16815|     {
 16816|         dprintf (GTC_LOG, ("committed %zd is %d%% of limit %zd",
 16817|             current_total_committed, (int)((float)current_total_committed * 100.0 / (float)heap_hard_limit),
 16818|             heap_hard_limit));
 16819|         bool full_compact_gc_p = false;
 16820|         if (joined_last_gc_before_oom)
 16821|         {
 16822|             gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_limit_before_oom);
 16823|             full_compact_gc_p = true;
 16824|         }
 16825|         else if ((current_total_committed * 10) >= (heap_hard_limit * 9))
 16826|         {
 16827|             size_t loh_frag = get_total_gen_fragmentation (loh_generation);
 16828|             if ((loh_frag * 8) >= heap_hard_limit)
 16829|             {
 16830|                 dprintf (GTC_LOG, ("loh frag: %zd > 1/8 of limit %zd", loh_frag, (heap_hard_limit / 8)));
 16831|                 gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_limit_loh_frag);
 16832|                 full_compact_gc_p = true;
 16833|             }
 16834|             else
 16835|             {
 16836|                 size_t est_loh_reclaim = get_total_gen_estimated_reclaim (loh_generation);
 16837|                 if ((est_loh_reclaim * 8) >= heap_hard_limit)
 16838|                 {
 16839|                     gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_limit_loh_reclaim);
 16840|                     full_compact_gc_p = true;
 16841|                 }
 16842|                 dprintf (GTC_LOG, ("loh est reclaim: %zd, 1/8 of limit %zd", est_loh_reclaim, (heap_hard_limit / 8)));
 16843|             }
 16844|         }
 16845|         if (full_compact_gc_p)
 16846|         {
 16847|             n = max_generation;
 16848|             *blocking_collection_p = TRUE;
 16849|             settings.loh_compaction = TRUE;
 16850|             dprintf (GTC_LOG, ("compacting LOH due to hard limit"));
 16851|         }
 16852|     }
 16853|     if ((conserve_mem_setting != 0) && (n == max_generation))
 16854|     {
 16855|         float frag_limit = 1.0f - conserve_mem_setting / 10.0f;
 16856|         size_t loh_size = get_total_gen_size (loh_generation);
 16857|         size_t gen2_size = get_total_gen_size (max_generation);
 16858|         float loh_frag_ratio = 0.0f;
 16859|         float combined_frag_ratio = 0.0f;
 16860|         if (loh_size != 0)
 16861|         {
 16862|             size_t loh_frag  = get_total_gen_fragmentation (loh_generation);
 16863|             size_t gen2_frag = get_total_gen_fragmentation (max_generation);
 16864|             loh_frag_ratio = (float)loh_frag / (float)loh_size;
 16865|             combined_frag_ratio = (float)(gen2_frag + loh_frag) / (float)(gen2_size + loh_size);
 16866|         }
 16867|         if (combined_frag_ratio > frag_limit)
 16868|         {
 16869|             dprintf (GTC_LOG, ("combined frag: %f > limit %f, loh frag: %f", combined_frag_ratio, frag_limit, loh_frag_ratio));
 16870|             gc_data_global.gen_to_condemn_reasons.set_condition (gen_max_high_frag_p);
 16871|             n = max_generation;
 16872|             *blocking_collection_p = TRUE;
 16873|             if (loh_frag_ratio > frag_limit)
 16874|             {
 16875|                 settings.loh_compaction = TRUE;
 16876|                 dprintf (GTC_LOG, ("compacting LOH due to GCConserveMem setting"));
 16877|             }
 16878|         }
 16879|     }
 16880| #ifdef BGC_SERVO_TUNING
 16881|     if (bgc_tuning::should_trigger_ngc2())
 16882|     {
 16883|         gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_servo_ngc);
 16884|         n = max_generation;
 16885|         *blocking_collection_p = TRUE;
 16886|     }
 16887|     if ((n < max_generation) && !gc_heap::background_running_p() &&
 16888|         bgc_tuning::stepping_trigger (settings.entry_memory_load, get_current_gc_index (max_generation)))
 16889|     {
 16890|         gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_servo_initial);
 16891|         n = max_generation;
 16892|         saved_bgc_tuning_reason = reason_bgc_stepping;
 16893|     }
 16894|     if ((n < max_generation) && bgc_tuning::should_trigger_bgc())
 16895|     {
 16896|         gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_servo_bgc);
 16897|         n = max_generation;
 16898|     }
 16899|     if (n == (max_generation - 1))
 16900|     {
 16901|         if (bgc_tuning::should_delay_alloc (max_generation))
 16902|         {
 16903|             gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_servo_postpone);
 16904|             n -= 1;
 16905|         }
 16906|     }
 16907| #endif //BGC_SERVO_TUNING
 16908|     if ((n == max_generation) && (*blocking_collection_p == FALSE))
 16909|     {
 16910|         settings.should_lock_elevation = FALSE;
 16911|         settings.elevation_locked_count = 0;
 16912|         dprintf (GTC_LOG, ("doing bgc, reset elevation"));
 16913|     }
 16914| #ifdef STRESS_HEAP
 16915| #ifdef BACKGROUND_GC
 16916|     if (n_original != max_generation &&
 16917|         g_pConfig->GetGCStressLevel() && gc_can_use_concurrent)
 16918|     {
 16919| #ifndef FEATURE_NATIVEAOT
 16920|         if (*blocking_collection_p)
 16921|         {
 16922|             GCStressPolicy::GlobalDisable();
 16923|         }
 16924|         else
 16925| #endif // !FEATURE_NATIVEAOT
 16926|         {
 16927|             gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_stress);
 16928|             n = max_generation;
 16929|         }
 16930|     }
 16931| #endif //BACKGROUND_GC
 16932| #endif //STRESS_HEAP
 16933| #ifdef BACKGROUND_GC
 16934|     if ((n == max_generation) && background_running_p())
 16935|     {
 16936|         n = max_generation - 1;
 16937|         dprintf (GTC_LOG, ("bgc in progress - 1 instead of 2"));
 16938|     }
 16939| #endif //BACKGROUND_GC
 16940|     return n;
 16941| }
 16942| inline
 16943| size_t get_survived_size (gc_history_per_heap* hist)
 16944| {
 16945|     size_t surv_size = 0;
 16946|     gc_generation_data* gen_data;
 16947|     for (int gen_number = 0; gen_number < total_generation_count; gen_number++)
 16948|     {
 16949|         gen_data = &(hist->gen_data[gen_number]);
 16950|         surv_size += (gen_data->size_after -
 16951|                       gen_data->free_list_space_after -
 16952|                       gen_data->free_obj_space_after);
 16953|     }
 16954|     return surv_size;
 16955| }
 16956| size_t gc_heap::get_total_survived_size()
 16957| {
 16958|     size_t total_surv_size = 0;
 16959| #ifdef MULTIPLE_HEAPS
 16960|     for (int i = 0; i < gc_heap::n_heaps; i++)
 16961|     {
 16962|         gc_heap* hp = gc_heap::g_heaps[i];
 16963|         gc_history_per_heap* current_gc_data_per_heap = hp->get_gc_data_per_heap();
 16964|         total_surv_size += get_survived_size (current_gc_data_per_heap);
 16965|     }
 16966| #else
 16967|     gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
 16968|     total_surv_size = get_survived_size (current_gc_data_per_heap);
 16969| #endif //MULTIPLE_HEAPS
 16970|     return total_surv_size;
 16971| }
 16972| size_t gc_heap::get_total_allocated_since_last_gc()
 16973| {
 16974|     size_t total_allocated_size = 0;
 16975| #ifdef MULTIPLE_HEAPS
 16976|     for (int i = 0; i < gc_heap::n_heaps; i++)
 16977|     {
 16978|         gc_heap* hp = gc_heap::g_heaps[i];
 16979| #else //MULTIPLE_HEAPS
 16980|     {
 16981|         gc_heap* hp = pGenGCHeap;
 16982| #endif //MULTIPLE_HEAPS
 16983|         total_allocated_size += hp->allocated_since_last_gc[0] + hp->allocated_since_last_gc[1];
 16984|         hp->allocated_since_last_gc[0] = 0;
 16985|         hp->allocated_since_last_gc[1] = 0;
 16986|     }
 16987|     return total_allocated_size;
 16988| }
 16989| size_t gc_heap::get_current_allocated()
 16990| {
 16991|     dynamic_data* dd = dynamic_data_of (0);
 16992|     size_t current_alloc = dd_desired_allocation (dd) - dd_new_allocation (dd);
 16993|     for (int i = uoh_start_generation; i < total_generation_count; i++)
 16994|     {
 16995|         dynamic_data* dd = dynamic_data_of (i);
 16996|         current_alloc += dd_desired_allocation (dd) - dd_new_allocation (dd);
 16997|     }
 16998|     return current_alloc;
 16999| }
 17000| size_t gc_heap::get_total_allocated()
 17001| {
 17002|     size_t total_current_allocated = 0;
 17003| #ifdef MULTIPLE_HEAPS
 17004|     for (int i = 0; i < gc_heap::n_heaps; i++)
 17005|     {
 17006|         gc_heap* hp = gc_heap::g_heaps[i];
 17007|         total_current_allocated += hp->get_current_allocated();
 17008|     }
 17009| #else
 17010|     total_current_allocated = get_current_allocated();
 17011| #endif //MULTIPLE_HEAPS
 17012|     return total_current_allocated;
 17013| }
 17014| size_t gc_heap::get_total_promoted()
 17015| {
 17016|     size_t total_promoted_size = 0;
 17017|     int highest_gen = ((settings.condemned_generation == max_generation) ?
 17018|                        (total_generation_count - 1) : settings.condemned_generation);
 17019| #ifdef MULTIPLE_HEAPS
 17020|     for (int i = 0; i < gc_heap::n_heaps; i++)
 17021|     {
 17022|         gc_heap* hp = gc_heap::g_heaps[i];
 17023| #else //MULTIPLE_HEAPS
 17024|     {
 17025|         gc_heap* hp = pGenGCHeap;
 17026| #endif //MULTIPLE_HEAPS
 17027|         for (int gen_number = 0; gen_number <= highest_gen; gen_number++)
 17028|         {
 17029|             total_promoted_size += dd_promoted_size (hp->dynamic_data_of (gen_number));
 17030|         }
 17031|     }
 17032|     return total_promoted_size;
 17033| }
 17034| #ifdef BGC_SERVO_TUNING
 17035| size_t gc_heap::get_total_generation_size (int gen_number)
 17036| {
 17037|     size_t total_generation_size = 0;
 17038| #ifdef MULTIPLE_HEAPS
 17039|     for (int i = 0; i < gc_heap::n_heaps; i++)
 17040|     {
 17041|         gc_heap* hp = gc_heap::g_heaps[i];
 17042| #else //MULTIPLE_HEAPS
 17043|     {
 17044|         gc_heap* hp = pGenGCHeap;
 17045| #endif //MULTIPLE_HEAPS
 17046|         total_generation_size += hp->generation_size (gen_number);
 17047|     }
 17048|     return total_generation_size;
 17049| }
 17050| size_t gc_heap::get_total_servo_alloc (int gen_number)
 17051| {
 17052|     size_t total_alloc = 0;
 17053| #ifdef MULTIPLE_HEAPS
 17054|     for (int i = 0; i < gc_heap::n_heaps; i++)
 17055|     {
 17056|         gc_heap* hp = gc_heap::g_heaps[i];
 17057| #else //MULTIPLE_HEAPS
 17058|     {
 17059|         gc_heap* hp = pGenGCHeap;
 17060| #endif //MULTIPLE_HEAPS
 17061|         generation* gen = hp->generation_of (gen_number);
 17062|         total_alloc += generation_free_list_allocated (gen);
 17063|         total_alloc += generation_end_seg_allocated (gen);
 17064|         total_alloc += generation_condemned_allocated (gen);
 17065|         total_alloc += generation_sweep_allocated (gen);
 17066|     }
 17067|     return total_alloc;
 17068| }
 17069| size_t gc_heap::get_total_bgc_promoted()
 17070| {
 17071|     size_t total_bgc_promoted = 0;
 17072| #ifdef MULTIPLE_HEAPS
 17073|     int num_heaps = gc_heap::n_heaps;
 17074| #else //MULTIPLE_HEAPS
 17075|     int num_heaps = 1;
 17076| #endif //MULTIPLE_HEAPS
 17077|     for (int i = 0; i < num_heaps; i++)
 17078|     {
 17079|         total_bgc_promoted += bpromoted_bytes (i);
 17080|     }
 17081|     return total_bgc_promoted;
 17082| }
 17083| size_t gc_heap::get_total_surv_size (int gen_number)
 17084| {
 17085|     size_t total_surv_size = 0;
 17086| #ifdef MULTIPLE_HEAPS
 17087|     for (int i = 0; i < gc_heap::n_heaps; i++)
 17088|     {
 17089|         gc_heap* hp = gc_heap::g_heaps[i];
 17090| #else //MULTIPLE_HEAPS
 17091|     {
 17092|         gc_heap* hp = pGenGCHeap;
 17093| #endif //MULTIPLE_HEAPS
 17094|         total_surv_size += dd_current_size (hp->dynamic_data_of (gen_number));
 17095|     }
 17096|     return total_surv_size;
 17097| }
 17098| size_t gc_heap::get_total_begin_data_size (int gen_number)
 17099| {
 17100|     size_t total_begin_data_size = 0;
 17101| #ifdef MULTIPLE_HEAPS
 17102|     for (int i = 0; i < gc_heap::n_heaps; i++)
 17103|     {
 17104|         gc_heap* hp = gc_heap::g_heaps[i];
 17105| #else //MULTIPLE_HEAPS
 17106|     {
 17107|         gc_heap* hp = pGenGCHeap;
 17108| #endif //MULTIPLE_HEAPS
 17109|         total_begin_data_size += dd_begin_data_size (hp->dynamic_data_of (gen_number));
 17110|     }
 17111|     return total_begin_data_size;
 17112| }
 17113| size_t gc_heap::get_total_generation_fl_size (int gen_number)
 17114| {
 17115|     size_t total_generation_fl_size = 0;
 17116| #ifdef MULTIPLE_HEAPS
 17117|     for (int i = 0; i < gc_heap::n_heaps; i++)
 17118|     {
 17119|         gc_heap* hp = gc_heap::g_heaps[i];
 17120| #else //MULTIPLE_HEAPS
 17121|     {
 17122|         gc_heap* hp = pGenGCHeap;
 17123| #endif //MULTIPLE_HEAPS
 17124|         total_generation_fl_size += generation_free_list_space (hp->generation_of (gen_number));
 17125|     }
 17126|     return total_generation_fl_size;
 17127| }
 17128| size_t gc_heap::get_current_gc_index (int gen_number)
 17129| {
 17130| #ifdef MULTIPLE_HEAPS
 17131|     gc_heap* hp = gc_heap::g_heaps[0];
 17132|     return dd_collection_count (hp->dynamic_data_of (gen_number));
 17133| #else
 17134|     return dd_collection_count (dynamic_data_of (gen_number));
 17135| #endif //MULTIPLE_HEAPS
 17136| }
 17137| #endif //BGC_SERVO_TUNING
 17138| size_t gc_heap::current_generation_size (int gen_number)
 17139| {
 17140|     dynamic_data* dd = dynamic_data_of (gen_number);
 17141|     size_t gen_size = (dd_current_size (dd) + dd_desired_allocation (dd)
 17142|                         - dd_new_allocation (dd));
 17143|     return gen_size;
 17144| }
 17145| #ifdef USE_REGIONS
 17146| bool gc_heap::try_get_new_free_region()
 17147| {
 17148|     heap_segment* region = 0;
 17149|     if (free_regions[basic_free_region].get_num_free_regions() > 0)
 17150|     {
 17151|         dprintf (REGIONS_LOG, ("h%d has %zd free regions %p", heap_number, free_regions[basic_free_region].get_num_free_regions(),
 17152|             heap_segment_mem (free_regions[basic_free_region].get_first_free_region())));
 17153|         return true;
 17154|     }
 17155|     else
 17156|     {
 17157|         region = allocate_new_region (__this, 0, false);
 17158|         if (region)
 17159|         {
 17160|             if (init_table_for_region (0, region))
 17161|             {
 17162|                 return_free_region (region);
 17163|                 dprintf (REGIONS_LOG, ("h%d got a new empty region %p", heap_number, region));
 17164|             }
 17165|             else
 17166|             {
 17167|                 region = 0;
 17168|             }
 17169|         }
 17170|     }
 17171|     return (region != 0);
 17172| }
 17173| bool gc_heap::init_table_for_region (int gen_number, heap_segment* region)
 17174| {
 17175| #ifdef BACKGROUND_GC
 17176|     dprintf (GC_TABLE_LOG, ("new seg %Ix, mark_array is %Ix",
 17177|         heap_segment_mem (region), mark_array));
 17178|     if (((region->flags & heap_segment_flags_ma_committed) == 0) &&
 17179|         !commit_mark_array_new_seg (__this, region))
 17180|     {
 17181|         dprintf (GC_TABLE_LOG, ("failed to commit mark array for the new region %Ix-%Ix",
 17182|             get_region_start (region), heap_segment_reserved (region)));
 17183|         decommit_region (region, gen_to_oh (gen_number), heap_number);
 17184|         return false;
 17185|     }
 17186|     if ((region->flags & heap_segment_flags_ma_committed) != 0)
 17187|     {
 17188|         bgc_verify_mark_array_cleared (region, true);
 17189|     }
 17190| #endif //BACKGROUND_GC
 17191|     if (gen_number <= max_generation)
 17192|     {
 17193|         size_t first_brick = brick_of (heap_segment_mem (region));
 17194|         set_brick (first_brick, -1);
 17195|     }
 17196|     else
 17197|     {
 17198|         assert (brick_table[brick_of (heap_segment_mem (region))] == 0);
 17199|     }
 17200|     return true;
 17201| }
 17202| #endif //USE_REGIONS
 17203| #ifdef _PREFAST_
 17204| #pragma warning(push)
 17205| #pragma warning(disable:6326) // "Potential comparison of a constant with another constant" is intentional in this function.
 17206| #endif //_PREFAST_
 17207| /*
 17208|     This is called by when we are actually doing a GC, or when we are just checking whether
 17209|     we would do a full blocking GC, in which case check_only_p is TRUE.
 17210|     The difference between calling this with check_only_p TRUE and FALSE is that when it's
 17211|     TRUE:
 17212|             settings.reason is ignored
 17213|             budgets are not checked (since they are checked before this is called)
 17214|             it doesn't change anything non local like generation_skip_ratio
 17215| */
 17216| int gc_heap::generation_to_condemn (int n_initial,
 17217|                                     BOOL* blocking_collection_p,
 17218|                                     BOOL* elevation_requested_p,
 17219|                                     BOOL check_only_p)
 17220| {
 17221|     gc_mechanisms temp_settings = settings;
 17222|     gen_to_condemn_tuning temp_condemn_reasons;
 17223|     gc_mechanisms* local_settings = (check_only_p ? &temp_settings : &settings);
 17224|     gen_to_condemn_tuning* local_condemn_reasons = (check_only_p ? &temp_condemn_reasons : &gen_to_condemn_reasons);
 17225|     if (!check_only_p)
 17226|     {
 17227|         if ((local_settings->reason == reason_oos_soh) || (local_settings->reason == reason_oos_loh))
 17228|         {
 17229|             assert (n_initial >= 1);
 17230|         }
 17231|         assert (settings.reason != reason_empty);
 17232|     }
 17233|     local_condemn_reasons->init();
 17234|     int n = n_initial;
 17235|     int n_alloc = n;
 17236|     if (heap_number == 0)
 17237|     {
 17238|         dprintf (GTC_LOG, ("init: %d(%d)", n_initial, settings.reason));
 17239|     }
 17240|     int i = 0;
 17241|     int temp_gen = 0;
 17242|     BOOL low_memory_detected = g_low_memory_status;
 17243|     uint32_t memory_load = 0;
 17244|     uint64_t available_physical = 0;
 17245|     uint64_t available_page_file = 0;
 17246|     BOOL check_memory = FALSE;
 17247|     BOOL high_fragmentation  = FALSE;
 17248|     BOOL v_high_memory_load  = FALSE;
 17249|     BOOL high_memory_load    = FALSE;
 17250|     BOOL low_ephemeral_space = FALSE;
 17251|     BOOL evaluate_elevation  = TRUE;
 17252|     *elevation_requested_p   = FALSE;
 17253|     *blocking_collection_p   = FALSE;
 17254|     BOOL check_max_gen_alloc = TRUE;
 17255| #ifdef STRESS_HEAP
 17256|     int orig_gen = n;
 17257| #endif //STRESS_HEAP
 17258|     if (!check_only_p)
 17259|     {
 17260|         dd_fragmentation (dynamic_data_of (0)) =
 17261|             generation_free_list_space (youngest_generation) +
 17262|             generation_free_obj_space (youngest_generation);
 17263|         for (int i = uoh_start_generation; i < total_generation_count; i++)
 17264|         {
 17265|             dd_fragmentation (dynamic_data_of (i)) =
 17266|                 generation_free_list_space (generation_of (i)) +
 17267|                 generation_free_obj_space (generation_of (i));
 17268|         }
 17269|         for (i = 0; i < total_generation_count; i++)
 17270|         {
 17271|             dynamic_data* dd = dynamic_data_of (i);
 17272|             dprintf (GTC_LOG, ("h%d: g%d: l: %zd (%zd)",
 17273|                 heap_number, i,
 17274|                 dd_new_allocation (dd),
 17275|                 dd_desired_allocation (dd)));
 17276|             dd_gc_new_allocation (dd) = dd_new_allocation (dd);
 17277|         }
 17278|         local_condemn_reasons->set_gen (gen_initial, n);
 17279|         temp_gen = n;
 17280| #ifdef BACKGROUND_GC
 17281|         if (gc_heap::background_running_p()
 17282| #ifdef BGC_SERVO_TUNING
 17283|             || bgc_tuning::fl_tuning_triggered
 17284|             || (bgc_tuning::enable_fl_tuning && bgc_tuning::use_stepping_trigger_p)
 17285| #endif //BGC_SERVO_TUNING
 17286|             )
 17287|         {
 17288|             check_max_gen_alloc = FALSE;
 17289|         }
 17290| #endif //BACKGROUND_GC
 17291|         if (check_max_gen_alloc)
 17292|         {
 17293|             for (int i = uoh_start_generation; i < total_generation_count; i++)
 17294|             {
 17295|                 if (get_new_allocation (i) <= 0)
 17296|                 {
 17297|                     n = max_generation;
 17298|                     local_condemn_reasons->set_gen (gen_alloc_budget, n);
 17299|                     dprintf (BGC_TUNING_LOG, ("BTL[GTC]: trigger based on gen%d b: %zd",
 17300|                              (i),
 17301|                              get_new_allocation (i)));
 17302|                     break;
 17303|                 }
 17304|             }
 17305|         }
 17306|         for (i = n+1; i <= (check_max_gen_alloc ? max_generation : (max_generation - 1)); i++)
 17307|         {
 17308|             if (get_new_allocation (i) <= 0)
 17309|             {
 17310|                 n = i;
 17311|                 if (n == max_generation)
 17312|                 {
 17313|                     dprintf (BGC_TUNING_LOG, ("BTL[GTC]: trigger based on gen2 b: %zd",
 17314|                             get_new_allocation (max_generation)));
 17315|                 }
 17316|             }
 17317|             else
 17318|                 break;
 17319|         }
 17320|     }
 17321|     if (n > temp_gen)
 17322|     {
 17323|         local_condemn_reasons->set_gen (gen_alloc_budget, n);
 17324|     }
 17325|     dprintf (GTC_LOG, ("h%d: g%d budget", heap_number, ((get_new_allocation (loh_generation) <= 0) ? 3 : n)));
 17326|     n_alloc = n;
 17327| #if defined(BACKGROUND_GC) && !defined(MULTIPLE_HEAPS)
 17328|     int n_time_max = max_generation;
 17329|     if (!check_only_p)
 17330|     {
 17331|         if (!check_max_gen_alloc)
 17332|         {
 17333|             n_time_max = max_generation - 1;
 17334|         }
 17335|     }
 17336|     if ((local_settings->pause_mode == pause_interactive) ||
 17337|         (local_settings->pause_mode == pause_sustained_low_latency))
 17338|     {
 17339|         dynamic_data* dd0 = dynamic_data_of (0);
 17340|         uint64_t now = GetHighPrecisionTimeStamp();
 17341|         temp_gen = n;
 17342|         for (i = (temp_gen+1); i <= n_time_max; i++)
 17343|         {
 17344|             dynamic_data* dd = dynamic_data_of (i);
 17345|             if ((now > dd_time_clock(dd) + dd_time_clock_interval(dd)) &&
 17346|                 (dd_gc_clock (dd0) > (dd_gc_clock (dd) + dd_gc_clock_interval(dd))) &&
 17347|                 ((n < max_generation) || ((dd_current_size (dd) < dd_max_size (dd0)))))
 17348|             {
 17349|                 n = min (i, n_time_max);
 17350|                 dprintf (GTC_LOG, ("time %d", n));
 17351|             }
 17352|         }
 17353|         if (n > temp_gen)
 17354|         {
 17355|             local_condemn_reasons->set_gen (gen_time_tuning, n);
 17356|             if (n == max_generation)
 17357|             {
 17358|                 dprintf (BGC_TUNING_LOG, ("BTL[GTC]: trigger based on time"));
 17359|             }
 17360|         }
 17361|     }
 17362|     if (n != n_alloc)
 17363|     {
 17364|         dprintf (GTC_LOG, ("Condemning %d based on time tuning and fragmentation", n));
 17365|     }
 17366| #endif //BACKGROUND_GC && !MULTIPLE_HEAPS
 17367|     if (n < (max_generation - 1))
 17368|     {
 17369|         if (dt_low_card_table_efficiency_p (tuning_deciding_condemned_gen))
 17370|         {
 17371|             n = max (n, max_generation - 1);
 17372|             local_settings->promotion = TRUE;
 17373|             dprintf (GTC_LOG, ("h%d: skip %d, c %d",
 17374|                         heap_number, generation_skip_ratio, n));
 17375|             local_condemn_reasons->set_condition (gen_low_card_p);
 17376|         }
 17377|     }
 17378|     if (!check_only_p)
 17379|     {
 17380|         generation_skip_ratio = 100;
 17381|     }
 17382|     if (dt_low_ephemeral_space_p (check_only_p ?
 17383|                                   tuning_deciding_full_gc :
 17384|                                   tuning_deciding_condemned_gen))
 17385|     {
 17386|         low_ephemeral_space = TRUE;
 17387|         n = max (n, max_generation - 1);
 17388|         local_condemn_reasons->set_condition (gen_low_ephemeral_p);
 17389|         dprintf (GTC_LOG, ("h%d: low eph", heap_number));
 17390|         if (!provisional_mode_triggered)
 17391|         {
 17392| #ifdef BACKGROUND_GC
 17393|             if (!gc_can_use_concurrent || (generation_free_list_space (generation_of (max_generation)) == 0))
 17394| #endif //BACKGROUND_GC
 17395|             {
 17396|                 if (dt_high_frag_p (tuning_deciding_condemned_gen,
 17397|                                     max_generation - 1,
 17398|                                     TRUE))
 17399|                 {
 17400|                     high_fragmentation = TRUE;
 17401|                     local_condemn_reasons->set_condition (gen_max_high_frag_e_p);
 17402|                     dprintf (GTC_LOG, ("heap%d: gen1 frag", heap_number));
 17403|                 }
 17404|             }
 17405|         }
 17406|     }
 17407| #ifdef USE_REGIONS
 17408|     if (!check_only_p)
 17409|     {
 17410|         if (!try_get_new_free_region())
 17411|         {
 17412|             dprintf (GTC_LOG, ("can't get an empty region -> full compacting"));
 17413|             last_gc_before_oom = TRUE;
 17414|         }
 17415|     }
 17416| #endif //USE_REGIONS
 17417|     temp_gen = n;
 17418|     for (i = n+1; i < max_generation; i++)
 17419|     {
 17420|         if (dt_high_frag_p (tuning_deciding_condemned_gen, i))
 17421|         {
 17422|             dprintf (GTC_LOG, ("h%d g%d too frag", heap_number, i));
 17423|             n = i;
 17424|         }
 17425|         else
 17426|             break;
 17427|     }
 17428|     if (low_ephemeral_space)
 17429|     {
 17430|         local_settings->promotion = TRUE;
 17431|     }
 17432|     if (n > temp_gen)
 17433|     {
 17434|         local_condemn_reasons->set_condition (gen_eph_high_frag_p);
 17435|     }
 17436|     if (!check_only_p)
 17437|     {
 17438|         if (settings.pause_mode == pause_low_latency)
 17439|         {
 17440|             if (!is_induced (settings.reason))
 17441|             {
 17442|                 n = min (n, max_generation - 1);
 17443|                 dprintf (GTC_LOG, ("low latency mode is enabled, condemning %d", n));
 17444|                 evaluate_elevation = FALSE;
 17445|                 goto exit;
 17446|             }
 17447|         }
 17448|     }
 17449|     check_memory = (check_only_p ?
 17450|                     (n >= 0) :
 17451|                     ((n >= 1) || low_memory_detected));
 17452|     if (check_memory)
 17453|     {
 17454|         get_memory_info (&memory_load, &available_physical, &available_page_file);
 17455|         if (heap_number == 0)
 17456|         {
 17457|             dprintf (GTC_LOG, ("ml: %d", memory_load));
 17458|         }
 17459| #ifdef USE_REGIONS
 17460|         uint32_t va_memory_load = global_region_allocator.get_va_memory_load();
 17461|         if (heap_number == 0)
 17462|         {
 17463|             dprintf (GTC_LOG, ("h%d ML %d, va ML %d", heap_number, memory_load, va_memory_load));
 17464|         }
 17465|         memory_load = max (memory_load, va_memory_load);
 17466| #endif //USE_REGIONS
 17467|         local_settings->entry_available_physical_mem = available_physical;
 17468|         local_settings->entry_memory_load = memory_load;
 17469|         if (memory_load >= high_memory_load_th || low_memory_detected)
 17470|         {
 17471| #ifdef SIMPLE_DPRINTF
 17472|             if (heap_number == 0)
 17473|             {
 17474|                 dprintf (GTC_LOG, ("tp: %zd, ap: %zd", total_physical_mem, available_physical));
 17475|             }
 17476| #endif //SIMPLE_DPRINTF
 17477|             high_memory_load = TRUE;
 17478|             if (memory_load >= v_high_memory_load_th || low_memory_detected)
 17479|             {
 17480|                 if (!high_fragmentation)
 17481|                 {
 17482|                     high_fragmentation = dt_estimate_reclaim_space_p (tuning_deciding_condemned_gen, max_generation);
 17483|                 }
 17484|                 v_high_memory_load = TRUE;
 17485|             }
 17486|             else
 17487|             {
 17488|                 if (!high_fragmentation)
 17489|                 {
 17490|                     high_fragmentation = dt_estimate_high_frag_p (tuning_deciding_condemned_gen, max_generation, available_physical);
 17491|                 }
 17492|             }
 17493|             if (high_fragmentation)
 17494|             {
 17495|                 if (high_memory_load)
 17496|                 {
 17497|                     local_condemn_reasons->set_condition (gen_max_high_frag_m_p);
 17498|                 }
 17499|                 else if (v_high_memory_load)
 17500|                 {
 17501|                     local_condemn_reasons->set_condition (gen_max_high_frag_vm_p);
 17502|                 }
 17503|             }
 17504|         }
 17505|     }
 17506|     dprintf (GTC_LOG, ("h%d: le: %d, hm: %d, vm: %d, f: %d",
 17507|                  heap_number, low_ephemeral_space, high_memory_load, v_high_memory_load,
 17508|                  high_fragmentation));
 17509| #ifndef USE_REGIONS
 17510|     if (should_expand_in_full_gc)
 17511|     {
 17512|         dprintf (GTC_LOG, ("h%d: expand_in_full - BLOCK", heap_number));
 17513|         *blocking_collection_p = TRUE;
 17514|         evaluate_elevation = FALSE;
 17515|         n = max_generation;
 17516|         local_condemn_reasons->set_condition (gen_expand_fullgc_p);
 17517|     }
 17518| #endif //!USE_REGIONS
 17519|     if (last_gc_before_oom)
 17520|     {
 17521|         dprintf (GTC_LOG, ("h%d: alloc full - BLOCK", heap_number));
 17522|         n = max_generation;
 17523|         *blocking_collection_p = TRUE;
 17524|         if ((local_settings->reason == reason_oos_loh) ||
 17525|             (local_settings->reason == reason_alloc_loh))
 17526|         {
 17527|             evaluate_elevation = FALSE;
 17528|         }
 17529|         local_condemn_reasons->set_condition (gen_before_oom);
 17530|     }
 17531|     if (!check_only_p)
 17532|     {
 17533|         if (is_induced_blocking (settings.reason) &&
 17534|             n_initial == max_generation
 17535|             IN_STRESS_HEAP( && !settings.stress_induced ))
 17536|         {
 17537|             if (heap_number == 0)
 17538|             {
 17539|                 dprintf (GTC_LOG, ("induced - BLOCK"));
 17540|             }
 17541|             *blocking_collection_p = TRUE;
 17542|             local_condemn_reasons->set_condition (gen_induced_fullgc_p);
 17543|             evaluate_elevation = FALSE;
 17544|         }
 17545|         if (settings.reason == reason_induced_noforce)
 17546|         {
 17547|             local_condemn_reasons->set_condition (gen_induced_noforce_p);
 17548|             evaluate_elevation = FALSE;
 17549|         }
 17550|     }
 17551|     if (!provisional_mode_triggered && evaluate_elevation && (low_ephemeral_space || high_memory_load || v_high_memory_load))
 17552|     {
 17553|         *elevation_requested_p = TRUE;
 17554| #ifdef HOST_64BIT
 17555|         if (high_memory_load || v_high_memory_load)
 17556|         {
 17557|             dynamic_data* dd_max = dynamic_data_of (max_generation);
 17558|             if (((float)dd_new_allocation (dd_max) / (float)dd_desired_allocation (dd_max)) < 0.9)
 17559|             {
 17560|                 dprintf (GTC_LOG, ("%zd left in gen2 alloc (%zd)",
 17561|                     dd_new_allocation (dd_max), dd_desired_allocation (dd_max)));
 17562|                 n = max_generation;
 17563|                 local_condemn_reasons->set_condition (gen_almost_max_alloc);
 17564|             }
 17565|         }
 17566|         if (n <= max_generation)
 17567|         {
 17568| #endif // HOST_64BIT
 17569|             if (high_fragmentation)
 17570|             {
 17571|                 n = max_generation;
 17572|                 dprintf (GTC_LOG, ("h%d: f full", heap_number));
 17573| #ifdef BACKGROUND_GC
 17574|                 if (high_memory_load || v_high_memory_load)
 17575|                 {
 17576|                     dprintf (GTC_LOG, ("h%d: bgc - BLOCK", heap_number));
 17577|                     *blocking_collection_p = TRUE;
 17578|                 }
 17579| #else
 17580|                 if (v_high_memory_load)
 17581|                 {
 17582|                     dprintf (GTC_LOG, ("h%d: - BLOCK", heap_number));
 17583|                     *blocking_collection_p = TRUE;
 17584|                 }
 17585| #endif //BACKGROUND_GC
 17586|             }
 17587|             else
 17588|             {
 17589|                 n = max (n, max_generation - 1);
 17590|                 dprintf (GTC_LOG, ("h%d: nf c %d", heap_number, n));
 17591|             }
 17592| #ifdef HOST_64BIT
 17593|         }
 17594| #endif // HOST_64BIT
 17595|     }
 17596|     if (!provisional_mode_triggered && (n == (max_generation - 1)) && (n_alloc < (max_generation -1)))
 17597|     {
 17598| #ifdef BGC_SERVO_TUNING
 17599|         if (!bgc_tuning::enable_fl_tuning)
 17600| #endif //BGC_SERVO_TUNING
 17601|         {
 17602|             dprintf (GTC_LOG, ("h%d: budget %d, check 2",
 17603|                         heap_number, n_alloc));
 17604|             if (get_new_allocation (max_generation) <= 0)
 17605|             {
 17606|                 dprintf (GTC_LOG, ("h%d: budget alloc", heap_number));
 17607|                 n = max_generation;
 17608|                 local_condemn_reasons->set_condition (gen_max_gen1);
 17609|             }
 17610|         }
 17611|     }
 17612|     if (!provisional_mode_triggered
 17613| #ifdef BGC_SERVO_TUNING
 17614|         && !bgc_tuning::enable_fl_tuning
 17615| #endif //BGC_SERVO_TUNING
 17616|         && (n == max_generation))
 17617|     {
 17618|         if (dt_high_frag_p (tuning_deciding_condemned_gen, n))
 17619|         {
 17620|             dprintf (GTC_LOG, ("h%d: g%d too frag", heap_number, n));
 17621|             local_condemn_reasons->set_condition (gen_max_high_frag_p);
 17622|             if (local_settings->pause_mode != pause_sustained_low_latency)
 17623|             {
 17624|                 *blocking_collection_p = TRUE;
 17625|             }
 17626|         }
 17627|     }
 17628| #ifdef BACKGROUND_GC
 17629|     if ((n == max_generation) && !(*blocking_collection_p))
 17630|     {
 17631|         if (heap_number == 0)
 17632|         {
 17633|             BOOL bgc_heap_too_small = TRUE;
 17634|             size_t gen2size = 0;
 17635|             size_t gen3size = 0;
 17636| #ifdef MULTIPLE_HEAPS
 17637|             for (int i = 0; i < n_heaps; i++)
 17638|             {
 17639|                 if (((g_heaps[i]->current_generation_size (max_generation)) > bgc_min_per_heap) ||
 17640|                     ((g_heaps[i]->current_generation_size (loh_generation)) > bgc_min_per_heap) ||
 17641|                     ((g_heaps[i]->current_generation_size (poh_generation)) > bgc_min_per_heap))
 17642|                 {
 17643|                     bgc_heap_too_small = FALSE;
 17644|                     break;
 17645|                 }
 17646|             }
 17647| #else //MULTIPLE_HEAPS
 17648|             if ((current_generation_size (max_generation) > bgc_min_per_heap) ||
 17649|                 (current_generation_size (loh_generation) > bgc_min_per_heap) ||
 17650|                 (current_generation_size (poh_generation) > bgc_min_per_heap))
 17651|             {
 17652|                 bgc_heap_too_small = FALSE;
 17653|             }
 17654| #endif //MULTIPLE_HEAPS
 17655|             if (bgc_heap_too_small)
 17656|             {
 17657|                 dprintf (GTC_LOG, ("gen2 and gen3 too small"));
 17658| #ifdef STRESS_HEAP
 17659|                 if (!settings.stress_induced)
 17660| #endif //STRESS_HEAP
 17661|                 {
 17662|                     *blocking_collection_p = TRUE;
 17663|                 }
 17664|                 local_condemn_reasons->set_condition (gen_gen2_too_small);
 17665|             }
 17666|         }
 17667|     }
 17668| #endif //BACKGROUND_GC
 17669| exit:
 17670|     if (!check_only_p)
 17671|     {
 17672| #ifdef STRESS_HEAP
 17673| #ifdef BACKGROUND_GC
 17674|         if (orig_gen != max_generation &&
 17675|             g_pConfig->GetGCStressLevel() && gc_can_use_concurrent)
 17676|         {
 17677|             *elevation_requested_p = FALSE;
 17678|         }
 17679| #endif //BACKGROUND_GC
 17680| #endif //STRESS_HEAP
 17681|         if (check_memory)
 17682|         {
 17683|             fgm_result.available_pagefile_mb = (size_t)(available_page_file / (1024 * 1024));
 17684|         }
 17685|         local_condemn_reasons->set_gen (gen_final_per_heap, n);
 17686|         get_gc_data_per_heap()->gen_to_condemn_reasons.init (local_condemn_reasons);
 17687| #ifdef DT_LOG
 17688|         local_condemn_reasons->print (heap_number);
 17689| #endif //DT_LOG
 17690|         if ((local_settings->reason == reason_oos_soh) ||
 17691|             (local_settings->reason == reason_oos_loh))
 17692|         {
 17693|             assert (n >= 1);
 17694|         }
 17695|     }
 17696|     return n;
 17697| }
 17698| #ifdef _PREFAST_
 17699| #pragma warning(pop)
 17700| #endif //_PREFAST_
 17701| inline
 17702| size_t gc_heap::min_reclaim_fragmentation_threshold (uint32_t num_heaps)
 17703| {
 17704|     size_t min_mem_based_on_available =
 17705|         (500 - (settings.entry_memory_load - high_memory_load_th) * 40) * 1024 * 1024 / num_heaps;
 17706|     size_t ten_percent_size = (size_t)((float)generation_size (max_generation) * 0.10);
 17707|     uint64_t three_percent_mem = mem_one_percent * 3 / num_heaps;
 17708| #ifdef SIMPLE_DPRINTF
 17709|     dprintf (GTC_LOG, ("min av: %zd, 10%% gen2: %zd, 3%% mem: %zd",
 17710|         min_mem_based_on_available, ten_percent_size, three_percent_mem));
 17711| #endif //SIMPLE_DPRINTF
 17712|     return (size_t)(min (min_mem_based_on_available, min (ten_percent_size, three_percent_mem)));
 17713| }
 17714| inline
 17715| uint64_t gc_heap::min_high_fragmentation_threshold(uint64_t available_mem, uint32_t num_heaps)
 17716| {
 17717|     return min (available_mem, (256*1024*1024)) / num_heaps;
 17718| }
 17719| enum {
 17720| CORINFO_EXCEPTION_GC = 0xE0004743 // 'GC'
 17721| };
 17722| #ifdef BACKGROUND_GC
 17723| void gc_heap::init_background_gc ()
 17724| {
 17725|     generation* gen = generation_of (max_generation);
 17726|     generation_allocation_pointer (gen)= 0;
 17727|     generation_allocation_limit (gen) = 0;
 17728|     generation_allocation_segment (gen) = heap_segment_rw (generation_start_segment (gen));
 17729|     PREFIX_ASSUME(generation_allocation_segment(gen) != NULL);
 17730| #ifdef DOUBLY_LINKED_FL
 17731|     generation_set_bgc_mark_bit_p (gen) = FALSE;
 17732| #endif //DOUBLY_LINKED_FL
 17733| #ifndef USE_REGIONS
 17734|     for (heap_segment* seg = generation_allocation_segment (gen); seg != ephemeral_heap_segment;
 17735|         seg = heap_segment_next_rw (seg))
 17736|     {
 17737|         heap_segment_plan_allocated (seg) = heap_segment_allocated (seg);
 17738|     }
 17739| #endif //!USE_REGIONS
 17740|     if (heap_number == 0)
 17741|     {
 17742|         dprintf (2, ("heap%d: bgc lowest: %p, highest: %p",
 17743|             heap_number,
 17744|             background_saved_lowest_address,
 17745|             background_saved_highest_address));
 17746|     }
 17747| }
 17748| #endif //BACKGROUND_GC
 17749| inline
 17750| void fire_drain_mark_list_event (size_t mark_list_objects)
 17751| {
 17752|     FIRE_EVENT(BGCDrainMark, mark_list_objects);
 17753| }
 17754| inline
 17755| void fire_revisit_event (size_t dirtied_pages,
 17756|                          size_t marked_objects,
 17757|                          BOOL large_objects_p)
 17758| {
 17759|     FIRE_EVENT(BGCRevisit, dirtied_pages, marked_objects, large_objects_p);
 17760| }
 17761| inline
 17762| void fire_overflow_event (uint8_t* overflow_min,
 17763|                           uint8_t* overflow_max,
 17764|                           size_t marked_objects,
 17765|                           int gen_number)
 17766| {
 17767|     FIRE_EVENT(BGCOverflow_V1, (uint64_t)overflow_min, (uint64_t)overflow_max, marked_objects, gen_number == loh_generation, gen_number);
 17768| }
 17769| void gc_heap::concurrent_print_time_delta (const char* msg)
 17770| {
 17771| #ifdef TRACE_GC
 17772|     uint64_t current_time = GetHighPrecisionTimeStamp();
 17773|     size_t elapsed_time_ms = (size_t)((current_time - time_bgc_last) / 1000);
 17774|     time_bgc_last = current_time;
 17775|     dprintf (2, ("h%d: %s T %zd ms", heap_number, msg, elapsed_time_ms));
 17776| #else
 17777|     UNREFERENCED_PARAMETER(msg);
 17778| #endif //TRACE_GC
 17779| }
 17780| void gc_heap::free_list_info (int gen_num, const char* msg)
 17781| {
 17782| #if defined (BACKGROUND_GC) && defined (TRACE_GC)
 17783|     dprintf (3, ("h%d: %s", heap_number, msg));
 17784|     for (int i = 0; i < total_generation_count; i++)
 17785|     {
 17786|         generation* gen = generation_of (i);
 17787|         if ((generation_allocation_size (gen) == 0) &&
 17788|             (generation_free_list_space (gen) == 0) &&
 17789|             (generation_free_obj_space (gen) == 0))
 17790|         {
 17791|         }
 17792|         else
 17793|         {
 17794|             dprintf (3, ("h%d: g%d: a-%zd, fl-%zd, fo-%zd",
 17795|                 heap_number, i,
 17796|                 generation_allocation_size (gen),
 17797|                 generation_free_list_space (gen),
 17798|                 generation_free_obj_space (gen)));
 17799|         }
 17800|     }
 17801| #else
 17802|     UNREFERENCED_PARAMETER(gen_num);
 17803|     UNREFERENCED_PARAMETER(msg);
 17804| #endif // BACKGROUND_GC && TRACE_GC
 17805| }
 17806| void gc_heap::update_collection_counts_for_no_gc()
 17807| {
 17808|     assert (settings.pause_mode == pause_no_gc);
 17809|     settings.condemned_generation = max_generation;
 17810| #ifdef MULTIPLE_HEAPS
 17811|     for (int i = 0; i < n_heaps; i++)
 17812|         g_heaps[i]->update_collection_counts();
 17813| #else //MULTIPLE_HEAPS
 17814|     update_collection_counts();
 17815| #endif //MULTIPLE_HEAPS
 17816|     full_gc_counts[gc_type_blocking]++;
 17817| }
 17818| BOOL gc_heap::should_proceed_with_gc()
 17819| {
 17820|     if (gc_heap::settings.pause_mode == pause_no_gc)
 17821|     {
 17822|         if (current_no_gc_region_info.started)
 17823|         {
 17824|             if (current_no_gc_region_info.soh_withheld_budget != 0)
 17825|             {
 17826|                 dprintf(1, ("[no_gc_callback] allocation budget exhausted with withheld, time to trigger callback\n"));
 17827| #ifdef MULTIPLE_HEAPS
 17828|                 for (int i = 0; i < gc_heap::n_heaps; i++)
 17829|                 {
 17830|                     gc_heap* hp = gc_heap::g_heaps [i];
 17831| #else
 17832|                 {
 17833|                     gc_heap* hp = pGenGCHeap;
 17834| #endif
 17835|                     dd_new_allocation (hp->dynamic_data_of (soh_gen0)) += current_no_gc_region_info.soh_withheld_budget;
 17836|                     dd_new_allocation (hp->dynamic_data_of (loh_generation)) += current_no_gc_region_info.loh_withheld_budget;
 17837|                 }
 17838|                 current_no_gc_region_info.soh_withheld_budget = 0;
 17839|                 current_no_gc_region_info.loh_withheld_budget = 0;
 17840|                 schedule_no_gc_callback (false);
 17841|                 current_no_gc_region_info.callback = nullptr;
 17842|                 return FALSE;
 17843|             }
 17844|             else
 17845|             {
 17846|                 dprintf(1, ("[no_gc_callback] GC triggered while in no_gc mode. Exiting no_gc mode.\n"));
 17847|                 restore_data_for_no_gc();
 17848|                 if (current_no_gc_region_info.callback != nullptr)
 17849|                 {
 17850|                     dprintf (1, ("[no_gc_callback] detaching callback on exit"));
 17851|                     schedule_no_gc_callback (true);
 17852|                 }
 17853|                 memset (&current_no_gc_region_info, 0, sizeof (current_no_gc_region_info));
 17854|             }
 17855|         }
 17856|         else
 17857|             return should_proceed_for_no_gc();
 17858|     }
 17859|     return TRUE;
 17860| }
 17861| void gc_heap::update_end_gc_time_per_heap()
 17862| {
 17863| #ifdef DYNAMIC_HEAP_COUNT
 17864|     size_t prev_gen2_end_time = 0;
 17865|     if ((heap_number == 0) && (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes) && (settings.condemned_generation == max_generation))
 17866|     {
 17867|         dynamic_data* dd = dynamic_data_of (max_generation);
 17868|         prev_gen2_end_time = dd_previous_time_clock (dd) + dd_gc_elapsed_time (dd);;
 17869|     }
 17870| #endif //DYNAMIC_HEAP_COUNT
 17871|     for (int gen_number = 0; gen_number <= settings.condemned_generation; gen_number++)
 17872|     {
 17873|         dynamic_data* dd = dynamic_data_of (gen_number);
 17874|         if (heap_number == 0)
 17875|         {
 17876|             dprintf (6666, ("prev gen%d GC end time: prev start %I64d + prev gc elapsed %Id = %I64d",
 17877|                 gen_number, dd_previous_time_clock (dd), dd_gc_elapsed_time (dd), (dd_previous_time_clock (dd) + dd_gc_elapsed_time (dd))));
 17878|         }
 17879|         dd_gc_elapsed_time (dd) = (size_t)(end_gc_time - dd_time_clock (dd));
 17880|         if (heap_number == 0)
 17881|         {
 17882|             dprintf (6666, ("updated NGC%d %Id elapsed time to %I64d - %I64d = %I64d", gen_number, dd_gc_clock (dd), end_gc_time, dd_time_clock (dd), dd_gc_elapsed_time (dd)));
 17883|         }
 17884|     }
 17885| #ifdef DYNAMIC_HEAP_COUNT
 17886|     if ((heap_number == 0) && (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes))
 17887|     {
 17888|         dynamic_heap_count_data_t::sample& sample = dynamic_heap_count_data.samples[dynamic_heap_count_data.sample_index];
 17889|         sample.elapsed_between_gcs = end_gc_time - last_suspended_end_time;
 17890|         sample.gc_pause_time = dd_gc_elapsed_time (dynamic_data_of (0));
 17891|         sample.msl_wait_time = get_msl_wait_time();
 17892|         dprintf (6666, ("sample#%d: this GC end %I64d - last sus end %I64d = %I64d, this GC pause %I64d, msl wait %I64d",
 17893|             dynamic_heap_count_data.sample_index, end_gc_time, last_suspended_end_time, sample.elapsed_between_gcs, sample.gc_pause_time, sample.msl_wait_time));
 17894|         last_suspended_end_time = end_gc_time;
 17895|         GCEventFireHeapCountSample_V1 (
 17896|             (uint64_t)VolatileLoadWithoutBarrier (&settings.gc_index),
 17897|             sample.elapsed_between_gcs,
 17898|             sample.gc_pause_time,
 17899|             sample.msl_wait_time);
 17900|         dynamic_heap_count_data.sample_index = (dynamic_heap_count_data.sample_index + 1) % dynamic_heap_count_data_t::sample_size;
 17901|         if (settings.condemned_generation == max_generation)
 17902|         {
 17903|             gc_index_full_gc_end = dd_gc_clock (dynamic_data_of (0));
 17904|             size_t elapsed_between_gen2_gcs = end_gc_time - prev_gen2_end_time;
 17905|             size_t gen2_elapsed_time = sample.gc_pause_time;
 17906|             dynamic_heap_count_data.gen2_gc_percents[dynamic_heap_count_data.gen2_sample_index] = (float)gen2_elapsed_time * 100.0f / elapsed_between_gen2_gcs;
 17907|             dprintf (6666, ("gen2 sample#%d: this GC end %I64d - last gen2 end %I64d = %I64d, GC elapsed %I64d, percent %.3f",
 17908|                 dynamic_heap_count_data.gen2_sample_index, end_gc_time, prev_gen2_end_time, elapsed_between_gen2_gcs,
 17909|                 gen2_elapsed_time, dynamic_heap_count_data.gen2_gc_percents[dynamic_heap_count_data.gen2_sample_index]));
 17910|             dynamic_heap_count_data.gen2_sample_index = (dynamic_heap_count_data.gen2_sample_index + 1) % dynamic_heap_count_data_t::sample_size;
 17911|         }
 17912|         calculate_new_heap_count ();
 17913|     }
 17914| #endif //DYNAMIC_HEAP_COUNT
 17915| }
 17916| void gc_heap::update_end_ngc_time()
 17917| {
 17918|     end_gc_time = GetHighPrecisionTimeStamp();
 17919| #ifdef HEAP_BALANCE_INSTRUMENTATION
 17920|     last_gc_end_time_us = end_gc_time;
 17921|     dprintf (HEAP_BALANCE_LOG, ("[GC#%zd-%zd-%zd]", settings.gc_index,
 17922|         (last_gc_end_time_us - dd_time_clock (dynamic_data_of (0))),
 17923|         dd_time_clock (dynamic_data_of (0))));
 17924| #endif //HEAP_BALANCE_INSTRUMENTATION
 17925| }
 17926| size_t gc_heap::exponential_smoothing (int gen, size_t collection_count, size_t desired_per_heap)
 17927| {
 17928|     size_t smoothing = min(3, collection_count);
 17929|     size_t desired_total = desired_per_heap * n_heaps;
 17930|     size_t new_smoothed_desired_total = desired_total / smoothing + ((smoothed_desired_total[gen] / smoothing) * (smoothing - 1));
 17931|     smoothed_desired_total[gen] = new_smoothed_desired_total;
 17932|     size_t new_smoothed_desired_per_heap = new_smoothed_desired_total / n_heaps;
 17933| #ifdef MULTIPLE_HEAPS
 17934|     gc_heap* hp = g_heaps[0];
 17935| #else //MULTIPLE_HEAPS
 17936|     gc_heap* hp = pGenGCHeap;
 17937| #endif //MULTIPLE_HEAPS
 17938|     dynamic_data* dd = hp->dynamic_data_of (gen);
 17939|     new_smoothed_desired_per_heap = max (new_smoothed_desired_per_heap, dd_min_size (dd));
 17940|     new_smoothed_desired_per_heap = Align (new_smoothed_desired_per_heap, get_alignment_constant (gen <= soh_gen2));
 17941|     dprintf (2, ("new smoothed_desired_per_heap for gen %d = %zd, desired_per_heap = %zd", gen, new_smoothed_desired_per_heap, desired_per_heap));
 17942|     return new_smoothed_desired_per_heap;
 17943| }
 17944| void gc_heap::gc1()
 17945| {
 17946| #ifdef BACKGROUND_GC
 17947|     assert (settings.concurrent == (uint32_t)(bgc_thread_id.IsCurrentThread()));
 17948| #endif //BACKGROUND_GC
 17949|     verify_soh_segment_list();
 17950|     int n = settings.condemned_generation;
 17951|     if (settings.reason == reason_pm_full_gc)
 17952|     {
 17953|         assert (n == max_generation);
 17954|         init_records();
 17955|         gen_to_condemn_tuning* local_condemn_reasons = &(get_gc_data_per_heap()->gen_to_condemn_reasons);
 17956|         local_condemn_reasons->init();
 17957|         local_condemn_reasons->set_gen (gen_initial, n);
 17958|         local_condemn_reasons->set_gen (gen_final_per_heap, n);
 17959|     }
 17960|     update_collection_counts ();
 17961| #ifdef BACKGROUND_GC
 17962|     bgc_alloc_lock->check();
 17963| #endif //BACKGROUND_GC
 17964|     free_list_info (max_generation, "beginning");
 17965|     vm_heap->GcCondemnedGeneration = settings.condemned_generation;
 17966|     assert (g_gc_card_table == card_table);
 17967| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
 17968|     assert (g_gc_card_bundle_table == card_bundle_table);
 17969| #endif
 17970|     {
 17971| #ifndef USE_REGIONS
 17972|         if (n == max_generation)
 17973|         {
 17974|             gc_low = lowest_address;
 17975|             gc_high = highest_address;
 17976|         }
 17977|         else
 17978|         {
 17979|             gc_low = generation_allocation_start (generation_of (n));
 17980|             gc_high = heap_segment_reserved (ephemeral_heap_segment);
 17981|         }
 17982| #endif //USE_REGIONS
 17983| #ifdef BACKGROUND_GC
 17984|         if (settings.concurrent)
 17985|         {
 17986| #ifdef TRACE_GC
 17987|             time_bgc_last = GetHighPrecisionTimeStamp();
 17988| #endif //TRACE_GC
 17989|             FIRE_EVENT(BGCBegin);
 17990|             concurrent_print_time_delta ("BGC");
 17991|             concurrent_print_time_delta ("RW");
 17992|             background_mark_phase();
 17993|             free_list_info (max_generation, "after mark phase");
 17994|             background_sweep();
 17995|             free_list_info (max_generation, "after sweep phase");
 17996|         }
 17997|         else
 17998| #endif //BACKGROUND_GC
 17999|         {
 18000|             mark_phase (n, FALSE);
 18001|             check_gen0_bricks();
 18002|             GCScan::GcRuntimeStructuresValid (FALSE);
 18003|             plan_phase (n);
 18004|             GCScan::GcRuntimeStructuresValid (TRUE);
 18005|             check_gen0_bricks();
 18006|         }
 18007|     }
 18008|     for (int gen_number = 0; gen_number <= min (max_generation,n+1); gen_number++)
 18009|     {
 18010|         generation* gn = generation_of (gen_number);
 18011|         if (settings.compaction)
 18012|         {
 18013|             generation_allocation_size (generation_of (gen_number)) += generation_pinned_allocation_compact_size (gn);
 18014|         }
 18015|         else
 18016|         {
 18017|             generation_allocation_size (generation_of (gen_number)) += generation_pinned_allocation_sweep_size (gn);
 18018|         }
 18019|         generation_pinned_allocation_sweep_size (gn) = 0;
 18020|         generation_pinned_allocation_compact_size (gn) = 0;
 18021|     }
 18022| #ifdef BACKGROUND_GC
 18023|     if (settings.concurrent)
 18024|     {
 18025|         dynamic_data* dd = dynamic_data_of (n);
 18026|         end_gc_time = GetHighPrecisionTimeStamp();
 18027|         size_t time_since_last_gen2 = 0;
 18028| #ifdef DYNAMIC_HEAP_COUNT
 18029|         if ((heap_number == 0) && (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes))
 18030|         {
 18031|             time_since_last_gen2 = (size_t)(end_gc_time - (dd_previous_time_clock (dd) + dd_gc_elapsed_time (dd)));
 18032|             dprintf (6666, ("BGC %Id end %I64d - (prev gen2 start %I64d + elapsed %Id = %I64d) = time inbewteen gen2 %Id",
 18033|                 dd_gc_clock (dd), end_gc_time, dd_previous_time_clock (dd), dd_gc_elapsed_time (dd), (dd_previous_time_clock (dd) + dd_gc_elapsed_time (dd)), time_since_last_gen2));
 18034|         }
 18035| #endif //DYNAMIC_HEAP_COUNT
 18036|         dd_gc_elapsed_time (dd) = (size_t)(end_gc_time - dd_time_clock (dd));
 18037| #ifdef DYNAMIC_HEAP_COUNT
 18038|         if ((heap_number == 0) && (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes))
 18039|         {
 18040|             dprintf (6666, ("updating BGC %Id elapsed time to %I64d - %I64d = %I64d", dd_gc_clock (dd), end_gc_time, dd_time_clock (dd), dd_gc_elapsed_time (dd)));
 18041|             float bgc_percent = (float)dd_gc_elapsed_time (dd) * 100.0f / (float)time_since_last_gen2;
 18042|             dynamic_heap_count_data.gen2_gc_percents[dynamic_heap_count_data.gen2_sample_index] = bgc_percent;
 18043|             dprintf (6666, ("gen2 sample %d elapsed %Id * 100 / time inbetween gen2 %Id = %.3f",
 18044|                 dynamic_heap_count_data.gen2_sample_index, dd_gc_elapsed_time (dd), time_since_last_gen2, bgc_percent));
 18045|             dynamic_heap_count_data.gen2_sample_index = (dynamic_heap_count_data.gen2_sample_index + 1) % dynamic_heap_count_data_t::sample_size;
 18046|             gc_index_full_gc_end = dd_gc_clock (dynamic_data_of (0));
 18047|         }
 18048| #endif //DYNAMIC_HEAP_COUNT
 18049| #ifdef HEAP_BALANCE_INSTRUMENTATION
 18050|         if (heap_number == 0)
 18051|         {
 18052|             last_gc_end_time_us = end_gc_time;
 18053|             dprintf (HEAP_BALANCE_LOG, ("[GC#%zd-%zd-BGC]", settings.gc_index, dd_gc_elapsed_time (dd)));
 18054|         }
 18055| #endif //HEAP_BALANCE_INSTRUMENTATION
 18056|         free_list_info (max_generation, "after computing new dynamic data");
 18057|         gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
 18058|         for (int gen_number = 0; gen_number < max_generation; gen_number++)
 18059|         {
 18060|             dprintf (2, ("end of BGC: gen%d new_alloc: %zd",
 18061|                          gen_number, dd_desired_allocation (dynamic_data_of (gen_number))));
 18062|             current_gc_data_per_heap->gen_data[gen_number].size_after = generation_size (gen_number);
 18063|             current_gc_data_per_heap->gen_data[gen_number].free_list_space_after = generation_free_list_space (generation_of (gen_number));
 18064|             current_gc_data_per_heap->gen_data[gen_number].free_obj_space_after = generation_free_obj_space (generation_of (gen_number));
 18065|         }
 18066|     }
 18067|     else
 18068| #endif //BACKGROUND_GC
 18069|     {
 18070|         free_list_info (max_generation, "end");
 18071|         for (int gen_number = 0; gen_number <= n; gen_number++)
 18072|         {
 18073|             compute_new_dynamic_data (gen_number);
 18074|         }
 18075|         if (n != max_generation)
 18076|         {
 18077|             for (int gen_number = (n + 1); gen_number < total_generation_count; gen_number++)
 18078|             {
 18079|                 get_gc_data_per_heap()->gen_data[gen_number].size_after = generation_size (gen_number);
 18080|                 get_gc_data_per_heap()->gen_data[gen_number].free_list_space_after = generation_free_list_space (generation_of (gen_number));
 18081|                 get_gc_data_per_heap()->gen_data[gen_number].free_obj_space_after = generation_free_obj_space (generation_of (gen_number));
 18082|             }
 18083|         }
 18084|         get_gc_data_per_heap()->maxgen_size_info.running_free_list_efficiency = (uint32_t)(generation_allocator_efficiency (generation_of (max_generation)) * 100);
 18085|         free_list_info (max_generation, "after computing new dynamic data");
 18086|     }
 18087|     if (n < max_generation)
 18088|     {
 18089|         int highest_gen_number =
 18090| #ifdef USE_REGIONS
 18091|             max_generation;
 18092| #else //USE_REGIONS
 18093|             1 + n;
 18094| #endif //USE_REGIONS
 18095|         for (int older_gen_idx = (1 + n); older_gen_idx <= highest_gen_number; older_gen_idx++)
 18096|         {
 18097|             compute_promoted_allocation (older_gen_idx);
 18098|             dynamic_data* dd = dynamic_data_of (older_gen_idx);
 18099|             size_t new_fragmentation = generation_free_list_space (generation_of (older_gen_idx)) +
 18100|                                        generation_free_obj_space (generation_of (older_gen_idx));
 18101| #ifdef BACKGROUND_GC
 18102|             if (current_c_gc_state != c_gc_state_planning)
 18103| #endif //BACKGROUND_GC
 18104|             {
 18105|                 if (settings.promotion)
 18106|                 {
 18107|                     dd_fragmentation (dd) = new_fragmentation;
 18108|                 }
 18109|                 else
 18110|                 {
 18111|                 }
 18112|             }
 18113|         }
 18114|     }
 18115| #ifdef BACKGROUND_GC
 18116|     if (!settings.concurrent)
 18117| #endif //BACKGROUND_GC
 18118|     {
 18119| #ifndef FEATURE_NATIVEAOT
 18120|         assert(GCToEEInterface::IsGCThread());
 18121| #endif // FEATURE_NATIVEAOT
 18122|         adjust_ephemeral_limits();
 18123|     }
 18124| #if defined(BACKGROUND_GC) && !defined(USE_REGIONS)
 18125|     assert (ephemeral_low == generation_allocation_start (generation_of ( max_generation -1)));
 18126|     assert (ephemeral_high == heap_segment_reserved (ephemeral_heap_segment));
 18127| #endif //BACKGROUND_GC && !USE_REGIONS
 18128|     if (fgn_maxgen_percent)
 18129|     {
 18130|         if (settings.condemned_generation == (max_generation - 1))
 18131|         {
 18132|             check_for_full_gc (max_generation - 1, 0);
 18133|         }
 18134|         else if (settings.condemned_generation == max_generation)
 18135|         {
 18136|             if (full_gc_approach_event_set
 18137| #ifdef MULTIPLE_HEAPS
 18138|                 && (heap_number == 0)
 18139| #endif //MULTIPLE_HEAPS
 18140|                 )
 18141|             {
 18142|                 dprintf (2, ("FGN-GC: setting gen2 end event"));
 18143|                 full_gc_approach_event.Reset();
 18144| #ifdef BACKGROUND_GC
 18145|                 fgn_last_gc_was_concurrent = settings.concurrent ? TRUE : FALSE;
 18146| #endif //BACKGROUND_GC
 18147|                 full_gc_end_event.Set();
 18148|                 full_gc_approach_event_set = false;
 18149|             }
 18150|         }
 18151|     }
 18152| #ifdef BACKGROUND_GC
 18153|     if (!settings.concurrent)
 18154| #endif //BACKGROUND_GC
 18155|     {
 18156|         if (alloc_contexts_used >= 1)
 18157|         {
 18158|             allocation_quantum = Align (min ((size_t)CLR_SIZE,
 18159|                                             (size_t)max (1024, get_new_allocation (0) / (2 * alloc_contexts_used))),
 18160|                                             get_alignment_constant(FALSE));
 18161|             dprintf (3, ("New allocation quantum: %zd(0x%zx)", allocation_quantum, allocation_quantum));
 18162|         }
 18163|     }
 18164| #ifdef USE_REGIONS
 18165|     if (end_gen0_region_space == uninitialized_end_gen0_region_space)
 18166|     {
 18167|         end_gen0_region_space = get_gen0_end_space (memory_type_reserved);
 18168|     }
 18169| #endif //USE_REGIONS
 18170|     descr_generations ("END");
 18171|     verify_soh_segment_list();
 18172| #ifdef BACKGROUND_GC
 18173|     if (gc_can_use_concurrent)
 18174|     {
 18175|         check_bgc_mark_stack_length();
 18176|     }
 18177|     assert (settings.concurrent == (uint32_t)(bgc_thread_id.IsCurrentThread()));
 18178| #endif //BACKGROUND_GC
 18179| #if defined(VERIFY_HEAP) || (defined (FEATURE_EVENT_TRACE) && defined(BACKGROUND_GC))
 18180|     if (FALSE
 18181| #ifdef VERIFY_HEAP
 18182|         || (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
 18183| #endif
 18184| #if defined(FEATURE_EVENT_TRACE) && defined(BACKGROUND_GC)
 18185|         || (bgc_heap_walk_for_etw_p && settings.concurrent)
 18186| #endif
 18187|         )
 18188|     {
 18189| #ifdef BACKGROUND_GC
 18190|         bool cooperative_mode = true;
 18191|         if (settings.concurrent)
 18192|         {
 18193|             cooperative_mode = enable_preemptive ();
 18194| #ifdef MULTIPLE_HEAPS
 18195|             bgc_t_join.join(this, gc_join_suspend_ee_verify);
 18196|             if (bgc_t_join.joined())
 18197|             {
 18198|                 bgc_threads_sync_event.Reset();
 18199|                 dprintf(2, ("Joining BGC threads to suspend EE for verify heap"));
 18200|                 bgc_t_join.restart();
 18201|             }
 18202|             if (heap_number == 0)
 18203|             {
 18204|                 enter_gc_lock_for_verify_heap();
 18205|                 suspend_EE();
 18206|                 bgc_threads_sync_event.Set();
 18207|             }
 18208|             else
 18209|             {
 18210|                 bgc_threads_sync_event.Wait(INFINITE, FALSE);
 18211|                 dprintf (2, ("bgc_threads_sync_event is signalled"));
 18212|             }
 18213| #else //MULTIPLE_HEAPS
 18214|             enter_gc_lock_for_verify_heap();
 18215|             suspend_EE();
 18216| #endif //MULTIPLE_HEAPS
 18217|             fix_allocation_contexts (FALSE);
 18218|         }
 18219| #endif //BACKGROUND_GC
 18220| #ifdef BACKGROUND_GC
 18221|         assert (settings.concurrent == (uint32_t)(bgc_thread_id.IsCurrentThread()));
 18222| #ifdef FEATURE_EVENT_TRACE
 18223|         if (bgc_heap_walk_for_etw_p && settings.concurrent)
 18224|         {
 18225|             GCToEEInterface::DiagWalkBGCSurvivors(__this);
 18226| #ifdef MULTIPLE_HEAPS
 18227|             bgc_t_join.join(this, gc_join_after_profiler_heap_walk);
 18228|             if (bgc_t_join.joined())
 18229|             {
 18230|                 bgc_t_join.restart();
 18231|             }
 18232| #endif // MULTIPLE_HEAPS
 18233|         }
 18234| #endif // FEATURE_EVENT_TRACE
 18235| #endif //BACKGROUND_GC
 18236| #ifdef VERIFY_HEAP
 18237|         if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
 18238|             verify_heap (FALSE);
 18239| #endif // VERIFY_HEAP
 18240| #ifdef BACKGROUND_GC
 18241|         if (settings.concurrent)
 18242|         {
 18243|             repair_allocation_contexts (TRUE);
 18244| #ifdef MULTIPLE_HEAPS
 18245|             bgc_t_join.join(this, gc_join_restart_ee_verify);
 18246|             if (bgc_t_join.joined())
 18247|             {
 18248|                 bgc_threads_sync_event.Reset();
 18249|                 dprintf(2, ("Joining BGC threads to restart EE after verify heap"));
 18250|                 bgc_t_join.restart();
 18251|             }
 18252|             if (heap_number == 0)
 18253|             {
 18254|                 restart_EE();
 18255|                 leave_gc_lock_for_verify_heap();
 18256|                 bgc_threads_sync_event.Set();
 18257|             }
 18258|             else
 18259|             {
 18260|                 bgc_threads_sync_event.Wait(INFINITE, FALSE);
 18261|                 dprintf (2, ("bgc_threads_sync_event is signalled"));
 18262|             }
 18263| #else //MULTIPLE_HEAPS
 18264|             restart_EE();
 18265|             leave_gc_lock_for_verify_heap();
 18266| #endif //MULTIPLE_HEAPS
 18267|             disable_preemptive (cooperative_mode);
 18268|         }
 18269| #endif //BACKGROUND_GC
 18270|     }
 18271| #endif //VERIFY_HEAP || (FEATURE_EVENT_TRACE && BACKGROUND_GC)
 18272| #ifdef MULTIPLE_HEAPS
 18273|     if (!settings.concurrent)
 18274|     {
 18275|         gc_t_join.join(this, gc_join_done);
 18276|         if (gc_t_join.joined ())
 18277|         {
 18278|             gc_heap::internal_gc_done = false;
 18279|             int limit = settings.condemned_generation;
 18280|             if (limit == max_generation)
 18281|             {
 18282|                 limit = total_generation_count-1;
 18283|             }
 18284|             for (int gen = 0; gen <= limit; gen++)
 18285|             {
 18286|                 size_t total_desired = 0;
 18287|                 size_t total_already_consumed = 0;
 18288|                 for (int i = 0; i < gc_heap::n_heaps; i++)
 18289|                 {
 18290|                     gc_heap* hp = gc_heap::g_heaps[i];
 18291|                     dynamic_data* dd = hp->dynamic_data_of (gen);
 18292|                     size_t temp_total_desired = total_desired + dd_desired_allocation (dd);
 18293|                     if (temp_total_desired < total_desired)
 18294|                     {
 18295|                         total_desired = (size_t)MAX_PTR;
 18296|                         break;
 18297|                     }
 18298|                     total_desired = temp_total_desired;
 18299|                     assert ((ptrdiff_t)dd_desired_allocation (dd) >= dd_new_allocation (dd));
 18300|                     size_t already_consumed = dd_desired_allocation (dd) - dd_new_allocation (dd);
 18301|                     size_t temp_total_already_consumed = total_already_consumed + already_consumed;
 18302|                     assert (temp_total_already_consumed >= total_already_consumed);
 18303|                     total_already_consumed = temp_total_already_consumed;
 18304|                 }
 18305|                 size_t desired_per_heap = Align (total_desired/gc_heap::n_heaps,
 18306|                                                     get_alignment_constant (gen <= max_generation));
 18307|                 size_t already_consumed_per_heap = total_already_consumed / gc_heap::n_heaps;
 18308|                 if (gen == 0)
 18309|                 {
 18310| #if 1 //subsumed by the linear allocation model
 18311|                     desired_per_heap = exponential_smoothing (gen, dd_collection_count (dynamic_data_of(gen)), desired_per_heap);
 18312| #endif //0
 18313|                     if (!heap_hard_limit)
 18314|                     {
 18315|                         gc_heap* hp = gc_heap::g_heaps[0];
 18316|                         dynamic_data* dd = hp->dynamic_data_of (gen);
 18317|                         size_t min_gc_size = dd_min_size(dd);
 18318|                         if ((min_gc_size <= GCToOSInterface::GetCacheSizePerLogicalCpu(TRUE)) &&
 18319|                             desired_per_heap <= 2*min_gc_size)
 18320|                         {
 18321|                             desired_per_heap = min_gc_size;
 18322|                         }
 18323|                     }
 18324| #ifdef HOST_64BIT
 18325|                     desired_per_heap = joined_youngest_desired (desired_per_heap);
 18326|                     dprintf (2, ("final gen0 new_alloc: %zd", desired_per_heap));
 18327| #endif // HOST_64BIT
 18328|                     gc_data_global.final_youngest_desired = desired_per_heap;
 18329|                 }
 18330| #if 1 //subsumed by the linear allocation model
 18331|                 if (gen >= uoh_start_generation)
 18332|                 {
 18333|                     desired_per_heap = exponential_smoothing (gen, dd_collection_count (dynamic_data_of (max_generation)), desired_per_heap);
 18334|                 }
 18335| #endif //0
 18336|                 for (int i = 0; i < gc_heap::n_heaps; i++)
 18337|                 {
 18338|                     gc_heap* hp = gc_heap::g_heaps[i];
 18339|                     dynamic_data* dd = hp->dynamic_data_of (gen);
 18340|                     dd_desired_allocation (dd) = desired_per_heap;
 18341|                     dd_gc_new_allocation (dd) = desired_per_heap;
 18342| #ifdef USE_REGIONS
 18343|                     dd_new_allocation (dd) = desired_per_heap - already_consumed_per_heap;
 18344| #else //USE_REGIONS
 18345|                     dd_new_allocation (dd) = desired_per_heap;
 18346| #endif //USE_REGIONS
 18347|                     if (gen == 0)
 18348|                     {
 18349|                         hp->fgn_last_alloc = desired_per_heap;
 18350|                     }
 18351|                 }
 18352|             }
 18353| #ifdef FEATURE_LOH_COMPACTION
 18354|             BOOL all_heaps_compacted_p = TRUE;
 18355| #endif //FEATURE_LOH_COMPACTION
 18356|             int max_gen0_must_clear_bricks = 0;
 18357|             for (int i = 0; i < gc_heap::n_heaps; i++)
 18358|             {
 18359|                 gc_heap* hp = gc_heap::g_heaps[i];
 18360|                 hp->decommit_ephemeral_segment_pages();
 18361|                 hp->rearrange_uoh_segments();
 18362| #ifdef FEATURE_LOH_COMPACTION
 18363|                 all_heaps_compacted_p &= hp->loh_compacted_p;
 18364| #endif //FEATURE_LOH_COMPACTION
 18365|                 max_gen0_must_clear_bricks = max(max_gen0_must_clear_bricks, hp->gen0_must_clear_bricks);
 18366|             }
 18367| #ifdef USE_REGIONS
 18368|             initGCShadow();
 18369|             distribute_free_regions();
 18370|             verify_region_to_generation_map ();
 18371|             compute_gc_and_ephemeral_range (settings.condemned_generation, true);
 18372|             stomp_write_barrier_ephemeral (ephemeral_low, ephemeral_high,
 18373|                                            map_region_to_generation_skewed, (uint8_t)min_segment_size_shr);
 18374| #endif //USE_REGIONS
 18375| #ifdef FEATURE_LOH_COMPACTION
 18376|             check_loh_compact_mode (all_heaps_compacted_p);
 18377| #endif //FEATURE_LOH_COMPACTION
 18378|             if (max_gen0_must_clear_bricks > 0)
 18379|             {
 18380|                 for (int i = 0; i < gc_heap::n_heaps; i++)
 18381|                 {
 18382|                     gc_heap* hp = gc_heap::g_heaps[i];
 18383|                     hp->gen0_must_clear_bricks = max_gen0_must_clear_bricks;
 18384|                 }
 18385|             }
 18386|             for (int i = 0; i < gc_heap::n_heaps; i++)
 18387|             {
 18388|                 g_heaps[i]->descr_generations ("END");
 18389| #ifdef USE_REGIONS
 18390|                 if (settings.condemned_generation == max_generation)
 18391|                 {
 18392|                     region_free_list::age_free_regions (g_heaps[i]->free_regions);
 18393|                     region_free_list::print (g_heaps[i]->free_regions, i, "END");
 18394|                 }
 18395|                 else
 18396|                 {
 18397|                     g_heaps[i]->free_regions[basic_free_region].age_free_regions();
 18398|                     g_heaps[i]->free_regions[basic_free_region].print (i, "END");
 18399|                 }
 18400| #endif //USE_REGIONS
 18401|             }
 18402|             fire_pevents();
 18403|             update_end_ngc_time();
 18404|             pm_full_gc_init_or_clear();
 18405|             gc_t_join.restart();
 18406|         }
 18407|         update_end_gc_time_per_heap();
 18408|         add_to_history_per_heap();
 18409|         alloc_context_count = 0;
 18410|         heap_select::mark_heap (heap_number);
 18411|     }
 18412| #else //MULTIPLE_HEAPS
 18413|     gc_data_global.final_youngest_desired =
 18414|         dd_desired_allocation (dynamic_data_of (0));
 18415| #ifdef FEATURE_LOH_COMPACTION
 18416|     check_loh_compact_mode (loh_compacted_p);
 18417| #endif //FEATURE_LOH_COMPACTION
 18418|     decommit_ephemeral_segment_pages();
 18419|     fire_pevents();
 18420|     if (!(settings.concurrent))
 18421|     {
 18422|         rearrange_uoh_segments();
 18423| #ifdef USE_REGIONS
 18424|         initGCShadow();
 18425|         distribute_free_regions();
 18426|         verify_region_to_generation_map ();
 18427|         compute_gc_and_ephemeral_range (settings.condemned_generation, true);
 18428|         stomp_write_barrier_ephemeral (ephemeral_low, ephemeral_high,
 18429|                                         map_region_to_generation_skewed, (uint8_t)min_segment_size_shr);
 18430|         if (settings.condemned_generation == max_generation)
 18431|         {
 18432|             region_free_list::age_free_regions(free_regions);
 18433|             region_free_list::print(free_regions, 0, "END");
 18434|         }
 18435|         else
 18436|         {
 18437|             free_regions[basic_free_region].age_free_regions();
 18438|             free_regions[basic_free_region].print (0, "END");
 18439|         }
 18440| #endif //USE_REGIONS
 18441|         update_end_ngc_time();
 18442|         update_end_gc_time_per_heap();
 18443|         add_to_history_per_heap();
 18444|         do_post_gc();
 18445|     }
 18446|     pm_full_gc_init_or_clear();
 18447| #ifdef BACKGROUND_GC
 18448|     recover_bgc_settings();
 18449| #endif //BACKGROUND_GC
 18450| #endif //MULTIPLE_HEAPS
 18451| #ifdef USE_REGIONS
 18452|     if (!(settings.concurrent) && (settings.condemned_generation == max_generation))
 18453|     {
 18454|         last_gc_before_oom = FALSE;
 18455|     }
 18456| #endif //USE_REGIONS
 18457| }
 18458| #ifdef DYNAMIC_HEAP_COUNT
 18459| bool gc_heap::prepare_rethread_fl_items()
 18460| {
 18461|     if (!min_fl_list)
 18462|     {
 18463|         min_fl_list = new (nothrow) min_fl_list_info [MAX_BUCKET_COUNT * n_max_heaps];
 18464|         if (min_fl_list == nullptr)
 18465|             return false;
 18466|     }
 18467|     if (!free_list_space_per_heap)
 18468|     {
 18469|         free_list_space_per_heap = new (nothrow) size_t[n_max_heaps];
 18470|         if (free_list_space_per_heap == nullptr)
 18471|             return false;
 18472|     }
 18473|     return true;
 18474| }
 18475| void gc_heap::rethread_fl_items(int gen_idx)
 18476| {
 18477|     uint32_t min_fl_list_size = sizeof (min_fl_list_info) * (MAX_BUCKET_COUNT * n_max_heaps);
 18478|     memset (min_fl_list, 0, min_fl_list_size);
 18479|     memset (free_list_space_per_heap, 0, sizeof(free_list_space_per_heap[0])*n_max_heaps);
 18480|     size_t num_fl_items = 0;
 18481|     size_t num_fl_items_rethreaded = 0;
 18482|     allocator* gen_allocator = generation_allocator (generation_of (gen_idx));
 18483|     gen_allocator->rethread_items (&num_fl_items, &num_fl_items_rethreaded, this, min_fl_list, free_list_space_per_heap, n_heaps);
 18484|     num_fl_items_rethreaded_stage2 = num_fl_items_rethreaded;
 18485| }
 18486| void gc_heap::merge_fl_from_other_heaps (int gen_idx, int to_n_heaps, int from_n_heaps)
 18487| {
 18488| #ifdef _DEBUG
 18489|     uint64_t start_us = GetHighPrecisionTimeStamp ();
 18490|     size_t total_num_fl_items_rethreaded_stage2 = 0;
 18491|     for (int hn = 0; hn < to_n_heaps; hn++)
 18492|     {
 18493|         gc_heap* hp = g_heaps[hn];
 18494|         total_num_fl_items_rethreaded_stage2 += hp->num_fl_items_rethreaded_stage2;
 18495|         min_fl_list_info* current_heap_min_fl_list = hp->min_fl_list;
 18496|         allocator* gen_allocator = generation_allocator (hp->generation_of (gen_idx));
 18497|         int num_buckets = gen_allocator->number_of_buckets();
 18498|         for (int i = 0; i < num_buckets; i++)
 18499|         {
 18500|             min_fl_list_info* current_bucket_min_fl_list = current_heap_min_fl_list + (i * to_n_heaps);
 18501|             for (int other_hn = 0; other_hn < from_n_heaps; other_hn++)
 18502|             {
 18503|                 min_fl_list_info* min_fl_other_heap = &current_bucket_min_fl_list[other_hn];
 18504|                 if (min_fl_other_heap->head)
 18505|                 {
 18506|                     if (other_hn == hn)
 18507|                     {
 18508|                         dprintf (8888, ("h%d has fl items for itself on the temp list?!", hn));
 18509|                         GCToOSInterface::DebugBreak ();
 18510|                     }
 18511|                 }
 18512|             }
 18513|         }
 18514|     }
 18515|     uint64_t elapsed = GetHighPrecisionTimeStamp () - start_us;
 18516|     dprintf (8888, ("rethreaded %Id items, merging took %I64dus (%I64dms)",
 18517|         total_num_fl_items_rethreaded_stage2, elapsed, (elapsed / 1000)));
 18518| #endif //_DEBUG
 18519|     for (int hn = 0; hn < to_n_heaps; hn++)
 18520|     {
 18521|         gc_heap* hp = g_heaps[hn];
 18522|         generation* gen = hp->generation_of (gen_idx);
 18523|         dynamic_data* dd = hp->dynamic_data_of (gen_idx);
 18524|         allocator* gen_allocator = generation_allocator (gen);
 18525|         gen_allocator->merge_items (hp, to_n_heaps, from_n_heaps);
 18526|         size_t free_list_space_decrease = 0;
 18527|         if (hn < from_n_heaps)
 18528|         {
 18529|             assert (hp->free_list_space_per_heap[hn] == 0);
 18530|             for (int to_hn = 0; to_hn < to_n_heaps; to_hn++)
 18531|             {
 18532|                 free_list_space_decrease += hp->free_list_space_per_heap[to_hn];
 18533|             }
 18534|         }
 18535|         dprintf (8888, ("heap %d gen %d %zd total free list space, %zd moved to other heaps",
 18536|             hn,
 18537|             gen_idx,
 18538|             generation_free_list_space (gen),
 18539|             free_list_space_decrease));
 18540|         assert (free_list_space_decrease <= generation_free_list_space (gen));
 18541|         generation_free_list_space (gen) -= free_list_space_decrease;
 18542|         if (gen_idx != max_generation)
 18543|         {
 18544|             assert (free_list_space_decrease <= dd_fragmentation (dd));
 18545|         }
 18546|         size_t free_list_space_increase = 0;
 18547|         for (int from_hn = 0; from_hn < from_n_heaps; from_hn++)
 18548|         {
 18549|             gc_heap* from_hp = g_heaps[from_hn];
 18550|             free_list_space_increase += from_hp->free_list_space_per_heap[hn];
 18551|         }
 18552|         dprintf (8888, ("heap %d gen %d %zd free list space moved from other heaps", hn, gen_idx, free_list_space_increase));
 18553|         generation_free_list_space (gen) += free_list_space_increase;
 18554|     }
 18555| #ifdef _DEBUG
 18556|     size_t total_fl_items_count = 0;
 18557|     size_t total_fl_items_for_oh_count = 0;
 18558|     for (int hn = 0; hn < to_n_heaps; hn++)
 18559|     {
 18560|         gc_heap* hp = g_heaps[hn];
 18561|         allocator* gen_allocator = generation_allocator (hp->generation_of (gen_idx));
 18562|         size_t fl_items_count = 0;
 18563|         size_t fl_items_for_oh_count = 0;
 18564|         gen_allocator->count_items (hp, &fl_items_count, &fl_items_for_oh_count);
 18565|         total_fl_items_count += fl_items_count;
 18566|         total_fl_items_for_oh_count += fl_items_for_oh_count;
 18567|     }
 18568|     dprintf (8888, ("total %Id fl items, %Id are for other heaps",
 18569|         total_fl_items_count, total_fl_items_for_oh_count));
 18570|     if (total_fl_items_for_oh_count)
 18571|     {
 18572|         GCToOSInterface::DebugBreak ();
 18573|     }
 18574| #endif //_DEBUG
 18575| }
 18576| #endif //DYNAMIC_HEAP_COUNT
 18577| void gc_heap::save_data_for_no_gc()
 18578| {
 18579|     current_no_gc_region_info.saved_pause_mode = settings.pause_mode;
 18580| #ifdef MULTIPLE_HEAPS
 18581|     for (int i = 0; i < n_heaps; i++)
 18582|     {
 18583|         current_no_gc_region_info.saved_gen0_min_size = dd_min_size (g_heaps[i]->dynamic_data_of (0));
 18584|         dd_min_size (g_heaps[i]->dynamic_data_of (0)) = min_balance_threshold;
 18585|         current_no_gc_region_info.saved_gen3_min_size = dd_min_size (g_heaps[i]->dynamic_data_of (loh_generation));
 18586|         dd_min_size (g_heaps[i]->dynamic_data_of (loh_generation)) = 0;
 18587|     }
 18588| #endif //MULTIPLE_HEAPS
 18589| }
 18590| void gc_heap::restore_data_for_no_gc()
 18591| {
 18592|     gc_heap::settings.pause_mode = current_no_gc_region_info.saved_pause_mode;
 18593| #ifdef MULTIPLE_HEAPS
 18594|     for (int i = 0; i < n_heaps; i++)
 18595|     {
 18596|         dd_min_size (g_heaps[i]->dynamic_data_of (0)) = current_no_gc_region_info.saved_gen0_min_size;
 18597|         dd_min_size (g_heaps[i]->dynamic_data_of (loh_generation)) = current_no_gc_region_info.saved_gen3_min_size;
 18598|     }
 18599| #endif //MULTIPLE_HEAPS
 18600| }
 18601| start_no_gc_region_status gc_heap::prepare_for_no_gc_region (uint64_t total_size,
 18602|                                                              BOOL loh_size_known,
 18603|                                                              uint64_t loh_size,
 18604|                                                              BOOL disallow_full_blocking)
 18605| {
 18606|     if (current_no_gc_region_info.started)
 18607|     {
 18608|         return start_no_gc_in_progress;
 18609|     }
 18610|     start_no_gc_region_status status = start_no_gc_success;
 18611|     save_data_for_no_gc();
 18612|     settings.pause_mode = pause_no_gc;
 18613|     current_no_gc_region_info.start_status = start_no_gc_success;
 18614|     uint64_t allocation_no_gc_loh = 0;
 18615|     uint64_t allocation_no_gc_soh = 0;
 18616|     assert(total_size != 0);
 18617|     if (loh_size_known)
 18618|     {
 18619|         assert(loh_size != 0);
 18620|         assert(loh_size <= total_size);
 18621|         allocation_no_gc_loh = loh_size;
 18622|         allocation_no_gc_soh = total_size - loh_size;
 18623|     }
 18624|     else
 18625|     {
 18626|         allocation_no_gc_soh = total_size;
 18627|         allocation_no_gc_loh = total_size;
 18628|     }
 18629|     int soh_align_const = get_alignment_constant (TRUE);
 18630| #ifdef USE_REGIONS
 18631|     size_t max_soh_allocated = SIZE_T_MAX;
 18632| #else
 18633|     size_t max_soh_allocated = soh_segment_size - segment_info_size - eph_gen_starts_size;
 18634| #endif
 18635|     size_t size_per_heap = 0;
 18636|     const double scale_factor = 1.05;
 18637|     int num_heaps = get_num_heaps();
 18638|     uint64_t total_allowed_soh_allocation = (uint64_t)max_soh_allocated * num_heaps;
 18639|     assert(total_allowed_soh_allocation <= SIZE_T_MAX);
 18640|     uint64_t total_allowed_loh_allocation = SIZE_T_MAX;
 18641|     uint64_t total_allowed_soh_alloc_scaled = allocation_no_gc_soh > 0 ? static_cast<uint64_t>(total_allowed_soh_allocation / scale_factor) : 0;
 18642|     uint64_t total_allowed_loh_alloc_scaled = allocation_no_gc_loh > 0 ? static_cast<uint64_t>(total_allowed_loh_allocation / scale_factor) : 0;
 18643|     if (allocation_no_gc_soh > total_allowed_soh_alloc_scaled ||
 18644|         allocation_no_gc_loh > total_allowed_loh_alloc_scaled)
 18645|     {
 18646|         status = start_no_gc_too_large;
 18647|         goto done;
 18648|     }
 18649|     if (allocation_no_gc_soh > 0)
 18650|     {
 18651|         allocation_no_gc_soh = static_cast<uint64_t>(allocation_no_gc_soh * scale_factor);
 18652|         allocation_no_gc_soh = min (allocation_no_gc_soh, total_allowed_soh_alloc_scaled);
 18653|     }
 18654|     if (allocation_no_gc_loh > 0)
 18655|     {
 18656|         allocation_no_gc_loh = static_cast<uint64_t>(allocation_no_gc_loh * scale_factor);
 18657|         allocation_no_gc_loh = min (allocation_no_gc_loh, total_allowed_loh_alloc_scaled);
 18658|     }
 18659|     if (disallow_full_blocking)
 18660|         current_no_gc_region_info.minimal_gc_p = TRUE;
 18661|     if (allocation_no_gc_soh != 0)
 18662|     {
 18663|         current_no_gc_region_info.soh_allocation_size = (size_t)allocation_no_gc_soh;
 18664|         size_per_heap = current_no_gc_region_info.soh_allocation_size;
 18665| #ifdef MULTIPLE_HEAPS
 18666|         size_per_heap /= n_heaps;
 18667|         for (int i = 0; i < n_heaps; i++)
 18668|         {
 18669|             g_heaps[i]->soh_allocation_no_gc = min (Align ((size_per_heap + min_balance_threshold), soh_align_const), max_soh_allocated);
 18670|         }
 18671| #else //MULTIPLE_HEAPS
 18672|         soh_allocation_no_gc = min (Align (size_per_heap, soh_align_const), max_soh_allocated);
 18673| #endif //MULTIPLE_HEAPS
 18674|     }
 18675|     if (allocation_no_gc_loh != 0)
 18676|     {
 18677|         current_no_gc_region_info.loh_allocation_size = (size_t)allocation_no_gc_loh;
 18678|         size_per_heap = current_no_gc_region_info.loh_allocation_size;
 18679| #ifdef MULTIPLE_HEAPS
 18680|         size_per_heap /= n_heaps;
 18681|         for (int i = 0; i < n_heaps; i++)
 18682|             g_heaps[i]->loh_allocation_no_gc = Align (size_per_heap, get_alignment_constant (FALSE));
 18683| #else //MULTIPLE_HEAPS
 18684|         loh_allocation_no_gc = Align (size_per_heap, get_alignment_constant (FALSE));
 18685| #endif //MULTIPLE_HEAPS
 18686|     }
 18687| done:
 18688|     if (status != start_no_gc_success)
 18689|         restore_data_for_no_gc();
 18690|     return status;
 18691| }
 18692| void gc_heap::handle_failure_for_no_gc()
 18693| {
 18694|     gc_heap::restore_data_for_no_gc();
 18695|     memset (&current_no_gc_region_info, 0, sizeof (current_no_gc_region_info));
 18696| }
 18697| start_no_gc_region_status gc_heap::get_start_no_gc_region_status()
 18698| {
 18699|     return current_no_gc_region_info.start_status;
 18700| }
 18701| void gc_heap::record_gcs_during_no_gc()
 18702| {
 18703|     if (current_no_gc_region_info.started)
 18704|     {
 18705|         current_no_gc_region_info.num_gcs++;
 18706|         if (is_induced (settings.reason))
 18707|             current_no_gc_region_info.num_gcs_induced++;
 18708|     }
 18709| }
 18710| BOOL gc_heap::find_loh_free_for_no_gc()
 18711| {
 18712|     allocator* loh_allocator = generation_allocator (generation_of (loh_generation));
 18713|     size_t size = loh_allocation_no_gc;
 18714|     for (unsigned int a_l_idx = loh_allocator->first_suitable_bucket(size); a_l_idx < loh_allocator->number_of_buckets(); a_l_idx++)
 18715|     {
 18716|         uint8_t* free_list = loh_allocator->alloc_list_head_of (a_l_idx);
 18717|         while (free_list)
 18718|         {
 18719|             size_t free_list_size = unused_array_size(free_list);
 18720|             if (free_list_size > size)
 18721|             {
 18722|                 dprintf (3, ("free item %zx(%zd) for no gc", (size_t)free_list, free_list_size));
 18723|                 return TRUE;
 18724|             }
 18725|             free_list = free_list_slot (free_list);
 18726|         }
 18727|     }
 18728|     return FALSE;
 18729| }
 18730| BOOL gc_heap::find_loh_space_for_no_gc()
 18731| {
 18732|     saved_loh_segment_no_gc = 0;
 18733|     if (find_loh_free_for_no_gc())
 18734|         return TRUE;
 18735|     heap_segment* seg = generation_allocation_segment (generation_of (loh_generation));
 18736|     while (seg)
 18737|     {
 18738|         size_t remaining = heap_segment_reserved (seg) - heap_segment_allocated (seg);
 18739|         if (remaining >= loh_allocation_no_gc)
 18740|         {
 18741|             saved_loh_segment_no_gc = seg;
 18742|             break;
 18743|         }
 18744|         seg = heap_segment_next (seg);
 18745|     }
 18746|     if (!saved_loh_segment_no_gc && current_no_gc_region_info.minimal_gc_p)
 18747|     {
 18748|         saved_loh_segment_no_gc = get_segment_for_uoh (loh_generation, get_uoh_seg_size (loh_allocation_no_gc)
 18749| #ifdef MULTIPLE_HEAPS
 18750|                                                       , this
 18751| #endif //MULTIPLE_HEAPS
 18752|                                                       );
 18753|     }
 18754|     return (saved_loh_segment_no_gc != 0);
 18755| }
 18756| BOOL gc_heap::loh_allocated_for_no_gc()
 18757| {
 18758|     if (!saved_loh_segment_no_gc)
 18759|         return FALSE;
 18760|     heap_segment* seg = generation_allocation_segment (generation_of (loh_generation));
 18761|     do
 18762|     {
 18763|         if (seg == saved_loh_segment_no_gc)
 18764|         {
 18765|             return FALSE;
 18766|         }
 18767|         seg = heap_segment_next (seg);
 18768|     } while (seg);
 18769|     return TRUE;
 18770| }
 18771| BOOL gc_heap::commit_loh_for_no_gc (heap_segment* seg)
 18772| {
 18773|     uint8_t* end_committed = heap_segment_allocated (seg) + loh_allocation_no_gc;
 18774|     assert (end_committed <= heap_segment_reserved (seg));
 18775|     return (grow_heap_segment (seg, end_committed));
 18776| }
 18777| void gc_heap::thread_no_gc_loh_segments()
 18778| {
 18779| #ifdef MULTIPLE_HEAPS
 18780|     for (int i = 0; i < n_heaps; i++)
 18781|     {
 18782|         gc_heap* hp = g_heaps[i];
 18783|         if (hp->loh_allocated_for_no_gc())
 18784|         {
 18785|             hp->thread_uoh_segment (loh_generation, hp->saved_loh_segment_no_gc);
 18786|             hp->saved_loh_segment_no_gc = 0;
 18787|         }
 18788|     }
 18789| #else //MULTIPLE_HEAPS
 18790|     if (loh_allocated_for_no_gc())
 18791|     {
 18792|         thread_uoh_segment (loh_generation, saved_loh_segment_no_gc);
 18793|         saved_loh_segment_no_gc = 0;
 18794|     }
 18795| #endif //MULTIPLE_HEAPS
 18796| }
 18797| void gc_heap::set_loh_allocations_for_no_gc()
 18798| {
 18799|     if (current_no_gc_region_info.loh_allocation_size != 0)
 18800|     {
 18801|         dynamic_data* dd = dynamic_data_of (loh_generation);
 18802|         dd_new_allocation (dd) = loh_allocation_no_gc;
 18803|         dd_gc_new_allocation (dd) = dd_new_allocation (dd);
 18804|     }
 18805| }
 18806| void gc_heap::set_soh_allocations_for_no_gc()
 18807| {
 18808|     if (current_no_gc_region_info.soh_allocation_size != 0)
 18809|     {
 18810|         dynamic_data* dd = dynamic_data_of (0);
 18811|         dd_new_allocation (dd) = soh_allocation_no_gc;
 18812|         dd_gc_new_allocation (dd) = dd_new_allocation (dd);
 18813| #ifdef MULTIPLE_HEAPS
 18814|         alloc_context_count = 0;
 18815| #endif //MULTIPLE_HEAPS
 18816|     }
 18817| }
 18818| void gc_heap::set_allocations_for_no_gc()
 18819| {
 18820| #ifdef MULTIPLE_HEAPS
 18821|     for (int i = 0; i < n_heaps; i++)
 18822|     {
 18823|         gc_heap* hp = g_heaps[i];
 18824|         hp->set_loh_allocations_for_no_gc();
 18825|         hp->set_soh_allocations_for_no_gc();
 18826|     }
 18827| #else //MULTIPLE_HEAPS
 18828|     set_loh_allocations_for_no_gc();
 18829|     set_soh_allocations_for_no_gc();
 18830| #endif //MULTIPLE_HEAPS
 18831| }
 18832| BOOL gc_heap::should_proceed_for_no_gc()
 18833| {
 18834|     BOOL gc_requested = FALSE;
 18835|     BOOL loh_full_gc_requested = FALSE;
 18836|     BOOL soh_full_gc_requested = FALSE;
 18837|     BOOL no_gc_requested = FALSE;
 18838|     BOOL get_new_loh_segments = FALSE;
 18839| #ifdef MULTIPLE_HEAPS
 18840|     gradual_decommit_in_progress_p = FALSE;
 18841| #endif //MULTIPLE_HEAPS
 18842|     gc_heap* hp = nullptr;
 18843|     if (current_no_gc_region_info.soh_allocation_size)
 18844|     {
 18845| #ifdef USE_REGIONS
 18846| #ifdef MULTIPLE_HEAPS
 18847|         for (int i = 0; i < n_heaps; i++)
 18848|         {
 18849|             hp = g_heaps[i];
 18850| #else
 18851|         {
 18852|             hp = pGenGCHeap;
 18853| #endif //MULTIPLE_HEAPS
 18854|             if (!hp->extend_soh_for_no_gc())
 18855|             {
 18856|                 soh_full_gc_requested = TRUE;
 18857| #ifdef MULTIPLE_HEAPS
 18858|                 break;
 18859| #endif //MULTIPLE_HEAPS
 18860|             }
 18861|         }
 18862| #else //USE_REGIONS
 18863| #ifdef MULTIPLE_HEAPS
 18864|         for (int i = 0; i < n_heaps; i++)
 18865|         {
 18866|             hp = g_heaps[i];
 18867| #else //MULTIPLE_HEAPS
 18868|         {
 18869|             hp = pGenGCHeap;
 18870| #endif //MULTIPLE_HEAPS
 18871|             size_t reserved_space = heap_segment_reserved (hp->ephemeral_heap_segment) - hp->alloc_allocated;
 18872|             if (reserved_space < hp->soh_allocation_no_gc)
 18873|             {
 18874|                 gc_requested = TRUE;
 18875| #ifdef MULTIPLE_HEAPS
 18876|                 break;
 18877| #endif //MULTIPLE_HEAPS
 18878|             }
 18879|         }
 18880|         if (!gc_requested)
 18881|         {
 18882| #ifdef MULTIPLE_HEAPS
 18883|             for (int i = 0; i < n_heaps; i++)
 18884|             {
 18885|                 hp = g_heaps[i];
 18886| #else //MULTIPLE_HEAPS
 18887|             {
 18888|                 hp = pGenGCHeap;
 18889| #endif //MULTIPLE_HEAPS
 18890|                 if (!(hp->grow_heap_segment (hp->ephemeral_heap_segment, (hp->alloc_allocated + hp->soh_allocation_no_gc))))
 18891|                 {
 18892|                     soh_full_gc_requested = TRUE;
 18893| #ifdef MULTIPLE_HEAPS
 18894|                     break;
 18895| #endif //MULTIPLE_HEAPS
 18896|                 }
 18897|             }
 18898|         }
 18899| #endif //USE_REGIONS
 18900|     }
 18901|     if (!current_no_gc_region_info.minimal_gc_p && gc_requested)
 18902|     {
 18903|         soh_full_gc_requested = TRUE;
 18904|     }
 18905|     no_gc_requested = !(soh_full_gc_requested || gc_requested);
 18906|     if (soh_full_gc_requested && current_no_gc_region_info.minimal_gc_p)
 18907|     {
 18908|         current_no_gc_region_info.start_status = start_no_gc_no_memory;
 18909|         goto done;
 18910|     }
 18911|     if (!soh_full_gc_requested && current_no_gc_region_info.loh_allocation_size)
 18912|     {
 18913| #ifdef MULTIPLE_HEAPS
 18914|         for (int i = 0; i < n_heaps; i++)
 18915|         {
 18916|             gc_heap* hp = g_heaps[i];
 18917|             if (!hp->find_loh_space_for_no_gc())
 18918|             {
 18919|                 loh_full_gc_requested = TRUE;
 18920|                 break;
 18921|             }
 18922|         }
 18923| #else //MULTIPLE_HEAPS
 18924|         if (!find_loh_space_for_no_gc())
 18925|             loh_full_gc_requested = TRUE;
 18926| #endif //MULTIPLE_HEAPS
 18927|         if (!loh_full_gc_requested)
 18928|         {
 18929| #ifdef MULTIPLE_HEAPS
 18930|             for (int i = 0; i < n_heaps; i++)
 18931|             {
 18932|                 gc_heap* hp = g_heaps[i];
 18933|                 if (hp->saved_loh_segment_no_gc &&!hp->commit_loh_for_no_gc (hp->saved_loh_segment_no_gc))
 18934|                 {
 18935|                     loh_full_gc_requested = TRUE;
 18936|                     break;
 18937|                 }
 18938|             }
 18939| #else //MULTIPLE_HEAPS
 18940|             if (saved_loh_segment_no_gc && !commit_loh_for_no_gc (saved_loh_segment_no_gc))
 18941|                 loh_full_gc_requested = TRUE;
 18942| #endif //MULTIPLE_HEAPS
 18943|         }
 18944|     }
 18945|     if (loh_full_gc_requested || soh_full_gc_requested)
 18946|     {
 18947|         if (current_no_gc_region_info.minimal_gc_p)
 18948|             current_no_gc_region_info.start_status = start_no_gc_no_memory;
 18949|     }
 18950|     no_gc_requested = !(loh_full_gc_requested || soh_full_gc_requested || gc_requested);
 18951|     if (current_no_gc_region_info.start_status == start_no_gc_success)
 18952|     {
 18953|         if (no_gc_requested)
 18954|             set_allocations_for_no_gc();
 18955|     }
 18956| done:
 18957|     if ((current_no_gc_region_info.start_status == start_no_gc_success) && !no_gc_requested)
 18958|         return TRUE;
 18959|     else
 18960|     {
 18961|         current_no_gc_region_info.started = TRUE;
 18962|         return FALSE;
 18963|     }
 18964| }
 18965| end_no_gc_region_status gc_heap::end_no_gc_region()
 18966| {
 18967|     dprintf (1, ("end no gc called"));
 18968|     end_no_gc_region_status status = end_no_gc_success;
 18969|     if (!(current_no_gc_region_info.started))
 18970|         status = end_no_gc_not_in_progress;
 18971|     if (current_no_gc_region_info.num_gcs_induced)
 18972|         status = end_no_gc_induced;
 18973|     else if (current_no_gc_region_info.num_gcs)
 18974|         status = end_no_gc_alloc_exceeded;
 18975|     if (settings.pause_mode == pause_no_gc)
 18976|     {
 18977|         restore_data_for_no_gc();
 18978|         if (current_no_gc_region_info.callback != nullptr)
 18979|         {
 18980|             dprintf (1, ("[no_gc_callback] detaching callback on exit"));
 18981|             schedule_no_gc_callback (true);
 18982|         }
 18983|     }
 18984|     memset (&current_no_gc_region_info, 0, sizeof (current_no_gc_region_info));
 18985|     return status;
 18986| }
 18987| void gc_heap::schedule_no_gc_callback (bool abandoned)
 18988| {
 18989|     current_no_gc_region_info.callback->abandoned = abandoned;
 18990|     if (!current_no_gc_region_info.callback->scheduled)
 18991|     {
 18992|         current_no_gc_region_info.callback->scheduled = true;
 18993|         schedule_finalizer_work(current_no_gc_region_info.callback);
 18994|     }
 18995| }
 18996| void gc_heap::schedule_finalizer_work (FinalizerWorkItem* callback)
 18997| {
 18998|     FinalizerWorkItem* prev;
 18999|     do
 19000|     {
 19001|         prev = finalizer_work;
 19002|         callback->next = prev;
 19003|     }
 19004|     while (Interlocked::CompareExchangePointer (&finalizer_work, callback, prev) != prev);
 19005|     if (prev == nullptr)
 19006|     {
 19007|         GCToEEInterface::EnableFinalization(true);
 19008|     }
 19009| }
 19010| void gc_heap::update_collection_counts ()
 19011| {
 19012|     dynamic_data* dd0 = dynamic_data_of (0);
 19013|     dd_gc_clock (dd0) += 1;
 19014|     uint64_t now = GetHighPrecisionTimeStamp();
 19015|     for (int i = 0; i <= settings.condemned_generation;i++)
 19016|     {
 19017|         dynamic_data* dd = dynamic_data_of (i);
 19018|         dd_collection_count (dd)++;
 19019|         if (i == max_generation)
 19020|         {
 19021|             dd_collection_count (dynamic_data_of (loh_generation))++;
 19022|             dd_collection_count(dynamic_data_of(poh_generation))++;
 19023|         }
 19024|         dd_gc_clock (dd) = dd_gc_clock (dd0);
 19025|         dd_previous_time_clock (dd) = dd_time_clock (dd);
 19026|         dd_time_clock (dd) = now;
 19027|     }
 19028| }
 19029| #ifdef USE_REGIONS
 19030| bool gc_heap::extend_soh_for_no_gc()
 19031| {
 19032|     size_t required = soh_allocation_no_gc;
 19033|     heap_segment* region = ephemeral_heap_segment;
 19034|     while (true)
 19035|     {
 19036|         uint8_t* allocated = (region == ephemeral_heap_segment) ?
 19037|                              alloc_allocated :
 19038|                              heap_segment_allocated (region);
 19039|         size_t available = heap_segment_reserved (region) - allocated;
 19040|         size_t commit = min (available, required);
 19041|         if (grow_heap_segment (region, allocated + commit))
 19042|         {
 19043|             required -= commit;
 19044|             if (required == 0)
 19045|             {
 19046|                 break;
 19047|             }
 19048|             region = heap_segment_next (region);
 19049|             if (region == nullptr)
 19050|             {
 19051|                 region = get_new_region (0);
 19052|                 if (region == nullptr)
 19053|                 {
 19054|                     break;
 19055|                 }
 19056|                 else
 19057|                 {
 19058|                     GCToEEInterface::DiagAddNewRegion(
 19059|                             0,
 19060|                             heap_segment_mem (region),
 19061|                             heap_segment_allocated (region),
 19062|                             heap_segment_reserved (region)
 19063|                         );
 19064|                 }
 19065|             }
 19066|         }
 19067|         else
 19068|         {
 19069|             break;
 19070|         }
 19071|     }
 19072|     return (required == 0);
 19073| }
 19074| #else
 19075| BOOL gc_heap::expand_soh_with_minimal_gc()
 19076| {
 19077|     if ((size_t)(heap_segment_reserved (ephemeral_heap_segment) - heap_segment_allocated (ephemeral_heap_segment)) >= soh_allocation_no_gc)
 19078|         return TRUE;
 19079|     heap_segment* new_seg = soh_get_segment_to_expand();
 19080|     if (new_seg)
 19081|     {
 19082|         if (g_gc_card_table != card_table)
 19083|             copy_brick_card_table();
 19084|         settings.promotion = TRUE;
 19085|         settings.demotion = FALSE;
 19086|         ephemeral_promotion = TRUE;
 19087|         int condemned_gen_number = max_generation - 1;
 19088|         int align_const = get_alignment_constant (TRUE);
 19089|         for (int i = 0; i <= condemned_gen_number; i++)
 19090|         {
 19091|             generation* gen = generation_of (i);
 19092|             saved_ephemeral_plan_start[i] = generation_allocation_start (gen);
 19093|             saved_ephemeral_plan_start_size[i] = Align (size (generation_allocation_start (gen)), align_const);
 19094|         }
 19095|         for (size_t b = brick_of (generation_allocation_start (generation_of (0)));
 19096|              b < brick_of (align_on_brick (heap_segment_allocated (ephemeral_heap_segment)));
 19097|              b++)
 19098|         {
 19099|             set_brick (b, -1);
 19100|         }
 19101|         size_t ephemeral_size = (heap_segment_allocated (ephemeral_heap_segment) -
 19102|                                 generation_allocation_start (generation_of (max_generation - 1)));
 19103|         heap_segment_next (ephemeral_heap_segment) = new_seg;
 19104|         ephemeral_heap_segment = new_seg;
 19105|         uint8_t*  start = heap_segment_mem (ephemeral_heap_segment);
 19106|         for (int i = condemned_gen_number; i >= 0; i--)
 19107|         {
 19108|             size_t gen_start_size = Align (min_obj_size);
 19109|             make_generation (i, ephemeral_heap_segment, start);
 19110|             generation* gen = generation_of (i);
 19111|             generation_plan_allocation_start (gen) = start;
 19112|             generation_plan_allocation_start_size (gen) = gen_start_size;
 19113|             start += gen_start_size;
 19114|         }
 19115|         heap_segment_used (ephemeral_heap_segment) = start - plug_skew;
 19116|         heap_segment_plan_allocated (ephemeral_heap_segment) = start;
 19117|         fix_generation_bounds (condemned_gen_number, generation_of (0));
 19118|         dd_gc_new_allocation (dynamic_data_of (max_generation)) -= ephemeral_size;
 19119|         dd_new_allocation (dynamic_data_of (max_generation)) = dd_gc_new_allocation (dynamic_data_of (max_generation));
 19120|         adjust_ephemeral_limits();
 19121|         return TRUE;
 19122|     }
 19123|     else
 19124|     {
 19125|         return FALSE;
 19126|     }
 19127| }
 19128| #endif //USE_REGIONS
 19129| void gc_heap::check_and_set_no_gc_oom()
 19130| {
 19131| #ifdef MULTIPLE_HEAPS
 19132|     for (int i = 0; i < n_heaps; i++)
 19133|     {
 19134|         gc_heap* hp = g_heaps[i];
 19135|         if (hp->no_gc_oom_p)
 19136|         {
 19137|             current_no_gc_region_info.start_status = start_no_gc_no_memory;
 19138|             hp->no_gc_oom_p = false;
 19139|         }
 19140|     }
 19141| #else
 19142|     if (no_gc_oom_p)
 19143|     {
 19144|         current_no_gc_region_info.start_status = start_no_gc_no_memory;
 19145|         no_gc_oom_p = false;
 19146|     }
 19147| #endif //MULTIPLE_HEAPS
 19148| }
 19149| void gc_heap::allocate_for_no_gc_after_gc()
 19150| {
 19151|     if (current_no_gc_region_info.minimal_gc_p)
 19152|         repair_allocation_contexts (TRUE);
 19153|     no_gc_oom_p = false;
 19154|     if (current_no_gc_region_info.start_status != start_no_gc_no_memory)
 19155|     {
 19156|         if (current_no_gc_region_info.soh_allocation_size != 0)
 19157|         {
 19158| #ifdef USE_REGIONS
 19159|             no_gc_oom_p = !extend_soh_for_no_gc();
 19160| #else
 19161|             if (((size_t)(heap_segment_reserved (ephemeral_heap_segment) - heap_segment_allocated (ephemeral_heap_segment)) < soh_allocation_no_gc) ||
 19162|                 (!grow_heap_segment (ephemeral_heap_segment, (heap_segment_allocated (ephemeral_heap_segment) + soh_allocation_no_gc))))
 19163|             {
 19164|                 no_gc_oom_p = true;
 19165|             }
 19166| #endif //USE_REGIONS
 19167| #ifdef MULTIPLE_HEAPS
 19168|             gc_t_join.join(this, gc_join_after_commit_soh_no_gc);
 19169|             if (gc_t_join.joined())
 19170| #endif //MULTIPLE_HEAPS
 19171|             {
 19172|                 check_and_set_no_gc_oom();
 19173| #ifdef MULTIPLE_HEAPS
 19174|                 gc_t_join.restart();
 19175| #endif //MULTIPLE_HEAPS
 19176|             }
 19177|         }
 19178|         if ((current_no_gc_region_info.start_status == start_no_gc_success) &&
 19179|             !(current_no_gc_region_info.minimal_gc_p) &&
 19180|             (current_no_gc_region_info.loh_allocation_size != 0))
 19181|         {
 19182|             gc_policy = policy_compact;
 19183|             saved_loh_segment_no_gc = 0;
 19184|             if (!find_loh_free_for_no_gc())
 19185|             {
 19186|                 heap_segment* seg = generation_allocation_segment (generation_of (loh_generation));
 19187|                 BOOL found_seg_p = FALSE;
 19188|                 while (seg)
 19189|                 {
 19190|                     if ((size_t)(heap_segment_reserved (seg) - heap_segment_allocated (seg)) >= loh_allocation_no_gc)
 19191|                     {
 19192|                         found_seg_p = TRUE;
 19193|                         if (!commit_loh_for_no_gc (seg))
 19194|                         {
 19195|                             no_gc_oom_p = true;
 19196|                             break;
 19197|                         }
 19198|                     }
 19199|                     seg = heap_segment_next (seg);
 19200|                 }
 19201|                 if (!found_seg_p)
 19202|                     gc_policy = policy_expand;
 19203|             }
 19204| #ifdef MULTIPLE_HEAPS
 19205|             gc_t_join.join(this, gc_join_expand_loh_no_gc);
 19206|             if (gc_t_join.joined())
 19207|             {
 19208|                 check_and_set_no_gc_oom();
 19209|                 if (current_no_gc_region_info.start_status == start_no_gc_success)
 19210|                 {
 19211|                     for (int i = 0; i < n_heaps; i++)
 19212|                     {
 19213|                         gc_heap* hp = g_heaps[i];
 19214|                         if (hp->gc_policy == policy_expand)
 19215|                         {
 19216|                             hp->saved_loh_segment_no_gc = get_segment_for_uoh (loh_generation, get_uoh_seg_size (loh_allocation_no_gc), hp);
 19217|                             if (!(hp->saved_loh_segment_no_gc))
 19218|                             {
 19219|                                 current_no_gc_region_info.start_status = start_no_gc_no_memory;
 19220|                                 break;
 19221|                             }
 19222|                         }
 19223|                     }
 19224|                 }
 19225|                 gc_t_join.restart();
 19226|             }
 19227| #else //MULTIPLE_HEAPS
 19228|             check_and_set_no_gc_oom();
 19229|             if ((current_no_gc_region_info.start_status == start_no_gc_success) && (gc_policy == policy_expand))
 19230|             {
 19231|                 saved_loh_segment_no_gc = get_segment_for_uoh (loh_generation, get_uoh_seg_size (loh_allocation_no_gc));
 19232|                 if (!saved_loh_segment_no_gc)
 19233|                     current_no_gc_region_info.start_status = start_no_gc_no_memory;
 19234|             }
 19235| #endif //MULTIPLE_HEAPS
 19236|             if ((current_no_gc_region_info.start_status == start_no_gc_success) && saved_loh_segment_no_gc)
 19237|             {
 19238|                 if (!commit_loh_for_no_gc (saved_loh_segment_no_gc))
 19239|                 {
 19240|                     no_gc_oom_p = true;
 19241|                 }
 19242|             }
 19243|         }
 19244|     }
 19245| #ifdef MULTIPLE_HEAPS
 19246|     gc_t_join.join(this, gc_join_final_no_gc);
 19247|     if (gc_t_join.joined())
 19248| #endif //MULTIPLE_HEAPS
 19249|     {
 19250|         check_and_set_no_gc_oom();
 19251|         if (current_no_gc_region_info.start_status == start_no_gc_success)
 19252|         {
 19253|             set_allocations_for_no_gc();
 19254|             current_no_gc_region_info.started = TRUE;
 19255|         }
 19256| #ifdef MULTIPLE_HEAPS
 19257|         gc_t_join.restart();
 19258| #endif //MULTIPLE_HEAPS
 19259|     }
 19260| }
 19261| void gc_heap::init_records()
 19262| {
 19263|     memset (&gc_data_per_heap, 0, sizeof (gc_data_per_heap));
 19264|     gc_data_per_heap.heap_index = heap_number;
 19265|     if (heap_number == 0)
 19266|         memset (&gc_data_global, 0, sizeof (gc_data_global));
 19267| #ifdef GC_CONFIG_DRIVEN
 19268|     memset (interesting_data_per_gc, 0, sizeof (interesting_data_per_gc));
 19269| #endif //GC_CONFIG_DRIVEN
 19270|     memset (&fgm_result, 0, sizeof (fgm_result));
 19271|     for (int i = 0; i < total_generation_count; i++)
 19272|     {
 19273|         gc_data_per_heap.gen_data[i].size_before = generation_size (i);
 19274|         generation* gen = generation_of (i);
 19275|         gc_data_per_heap.gen_data[i].free_list_space_before = generation_free_list_space (gen);
 19276|         gc_data_per_heap.gen_data[i].free_obj_space_before = generation_free_obj_space (gen);
 19277|     }
 19278| #ifdef USE_REGIONS
 19279|     end_gen0_region_space = uninitialized_end_gen0_region_space;
 19280|     end_gen0_region_committed_space = 0;
 19281|     gen0_pinned_free_space = 0;
 19282|     gen0_large_chunk_found = false;
 19283|     num_regions_freed_in_sweep = 0;
 19284| #endif //USE_REGIONS
 19285|     sufficient_gen0_space_p = FALSE;
 19286| #ifdef MULTIPLE_HEAPS
 19287|     gen0_allocated_after_gc_p = false;
 19288| #endif //MULTIPLE_HEAPS
 19289| #if defined (_DEBUG) && defined (VERIFY_HEAP)
 19290|     verify_pinned_queue_p = FALSE;
 19291| #endif // _DEBUG && VERIFY_HEAP
 19292| }
 19293| void gc_heap::pm_full_gc_init_or_clear()
 19294| {
 19295|     if (settings.condemned_generation == (max_generation - 1))
 19296|     {
 19297|         if (pm_trigger_full_gc)
 19298|         {
 19299| #ifdef MULTIPLE_HEAPS
 19300|             do_post_gc();
 19301| #endif //MULTIPLE_HEAPS
 19302|             dprintf (GTC_LOG, ("init for PM triggered full GC"));
 19303|             uint32_t saved_entry_memory_load = settings.entry_memory_load;
 19304|             settings.init_mechanisms();
 19305|             settings.reason = reason_pm_full_gc;
 19306|             settings.condemned_generation = max_generation;
 19307|             settings.entry_memory_load = saved_entry_memory_load;
 19308|             assert (settings.entry_memory_load > 0);
 19309|             settings.gc_index += 1;
 19310|             do_pre_gc();
 19311|         }
 19312|     }
 19313|     else if (settings.reason == reason_pm_full_gc)
 19314|     {
 19315|         assert (settings.condemned_generation == max_generation);
 19316|         assert (pm_trigger_full_gc);
 19317|         pm_trigger_full_gc = false;
 19318|         dprintf (GTC_LOG, ("PM triggered full GC done"));
 19319|     }
 19320| }
 19321| void gc_heap::garbage_collect_pm_full_gc()
 19322| {
 19323|     assert (settings.condemned_generation == max_generation);
 19324|     assert (settings.reason == reason_pm_full_gc);
 19325|     assert (!settings.concurrent);
 19326|     gc1();
 19327| }
 19328| void gc_heap::garbage_collect (int n)
 19329| {
 19330|     gc_pause_mode saved_settings_pause_mode = settings.pause_mode;
 19331|     alloc_contexts_used = 0;
 19332|     fix_allocation_contexts (TRUE);
 19333| #ifdef MULTIPLE_HEAPS
 19334| #ifdef JOIN_STATS
 19335|     gc_t_join.start_ts(this);
 19336| #endif //JOIN_STATS
 19337|     check_gen0_bricks();
 19338|     clear_gen0_bricks();
 19339| #endif //MULTIPLE_HEAPS
 19340|     if ((settings.pause_mode == pause_no_gc) && current_no_gc_region_info.minimal_gc_p)
 19341|     {
 19342| #ifdef MULTIPLE_HEAPS
 19343|         gc_t_join.join(this, gc_join_minimal_gc);
 19344|         if (gc_t_join.joined())
 19345| #endif //MULTIPLE_HEAPS
 19346|         {
 19347| #ifndef USE_REGIONS
 19348| #ifdef MULTIPLE_HEAPS
 19349|             for (int i = 0; i < n_heaps; i++)
 19350|             {
 19351|                 if (!(g_heaps[i]->expand_soh_with_minimal_gc()))
 19352|                     current_no_gc_region_info.start_status = start_no_gc_no_memory;
 19353|             }
 19354| #else
 19355|             if (!expand_soh_with_minimal_gc())
 19356|                 current_no_gc_region_info.start_status = start_no_gc_no_memory;
 19357| #endif //MULTIPLE_HEAPS
 19358| #endif //!USE_REGIONS
 19359|             update_collection_counts_for_no_gc();
 19360| #ifdef MULTIPLE_HEAPS
 19361|             gc_start_event.Reset();
 19362|             gc_t_join.restart();
 19363| #endif //MULTIPLE_HEAPS
 19364|         }
 19365|         goto done;
 19366|     }
 19367|     init_records();
 19368|     settings.reason = gc_trigger_reason;
 19369|     num_pinned_objects = 0;
 19370| #ifdef STRESS_HEAP
 19371|     if (settings.reason == reason_gcstress)
 19372|     {
 19373|         settings.reason = reason_induced;
 19374|         settings.stress_induced = TRUE;
 19375|     }
 19376| #endif // STRESS_HEAP
 19377| #ifdef MULTIPLE_HEAPS
 19378| #ifdef STRESS_DYNAMIC_HEAP_COUNT
 19379|     Interlocked::Increment (&heaps_in_this_gc);
 19380| #endif //STRESS_DYNAMIC_HEAP_COUNT
 19381|     dprintf (3, ("Joining for max generation to condemn"));
 19382|     condemned_generation_num = generation_to_condemn (n,
 19383|                                                       &blocking_collection,
 19384|                                                       &elevation_requested,
 19385|                                                       FALSE);
 19386|     gc_t_join.join(this, gc_join_generation_determined);
 19387|     if (gc_t_join.joined())
 19388| #endif //MULTIPLE_HEAPS
 19389|     {
 19390| #ifdef FEATURE_BASICFREEZE
 19391|         seg_table->delete_old_slots();
 19392| #endif //FEATURE_BASICFREEZE
 19393| #ifdef MULTIPLE_HEAPS
 19394| #ifdef STRESS_DYNAMIC_HEAP_COUNT
 19395|         dprintf (9999, ("%d heaps, join sees %d, actually joined %d, %d idle threads (%d)",
 19396|             n_heaps, gc_t_join.get_num_threads (), heaps_in_this_gc,
 19397|             VolatileLoadWithoutBarrier(&dynamic_heap_count_data.idle_thread_count), (n_max_heaps - n_heaps)));
 19398|         if (heaps_in_this_gc != n_heaps)
 19399|         {
 19400|             dprintf (9999, ("should have %d heaps but actually have %d!!", n_heaps, heaps_in_this_gc));
 19401|             GCToOSInterface::DebugBreak ();
 19402|         }
 19403|         heaps_in_this_gc = 0;
 19404| #endif //STRESS_DYNAMIC_HEAP_COUNT
 19405|         for (int i = 0; i < n_heaps; i++)
 19406|         {
 19407|             gc_heap* hp = g_heaps[i];
 19408|             if (g_gc_card_table != hp->card_table)
 19409|                 hp->copy_brick_card_table();
 19410|             hp->delay_free_segments();
 19411|         }
 19412| #else //MULTIPLE_HEAPS
 19413|         if (g_gc_card_table != card_table)
 19414|             copy_brick_card_table();
 19415|         delay_free_segments();
 19416| #endif //MULTIPLE_HEAPS
 19417|         BOOL should_evaluate_elevation = TRUE;
 19418|         BOOL should_do_blocking_collection = FALSE;
 19419| #ifdef MULTIPLE_HEAPS
 19420|         int gen_max = condemned_generation_num;
 19421|         for (int i = 0; i < n_heaps; i++)
 19422|         {
 19423|             if (gen_max < g_heaps[i]->condemned_generation_num)
 19424|                 gen_max = g_heaps[i]->condemned_generation_num;
 19425|             if (should_evaluate_elevation && !(g_heaps[i]->elevation_requested))
 19426|                 should_evaluate_elevation = FALSE;
 19427|             if ((!should_do_blocking_collection) && (g_heaps[i]->blocking_collection))
 19428|                 should_do_blocking_collection = TRUE;
 19429|         }
 19430|         settings.condemned_generation = gen_max;
 19431| #else //MULTIPLE_HEAPS
 19432|         settings.condemned_generation = generation_to_condemn (n,
 19433|                                                             &blocking_collection,
 19434|                                                             &elevation_requested,
 19435|                                                             FALSE);
 19436|         should_evaluate_elevation = elevation_requested;
 19437|         should_do_blocking_collection = blocking_collection;
 19438| #endif //MULTIPLE_HEAPS
 19439|         settings.condemned_generation = joined_generation_to_condemn (
 19440|                                             should_evaluate_elevation,
 19441|                                             n,
 19442|                                             settings.condemned_generation,
 19443|                                             &should_do_blocking_collection
 19444|                                             STRESS_HEAP_ARG(n)
 19445|                                             );
 19446|         STRESS_LOG1(LF_GCROOTS|LF_GC|LF_GCALLOC, LL_INFO10,
 19447|                 "condemned generation num: %d\n", settings.condemned_generation);
 19448|         record_gcs_during_no_gc();
 19449|         if (settings.condemned_generation > 1)
 19450|             settings.promotion = TRUE;
 19451| #ifdef HEAP_ANALYZE
 19452|         if (GCToEEInterface::AnalyzeSurvivorsRequested(settings.condemned_generation))
 19453|         {
 19454|             heap_analyze_enabled = TRUE;
 19455|         }
 19456| #endif // HEAP_ANALYZE
 19457|         GCToEEInterface::DiagGCStart(settings.condemned_generation, settings.reason == reason_induced);
 19458| #ifdef BACKGROUND_GC
 19459|         if ((settings.condemned_generation == max_generation) &&
 19460|             (should_do_blocking_collection == FALSE) &&
 19461|             gc_can_use_concurrent &&
 19462|             !temp_disable_concurrent_p &&
 19463|             ((settings.pause_mode == pause_interactive) || (settings.pause_mode == pause_sustained_low_latency)))
 19464|         {
 19465|             keep_bgc_threads_p = TRUE;
 19466|             c_write (settings.concurrent, TRUE);
 19467|             memset (&bgc_data_global, 0, sizeof(bgc_data_global));
 19468|             memcpy (&bgc_data_global, &gc_data_global, sizeof(gc_data_global));
 19469|         }
 19470| #endif //BACKGROUND_GC
 19471|         settings.gc_index = (uint32_t)dd_collection_count (dynamic_data_of (0)) + 1;
 19472| #ifdef MULTIPLE_HEAPS
 19473|         hb_log_balance_activities();
 19474|         hb_log_new_allocation();
 19475| #endif //MULTIPLE_HEAPS
 19476|         GCToEEInterface::GcStartWork (settings.condemned_generation,
 19477|                                 max_generation);
 19478|         do_pre_gc();
 19479| #ifdef MULTIPLE_HEAPS
 19480|         dprintf (9999, ("in GC, resetting gc_start"));
 19481|         gc_start_event.Reset();
 19482|         dprintf(3, ("Starting all gc threads for gc"));
 19483|         gc_t_join.restart();
 19484| #endif //MULTIPLE_HEAPS
 19485|     }
 19486|     descr_generations ("BEGIN");
 19487| #if defined(TRACE_GC) && defined(USE_REGIONS)
 19488|     if (heap_number == 0)
 19489|     {
 19490| #ifdef MULTIPLE_HEAPS
 19491|         for (int i = 0; i < n_heaps; i++)
 19492|         {
 19493|             gc_heap *hp = g_heaps[i];
 19494| #else //MULTIPLE_HEAPS
 19495|         {
 19496|             gc_heap* hp = pGenGCHeap;
 19497|             const int i = 0;
 19498| #endif //MULTIPLE_HEAPS
 19499|             if (settings.condemned_generation == max_generation)
 19500|             {
 19501|                 region_free_list::print(hp->free_regions, i, "BEGIN");
 19502|             }
 19503|             else
 19504|             {
 19505|                 hp->free_regions[basic_free_region].print (i, "BEGIN");
 19506|             }
 19507|         }
 19508|     }
 19509| #endif // TRACE_GC && USE_REGIONS
 19510| #ifdef VERIFY_HEAP
 19511|     if ((GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC) &&
 19512|        !(GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_POST_GC_ONLY))
 19513|     {
 19514|         verify_heap (TRUE);
 19515|     }
 19516|     if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_BARRIERCHECK)
 19517|         checkGCWriteBarrier();
 19518| #endif // VERIFY_HEAP
 19519| #ifdef BACKGROUND_GC
 19520|     if (settings.concurrent)
 19521|     {
 19522|         assert (settings.condemned_generation == max_generation);
 19523|         settings.compaction = FALSE;
 19524|         saved_bgc_settings = settings;
 19525| #ifdef MULTIPLE_HEAPS
 19526|         if (heap_number == 0)
 19527|         {
 19528|             for (int i = 0; i < n_heaps; i++)
 19529|             {
 19530|                 prepare_bgc_thread (g_heaps[i]);
 19531|             }
 19532|             dprintf (2, ("setting bgc_threads_sync_event"));
 19533|             bgc_threads_sync_event.Set();
 19534|         }
 19535|         else
 19536|         {
 19537|             bgc_threads_sync_event.Wait(INFINITE, FALSE);
 19538|             dprintf (2, ("bgc_threads_sync_event is signalled"));
 19539|         }
 19540| #else
 19541|         prepare_bgc_thread(0);
 19542| #endif //MULTIPLE_HEAPS
 19543| #ifdef MULTIPLE_HEAPS
 19544|         gc_t_join.join(this, gc_join_start_bgc);
 19545|         if (gc_t_join.joined())
 19546| #endif //MULTIPLE_HEAPS
 19547|         {
 19548|             do_concurrent_p = TRUE;
 19549|             do_ephemeral_gc_p = FALSE;
 19550| #ifdef MULTIPLE_HEAPS
 19551|             dprintf(2, ("Joined to perform a background GC"));
 19552|             for (int i = 0; i < n_heaps; i++)
 19553|             {
 19554|                 gc_heap* hp = g_heaps[i];
 19555|                 if (!(hp->bgc_thread) || !hp->commit_mark_array_bgc_init())
 19556|                 {
 19557|                     do_concurrent_p = FALSE;
 19558|                     break;
 19559|                 }
 19560|                 else
 19561|                 {
 19562|                     hp->background_saved_lowest_address = hp->lowest_address;
 19563|                     hp->background_saved_highest_address = hp->highest_address;
 19564|                 }
 19565|             }
 19566| #else
 19567|             do_concurrent_p = (!!bgc_thread && commit_mark_array_bgc_init());
 19568|             if (do_concurrent_p)
 19569|             {
 19570|                 background_saved_lowest_address = lowest_address;
 19571|                 background_saved_highest_address = highest_address;
 19572|             }
 19573| #endif //MULTIPLE_HEAPS
 19574|             if (do_concurrent_p)
 19575|             {
 19576| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 19577|                 SoftwareWriteWatch::EnableForGCHeap();
 19578| #endif //FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 19579| #ifdef MULTIPLE_HEAPS
 19580|                 for (int i = 0; i < n_heaps; i++)
 19581|                     g_heaps[i]->current_bgc_state = bgc_initialized;
 19582| #else
 19583|                 current_bgc_state = bgc_initialized;
 19584| #endif //MULTIPLE_HEAPS
 19585|                 int gen = check_for_ephemeral_alloc();
 19586|                 dont_restart_ee_p = TRUE;
 19587|                 if (gen == -1)
 19588|                 {
 19589| #ifdef MULTIPLE_HEAPS
 19590|                     for (int i = 0; i < n_heaps; i++)
 19591|                     {
 19592|                         generation_allocation_pointer (g_heaps[i]->generation_of (0)) =  0;
 19593|                         generation_allocation_limit (g_heaps[i]->generation_of (0)) = 0;
 19594|                     }
 19595| #else
 19596|                     generation_allocation_pointer (youngest_generation) =  0;
 19597|                     generation_allocation_limit (youngest_generation) = 0;
 19598| #endif //MULTIPLE_HEAPS
 19599|                 }
 19600|                 else
 19601|                 {
 19602|                     do_ephemeral_gc_p = TRUE;
 19603|                     settings.init_mechanisms();
 19604|                     settings.condemned_generation = gen;
 19605|                     settings.gc_index = (size_t)dd_collection_count (dynamic_data_of (0)) + 2;
 19606|                     do_pre_gc();
 19607|                     dprintf (GTC_LOG, ("doing gen%d before doing a bgc", gen));
 19608|                 }
 19609|                 if (!do_ephemeral_gc_p)
 19610|                 {
 19611|                     do_background_gc();
 19612|                 }
 19613|             }
 19614|             else
 19615|             {
 19616|                 settings.compaction = TRUE;
 19617|                 c_write (settings.concurrent, FALSE);
 19618|             }
 19619| #ifdef MULTIPLE_HEAPS
 19620|             gc_t_join.restart();
 19621| #endif //MULTIPLE_HEAPS
 19622|         }
 19623|         if (do_concurrent_p)
 19624|         {
 19625|             memset (&bgc_data_per_heap, 0, sizeof (bgc_data_per_heap));
 19626|             memcpy (&bgc_data_per_heap, &gc_data_per_heap, sizeof(gc_data_per_heap));
 19627|             if (do_ephemeral_gc_p)
 19628|             {
 19629|                 dprintf (2, ("GC threads running, doing gen%d GC", settings.condemned_generation));
 19630|                 gen_to_condemn_reasons.init();
 19631|                 gen_to_condemn_reasons.set_condition (gen_before_bgc);
 19632|                 gc_data_per_heap.gen_to_condemn_reasons.init (&gen_to_condemn_reasons);
 19633|                 gc1();
 19634| #ifdef MULTIPLE_HEAPS
 19635|                 gc_t_join.join(this, gc_join_bgc_after_ephemeral);
 19636|                 if (gc_t_join.joined())
 19637| #endif //MULTIPLE_HEAPS
 19638|                 {
 19639| #ifdef MULTIPLE_HEAPS
 19640|                     do_post_gc();
 19641| #endif //MULTIPLE_HEAPS
 19642|                     settings = saved_bgc_settings;
 19643|                     assert (settings.concurrent);
 19644|                     do_background_gc();
 19645| #ifdef MULTIPLE_HEAPS
 19646|                     gc_t_join.restart();
 19647| #endif //MULTIPLE_HEAPS
 19648|                 }
 19649|             }
 19650|         }
 19651|         else
 19652|         {
 19653|             dprintf (2, ("couldn't create BGC threads, reverting to doing a blocking GC"));
 19654|             gc1();
 19655|         }
 19656|     }
 19657|     else
 19658| #endif //BACKGROUND_GC
 19659|     {
 19660|         gc1();
 19661|     }
 19662| #ifndef MULTIPLE_HEAPS
 19663|     allocation_running_time = GCToOSInterface::GetLowPrecisionTimeStamp();
 19664|     allocation_running_amount = dd_new_allocation (dynamic_data_of (0));
 19665|     fgn_last_alloc = dd_new_allocation (dynamic_data_of (0));
 19666| #endif //MULTIPLE_HEAPS
 19667| done:
 19668|     if (saved_settings_pause_mode == pause_no_gc)
 19669|         allocate_for_no_gc_after_gc();
 19670| }
 19671| #define mark_stack_empty_p() (mark_stack_base == mark_stack_tos)
 19672| inline
 19673| size_t gc_heap::get_promoted_bytes()
 19674| {
 19675| #ifdef USE_REGIONS
 19676|     if (!survived_per_region)
 19677|     {
 19678|         dprintf (REGIONS_LOG, ("no space to store promoted bytes"));
 19679|         return 0;
 19680|     }
 19681|     dprintf (3, ("h%d getting surv", heap_number));
 19682|     size_t promoted = 0;
 19683|     for (size_t i = 0; i < region_count; i++)
 19684|     {
 19685|         if (survived_per_region[i] > 0)
 19686|         {
 19687|             heap_segment* region = get_region_at_index (i);
 19688|             dprintf (REGIONS_LOG, ("h%d region[%zd] %p(g%d)(%s) surv: %zd(%p)",
 19689|                 heap_number, i,
 19690|                 heap_segment_mem (region),
 19691|                 heap_segment_gen_num (region),
 19692|                 (heap_segment_loh_p (region) ? "LOH" : (heap_segment_poh_p (region) ? "POH" :"SOH")),
 19693|                 survived_per_region[i],
 19694|                 &survived_per_region[i]));
 19695|             promoted += survived_per_region[i];
 19696|         }
 19697|     }
 19698| #ifdef _DEBUG
 19699|     dprintf (REGIONS_LOG, ("h%d global recorded %zd, regions recorded %zd",
 19700|         heap_number, promoted_bytes (heap_number), promoted));
 19701|     assert (promoted_bytes (heap_number) == promoted);
 19702| #endif //_DEBUG
 19703|     return promoted;
 19704| #else //USE_REGIONS
 19705| #ifdef MULTIPLE_HEAPS
 19706|     return g_promoted [heap_number*16];
 19707| #else //MULTIPLE_HEAPS
 19708|     return g_promoted;
 19709| #endif //MULTIPLE_HEAPS
 19710| #endif //USE_REGIONS
 19711| }
 19712| #ifdef USE_REGIONS
 19713| void gc_heap::sync_promoted_bytes()
 19714| {
 19715|     int condemned_gen_number = settings.condemned_generation;
 19716|     int highest_gen_number = ((condemned_gen_number == max_generation) ?
 19717|                               (total_generation_count - 1) : settings.condemned_generation);
 19718|     int stop_gen_idx = get_stop_generation_index (condemned_gen_number);
 19719| #ifdef MULTIPLE_HEAPS
 19720|     for (int i = 0; i < n_heaps; i++)
 19721|     {
 19722|         gc_heap* hp = g_heaps[i];
 19723| #else //MULTIPLE_HEAPS
 19724|     {
 19725|         gc_heap* hp = pGenGCHeap;
 19726| #endif //MULTIPLE_HEAPS
 19727|         for (int gen_idx = highest_gen_number; gen_idx >= stop_gen_idx; gen_idx--)
 19728|         {
 19729|             generation* condemned_gen = hp->generation_of (gen_idx);
 19730|             heap_segment* current_region = heap_segment_rw (generation_start_segment (condemned_gen));
 19731|             while (current_region)
 19732|             {
 19733|                 size_t region_index = get_basic_region_index_for_address (heap_segment_mem (current_region));
 19734| #ifdef MULTIPLE_HEAPS
 19735|                 size_t total_surv = 0;
 19736|                 size_t total_old_card_surv = 0;
 19737|                 for (int hp_idx = 0; hp_idx < n_heaps; hp_idx++)
 19738|                 {
 19739|                     total_surv += g_heaps[hp_idx]->survived_per_region[region_index];
 19740|                     total_old_card_surv += g_heaps[hp_idx]->old_card_survived_per_region[region_index];
 19741|                 }
 19742|                 heap_segment_survived (current_region) = total_surv;
 19743|                 heap_segment_old_card_survived (current_region) = (int)total_old_card_surv;
 19744| #else
 19745|                 heap_segment_survived (current_region) = survived_per_region[region_index];
 19746|                 heap_segment_old_card_survived (current_region) =
 19747|                     (int)(old_card_survived_per_region[region_index]);
 19748| #endif //MULTIPLE_HEAPS
 19749|                 dprintf (REGIONS_LOG, ("region #%zd %p surv %zd, old card surv %d",
 19750|                     region_index,
 19751|                     heap_segment_mem (current_region),
 19752|                     heap_segment_survived (current_region),
 19753|                     heap_segment_old_card_survived (current_region)));
 19754|                 current_region = heap_segment_next (current_region);
 19755|             }
 19756|         }
 19757|     }
 19758| }
 19759| #ifdef MULTIPLE_HEAPS
 19760| void gc_heap::set_heap_for_contained_basic_regions (heap_segment* region, gc_heap* hp)
 19761| {
 19762|     uint8_t* region_start = get_region_start (region);
 19763|     uint8_t* region_end = heap_segment_reserved (region);
 19764|     int num_basic_regions = (int)((region_end - region_start) >> min_segment_size_shr);
 19765|     for (int i = 0; i < num_basic_regions; i++)
 19766|     {
 19767|         uint8_t* basic_region_start = region_start + ((size_t)i << min_segment_size_shr);
 19768|         heap_segment* basic_region = get_region_info (basic_region_start);
 19769|         heap_segment_heap (basic_region) = hp;
 19770|     }
 19771| }
 19772| heap_segment* gc_heap::unlink_first_rw_region (int gen_idx)
 19773| {
 19774|     generation* gen = generation_of (gen_idx);
 19775|     heap_segment* prev_region = generation_tail_ro_region (gen);
 19776|     heap_segment* region = nullptr;
 19777|     if (prev_region)
 19778|     {
 19779|         assert (heap_segment_read_only_p (prev_region));
 19780|         region = heap_segment_next (prev_region);
 19781|         assert (region != nullptr);
 19782|         if (heap_segment_next (region) == nullptr)
 19783|         {
 19784|             assert (region == generation_tail_region (gen));
 19785|             return nullptr;
 19786|         }
 19787|         heap_segment_next (prev_region) = heap_segment_next (region);
 19788|     }
 19789|     else
 19790|     {
 19791|         region = generation_start_segment (gen);
 19792|         assert (region != nullptr);
 19793|         if (heap_segment_next (region) == nullptr)
 19794|         {
 19795|             assert (region == generation_tail_region (gen));
 19796|             return nullptr;
 19797|         }
 19798|         generation_start_segment (gen) = heap_segment_next (region);
 19799|     }
 19800|     assert (region != generation_tail_region (gen));
 19801|     assert (!heap_segment_read_only_p (region));
 19802|     dprintf (REGIONS_LOG, ("unlink_first_rw_region on heap: %d gen: %d region: %p", heap_number, gen_idx, heap_segment_mem (region)));
 19803| #if defined(_DEBUG) && defined(HOST_64BIT)
 19804| #ifndef COMMITTED_BYTES_SHADOW
 19805|     if (heap_hard_limit)
 19806| #endif //!COMMITTED_BYTES_SHADOW
 19807|     {
 19808|         int old_oh = heap_segment_oh (region);
 19809|         int old_heap = heap_segment_heap (region)->heap_number;
 19810|         dprintf(3, ("commit-accounting:  from %d to temp [%p, %p) for heap %d", old_oh, get_region_start (region), heap_segment_committed (region), old_heap));
 19811|         size_t committed = heap_segment_committed (region) - get_region_start (region);
 19812|         check_commit_cs.Enter();
 19813|         assert (g_heaps[old_heap]->committed_by_oh_per_heap[old_oh] >= committed);
 19814|         g_heaps[old_heap]->committed_by_oh_per_heap[old_oh] -= committed;
 19815|         check_commit_cs.Leave();
 19816|     }
 19817| #endif // _DEBUG && HOST_64BIT
 19818|     set_heap_for_contained_basic_regions (region, nullptr);
 19819|     return region;
 19820| }
 19821| void gc_heap::thread_rw_region_front (int gen_idx, heap_segment* region)
 19822| {
 19823|     generation* gen = generation_of (gen_idx);
 19824|     assert (!heap_segment_read_only_p (region));
 19825|     heap_segment* prev_region = generation_tail_ro_region (gen);
 19826|     if (prev_region)
 19827|     {
 19828|         heap_segment_next (region) = heap_segment_next (prev_region);
 19829|         heap_segment_next (prev_region) = region;
 19830|     }
 19831|     else
 19832|     {
 19833|         heap_segment_next (region) = generation_start_segment (gen);
 19834|         generation_start_segment (gen) = region;
 19835|     }
 19836|     if (heap_segment_next (region) == nullptr)
 19837|     {
 19838|         generation_tail_region (gen) = region;
 19839|     }
 19840|     dprintf (REGIONS_LOG, ("thread_rw_region_front on heap: %d gen: %d region: %p", heap_number, gen_idx, heap_segment_mem (region)));
 19841| #if defined(_DEBUG) && defined(HOST_64BIT)
 19842| #ifndef COMMITTED_BYTES_SHADOW
 19843|     if (heap_hard_limit)
 19844| #endif //!COMMITTED_BYTES_SHADOW
 19845|     {
 19846|         int new_oh = gen_to_oh (gen_idx);
 19847|         int new_heap = this->heap_number;
 19848|         dprintf(3, ("commit-accounting:  from temp to %d [%p, %p) for heap %d", new_oh, get_region_start (region), heap_segment_committed (region), new_heap));
 19849|         size_t committed = heap_segment_committed (region) - get_region_start (region);
 19850|         check_commit_cs.Enter();
 19851|         assert (heap_segment_heap (region) == nullptr);
 19852|         g_heaps[new_heap]->committed_by_oh_per_heap[new_oh] += committed;
 19853|         check_commit_cs.Leave();
 19854|     }
 19855| #endif // _DEBUG && HOST_64BIT
 19856|     set_heap_for_contained_basic_regions (region, this);
 19857| }
 19858| #endif // MULTIPLE_HEAPS
 19859| void gc_heap::equalize_promoted_bytes(int condemned_gen_number)
 19860| {
 19861| #ifdef MULTIPLE_HEAPS
 19862|     int highest_gen_number = ((condemned_gen_number == max_generation) ?
 19863|         (total_generation_count - 1) : condemned_gen_number);
 19864|     int stop_gen_idx = get_stop_generation_index (condemned_gen_number);
 19865|     for (int gen_idx = highest_gen_number; gen_idx >= stop_gen_idx; gen_idx--)
 19866|     {
 19867|         size_t total_surv = 0;
 19868|         size_t max_surv_per_heap = 0;
 19869|         size_t surv_per_heap[MAX_SUPPORTED_CPUS];
 19870|         for (int i = 0; i < n_heaps; i++)
 19871|         {
 19872|             surv_per_heap[i] = 0;
 19873|             gc_heap* hp = g_heaps[i];
 19874|             generation* condemned_gen = hp->generation_of (gen_idx);
 19875|             heap_segment* current_region = heap_segment_rw (generation_start_segment (condemned_gen));
 19876|             while (current_region)
 19877|             {
 19878|                 total_surv += heap_segment_survived (current_region);
 19879|                 surv_per_heap[i] += heap_segment_survived (current_region);
 19880|                 current_region = heap_segment_next (current_region);
 19881|             }
 19882|             max_surv_per_heap = max (max_surv_per_heap, surv_per_heap[i]);
 19883|             dprintf (REGIONS_LOG, ("gen: %d heap %d surv: %zd", gen_idx, i, surv_per_heap[i]));
 19884|         }
 19885|         size_t avg_surv_per_heap = (total_surv + n_heaps - 1) / n_heaps;
 19886|         if (avg_surv_per_heap != 0)
 19887|         {
 19888|             dprintf (REGIONS_LOG, ("before equalize: gen: %d avg surv: %zd max_surv: %zd imbalance: %zd", gen_idx, avg_surv_per_heap, max_surv_per_heap, max_surv_per_heap*100/avg_surv_per_heap));
 19889|         }
 19890|         heap_segment* surplus_regions = nullptr;
 19891|         size_t max_deficit = 0;
 19892|         size_t max_survived = 0;
 19893|         for (int i = 0; i < n_heaps; i++)
 19894|         {
 19895|             while (surv_per_heap[i] > avg_surv_per_heap)
 19896|             {
 19897|                 heap_segment* region = g_heaps[i]->unlink_first_rw_region (gen_idx);
 19898|                 if (region == nullptr)
 19899|                 {
 19900|                     break;
 19901|                 }
 19902|                 assert (surv_per_heap[i] >= heap_segment_survived (region));
 19903|                 dprintf (REGIONS_LOG, ("heap: %d surv: %zd - %zd = %zd",
 19904|                     i,
 19905|                     surv_per_heap[i],
 19906|                     heap_segment_survived (region),
 19907|                     surv_per_heap[i] - heap_segment_survived (region)));
 19908|                 surv_per_heap[i] -= heap_segment_survived (region);
 19909|                 heap_segment_next (region) = surplus_regions;
 19910|                 surplus_regions = region;
 19911|                 max_survived = max (max_survived, heap_segment_survived (region));
 19912|             }
 19913|             if (surv_per_heap[i] < avg_surv_per_heap)
 19914|             {
 19915|                 size_t deficit = avg_surv_per_heap - surv_per_heap[i];
 19916|                 max_deficit = max (max_deficit, deficit);
 19917|             }
 19918|         }
 19919|         for (int i = 0; i < n_heaps; i++)
 19920|         {
 19921|             gc_heap* hp = g_heaps[i];
 19922|             generation* gen = hp->generation_of (gen_idx);
 19923|             if (heap_segment_rw (generation_start_segment (gen)) == nullptr)
 19924|             {
 19925|                 heap_segment* start_region = surplus_regions;
 19926|                 if (start_region != nullptr)
 19927|                 {
 19928|                     surplus_regions = heap_segment_next (start_region);
 19929|                 }
 19930|                 else
 19931|                 {
 19932|                     for (int j = 0; j < n_heaps; j++)
 19933|                     {
 19934|                         start_region = g_heaps[j]->unlink_first_rw_region (gen_idx);
 19935|                         if (start_region != nullptr)
 19936|                         {
 19937|                             surv_per_heap[j] -= heap_segment_survived (start_region);
 19938|                             size_t deficit = avg_surv_per_heap - surv_per_heap[j];
 19939|                             max_deficit = max (max_deficit, deficit);
 19940|                             break;
 19941|                         }
 19942|                     }
 19943|                 }
 19944|                 assert (start_region);
 19945|                 dprintf (3, ("making sure heap %d gen %d has at least one region by adding region %zx", start_region));
 19946|                 heap_segment_next (start_region) = nullptr;
 19947|                 set_heap_for_contained_basic_regions (start_region, hp);
 19948|                 max_survived = max (max_survived, heap_segment_survived (start_region));
 19949|                 hp->thread_start_region (gen, start_region);
 19950|                 surv_per_heap[i] += heap_segment_survived (start_region);
 19951|             }
 19952|         }
 19953|         const int NUM_SIZE_CLASSES = 16;
 19954|         heap_segment* surplus_regions_by_size_class[NUM_SIZE_CLASSES];
 19955|         memset (surplus_regions_by_size_class, 0, sizeof(surplus_regions_by_size_class));
 19956|         double survived_scale_factor = ((double)NUM_SIZE_CLASSES) / (max_survived + 1);
 19957|         heap_segment* next_region;
 19958|         for (heap_segment* region = surplus_regions; region != nullptr; region = next_region)
 19959|         {
 19960|             size_t size_class = (size_t)(heap_segment_survived (region)*survived_scale_factor);
 19961|             assert ((0 <= size_class) && (size_class < NUM_SIZE_CLASSES));
 19962|             next_region = heap_segment_next (region);
 19963|             heap_segment_next (region) = surplus_regions_by_size_class[size_class];
 19964|             surplus_regions_by_size_class[size_class] = region;
 19965|         }
 19966|         int next_heap_in_size_class[MAX_SUPPORTED_CPUS];
 19967|         int heaps_by_deficit_size_class[NUM_SIZE_CLASSES];
 19968|         for (int i = 0; i < NUM_SIZE_CLASSES; i++)
 19969|         {
 19970|             heaps_by_deficit_size_class[i] = -1;
 19971|         }
 19972|         double deficit_scale_factor = ((double)NUM_SIZE_CLASSES) / (max_deficit + 1);
 19973|         for (int i = 0; i < n_heaps; i++)
 19974|         {
 19975|             if (avg_surv_per_heap > surv_per_heap[i])
 19976|             {
 19977|                 size_t deficit = avg_surv_per_heap - surv_per_heap[i];
 19978|                 int size_class = (int)(deficit*deficit_scale_factor);
 19979|                 assert ((0 <= size_class) && (size_class < NUM_SIZE_CLASSES));
 19980|                 next_heap_in_size_class[i] = heaps_by_deficit_size_class[size_class];
 19981|                 heaps_by_deficit_size_class[size_class] = i;
 19982|             }
 19983|         }
 19984|         int region_size_class = NUM_SIZE_CLASSES - 1;
 19985|         int heap_size_class = NUM_SIZE_CLASSES - 1;
 19986|         while (region_size_class >= 0)
 19987|         {
 19988|             heap_segment* region = surplus_regions_by_size_class[region_size_class];
 19989|             if (region == nullptr)
 19990|             {
 19991|                 region_size_class--;
 19992|                 continue;
 19993|             }
 19994|             int heap_num;
 19995|             while (true)
 19996|             {
 19997|                 if (heap_size_class < 0)
 19998|                 {
 19999|                     heap_num = 0;
 20000|                     break;
 20001|                 }
 20002|                 heap_num = heaps_by_deficit_size_class[heap_size_class];
 20003|                 if (heap_num >= 0)
 20004|                 {
 20005|                     break;
 20006|                 }
 20007|                 heap_size_class--;
 20008|             }
 20009|             surplus_regions_by_size_class[region_size_class] = heap_segment_next (region);
 20010|             g_heaps[heap_num]->thread_rw_region_front (gen_idx, region);
 20011|             dprintf (REGIONS_LOG, ("heap: %d surv: %zd + %zd = %zd",
 20012|                 heap_num,
 20013|                 surv_per_heap[heap_num],
 20014|                 heap_segment_survived (region),
 20015|                 surv_per_heap[heap_num] + heap_segment_survived (region)));
 20016|             surv_per_heap[heap_num] += heap_segment_survived (region);
 20017|             if (heap_size_class < 0)
 20018|             {
 20019|                 continue;
 20020|             }
 20021|             if (surv_per_heap[heap_num] >= avg_surv_per_heap)
 20022|             {
 20023|                 heaps_by_deficit_size_class[heap_size_class] = next_heap_in_size_class[heap_num];
 20024|                 continue;
 20025|             }
 20026|             size_t new_deficit = avg_surv_per_heap - surv_per_heap[heap_num];
 20027|             int new_heap_size_class = (int)(new_deficit*deficit_scale_factor);
 20028|             if (new_heap_size_class != heap_size_class)
 20029|             {
 20030|                 assert (new_heap_size_class < heap_size_class);
 20031|                 assert ((0 <= new_heap_size_class) && (new_heap_size_class < NUM_SIZE_CLASSES));
 20032|                 heaps_by_deficit_size_class[heap_size_class] = next_heap_in_size_class[heap_num];
 20033|                 next_heap_in_size_class[heap_num] = heaps_by_deficit_size_class[new_heap_size_class];
 20034|                 heaps_by_deficit_size_class[new_heap_size_class] = heap_num;
 20035|             }
 20036|         }
 20037|         for (int i = 0; i < n_heaps; i++)
 20038|         {
 20039|             gc_heap* hp = g_heaps[i];
 20040|             hp->verify_regions (gen_idx, true, true);
 20041|         }
 20042| #ifdef TRACE_GC
 20043|         max_surv_per_heap = 0;
 20044|         for (int i = 0; i < n_heaps; i++)
 20045|         {
 20046|             max_surv_per_heap = max (max_surv_per_heap, surv_per_heap[i]);
 20047|         }
 20048|         if (avg_surv_per_heap != 0)
 20049|         {
 20050|             dprintf (REGIONS_LOG, ("after equalize: gen: %d avg surv: %zd max_surv: %zd imbalance: %zd", gen_idx, avg_surv_per_heap, max_surv_per_heap, max_surv_per_heap*100/avg_surv_per_heap));
 20051|         }
 20052| #endif // TRACE_GC
 20053|     }
 20054| #endif //MULTIPLE_HEAPS
 20055| }
 20056| #ifdef DYNAMIC_HEAP_COUNT
 20057| #define DECOMMISSIONED_VALUE 0xdec0dec0dec0dec0
 20058| static const size_t DECOMMISSIONED_SIZE_T = DECOMMISSIONED_VALUE;
 20059| static const ptrdiff_t DECOMMISSIONED_PTRDIFF_T = (ptrdiff_t)DECOMMISSIONED_VALUE;
 20060| static const ptrdiff_t DECOMMISSIONED_UINT64_T = (uint64_t)DECOMMISSIONED_VALUE;
 20061| static uint8_t* const DECOMMISSIONED_UINT8_T_P = (uint8_t*)DECOMMISSIONED_VALUE;
 20062| static uint8_t** const DECOMMISSIONED_UINT8_T_PP = (uint8_t**)DECOMMISSIONED_VALUE;
 20063| static PTR_heap_segment const DECOMMISSIONED_REGION_P = (PTR_heap_segment)DECOMMISSIONED_VALUE;
 20064| static mark* const DECOMMISSIONED_MARK_P = (mark*)DECOMMISSIONED_VALUE;
 20065| static const BOOL DECOMMISSIONED_BOOL = 0xdec0dec0;
 20066| static const BOOL DECOMMISSIONED_INT = (int)0xdec0dec0;
 20067| static const float DECOMMISSIONED_FLOAT = (float)DECOMMISSIONED_VALUE;
 20068| static const ptrdiff_t UNINITIALIZED_VALUE  = 0xbaadbaadbaadbaad;
 20069| void gc_heap::check_decommissioned_heap()
 20070| {
 20071|     assert (generation_skip_ratio               == DECOMMISSIONED_INT);
 20072|     assert (gen0_must_clear_bricks              == DECOMMISSIONED_INT);
 20073|     assert (freeable_uoh_segment                == DECOMMISSIONED_REGION_P);
 20074| #ifdef BACKGROUND_GC
 20075|     assert (freeable_soh_segment                == DECOMMISSIONED_REGION_P);
 20076| #endif //BACKGROUND_GC
 20077| #ifdef FEATURE_LOH_COMPACTION
 20078|     assert (loh_pinned_queue_length             == DECOMMISSIONED_SIZE_T);
 20079|     assert (loh_pinned_queue_decay              == DECOMMISSIONED_INT);
 20080|     assert (loh_pinned_queue                    == DECOMMISSIONED_MARK_P);
 20081| #endif //FEATURE_LOH_COMPACTION
 20082|     assert (gen0_bricks_cleared                 == DECOMMISSIONED_BOOL);
 20083|     assert (alloc_allocated                     == DECOMMISSIONED_UINT8_T_P);
 20084|     assert (ephemeral_heap_segment              == DECOMMISSIONED_REGION_P);
 20085| #ifdef USE_REGIONS
 20086| #endif //USE_REGIONS
 20087|     assert (more_space_lock_soh.lock            == lock_decommissioned);
 20088|     assert (more_space_lock_uoh.lock            == lock_decommissioned);
 20089|     assert (soh_allocation_no_gc                == DECOMMISSIONED_SIZE_T);
 20090|     assert (loh_allocation_no_gc                == DECOMMISSIONED_SIZE_T);
 20091|     for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20092|     {
 20093|         generation* gen = generation_of (gen_idx);
 20094|         assert (generation_start_segment                   (gen) == DECOMMISSIONED_REGION_P);
 20095|         assert (generation_allocation_segment              (gen) == DECOMMISSIONED_REGION_P);
 20096|         assert (generation_tail_region                     (gen) == DECOMMISSIONED_REGION_P);
 20097|         assert (generation_tail_ro_region                  (gen) == DECOMMISSIONED_REGION_P);
 20098|         assert (generation_allocation_context_start_region (gen) == DECOMMISSIONED_UINT8_T_P);
 20099|         assert (gen->plan_start_segment                          == DECOMMISSIONED_REGION_P);
 20100|         assert (generation_free_list_allocated             (gen) == DECOMMISSIONED_SIZE_T);
 20101|         assert (generation_end_seg_allocated               (gen) == DECOMMISSIONED_SIZE_T);
 20102|         assert (generation_allocate_end_seg_p              (gen) == DECOMMISSIONED_BOOL);
 20103|         assert (generation_condemned_allocated             (gen) == DECOMMISSIONED_SIZE_T);
 20104|         assert (generation_sweep_allocated                 (gen) == DECOMMISSIONED_SIZE_T);
 20105|         assert (generation_free_list_space                 (gen) == DECOMMISSIONED_SIZE_T);
 20106|         assert (generation_free_obj_space                  (gen) == DECOMMISSIONED_SIZE_T);
 20107|         assert (generation_allocation_size                 (gen) == DECOMMISSIONED_SIZE_T);
 20108|         assert (generation_pinned_allocation_compact_size  (gen) == DECOMMISSIONED_SIZE_T);
 20109|         assert (generation_pinned_allocation_sweep_size    (gen) == DECOMMISSIONED_SIZE_T);
 20110|         assert (gen->gen_num                                     == DECOMMISSIONED_INT);
 20111| #ifdef DOUBLY_LINKED_FL
 20112|         assert (generation_set_bgc_mark_bit_p              (gen) == DECOMMISSIONED_BOOL);
 20113|         assert (generation_last_free_list_allocated        (gen) == DECOMMISSIONED_UINT8_T_P);
 20114| #endif //DOUBLY_LINKED_FL
 20115|         dynamic_data* dd = dynamic_data_of (gen_idx);
 20116|         assert (dd_new_allocation                  (dd) == DECOMMISSIONED_PTRDIFF_T);
 20117|         assert (dd_gc_new_allocation               (dd) == DECOMMISSIONED_PTRDIFF_T);
 20118|         assert (dd_surv                     (dd) == (float)DECOMMISSIONED_VALUE);
 20119|         assert (dd_desired_allocation              (dd) == DECOMMISSIONED_SIZE_T);
 20120|         assert (dd_begin_data_size                 (dd) == DECOMMISSIONED_SIZE_T);
 20121|         assert (dd_survived_size                   (dd) == DECOMMISSIONED_SIZE_T);
 20122|         assert (dd_pinned_survived_size            (dd) == DECOMMISSIONED_SIZE_T);
 20123|         assert (dd_artificial_pinned_survived_size (dd) == DECOMMISSIONED_SIZE_T);
 20124|         assert (dd_added_pinned_size               (dd) == DECOMMISSIONED_SIZE_T);
 20125| #ifdef SHORT_PLUGS
 20126|         assert (dd_padding_size                    (dd) == DECOMMISSIONED_SIZE_T);
 20127| #endif //SHORT_PLUGS
 20128| #if defined (RESPECT_LARGE_ALIGNMENT) || defined (FEATURE_STRUCTALIGN)
 20129|         assert (dd_num_npinned_plugs               (dd) == DECOMMISSIONED_SIZE_T);
 20130| #endif //RESPECT_LARGE_ALIGNMENT || FEATURE_STRUCTALIGN
 20131|         assert (dd_current_size                    (dd) == DECOMMISSIONED_SIZE_T);
 20132|         assert (dd_collection_count                (dd) == DECOMMISSIONED_SIZE_T);
 20133|         assert (dd_promoted_size                   (dd) == DECOMMISSIONED_SIZE_T);
 20134|         assert (dd_freach_previous_promotion       (dd) == DECOMMISSIONED_SIZE_T);
 20135|         assert (dd_fragmentation                   (dd) == DECOMMISSIONED_SIZE_T);
 20136|         assert (dd_gc_clock                        (dd) == DECOMMISSIONED_SIZE_T);
 20137|         assert (dd_time_clock                      (dd) == DECOMMISSIONED_SIZE_T);
 20138|         assert (dd_previous_time_clock             (dd) == DECOMMISSIONED_SIZE_T);
 20139|         assert (dd_gc_elapsed_time                 (dd) == DECOMMISSIONED_SIZE_T);
 20140|     }
 20141| }
 20142| void gc_heap::decommission_heap()
 20143| {
 20144|     set_gc_done();
 20145|     generation_skip_ratio               = DECOMMISSIONED_INT;
 20146|     gen0_must_clear_bricks              = DECOMMISSIONED_INT;
 20147|     freeable_uoh_segment                = DECOMMISSIONED_REGION_P;
 20148|     memset ((void *)gen2_alloc_list, DECOMMISSIONED_INT, sizeof(gen2_alloc_list[0])*(NUM_GEN2_ALIST - 1));
 20149| #ifdef BACKGROUND_GC
 20150|     freeable_soh_segment                = DECOMMISSIONED_REGION_P;
 20151| #endif //BACKGROUND_GC
 20152| #ifdef FEATURE_LOH_COMPACTION
 20153|     loh_pinned_queue_length             = DECOMMISSIONED_SIZE_T;
 20154|     loh_pinned_queue_decay              = DECOMMISSIONED_INT;
 20155|     loh_pinned_queue                    = DECOMMISSIONED_MARK_P;
 20156| #endif //FEATURE_LOH_COMPACTION
 20157|     gen0_bricks_cleared                 = DECOMMISSIONED_BOOL;
 20158|     memset ((void *)loh_alloc_list, DECOMMISSIONED_INT, sizeof(loh_alloc_list));
 20159|     memset ((void *)poh_alloc_list, DECOMMISSIONED_INT, sizeof(poh_alloc_list));
 20160|     alloc_allocated                     = DECOMMISSIONED_UINT8_T_P;
 20161|     ephemeral_heap_segment              = DECOMMISSIONED_REGION_P;
 20162| #ifdef USE_REGIONS
 20163|     memset ((void *)free_regions, DECOMMISSIONED_INT, sizeof(free_regions));
 20164| #endif //USE_REGIONS
 20165|     assert (more_space_lock_soh.lock    == lock_free);
 20166|     more_space_lock_soh.lock            = lock_decommissioned;
 20167|     assert (more_space_lock_uoh.lock    == lock_free);
 20168|     more_space_lock_uoh.lock            = lock_decommissioned;
 20169|     soh_allocation_no_gc                = DECOMMISSIONED_SIZE_T;
 20170|     loh_allocation_no_gc                = DECOMMISSIONED_SIZE_T;
 20171|     for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20172|     {
 20173|         generation* gen = generation_of (gen_idx);
 20174|         generation_allocator (gen)->clear();
 20175|         memset (generation_alloc_context           (gen),  DECOMMISSIONED_INT, sizeof(alloc_context));
 20176|         generation_start_segment                   (gen) = DECOMMISSIONED_REGION_P;
 20177|         generation_allocation_segment              (gen) = DECOMMISSIONED_REGION_P;
 20178|         generation_allocation_context_start_region (gen) = DECOMMISSIONED_UINT8_T_P;
 20179|         generation_tail_region                     (gen) = DECOMMISSIONED_REGION_P;
 20180|         gen->plan_start_segment                          = DECOMMISSIONED_REGION_P;
 20181|         generation_tail_ro_region                  (gen) = DECOMMISSIONED_REGION_P;
 20182|         generation_free_list_allocated             (gen) = DECOMMISSIONED_SIZE_T;
 20183|         generation_end_seg_allocated               (gen) = DECOMMISSIONED_SIZE_T;
 20184|         generation_allocate_end_seg_p              (gen) = DECOMMISSIONED_BOOL;
 20185|         generation_condemned_allocated             (gen) = DECOMMISSIONED_SIZE_T;
 20186|         generation_sweep_allocated                 (gen) = DECOMMISSIONED_SIZE_T;
 20187|         generation_free_list_space                 (gen) = DECOMMISSIONED_SIZE_T;
 20188|         generation_free_obj_space                  (gen) = DECOMMISSIONED_SIZE_T;
 20189|         generation_allocation_size                 (gen) = DECOMMISSIONED_SIZE_T;
 20190|         generation_pinned_allocation_compact_size  (gen) = DECOMMISSIONED_SIZE_T;
 20191|         generation_pinned_allocation_sweep_size    (gen) = DECOMMISSIONED_SIZE_T;
 20192|         gen->gen_num                                     = DECOMMISSIONED_INT;
 20193| #ifdef DOUBLY_LINKED_FL
 20194|         generation_set_bgc_mark_bit_p              (gen) = DECOMMISSIONED_BOOL;
 20195|         generation_last_free_list_allocated        (gen) = DECOMMISSIONED_UINT8_T_P;
 20196| #endif //DOUBLY_LINKED_FL
 20197|         dynamic_data* dd = dynamic_data_of (gen_idx);
 20198|         dd_new_allocation                  (dd) = DECOMMISSIONED_SIZE_T;
 20199|         dd_gc_new_allocation               (dd) = DECOMMISSIONED_PTRDIFF_T;
 20200|         dd_surv                     (dd) = (float)DECOMMISSIONED_VALUE;
 20201|         dd_desired_allocation              (dd) = DECOMMISSIONED_SIZE_T;
 20202|         dd_begin_data_size                 (dd) = DECOMMISSIONED_SIZE_T;
 20203|         dd_survived_size                   (dd) = DECOMMISSIONED_SIZE_T;
 20204|         dd_pinned_survived_size            (dd) = DECOMMISSIONED_SIZE_T;
 20205|         dd_artificial_pinned_survived_size (dd) = DECOMMISSIONED_SIZE_T;
 20206|         dd_added_pinned_size               (dd) = DECOMMISSIONED_SIZE_T;
 20207| #ifdef SHORT_PLUGS
 20208|         dd_padding_size                    (dd) = DECOMMISSIONED_SIZE_T;
 20209| #endif //SHORT_PLUGS
 20210| #if defined (RESPECT_LARGE_ALIGNMENT) || defined (FEATURE_STRUCTALIGN)
 20211|         dd_num_npinned_plugs               (dd) = DECOMMISSIONED_SIZE_T;
 20212| #endif //RESPECT_LARGE_ALIGNMENT || FEATURE_STRUCTALIGN
 20213|         dd_current_size                    (dd) = DECOMMISSIONED_SIZE_T;
 20214|         dd_collection_count                (dd) = DECOMMISSIONED_SIZE_T;
 20215|         dd_promoted_size                   (dd) = DECOMMISSIONED_SIZE_T;
 20216|         dd_freach_previous_promotion       (dd) = DECOMMISSIONED_SIZE_T;
 20217|         dd_fragmentation                   (dd) = DECOMMISSIONED_SIZE_T;
 20218|         dd_gc_clock                        (dd) = DECOMMISSIONED_SIZE_T;
 20219|         dd_time_clock                      (dd) = DECOMMISSIONED_SIZE_T;
 20220|         dd_previous_time_clock             (dd) = DECOMMISSIONED_SIZE_T;
 20221|         dd_gc_elapsed_time                 (dd) = DECOMMISSIONED_SIZE_T;
 20222|     }
 20223| }
 20224| void gc_heap::recommission_heap()
 20225| {
 20226|     generation_skip_ratio               = 0;
 20227|     gen0_must_clear_bricks              = 0;
 20228|     freeable_uoh_segment                = nullptr;
 20229|     memset ((void *)gen2_alloc_list, 0, sizeof(gen2_alloc_list));
 20230| #ifdef BACKGROUND_GC
 20231|     freeable_soh_segment                = nullptr;
 20232| #endif //BACKGROUND_GC
 20233| #ifdef FEATURE_LOH_COMPACTION
 20234|     loh_pinned_queue_length             = 0;
 20235|     loh_pinned_queue_decay              = 0;
 20236|     loh_pinned_queue                    = 0;
 20237| #endif //FEATURE_LOH_COMPACTION
 20238|     gen0_bricks_cleared                 = FALSE;
 20239|     memset ((void *)loh_alloc_list, 0, sizeof(loh_alloc_list));
 20240|     memset ((void *)poh_alloc_list, 0, sizeof(poh_alloc_list));
 20241|     alloc_allocated                     = 0;
 20242|     ephemeral_heap_segment              = nullptr;
 20243|     for (int kind = 0; kind < count_free_region_kinds; kind++)
 20244|     {
 20245|         free_regions[kind].reset();
 20246|     }
 20247|     more_space_lock_soh.lock            = lock_free;
 20248|     more_space_lock_uoh.lock            = lock_free;
 20249|     soh_allocation_no_gc                = 0;
 20250|     loh_allocation_no_gc                = 0;
 20251|     bgc_alloc_lock->init();
 20252|     gc_heap* heap0 = g_heaps[0];
 20253|     for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20254|     {
 20255|         generation* gen = generation_of (gen_idx);
 20256|         generation_allocator (gen)->clear();
 20257|         memset (generation_alloc_context           (gen), 0, sizeof(alloc_context));
 20258|         generation_start_segment                   (gen) = nullptr;
 20259|         generation_tail_ro_region                  (gen) = nullptr;
 20260|         generation_tail_region                     (gen) = nullptr;
 20261|         generation_allocation_segment              (gen) = nullptr;
 20262|         generation_allocation_context_start_region (gen) = nullptr;
 20263|         gen->plan_start_segment                          = nullptr;
 20264|         generation_free_list_allocated             (gen) = 0;
 20265|         generation_end_seg_allocated               (gen) = 0;
 20266|         generation_allocate_end_seg_p              (gen) = 0;
 20267|         generation_condemned_allocated             (gen) = 0;
 20268|         generation_sweep_allocated                 (gen) = 0;
 20269|         generation_free_list_space                 (gen) = 0;
 20270|         generation_free_obj_space                  (gen) = 0;
 20271|         generation_allocation_size                 (gen) = 0;
 20272|         generation_pinned_allocation_compact_size  (gen) = 0;
 20273|         generation_pinned_allocation_sweep_size    (gen) = 0;
 20274|         gen->gen_num                                     = gen_idx;
 20275| #ifdef DOUBLY_LINKED_FL
 20276|         generation_set_bgc_mark_bit_p              (gen) = FALSE;
 20277|         generation_last_free_list_allocated        (gen) = nullptr;
 20278| #endif //DOUBLY_LINKED_FL
 20279|         dynamic_data* dd = dynamic_data_of (gen_idx);
 20280|         dynamic_data* heap0_dd = heap0->dynamic_data_of (gen_idx);
 20281|         dd_time_clock     (dd) = dd_time_clock (heap0_dd);
 20282|         dd_collection_count (dd) = dd_collection_count (heap0_dd);
 20283|         dd_promoted_size                   (dd) = 0;
 20284|         dd_fragmentation                   (dd) = 0;
 20285|         dd_gc_clock                        (dd) = dd_gc_clock (heap0_dd);
 20286|         dd_new_allocation                  (dd) = UNINITIALIZED_VALUE;
 20287|         dd_desired_allocation              (dd) = UNINITIALIZED_VALUE;
 20288|         dd_gc_new_allocation               (dd) = UNINITIALIZED_VALUE;
 20289|         dd_surv                     (dd) = (float)UNINITIALIZED_VALUE;
 20290|         dd_begin_data_size                 (dd) = UNINITIALIZED_VALUE;
 20291|         dd_survived_size                   (dd) = UNINITIALIZED_VALUE;
 20292|         dd_pinned_survived_size            (dd) = UNINITIALIZED_VALUE;
 20293|         dd_artificial_pinned_survived_size (dd) = UNINITIALIZED_VALUE;
 20294|         dd_added_pinned_size               (dd) = UNINITIALIZED_VALUE;
 20295| #ifdef SHORT_PLUGS
 20296|         dd_padding_size                    (dd) = UNINITIALIZED_VALUE;
 20297| #endif //SHORT_PLUGS
 20298| #if defined (RESPECT_LARGE_ALIGNMENT) || defined (FEATURE_STRUCTALIGN)
 20299|         dd_num_npinned_plugs               (dd) = UNINITIALIZED_VALUE;
 20300| #endif //RESPECT_LARGE_ALIGNMENT || FEATURE_STRUCTALIGN
 20301|         dd_current_size                    (dd) = UNINITIALIZED_VALUE;
 20302|         dd_freach_previous_promotion       (dd) = UNINITIALIZED_VALUE;
 20303|         dd_previous_time_clock             (dd) = UNINITIALIZED_VALUE;
 20304|         dd_gc_elapsed_time                 (dd) = UNINITIALIZED_VALUE;
 20305|     }
 20306| #ifdef SPINLOCK_HISTORY
 20307|     spinlock_info_index = 0;
 20308|     current_uoh_alloc_state = (allocation_state)-1;
 20309| #endif //SPINLOCK_HISTORY
 20310| #ifdef RECORD_LOH_STATE
 20311|     loh_state_index = 0;
 20312| #endif //RECORD_LOH_STATE
 20313| }
 20314| float median_of_3 (float a, float b, float c)
 20315| {
 20316| #define compare_and_swap(i, j)          \
 20317|         {                               \
 20318|             if (i < j)                  \
 20319|             {                           \
 20320|                 float t = i;            \
 20321|                           i = j;        \
 20322|                               j = t;    \
 20323|             }                           \
 20324|         }
 20325|     compare_and_swap (b, a);
 20326|     compare_and_swap (c, a);
 20327|     compare_and_swap (c, b);
 20328| #undef compare_and_swap
 20329|     return b;
 20330| }
 20331| size_t gc_heap::get_num_completed_gcs ()
 20332| {
 20333|     size_t num_completed_gcs = settings.gc_index;
 20334| #ifdef BACKGROUND_GC
 20335|     if (g_heaps[0]->is_bgc_in_progress ())
 20336|     {
 20337|         num_completed_gcs--;
 20338|         dprintf (6666, ("BGC in prog, completed GCs -> %Id", num_completed_gcs));
 20339|     }
 20340| #endif //BACKGROUND_GC
 20341|     return num_completed_gcs;
 20342| }
 20343| int gc_heap::calculate_new_heap_count ()
 20344| {
 20345|     assert (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes);
 20346|     size_t num_completed_gcs = get_num_completed_gcs ();
 20347|     dprintf (6666, ("current GC %Id(completed: %Id), prev completed GCs %Id, last full GC happened at index %Id",
 20348|         VolatileLoadWithoutBarrier (&settings.gc_index), num_completed_gcs, dynamic_heap_count_data.prev_num_completed_gcs, gc_index_full_gc_end));
 20349|     if (num_completed_gcs < (dynamic_heap_count_data.prev_num_completed_gcs + dynamic_heap_count_data_t::sample_size))
 20350|     {
 20351|         dprintf (6666, ("not enough GCs, skipping"));
 20352|         return n_heaps;
 20353|     }
 20354|     float median_gen2_tcp_percent = 0.0f;
 20355|     if (gc_index_full_gc_end >= (settings.gc_index - dynamic_heap_count_data_t::sample_size))
 20356|     {
 20357|         median_gen2_tcp_percent = dynamic_heap_count_data.get_median_gen2_gc_percent ();
 20358|     }
 20359|     float throughput_cost_percents[dynamic_heap_count_data_t::sample_size];
 20360|     for (int i = 0; i < dynamic_heap_count_data_t::sample_size; i++)
 20361|     {
 20362|         dynamic_heap_count_data_t::sample& sample = dynamic_heap_count_data.samples[i];
 20363|         throughput_cost_percents[i] = (sample.elapsed_between_gcs ? (((float)sample.msl_wait_time / n_heaps + sample.gc_pause_time) * 100.0f / (float)sample.elapsed_between_gcs) : 0.0f);
 20364|         assert (throughput_cost_percents[i] >= 0.0);
 20365|         if (throughput_cost_percents[i] > 100.0)
 20366|             throughput_cost_percents[i] = 100.0;
 20367|         dprintf (6666, ("sample %d: msl %I64d / %d + pause %I64d / elapsed %I64d = throughput_cost_percent: %.3f", i,
 20368|             sample.msl_wait_time, n_heaps, sample.gc_pause_time, sample.elapsed_between_gcs, throughput_cost_percents[i]));
 20369|     }
 20370|     float median_throughput_cost_percent = median_of_3 (throughput_cost_percents[0], throughput_cost_percents[1], throughput_cost_percents[2]);
 20371|     const float smoothing = 3;
 20372|     float smoothed_median_throughput_cost_percent = dynamic_heap_count_data.smoothed_median_throughput_cost_percent;
 20373|     if (smoothed_median_throughput_cost_percent != 0.0f)
 20374|     {
 20375|         smoothed_median_throughput_cost_percent = median_throughput_cost_percent / smoothing + (smoothed_median_throughput_cost_percent / smoothing) * (smoothing - 1);
 20376|     }
 20377|     else
 20378|     {
 20379|         smoothed_median_throughput_cost_percent = median_throughput_cost_percent;
 20380|     }
 20381|     dprintf (6666, ("median tcp: %.3f, smoothed tcp: %.3f, gen2 tcp %.3f(%.3f, %.3f, %.3f)",
 20382|         median_throughput_cost_percent, smoothed_median_throughput_cost_percent, median_gen2_tcp_percent,
 20383|         dynamic_heap_count_data.gen2_gc_percents[0], dynamic_heap_count_data.gen2_gc_percents[1], dynamic_heap_count_data.gen2_gc_percents[2]));
 20384|     size_t heap_size = 0;
 20385|     for (int i = 0; i < n_heaps; i++)
 20386|     {
 20387|         gc_heap* hp = g_heaps[i];
 20388|         for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20389|         {
 20390|             dynamic_data* dd = hp->dynamic_data_of (gen_idx);
 20391|             heap_size += dd_current_size (dd) + dd_desired_allocation (dd);
 20392|             dprintf (3, ("h%d g%d current: %zd desired allocation: %zd", i, gen_idx, dd_promoted_size (dd), dd_desired_allocation (dd)));
 20393|         }
 20394|     }
 20395|     size_t heap_space_cost_per_heap = dd_min_size (g_heaps[0]->dynamic_data_of (0));
 20396|     float percent_heap_space_cost_per_heap = heap_space_cost_per_heap * 100.0f / heap_size;
 20397|     int step_up = (n_heaps + 1) / 2;
 20398|     int extra_heaps = 1 + (n_max_heaps >= 32);
 20399|     step_up = min (step_up, n_max_heaps - extra_heaps - n_heaps);
 20400|     int step_down = (n_heaps + 1) / 3;
 20401|     float tcp_reduction_per_step_up = smoothed_median_throughput_cost_percent * step_up / (n_heaps + step_up);
 20402|     float tcp_increase_per_step_down = smoothed_median_throughput_cost_percent * step_down / (n_heaps - step_down);
 20403|     float scp_increase_per_step_up = percent_heap_space_cost_per_heap * step_up;
 20404|     float scp_decrease_per_step_down = percent_heap_space_cost_per_heap * step_down;
 20405|     dprintf (6666, ("[CHP] u %d, d %d | space cost %Id / heap %Id(%.2fmb) = scp %.3f (u: %.3f, d: %.3f) | stcp %.3f, u * %.1f = %.3f, d * %.1f = %.3f",
 20406|         step_up, step_down,
 20407|         heap_space_cost_per_heap, heap_size, ((float)heap_size / (float)1000 / (float)1000), percent_heap_space_cost_per_heap,
 20408|         scp_increase_per_step_up, scp_decrease_per_step_down,
 20409|         smoothed_median_throughput_cost_percent,
 20410|         ((float)step_up / (float)(n_heaps + step_up)), tcp_reduction_per_step_up,
 20411|         ((float)step_down / (float)(n_heaps - step_down)), tcp_increase_per_step_down));
 20412| #ifdef STRESS_DYNAMIC_HEAP_COUNT
 20413|     int new_n_heaps = (int)gc_rand::get_rand (n_max_heaps - 1) + 1;
 20414|     if ((new_n_heaps < n_heaps) && (dynamic_heap_count_data.lowest_heap_with_msl_uoh != -1))
 20415|     {
 20416|         new_n_heaps = min (dynamic_heap_count_data.lowest_heap_with_msl_uoh, new_n_heaps);
 20417|         new_n_heaps = max (new_n_heaps, 1);
 20418|     }
 20419|     dprintf (6666, ("stress %d -> %d", n_heaps, new_n_heaps));
 20420| #else //STRESS_DYNAMIC_HEAP_COUNT
 20421|     int new_n_heaps = n_heaps;
 20422|     if (median_throughput_cost_percent > 10.0f)
 20423|     {
 20424|         new_n_heaps = (int)(n_heaps * (median_throughput_cost_percent / 5.0));
 20425|         dprintf (6666, ("[CHP0] tcp %.3f -> %d * %.3f = %d", median_throughput_cost_percent, n_heaps, (median_throughput_cost_percent / 5.0), new_n_heaps));
 20426|         new_n_heaps = min (new_n_heaps, n_max_heaps - extra_heaps);
 20427|     }
 20428|     else if ((smoothed_median_throughput_cost_percent > 5.0f) || (median_gen2_tcp_percent > 10.0f))
 20429|     {
 20430|         if (smoothed_median_throughput_cost_percent > 5.0f)
 20431|         {
 20432|             dprintf (6666, ("[CHP1] stcp %.3f > 5, %d + %d = %d", smoothed_median_throughput_cost_percent, n_heaps, step_up, (n_heaps + step_up)));
 20433|         }
 20434|         else
 20435|         {
 20436|             dprintf (6666, ("[CHP2] tcp %.3f > 10, %d + %d = %d", median_gen2_tcp_percent, n_heaps, step_up, (n_heaps + step_up)));
 20437|         }
 20438|         new_n_heaps += step_up;
 20439|     }
 20440|     else if ((tcp_reduction_per_step_up - scp_increase_per_step_up) >= 1.0f)
 20441|     {
 20442|         dprintf (6666, ("[CHP3] % .3f - % .3f = % .3f, % d + % d = % d",
 20443|             tcp_reduction_per_step_up, scp_increase_per_step_up, (tcp_reduction_per_step_up - scp_increase_per_step_up),
 20444|             n_heaps, step_up, (n_heaps + step_up)));
 20445|         new_n_heaps += step_up;
 20446|     }
 20447|     else if ((smoothed_median_throughput_cost_percent < 1.0f) &&
 20448|         (median_gen2_tcp_percent < 5.0f) &&
 20449|         ((scp_decrease_per_step_down - tcp_increase_per_step_down) >= 1.0f))
 20450|     {
 20451|         dprintf (6666, ("[CHP4] stcp %.3f tcp %.3f, %.3f - %.3f = %.3f, %d + %d = %d",
 20452|             smoothed_median_throughput_cost_percent, median_gen2_tcp_percent,
 20453|             scp_decrease_per_step_down, tcp_increase_per_step_down, (scp_decrease_per_step_down - tcp_increase_per_step_down),
 20454|             n_heaps, step_up, (n_heaps + step_up)));
 20455|         new_n_heaps -= step_down;
 20456|     }
 20457|     assert (new_n_heaps >= 1);
 20458|     assert (new_n_heaps <= n_max_heaps);
 20459| #endif //STRESS_DYNAMIC_HEAP_COUNT
 20460|     dynamic_heap_count_data.median_throughput_cost_percent = median_throughput_cost_percent;
 20461|     dynamic_heap_count_data.smoothed_median_throughput_cost_percent = smoothed_median_throughput_cost_percent;
 20462|     dynamic_heap_count_data.percent_heap_space_cost_per_heap = percent_heap_space_cost_per_heap;
 20463|     dynamic_heap_count_data.tcp_reduction_per_step_up = tcp_reduction_per_step_up;
 20464|     dynamic_heap_count_data.tcp_increase_per_step_down = tcp_increase_per_step_down;
 20465|     dynamic_heap_count_data.scp_increase_per_step_up = scp_increase_per_step_up;
 20466|     dynamic_heap_count_data.scp_decrease_per_step_down = scp_decrease_per_step_down;
 20467|     GCEventFireHeapCountTuning_V1 (
 20468|         (uint16_t)dynamic_heap_count_data.new_n_heaps,
 20469|         (uint64_t)VolatileLoadWithoutBarrier (&settings.gc_index),
 20470|         dynamic_heap_count_data.median_throughput_cost_percent,
 20471|         dynamic_heap_count_data.smoothed_median_throughput_cost_percent,
 20472|         dynamic_heap_count_data.tcp_reduction_per_step_up,
 20473|         dynamic_heap_count_data.tcp_increase_per_step_down,
 20474|         dynamic_heap_count_data.scp_increase_per_step_up,
 20475|         dynamic_heap_count_data.scp_decrease_per_step_down
 20476|     );
 20477|     dynamic_heap_count_data.prev_num_completed_gcs = num_completed_gcs;
 20478|     if (new_n_heaps != n_heaps)
 20479|     {
 20480|         dprintf (6666, ("should change! %d->%d", n_heaps, new_n_heaps));
 20481|         dynamic_heap_count_data.heap_count_to_change_to = new_n_heaps;
 20482|         dynamic_heap_count_data.should_change_heap_count = true;
 20483|     }
 20484|     return new_n_heaps;
 20485| }
 20486| void gc_heap::check_heap_count ()
 20487| {
 20488|     dynamic_heap_count_data.new_n_heaps = dynamic_heap_count_data.heap_count_to_change_to;
 20489|     assert (dynamic_heap_count_data.new_n_heaps != n_heaps);
 20490|     if (dynamic_heap_count_data.new_n_heaps != n_heaps)
 20491|     {
 20492|         dprintf (9999, ("h0 suspending EE in check"));
 20493|         GCToEEInterface::SuspendEE(SUSPEND_FOR_GC_PREP);
 20494|         dprintf (9999, ("h0 suspended EE in check"));
 20495| #ifdef BACKGROUND_GC
 20496|         if (gc_heap::background_running_p())
 20497|         {
 20498|             dynamic_heap_count_data.new_n_heaps = n_heaps;
 20499|             dprintf (6666, ("can't change heap count! BGC in progress"));
 20500|             GCToEEInterface::RestartEE(TRUE);
 20501|         }
 20502| #endif //BACKGROUND_GC
 20503|     }
 20504|     if (dynamic_heap_count_data.new_n_heaps != n_heaps)
 20505|     {
 20506|         dprintf (6666, ("prep to change from %d to %d", n_heaps, dynamic_heap_count_data.new_n_heaps));
 20507|         if (!prepare_to_change_heap_count (dynamic_heap_count_data.new_n_heaps))
 20508|         {
 20509|             dynamic_heap_count_data.new_n_heaps = n_heaps;
 20510|         }
 20511|     }
 20512|     if (dynamic_heap_count_data.new_n_heaps == n_heaps)
 20513|     {
 20514|         dynamic_heap_count_data.prev_num_completed_gcs = get_num_completed_gcs ();
 20515|         dynamic_heap_count_data.should_change_heap_count = false;
 20516|         dprintf (6666, ("heap count stays the same %d, no work to do, set prev completed to %Id", dynamic_heap_count_data.new_n_heaps, dynamic_heap_count_data.prev_num_completed_gcs));
 20517|         return;
 20518|     }
 20519|     int new_n_heaps = dynamic_heap_count_data.new_n_heaps;
 20520|     assert (!(dynamic_heap_count_data.init_only_p));
 20521|     {
 20522|         dprintf (9999, ("changing join hp %d->%d", n_heaps, new_n_heaps));
 20523|         int max_threads_to_wake = max (n_heaps, new_n_heaps);
 20524|         gc_t_join.update_n_threads (max_threads_to_wake);
 20525|         assert (dynamic_heap_count_data.new_n_heaps != n_heaps);
 20526|         if (n_heaps < new_n_heaps)
 20527|         {
 20528|             int saved_idle_thread_count = dynamic_heap_count_data.idle_thread_count;
 20529|             Interlocked::ExchangeAdd (&dynamic_heap_count_data.idle_thread_count, (n_heaps - new_n_heaps));
 20530|             dprintf (9999, ("GC thread %d setting idle events for h%d-h%d, total idle %d -> %d", heap_number, n_heaps, (new_n_heaps - 1),
 20531|                 saved_idle_thread_count, VolatileLoadWithoutBarrier (&dynamic_heap_count_data.idle_thread_count)));
 20532|             for (int heap_idx = n_heaps; heap_idx < new_n_heaps; heap_idx++)
 20533|             {
 20534|                 g_heaps[heap_idx]->gc_idle_thread_event.Set();
 20535| #ifdef BACKGROUND_GC
 20536|                 g_heaps[heap_idx]->bgc_idle_thread_event.Set();
 20537| #endif //BACKGROUND_GC
 20538|             }
 20539|         }
 20540|         gc_start_event.Set();
 20541|     }
 20542|     int old_n_heaps = n_heaps;
 20543|     (dynamic_heap_count_data.heap_count_change_count)++;
 20544|     change_heap_count (dynamic_heap_count_data.new_n_heaps);
 20545|     GCToEEInterface::RestartEE(TRUE);
 20546|     dprintf (9999, ("h0 restarted EE"));
 20547|     dynamic_heap_count_data.smoothed_median_throughput_cost_percent = dynamic_heap_count_data.smoothed_median_throughput_cost_percent / n_heaps * old_n_heaps;
 20548|     dprintf (6666, ("h0 finished changing, set should change to false!"));
 20549|     dynamic_heap_count_data.should_change_heap_count = false;
 20550| }
 20551| bool gc_heap::prepare_to_change_heap_count (int new_n_heaps)
 20552| {
 20553|     dprintf (9999, ("trying to change heap count %d -> %d", n_heaps, new_n_heaps));
 20554|     int old_n_heaps = n_heaps;
 20555|     for (int i = 0; i < old_n_heaps; i++)
 20556|     {
 20557|         gc_heap* hp = g_heaps[i];
 20558|         if (!hp->prepare_rethread_fl_items())
 20559|         {
 20560|             return false;
 20561|         }
 20562|     }
 20563|     if (new_n_heaps < old_n_heaps)
 20564|     {
 20565|         int to_heap_number = 0;
 20566|         for (int i = new_n_heaps; i < old_n_heaps; i++)
 20567|         {
 20568|             gc_heap* from_hp = g_heaps[i];
 20569|             gc_heap* to_hp = g_heaps[to_heap_number];
 20570|             if (!to_hp->finalize_queue->MergeFinalizationData (from_hp->finalize_queue))
 20571|             {
 20572|                 dprintf (3, ("failed to merge finalization from heap %d into heap %d", i, to_heap_number));
 20573|                 return false;
 20574|             }
 20575|             to_heap_number = (to_heap_number + 1) % new_n_heaps;
 20576|         }
 20577|     }
 20578|     for (int i = 0; i < old_n_heaps; i++)
 20579|     {
 20580|         gc_heap* hp = g_heaps[i];
 20581|         hp->delay_free_segments ();
 20582|     }
 20583|     ptrdiff_t region_count_in_gen[total_generation_count];
 20584|     for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20585|     {
 20586|         region_count_in_gen[gen_idx] = 0;
 20587|     }
 20588|     if (old_n_heaps < new_n_heaps)
 20589|     {
 20590|         for (int i = 0; i < old_n_heaps; i++)
 20591|         {
 20592|             gc_heap* hp = g_heaps[i];
 20593|             for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20594|             {
 20595|                 generation* gen = hp->generation_of (gen_idx);
 20596|                 for (heap_segment* region = heap_segment_rw (generation_start_segment (gen));
 20597|                      region != nullptr;
 20598|                      region = heap_segment_next (region))
 20599|                 {
 20600|                     region_count_in_gen[gen_idx]++;
 20601|                 }
 20602|             }
 20603|         }
 20604|         bool success = true;
 20605|         for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20606|         {
 20607|             const size_t size = gen_idx > soh_gen2 ? global_region_allocator.get_large_region_alignment() : 0;
 20608|             while (region_count_in_gen[gen_idx] < new_n_heaps)
 20609|             {
 20610|                 int kind = gen_idx > soh_gen2 ? large_free_region : basic_free_region;
 20611|                 bool found_free_regions = false;
 20612|                 for (int i = 0; i < old_n_heaps; i++)
 20613|                 {
 20614|                     gc_heap* hp = g_heaps[i];
 20615|                     if (hp->free_regions[kind].get_num_free_regions() > 0)
 20616|                     {
 20617|                         heap_segment* region = hp->get_new_region (gen_idx, size);
 20618|                         assert (region != nullptr);
 20619|                         region_count_in_gen[gen_idx]++;
 20620|                         found_free_regions = true;
 20621|                         if (region_count_in_gen[gen_idx] == new_n_heaps)
 20622|                             break;
 20623|                     }
 20624|                 }
 20625|                 if (!found_free_regions)
 20626|                 {
 20627|                     break;
 20628|                 }
 20629|             }
 20630|             while (region_count_in_gen[gen_idx] < new_n_heaps)
 20631|             {
 20632|                 if (g_heaps[0]->get_new_region (gen_idx, size) == nullptr)
 20633|                 {
 20634|                     success = false;
 20635|                     break;
 20636|                 }
 20637|                 region_count_in_gen[gen_idx]++;
 20638|             }
 20639|             if (!success)
 20640|             {
 20641|                 return false;
 20642|             }
 20643|         }
 20644|     }
 20645|     return true;
 20646| }
 20647| bool gc_heap::change_heap_count (int new_n_heaps)
 20648| {
 20649|     dprintf (9999, ("BEG heap%d changing %d->%d", heap_number, n_heaps, new_n_heaps));
 20650|     int old_n_heaps = n_heaps;
 20651|     bool init_only_p = dynamic_heap_count_data.init_only_p;
 20652|     {
 20653|         gc_t_join.join (this, gc_join_merge_temp_fl);
 20654|         if (gc_t_join.joined ())
 20655|         {
 20656| #ifdef BACKGROUND_GC
 20657|             bgc_t_join.update_n_threads (new_n_heaps);
 20658| #endif //BACKGROUND_GC
 20659|             dynamic_heap_count_data.init_only_p = false;
 20660|             dprintf (9999, ("in change h%d resetting gc_start, update bgc join to %d heaps", heap_number, new_n_heaps));
 20661|             gc_start_event.Reset();
 20662|             gc_t_join.restart ();
 20663|         }
 20664|     }
 20665|     assert (dynamic_heap_count_data.new_n_heaps != old_n_heaps);
 20666|     dprintf (9999, ("Waiting h0 heap%d changing %d->%d", heap_number, n_heaps, new_n_heaps));
 20667|     if (heap_number == 0)
 20668|     {
 20669|         dprintf (3, ("switching heap count from %d to %d heaps", old_n_heaps, new_n_heaps));
 20670|         int from_heap_number = 0;
 20671|         for (int i = old_n_heaps; i < new_n_heaps; i++)
 20672|         {
 20673|             gc_heap* to_hp = g_heaps[i];
 20674|             gc_heap* from_hp = g_heaps[from_heap_number];
 20675|             if (!from_hp->finalize_queue->SplitFinalizationData (to_hp->finalize_queue))
 20676|             {
 20677|                 dprintf (3, ("failed to split finalization data between heaps %d and %d", from_heap_number, i));
 20678|             }
 20679|             from_heap_number = (from_heap_number + 1) % old_n_heaps;
 20680|         }
 20681|         BOOL unified_gen0_bricks_cleared = TRUE;
 20682|         for (int i = 0; i < old_n_heaps; i++)
 20683|         {
 20684|             gc_heap* hp = g_heaps[i];
 20685|             if (!init_only_p)
 20686|             {
 20687|                 hp->fix_allocation_contexts (TRUE);
 20688|             }
 20689|             if (unified_gen0_bricks_cleared && (hp->gen0_bricks_cleared == FALSE))
 20690|             {
 20691|                 unified_gen0_bricks_cleared = FALSE;
 20692|             }
 20693|             for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20694|             {
 20695|                 generation* gen = hp->generation_of (gen_idx);
 20696|                 for (heap_segment* region = heap_segment_rw (generation_start_segment (gen));
 20697|                      region != nullptr;
 20698|                      region = heap_segment_next (region))
 20699|                 {
 20700|                     heap_segment_survived (region) = heap_segment_allocated (region) - heap_segment_mem (region);
 20701|                 }
 20702|             }
 20703|         }
 20704|         if (old_n_heaps < new_n_heaps)
 20705|         {
 20706|             for (int i = old_n_heaps; i < new_n_heaps; i++)
 20707|             {
 20708|                 gc_heap* hp = g_heaps[i];
 20709|                 hp->check_decommissioned_heap();
 20710|                 hp->recommission_heap();
 20711|             }
 20712|         }
 20713|         if (new_n_heaps < old_n_heaps)
 20714|         {
 20715|             assert (new_n_heaps > 0);
 20716|             for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20717|             {
 20718|                 for (int i = new_n_heaps; i < old_n_heaps; i++)
 20719|                 {
 20720|                     gc_heap* hp = g_heaps[i];
 20721|                     int dest_heap_number = i % new_n_heaps;
 20722|                     gc_heap* hpd = g_heaps[dest_heap_number];
 20723|                     generation* hpd_gen = hpd->generation_of (gen_idx);
 20724|                     generation* gen = hp->generation_of (gen_idx);
 20725|                     heap_segment* start_region = generation_start_segment (gen);
 20726|                     heap_segment* tail_ro_region = generation_tail_ro_region (gen);
 20727|                     heap_segment* tail_region = generation_tail_region (gen);
 20728|                     for (heap_segment* region = start_region; region != nullptr; region = heap_segment_next(region))
 20729|                     {
 20730|                         set_heap_for_contained_basic_regions (region, hpd);
 20731|                     }
 20732|                     if (tail_ro_region != nullptr)
 20733|                     {
 20734|                         heap_segment* start_rw_region = heap_segment_next (tail_ro_region);
 20735|                         heap_segment* hpd_tail_ro_region = generation_tail_ro_region (hpd_gen);
 20736|                         if (hpd_tail_ro_region != nullptr)
 20737|                         {
 20738|                             heap_segment_next (tail_ro_region) = heap_segment_next (hpd_tail_ro_region);
 20739|                             heap_segment_next (hpd_tail_ro_region) = start_region;
 20740|                         }
 20741|                         else
 20742|                         {
 20743|                             heap_segment_next (tail_ro_region) = generation_start_segment (hpd_gen);
 20744|                             generation_start_segment (hpd_gen) = start_region;
 20745|                         }
 20746|                         generation_tail_ro_region (hpd_gen) = tail_ro_region;
 20747|                         start_region = start_rw_region;
 20748|                     }
 20749|                     heap_segment* hpd_tail_region = generation_tail_region (hpd_gen);
 20750|                     heap_segment_next (hpd_tail_region) = start_region;
 20751|                     generation_tail_region (hpd_gen) = tail_region;
 20752|                     generation_start_segment (gen) = nullptr;
 20753|                     generation_tail_ro_region (gen) = nullptr;
 20754|                     generation_tail_region (gen) = nullptr;
 20755|                 }
 20756|             }
 20757|         }
 20758|         for (int i = new_n_heaps; i < old_n_heaps; i++)
 20759|         {
 20760|             gc_heap* hp = g_heaps[i];
 20761|             int dest_heap_number = i % new_n_heaps;
 20762|             gc_heap* hpd = g_heaps[dest_heap_number];
 20763|             for (int kind = 0; kind < count_free_region_kinds; kind++)
 20764|             {
 20765|                 hpd->free_regions[kind].transfer_regions(&hp->free_regions[kind]);
 20766|             }
 20767|         }
 20768|         dprintf (9999, ("h%d changing %d->%d", heap_number, n_heaps, new_n_heaps));
 20769|         n_heaps = new_n_heaps;
 20770|         equalize_promoted_bytes (max_generation);
 20771|         for (int i = 0; i < new_n_heaps; i++)
 20772|         {
 20773|             gc_heap* hp = g_heaps[i];
 20774|             hp->gen0_bricks_cleared = unified_gen0_bricks_cleared;
 20775|             generation* gen0 = hp->generation_of (0);
 20776|             if ((hp->ephemeral_heap_segment == nullptr) ||
 20777|                 (heap_segment_heap (hp->ephemeral_heap_segment) != hp))
 20778|             {
 20779|                 hp->ephemeral_heap_segment = heap_segment_rw (generation_start_segment (gen0));
 20780|                 hp->alloc_allocated = heap_segment_allocated (hp->ephemeral_heap_segment);
 20781|             }
 20782|             for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20783|             {
 20784|                 generation* gen = hp->generation_of (gen_idx);
 20785|                 heap_segment *allocation_region = generation_allocation_segment (gen);
 20786|                 if ((allocation_region == nullptr) ||
 20787|                     (heap_segment_heap (allocation_region) != hp))
 20788|                 {
 20789|                     generation_allocation_segment (gen) = heap_segment_rw (generation_start_segment (gen));
 20790|                 }
 20791|                 generation_free_obj_space (gen) = 0;
 20792|             }
 20793|         }
 20794|     }
 20795|     dprintf (3, ("individual heap%d changing %d->%d", heap_number, n_heaps, new_n_heaps));
 20796|     if (!init_only_p)
 20797|     {
 20798|         gc_t_join.join (this, gc_join_merge_temp_fl);
 20799|         if (gc_t_join.joined ())
 20800|         {
 20801|             gc_t_join.restart ();
 20802|         }
 20803|         for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20804|         {
 20805|             if (heap_number < old_n_heaps)
 20806|             {
 20807|                 dprintf (3, ("h%d calling per heap work!", heap_number));
 20808|                 rethread_fl_items (gen_idx);
 20809|             }
 20810|             gc_t_join.join (this, gc_join_merge_temp_fl);
 20811|             if (gc_t_join.joined ())
 20812|             {
 20813|                 merge_fl_from_other_heaps (gen_idx, new_n_heaps, old_n_heaps);
 20814|                 gc_t_join.restart ();
 20815|             }
 20816|         }
 20817| #ifdef BACKGROUND_GC
 20818|         bgc_alloc_lock->check();
 20819| #endif //BACKGROUND_GC
 20820|     }
 20821|     if (heap_number == 0)
 20822|     {
 20823|         ptrdiff_t budget_per_heap[total_generation_count];
 20824|         for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20825|         {
 20826|             ptrdiff_t total_budget = 0;
 20827|             for (int i = 0; i < old_n_heaps; i++)
 20828|             {
 20829|                 gc_heap* hp = g_heaps[i];
 20830|                 dynamic_data* dd = hp->dynamic_data_of (gen_idx);
 20831|                 total_budget += dd_new_allocation (dd);
 20832|             }
 20833|             int max_n_heaps = max (old_n_heaps, new_n_heaps);
 20834|             budget_per_heap[gen_idx] = Align (total_budget/max_n_heaps, get_alignment_constant (gen_idx <= max_generation));
 20835|             dprintf (6666, ("g%d: total budget: %zd budget per heap: %zd", gen_idx, total_budget, budget_per_heap[gen_idx]));
 20836|         }
 20837|         for (int i = 0; i < new_n_heaps; i++)
 20838|         {
 20839|             gc_heap* hp = g_heaps[i];
 20840|             for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
 20841|             {
 20842|                 dynamic_data* dd = hp->dynamic_data_of (gen_idx);
 20843|                 dd_new_allocation (dd) = max (budget_per_heap[gen_idx], (ptrdiff_t)dd_min_size (dd));
 20844|                 dd_desired_allocation (dd) = dd_new_allocation (dd);
 20845|                 generation* gen = hp->generation_of (gen_idx);
 20846|                 size_t gen_size = hp->generation_size (gen_idx);
 20847|                 dd_fragmentation (dd) = generation_free_list_space (gen);
 20848|                 assert (gen_size >= dd_fragmentation (dd));
 20849|                 dd_current_size (dd) = gen_size - dd_fragmentation (dd);
 20850|                 dprintf (6666, ("h%d g%d: new allocation: %zd generation_size: %zd fragmentation: %zd current_size: %zd",
 20851|                     i,
 20852|                     gen_idx,
 20853|                     dd_new_allocation (dd),
 20854|                     gen_size,
 20855|                     dd_fragmentation (dd),
 20856|                     dd_current_size (dd)));
 20857|             }
 20858|         }
 20859|         for (int i = n_heaps; i < old_n_heaps; i++)
 20860|         {
 20861|             gc_heap* hp = g_heaps[i];
 20862|             hp->decommission_heap();
 20863|         }
 20864|         if (!init_only_p)
 20865|         {
 20866|             fix_allocation_contexts_heaps();
 20867|         }
 20868|         dynamic_heap_count_data.last_n_heaps = old_n_heaps;
 20869|     }
 20870|     if (new_n_heaps < old_n_heaps)
 20871|     {
 20872|         gc_t_join.join (this, gc_join_merge_temp_fl);
 20873|         if (gc_t_join.joined ())
 20874|         {
 20875|             dprintf (9999, ("now changing the join heap count to the smaller one %d", new_n_heaps));
 20876|             gc_t_join.update_n_threads (new_n_heaps);
 20877|             gc_t_join.restart ();
 20878|         }
 20879|     }
 20880|     return true;
 20881| }
 20882| size_t gc_heap::get_msl_wait_time()
 20883| {
 20884|     assert (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes);
 20885|     size_t msl_wait_since_pause = 0;
 20886|     for (int i = 0; i < n_heaps; i++)
 20887|     {
 20888|         gc_heap* hp = g_heaps[i];
 20889|         msl_wait_since_pause += hp->more_space_lock_soh.msl_wait_time;
 20890|         hp->more_space_lock_soh.msl_wait_time = 0;
 20891|         msl_wait_since_pause += hp->more_space_lock_uoh.msl_wait_time;
 20892|         hp->more_space_lock_uoh.msl_wait_time = 0;
 20893|     }
 20894|     return msl_wait_since_pause;
 20895| }
 20896| #endif //DYNAMIC_HEAP_COUNT
 20897| #endif //USE_REGIONS
 20898| #if !defined(USE_REGIONS) || defined(_DEBUG)
 20899| inline
 20900| void gc_heap::init_promoted_bytes()
 20901| {
 20902| #ifdef MULTIPLE_HEAPS
 20903|     g_promoted [heap_number*16] = 0;
 20904| #else //MULTIPLE_HEAPS
 20905|     g_promoted = 0;
 20906| #endif //MULTIPLE_HEAPS
 20907| }
 20908| size_t& gc_heap::promoted_bytes (int thread)
 20909| {
 20910| #ifdef MULTIPLE_HEAPS
 20911|     return g_promoted [thread*16];
 20912| #else //MULTIPLE_HEAPS
 20913|     UNREFERENCED_PARAMETER(thread);
 20914|     return g_promoted;
 20915| #endif //MULTIPLE_HEAPS
 20916| }
 20917| #endif //!USE_REGIONS || _DEBUG
 20918| inline
 20919| void gc_heap::add_to_promoted_bytes (uint8_t* object, int thread)
 20920| {
 20921|     size_t obj_size = size (object);
 20922|     add_to_promoted_bytes (object, obj_size, thread);
 20923| }
 20924| inline
 20925| void gc_heap::add_to_promoted_bytes (uint8_t* object, size_t obj_size, int thread)
 20926| {
 20927|     assert (thread == heap_number);
 20928| #ifdef USE_REGIONS
 20929|     if (survived_per_region)
 20930|     {
 20931|         survived_per_region[get_basic_region_index_for_address (object)] += obj_size;
 20932|     }
 20933| #endif //USE_REGIONS
 20934| #if !defined(USE_REGIONS) || defined(_DEBUG)
 20935| #ifdef MULTIPLE_HEAPS
 20936|     g_promoted [heap_number*16] += obj_size;
 20937| #else //MULTIPLE_HEAPS
 20938|     g_promoted += obj_size;
 20939| #endif //MULTIPLE_HEAPS
 20940| #endif //!USE_REGIONS || _DEBUG
 20941| #ifdef _DEBUG
 20942| #endif //_DEBUG
 20943| }
 20944| heap_segment* gc_heap::find_segment (uint8_t* interior, BOOL small_segment_only_p)
 20945| {
 20946|     heap_segment* seg = seg_mapping_table_segment_of (interior);
 20947|     if (seg)
 20948|     {
 20949|         if (small_segment_only_p && heap_segment_uoh_p (seg))
 20950|             return 0;
 20951|     }
 20952|     return seg;
 20953| }
 20954| #if !defined(_DEBUG) && !defined(__GNUC__)
 20955| inline // This causes link errors if global optimization is off
 20956| #endif //!_DEBUG && !__GNUC__
 20957| gc_heap* gc_heap::heap_of (uint8_t* o)
 20958| {
 20959| #ifdef MULTIPLE_HEAPS
 20960|     if (o == 0)
 20961|         return g_heaps [0];
 20962|     gc_heap* hp = seg_mapping_table_heap_of (o);
 20963|     return (hp ? hp : g_heaps[0]);
 20964| #else //MULTIPLE_HEAPS
 20965|     UNREFERENCED_PARAMETER(o);
 20966|     return __this;
 20967| #endif //MULTIPLE_HEAPS
 20968| }
 20969| inline
 20970| gc_heap* gc_heap::heap_of_gc (uint8_t* o)
 20971| {
 20972| #ifdef MULTIPLE_HEAPS
 20973|     if (o == 0)
 20974|         return g_heaps [0];
 20975|     gc_heap* hp = seg_mapping_table_heap_of_gc (o);
 20976|     return (hp ? hp : g_heaps[0]);
 20977| #else //MULTIPLE_HEAPS
 20978|     UNREFERENCED_PARAMETER(o);
 20979|     return __this;
 20980| #endif //MULTIPLE_HEAPS
 20981| }
 20982| uint8_t* gc_heap::find_object (uint8_t* interior)
 20983| {
 20984|     assert (interior != 0);
 20985|     if (!gen0_bricks_cleared)
 20986|     {
 20987| #ifdef MULTIPLE_HEAPS
 20988|         assert (!"Should have already been done in server GC");
 20989| #endif //MULTIPLE_HEAPS
 20990|         clear_gen0_bricks();
 20991|     }
 20992|     gen0_must_clear_bricks = FFIND_DECAY;
 20993|     int brick_entry = get_brick_entry(brick_of (interior));
 20994|     if (brick_entry == 0)
 20995|     {
 20996|         heap_segment* seg = find_segment (interior, FALSE);
 20997|         if (seg)
 20998|         {
 20999| #ifdef FEATURE_CONSERVATIVE_GC
 21000|             if (interior >= heap_segment_allocated(seg))
 21001|                 return 0;
 21002| #endif
 21003|             int align_const = get_alignment_constant (heap_segment_read_only_p (seg)
 21004| #ifdef FEATURE_CONSERVATIVE_GC
 21005|                                                        || (GCConfig::GetConservativeGC() && !heap_segment_uoh_p (seg))
 21006| #endif
 21007|                                                       );
 21008|             assert (interior < heap_segment_allocated (seg));
 21009|             uint8_t* o = heap_segment_mem (seg);
 21010|             while (o < heap_segment_allocated (seg))
 21011|             {
 21012|                 uint8_t* next_o = o + Align (size (o), align_const);
 21013|                 assert (next_o > o);
 21014|                 if ((o <= interior) && (interior < next_o))
 21015|                     return o;
 21016|                 o = next_o;
 21017|             }
 21018|             return 0;
 21019|         }
 21020|         else
 21021|         {
 21022|             return 0;
 21023|         }
 21024|     }
 21025|     else
 21026|     {
 21027|         heap_segment* seg = find_segment (interior, TRUE);
 21028|         if (seg)
 21029|         {
 21030| #ifdef FEATURE_CONSERVATIVE_GC
 21031|             if (interior >= heap_segment_allocated (seg))
 21032|                 return 0;
 21033| #else
 21034|             assert (interior < heap_segment_allocated (seg));
 21035| #endif
 21036|             uint8_t* o = find_first_object (interior, heap_segment_mem (seg));
 21037|             return o;
 21038|         }
 21039|         else
 21040|             return 0;
 21041|     }
 21042| }
 21043| #ifdef MULTIPLE_HEAPS
 21044| #ifdef GC_CONFIG_DRIVEN
 21045| #define m_boundary(o) {if (mark_list_index <= mark_list_end) {*mark_list_index = o;mark_list_index++;} else {mark_list_index++;}}
 21046| #else //GC_CONFIG_DRIVEN
 21047| #define m_boundary(o) {if (mark_list_index <= mark_list_end) {*mark_list_index = o;mark_list_index++;}}
 21048| #endif //GC_CONFIG_DRIVEN
 21049| #define m_boundary_fullgc(o) {}
 21050| #else //MULTIPLE_HEAPS
 21051| #ifdef GC_CONFIG_DRIVEN
 21052| #define m_boundary(o) {if (mark_list_index <= mark_list_end) {*mark_list_index = o;mark_list_index++;} else {mark_list_index++;} if (slow > o) slow = o; if (shigh < o) shigh = o;}
 21053| #else
 21054| #define m_boundary(o) {if (mark_list_index <= mark_list_end) {*mark_list_index = o;mark_list_index++;}if (slow > o) slow = o; if (shigh < o) shigh = o;}
 21055| #endif //GC_CONFIG_DRIVEN
 21056| #define m_boundary_fullgc(o) {if (slow > o) slow = o; if (shigh < o) shigh = o;}
 21057| #endif //MULTIPLE_HEAPS
 21058| inline
 21059| BOOL gc_heap::gc_mark1 (uint8_t* o)
 21060| {
 21061|     BOOL marked = !marked (o);
 21062|     set_marked (o);
 21063|     dprintf (3, ("*%zx*, newly marked: %d", (size_t)o, marked));
 21064| #if defined(USE_REGIONS) && defined(_DEBUG)
 21065|     heap_segment* seg = seg_mapping_table_segment_of (o);
 21066|     if (o > heap_segment_allocated (seg))
 21067|     {
 21068|         dprintf (REGIONS_LOG, ("%p is in seg %zx(%p) but beyond alloc %p!!",
 21069|             o, (size_t)seg, heap_segment_mem (seg), heap_segment_allocated (seg)));
 21070|         GCToOSInterface::DebugBreak();
 21071|     }
 21072| #endif //USE_REGIONS && _DEBUG
 21073|     return marked;
 21074| }
 21075| #ifdef USE_REGIONS
 21076| inline bool is_in_heap_range (uint8_t* o)
 21077| {
 21078| #ifdef FEATURE_BASICFREEZE
 21079|     assert (((g_gc_lowest_address <= o) && (o < g_gc_highest_address)) ||
 21080|         (o == nullptr) || (ro_segment_lookup (o) != nullptr));
 21081|     return ((g_gc_lowest_address <= o) && (o < g_gc_highest_address));
 21082| #else //FEATURE_BASICFREEZE
 21083|     assert ((o == nullptr) || (g_gc_lowest_address <= o) && (o < g_gc_highest_address));
 21084|     return (o != nullptr);
 21085| #endif //FEATURE_BASICFREEZE
 21086| }
 21087| inline bool gc_heap::is_in_gc_range (uint8_t* o)
 21088| {
 21089| #ifdef FEATURE_BASICFREEZE
 21090|     assert (((g_gc_lowest_address <= o) && (o < g_gc_highest_address)) ||
 21091|         (o == nullptr) || (ro_segment_lookup (o) != nullptr));
 21092| #else //FEATURE_BASICFREEZE
 21093|     assert ((o == nullptr) || (g_gc_lowest_address <= o) && (o < g_gc_highest_address));
 21094| #endif //FEATURE_BASICFREEZE
 21095|     return ((gc_low <= o) && (o < gc_high));
 21096| }
 21097| #endif //USE_REGIONS
 21098| inline
 21099| BOOL gc_heap::gc_mark (uint8_t* o, uint8_t* low, uint8_t* high, int condemned_gen)
 21100| {
 21101| #ifdef USE_REGIONS
 21102|     if ((o >= low) && (o < high))
 21103|     {
 21104|         if (condemned_gen != max_generation && get_region_gen_num (o) > condemned_gen)
 21105|         {
 21106|             return FALSE;
 21107|         }
 21108|         BOOL already_marked = marked (o);
 21109|         if (already_marked)
 21110|         {
 21111|             return FALSE;
 21112|         }
 21113|         set_marked (o);
 21114|         return TRUE;
 21115|     }
 21116|     return FALSE;
 21117| #else //USE_REGIONS
 21118|     assert (condemned_gen == -1);
 21119|     BOOL marked = FALSE;
 21120|     if ((o >= low) && (o < high))
 21121|         marked = gc_mark1 (o);
 21122| #ifdef MULTIPLE_HEAPS
 21123|     else if (o)
 21124|     {
 21125|         gc_heap* hp = heap_of_gc (o);
 21126|         assert (hp);
 21127|         if ((o >= hp->gc_low) && (o < hp->gc_high))
 21128|             marked = gc_mark1 (o);
 21129|     }
 21130| #ifdef SNOOP_STATS
 21131|     snoop_stat.objects_checked_count++;
 21132|     if (marked)
 21133|     {
 21134|         snoop_stat.objects_marked_count++;
 21135|     }
 21136|     if (!o)
 21137|     {
 21138|         snoop_stat.zero_ref_count++;
 21139|     }
 21140| #endif //SNOOP_STATS
 21141| #endif //MULTIPLE_HEAPS
 21142|     return marked;
 21143| #endif //USE_REGIONS
 21144| }
 21145| #ifdef BACKGROUND_GC
 21146| inline
 21147| BOOL gc_heap::background_marked (uint8_t* o)
 21148| {
 21149|     return mark_array_marked (o);
 21150| }
 21151| inline
 21152| BOOL gc_heap::background_mark1 (uint8_t* o)
 21153| {
 21154|     BOOL to_mark = !mark_array_marked (o);
 21155|     dprintf (3, ("b*%zx*b(%d)", (size_t)o, (to_mark ? 1 : 0)));
 21156|     if (to_mark)
 21157|     {
 21158|         mark_array_set_marked (o);
 21159|         dprintf (4, ("n*%zx*n", (size_t)o));
 21160|         return TRUE;
 21161|     }
 21162|     else
 21163|         return FALSE;
 21164| }
 21165| inline
 21166| BOOL gc_heap::background_mark (uint8_t* o, uint8_t* low, uint8_t* high)
 21167| {
 21168|     BOOL marked = FALSE;
 21169|     if ((o >= low) && (o < high))
 21170|         marked = background_mark1 (o);
 21171| #ifdef MULTIPLE_HEAPS
 21172|     else if (o)
 21173|     {
 21174|         gc_heap* hp = heap_of (o);
 21175|         assert (hp);
 21176|         if ((o >= hp->background_saved_lowest_address) && (o < hp->background_saved_highest_address))
 21177|             marked = background_mark1 (o);
 21178|     }
 21179| #endif //MULTIPLE_HEAPS
 21180|     return marked;
 21181| }
 21182| #endif //BACKGROUND_GC
 21183| #define new_start() {if (ppstop <= start) {break;} else {parm = start}}
 21184| #define ignore_start 0
 21185| #define use_start 1
 21186| #define go_through_object(mt,o,size,parm,start,start_useful,limit,exp)      \
 21187| {                                                                           \
 21188|     CGCDesc* map = CGCDesc::GetCGCDescFromMT((MethodTable*)(mt));           \
 21189|     CGCDescSeries* cur = map->GetHighestSeries();                           \
 21190|     ptrdiff_t cnt = (ptrdiff_t) map->GetNumSeries();                        \
 21191|                                                                             \
 21192|     if (cnt >= 0)                                                           \
 21193|     {                                                                       \
 21194|         CGCDescSeries* last = map->GetLowestSeries();                       \
 21195|         uint8_t** parm = 0;                                                 \
 21196|         do                                                                  \
 21197|         {                                                                   \
 21198|             assert (parm <= (uint8_t**)((o) + cur->GetSeriesOffset()));     \
 21199|             parm = (uint8_t**)((o) + cur->GetSeriesOffset());               \
 21200|             uint8_t** ppstop =                                              \
 21201|                 (uint8_t**)((uint8_t*)parm + cur->GetSeriesSize() + (size));\
 21202|             if (!start_useful || (uint8_t*)ppstop > (start))                \
 21203|             {                                                               \
 21204|                 if (start_useful && (uint8_t*)parm < (start)) parm = (uint8_t**)(start);\
 21205|                 while (parm < ppstop)                                       \
 21206|                 {                                                           \
 21207|                    {exp}                                                    \
 21208|                    parm++;                                                  \
 21209|                 }                                                           \
 21210|             }                                                               \
 21211|             cur--;                                                          \
 21212|                                                                             \
 21213|         } while (cur >= last);                                              \
 21214|     }                                                                       \
 21215|     else                                                                    \
 21216|     {                                                                       \
 21217|         /* Handle the repeating case - array of valuetypes */               \
 21218|         uint8_t** parm = (uint8_t**)((o) + cur->startoffset);               \
 21219|         if (start_useful && start > (uint8_t*)parm)                         \
 21220|         {                                                                   \
 21221|             ptrdiff_t cs = mt->RawGetComponentSize();                         \
 21222|             parm = (uint8_t**)((uint8_t*)parm + (((start) - (uint8_t*)parm)/cs)*cs); \
 21223|         }                                                                   \
 21224|         while ((uint8_t*)parm < ((o)+(size)-plug_skew))                     \
 21225|         {                                                                   \
 21226|             for (ptrdiff_t __i = 0; __i > cnt; __i--)                         \
 21227|             {                                                               \
 21228|                 HALF_SIZE_T skip =  (cur->val_serie + __i)->skip;           \
 21229|                 HALF_SIZE_T nptrs = (cur->val_serie + __i)->nptrs;          \
 21230|                 uint8_t** ppstop = parm + nptrs;                            \
 21231|                 if (!start_useful || (uint8_t*)ppstop > (start))            \
 21232|                 {                                                           \
 21233|                     if (start_useful && (uint8_t*)parm < (start)) parm = (uint8_t**)(start);      \
 21234|                     do                                                      \
 21235|                     {                                                       \
 21236|                        {exp}                                                \
 21237|                        parm++;                                              \
 21238|                     } while (parm < ppstop);                                \
 21239|                 }                                                           \
 21240|                 parm = (uint8_t**)((uint8_t*)ppstop + skip);                \
 21241|             }                                                               \
 21242|         }                                                                   \
 21243|     }                                                                       \
 21244| }
 21245| #define go_through_object_nostart(mt,o,size,parm,exp) {go_through_object(mt,o,size,parm,o,ignore_start,(o + size),exp); }
 21246| #ifndef COLLECTIBLE_CLASS
 21247| #define go_through_object_cl(mt,o,size,parm,exp)                            \
 21248| {                                                                           \
 21249|     if (header(o)->ContainsPointers())                                      \
 21250|     {                                                                       \
 21251|         go_through_object_nostart(mt,o,size,parm,exp);                      \
 21252|     }                                                                       \
 21253| }
 21254| #else //COLLECTIBLE_CLASS
 21255| #define go_through_object_cl(mt,o,size,parm,exp)                            \
 21256| {                                                                           \
 21257|     if (header(o)->Collectible())                                           \
 21258|     {                                                                       \
 21259|         uint8_t* class_obj = get_class_object (o);                             \
 21260|         uint8_t** parm = &class_obj;                                           \
 21261|         do {exp} while (false);                                             \
 21262|     }                                                                       \
 21263|     if (header(o)->ContainsPointers())                                      \
 21264|     {                                                                       \
 21265|         go_through_object_nostart(mt,o,size,parm,exp);                      \
 21266|     }                                                                       \
 21267| }
 21268| #endif //COLLECTIBLE_CLASS
 21269| void gc_heap::enque_pinned_plug (uint8_t* plug,
 21270|                                  BOOL save_pre_plug_info_p,
 21271|                                  uint8_t* last_object_in_last_plug)
 21272| {
 21273|     if (mark_stack_array_length <= mark_stack_tos)
 21274|     {
 21275|         if (!grow_mark_stack (mark_stack_array, mark_stack_array_length, MARK_STACK_INITIAL_LENGTH))
 21276|         {
 21277|             GCToEEInterface::HandleFatalError((unsigned int)CORINFO_EXCEPTION_GC);
 21278|         }
 21279|     }
 21280|     dprintf (3, ("enqueuing P #%zd(%p): %p. oldest: %zd, LO: %p, pre: %d",
 21281|         mark_stack_tos, &mark_stack_array[mark_stack_tos], plug, mark_stack_bos, last_object_in_last_plug, (save_pre_plug_info_p ? 1 : 0)));
 21282|     mark& m = mark_stack_array[mark_stack_tos];
 21283|     m.first = plug;
 21284|     m.saved_pre_p = save_pre_plug_info_p;
 21285|     if (save_pre_plug_info_p)
 21286|     {
 21287|         size_t special_bits = clear_special_bits (last_object_in_last_plug);
 21288|         memcpy (&(m.saved_pre_plug), &(((plug_and_gap*)plug)[-1]), sizeof (gap_reloc_pair));
 21289|         set_special_bits (last_object_in_last_plug, special_bits);
 21290|         memcpy (&(m.saved_pre_plug_reloc), &(((plug_and_gap*)plug)[-1]), sizeof (gap_reloc_pair));
 21291|         size_t last_obj_size = plug - last_object_in_last_plug;
 21292|         if (last_obj_size < min_pre_pin_obj_size)
 21293|         {
 21294|             record_interesting_data_point (idp_pre_short);
 21295| #ifdef SHORT_PLUGS
 21296|             if (is_plug_padded (last_object_in_last_plug))
 21297|                 record_interesting_data_point (idp_pre_short_padded);
 21298| #endif //SHORT_PLUGS
 21299|             dprintf (3, ("encountered a short object %p right before pinned plug %p!",
 21300|                          last_object_in_last_plug, plug));
 21301|             m.set_pre_short();
 21302| #ifdef COLLECTIBLE_CLASS
 21303|             if (is_collectible (last_object_in_last_plug))
 21304|             {
 21305|                 m.set_pre_short_collectible();
 21306|             }
 21307| #endif //COLLECTIBLE_CLASS
 21308|             if (contain_pointers (last_object_in_last_plug))
 21309|             {
 21310|                 dprintf (3, ("short object: %p(%zx)", last_object_in_last_plug, last_obj_size));
 21311|                 go_through_object_nostart (method_table(last_object_in_last_plug), last_object_in_last_plug, last_obj_size, pval,
 21312|                     {
 21313|                         size_t gap_offset = (((size_t)pval - (size_t)(plug - sizeof (gap_reloc_pair) - plug_skew))) / sizeof (uint8_t*);
 21314|                         dprintf (3, ("member: %p->%p, %zd ptrs from beginning of gap", (uint8_t*)pval, *pval, gap_offset));
 21315|                         m.set_pre_short_bit (gap_offset);
 21316|                     }
 21317|                 );
 21318|             }
 21319|         }
 21320|     }
 21321|     m.saved_post_p = FALSE;
 21322| }
 21323| void gc_heap::save_post_plug_info (uint8_t* last_pinned_plug, uint8_t* last_object_in_last_plug, uint8_t* post_plug)
 21324| {
 21325| #ifndef _DEBUG
 21326|     UNREFERENCED_PARAMETER(last_pinned_plug);
 21327| #endif //_DEBUG
 21328|     mark& m = mark_stack_array[mark_stack_tos - 1];
 21329|     assert (last_pinned_plug == m.first);
 21330|     m.saved_post_plug_info_start = (uint8_t*)&(((plug_and_gap*)post_plug)[-1]);
 21331|     size_t special_bits = clear_special_bits (last_object_in_last_plug);
 21332|     memcpy (&(m.saved_post_plug), m.saved_post_plug_info_start, sizeof (gap_reloc_pair));
 21333|     set_special_bits (last_object_in_last_plug, special_bits);
 21334|     memcpy (&(m.saved_post_plug_reloc), m.saved_post_plug_info_start, sizeof (gap_reloc_pair));
 21335|     m.saved_post_p = TRUE;
 21336| #ifdef _DEBUG
 21337|     m.saved_post_plug_debug.gap = 1;
 21338| #endif //_DEBUG
 21339|     dprintf (3, ("PP %p has NP %p right after", last_pinned_plug, post_plug));
 21340|     size_t last_obj_size = post_plug - last_object_in_last_plug;
 21341|     if (last_obj_size < min_pre_pin_obj_size)
 21342|     {
 21343|         dprintf (3, ("PP %p last obj %p is too short", last_pinned_plug, last_object_in_last_plug));
 21344|         record_interesting_data_point (idp_post_short);
 21345| #ifdef SHORT_PLUGS
 21346|         if (is_plug_padded (last_object_in_last_plug))
 21347|             record_interesting_data_point (idp_post_short_padded);
 21348| #endif //SHORT_PLUGS
 21349|         m.set_post_short();
 21350| #if defined (_DEBUG) && defined (VERIFY_HEAP)
 21351|         verify_pinned_queue_p = TRUE;
 21352| #endif // _DEBUG && VERIFY_HEAP
 21353| #ifdef COLLECTIBLE_CLASS
 21354|         if (is_collectible (last_object_in_last_plug))
 21355|         {
 21356|             m.set_post_short_collectible();
 21357|         }
 21358| #endif //COLLECTIBLE_CLASS
 21359|         if (contain_pointers (last_object_in_last_plug))
 21360|         {
 21361|             dprintf (3, ("short object: %p(%zx)", last_object_in_last_plug, last_obj_size));
 21362|             go_through_object_nostart (method_table(last_object_in_last_plug), last_object_in_last_plug, last_obj_size, pval,
 21363|                 {
 21364|                     size_t gap_offset = (((size_t)pval - (size_t)(post_plug - sizeof (gap_reloc_pair) - plug_skew))) / sizeof (uint8_t*);
 21365|                     dprintf (3, ("member: %p->%p, %zd ptrs from beginning of gap", (uint8_t*)pval, *pval, gap_offset));
 21366|                     m.set_post_short_bit (gap_offset);
 21367|                 }
 21368|             );
 21369|         }
 21370|     }
 21371| }
 21372| #if defined(TARGET_AMD64) || defined(TARGET_X86) || defined(TARGET_ARM64) || defined(TARGET_RISCV64)
 21373| #define PREFETCH
 21374| #endif
 21375| #ifdef PREFETCH
 21376| inline void Prefetch(void* addr)
 21377| {
 21378| #ifdef TARGET_WINDOWS
 21379| #if defined(TARGET_AMD64) || defined(TARGET_X86)
 21380| #ifndef _MM_HINT_T0
 21381| #define _MM_HINT_T0 1
 21382| #endif
 21383|     _mm_prefetch((const char*)addr, _MM_HINT_T0);
 21384| #elif defined(TARGET_ARM64)
 21385|     __prefetch((const char*)addr);
 21386| #endif //defined(TARGET_AMD64) || defined(TARGET_X86)
 21387| #elif defined(TARGET_UNIX)
 21388|     __builtin_prefetch(addr);
 21389| #else //!(TARGET_WINDOWS || TARGET_UNIX)
 21390|     UNREFERENCED_PARAMETER(addr);
 21391| #endif //TARGET_WINDOWS
 21392| }
 21393| #else //PREFETCH
 21394| inline void Prefetch (void* addr)
 21395| {
 21396|     UNREFERENCED_PARAMETER(addr);
 21397| }
 21398| #endif //PREFETCH
 21399| #ifdef MH_SC_MARK
 21400| inline
 21401| VOLATILE(uint8_t*)& gc_heap::ref_mark_stack (gc_heap* hp, int index)
 21402| {
 21403|     return ((VOLATILE(uint8_t*)*)(hp->mark_stack_array))[index];
 21404| }
 21405| #endif //MH_SC_MARK
 21406| #define stolen 2
 21407| #define partial 1
 21408| #define partial_object 3
 21409| inline
 21410| uint8_t* ref_from_slot (uint8_t* r)
 21411| {
 21412|     return (uint8_t*)((size_t)r & ~(stolen | partial));
 21413| }
 21414| inline
 21415| BOOL stolen_p (uint8_t* r)
 21416| {
 21417|     return (((size_t)r&2) && !((size_t)r&1));
 21418| }
 21419| inline
 21420| BOOL ready_p (uint8_t* r)
 21421| {
 21422|     return ((size_t)r != 1);
 21423| }
 21424| inline
 21425| BOOL partial_p (uint8_t* r)
 21426| {
 21427|     return (((size_t)r&1) && !((size_t)r&2));
 21428| }
 21429| inline
 21430| BOOL straight_ref_p (uint8_t* r)
 21431| {
 21432|     return (!stolen_p (r) && !partial_p (r));
 21433| }
 21434| inline
 21435| BOOL partial_object_p (uint8_t* r)
 21436| {
 21437|     return (((size_t)r & partial_object) == partial_object);
 21438| }
 21439| inline
 21440| BOOL ref_p (uint8_t* r)
 21441| {
 21442|     return (straight_ref_p (r) || partial_object_p (r));
 21443| }
 21444| mark_queue_t::mark_queue_t()
 21445| #ifdef MARK_PHASE_PREFETCH
 21446|     : curr_slot_index(0)
 21447| #endif //MARK_PHASE_PREFETCH
 21448| {
 21449| #ifdef MARK_PHASE_PREFETCH
 21450|     for (size_t i = 0; i < slot_count; i++)
 21451|     {
 21452|         slot_table[i] = nullptr;
 21453|     }
 21454| #endif //MARK_PHASE_PREFETCH
 21455| }
 21456| FORCEINLINE
 21457| uint8_t *mark_queue_t::queue_mark(uint8_t *o)
 21458| {
 21459| #ifdef MARK_PHASE_PREFETCH
 21460|     Prefetch (o);
 21461|     size_t slot_index = curr_slot_index;
 21462|     uint8_t* old_o = slot_table[slot_index];
 21463|     slot_table[slot_index] = o;
 21464|     curr_slot_index = (slot_index + 1) % slot_count;
 21465|     if (old_o == nullptr)
 21466|         return nullptr;
 21467| #else //MARK_PHASE_PREFETCH
 21468|     uint8_t* old_o = o;
 21469| #endif //MARK_PHASE_PREFETCH
 21470|     BOOL already_marked = marked (old_o);
 21471|     if (already_marked)
 21472|     {
 21473|         return nullptr;
 21474|     }
 21475|     set_marked (old_o);
 21476|     return old_o;
 21477| }
 21478| FORCEINLINE
 21479| uint8_t *mark_queue_t::queue_mark(uint8_t *o, int condemned_gen)
 21480| {
 21481| #ifdef USE_REGIONS
 21482|     if (!is_in_heap_range (o))
 21483|     {
 21484|         return nullptr;
 21485|     }
 21486|     if (condemned_gen != max_generation && gc_heap::get_region_gen_num (o) > condemned_gen)
 21487|     {
 21488|         return nullptr;
 21489|     }
 21490|     return queue_mark(o);
 21491| #else //USE_REGIONS
 21492|     assert (condemned_gen == -1);
 21493| #ifdef MULTIPLE_HEAPS
 21494|     if (o)
 21495|     {
 21496|         gc_heap* hp = gc_heap::heap_of_gc (o);
 21497|         assert (hp);
 21498|         if ((o >= hp->gc_low) && (o < hp->gc_high))
 21499|             return queue_mark (o);
 21500|     }
 21501| #else //MULTIPLE_HEAPS
 21502|     if ((o >= gc_heap::gc_low) && (o < gc_heap::gc_high))
 21503|         return queue_mark (o);
 21504| #endif //MULTIPLE_HEAPS
 21505|     return nullptr;
 21506| #endif //USE_REGIONS
 21507| }
 21508| uint8_t* mark_queue_t::get_next_marked()
 21509| {
 21510| #ifdef MARK_PHASE_PREFETCH
 21511|     size_t slot_index = curr_slot_index;
 21512|     size_t empty_slot_count = 0;
 21513|     while (empty_slot_count < slot_count)
 21514|     {
 21515|         uint8_t* o = slot_table[slot_index];
 21516|         slot_table[slot_index] = nullptr;
 21517|         slot_index = (slot_index + 1) % slot_count;
 21518|         if (o != nullptr)
 21519|         {
 21520|             BOOL already_marked = marked (o);
 21521|             if (!already_marked)
 21522|             {
 21523|                 set_marked (o);
 21524|                 curr_slot_index = slot_index;
 21525|                 return o;
 21526|             }
 21527|         }
 21528|         empty_slot_count++;
 21529|     }
 21530| #endif //MARK_PHASE_PREFETCH
 21531|     return nullptr;
 21532| }
 21533| void mark_queue_t::verify_empty()
 21534| {
 21535| #ifdef MARK_PHASE_PREFETCH
 21536|     for (size_t slot_index = 0; slot_index < slot_count; slot_index++)
 21537|     {
 21538|         assert(slot_table[slot_index] == nullptr);
 21539|     }
 21540| #endif //MARK_PHASE_PREFETCH
 21541| }
 21542| void gc_heap::mark_object_simple1 (uint8_t* oo, uint8_t* start THREAD_NUMBER_DCL)
 21543| {
 21544|     SERVER_SC_MARK_VOLATILE(uint8_t*)* mark_stack_tos = (SERVER_SC_MARK_VOLATILE(uint8_t*)*)mark_stack_array;
 21545|     SERVER_SC_MARK_VOLATILE(uint8_t*)* mark_stack_limit = (SERVER_SC_MARK_VOLATILE(uint8_t*)*)&mark_stack_array[mark_stack_array_length];
 21546|     SERVER_SC_MARK_VOLATILE(uint8_t*)* mark_stack_base = mark_stack_tos;
 21547| #ifdef SORT_MARK_STACK
 21548|     SERVER_SC_MARK_VOLATILE(uint8_t*)* sorted_tos = mark_stack_base;
 21549| #endif //SORT_MARK_STACK
 21550|     BOOL  full_p = (settings.condemned_generation == max_generation);
 21551|     int condemned_gen =
 21552| #ifdef USE_REGIONS
 21553|         settings.condemned_generation;
 21554| #else
 21555|         -1;
 21556| #endif //USE_REGIONS
 21557|     assert ((start >= oo) && (start < oo+size(oo)));
 21558| #ifndef MH_SC_MARK
 21559|     *mark_stack_tos = oo;
 21560| #endif //!MH_SC_MARK
 21561|     while (1)
 21562|     {
 21563| #ifdef MULTIPLE_HEAPS
 21564| #else  //MULTIPLE_HEAPS
 21565|         const int thread = 0;
 21566| #endif //MULTIPLE_HEAPS
 21567|         if (oo && ((size_t)oo != 4))
 21568|         {
 21569|             size_t s = 0;
 21570|             if (stolen_p (oo))
 21571|             {
 21572|                 --mark_stack_tos;
 21573|                 goto next_level;
 21574|             }
 21575|             else if (!partial_p (oo) && ((s = size (oo)) < (partial_size_th*sizeof (uint8_t*))))
 21576|             {
 21577|                 BOOL overflow_p = FALSE;
 21578|                 if (mark_stack_tos + (s) /sizeof (uint8_t*) >= (mark_stack_limit  - 1))
 21579|                 {
 21580|                     size_t num_components = ((method_table(oo))->HasComponentSize() ? ((CObjectHeader*)oo)->GetNumComponents() : 0);
 21581|                     if (mark_stack_tos + CGCDesc::GetNumPointers(method_table(oo), s, num_components) >= (mark_stack_limit - 1))
 21582|                     {
 21583|                         overflow_p = TRUE;
 21584|                     }
 21585|                 }
 21586|                 if (overflow_p == FALSE)
 21587|                 {
 21588|                     dprintf(3,("pushing mark for %zx ", (size_t)oo));
 21589|                     go_through_object_cl (method_table(oo), oo, s, ppslot,
 21590|                                           {
 21591|                                               uint8_t* o = mark_queue.queue_mark(*ppslot, condemned_gen);
 21592|                                               if (o != nullptr)
 21593|                                               {
 21594|                                                   if (full_p)
 21595|                                                   {
 21596|                                                       m_boundary_fullgc (o);
 21597|                                                   }
 21598|                                                   else
 21599|                                                   {
 21600|                                                       m_boundary (o);
 21601|                                                   }
 21602|                                                   add_to_promoted_bytes (o, thread);
 21603|                                                   if (contain_pointers_or_collectible (o))
 21604|                                                   {
 21605|                                                       *(mark_stack_tos++) = o;
 21606|                                                   }
 21607|                                               }
 21608|                                           }
 21609|                         );
 21610|                 }
 21611|                 else
 21612|                 {
 21613|                     dprintf(3,("mark stack overflow for object %zx ", (size_t)oo));
 21614|                     min_overflow_address = min (min_overflow_address, oo);
 21615|                     max_overflow_address = max (max_overflow_address, oo);
 21616|                 }
 21617|             }
 21618|             else
 21619|             {
 21620|                 if (partial_p (oo))
 21621|                 {
 21622|                     start = ref_from_slot (oo);
 21623|                     oo = ref_from_slot (*(--mark_stack_tos));
 21624|                     dprintf (4, ("oo: %zx, start: %zx\n", (size_t)oo, (size_t)start));
 21625|                     assert ((oo < start) && (start < (oo + size (oo))));
 21626|                 }
 21627| #ifdef COLLECTIBLE_CLASS
 21628|                 else
 21629|                 {
 21630|                     if (is_collectible (oo))
 21631|                     {
 21632|                         uint8_t* class_obj = get_class_object (oo);
 21633|                         if (gc_mark (class_obj, gc_low, gc_high, condemned_gen))
 21634|                         {
 21635|                             if (full_p)
 21636|                             {
 21637|                                 m_boundary_fullgc (class_obj);
 21638|                             }
 21639|                             else
 21640|                             {
 21641|                                 m_boundary (class_obj);
 21642|                             }
 21643|                             add_to_promoted_bytes (class_obj, thread);
 21644|                             *(mark_stack_tos++) = class_obj;
 21645|                             *mark_stack_tos = oo;
 21646|                         }
 21647|                     }
 21648|                     if (!contain_pointers (oo))
 21649|                     {
 21650|                         goto next_level;
 21651|                     }
 21652|                 }
 21653| #endif //COLLECTIBLE_CLASS
 21654|                 s = size (oo);
 21655|                 BOOL overflow_p = FALSE;
 21656|                 if (mark_stack_tos + (num_partial_refs + 2)  >= mark_stack_limit)
 21657|                 {
 21658|                     overflow_p = TRUE;
 21659|                 }
 21660|                 if (overflow_p == FALSE)
 21661|                 {
 21662|                     dprintf(3,("pushing mark for %zx ", (size_t)oo));
 21663|                     SERVER_SC_MARK_VOLATILE(uint8_t*)* place = ++mark_stack_tos;
 21664|                     mark_stack_tos++;
 21665| #ifdef MH_SC_MARK
 21666|                     *(place-1) = 0;
 21667|                     *(place) = (uint8_t*)partial;
 21668| #endif //MH_SC_MARK
 21669|                     int i = num_partial_refs;
 21670|                     uint8_t* ref_to_continue = 0;
 21671|                     go_through_object (method_table(oo), oo, s, ppslot,
 21672|                                        start, use_start, (oo + s),
 21673|                                        {
 21674|                                            uint8_t* o = mark_queue.queue_mark(*ppslot, condemned_gen);
 21675|                                            if (o != nullptr)
 21676|                                            {
 21677|                                                 if (full_p)
 21678|                                                 {
 21679|                                                     m_boundary_fullgc (o);
 21680|                                                 }
 21681|                                                 else
 21682|                                                 {
 21683|                                                     m_boundary (o);
 21684|                                                 }
 21685|                                                 add_to_promoted_bytes (o, thread);
 21686|                                                 if (contain_pointers_or_collectible (o))
 21687|                                                 {
 21688|                                                     *(mark_stack_tos++) = o;
 21689|                                                     if (--i == 0)
 21690|                                                     {
 21691|                                                         ref_to_continue = (uint8_t*)((size_t)(ppslot+1) | partial);
 21692|                                                         goto more_to_do;
 21693|                                                     }
 21694|                                                 }
 21695|                                            }
 21696|                                        }
 21697|                         );
 21698|                     assert (ref_to_continue == 0);
 21699| #ifdef MH_SC_MARK
 21700|                     assert ((*(place-1)) == (uint8_t*)0);
 21701| #else //MH_SC_MARK
 21702|                     *(place-1) = 0;
 21703| #endif //MH_SC_MARK
 21704|                     *place = 0;
 21705| more_to_do:
 21706|                     if (ref_to_continue)
 21707|                     {
 21708| #ifdef MH_SC_MARK
 21709|                         assert ((*(place-1)) == (uint8_t*)0);
 21710|                         *(place-1) = (uint8_t*)((size_t)oo | partial_object);
 21711|                         assert (((*place) == (uint8_t*)1) || ((*place) == (uint8_t*)2));
 21712| #endif //MH_SC_MARK
 21713|                         *place = ref_to_continue;
 21714|                     }
 21715|                 }
 21716|                 else
 21717|                 {
 21718|                     dprintf(3,("mark stack overflow for object %zx ", (size_t)oo));
 21719|                     min_overflow_address = min (min_overflow_address, oo);
 21720|                     max_overflow_address = max (max_overflow_address, oo);
 21721|                 }
 21722|             }
 21723| #ifdef SORT_MARK_STACK
 21724|             if (mark_stack_tos > sorted_tos + mark_stack_array_length/8)
 21725|             {
 21726|                 rqsort1 (sorted_tos, mark_stack_tos-1);
 21727|                 sorted_tos = mark_stack_tos-1;
 21728|             }
 21729| #endif //SORT_MARK_STACK
 21730|         }
 21731|     next_level:
 21732|         if (!(mark_stack_empty_p()))
 21733|         {
 21734|             oo = *(--mark_stack_tos);
 21735|             start = oo;
 21736| #ifdef SORT_MARK_STACK
 21737|             sorted_tos = min ((size_t)sorted_tos, (size_t)mark_stack_tos);
 21738| #endif //SORT_MARK_STACK
 21739|         }
 21740|         else
 21741|             break;
 21742|     }
 21743| }
 21744| #ifdef MH_SC_MARK
 21745| BOOL same_numa_node_p (int hn1, int hn2)
 21746| {
 21747|     return (heap_select::find_numa_node_from_heap_no (hn1) == heap_select::find_numa_node_from_heap_no (hn2));
 21748| }
 21749| int find_next_buddy_heap (int this_heap_number, int current_buddy, int n_heaps)
 21750| {
 21751|     int hn = (current_buddy+1)%n_heaps;
 21752|     while (hn != current_buddy)
 21753|     {
 21754|         if ((this_heap_number != hn) && (same_numa_node_p (this_heap_number, hn)))
 21755|             return hn;
 21756|         hn = (hn+1)%n_heaps;
 21757|     }
 21758|     return current_buddy;
 21759| }
 21760| void
 21761| gc_heap::mark_steal()
 21762| {
 21763|     mark_stack_busy() = 0;
 21764|     for (int i = 0; i < max_snoop_level; i++)
 21765|     {
 21766|         ((VOLATILE(uint8_t*)*)(mark_stack_array))[i] = 0;
 21767|     }
 21768|     int thpn = find_next_buddy_heap (heap_number, heap_number, n_heaps);
 21769| #ifdef SNOOP_STATS
 21770|         dprintf (SNOOP_LOG, ("(GC%d)heap%d: start snooping %d", settings.gc_index, heap_number, (heap_number+1)%n_heaps));
 21771|         uint64_t begin_tick = GCToOSInterface::GetLowPrecisionTimeStamp();
 21772| #endif //SNOOP_STATS
 21773|     int idle_loop_count = 0;
 21774|     int first_not_ready_level = 0;
 21775|     while (1)
 21776|     {
 21777|         gc_heap* hp = g_heaps [thpn];
 21778|         int level = first_not_ready_level;
 21779|         first_not_ready_level = 0;
 21780|         while (check_next_mark_stack (hp) && (level < (max_snoop_level-1)))
 21781|         {
 21782|             idle_loop_count = 0;
 21783| #ifdef SNOOP_STATS
 21784|             snoop_stat.busy_count++;
 21785|             dprintf (SNOOP_LOG, ("heap%d: looking at next heap level %d stack contents: %zx",
 21786|                                  heap_number, level, (int)((uint8_t**)(hp->mark_stack_array))[level]));
 21787| #endif //SNOOP_STATS
 21788|             uint8_t* o = ref_mark_stack (hp, level);
 21789|             uint8_t* start = o;
 21790|             if (ref_p (o))
 21791|             {
 21792|                 mark_stack_busy() = 1;
 21793|                 BOOL success = TRUE;
 21794|                 uint8_t* next = (ref_mark_stack (hp, level+1));
 21795|                 if (ref_p (next))
 21796|                 {
 21797|                     if (((size_t)o > 4) && !partial_object_p (o))
 21798|                     {
 21799|                         success = (Interlocked::CompareExchangePointer (&ref_mark_stack (hp, level), (uint8_t*)4, o)==o);
 21800| #ifdef SNOOP_STATS
 21801|                         snoop_stat.interlocked_count++;
 21802|                         if (success)
 21803|                             snoop_stat.normal_count++;
 21804| #endif //SNOOP_STATS
 21805|                     }
 21806|                     else
 21807|                     {
 21808|                         level++;
 21809| #ifdef SNOOP_STATS
 21810|                         snoop_stat.stolen_or_pm_count++;
 21811| #endif //SNOOP_STATS
 21812|                         success = FALSE;
 21813|                     }
 21814|                 }
 21815|                 else if (stolen_p (next))
 21816|                 {
 21817|                     success = FALSE;
 21818|                     level+=2;
 21819| #ifdef SNOOP_STATS
 21820|                     snoop_stat.stolen_entry_count++;
 21821| #endif //SNOOP_STATS
 21822|                 }
 21823|                 else
 21824|                 {
 21825|                     assert (partial_p (next));
 21826|                     start = ref_from_slot (next);
 21827|                     o = ref_from_slot (ref_mark_stack (hp, level));
 21828|                     if (o && start)
 21829|                     {
 21830|                         success = (Interlocked::CompareExchangePointer (&ref_mark_stack (hp, level+1),
 21831|                                                                         (uint8_t*)stolen, next) == next);
 21832| #ifdef SNOOP_STATS
 21833|                         snoop_stat.interlocked_count++;
 21834|                         if (success)
 21835|                         {
 21836|                             snoop_stat.partial_mark_parent_count++;
 21837|                         }
 21838| #endif //SNOOP_STATS
 21839|                     }
 21840|                     else
 21841|                     {
 21842|                         success = FALSE;
 21843|                         if (first_not_ready_level == 0)
 21844|                         {
 21845|                             first_not_ready_level = level;
 21846|                         }
 21847|                         level+=2;
 21848| #ifdef SNOOP_STATS
 21849|                         snoop_stat.pm_not_ready_count++;
 21850| #endif //SNOOP_STATS
 21851|                     }
 21852|                 }
 21853|                 if (success)
 21854|                 {
 21855| #ifdef SNOOP_STATS
 21856|                     dprintf (SNOOP_LOG, ("heap%d: marking %zx from %d [%d] tl:%dms",
 21857|                             heap_number, (size_t)o, (heap_number+1)%n_heaps, level,
 21858|                             (GCToOSInterface::GetLowPrecisionTimeStamp()-begin_tick)));
 21859|                     uint64_t start_tick = GCToOSInterface::GetLowPrecisionTimeStamp();
 21860| #endif //SNOOP_STATS
 21861|                     mark_object_simple1 (o, start, heap_number);
 21862| #ifdef SNOOP_STATS
 21863|                     dprintf (SNOOP_LOG, ("heap%d: done marking %zx from %d [%d] %dms tl:%dms",
 21864|                             heap_number, (size_t)o, (heap_number+1)%n_heaps, level,
 21865|                             (GCToOSInterface::GetLowPrecisionTimeStamp()-start_tick),(GCToOSInterface::GetLowPrecisionTimeStamp()-begin_tick)));
 21866| #endif //SNOOP_STATS
 21867|                     mark_stack_busy() = 0;
 21868|                     for (int i = 0; i < max_snoop_level; i++)
 21869|                     {
 21870|                         if (((uint8_t**)mark_stack_array)[i] != 0)
 21871|                         {
 21872|                             ((VOLATILE(uint8_t*)*)(mark_stack_array))[i] = 0;
 21873| #ifdef SNOOP_STATS
 21874|                             snoop_stat.stack_bottom_clear_count++;
 21875| #endif //SNOOP_STATS
 21876|                         }
 21877|                     }
 21878|                     level = 0;
 21879|                 }
 21880|                 mark_stack_busy() = 0;
 21881|             }
 21882|             else
 21883|             {
 21884|                 level++;
 21885|             }
 21886|         }
 21887|         if ((first_not_ready_level != 0) && hp->mark_stack_busy())
 21888|         {
 21889|             continue;
 21890|         }
 21891|         if (!hp->mark_stack_busy())
 21892|         {
 21893|             first_not_ready_level = 0;
 21894|             idle_loop_count++;
 21895|             if ((idle_loop_count % (6) )==1)
 21896|             {
 21897| #ifdef SNOOP_STATS
 21898|                 snoop_stat.switch_to_thread_count++;
 21899| #endif //SNOOP_STATS
 21900|                 GCToOSInterface::Sleep(1);
 21901|             }
 21902|             int free_count = 1;
 21903| #ifdef SNOOP_STATS
 21904|             snoop_stat.stack_idle_count++;
 21905| #endif //SNOOP_STATS
 21906|             for (int hpn = (heap_number+1)%n_heaps; hpn != heap_number;)
 21907|             {
 21908|                 if (!((g_heaps [hpn])->mark_stack_busy()))
 21909|                 {
 21910|                     free_count++;
 21911| #ifdef SNOOP_STATS
 21912|                 dprintf (SNOOP_LOG, ("heap%d: %d idle", heap_number, free_count));
 21913| #endif //SNOOP_STATS
 21914|                 }
 21915|                 else if (same_numa_node_p (hpn, heap_number) || ((idle_loop_count%1000))==999)
 21916|                 {
 21917|                     thpn = hpn;
 21918|                     break;
 21919|                 }
 21920|                 hpn = (hpn+1)%n_heaps;
 21921|                 YieldProcessor();
 21922|             }
 21923|             if (free_count == n_heaps)
 21924|             {
 21925|                 break;
 21926|             }
 21927|         }
 21928|     }
 21929| }
 21930| inline
 21931| BOOL gc_heap::check_next_mark_stack (gc_heap* next_heap)
 21932| {
 21933| #ifdef SNOOP_STATS
 21934|     snoop_stat.check_level_count++;
 21935| #endif //SNOOP_STATS
 21936|     return (next_heap->mark_stack_busy()>=1);
 21937| }
 21938| #endif //MH_SC_MARK
 21939| #ifdef SNOOP_STATS
 21940| void gc_heap::print_snoop_stat()
 21941| {
 21942|     dprintf (1234, ("%4s | %8s | %8s | %8s | %8s | %8s | %8s | %8s",
 21943|         "heap", "check", "zero", "mark", "stole", "pstack", "nstack", "nonsk"));
 21944|     dprintf (1234, ("%4d | %8d | %8d | %8d | %8d | %8d | %8d | %8d",
 21945|         snoop_stat.heap_index,
 21946|         snoop_stat.objects_checked_count,
 21947|         snoop_stat.zero_ref_count,
 21948|         snoop_stat.objects_marked_count,
 21949|         snoop_stat.stolen_stack_count,
 21950|         snoop_stat.partial_stack_count,
 21951|         snoop_stat.normal_stack_count,
 21952|         snoop_stat.non_stack_count));
 21953|     dprintf (1234, ("%4s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s",
 21954|         "heap", "level", "busy", "xchg", "pmparent", "s_pm", "stolen", "nready", "clear"));
 21955|     dprintf (1234, ("%4d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d\n",
 21956|         snoop_stat.heap_index,
 21957|         snoop_stat.check_level_count,
 21958|         snoop_stat.busy_count,
 21959|         snoop_stat.interlocked_count,
 21960|         snoop_stat.partial_mark_parent_count,
 21961|         snoop_stat.stolen_or_pm_count,
 21962|         snoop_stat.stolen_entry_count,
 21963|         snoop_stat.pm_not_ready_count,
 21964|         snoop_stat.normal_count,
 21965|         snoop_stat.stack_bottom_clear_count));
 21966|     printf ("\n%4s | %8s | %8s | %8s | %8s | %8s\n",
 21967|         "heap", "check", "zero", "mark", "idle", "switch");
 21968|     printf ("%4d | %8d | %8d | %8d | %8d | %8d\n",
 21969|         snoop_stat.heap_index,
 21970|         snoop_stat.objects_checked_count,
 21971|         snoop_stat.zero_ref_count,
 21972|         snoop_stat.objects_marked_count,
 21973|         snoop_stat.stack_idle_count,
 21974|         snoop_stat.switch_to_thread_count);
 21975|     printf ("%4s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s\n",
 21976|         "heap", "level", "busy", "xchg", "pmparent", "s_pm", "stolen", "nready", "normal", "clear");
 21977|     printf ("%4d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d\n",
 21978|         snoop_stat.heap_index,
 21979|         snoop_stat.check_level_count,
 21980|         snoop_stat.busy_count,
 21981|         snoop_stat.interlocked_count,
 21982|         snoop_stat.partial_mark_parent_count,
 21983|         snoop_stat.stolen_or_pm_count,
 21984|         snoop_stat.stolen_entry_count,
 21985|         snoop_stat.pm_not_ready_count,
 21986|         snoop_stat.normal_count,
 21987|         snoop_stat.stack_bottom_clear_count);
 21988| }
 21989| #endif //SNOOP_STATS
 21990| #ifdef HEAP_ANALYZE
 21991| void
 21992| gc_heap::ha_mark_object_simple (uint8_t** po THREAD_NUMBER_DCL)
 21993| {
 21994|     if (!internal_root_array)
 21995|     {
 21996|         internal_root_array = new (nothrow) uint8_t* [internal_root_array_length];
 21997|         if (!internal_root_array)
 21998|         {
 21999|             heap_analyze_success = FALSE;
 22000|         }
 22001|     }
 22002|     if (heap_analyze_success && (internal_root_array_length <= internal_root_array_index))
 22003|     {
 22004|         size_t new_size = 2*internal_root_array_length;
 22005|         uint64_t available_physical = 0;
 22006|         get_memory_info (NULL, &available_physical);
 22007|         if (new_size > (size_t)(available_physical / 10))
 22008|         {
 22009|             heap_analyze_success = FALSE;
 22010|         }
 22011|         else
 22012|         {
 22013|             uint8_t** tmp = new (nothrow) uint8_t* [new_size];
 22014|             if (tmp)
 22015|             {
 22016|                 memcpy (tmp, internal_root_array,
 22017|                         internal_root_array_length*sizeof (uint8_t*));
 22018|                 delete[] internal_root_array;
 22019|                 internal_root_array = tmp;
 22020|                 internal_root_array_length = new_size;
 22021|             }
 22022|             else
 22023|             {
 22024|                 heap_analyze_success = FALSE;
 22025|             }
 22026|         }
 22027|     }
 22028|     if (heap_analyze_success)
 22029|     {
 22030|         PREFIX_ASSUME(internal_root_array_index < internal_root_array_length);
 22031|         uint8_t* ref = (uint8_t*)po;
 22032|         if (!current_obj ||
 22033|             !((ref >= current_obj) && (ref < (current_obj + current_obj_size))))
 22034|         {
 22035|             gc_heap* hp = gc_heap::heap_of (ref);
 22036|             current_obj = hp->find_object (ref);
 22037|             current_obj_size = size (current_obj);
 22038|             internal_root_array[internal_root_array_index] = current_obj;
 22039|             internal_root_array_index++;
 22040|         }
 22041|     }
 22042|     mark_object_simple (po THREAD_NUMBER_ARG);
 22043| }
 22044| #endif //HEAP_ANALYZE
 22045| void
 22046| gc_heap::mark_object_simple (uint8_t** po THREAD_NUMBER_DCL)
 22047| {
 22048|     int condemned_gen =
 22049| #ifdef USE_REGIONS
 22050|         settings.condemned_generation;
 22051| #else
 22052|         -1;
 22053| #endif //USE_REGIONS
 22054|     uint8_t* o = *po;
 22055| #ifndef MULTIPLE_HEAPS
 22056|     const int thread = 0;
 22057| #endif //MULTIPLE_HEAPS
 22058|     {
 22059| #ifdef SNOOP_STATS
 22060|         snoop_stat.objects_checked_count++;
 22061| #endif //SNOOP_STATS
 22062|         o = mark_queue.queue_mark (o);
 22063|         if (o != nullptr)
 22064|         {
 22065|             m_boundary (o);
 22066|             size_t s = size (o);
 22067|             add_to_promoted_bytes (o, s, thread);
 22068|             {
 22069|                 go_through_object_cl (method_table(o), o, s, poo,
 22070|                                         {
 22071|                                             uint8_t* oo = mark_queue.queue_mark(*poo, condemned_gen);
 22072|                                             if (oo != nullptr)
 22073|                                             {
 22074|                                                 m_boundary (oo);
 22075|                                                 add_to_promoted_bytes (oo, thread);
 22076|                                                 if (contain_pointers_or_collectible (oo))
 22077|                                                     mark_object_simple1 (oo, oo THREAD_NUMBER_ARG);
 22078|                                             }
 22079|                                         }
 22080|                     );
 22081|             }
 22082|         }
 22083|     }
 22084| }
 22085| inline
 22086| void gc_heap::mark_object (uint8_t* o THREAD_NUMBER_DCL)
 22087| {
 22088| #ifdef USE_REGIONS
 22089|     if (is_in_gc_range (o) && is_in_condemned_gc (o))
 22090|     {
 22091|         mark_object_simple (&o THREAD_NUMBER_ARG);
 22092|     }
 22093| #else //USE_REGIONS
 22094|     if ((o >= gc_low) && (o < gc_high))
 22095|         mark_object_simple (&o THREAD_NUMBER_ARG);
 22096| #ifdef MULTIPLE_HEAPS
 22097|     else if (o)
 22098|     {
 22099|         gc_heap* hp = heap_of (o);
 22100|         assert (hp);
 22101|         if ((o >= hp->gc_low) && (o < hp->gc_high))
 22102|             mark_object_simple (&o THREAD_NUMBER_ARG);
 22103|     }
 22104| #endif //MULTIPLE_HEAPS
 22105| #endif //USE_REGIONS
 22106| }
 22107| void gc_heap::drain_mark_queue ()
 22108| {
 22109|     int condemned_gen =
 22110| #ifdef USE_REGIONS
 22111|         settings.condemned_generation;
 22112| #else
 22113|         -1;
 22114| #endif //USE_REGIONS
 22115| #ifdef MULTIPLE_HEAPS
 22116|     THREAD_FROM_HEAP;
 22117| #else
 22118|     const int thread = 0;
 22119| #endif //MULTIPLE_HEAPS
 22120|     uint8_t* o;
 22121|     while ((o = mark_queue.get_next_marked()) != nullptr)
 22122|     {
 22123|         m_boundary (o);
 22124|         size_t s = size (o);
 22125|         add_to_promoted_bytes (o, s, thread);
 22126|         if (contain_pointers_or_collectible (o))
 22127|         {
 22128|             go_through_object_cl (method_table(o), o, s, poo,
 22129|                                     {
 22130|                                         uint8_t* oo = mark_queue.queue_mark(*poo, condemned_gen);
 22131|                                         if (oo != nullptr)
 22132|                                         {
 22133|                                             m_boundary (oo);
 22134|                                             add_to_promoted_bytes (oo, thread);
 22135|                                             if (contain_pointers_or_collectible (oo))
 22136|                                                 mark_object_simple1 (oo, oo THREAD_NUMBER_ARG);
 22137|                                         }
 22138|                                     }
 22139|                 );
 22140|         }
 22141|     }
 22142| }
 22143| #ifdef BACKGROUND_GC
 22144| #ifdef USE_REGIONS
 22145| void gc_heap::set_background_overflow_p (uint8_t* oo)
 22146| {
 22147|     heap_segment* overflow_region = get_region_info_for_address (oo);
 22148|     overflow_region->flags |= heap_segment_flags_overflow;
 22149|     dprintf (3,("setting overflow flag for region %p", heap_segment_mem (overflow_region)));
 22150|     background_overflow_p = TRUE;
 22151| }
 22152| #endif //USE_REGIONS
 22153| void gc_heap::background_mark_simple1 (uint8_t* oo THREAD_NUMBER_DCL)
 22154| {
 22155|     uint8_t** mark_stack_limit = &background_mark_stack_array[background_mark_stack_array_length];
 22156| #ifdef SORT_MARK_STACK
 22157|     uint8_t** sorted_tos = background_mark_stack_array;
 22158| #endif //SORT_MARK_STACK
 22159|     background_mark_stack_tos = background_mark_stack_array;
 22160|     while (1)
 22161|     {
 22162| #ifdef MULTIPLE_HEAPS
 22163| #else  //MULTIPLE_HEAPS
 22164|         const int thread = 0;
 22165| #endif //MULTIPLE_HEAPS
 22166|         if (oo)
 22167|         {
 22168|             size_t s = 0;
 22169|             if ((((size_t)oo & 1) == 0) && ((s = size (oo)) < (partial_size_th*sizeof (uint8_t*))))
 22170|             {
 22171|                 BOOL overflow_p = FALSE;
 22172|                 if (background_mark_stack_tos + (s) /sizeof (uint8_t*) >= (mark_stack_limit - 1))
 22173|                 {
 22174|                     size_t num_components = ((method_table(oo))->HasComponentSize() ? ((CObjectHeader*)oo)->GetNumComponents() : 0);
 22175|                     size_t num_pointers = CGCDesc::GetNumPointers(method_table(oo), s, num_components);
 22176|                     if (background_mark_stack_tos + num_pointers >= (mark_stack_limit - 1))
 22177|                     {
 22178|                         dprintf (2, ("h%d: %zd left, obj (mt: %p) %zd ptrs",
 22179|                             heap_number,
 22180|                             (size_t)(mark_stack_limit - 1 - background_mark_stack_tos),
 22181|                             method_table(oo),
 22182|                             num_pointers));
 22183|                         bgc_overflow_count++;
 22184|                         overflow_p = TRUE;
 22185|                     }
 22186|                 }
 22187|                 if (overflow_p == FALSE)
 22188|                 {
 22189|                     dprintf(3,("pushing mark for %zx ", (size_t)oo));
 22190|                     go_through_object_cl (method_table(oo), oo, s, ppslot,
 22191|                     {
 22192|                         uint8_t* o = *ppslot;
 22193|                         Prefetch(o);
 22194|                         if (background_mark (o,
 22195|                                              background_saved_lowest_address,
 22196|                                              background_saved_highest_address))
 22197|                         {
 22198|                             size_t obj_size = size (o);
 22199|                             bpromoted_bytes (thread) += obj_size;
 22200|                             if (contain_pointers_or_collectible (o))
 22201|                             {
 22202|                                 *(background_mark_stack_tos++) = o;
 22203|                             }
 22204|                         }
 22205|                     }
 22206|                         );
 22207|                 }
 22208|                 else
 22209|                 {
 22210|                     dprintf (3,("background mark stack overflow for object %zx ", (size_t)oo));
 22211| #ifdef USE_REGIONS
 22212|                     set_background_overflow_p (oo);
 22213| #else //USE_REGIONS
 22214|                     background_min_overflow_address = min (background_min_overflow_address, oo);
 22215|                     background_max_overflow_address = max (background_max_overflow_address, oo);
 22216| #endif //USE_REGIONS
 22217|                 }
 22218|             }
 22219|             else
 22220|             {
 22221|                 uint8_t* start = oo;
 22222|                 if ((size_t)oo & 1)
 22223|                 {
 22224|                     oo = (uint8_t*)((size_t)oo & ~1);
 22225|                     start = *(--background_mark_stack_tos);
 22226|                     dprintf (4, ("oo: %zx, start: %zx\n", (size_t)oo, (size_t)start));
 22227|                 }
 22228| #ifdef COLLECTIBLE_CLASS
 22229|                 else
 22230|                 {
 22231|                     if (is_collectible (oo))
 22232|                     {
 22233|                         uint8_t* class_obj = get_class_object (oo);
 22234|                         if (background_mark (class_obj,
 22235|                                             background_saved_lowest_address,
 22236|                                             background_saved_highest_address))
 22237|                         {
 22238|                             size_t obj_size = size (class_obj);
 22239|                             bpromoted_bytes (thread) += obj_size;
 22240|                             *(background_mark_stack_tos++) = class_obj;
 22241|                         }
 22242|                     }
 22243|                     if (!contain_pointers (oo))
 22244|                     {
 22245|                         goto next_level;
 22246|                     }
 22247|                 }
 22248| #endif //COLLECTIBLE_CLASS
 22249|                 s = size (oo);
 22250|                 BOOL overflow_p = FALSE;
 22251|                 if (background_mark_stack_tos + (num_partial_refs + 2)  >= mark_stack_limit)
 22252|                 {
 22253|                     size_t num_components = ((method_table(oo))->HasComponentSize() ? ((CObjectHeader*)oo)->GetNumComponents() : 0);
 22254|                     size_t num_pointers = CGCDesc::GetNumPointers(method_table(oo), s, num_components);
 22255|                     dprintf (2, ("h%d: PM: %zd left, obj %p (mt: %p) start: %p, total: %zd",
 22256|                         heap_number,
 22257|                         (size_t)(mark_stack_limit - background_mark_stack_tos),
 22258|                         oo,
 22259|                         method_table(oo),
 22260|                         start,
 22261|                         num_pointers));
 22262|                     bgc_overflow_count++;
 22263|                     overflow_p = TRUE;
 22264|                 }
 22265|                 if (overflow_p == FALSE)
 22266|                 {
 22267|                     dprintf(3,("pushing mark for %zx ", (size_t)oo));
 22268|                     uint8_t** place = background_mark_stack_tos++;
 22269|                     *(place) = start;
 22270|                     *(background_mark_stack_tos++) = (uint8_t*)((size_t)oo | 1);
 22271|                     int num_pushed_refs = num_partial_refs;
 22272|                     int num_processed_refs = num_pushed_refs * 16;
 22273|                     go_through_object (method_table(oo), oo, s, ppslot,
 22274|                                        start, use_start, (oo + s),
 22275|                     {
 22276|                         uint8_t* o = *ppslot;
 22277|                         Prefetch(o);
 22278|                         if (background_mark (o,
 22279|                                             background_saved_lowest_address,
 22280|                                             background_saved_highest_address))
 22281|                         {
 22282|                             size_t obj_size = size (o);
 22283|                             bpromoted_bytes (thread) += obj_size;
 22284|                             if (contain_pointers_or_collectible (o))
 22285|                             {
 22286|                                 *(background_mark_stack_tos++) = o;
 22287|                                 if (--num_pushed_refs == 0)
 22288|                                 {
 22289|                                     *place = (uint8_t*)(ppslot+1);
 22290|                                     goto more_to_do;
 22291|                                 }
 22292|                             }
 22293|                         }
 22294|                         if (--num_processed_refs == 0)
 22295|                         {
 22296|                             *place = (uint8_t*)(ppslot + 1);
 22297|                             goto more_to_do;
 22298|                         }
 22299|                         }
 22300|                         );
 22301|                     *place = 0;
 22302|                     *(place+1) = 0;
 22303|                 more_to_do:;
 22304|                 }
 22305|                 else
 22306|                 {
 22307|                     dprintf (3,("background mark stack overflow for object %zx ", (size_t)oo));
 22308| #ifdef USE_REGIONS
 22309|                     set_background_overflow_p (oo);
 22310| #else //USE_REGIONS
 22311|                     background_min_overflow_address = min (background_min_overflow_address, oo);
 22312|                     background_max_overflow_address = max (background_max_overflow_address, oo);
 22313| #endif //USE_REGIONS
 22314|                 }
 22315|             }
 22316|         }
 22317| #ifdef SORT_MARK_STACK
 22318|         if (background_mark_stack_tos > sorted_tos + mark_stack_array_length/8)
 22319|         {
 22320|             rqsort1 (sorted_tos, background_mark_stack_tos-1);
 22321|             sorted_tos = background_mark_stack_tos-1;
 22322|         }
 22323| #endif //SORT_MARK_STACK
 22324| #ifdef COLLECTIBLE_CLASS
 22325| next_level:
 22326| #endif // COLLECTIBLE_CLASS
 22327|         allow_fgc();
 22328|         if (!(background_mark_stack_tos == background_mark_stack_array))
 22329|         {
 22330|             oo = *(--background_mark_stack_tos);
 22331| #ifdef SORT_MARK_STACK
 22332|             sorted_tos = (uint8_t**)min ((size_t)sorted_tos, (size_t)background_mark_stack_tos);
 22333| #endif //SORT_MARK_STACK
 22334|         }
 22335|         else
 22336|             break;
 22337|     }
 22338|     assert (background_mark_stack_tos == background_mark_stack_array);
 22339| }
 22340| void
 22341| gc_heap::background_mark_simple (uint8_t* o THREAD_NUMBER_DCL)
 22342| {
 22343| #ifdef MULTIPLE_HEAPS
 22344| #else  //MULTIPLE_HEAPS
 22345|     const int thread = 0;
 22346| #endif //MULTIPLE_HEAPS
 22347|     {
 22348|         dprintf (3, ("bmarking %p", o));
 22349|         if (background_mark1 (o))
 22350|         {
 22351|             size_t s = size (o);
 22352|             bpromoted_bytes (thread) += s;
 22353|             if (contain_pointers_or_collectible (o))
 22354|             {
 22355|                 background_mark_simple1 (o THREAD_NUMBER_ARG);
 22356|             }
 22357|         }
 22358|         allow_fgc();
 22359|     }
 22360| }
 22361| inline
 22362| uint8_t* gc_heap::background_mark_object (uint8_t* o THREAD_NUMBER_DCL)
 22363| {
 22364|     if ((o >= background_saved_lowest_address) && (o < background_saved_highest_address))
 22365|     {
 22366|         background_mark_simple (o THREAD_NUMBER_ARG);
 22367|     }
 22368|     else
 22369|     {
 22370|         if (o)
 22371|         {
 22372|             dprintf (3, ("or-%p", o));
 22373|         }
 22374|     }
 22375|     return o;
 22376| }
 22377| void gc_heap::background_promote (Object** ppObject, ScanContext* sc, uint32_t flags)
 22378| {
 22379|     UNREFERENCED_PARAMETER(sc);
 22380|     assert (settings.concurrent);
 22381|     THREAD_NUMBER_FROM_CONTEXT;
 22382| #ifndef MULTIPLE_HEAPS
 22383|     const int thread = 0;
 22384| #endif //!MULTIPLE_HEAPS
 22385|     uint8_t* o = (uint8_t*)*ppObject;
 22386|     if (!is_in_find_object_range (o))
 22387|     {
 22388|         return;
 22389|     }
 22390| #ifdef DEBUG_DestroyedHandleValue
 22391|     if (o == (uint8_t*)DEBUG_DestroyedHandleValue)
 22392|         return;
 22393| #endif //DEBUG_DestroyedHandleValue
 22394|     HEAP_FROM_THREAD;
 22395|     gc_heap* hp = gc_heap::heap_of (o);
 22396|     if ((o < hp->background_saved_lowest_address) || (o >= hp->background_saved_highest_address))
 22397|     {
 22398|         return;
 22399|     }
 22400|     if (flags & GC_CALL_INTERIOR)
 22401|     {
 22402|         o = hp->find_object (o);
 22403|         if (o == 0)
 22404|             return;
 22405|     }
 22406| #ifdef FEATURE_CONSERVATIVE_GC
 22407|     if (GCConfig::GetConservativeGC() && ((CObjectHeader*)o)->IsFree())
 22408|     {
 22409|         return;
 22410|     }
 22411| #endif //FEATURE_CONSERVATIVE_GC
 22412| #ifdef _DEBUG
 22413|     ((CObjectHeader*)o)->Validate();
 22414| #endif //_DEBUG
 22415|     STRESS_LOG3(LF_GC|LF_GCROOTS, LL_INFO1000000, "    GCHeap::Promote: Promote GC Root *%p = %p MT = %pT", ppObject, o, o ? ((Object*) o)->GetGCSafeMethodTable() : NULL);
 22416|     hpt->background_mark_simple (o THREAD_NUMBER_ARG);
 22417| }
 22418| void
 22419| gc_heap::scan_background_roots (promote_func* fn, int hn, ScanContext *pSC)
 22420| {
 22421|     ScanContext sc;
 22422|     if (pSC == 0)
 22423|         pSC = &sc;
 22424|     pSC->thread_number = hn;
 22425|     pSC->thread_count = n_heaps;
 22426|     BOOL relocate_p = (fn == &GCHeap::Relocate);
 22427|     dprintf (3, ("Scanning background mark list"));
 22428|     size_t mark_list_finger = 0;
 22429|     while (mark_list_finger < c_mark_list_index)
 22430|     {
 22431|         uint8_t** o = &c_mark_list [mark_list_finger];
 22432|         if (!relocate_p)
 22433|         {
 22434|             size_t s = size (*o);
 22435|             assert (Align (s) >= Align (min_obj_size));
 22436|             dprintf(3,("background root %zx", (size_t)*o));
 22437|         }
 22438|         (*fn) ((Object**)o, pSC, 0);
 22439|         mark_list_finger++;
 22440|     }
 22441|     dprintf (3, ("Scanning background mark stack"));
 22442|     uint8_t** finger = background_mark_stack_array;
 22443|     while (finger < background_mark_stack_tos)
 22444|     {
 22445|         if ((finger + 1) < background_mark_stack_tos)
 22446|         {
 22447|             uint8_t* parent_obj = *(finger + 1);
 22448|             if ((size_t)parent_obj & 1)
 22449|             {
 22450|                 uint8_t* place = *finger;
 22451|                 size_t place_offset = 0;
 22452|                 uint8_t* real_parent_obj = (uint8_t*)((size_t)parent_obj & ~1);
 22453|                 if (relocate_p)
 22454|                 {
 22455|                     *(finger + 1) = real_parent_obj;
 22456|                     place_offset = place - real_parent_obj;
 22457|                     dprintf(3,("relocating background root %zx", (size_t)real_parent_obj));
 22458|                     (*fn) ((Object**)(finger + 1), pSC, 0);
 22459|                     real_parent_obj = *(finger + 1);
 22460|                     *finger = real_parent_obj + place_offset;
 22461|                     *(finger + 1) = (uint8_t*)((size_t)real_parent_obj | 1);
 22462|                     dprintf(3,("roots changed to %p, %p", *finger, *(finger + 1)));
 22463|                 }
 22464|                 else
 22465|                 {
 22466|                     uint8_t** temp = &real_parent_obj;
 22467|                     dprintf(3,("marking background root %zx", (size_t)real_parent_obj));
 22468|                     (*fn) ((Object**)temp, pSC, 0);
 22469|                 }
 22470|                 finger += 2;
 22471|                 continue;
 22472|             }
 22473|         }
 22474|         dprintf(3,("background root %zx", (size_t)*finger));
 22475|         (*fn) ((Object**)finger, pSC, 0);
 22476|         finger++;
 22477|     }
 22478| }
 22479| void gc_heap::grow_bgc_mark_stack (size_t new_size)
 22480| {
 22481|     if ((background_mark_stack_array_length < new_size) &&
 22482|         ((new_size - background_mark_stack_array_length) > (background_mark_stack_array_length / 2)))
 22483|     {
 22484|         dprintf (2, ("h%d: ov grow to %zd", heap_number, new_size));
 22485|         uint8_t** tmp = new (nothrow) uint8_t* [new_size];
 22486|         if (tmp)
 22487|         {
 22488|             delete [] background_mark_stack_array;
 22489|             background_mark_stack_array = tmp;
 22490|             background_mark_stack_array_length = new_size;
 22491|             background_mark_stack_tos = background_mark_stack_array;
 22492|         }
 22493|     }
 22494| }
 22495| void gc_heap::check_bgc_mark_stack_length()
 22496| {
 22497|     if ((settings.condemned_generation < (max_generation - 1)) || gc_heap::background_running_p())
 22498|         return;
 22499|     size_t total_heap_size = get_total_heap_size();
 22500|     if (total_heap_size < ((size_t)4*1024*1024*1024))
 22501|         return;
 22502| #ifdef MULTIPLE_HEAPS
 22503|     int total_heaps = n_heaps;
 22504| #else
 22505|     int total_heaps = 1;
 22506| #endif //MULTIPLE_HEAPS
 22507|     size_t size_based_on_heap = total_heap_size / (size_t)(100 * 100 * total_heaps * sizeof (uint8_t*));
 22508|     size_t new_size = max (background_mark_stack_array_length, size_based_on_heap);
 22509|     grow_bgc_mark_stack (new_size);
 22510| }
 22511| uint8_t* gc_heap::background_seg_end (heap_segment* seg, BOOL concurrent_p)
 22512| {
 22513| #ifndef USE_REGIONS
 22514|     if (concurrent_p && (seg == saved_overflow_ephemeral_seg))
 22515|     {
 22516|         return background_min_soh_overflow_address;
 22517|     }
 22518|     else
 22519| #endif //!USE_REGIONS
 22520|     {
 22521|         return heap_segment_allocated (seg);
 22522|     }
 22523| }
 22524| uint8_t* gc_heap::background_first_overflow (uint8_t* min_add,
 22525|                                           heap_segment* seg,
 22526|                                           BOOL concurrent_p,
 22527|                                           BOOL small_object_p)
 22528| {
 22529| #ifdef USE_REGIONS
 22530|         return heap_segment_mem (seg);
 22531| #else
 22532|     uint8_t* o = 0;
 22533|     if (small_object_p)
 22534|     {
 22535|         if (in_range_for_segment (min_add, seg))
 22536|         {
 22537|             if (min_add >= heap_segment_allocated (seg))
 22538|             {
 22539|                 return min_add;
 22540|             }
 22541|             else
 22542|             {
 22543|                 if (concurrent_p &&
 22544|                     ((seg == saved_overflow_ephemeral_seg) && (min_add >= background_min_soh_overflow_address)))
 22545|                 {
 22546|                     return background_min_soh_overflow_address;
 22547|                 }
 22548|                 else
 22549|                 {
 22550|                     o = find_first_object (min_add, heap_segment_mem (seg));
 22551|                     return o;
 22552|                 }
 22553|             }
 22554|         }
 22555|     }
 22556|     o = max (heap_segment_mem (seg), min_add);
 22557|     return o;
 22558| #endif //USE_REGIONS
 22559| }
 22560| void gc_heap::background_process_mark_overflow_internal (uint8_t* min_add, uint8_t* max_add,
 22561|                                                          BOOL concurrent_p)
 22562| {
 22563|     if (concurrent_p)
 22564|     {
 22565|         current_bgc_state = bgc_overflow_soh;
 22566|     }
 22567|     size_t total_marked_objects = 0;
 22568| #ifdef MULTIPLE_HEAPS
 22569|     int thread = heap_number;
 22570| #endif //MULTIPLE_HEAPS
 22571|     int start_gen_idx = get_start_generation_index();
 22572| #ifdef USE_REGIONS
 22573|     if (concurrent_p)
 22574|         start_gen_idx = max_generation;
 22575| #endif //USE_REGIONS
 22576|     exclusive_sync* loh_alloc_lock = 0;
 22577| #ifndef USE_REGIONS
 22578|     dprintf (2,("Processing Mark overflow [%zx %zx]", (size_t)min_add, (size_t)max_add));
 22579| #endif
 22580| #ifdef MULTIPLE_HEAPS
 22581|     int h_start = (concurrent_p ? heap_number : 0);
 22582|     int h_end = (concurrent_p ? (heap_number + 1) : n_heaps);
 22583|     for (int hi = h_start; hi < h_end; hi++)
 22584|     {
 22585|         gc_heap*  hp = (concurrent_p ? this : g_heaps [(heap_number + hi) % n_heaps]);
 22586| #else
 22587|     {
 22588|         gc_heap*  hp = 0;
 22589| #endif //MULTIPLE_HEAPS
 22590|         BOOL small_object_segments = TRUE;
 22591|         loh_alloc_lock = hp->bgc_alloc_lock;
 22592|         for (int i = start_gen_idx; i < total_generation_count; i++)
 22593|         {
 22594|             int align_const = get_alignment_constant (small_object_segments);
 22595|             generation* gen = hp->generation_of (i);
 22596|             heap_segment* seg = heap_segment_in_range (generation_start_segment (gen));
 22597|             PREFIX_ASSUME(seg != NULL);
 22598|             uint8_t* current_min_add = min_add;
 22599|             uint8_t* current_max_add = max_add;
 22600|             while (seg)
 22601|             {
 22602| #ifdef USE_REGIONS
 22603|                 if (heap_segment_overflow_p (seg))
 22604|                 {
 22605|                     seg->flags &= ~heap_segment_flags_overflow;
 22606|                     current_min_add = heap_segment_mem (seg);
 22607|                     current_max_add = heap_segment_allocated (seg);
 22608|                     dprintf (2,("Processing Mark overflow [%zx %zx]", (size_t)current_min_add, (size_t)current_max_add));
 22609|                 }
 22610|                 else
 22611|                 {
 22612|                     current_min_add = current_max_add = 0;
 22613|                 }
 22614| #endif //USE_REGIONS
 22615|                 uint8_t* o = hp->background_first_overflow (current_min_add, seg, concurrent_p, small_object_segments);
 22616|                 while ((o < hp->background_seg_end (seg, concurrent_p)) && (o <= current_max_add))
 22617|                 {
 22618|                     dprintf (3, ("considering %zx", (size_t)o));
 22619|                     size_t s;
 22620|                     if (concurrent_p && !small_object_segments)
 22621|                     {
 22622|                         loh_alloc_lock->bgc_mark_set (o);
 22623|                         if (((CObjectHeader*)o)->IsFree())
 22624|                         {
 22625|                             s = unused_array_size (o);
 22626|                         }
 22627|                         else
 22628|                         {
 22629|                             s = size (o);
 22630|                         }
 22631|                     }
 22632|                     else
 22633|                     {
 22634|                         s = size (o);
 22635|                     }
 22636|                     if (background_object_marked (o, FALSE) && contain_pointers_or_collectible (o))
 22637|                     {
 22638|                         total_marked_objects++;
 22639|                         go_through_object_cl (method_table(o), o, s, poo,
 22640|                                               uint8_t* oo = *poo;
 22641|                                               background_mark_object (oo THREAD_NUMBER_ARG);
 22642|                                              );
 22643|                     }
 22644|                     if (concurrent_p && !small_object_segments)
 22645|                     {
 22646|                         loh_alloc_lock->bgc_mark_done ();
 22647|                     }
 22648|                     o = o + Align (s, align_const);
 22649|                     if (concurrent_p)
 22650|                     {
 22651|                         allow_fgc();
 22652|                     }
 22653|                 }
 22654| #ifdef USE_REGIONS
 22655|                 if (current_max_add != 0)
 22656| #endif //USE_REGIONS
 22657|                 {
 22658|                     dprintf (2, ("went through overflow objects in segment %p (%d) (so far %zd marked)",
 22659|                         heap_segment_mem (seg), (small_object_segments ? 0 : 1), total_marked_objects));
 22660|                 }
 22661| #ifndef USE_REGIONS
 22662|                 if (concurrent_p && (seg == hp->saved_overflow_ephemeral_seg))
 22663|                 {
 22664|                     break;
 22665|                 }
 22666| #endif //!USE_REGIONS
 22667|                 seg = heap_segment_next_in_range (seg);
 22668|             }
 22669|             if (concurrent_p)
 22670|             {
 22671|                 current_bgc_state = bgc_overflow_uoh;
 22672|             }
 22673|             dprintf (2, ("h%d: SOH: ov-mo: %zd", heap_number, total_marked_objects));
 22674|             fire_overflow_event (min_add, max_add, total_marked_objects, i);
 22675|             if (i >= soh_gen2)
 22676|             {
 22677|                 concurrent_print_time_delta (concurrent_p ? "Cov SOH" : "Nov SOH");
 22678|                 small_object_segments = FALSE;
 22679|             }
 22680|             total_marked_objects = 0;
 22681|         }
 22682|     }
 22683| }
 22684| BOOL gc_heap::background_process_mark_overflow (BOOL concurrent_p)
 22685| {
 22686|     BOOL grow_mark_array_p = TRUE;
 22687|     if (concurrent_p)
 22688|     {
 22689|         assert (!processed_eph_overflow_p);
 22690| #ifndef USE_REGIONS
 22691|         if ((background_max_overflow_address != 0) &&
 22692|             (background_min_overflow_address != MAX_PTR))
 22693|         {
 22694|             saved_overflow_ephemeral_seg = ephemeral_heap_segment;
 22695|             background_max_soh_overflow_address = heap_segment_reserved (saved_overflow_ephemeral_seg);
 22696|             background_min_soh_overflow_address = generation_allocation_start (generation_of (max_generation - 1));
 22697|         }
 22698| #endif //!USE_REGIONS
 22699|     }
 22700|     else
 22701|     {
 22702| #ifndef USE_REGIONS
 22703|         assert ((saved_overflow_ephemeral_seg == 0) ||
 22704|                 ((background_max_soh_overflow_address != 0) &&
 22705|                  (background_min_soh_overflow_address != MAX_PTR)));
 22706| #endif //!USE_REGIONS
 22707|         if (!processed_eph_overflow_p)
 22708|         {
 22709| #ifdef USE_REGIONS
 22710|             if (!background_overflow_p)
 22711| #else
 22712|             if ((background_max_overflow_address == 0) && (background_min_overflow_address == MAX_PTR))
 22713| #endif //USE_REGIONS
 22714|             {
 22715|                 dprintf (2, ("final processing mark overflow - no more overflow since last time"));
 22716|                 grow_mark_array_p = FALSE;
 22717|             }
 22718| #ifdef USE_REGIONS
 22719|             background_overflow_p = TRUE;
 22720| #else
 22721|             background_min_overflow_address = min (background_min_overflow_address,
 22722|                                                 background_min_soh_overflow_address);
 22723|             background_max_overflow_address = max (background_max_overflow_address,
 22724|                                                 background_max_soh_overflow_address);
 22725| #endif //!USE_REGIONS
 22726|             processed_eph_overflow_p = TRUE;
 22727|         }
 22728|     }
 22729|     BOOL  overflow_p = FALSE;
 22730| recheck:
 22731| #ifdef USE_REGIONS
 22732|     if (background_overflow_p)
 22733| #else
 22734|     if ((! ((background_max_overflow_address == 0)) ||
 22735|          ! ((background_min_overflow_address == MAX_PTR))))
 22736| #endif
 22737|     {
 22738|         overflow_p = TRUE;
 22739|         if (grow_mark_array_p)
 22740|         {
 22741|             size_t new_size = max (MARK_STACK_INITIAL_LENGTH, 2*background_mark_stack_array_length);
 22742|             if ((new_size * sizeof(mark)) > 100*1024)
 22743|             {
 22744|                 size_t new_max_size = (get_total_heap_size() / 10) / sizeof(mark);
 22745|                 new_size = min(new_max_size, new_size);
 22746|             }
 22747|             grow_bgc_mark_stack (new_size);
 22748|         }
 22749|         else
 22750|         {
 22751|             grow_mark_array_p = TRUE;
 22752|         }
 22753| #ifdef USE_REGIONS
 22754|         uint8_t*  min_add = 0;
 22755|         uint8_t*  max_add = 0;
 22756|         background_overflow_p = FALSE;
 22757| #else
 22758|         uint8_t*  min_add = background_min_overflow_address;
 22759|         uint8_t*  max_add = background_max_overflow_address;
 22760|         background_max_overflow_address = 0;
 22761|         background_min_overflow_address = MAX_PTR;
 22762| #endif
 22763|         background_process_mark_overflow_internal (min_add, max_add, concurrent_p);
 22764|         if (!concurrent_p)
 22765|         {
 22766|             goto recheck;
 22767|         }
 22768|     }
 22769|     return overflow_p;
 22770| }
 22771| #endif //BACKGROUND_GC
 22772| inline
 22773| void gc_heap::mark_through_object (uint8_t* oo, BOOL mark_class_object_p THREAD_NUMBER_DCL)
 22774| {
 22775| #ifndef COLLECTIBLE_CLASS
 22776|     UNREFERENCED_PARAMETER(mark_class_object_p);
 22777|     BOOL to_mark_class_object = FALSE;
 22778| #else //COLLECTIBLE_CLASS
 22779|     BOOL to_mark_class_object = (mark_class_object_p && (is_collectible(oo)));
 22780| #endif //COLLECTIBLE_CLASS
 22781|     if (contain_pointers (oo) || to_mark_class_object)
 22782|     {
 22783|         dprintf(3,( "Marking through %zx", (size_t)oo));
 22784|         size_t s = size (oo);
 22785| #ifdef COLLECTIBLE_CLASS
 22786|         if (to_mark_class_object)
 22787|         {
 22788|             uint8_t* class_obj = get_class_object (oo);
 22789|             mark_object (class_obj THREAD_NUMBER_ARG);
 22790|         }
 22791| #endif //COLLECTIBLE_CLASS
 22792|         if (contain_pointers (oo))
 22793|         {
 22794|             go_through_object_nostart (method_table(oo), oo, s, po,
 22795|                                 uint8_t* o = *po;
 22796|                                 mark_object (o THREAD_NUMBER_ARG);
 22797|                                 );
 22798|         }
 22799|     }
 22800| }
 22801| size_t gc_heap::get_total_heap_size()
 22802| {
 22803|     size_t total_heap_size = 0;
 22804| #ifdef MULTIPLE_HEAPS
 22805|     int hn = 0;
 22806|     for (hn = 0; hn < gc_heap::n_heaps; hn++)
 22807|     {
 22808|         gc_heap* hp2 = gc_heap::g_heaps [hn];
 22809|         for (int i = max_generation; i < total_generation_count; i++)
 22810|         {
 22811|             total_heap_size += hp2->generation_sizes (hp2->generation_of (i));
 22812|         }
 22813|     }
 22814| #else
 22815|     for (int i = max_generation; i < total_generation_count; i++)
 22816|     {
 22817|         total_heap_size += generation_sizes (generation_of (i));
 22818|     }
 22819| #endif //MULTIPLE_HEAPS
 22820|     return total_heap_size;
 22821| }
 22822| size_t gc_heap::get_total_fragmentation()
 22823| {
 22824|     size_t total_fragmentation = 0;
 22825| #ifdef MULTIPLE_HEAPS
 22826|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 22827|     {
 22828|         gc_heap* hp = gc_heap::g_heaps[hn];
 22829| #else //MULTIPLE_HEAPS
 22830|     {
 22831|         gc_heap* hp = pGenGCHeap;
 22832| #endif //MULTIPLE_HEAPS
 22833|         for (int i = 0; i < total_generation_count; i++)
 22834|         {
 22835|             generation* gen = hp->generation_of (i);
 22836|             total_fragmentation += (generation_free_list_space (gen) + generation_free_obj_space (gen));
 22837|         }
 22838|     }
 22839|     return total_fragmentation;
 22840| }
 22841| size_t gc_heap::get_total_gen_fragmentation (int gen_number)
 22842| {
 22843|     size_t total_fragmentation = 0;
 22844| #ifdef MULTIPLE_HEAPS
 22845|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 22846|     {
 22847|         gc_heap* hp = gc_heap::g_heaps[hn];
 22848| #else //MULTIPLE_HEAPS
 22849|     {
 22850|         gc_heap* hp = pGenGCHeap;
 22851| #endif //MULTIPLE_HEAPS
 22852|         generation* gen = hp->generation_of (gen_number);
 22853|         total_fragmentation += (generation_free_list_space (gen) + generation_free_obj_space (gen));
 22854|     }
 22855|     return total_fragmentation;
 22856| }
 22857| #ifdef USE_REGIONS
 22858| int gc_heap::get_total_new_gen0_regions_in_plns ()
 22859| {
 22860|     int total_new_gen0_regions_in_plns = 0;
 22861| #ifdef MULTIPLE_HEAPS
 22862|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 22863|     {
 22864|         gc_heap* hp = gc_heap::g_heaps[hn];
 22865| #else //MULTIPLE_HEAPS
 22866|     {
 22867|         gc_heap* hp = pGenGCHeap;
 22868| #endif //MULTIPLE_HEAPS
 22869|         total_new_gen0_regions_in_plns += hp->new_gen0_regions_in_plns;
 22870|     }
 22871|     return total_new_gen0_regions_in_plns;
 22872| }
 22873| int gc_heap::get_total_new_regions_in_prr ()
 22874| {
 22875|     int total_new_regions_in_prr = 0;
 22876| #ifdef MULTIPLE_HEAPS
 22877|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 22878|     {
 22879|         gc_heap* hp = gc_heap::g_heaps[hn];
 22880| #else //MULTIPLE_HEAPS
 22881|         {
 22882|             gc_heap* hp = pGenGCHeap;
 22883| #endif //MULTIPLE_HEAPS
 22884|             total_new_regions_in_prr += hp->new_regions_in_prr;
 22885|         }
 22886|         return total_new_regions_in_prr;
 22887| }
 22888| int gc_heap::get_total_new_regions_in_threading ()
 22889| {
 22890|     int total_new_regions_in_threading = 0;
 22891| #ifdef MULTIPLE_HEAPS
 22892|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 22893|     {
 22894|         gc_heap* hp = gc_heap::g_heaps[hn];
 22895| #else //MULTIPLE_HEAPS
 22896|     {
 22897|         gc_heap* hp = pGenGCHeap;
 22898| #endif //MULTIPLE_HEAPS
 22899|         total_new_regions_in_threading += hp->new_regions_in_threading;
 22900|     }
 22901|     return total_new_regions_in_threading;
 22902| }
 22903| #endif //USE_REGIONS
 22904| size_t gc_heap::get_total_gen_estimated_reclaim (int gen_number)
 22905| {
 22906|     size_t total_estimated_reclaim = 0;
 22907| #ifdef MULTIPLE_HEAPS
 22908|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 22909|     {
 22910|         gc_heap* hp = gc_heap::g_heaps[hn];
 22911| #else //MULTIPLE_HEAPS
 22912|     {
 22913|         gc_heap* hp = pGenGCHeap;
 22914| #endif //MULTIPLE_HEAPS
 22915|         total_estimated_reclaim += hp->estimated_reclaim (gen_number);
 22916|     }
 22917|     return total_estimated_reclaim;
 22918| }
 22919| size_t gc_heap::get_total_gen_size (int gen_number)
 22920| {
 22921| #ifdef MULTIPLE_HEAPS
 22922|     size_t size = 0;
 22923|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 22924|     {
 22925|         gc_heap* hp = gc_heap::g_heaps[hn];
 22926|         size += hp->generation_size (gen_number);
 22927|     }
 22928| #else
 22929|     size_t size = generation_size (gen_number);
 22930| #endif //MULTIPLE_HEAPS
 22931|     return size;
 22932| }
 22933| size_t gc_heap::committed_size()
 22934| {
 22935|     size_t total_committed = 0;
 22936|     const size_t kB = 1024;
 22937|     for (int i = get_start_generation_index(); i < total_generation_count; i++)
 22938|     {
 22939|         generation* gen = generation_of (i);
 22940|         heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 22941|         size_t gen_committed = 0;
 22942|         size_t gen_allocated = 0;
 22943|         while (seg)
 22944|         {
 22945|             uint8_t* start =
 22946| #ifdef USE_REGIONS
 22947|                 get_region_start (seg);
 22948| #else
 22949|                 (uint8_t*)seg;
 22950| #endif //USE_REGIONS
 22951|             gen_committed += heap_segment_committed (seg) - start;
 22952|             gen_allocated += heap_segment_allocated (seg) - start;
 22953|             seg = heap_segment_next (seg);
 22954|         }
 22955|         dprintf (3, ("h%d committed in gen%d %zdkB, allocated %zdkB, committed-allocated %zdkB", heap_number, i, gen_committed/kB, gen_allocated/kB, (gen_committed - gen_allocated)/kB));
 22956|         total_committed += gen_committed;
 22957|     }
 22958| #ifdef USE_REGIONS
 22959|     size_t committed_in_free = 0;
 22960|     for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
 22961|     {
 22962|         committed_in_free += free_regions[kind].get_size_committed_in_free();
 22963|     }
 22964|     dprintf (3, ("h%d committed in free %zdkB", heap_number, committed_in_free/kB));
 22965|     total_committed += committed_in_free;
 22966| #endif //USE_REGIONS
 22967|     return total_committed;
 22968| }
 22969| size_t gc_heap::get_total_committed_size()
 22970| {
 22971|     size_t total_committed = 0;
 22972| #ifdef MULTIPLE_HEAPS
 22973|     int hn = 0;
 22974|     for (hn = 0; hn < gc_heap::n_heaps; hn++)
 22975|     {
 22976|         gc_heap* hp = gc_heap::g_heaps [hn];
 22977|         total_committed += hp->committed_size();
 22978|     }
 22979| #else
 22980|     total_committed = committed_size();
 22981| #endif //MULTIPLE_HEAPS
 22982|     return total_committed;
 22983| }
 22984| size_t gc_heap::uoh_committed_size (int gen_number, size_t* allocated)
 22985| {
 22986|     generation* gen = generation_of (gen_number);
 22987|     heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 22988|     size_t total_committed = 0;
 22989|     size_t total_allocated = 0;
 22990|     while (seg)
 22991|     {
 22992|         uint8_t* start =
 22993| #ifdef USE_REGIONS
 22994|             get_region_start (seg);
 22995| #else
 22996|             (uint8_t*)seg;
 22997| #endif //USE_REGIONS
 22998|         total_committed += heap_segment_committed (seg) - start;
 22999|         total_allocated += heap_segment_allocated (seg) - start;
 23000|         seg = heap_segment_next (seg);
 23001|     }
 23002|     *allocated = total_allocated;
 23003|     return total_committed;
 23004| }
 23005| void gc_heap::get_memory_info (uint32_t* memory_load,
 23006|                                uint64_t* available_physical,
 23007|                                uint64_t* available_page_file)
 23008| {
 23009|     GCToOSInterface::GetMemoryStatus(is_restricted_physical_mem ? total_physical_mem  : 0,  memory_load, available_physical, available_page_file);
 23010| }
 23011| BOOL gc_heap::process_mark_overflow(int condemned_gen_number)
 23012| {
 23013|     size_t last_promoted_bytes = get_promoted_bytes();
 23014|     BOOL  overflow_p = FALSE;
 23015| recheck:
 23016|     drain_mark_queue();
 23017|     if ((! (max_overflow_address == 0) ||
 23018|          ! (min_overflow_address == MAX_PTR)))
 23019|     {
 23020|         overflow_p = TRUE;
 23021|         size_t new_size =
 23022|             max (MARK_STACK_INITIAL_LENGTH, 2*mark_stack_array_length);
 23023|         if ((new_size * sizeof(mark)) > 100*1024)
 23024|         {
 23025|             size_t new_max_size = (get_total_heap_size() / 10) / sizeof(mark);
 23026|             new_size = min(new_max_size, new_size);
 23027|         }
 23028|         if ((mark_stack_array_length < new_size) &&
 23029|             ((new_size - mark_stack_array_length) > (mark_stack_array_length / 2)))
 23030|         {
 23031|             mark* tmp = new (nothrow) mark [new_size];
 23032|             if (tmp)
 23033|             {
 23034|                 delete mark_stack_array;
 23035|                 mark_stack_array = tmp;
 23036|                 mark_stack_array_length = new_size;
 23037|             }
 23038|         }
 23039|         uint8_t*  min_add = min_overflow_address;
 23040|         uint8_t*  max_add = max_overflow_address;
 23041|         max_overflow_address = 0;
 23042|         min_overflow_address = MAX_PTR;
 23043|         process_mark_overflow_internal (condemned_gen_number, min_add, max_add);
 23044|         goto recheck;
 23045|     }
 23046|     size_t current_promoted_bytes = get_promoted_bytes();
 23047|     if (current_promoted_bytes != last_promoted_bytes)
 23048|         fire_mark_event (ETW::GC_ROOT_OVERFLOW, current_promoted_bytes, last_promoted_bytes);
 23049|     return overflow_p;
 23050| }
 23051| void gc_heap::process_mark_overflow_internal (int condemned_gen_number,
 23052|                                               uint8_t* min_add, uint8_t* max_add)
 23053| {
 23054| #ifdef MULTIPLE_HEAPS
 23055|     int thread = heap_number;
 23056| #endif //MULTIPLE_HEAPS
 23057|     BOOL  full_p = (condemned_gen_number == max_generation);
 23058|     dprintf(3,("Processing Mark overflow [%zx %zx]", (size_t)min_add, (size_t)max_add));
 23059|     size_t obj_count = 0;
 23060| #ifdef MULTIPLE_HEAPS
 23061|     for (int hi = 0; hi < n_heaps; hi++)
 23062|     {
 23063|         gc_heap*  hp = g_heaps [(heap_number + hi) % n_heaps];
 23064| #else
 23065|     {
 23066|         gc_heap*  hp = 0;
 23067| #endif //MULTIPLE_HEAPS
 23068|         int gen_limit = full_p ? total_generation_count : condemned_gen_number + 1;
 23069|         for (int i = get_stop_generation_index (condemned_gen_number); i < gen_limit; i++)
 23070|         {
 23071|             generation* gen = hp->generation_of (i);
 23072|             heap_segment* seg = heap_segment_in_range (generation_start_segment (gen));
 23073|             int align_const = get_alignment_constant (i < uoh_start_generation);
 23074|             PREFIX_ASSUME(seg != NULL);
 23075|             while (seg)
 23076|             {
 23077|                 uint8_t*  o = max (heap_segment_mem (seg), min_add);
 23078|                 uint8_t*  end = heap_segment_allocated (seg);
 23079|                 while ((o < end) && (o <= max_add))
 23080|                 {
 23081|                     assert ((min_add <= o) && (max_add >= o));
 23082|                     dprintf (3, ("considering %zx", (size_t)o));
 23083|                     if (marked (o))
 23084|                     {
 23085|                         mark_through_object (o, TRUE THREAD_NUMBER_ARG);
 23086|                         obj_count++;
 23087|                     }
 23088|                     o = o + Align (size (o), align_const);
 23089|                 }
 23090|                 seg = heap_segment_next_in_range (seg);
 23091|             }
 23092|         }
 23093| #ifndef MULTIPLE_HEAPS
 23094|         assert (obj_count > 0);
 23095| #endif //MULTIPLE_HEAPS
 23096|     }
 23097| }
 23098| #ifdef MULTIPLE_HEAPS
 23099| static VOLATILE(BOOL) s_fUnpromotedHandles = FALSE;
 23100| static VOLATILE(BOOL) s_fUnscannedPromotions = FALSE;
 23101| static VOLATILE(BOOL) s_fScanRequired;
 23102| void gc_heap::scan_dependent_handles (int condemned_gen_number, ScanContext *sc, BOOL initial_scan_p)
 23103| {
 23104|     s_fUnscannedPromotions = TRUE;
 23105|     while (true)
 23106|     {
 23107|         if (GCScan::GcDhUnpromotedHandlesExist(sc))
 23108|             s_fUnpromotedHandles = TRUE;
 23109|         drain_mark_queue();
 23110|         gc_t_join.join(this, gc_join_scan_dependent_handles);
 23111|         if (gc_t_join.joined())
 23112|         {
 23113|             s_fScanRequired = s_fUnscannedPromotions && s_fUnpromotedHandles;
 23114|             s_fUnscannedPromotions = FALSE;
 23115|             s_fUnpromotedHandles = FALSE;
 23116|             if (!s_fScanRequired)
 23117|             {
 23118|                 if (!initial_scan_p)
 23119|                 {
 23120|                     uint8_t* all_heaps_max = 0;
 23121|                     uint8_t* all_heaps_min = MAX_PTR;
 23122|                     int i;
 23123|                     for (i = 0; i < n_heaps; i++)
 23124|                     {
 23125|                         if (all_heaps_max < g_heaps[i]->max_overflow_address)
 23126|                             all_heaps_max = g_heaps[i]->max_overflow_address;
 23127|                         if (all_heaps_min > g_heaps[i]->min_overflow_address)
 23128|                             all_heaps_min = g_heaps[i]->min_overflow_address;
 23129|                     }
 23130|                     for (i = 0; i < n_heaps; i++)
 23131|                     {
 23132|                         g_heaps[i]->max_overflow_address = all_heaps_max;
 23133|                         g_heaps[i]->min_overflow_address = all_heaps_min;
 23134|                     }
 23135|                 }
 23136|             }
 23137|             dprintf(3, ("Starting all gc thread mark stack overflow processing"));
 23138|             gc_t_join.restart();
 23139|         }
 23140|         if (process_mark_overflow(condemned_gen_number))
 23141|             s_fUnscannedPromotions = TRUE;
 23142|         if (!s_fScanRequired)
 23143|             break;
 23144|         gc_t_join.join(this, gc_join_rescan_dependent_handles);
 23145|         if (gc_t_join.joined())
 23146|         {
 23147|             dprintf(3, ("Starting all gc thread for dependent handle promotion"));
 23148|             gc_t_join.restart();
 23149|         }
 23150|         if (GCScan::GcDhUnpromotedHandlesExist(sc))
 23151|             if (GCScan::GcDhReScan(sc))
 23152|                 s_fUnscannedPromotions = TRUE;
 23153|     }
 23154| }
 23155| #else //MULTIPLE_HEAPS
 23156| void gc_heap::scan_dependent_handles (int condemned_gen_number, ScanContext *sc, BOOL initial_scan_p)
 23157| {
 23158|     UNREFERENCED_PARAMETER(initial_scan_p);
 23159|     bool fUnscannedPromotions = true;
 23160|     while (GCScan::GcDhUnpromotedHandlesExist(sc) && fUnscannedPromotions)
 23161|     {
 23162|         fUnscannedPromotions = false;
 23163|         if (process_mark_overflow(condemned_gen_number))
 23164|             fUnscannedPromotions = true;
 23165|         mark_queue.verify_empty();
 23166|         if (GCScan::GcDhReScan(sc))
 23167|             fUnscannedPromotions = true;
 23168|     }
 23169|     process_mark_overflow(condemned_gen_number);
 23170| }
 23171| #endif //MULTIPLE_HEAPS
 23172| size_t gc_heap::get_generation_start_size (int gen_number)
 23173| {
 23174| #ifdef USE_REGIONS
 23175|     return 0;
 23176| #else
 23177|     return Align (size (generation_allocation_start (generation_of (gen_number))),
 23178|                   get_alignment_constant (gen_number <= max_generation));
 23179| #endif //!USE_REGIONS
 23180| }
 23181| inline
 23182| int gc_heap::get_num_heaps()
 23183| {
 23184| #ifdef MULTIPLE_HEAPS
 23185|     return n_heaps;
 23186| #else
 23187|     return 1;
 23188| #endif //MULTIPLE_HEAPS
 23189| }
 23190| BOOL gc_heap::decide_on_promotion_surv (size_t threshold)
 23191| {
 23192| #ifdef MULTIPLE_HEAPS
 23193|     for (int i = 0; i < gc_heap::n_heaps; i++)
 23194|     {
 23195|         gc_heap* hp = gc_heap::g_heaps[i];
 23196| #else //MULTIPLE_HEAPS
 23197|     {
 23198|         gc_heap* hp = pGenGCHeap;
 23199| #endif //MULTIPLE_HEAPS
 23200|         dynamic_data* dd = hp->dynamic_data_of (min ((settings.condemned_generation + 1), max_generation));
 23201|         size_t older_gen_size = dd_current_size (dd) + (dd_desired_allocation (dd) - dd_new_allocation (dd));
 23202|         size_t promoted = hp->total_promoted_bytes;
 23203|         dprintf (2, ("promotion threshold: %zd, promoted bytes: %zd size n+1: %zd",
 23204|             threshold, promoted, older_gen_size));
 23205|         if ((threshold > (older_gen_size)) || (promoted > threshold))
 23206|         {
 23207|             return TRUE;
 23208|         }
 23209|     }
 23210|     return FALSE;
 23211| }
 23212| inline
 23213| void gc_heap::fire_mark_event (int root_type, size_t& current_promoted_bytes, size_t& last_promoted_bytes)
 23214| {
 23215| #ifdef FEATURE_EVENT_TRACE
 23216|     if (informational_event_enabled_p)
 23217|     {
 23218|         current_promoted_bytes = get_promoted_bytes();
 23219|         size_t root_promoted = current_promoted_bytes - last_promoted_bytes;
 23220|         dprintf (3, ("h%d marked root %s: %zd (%zd - %zd)",
 23221|             heap_number, str_root_kinds[root_type], root_promoted,
 23222|             current_promoted_bytes, last_promoted_bytes));
 23223|         FIRE_EVENT(GCMarkWithType, heap_number, root_type, root_promoted);
 23224|         last_promoted_bytes = current_promoted_bytes;
 23225|     }
 23226| #endif // FEATURE_EVENT_TRACE
 23227| }
 23228| #ifdef FEATURE_EVENT_TRACE
 23229| inline
 23230| void gc_heap::record_mark_time (uint64_t& mark_time,
 23231|                                 uint64_t& current_mark_time,
 23232|                                 uint64_t& last_mark_time)
 23233| {
 23234|     if (informational_event_enabled_p)
 23235|     {
 23236|         current_mark_time = GetHighPrecisionTimeStamp();
 23237|         mark_time = limit_time_to_uint32 (current_mark_time - last_mark_time);
 23238|         dprintf (3, ("%zd - %zd = %zd",
 23239|             current_mark_time, last_mark_time, (current_mark_time - last_mark_time)));
 23240|         last_mark_time = current_mark_time;
 23241|     }
 23242| }
 23243| #endif // FEATURE_EVENT_TRACE
 23244| #ifdef USE_REGIONS
 23245| void gc_heap::verify_region_to_generation_map()
 23246| {
 23247| #ifdef _DEBUG
 23248|     uint8_t* local_ephemeral_low = MAX_PTR;
 23249|     uint8_t* local_ephemeral_high = nullptr;
 23250|     for (int gen_number = soh_gen0; gen_number < total_generation_count; gen_number++)
 23251|     {
 23252| #ifdef MULTIPLE_HEAPS
 23253|         for (int i = 0; i < n_heaps; i++)
 23254|         {
 23255|             gc_heap* hp = g_heaps[i];
 23256| #else //MULTIPLE_HEAPS
 23257|         {
 23258|             gc_heap* hp = pGenGCHeap;
 23259| #endif //MULTIPLE_HEAPS
 23260|             generation *gen = hp->generation_of (gen_number);
 23261|             for (heap_segment *region = generation_start_segment (gen); region != nullptr; region = heap_segment_next (region))
 23262|             {
 23263|                 if (heap_segment_read_only_p (region))
 23264|                 {
 23265|                     continue;
 23266|                 }
 23267|                 size_t region_index_start = get_basic_region_index_for_address (get_region_start (region));
 23268|                 size_t region_index_end = get_basic_region_index_for_address (heap_segment_reserved (region));
 23269|                 int gen_num = min (gen_number, soh_gen2);
 23270|                 assert (gen_num == heap_segment_gen_num (region));
 23271|                 int plan_gen_num = heap_segment_plan_gen_num (region);
 23272|                 bool is_demoted = (region->flags & heap_segment_flags_demoted) != 0;
 23273|                 bool is_sweep_in_plan = heap_segment_swept_in_plan (region);
 23274|                 for (size_t region_index = region_index_start; region_index < region_index_end; region_index++)
 23275|                 {
 23276|                     region_info region_info_bits = map_region_to_generation[region_index];
 23277|                     assert ((region_info_bits & RI_GEN_MASK) == gen_num);
 23278|                     assert ((region_info_bits >> RI_PLAN_GEN_SHR) == plan_gen_num);
 23279|                     assert (((region_info_bits & RI_SIP) != 0) == is_sweep_in_plan);
 23280|                     assert (((region_info_bits & RI_DEMOTED) != 0) == is_demoted);
 23281|                 }
 23282|             }
 23283|         }
 23284|     }
 23285| #endif //_DEBUG
 23286| }
 23287| void gc_heap::compute_gc_and_ephemeral_range (int condemned_gen_number, bool end_of_gc_p)
 23288| {
 23289|     ephemeral_low = MAX_PTR;
 23290|     ephemeral_high = nullptr;
 23291|     gc_low = MAX_PTR;
 23292|     gc_high = nullptr;
 23293|     if (condemned_gen_number >= soh_gen2 || end_of_gc_p)
 23294|     {
 23295|         gc_low = g_gc_lowest_address;
 23296|         gc_high = g_gc_highest_address;
 23297|     }
 23298|     if (end_of_gc_p)
 23299|     {
 23300| #if 1
 23301|         ephemeral_low = g_gc_lowest_address;
 23302| #else
 23303|         uint8_t* addr = g_gc_lowest_address;
 23304|         while (true)
 23305|         {
 23306|             heap_segment* region = get_region_info (addr);
 23307|             if (is_free_region (region))
 23308|                 break;
 23309|             if (heap_segment_gen_num (region) <= soh_gen1)
 23310|                 break;
 23311|             addr += ((size_t)1) << min_segment_size_shr;
 23312|         }
 23313|         ephemeral_low = addr;
 23314| #endif
 23315|         ephemeral_high = g_gc_highest_address;
 23316|     }
 23317|     else
 23318|     {
 23319|         for (int gen_number = soh_gen0; gen_number <= soh_gen1; gen_number++)
 23320|         {
 23321| #ifdef MULTIPLE_HEAPS
 23322|             for (int i = 0; i < n_heaps; i++)
 23323|             {
 23324|                 gc_heap* hp = g_heaps[i];
 23325| #else //MULTIPLE_HEAPS
 23326|             {
 23327|                 gc_heap* hp = pGenGCHeap;
 23328| #endif //MULTIPLE_HEAPS
 23329|                 generation *gen = hp->generation_of (gen_number);
 23330|                 for (heap_segment *region = generation_start_segment (gen); region != nullptr; region = heap_segment_next (region))
 23331|                 {
 23332|                     ephemeral_low = min ((uint8_t*)ephemeral_low, get_region_start (region));
 23333|                     ephemeral_high = max ((uint8_t*)ephemeral_high, heap_segment_reserved (region));
 23334|                     if (gen_number <= condemned_gen_number)
 23335|                     {
 23336|                         gc_low = min (gc_low, get_region_start (region));
 23337|                         gc_high = max (gc_high, heap_segment_reserved (region));
 23338|                     }
 23339|                 }
 23340|             }
 23341|         }
 23342|     }
 23343|     dprintf (2, ("ephemeral_low = %p, ephemeral_high = %p, gc_low = %p, gc_high = %p", (uint8_t*)ephemeral_low, (uint8_t*)ephemeral_high, gc_low, gc_high));
 23344| }
 23345| #endif //USE_REGIONS
 23346| void gc_heap::mark_phase (int condemned_gen_number, BOOL mark_only_p)
 23347| {
 23348|     assert (settings.concurrent == FALSE);
 23349|     ScanContext sc;
 23350|     sc.thread_number = heap_number;
 23351|     sc.thread_count = n_heaps;
 23352|     sc.promotion = TRUE;
 23353|     sc.concurrent = FALSE;
 23354|     dprintf (2, (ThreadStressLog::gcStartMarkMsg(), heap_number, condemned_gen_number));
 23355|     BOOL  full_p = (condemned_gen_number == max_generation);
 23356|     int gen_to_init = condemned_gen_number;
 23357|     if (condemned_gen_number == max_generation)
 23358|     {
 23359|         gen_to_init = total_generation_count - 1;
 23360|     }
 23361|     for (int gen_idx = 0; gen_idx <= gen_to_init; gen_idx++)
 23362|     {
 23363|         dynamic_data* dd = dynamic_data_of (gen_idx);
 23364|         dd_begin_data_size (dd) = generation_size (gen_idx) -
 23365|                                    dd_fragmentation (dd) -
 23366| #ifdef USE_REGIONS
 23367|                                    0;
 23368| #else
 23369|                                    get_generation_start_size (gen_idx);
 23370| #endif //USE_REGIONS
 23371|         dprintf (2, ("begin data size for gen%d is %zd", gen_idx, dd_begin_data_size (dd)));
 23372|         dd_survived_size (dd) = 0;
 23373|         dd_pinned_survived_size (dd) = 0;
 23374|         dd_artificial_pinned_survived_size (dd) = 0;
 23375|         dd_added_pinned_size (dd) = 0;
 23376| #ifdef SHORT_PLUGS
 23377|         dd_padding_size (dd) = 0;
 23378| #endif //SHORT_PLUGS
 23379| #if defined (RESPECT_LARGE_ALIGNMENT) || defined (FEATURE_STRUCTALIGN)
 23380|         dd_num_npinned_plugs (dd) = 0;
 23381| #endif //RESPECT_LARGE_ALIGNMENT || FEATURE_STRUCTALIGN
 23382|     }
 23383|     if (gen0_must_clear_bricks > 0)
 23384|         gen0_must_clear_bricks--;
 23385|     size_t last_promoted_bytes = 0;
 23386|     size_t current_promoted_bytes = 0;
 23387| #if !defined(USE_REGIONS) || defined(_DEBUG)
 23388|     init_promoted_bytes();
 23389| #endif //!USE_REGIONS || _DEBUG
 23390|     reset_mark_stack();
 23391| #ifdef SNOOP_STATS
 23392|     memset (&snoop_stat, 0, sizeof(snoop_stat));
 23393|     snoop_stat.heap_index = heap_number;
 23394| #endif //SNOOP_STATS
 23395| #ifdef MH_SC_MARK
 23396|     if (full_p)
 23397|     {
 23398|         for (int i = 0; i < max_snoop_level; i++)
 23399|         {
 23400|             ((uint8_t**)(mark_stack_array))[i] = 0;
 23401|         }
 23402|         mark_stack_busy() = 1;
 23403|     }
 23404| #endif //MH_SC_MARK
 23405|     static uint32_t num_sizedrefs = 0;
 23406| #ifdef MH_SC_MARK
 23407|     static BOOL do_mark_steal_p = FALSE;
 23408| #endif //MH_SC_MARK
 23409| #ifdef FEATURE_CARD_MARKING_STEALING
 23410|     reset_card_marking_enumerators();
 23411| #endif // FEATURE_CARD_MARKING_STEALING
 23412| #ifdef STRESS_REGIONS
 23413|     heap_segment* gen0_region = generation_start_segment (generation_of (0));
 23414|     while (gen0_region)
 23415|     {
 23416|         size_t gen0_region_size = heap_segment_allocated (gen0_region) - heap_segment_mem (gen0_region);
 23417|         if (gen0_region_size > 0)
 23418|         {
 23419|             if ((num_gen0_regions % pinning_seg_interval) == 0)
 23420|             {
 23421|                 dprintf (REGIONS_LOG, ("h%d potentially creating pinning in region %zx",
 23422|                     heap_number, heap_segment_mem (gen0_region)));
 23423|                 int align_const = get_alignment_constant (TRUE);
 23424|                 uint8_t* boundary = heap_segment_mem (gen0_region);
 23425|                 uint8_t* obj_to_pin = boundary;
 23426|                 int num_pinned_objs = 0;
 23427|                 while (obj_to_pin < heap_segment_allocated (gen0_region))
 23428|                 {
 23429|                     if (obj_to_pin >= boundary && !((CObjectHeader*)obj_to_pin)->IsFree())
 23430|                     {
 23431|                         pin_by_gc (obj_to_pin);
 23432|                         num_pinned_objs++;
 23433|                         if (num_pinned_objs >= 2)
 23434|                             break;
 23435|                         boundary += (gen0_region_size / 2) + 1;
 23436|                     }
 23437|                     obj_to_pin += Align (size (obj_to_pin), align_const);
 23438|                 }
 23439|             }
 23440|         }
 23441|         num_gen0_regions++;
 23442|         gen0_region = heap_segment_next (gen0_region);
 23443|     }
 23444| #endif //STRESS_REGIONS
 23445| #ifdef FEATURE_EVENT_TRACE
 23446|     static uint64_t current_mark_time = 0;
 23447|     static uint64_t last_mark_time = 0;
 23448| #endif //FEATURE_EVENT_TRACE
 23449| #ifdef USE_REGIONS
 23450|     special_sweep_p = false;
 23451| #endif //USE_REGIONS
 23452| #ifdef MULTIPLE_HEAPS
 23453|     gc_t_join.join(this, gc_join_begin_mark_phase);
 23454|     if (gc_t_join.joined())
 23455| #endif //MULTIPLE_HEAPS
 23456|     {
 23457|         maxgen_size_inc_p = false;
 23458| #ifdef USE_REGIONS
 23459|         region_count = global_region_allocator.get_used_region_count();
 23460|         grow_mark_list_piece();
 23461|         verify_region_to_generation_map();
 23462|         compute_gc_and_ephemeral_range (condemned_gen_number, false);
 23463| #endif //USE_REGIONS
 23464|         GCToEEInterface::BeforeGcScanRoots(condemned_gen_number, /* is_bgc */ false, /* is_concurrent */ false);
 23465|         num_sizedrefs = GCToEEInterface::GetTotalNumSizedRefHandles();
 23466| #ifdef FEATURE_EVENT_TRACE
 23467|         informational_event_enabled_p = EVENT_ENABLED (GCMarkWithType);
 23468|         if (informational_event_enabled_p)
 23469|         {
 23470|             last_mark_time = GetHighPrecisionTimeStamp();
 23471|             gc_time_info[time_mark_sizedref] = 0;
 23472|         }
 23473| #endif //FEATURE_EVENT_TRACE
 23474| #ifdef MULTIPLE_HEAPS
 23475| #ifdef MH_SC_MARK
 23476|         if (full_p)
 23477|         {
 23478|             size_t total_heap_size = get_total_heap_size();
 23479|             if (total_heap_size > (100 * 1024 * 1024))
 23480|             {
 23481|                 do_mark_steal_p = TRUE;
 23482|             }
 23483|             else
 23484|             {
 23485|                 do_mark_steal_p = FALSE;
 23486|             }
 23487|         }
 23488|         else
 23489|         {
 23490|             do_mark_steal_p = FALSE;
 23491|         }
 23492| #endif //MH_SC_MARK
 23493|         gc_t_join.restart();
 23494| #endif //MULTIPLE_HEAPS
 23495|     }
 23496|     {
 23497|         assert (g_mark_list);
 23498| #ifdef MULTIPLE_HEAPS
 23499|         mark_list_size = g_mark_list_total_size / n_heaps;
 23500|         mark_list = &g_mark_list [heap_number*mark_list_size];
 23501| #else
 23502|         mark_list = g_mark_list;
 23503| #endif //MULTIPLE_HEAPS
 23504|         if (condemned_gen_number < max_generation)
 23505|             mark_list_end = &mark_list [mark_list_size-1];
 23506|         else
 23507|             mark_list_end = &mark_list [0];
 23508|         mark_list_index = &mark_list [0];
 23509| #ifdef USE_REGIONS
 23510|         if (g_mark_list_piece != nullptr)
 23511|         {
 23512| #ifdef MULTIPLE_HEAPS
 23513|             mark_list_piece_start = &g_mark_list_piece[heap_number * 2 * g_mark_list_piece_size];
 23514|             mark_list_piece_end = &mark_list_piece_start[g_mark_list_piece_size];
 23515| #endif //MULTIPLE_HEAPS
 23516|             survived_per_region = (size_t*)&g_mark_list_piece[heap_number * 2 * g_mark_list_piece_size];
 23517|             old_card_survived_per_region = (size_t*)&survived_per_region[g_mark_list_piece_size];
 23518|             size_t region_info_to_clear = region_count * sizeof (size_t);
 23519|             memset (survived_per_region, 0, region_info_to_clear);
 23520|             memset (old_card_survived_per_region, 0, region_info_to_clear);
 23521|         }
 23522|         else
 23523|         {
 23524| #ifdef MULTIPLE_HEAPS
 23525|             mark_list_piece_start = nullptr;
 23526|             mark_list_piece_end = nullptr;
 23527|             mark_list_end = &mark_list[0];
 23528| #endif //MULTIPLE_HEAPS
 23529|             survived_per_region = nullptr;
 23530|             old_card_survived_per_region = nullptr;
 23531|         }
 23532| #endif // USE_REGIONS && MULTIPLE_HEAPS
 23533| #ifndef MULTIPLE_HEAPS
 23534|         shigh = (uint8_t*) 0;
 23535|         slow  = MAX_PTR;
 23536| #endif //MULTIPLE_HEAPS
 23537|         if ((condemned_gen_number == max_generation) && (num_sizedrefs > 0))
 23538|         {
 23539|             GCScan::GcScanSizedRefs(GCHeap::Promote, condemned_gen_number, max_generation, &sc);
 23540|             drain_mark_queue();
 23541|             fire_mark_event (ETW::GC_ROOT_SIZEDREF, current_promoted_bytes, last_promoted_bytes);
 23542| #ifdef MULTIPLE_HEAPS
 23543|             gc_t_join.join(this, gc_join_scan_sizedref_done);
 23544|             if (gc_t_join.joined())
 23545| #endif //MULTIPLE_HEAPS
 23546|             {
 23547| #ifdef FEATURE_EVENT_TRACE
 23548|                 record_mark_time (gc_time_info[time_mark_sizedref], current_mark_time, last_mark_time);
 23549| #endif //FEATURE_EVENT_TRACE
 23550| #ifdef MULTIPLE_HEAPS
 23551|                 dprintf(3, ("Done with marking all sized refs. Starting all gc thread for marking other strong roots"));
 23552|                 gc_t_join.restart();
 23553| #endif //MULTIPLE_HEAPS
 23554|             }
 23555|         }
 23556| #ifdef FEATURE_BASICFREEZE
 23557| #ifdef USE_REGIONS
 23558|         assert (!ro_segments_in_range);
 23559| #else //USE_REGIONS
 23560|         if (ro_segments_in_range)
 23561|         {
 23562|             dprintf(3,("Marking in range ro segments"));
 23563|             mark_ro_segments();
 23564|         }
 23565| #endif //USE_REGIONS
 23566| #endif //FEATURE_BASICFREEZE
 23567|         dprintf(3,("Marking Roots"));
 23568|         GCScan::GcScanRoots(GCHeap::Promote,
 23569|                                 condemned_gen_number, max_generation,
 23570|                                 &sc);
 23571|         drain_mark_queue();
 23572|         fire_mark_event (ETW::GC_ROOT_STACK, current_promoted_bytes, last_promoted_bytes);
 23573| #ifdef BACKGROUND_GC
 23574|         if (gc_heap::background_running_p())
 23575|         {
 23576|             scan_background_roots (GCHeap::Promote, heap_number, &sc);
 23577|             drain_mark_queue();
 23578|             fire_mark_event (ETW::GC_ROOT_BGC, current_promoted_bytes, last_promoted_bytes);
 23579|         }
 23580| #endif //BACKGROUND_GC
 23581| #ifdef FEATURE_PREMORTEM_FINALIZATION
 23582|         dprintf(3, ("Marking finalization data"));
 23583|         finalize_queue->GcScanRoots(GCHeap::Promote, heap_number, 0);
 23584|         drain_mark_queue();
 23585|         fire_mark_event (ETW::GC_ROOT_FQ, current_promoted_bytes, last_promoted_bytes);
 23586| #endif // FEATURE_PREMORTEM_FINALIZATION
 23587|         dprintf(3,("Marking handle table"));
 23588|         GCScan::GcScanHandles(GCHeap::Promote,
 23589|                                     condemned_gen_number, max_generation,
 23590|                                     &sc);
 23591|         drain_mark_queue();
 23592|         fire_mark_event (ETW::GC_ROOT_HANDLES, current_promoted_bytes, last_promoted_bytes);
 23593|         if (!full_p)
 23594|         {
 23595| #ifdef USE_REGIONS
 23596|             save_current_survived();
 23597| #endif //USE_REGIONS
 23598| #ifdef FEATURE_CARD_MARKING_STEALING
 23599|             n_eph_soh = 0;
 23600|             n_gen_soh = 0;
 23601|             n_eph_loh = 0;
 23602|             n_gen_loh = 0;
 23603| #endif //FEATURE_CARD_MARKING_STEALING
 23604| #ifdef CARD_BUNDLE
 23605| #ifdef MULTIPLE_HEAPS
 23606|             if (gc_t_join.r_join(this, gc_r_join_update_card_bundle))
 23607|             {
 23608| #endif //MULTIPLE_HEAPS
 23609| #ifndef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
 23610|                 update_card_table_bundle();
 23611| #endif
 23612|                 if (card_bundles_enabled())
 23613|                 {
 23614|                     verify_card_bundles();
 23615|                 }
 23616| #ifdef MULTIPLE_HEAPS
 23617|                 gc_t_join.r_restart();
 23618|             }
 23619| #endif //MULTIPLE_HEAPS
 23620| #endif //CARD_BUNDLE
 23621|             card_fn mark_object_fn = &gc_heap::mark_object_simple;
 23622| #ifdef HEAP_ANALYZE
 23623|             heap_analyze_success = TRUE;
 23624|             if (heap_analyze_enabled)
 23625|             {
 23626|                 internal_root_array_index = 0;
 23627|                 current_obj = 0;
 23628|                 current_obj_size = 0;
 23629|                 mark_object_fn = &gc_heap::ha_mark_object_simple;
 23630|             }
 23631| #endif //HEAP_ANALYZE
 23632| #if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
 23633|             if (!card_mark_done_soh)
 23634| #endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
 23635|             {
 23636|                 dprintf (3, ("Marking cross generation pointers on heap %d", heap_number));
 23637|                 mark_through_cards_for_segments(mark_object_fn, FALSE THIS_ARG);
 23638| #if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
 23639|                 card_mark_done_soh = true;
 23640| #endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
 23641|             }
 23642| #if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
 23643|             if (!card_mark_done_uoh)
 23644| #endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
 23645|             {
 23646|                 dprintf (3, ("Marking cross generation pointers for uoh objects on heap %d", heap_number));
 23647|                 for (int i = uoh_start_generation; i < total_generation_count; i++)
 23648|                 {
 23649| #ifndef ALLOW_REFERENCES_IN_POH
 23650|                     if (i != poh_generation)
 23651| #endif //ALLOW_REFERENCES_IN_POH
 23652|                         mark_through_cards_for_uoh_objects(mark_object_fn, i, FALSE THIS_ARG);
 23653|                 }
 23654| #if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
 23655|                 card_mark_done_uoh = true;
 23656| #endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
 23657|             }
 23658| #if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
 23659|             for (int i = 0; i < gc_heap::n_heaps; i++)
 23660|             {
 23661|                 int heap_number_to_look_at = (i + heap_number) % gc_heap::n_heaps;
 23662|                 gc_heap* hp = gc_heap::g_heaps[heap_number_to_look_at];
 23663|                 if (!hp->card_mark_done_soh)
 23664|                 {
 23665|                     dprintf(3, ("Marking cross generation pointers on heap %d", hp->heap_number));
 23666|                     hp->mark_through_cards_for_segments(mark_object_fn, FALSE THIS_ARG);
 23667|                     hp->card_mark_done_soh = true;
 23668|                 }
 23669|                 if (!hp->card_mark_done_uoh)
 23670|                 {
 23671|                     dprintf(3, ("Marking cross generation pointers for large objects on heap %d", hp->heap_number));
 23672|                     for (int i = uoh_start_generation; i < total_generation_count; i++)
 23673|                     {
 23674| #ifndef ALLOW_REFERENCES_IN_POH
 23675|                         if (i != poh_generation)
 23676| #endif //ALLOW_REFERENCES_IN_POH
 23677|                             hp->mark_through_cards_for_uoh_objects(mark_object_fn, i, FALSE THIS_ARG);
 23678|                     }
 23679|                     hp->card_mark_done_uoh = true;
 23680|                 }
 23681|             }
 23682| #endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
 23683| #ifdef USE_REGIONS
 23684|             update_old_card_survived();
 23685| #endif //USE_REGIONS
 23686|             drain_mark_queue();
 23687|             fire_mark_event (ETW::GC_ROOT_OLDER, current_promoted_bytes, last_promoted_bytes);
 23688|         }
 23689|     }
 23690| #ifdef MH_SC_MARK
 23691|     if (do_mark_steal_p)
 23692|     {
 23693|         mark_steal();
 23694|         drain_mark_queue();
 23695|         fire_mark_event (ETW::GC_ROOT_STEAL, current_promoted_bytes, last_promoted_bytes);
 23696|     }
 23697| #endif //MH_SC_MARK
 23698|     GCScan::GcDhInitialScan(GCHeap::Promote, condemned_gen_number, max_generation, &sc);
 23699|     scan_dependent_handles(condemned_gen_number, &sc, true);
 23700|     mark_queue.verify_empty();
 23701|     fire_mark_event (ETW::GC_ROOT_DH_HANDLES, current_promoted_bytes, last_promoted_bytes);
 23702| #ifdef MULTIPLE_HEAPS
 23703|     dprintf(3, ("Joining for short weak handle scan"));
 23704|     gc_t_join.join(this, gc_join_null_dead_short_weak);
 23705|     if (gc_t_join.joined())
 23706| #endif //MULTIPLE_HEAPS
 23707|     {
 23708| #ifdef FEATURE_EVENT_TRACE
 23709|         record_mark_time (gc_time_info[time_mark_roots], current_mark_time, last_mark_time);
 23710| #endif //FEATURE_EVENT_TRACE
 23711|         uint64_t promoted_bytes_global = 0;
 23712| #ifdef HEAP_ANALYZE
 23713|         heap_analyze_enabled = FALSE;
 23714| #ifdef MULTIPLE_HEAPS
 23715|         for (int i = 0; i < n_heaps; i++)
 23716|         {
 23717|             promoted_bytes_global += g_heaps[i]->get_promoted_bytes();
 23718|         }
 23719| #else
 23720|         promoted_bytes_global = get_promoted_bytes();
 23721| #endif //MULTIPLE_HEAPS
 23722|         GCToEEInterface::AnalyzeSurvivorsFinished (settings.gc_index, condemned_gen_number, promoted_bytes_global, GCHeap::ReportGenerationBounds);
 23723| #endif // HEAP_ANALYZE
 23724|         GCToEEInterface::AfterGcScanRoots (condemned_gen_number, max_generation, &sc);
 23725| #ifdef MULTIPLE_HEAPS
 23726|         if (!full_p)
 23727|         {
 23728|             gc_t_join.r_init();
 23729|         }
 23730|         dprintf(3, ("Starting all gc thread for short weak handle scan"));
 23731|         gc_t_join.restart();
 23732| #endif //MULTIPLE_HEAPS
 23733|     }
 23734| #ifdef FEATURE_CARD_MARKING_STEALING
 23735|     reset_card_marking_enumerators();
 23736|     if (!full_p)
 23737|     {
 23738|         int generation_skip_ratio_soh = ((n_eph_soh > MIN_SOH_CROSS_GEN_REFS) ?
 23739|                                          (int)(((float)n_gen_soh / (float)n_eph_soh) * 100) : 100);
 23740|         int generation_skip_ratio_loh = ((n_eph_loh > MIN_LOH_CROSS_GEN_REFS) ?
 23741|                                          (int)(((float)n_gen_loh / (float)n_eph_loh) * 100) : 100);
 23742|         generation_skip_ratio = min (generation_skip_ratio_soh, generation_skip_ratio_loh);
 23743|         dprintf (2, ("h%d skip ratio soh: %d, loh: %d", heap_number,
 23744|             generation_skip_ratio_soh, generation_skip_ratio_loh));
 23745|     }
 23746| #endif // FEATURE_CARD_MARKING_STEALING
 23747|     GCScan::GcShortWeakPtrScan (condemned_gen_number, max_generation,&sc);
 23748| #ifdef MULTIPLE_HEAPS
 23749|     dprintf(3, ("Joining for finalization"));
 23750|     gc_t_join.join(this, gc_join_scan_finalization);
 23751|     if (gc_t_join.joined())
 23752|     {
 23753| #endif //MULTIPLE_HEAPS
 23754| #ifdef FEATURE_EVENT_TRACE
 23755|         record_mark_time (gc_time_info[time_mark_short_weak], current_mark_time, last_mark_time);
 23756| #endif //FEATURE_EVENT_TRACE
 23757| #ifdef MULTIPLE_HEAPS
 23758|         dprintf(3, ("Starting all gc thread for Finalization"));
 23759|         gc_t_join.restart();
 23760|     }
 23761| #endif //MULTIPLE_HEAPS
 23762|     size_t promoted_bytes_live = get_promoted_bytes();
 23763| #ifdef FEATURE_PREMORTEM_FINALIZATION
 23764|     dprintf (3, ("Finalize marking"));
 23765|     finalize_queue->ScanForFinalization (GCHeap::Promote, condemned_gen_number, mark_only_p, __this);
 23766|     drain_mark_queue();
 23767|     fire_mark_event (ETW::GC_ROOT_NEW_FQ, current_promoted_bytes, last_promoted_bytes);
 23768|     GCToEEInterface::DiagWalkFReachableObjects(__this);
 23769|     scan_dependent_handles(condemned_gen_number, &sc, false);
 23770|     mark_queue.verify_empty();
 23771|     fire_mark_event (ETW::GC_ROOT_DH_HANDLES, current_promoted_bytes, last_promoted_bytes);
 23772| #endif //FEATURE_PREMORTEM_FINALIZATION
 23773|     total_promoted_bytes = get_promoted_bytes();
 23774| #ifdef MULTIPLE_HEAPS
 23775|     static VOLATILE(int32_t) syncblock_scan_p;
 23776|     dprintf(3, ("Joining for weak pointer deletion"));
 23777|     gc_t_join.join(this, gc_join_null_dead_long_weak);
 23778|     if (gc_t_join.joined())
 23779|     {
 23780|         dprintf(3, ("Starting all gc thread for weak pointer deletion"));
 23781| #endif //MULTIPLE_HEAPS
 23782| #ifdef FEATURE_EVENT_TRACE
 23783|         record_mark_time (gc_time_info[time_mark_scan_finalization], current_mark_time, last_mark_time);
 23784| #endif //FEATURE_EVENT_TRACE
 23785| #ifdef USE_REGIONS
 23786|         sync_promoted_bytes();
 23787|         equalize_promoted_bytes(settings.condemned_generation);
 23788| #endif //USE_REGIONS
 23789| #ifdef MULTIPLE_HEAPS
 23790|         syncblock_scan_p = 0;
 23791|         gc_t_join.restart();
 23792|     }
 23793| #endif //MULTIPLE_HEAPS
 23794|     GCScan::GcWeakPtrScan (condemned_gen_number, max_generation, &sc);
 23795| #ifdef MULTIPLE_HEAPS
 23796|     size_t total_mark_list_size = sort_mark_list();
 23797|     if ((syncblock_scan_p == 0) && (Interlocked::Increment(&syncblock_scan_p) == 1))
 23798| #endif //MULTIPLE_HEAPS
 23799|     {
 23800|         GCScan::GcWeakPtrScanBySingleThread(condemned_gen_number, max_generation, &sc);
 23801|     }
 23802| #ifdef MULTIPLE_HEAPS
 23803|     dprintf (3, ("Joining for sync block cache entry scanning"));
 23804|     gc_t_join.join(this, gc_join_null_dead_syncblk);
 23805|     if (gc_t_join.joined())
 23806| #endif //MULTIPLE_HEAPS
 23807|     {
 23808| #ifdef FEATURE_EVENT_TRACE
 23809|         record_mark_time (gc_time_info[time_plan - 1], current_mark_time, last_mark_time);
 23810|         gc_time_info[time_plan] = last_mark_time;
 23811| #endif //FEATURE_EVENT_TRACE
 23812|         if (!settings.promotion)
 23813|         {
 23814|             size_t m = 0;
 23815|             for (int n = 0; n <= condemned_gen_number;n++)
 23816|             {
 23817| #ifdef MULTIPLE_HEAPS
 23818|                 m +=  (size_t)(dd_min_size (dynamic_data_of (n))*(n+1)*0.1);
 23819| #else
 23820|                 m +=  (size_t)(dd_min_size (dynamic_data_of (n))*(n+1)*0.06);
 23821| #endif //MULTIPLE_HEAPS
 23822|             }
 23823|             settings.promotion = decide_on_promotion_surv (m);
 23824|         }
 23825| #ifdef MULTIPLE_HEAPS
 23826| #ifdef SNOOP_STATS
 23827|         if (do_mark_steal_p)
 23828|         {
 23829|             size_t objects_checked_count = 0;
 23830|             size_t zero_ref_count = 0;
 23831|             size_t objects_marked_count = 0;
 23832|             size_t check_level_count = 0;
 23833|             size_t busy_count = 0;
 23834|             size_t interlocked_count = 0;
 23835|             size_t partial_mark_parent_count = 0;
 23836|             size_t stolen_or_pm_count = 0;
 23837|             size_t stolen_entry_count = 0;
 23838|             size_t pm_not_ready_count = 0;
 23839|             size_t normal_count = 0;
 23840|             size_t stack_bottom_clear_count = 0;
 23841|             for (int i = 0; i < n_heaps; i++)
 23842|             {
 23843|                 gc_heap* hp = g_heaps[i];
 23844|                 hp->print_snoop_stat();
 23845|                 objects_checked_count += hp->snoop_stat.objects_checked_count;
 23846|                 zero_ref_count += hp->snoop_stat.zero_ref_count;
 23847|                 objects_marked_count += hp->snoop_stat.objects_marked_count;
 23848|                 check_level_count += hp->snoop_stat.check_level_count;
 23849|                 busy_count += hp->snoop_stat.busy_count;
 23850|                 interlocked_count += hp->snoop_stat.interlocked_count;
 23851|                 partial_mark_parent_count += hp->snoop_stat.partial_mark_parent_count;
 23852|                 stolen_or_pm_count += hp->snoop_stat.stolen_or_pm_count;
 23853|                 stolen_entry_count += hp->snoop_stat.stolen_entry_count;
 23854|                 pm_not_ready_count += hp->snoop_stat.pm_not_ready_count;
 23855|                 normal_count += hp->snoop_stat.normal_count;
 23856|                 stack_bottom_clear_count += hp->snoop_stat.stack_bottom_clear_count;
 23857|             }
 23858|             fflush (stdout);
 23859|             printf ("-------total stats-------\n");
 23860|             printf ("%8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s\n",
 23861|                 "checked", "zero", "marked", "level", "busy", "xchg", "pmparent", "s_pm", "stolen", "nready", "normal", "clear");
 23862|             printf ("%8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d\n",
 23863|                 objects_checked_count,
 23864|                 zero_ref_count,
 23865|                 objects_marked_count,
 23866|                 check_level_count,
 23867|                 busy_count,
 23868|                 interlocked_count,
 23869|                 partial_mark_parent_count,
 23870|                 stolen_or_pm_count,
 23871|                 stolen_entry_count,
 23872|                 pm_not_ready_count,
 23873|                 normal_count,
 23874|                 stack_bottom_clear_count);
 23875|         }
 23876| #endif //SNOOP_STATS
 23877|         dprintf(3, ("Starting all threads for end of mark phase"));
 23878|         gc_t_join.restart();
 23879| #endif //MULTIPLE_HEAPS
 23880|     }
 23881| #if defined(MULTIPLE_HEAPS) && !defined(USE_REGIONS)
 23882|     merge_mark_lists (total_mark_list_size);
 23883| #endif //MULTIPLE_HEAPS && !USE_REGIONS
 23884|     finalization_promoted_bytes = total_promoted_bytes - promoted_bytes_live;
 23885|     mark_queue.verify_empty();
 23886|     dprintf(2,("---- End of mark phase ----"));
 23887| }
 23888| inline
 23889| void gc_heap::pin_object (uint8_t* o, uint8_t** ppObject)
 23890| {
 23891|     dprintf (3, ("Pinning %zx->%zx", (size_t)ppObject, (size_t)o));
 23892|     set_pinned (o);
 23893| #ifdef FEATURE_EVENT_TRACE
 23894|     if(EVENT_ENABLED(PinObjectAtGCTime))
 23895|     {
 23896|         fire_etw_pin_object_event(o, ppObject);
 23897|     }
 23898| #endif // FEATURE_EVENT_TRACE
 23899|     num_pinned_objects++;
 23900| }
 23901| size_t gc_heap::get_total_pinned_objects()
 23902| {
 23903| #ifdef MULTIPLE_HEAPS
 23904|     size_t total_num_pinned_objects = 0;
 23905|     for (int i = 0; i < gc_heap::n_heaps; i++)
 23906|     {
 23907|         gc_heap* hp = gc_heap::g_heaps[i];
 23908|         total_num_pinned_objects += hp->num_pinned_objects;
 23909|     }
 23910|     return total_num_pinned_objects;
 23911| #else //MULTIPLE_HEAPS
 23912|     return num_pinned_objects;
 23913| #endif //MULTIPLE_HEAPS
 23914| }
 23915| void gc_heap::reinit_pinned_objects()
 23916| {
 23917| #ifdef MULTIPLE_HEAPS
 23918|     for (int i = 0; i < gc_heap::n_heaps; i++)
 23919|     {
 23920|         gc_heap::g_heaps[i]->num_pinned_objects = 0;
 23921|     }
 23922| #else //MULTIPLE_HEAPS
 23923|     num_pinned_objects = 0;
 23924| #endif //MULTIPLE_HEAPS
 23925| }
 23926| void gc_heap::reset_mark_stack ()
 23927| {
 23928|     reset_pinned_queue();
 23929|     max_overflow_address = 0;
 23930|     min_overflow_address = MAX_PTR;
 23931| }
 23932| #ifdef FEATURE_STRUCTALIGN
 23933| #if defined (TARGET_AMD64)
 23934| #define brick_bits (12)
 23935| #else
 23936| #define brick_bits (11)
 23937| #endif //TARGET_AMD64
 23938| C_ASSERT(brick_size == (1 << brick_bits));
 23939| #define child_bits (brick_bits + 1 - LOG2_PTRSIZE)
 23940| #define pad_bits (sizeof(short) * 8 - child_bits)
 23941| #define child_from_short(w) (((signed short)(w) / (1 << (pad_bits - LOG2_PTRSIZE))) & ~((1 << LOG2_PTRSIZE) - 1))
 23942| #define pad_mask ((1 << pad_bits) - 1)
 23943| #define pad_from_short(w) ((size_t)(w) & pad_mask)
 23944| #else // FEATURE_STRUCTALIGN
 23945| #define child_from_short(w) (w)
 23946| #endif // FEATURE_STRUCTALIGN
 23947| inline
 23948| short node_left_child(uint8_t* node)
 23949| {
 23950|     return child_from_short(((plug_and_pair*)node)[-1].m_pair.left);
 23951| }
 23952| inline
 23953| void set_node_left_child(uint8_t* node, ptrdiff_t val)
 23954| {
 23955|     assert (val > -(ptrdiff_t)brick_size);
 23956|     assert (val < (ptrdiff_t)brick_size);
 23957|     assert (Aligned (val));
 23958| #ifdef FEATURE_STRUCTALIGN
 23959|     size_t pad = pad_from_short(((plug_and_pair*)node)[-1].m_pair.left);
 23960|     ((plug_and_pair*)node)[-1].m_pair.left = ((short)val << (pad_bits - LOG2_PTRSIZE)) | (short)pad;
 23961| #else // FEATURE_STRUCTALIGN
 23962|     ((plug_and_pair*)node)[-1].m_pair.left = (short)val;
 23963| #endif // FEATURE_STRUCTALIGN
 23964|     assert (node_left_child (node) == val);
 23965| }
 23966| inline
 23967| short node_right_child(uint8_t* node)
 23968| {
 23969|     return child_from_short(((plug_and_pair*)node)[-1].m_pair.right);
 23970| }
 23971| inline
 23972| void set_node_right_child(uint8_t* node, ptrdiff_t val)
 23973| {
 23974|     assert (val > -(ptrdiff_t)brick_size);
 23975|     assert (val < (ptrdiff_t)brick_size);
 23976|     assert (Aligned (val));
 23977| #ifdef FEATURE_STRUCTALIGN
 23978|     size_t pad = pad_from_short(((plug_and_pair*)node)[-1].m_pair.right);
 23979|     ((plug_and_pair*)node)[-1].m_pair.right = ((short)val << (pad_bits - LOG2_PTRSIZE)) | (short)pad;
 23980| #else // FEATURE_STRUCTALIGN
 23981|     ((plug_and_pair*)node)[-1].m_pair.right = (short)val;
 23982| #endif // FEATURE_STRUCTALIGN
 23983|     assert (node_right_child (node) == val);
 23984| }
 23985| #ifdef FEATURE_STRUCTALIGN
 23986| void node_aligninfo (uint8_t* node, int& requiredAlignment, ptrdiff_t& pad)
 23987| {
 23988|     short left = ((plug_and_pair*)node)[-1].m_pair.left;
 23989|     short right = ((plug_and_pair*)node)[-1].m_pair.right;
 23990|     ptrdiff_t pad_shifted = (pad_from_short(left) << pad_bits) | pad_from_short(right);
 23991|     ptrdiff_t aligninfo = pad_shifted * DATA_ALIGNMENT;
 23992|     ptrdiff_t x = aligninfo;
 23993|     x |= x >> 8;
 23994|     x |= x >> 4;
 23995|     x |= x >> 2;
 23996|     x |= x >> 1;
 23997|     requiredAlignment = (int)(x ^ (x >> 1));
 23998|     pad = aligninfo - requiredAlignment;
 23999|     pad += AdjustmentForMinPadSize(pad, requiredAlignment);
 24000| }
 24001| inline
 24002| ptrdiff_t node_alignpad (uint8_t* node)
 24003| {
 24004|     int requiredAlignment;
 24005|     ptrdiff_t alignpad;
 24006|     node_aligninfo (node, requiredAlignment, alignpad);
 24007|     return alignpad;
 24008| }
 24009| void clear_node_aligninfo (uint8_t* node)
 24010| {
 24011|     ((plug_and_pair*)node)[-1].m_pair.left &= ~0 << pad_bits;
 24012|     ((plug_and_pair*)node)[-1].m_pair.right &= ~0 << pad_bits;
 24013| }
 24014| void set_node_aligninfo (uint8_t* node, int requiredAlignment, ptrdiff_t pad)
 24015| {
 24016|     ptrdiff_t aligninfo = (size_t)requiredAlignment + (pad & (requiredAlignment-1));
 24017|     assert (Aligned (aligninfo));
 24018|     ptrdiff_t aligninfo_shifted = aligninfo / DATA_ALIGNMENT;
 24019|     assert (aligninfo_shifted < (1 << (pad_bits + pad_bits)));
 24020|     ptrdiff_t hi = aligninfo_shifted >> pad_bits;
 24021|     assert (pad_from_short(((plug_and_gap*)node)[-1].m_pair.left) == 0);
 24022|     ((plug_and_pair*)node)[-1].m_pair.left |= hi;
 24023|     ptrdiff_t lo = aligninfo_shifted & pad_mask;
 24024|     assert (pad_from_short(((plug_and_gap*)node)[-1].m_pair.right) == 0);
 24025|     ((plug_and_pair*)node)[-1].m_pair.right |= lo;
 24026| #ifdef _DEBUG
 24027|     int requiredAlignment2;
 24028|     ptrdiff_t pad2;
 24029|     node_aligninfo (node, requiredAlignment2, pad2);
 24030|     assert (requiredAlignment == requiredAlignment2);
 24031|     assert (pad == pad2);
 24032| #endif // _DEBUG
 24033| }
 24034| #endif // FEATURE_STRUCTALIGN
 24035| inline
 24036| void loh_set_node_relocation_distance(uint8_t* node, ptrdiff_t val)
 24037| {
 24038|     ptrdiff_t* place = &(((loh_obj_and_pad*)node)[-1].reloc);
 24039|     *place = val;
 24040| }
 24041| inline
 24042| ptrdiff_t loh_node_relocation_distance(uint8_t* node)
 24043| {
 24044|     return (((loh_obj_and_pad*)node)[-1].reloc);
 24045| }
 24046| inline
 24047| ptrdiff_t node_relocation_distance (uint8_t* node)
 24048| {
 24049|     return (((plug_and_reloc*)(node))[-1].reloc & ~3);
 24050| }
 24051| inline
 24052| void set_node_relocation_distance(uint8_t* node, ptrdiff_t val)
 24053| {
 24054|     assert (val == (val & ~3));
 24055|     ptrdiff_t* place = &(((plug_and_reloc*)node)[-1].reloc);
 24056|     *place &= 1;
 24057|     *place |= val;
 24058| }
 24059| #define node_left_p(node) (((plug_and_reloc*)(node))[-1].reloc & 2)
 24060| #define set_node_left(node) ((plug_and_reloc*)(node))[-1].reloc |= 2;
 24061| #ifndef FEATURE_STRUCTALIGN
 24062| void set_node_realigned(uint8_t* node)
 24063| {
 24064|     ((plug_and_reloc*)(node))[-1].reloc |= 1;
 24065| }
 24066| void clear_node_realigned(uint8_t* node)
 24067| {
 24068| #ifdef RESPECT_LARGE_ALIGNMENT
 24069|     ((plug_and_reloc*)(node))[-1].reloc &= ~1;
 24070| #else
 24071|     UNREFERENCED_PARAMETER(node);
 24072| #endif //RESPECT_LARGE_ALIGNMENT
 24073| }
 24074| #endif // FEATURE_STRUCTALIGN
 24075| inline
 24076| size_t  node_gap_size (uint8_t* node)
 24077| {
 24078|     return ((plug_and_gap *)node)[-1].gap;
 24079| }
 24080| void set_gap_size (uint8_t* node, size_t size)
 24081| {
 24082|     assert (Aligned (size));
 24083|     ((plug_and_gap *)node)[-1].reloc = 0;
 24084|     ((plug_and_gap *)node)[-1].lr =0;
 24085|     ((plug_and_gap *)node)[-1].gap = size;
 24086|     assert ((size == 0 )||(size >= sizeof(plug_and_reloc)));
 24087| }
 24088| uint8_t* gc_heap::insert_node (uint8_t* new_node, size_t sequence_number,
 24089|                    uint8_t* tree, uint8_t* last_node)
 24090| {
 24091|     dprintf (3, ("IN: %zx(%zx), T: %zx(%zx), L: %zx(%zx) [%zx]",
 24092|                  (size_t)new_node, brick_of(new_node),
 24093|                  (size_t)tree, brick_of(tree),
 24094|                  (size_t)last_node, brick_of(last_node),
 24095|                  sequence_number));
 24096|     if (power_of_two_p (sequence_number))
 24097|     {
 24098|         set_node_left_child (new_node, (tree - new_node));
 24099|         dprintf (3, ("NT: %zx, LC->%zx", (size_t)new_node, (tree - new_node)));
 24100|         tree = new_node;
 24101|     }
 24102|     else
 24103|     {
 24104|         if (oddp (sequence_number))
 24105|         {
 24106|             set_node_right_child (last_node, (new_node - last_node));
 24107|             dprintf (3, ("%p RC->%zx", last_node, (new_node - last_node)));
 24108|         }
 24109|         else
 24110|         {
 24111|             uint8_t*  earlier_node = tree;
 24112|             size_t imax = logcount(sequence_number) - 2;
 24113|             for (size_t i = 0; i != imax; i++)
 24114|             {
 24115|                 earlier_node = earlier_node + node_right_child (earlier_node);
 24116|             }
 24117|             int tmp_offset = node_right_child (earlier_node);
 24118|             assert (tmp_offset); // should never be empty
 24119|             set_node_left_child (new_node, ((earlier_node + tmp_offset ) - new_node));
 24120|             set_node_right_child (earlier_node, (new_node - earlier_node));
 24121|             dprintf (3, ("%p LC->%zx, %p RC->%zx",
 24122|                 new_node, ((earlier_node + tmp_offset ) - new_node),
 24123|                 earlier_node, (new_node - earlier_node)));
 24124|         }
 24125|     }
 24126|     return tree;
 24127| }
 24128| size_t gc_heap::update_brick_table (uint8_t* tree, size_t current_brick,
 24129|                                     uint8_t* x, uint8_t* plug_end)
 24130| {
 24131|     dprintf (3, ("tree: %p, current b: %zx, x: %p, plug_end: %p",
 24132|         tree, current_brick, x, plug_end));
 24133|     if (tree != NULL)
 24134|     {
 24135|         dprintf (3, ("b- %zx->%zx pointing to tree %p",
 24136|             current_brick, (size_t)(tree - brick_address (current_brick)), tree));
 24137|         set_brick (current_brick, (tree - brick_address (current_brick)));
 24138|     }
 24139|     else
 24140|     {
 24141|         dprintf (3, ("b- %zx->-1", current_brick));
 24142|         set_brick (current_brick, -1);
 24143|     }
 24144|     size_t  b = 1 + current_brick;
 24145|     ptrdiff_t  offset = 0;
 24146|     size_t last_br = brick_of (plug_end-1);
 24147|     current_brick = brick_of (x-1);
 24148|     dprintf (3, ("ubt: %zx->%zx]->%zx]", b, last_br, current_brick));
 24149|     while (b <= current_brick)
 24150|     {
 24151|         if (b <= last_br)
 24152|         {
 24153|             set_brick (b, --offset);
 24154|         }
 24155|         else
 24156|         {
 24157|             set_brick (b,-1);
 24158|         }
 24159|         b++;
 24160|     }
 24161|     return brick_of (x);
 24162| }
 24163| #ifndef USE_REGIONS
 24164| void gc_heap::plan_generation_start (generation* gen, generation* consing_gen, uint8_t* next_plug_to_allocate)
 24165| {
 24166| #ifdef HOST_64BIT
 24167|     if (gen == youngest_generation)
 24168|     {
 24169|         heap_segment* seg = ephemeral_heap_segment;
 24170|         size_t mark_stack_large_bos = mark_stack_bos;
 24171|         size_t large_plug_pos = 0;
 24172|         while (mark_stack_large_bos < mark_stack_tos)
 24173|         {
 24174|             if (mark_stack_array[mark_stack_large_bos].len > demotion_plug_len_th)
 24175|             {
 24176|                 while (mark_stack_bos <= mark_stack_large_bos)
 24177|                 {
 24178|                     size_t entry = deque_pinned_plug();
 24179|                     size_t len = pinned_len (pinned_plug_of (entry));
 24180|                     uint8_t* plug = pinned_plug (pinned_plug_of(entry));
 24181|                     if (len > demotion_plug_len_th)
 24182|                     {
 24183|                         dprintf (2, ("ps(%d): S %p (%zd)(%p)", gen->gen_num, plug, len, (plug+len)));
 24184|                     }
 24185|                     pinned_len (pinned_plug_of (entry)) = plug - generation_allocation_pointer (consing_gen);
 24186|                     assert(mark_stack_array[entry].len == 0 ||
 24187|                             mark_stack_array[entry].len >= Align(min_obj_size));
 24188|                     generation_allocation_pointer (consing_gen) = plug + len;
 24189|                     generation_allocation_limit (consing_gen) = heap_segment_plan_allocated (seg);
 24190|                     set_allocator_next_pin (consing_gen);
 24191|                 }
 24192|             }
 24193|             mark_stack_large_bos++;
 24194|         }
 24195|     }
 24196| #endif // HOST_64BIT
 24197|     generation_plan_allocation_start (gen) =
 24198|         allocate_in_condemned_generations (consing_gen, Align (min_obj_size), -1);
 24199|     generation_plan_allocation_start_size (gen) = Align (min_obj_size);
 24200|     size_t allocation_left = (size_t)(generation_allocation_limit (consing_gen) - generation_allocation_pointer (consing_gen));
 24201|     if (next_plug_to_allocate)
 24202|     {
 24203|         size_t dist_to_next_plug = (size_t)(next_plug_to_allocate - generation_allocation_pointer (consing_gen));
 24204|         if (allocation_left > dist_to_next_plug)
 24205|         {
 24206|             allocation_left = dist_to_next_plug;
 24207|         }
 24208|     }
 24209|     if (allocation_left < Align (min_obj_size))
 24210|     {
 24211|         generation_plan_allocation_start_size (gen) += allocation_left;
 24212|         generation_allocation_pointer (consing_gen) += allocation_left;
 24213|     }
 24214|     dprintf (2, ("plan alloc gen%d(%p) start at %zx (ptr: %p, limit: %p, next: %p)", gen->gen_num,
 24215|         generation_plan_allocation_start (gen),
 24216|         generation_plan_allocation_start_size (gen),
 24217|         generation_allocation_pointer (consing_gen), generation_allocation_limit (consing_gen),
 24218|         next_plug_to_allocate));
 24219| }
 24220| void gc_heap::realloc_plan_generation_start (generation* gen, generation* consing_gen)
 24221| {
 24222|     BOOL adjacentp = FALSE;
 24223|     generation_plan_allocation_start (gen) =
 24224|         allocate_in_expanded_heap (consing_gen, Align(min_obj_size), adjacentp, 0,
 24225| #ifdef SHORT_PLUGS
 24226|                                    FALSE, NULL,
 24227| #endif //SHORT_PLUGS
 24228|                                    FALSE, -1 REQD_ALIGN_AND_OFFSET_ARG);
 24229|     generation_plan_allocation_start_size (gen) = Align (min_obj_size);
 24230|     size_t allocation_left = (size_t)(generation_allocation_limit (consing_gen) - generation_allocation_pointer (consing_gen));
 24231|     if ((allocation_left < Align (min_obj_size)) &&
 24232|          (generation_allocation_limit (consing_gen)!=heap_segment_plan_allocated (generation_allocation_segment (consing_gen))))
 24233|     {
 24234|         generation_plan_allocation_start_size (gen) += allocation_left;
 24235|         generation_allocation_pointer (consing_gen) += allocation_left;
 24236|     }
 24237|     dprintf (1, ("plan re-alloc gen%d start at %p (ptr: %p, limit: %p)", gen->gen_num,
 24238|         generation_plan_allocation_start (consing_gen),
 24239|         generation_allocation_pointer (consing_gen),
 24240|         generation_allocation_limit (consing_gen)));
 24241| }
 24242| void gc_heap::plan_generation_starts (generation*& consing_gen)
 24243| {
 24244|     int  gen_number = settings.condemned_generation;
 24245|     while (gen_number >= 0)
 24246|     {
 24247|         if (gen_number < max_generation)
 24248|         {
 24249|             consing_gen = ensure_ephemeral_heap_segment (consing_gen);
 24250|         }
 24251|         generation* gen = generation_of (gen_number);
 24252|         if (0 == generation_plan_allocation_start (gen))
 24253|         {
 24254|             plan_generation_start (gen, consing_gen, 0);
 24255|             assert (generation_plan_allocation_start (gen));
 24256|         }
 24257|         gen_number--;
 24258|     }
 24259|     heap_segment_plan_allocated (ephemeral_heap_segment) =
 24260|         generation_allocation_pointer (consing_gen);
 24261| }
 24262| void gc_heap::advance_pins_for_demotion (generation* gen)
 24263| {
 24264|     uint8_t* original_youngest_start = generation_allocation_start (youngest_generation);
 24265|     heap_segment* seg = ephemeral_heap_segment;
 24266|     if ((!(pinned_plug_que_empty_p())))
 24267|     {
 24268|         size_t gen1_pinned_promoted = generation_pinned_allocation_compact_size (generation_of (max_generation));
 24269|         size_t gen1_pins_left = dd_pinned_survived_size (dynamic_data_of (max_generation - 1)) - gen1_pinned_promoted;
 24270|         size_t total_space_to_skip = last_gen1_pin_end - generation_allocation_pointer (gen);
 24271|         float pin_frag_ratio = (float)gen1_pins_left / (float)total_space_to_skip;
 24272|         float pin_surv_ratio = (float)gen1_pins_left / (float)(dd_survived_size (dynamic_data_of (max_generation - 1)));
 24273|         if ((pin_frag_ratio > 0.15) && (pin_surv_ratio > 0.30))
 24274|         {
 24275|             while (!pinned_plug_que_empty_p() &&
 24276|                     (pinned_plug (oldest_pin()) < original_youngest_start))
 24277|             {
 24278|                 size_t entry = deque_pinned_plug();
 24279|                 size_t len = pinned_len (pinned_plug_of (entry));
 24280|                 uint8_t* plug = pinned_plug (pinned_plug_of(entry));
 24281|                 pinned_len (pinned_plug_of (entry)) = plug - generation_allocation_pointer (gen);
 24282|                 assert(mark_stack_array[entry].len == 0 ||
 24283|                         mark_stack_array[entry].len >= Align(min_obj_size));
 24284|                 generation_allocation_pointer (gen) = plug + len;
 24285|                 generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
 24286|                 set_allocator_next_pin (gen);
 24287|                 int frgn = object_gennum (plug);
 24288|                 if ((frgn != (int)max_generation) && settings.promotion)
 24289|                 {
 24290|                     int togn = object_gennum_plan (plug);
 24291|                     generation_pinned_allocation_sweep_size ((generation_of (frgn +1))) += len;
 24292|                     if (frgn < togn)
 24293|                     {
 24294|                         generation_pinned_allocation_compact_size (generation_of (togn)) += len;
 24295|                     }
 24296|                 }
 24297|                 dprintf (2, ("skipping gap %zu, pin %p (%zd)",
 24298|                     pinned_len (pinned_plug_of (entry)), plug, len));
 24299|             }
 24300|         }
 24301|         dprintf (2, ("ad_p_d: PL: %zd, SL: %zd, pfr: %d, psr: %d",
 24302|             gen1_pins_left, total_space_to_skip, (int)(pin_frag_ratio*100), (int)(pin_surv_ratio*100)));
 24303|     }
 24304| }
 24305| void gc_heap::process_ephemeral_boundaries (uint8_t* x,
 24306|                                             int& active_new_gen_number,
 24307|                                             int& active_old_gen_number,
 24308|                                             generation*& consing_gen,
 24309|                                             BOOL& allocate_in_condemned)
 24310| {
 24311| retry:
 24312|     if ((active_old_gen_number > 0) &&
 24313|         (x >= generation_allocation_start (generation_of (active_old_gen_number - 1))))
 24314|     {
 24315|         dprintf (2, ("crossing gen%d, x is %p", active_old_gen_number - 1, x));
 24316|         if (!pinned_plug_que_empty_p())
 24317|         {
 24318|             dprintf (2, ("oldest pin: %p(%zd)",
 24319|                 pinned_plug (oldest_pin()),
 24320|                 (x - pinned_plug (oldest_pin()))));
 24321|         }
 24322|         if (active_old_gen_number <= (settings.promotion ? (max_generation - 1) : max_generation))
 24323|         {
 24324|             active_new_gen_number--;
 24325|         }
 24326|         active_old_gen_number--;
 24327|         assert ((!settings.promotion) || (active_new_gen_number>0));
 24328|         if (active_new_gen_number == (max_generation - 1))
 24329|         {
 24330| #ifdef FREE_USAGE_STATS
 24331|             if (settings.condemned_generation == max_generation)
 24332|             {
 24333|                 generation* gen_2 = generation_of (max_generation);
 24334|                 generation* gen_1 = generation_of (max_generation - 1);
 24335|                 size_t total_num_pinned_free_spaces_left = 0;
 24336|                 for (int j = 0; j < NUM_GEN_POWER2; j++)
 24337|                 {
 24338|                     dprintf (1, ("[h%d][#%zd]2^%d: current: %zd, S: 2: %zd, 1: %zd(%zd)",
 24339|                         heap_number,
 24340|                         settings.gc_index,
 24341|                         (j + 10),
 24342|                         gen_2->gen_current_pinned_free_spaces[j],
 24343|                         gen_2->gen_plugs[j], gen_1->gen_plugs[j],
 24344|                         (gen_2->gen_plugs[j] + gen_1->gen_plugs[j])));
 24345|                     total_num_pinned_free_spaces_left += gen_2->gen_current_pinned_free_spaces[j];
 24346|                 }
 24347|                 float pinned_free_list_efficiency = 0;
 24348|                 size_t total_pinned_free_space = generation_allocated_in_pinned_free (gen_2) + generation_pinned_free_obj_space (gen_2);
 24349|                 if (total_pinned_free_space != 0)
 24350|                 {
 24351|                     pinned_free_list_efficiency = (float)(generation_allocated_in_pinned_free (gen_2)) / (float)total_pinned_free_space;
 24352|                 }
 24353|                 dprintf (1, ("[h%d] gen2 allocated %zd bytes with %zd bytes pinned free spaces (effi: %d%%), %zd (%zd) left",
 24354|                             heap_number,
 24355|                             generation_allocated_in_pinned_free (gen_2),
 24356|                             total_pinned_free_space,
 24357|                             (int)(pinned_free_list_efficiency * 100),
 24358|                             generation_pinned_free_obj_space (gen_2),
 24359|                             total_num_pinned_free_spaces_left));
 24360|             }
 24361| #endif //FREE_USAGE_STATS
 24362|             while (!pinned_plug_que_empty_p() &&
 24363|                    (!in_range_for_segment ((pinned_plug (oldest_pin())), ephemeral_heap_segment)))
 24364|             {
 24365|                 size_t  entry = deque_pinned_plug();
 24366|                 mark*  m = pinned_plug_of (entry);
 24367|                 uint8_t*  plug = pinned_plug (m);
 24368|                 size_t  len = pinned_len (m);
 24369|                 heap_segment* nseg = heap_segment_in_range (generation_allocation_segment (consing_gen));
 24370|                 PREFIX_ASSUME(nseg != NULL);
 24371|                 while (!((plug >= generation_allocation_pointer (consing_gen))&&
 24372|                         (plug < heap_segment_allocated (nseg))))
 24373|                 {
 24374|                     assert (generation_allocation_pointer (consing_gen)>=
 24375|                             heap_segment_mem (nseg));
 24376|                     assert (generation_allocation_pointer (consing_gen)<=
 24377|                             heap_segment_committed (nseg));
 24378|                     heap_segment_plan_allocated (nseg) =
 24379|                         generation_allocation_pointer (consing_gen);
 24380|                     nseg = heap_segment_next_rw (nseg);
 24381|                     generation_allocation_segment (consing_gen) = nseg;
 24382|                     generation_allocation_pointer (consing_gen) =
 24383|                         heap_segment_mem (nseg);
 24384|                 }
 24385|                 set_new_pin_info (m, generation_allocation_pointer (consing_gen));
 24386|                 assert(pinned_len(m) == 0 || pinned_len(m) >= Align(min_obj_size));
 24387|                 generation_allocation_pointer (consing_gen) = plug + len;
 24388|                 generation_allocation_limit (consing_gen) =
 24389|                     generation_allocation_pointer (consing_gen);
 24390|             }
 24391|             allocate_in_condemned = TRUE;
 24392|             consing_gen = ensure_ephemeral_heap_segment (consing_gen);
 24393|         }
 24394|         if (active_new_gen_number != max_generation)
 24395|         {
 24396|             if (active_new_gen_number == (max_generation - 1))
 24397|             {
 24398|                 maxgen_pinned_compact_before_advance = generation_pinned_allocation_compact_size (generation_of (max_generation));
 24399|                 if (!demote_gen1_p)
 24400|                     advance_pins_for_demotion (consing_gen);
 24401|             }
 24402|             plan_generation_start (generation_of (active_new_gen_number), consing_gen, x);
 24403|             dprintf (2, ("process eph: allocated gen%d start at %p",
 24404|                 active_new_gen_number,
 24405|                 generation_plan_allocation_start (generation_of (active_new_gen_number))));
 24406|             if ((demotion_low == MAX_PTR) && !pinned_plug_que_empty_p())
 24407|             {
 24408|                 uint8_t* pplug = pinned_plug (oldest_pin());
 24409|                 if (object_gennum (pplug) > 0)
 24410|                 {
 24411|                     demotion_low = pplug;
 24412|                     dprintf (3, ("process eph: dlow->%p", demotion_low));
 24413|                 }
 24414|             }
 24415|             assert (generation_plan_allocation_start (generation_of (active_new_gen_number)));
 24416|         }
 24417|         goto retry;
 24418|     }
 24419| }
 24420| #endif //!USE_REGIONS
 24421| #ifdef FEATURE_BASICFREEZE
 24422| inline
 24423| void gc_heap::seg_set_mark_bits (heap_segment* seg)
 24424| {
 24425|     uint8_t* o = heap_segment_mem (seg);
 24426|     while (o < heap_segment_allocated (seg))
 24427|     {
 24428|         set_marked (o);
 24429|         o = o + Align (size(o));
 24430|     }
 24431| }
 24432| inline
 24433| void gc_heap::seg_clear_mark_bits (heap_segment* seg)
 24434| {
 24435|     uint8_t* o = heap_segment_mem (seg);
 24436|     while (o < heap_segment_allocated (seg))
 24437|     {
 24438|         if (marked (o))
 24439|         {
 24440|             clear_marked (o);
 24441|         }
 24442|         o = o + Align (size (o));
 24443|     }
 24444| }
 24445| void gc_heap::mark_ro_segments()
 24446| {
 24447| #ifdef USE_REGIONS
 24448|     assert (!ro_segments_in_range);
 24449| #else //USE_REGIONS
 24450|     if ((settings.condemned_generation == max_generation) && ro_segments_in_range)
 24451|     {
 24452|         heap_segment* seg = generation_start_segment (generation_of (max_generation));
 24453|         while (seg)
 24454|         {
 24455|             if (!heap_segment_read_only_p (seg))
 24456|                 break;
 24457|             if (heap_segment_in_range_p (seg))
 24458|             {
 24459| #ifdef BACKGROUND_GC
 24460|                 if (settings.concurrent)
 24461|                 {
 24462|                     seg_set_mark_array_bits_soh (seg);
 24463|                 }
 24464|                 else
 24465| #endif //BACKGROUND_GC
 24466|                 {
 24467|                     seg_set_mark_bits (seg);
 24468|                 }
 24469|             }
 24470|             seg = heap_segment_next (seg);
 24471|         }
 24472|     }
 24473| #endif //USE_REGIONS
 24474| }
 24475| void gc_heap::sweep_ro_segments()
 24476| {
 24477| #ifdef USE_REGIONS
 24478|     assert (!ro_segments_in_range);
 24479| #else //USE_REGIONS
 24480|     if ((settings.condemned_generation == max_generation) && ro_segments_in_range)
 24481|     {
 24482|         heap_segment* seg = generation_start_segment (generation_of (max_generation));;
 24483|         while (seg)
 24484|         {
 24485|             if (!heap_segment_read_only_p (seg))
 24486|                 break;
 24487|             if (heap_segment_in_range_p (seg))
 24488|             {
 24489| #ifdef BACKGROUND_GC
 24490|                 if (settings.concurrent)
 24491|                 {
 24492|                     seg_clear_mark_array_bits_soh (seg);
 24493|                 }
 24494|                 else
 24495| #endif //BACKGROUND_GC
 24496|                 {
 24497|                     seg_clear_mark_bits (seg);
 24498|                 }
 24499|             }
 24500|             seg = heap_segment_next (seg);
 24501|         }
 24502|     }
 24503| #endif //USE_REGIONS
 24504| }
 24505| #endif // FEATURE_BASICFREEZE
 24506| #ifdef FEATURE_LOH_COMPACTION
 24507| inline
 24508| BOOL gc_heap::loh_pinned_plug_que_empty_p()
 24509| {
 24510|     return (loh_pinned_queue_bos == loh_pinned_queue_tos);
 24511| }
 24512| void gc_heap::loh_set_allocator_next_pin()
 24513| {
 24514|     if (!(loh_pinned_plug_que_empty_p()))
 24515|     {
 24516|         mark*  oldest_entry = loh_oldest_pin();
 24517|         uint8_t* plug = pinned_plug (oldest_entry);
 24518|         generation* gen = large_object_generation;
 24519|         if ((plug >= generation_allocation_pointer (gen)) &&
 24520|             (plug <  generation_allocation_limit (gen)))
 24521|         {
 24522|             generation_allocation_limit (gen) = pinned_plug (oldest_entry);
 24523|         }
 24524|         else
 24525|             assert (!((plug < generation_allocation_pointer (gen)) &&
 24526|                       (plug >= heap_segment_mem (generation_allocation_segment (gen)))));
 24527|     }
 24528| }
 24529| size_t gc_heap::loh_deque_pinned_plug ()
 24530| {
 24531|     size_t m = loh_pinned_queue_bos;
 24532|     loh_pinned_queue_bos++;
 24533|     return m;
 24534| }
 24535| inline
 24536| mark* gc_heap::loh_pinned_plug_of (size_t bos)
 24537| {
 24538|     return &loh_pinned_queue[bos];
 24539| }
 24540| inline
 24541| mark* gc_heap::loh_oldest_pin()
 24542| {
 24543|     return loh_pinned_plug_of (loh_pinned_queue_bos);
 24544| }
 24545| BOOL gc_heap::loh_enque_pinned_plug (uint8_t* plug, size_t len)
 24546| {
 24547|     assert(len >= Align(min_obj_size, get_alignment_constant (FALSE)));
 24548|     if (loh_pinned_queue_length <= loh_pinned_queue_tos)
 24549|     {
 24550|         if (!grow_mark_stack (loh_pinned_queue, loh_pinned_queue_length, LOH_PIN_QUEUE_LENGTH))
 24551|         {
 24552|             return FALSE;
 24553|         }
 24554|     }
 24555|     dprintf (3, (" P: %p(%zd)", plug, len));
 24556|     mark& m = loh_pinned_queue[loh_pinned_queue_tos];
 24557|     m.first = plug;
 24558|     m.len = len;
 24559|     loh_pinned_queue_tos++;
 24560|     loh_set_allocator_next_pin();
 24561|     return TRUE;
 24562| }
 24563| inline
 24564| BOOL gc_heap::loh_size_fit_p (size_t size, uint8_t* alloc_pointer, uint8_t* alloc_limit, bool end_p)
 24565| {
 24566|     dprintf (1235, ("trying to fit %zd(%zd) between %p and %p (%zd)",
 24567|         size,
 24568|         (2* AlignQword (loh_padding_obj_size) +  size),
 24569|         alloc_pointer,
 24570|         alloc_limit,
 24571|         (alloc_limit - alloc_pointer)));
 24572|     size_t pad = 1 + (end_p ? 0 : 1);
 24573|     pad *= AlignQword (loh_padding_obj_size);
 24574|     return ((alloc_pointer + pad + size) <= alloc_limit);
 24575| }
 24576| uint8_t* gc_heap::loh_allocate_in_condemned (size_t size)
 24577| {
 24578|     generation* gen = large_object_generation;
 24579|     dprintf (1235, ("E: p:%p, l:%p, s: %zd",
 24580|         generation_allocation_pointer (gen),
 24581|         generation_allocation_limit (gen),
 24582|         size));
 24583| retry:
 24584|     {
 24585|         heap_segment* seg = generation_allocation_segment (gen);
 24586|         if (!(loh_size_fit_p (size, generation_allocation_pointer (gen), generation_allocation_limit (gen),
 24587|                               (generation_allocation_limit (gen) == heap_segment_plan_allocated (seg)))))
 24588|         {
 24589|             if ((!(loh_pinned_plug_que_empty_p()) &&
 24590|                  (generation_allocation_limit (gen) ==
 24591|                   pinned_plug (loh_oldest_pin()))))
 24592|             {
 24593|                 mark* m = loh_pinned_plug_of (loh_deque_pinned_plug());
 24594|                 size_t len = pinned_len (m);
 24595|                 uint8_t* plug = pinned_plug (m);
 24596|                 dprintf (1235, ("AIC: %p->%p(%zd)", generation_allocation_pointer (gen), plug, plug - generation_allocation_pointer (gen)));
 24597|                 pinned_len (m) = plug - generation_allocation_pointer (gen);
 24598|                 generation_allocation_pointer (gen) = plug + len;
 24599|                 generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
 24600|                 loh_set_allocator_next_pin();
 24601|                 dprintf (1235, ("s: p: %p, l: %p (%zd)",
 24602|                     generation_allocation_pointer (gen),
 24603|                     generation_allocation_limit (gen),
 24604|                     (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
 24605|                 goto retry;
 24606|             }
 24607|             if (generation_allocation_limit (gen) != heap_segment_plan_allocated (seg))
 24608|             {
 24609|                 generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
 24610|                 dprintf (1235, ("l->pa(%p)", generation_allocation_limit (gen)));
 24611|             }
 24612|             else
 24613|             {
 24614|                 if (heap_segment_plan_allocated (seg) != heap_segment_committed (seg))
 24615|                 {
 24616|                     heap_segment_plan_allocated (seg) = heap_segment_committed (seg);
 24617|                     generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
 24618|                     dprintf (1235, ("l->c(%p)", generation_allocation_limit (gen)));
 24619|                 }
 24620|                 else
 24621|                 {
 24622|                     if (loh_size_fit_p (size, generation_allocation_pointer (gen), heap_segment_reserved (seg), true) &&
 24623|                         (grow_heap_segment (seg, (generation_allocation_pointer (gen) + size + 2* AlignQword (loh_padding_obj_size)))))
 24624|                     {
 24625|                         dprintf (1235, ("growing seg from %p to %p\n", heap_segment_committed (seg),
 24626|                                          (generation_allocation_pointer (gen) + size)));
 24627|                         heap_segment_plan_allocated (seg) = heap_segment_committed (seg);
 24628|                         generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
 24629|                         dprintf (1235, ("g: p: %p, l: %p (%zd)",
 24630|                             generation_allocation_pointer (gen),
 24631|                             generation_allocation_limit (gen),
 24632|                             (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
 24633|                     }
 24634|                     else
 24635|                     {
 24636|                         heap_segment* next_seg = heap_segment_next (seg);
 24637|                         assert (generation_allocation_pointer (gen)>=
 24638|                                 heap_segment_mem (seg));
 24639|                         if (!loh_pinned_plug_que_empty_p() &&
 24640|                             ((pinned_plug (loh_oldest_pin()) <
 24641|                               heap_segment_allocated (seg)) &&
 24642|                              (pinned_plug (loh_oldest_pin()) >=
 24643|                               generation_allocation_pointer (gen))))
 24644|                         {
 24645|                             LOG((LF_GC, LL_INFO10, "remaining pinned plug %zx while leaving segment on allocation",
 24646|                                          pinned_plug (loh_oldest_pin())));
 24647|                             dprintf (1, ("queue empty: %d", loh_pinned_plug_que_empty_p()));
 24648|                             FATAL_GC_ERROR();
 24649|                         }
 24650|                         assert (generation_allocation_pointer (gen)>=
 24651|                                 heap_segment_mem (seg));
 24652|                         assert (generation_allocation_pointer (gen)<=
 24653|                                 heap_segment_committed (seg));
 24654|                         heap_segment_plan_allocated (seg) = generation_allocation_pointer (gen);
 24655|                         if (next_seg)
 24656|                         {
 24657|                             generation_allocation_segment (gen) = next_seg;
 24658|                             generation_allocation_pointer (gen) = heap_segment_mem (next_seg);
 24659|                             generation_allocation_limit (gen) = generation_allocation_pointer (gen);
 24660|                             dprintf (1235, ("n: p: %p, l: %p (%zd)",
 24661|                                 generation_allocation_pointer (gen),
 24662|                                 generation_allocation_limit (gen),
 24663|                                 (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
 24664|                         }
 24665|                         else
 24666|                         {
 24667|                             dprintf (1, ("We ran out of space compacting, shouldn't happen"));
 24668|                             FATAL_GC_ERROR();
 24669|                         }
 24670|                     }
 24671|                 }
 24672|             }
 24673|             loh_set_allocator_next_pin();
 24674|             dprintf (1235, ("r: p: %p, l: %p (%zd)",
 24675|                 generation_allocation_pointer (gen),
 24676|                 generation_allocation_limit (gen),
 24677|                 (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
 24678|             goto retry;
 24679|         }
 24680|     }
 24681|     {
 24682|         assert (generation_allocation_pointer (gen)>=
 24683|                 heap_segment_mem (generation_allocation_segment (gen)));
 24684|         uint8_t* result = generation_allocation_pointer (gen);
 24685|         size_t loh_pad = AlignQword (loh_padding_obj_size);
 24686|         generation_allocation_pointer (gen) += size + loh_pad;
 24687|         assert (generation_allocation_pointer (gen) <= generation_allocation_limit (gen));
 24688|         dprintf (1235, ("p: %p, l: %p (%zd)",
 24689|             generation_allocation_pointer (gen),
 24690|             generation_allocation_limit (gen),
 24691|             (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
 24692|         assert (result + loh_pad);
 24693|         return result + loh_pad;
 24694|     }
 24695| }
 24696| BOOL gc_heap::loh_compaction_requested()
 24697| {
 24698|     return (loh_compaction_always_p || (loh_compaction_mode != loh_compaction_default));
 24699| }
 24700| inline
 24701| void gc_heap::check_loh_compact_mode (BOOL all_heaps_compacted_p)
 24702| {
 24703|     if (settings.loh_compaction && (loh_compaction_mode == loh_compaction_once))
 24704|     {
 24705|         if (all_heaps_compacted_p)
 24706|         {
 24707|             loh_compaction_mode = loh_compaction_default;
 24708|         }
 24709|     }
 24710| }
 24711| BOOL gc_heap::plan_loh()
 24712| {
 24713| #ifdef FEATURE_EVENT_TRACE
 24714|     uint64_t start_time = 0, end_time;
 24715|     if (informational_event_enabled_p)
 24716|     {
 24717|         memset (loh_compact_info, 0, (sizeof (etw_loh_compact_info) * get_num_heaps()));
 24718|         start_time = GetHighPrecisionTimeStamp();
 24719|     }
 24720| #endif //FEATURE_EVENT_TRACE
 24721|     if (!loh_pinned_queue)
 24722|     {
 24723|         loh_pinned_queue = new (nothrow) (mark [LOH_PIN_QUEUE_LENGTH]);
 24724|         if (!loh_pinned_queue)
 24725|         {
 24726|             dprintf (1, ("Cannot allocate the LOH pinned queue (%zd bytes), no compaction",
 24727|                          LOH_PIN_QUEUE_LENGTH * sizeof (mark)));
 24728|             return FALSE;
 24729|         }
 24730|         loh_pinned_queue_length = LOH_PIN_QUEUE_LENGTH;
 24731|     }
 24732|     loh_pinned_queue_decay = LOH_PIN_DECAY;
 24733|     loh_pinned_queue_tos = 0;
 24734|     loh_pinned_queue_bos = 0;
 24735|     generation* gen        = large_object_generation;
 24736|     heap_segment* start_seg = heap_segment_rw (generation_start_segment (gen));
 24737|     PREFIX_ASSUME(start_seg != NULL);
 24738|     heap_segment* seg      = start_seg;
 24739|     uint8_t* o             = get_uoh_start_object (seg, gen);
 24740|     dprintf (1235, ("before GC LOH size: %zd, free list: %zd, free obj: %zd\n",
 24741|         generation_size (loh_generation),
 24742|         generation_free_list_space (gen),
 24743|         generation_free_obj_space (gen)));
 24744|     while (seg)
 24745|     {
 24746|         heap_segment_plan_allocated (seg) = heap_segment_mem (seg);
 24747|         seg = heap_segment_next (seg);
 24748|     }
 24749|     seg = start_seg;
 24750|     heap_segment_plan_allocated (seg) = o;
 24751|     generation_allocation_pointer (gen) = o;
 24752|     generation_allocation_limit (gen) = generation_allocation_pointer (gen);
 24753|     generation_allocation_segment (gen) = start_seg;
 24754|     uint8_t* free_space_start = o;
 24755|     uint8_t* free_space_end = o;
 24756|     uint8_t* new_address = 0;
 24757|     while (1)
 24758|     {
 24759|         if (o >= heap_segment_allocated (seg))
 24760|         {
 24761|             seg = heap_segment_next (seg);
 24762|             if (seg == 0)
 24763|             {
 24764|                 break;
 24765|             }
 24766|             o = heap_segment_mem (seg);
 24767|         }
 24768|         if (marked (o))
 24769|         {
 24770|             free_space_end = o;
 24771|             size_t size = AlignQword (size (o));
 24772|             dprintf (1235, ("%p(%zd) M", o, size));
 24773|             if (pinned (o))
 24774|             {
 24775|                 if (!loh_enque_pinned_plug (o, size))
 24776|                 {
 24777|                     return FALSE;
 24778|                 }
 24779|                 new_address = o;
 24780|             }
 24781|             else
 24782|             {
 24783|                 new_address = loh_allocate_in_condemned (size);
 24784|             }
 24785|             loh_set_node_relocation_distance (o, (new_address - o));
 24786|             dprintf (1235, ("lobj %p-%p -> %p-%p (%zd)", o, (o + size), new_address, (new_address + size), (new_address - o)));
 24787|             o = o + size;
 24788|             free_space_start = o;
 24789|             if (o < heap_segment_allocated (seg))
 24790|             {
 24791|                 assert (!marked (o));
 24792|             }
 24793|         }
 24794|         else
 24795|         {
 24796|             while (o < heap_segment_allocated (seg) && !marked (o))
 24797|             {
 24798|                 dprintf (1235, ("%p(%zd) F (%d)", o, AlignQword (size (o)), ((method_table (o) == g_gc_pFreeObjectMethodTable) ? 1 : 0)));
 24799|                 o = o + AlignQword (size (o));
 24800|             }
 24801|         }
 24802|     }
 24803|     while (!loh_pinned_plug_que_empty_p())
 24804|     {
 24805|         mark* m = loh_pinned_plug_of (loh_deque_pinned_plug());
 24806|         size_t len = pinned_len (m);
 24807|         uint8_t* plug = pinned_plug (m);
 24808|         heap_segment* nseg = heap_segment_rw (generation_allocation_segment (gen));
 24809|         while ((plug < generation_allocation_pointer (gen)) ||
 24810|                (plug >= heap_segment_allocated (nseg)))
 24811|         {
 24812|             assert ((plug < heap_segment_mem (nseg)) ||
 24813|                     (plug > heap_segment_reserved (nseg)));
 24814|             assert (generation_allocation_pointer (gen)>=
 24815|                     heap_segment_mem (nseg));
 24816|             assert (generation_allocation_pointer (gen)<=
 24817|                     heap_segment_committed (nseg));
 24818|             heap_segment_plan_allocated (nseg) =
 24819|                 generation_allocation_pointer (gen);
 24820|             nseg = heap_segment_next_rw (nseg);
 24821|             generation_allocation_segment (gen) = nseg;
 24822|             generation_allocation_pointer (gen) =
 24823|                 heap_segment_mem (nseg);
 24824|         }
 24825|         dprintf (1235, ("SP: %p->%p(%zd)", generation_allocation_pointer (gen), plug, plug - generation_allocation_pointer (gen)));
 24826|         pinned_len (m) = plug - generation_allocation_pointer (gen);
 24827|         generation_allocation_pointer (gen) = plug + len;
 24828|     }
 24829|     heap_segment_plan_allocated (generation_allocation_segment (gen)) = generation_allocation_pointer (gen);
 24830|     generation_allocation_pointer (gen) = 0;
 24831|     generation_allocation_limit (gen) = 0;
 24832| #ifdef FEATURE_EVENT_TRACE
 24833|     if (informational_event_enabled_p)
 24834|     {
 24835|         end_time = GetHighPrecisionTimeStamp();
 24836|         loh_compact_info[heap_number].time_plan = limit_time_to_uint32 (end_time - start_time);
 24837|     }
 24838| #endif //FEATURE_EVENT_TRACE
 24839|     return TRUE;
 24840| }
 24841| void gc_heap::compact_loh()
 24842| {
 24843|     assert (loh_compaction_requested() || heap_hard_limit || conserve_mem_setting);
 24844| #ifdef FEATURE_EVENT_TRACE
 24845|     uint64_t start_time = 0, end_time;
 24846|     if (informational_event_enabled_p)
 24847|     {
 24848|         start_time = GetHighPrecisionTimeStamp();
 24849|     }
 24850| #endif //FEATURE_EVENT_TRACE
 24851|     generation* gen        = large_object_generation;
 24852|     heap_segment* start_seg = heap_segment_rw (generation_start_segment (gen));
 24853|     PREFIX_ASSUME(start_seg != NULL);
 24854|     heap_segment* seg      = start_seg;
 24855|     heap_segment* prev_seg = 0;
 24856|     uint8_t* o             = get_uoh_start_object (seg, gen);
 24857|     uint8_t* free_space_start = o;
 24858|     uint8_t* free_space_end = o;
 24859|     generation_allocator (gen)->clear();
 24860|     generation_free_list_space (gen) = 0;
 24861|     generation_free_obj_space (gen) = 0;
 24862|     loh_pinned_queue_bos = 0;
 24863|     while (1)
 24864|     {
 24865|         if (o >= heap_segment_allocated (seg))
 24866|         {
 24867|             heap_segment* next_seg = heap_segment_next (seg);
 24868|             if ((heap_segment_plan_allocated (seg) == heap_segment_mem (seg)) &&
 24869|                 (seg != start_seg) && !heap_segment_read_only_p (seg))
 24870|             {
 24871|                 dprintf (3, ("Preparing empty large segment %zx", (size_t)seg));
 24872|                 assert (prev_seg);
 24873|                 heap_segment_next (prev_seg) = next_seg;
 24874|                 heap_segment_next (seg) = freeable_uoh_segment;
 24875|                 freeable_uoh_segment = seg;
 24876| #ifdef USE_REGIONS
 24877|                 update_start_tail_regions (gen, seg, prev_seg, next_seg);
 24878| #endif //USE_REGIONS
 24879|             }
 24880|             else
 24881|             {
 24882|                 if (!heap_segment_read_only_p (seg))
 24883|                 {
 24884|                     if (heap_segment_plan_allocated (seg) > heap_segment_allocated (seg))
 24885|                     {
 24886|                         if ((heap_segment_plan_allocated (seg) - plug_skew)  > heap_segment_used (seg))
 24887|                         {
 24888|                             heap_segment_used (seg) = heap_segment_plan_allocated (seg) - plug_skew;
 24889|                         }
 24890|                     }
 24891|                     heap_segment_allocated (seg) = heap_segment_plan_allocated (seg);
 24892|                     dprintf (3, ("Trimming seg to %p[", heap_segment_allocated (seg)));
 24893|                     decommit_heap_segment_pages (seg, 0);
 24894|                     dprintf (1236, ("CLOH: seg: %p, alloc: %p, used: %p, committed: %p",
 24895|                         seg,
 24896|                         heap_segment_allocated (seg),
 24897|                         heap_segment_used (seg),
 24898|                         heap_segment_committed (seg)));
 24899|                     dprintf (1236, ("CLOH: used is set to %p", heap_segment_used (seg)));
 24900|                 }
 24901|                 prev_seg = seg;
 24902|             }
 24903|             seg = next_seg;
 24904|             if (seg == 0)
 24905|                 break;
 24906|             else
 24907|             {
 24908|                 o = heap_segment_mem (seg);
 24909|             }
 24910|         }
 24911|         if (marked (o))
 24912|         {
 24913|             free_space_end = o;
 24914|             size_t size = AlignQword (size (o));
 24915|             size_t loh_pad;
 24916|             uint8_t* reloc = o;
 24917|             clear_marked (o);
 24918|             if (pinned (o))
 24919|             {
 24920|                 mark* m = loh_pinned_plug_of (loh_deque_pinned_plug());
 24921|                 uint8_t* plug = pinned_plug (m);
 24922|                 assert (plug == o);
 24923|                 loh_pad = pinned_len (m);
 24924|                 clear_pinned (o);
 24925|             }
 24926|             else
 24927|             {
 24928|                 loh_pad = AlignQword (loh_padding_obj_size);
 24929|                 reloc += loh_node_relocation_distance (o);
 24930|                 gcmemcopy (reloc, o, size, TRUE);
 24931|             }
 24932|             thread_gap ((reloc - loh_pad), loh_pad, gen);
 24933|             o = o + size;
 24934|             free_space_start = o;
 24935|             if (o < heap_segment_allocated (seg))
 24936|             {
 24937|                 assert (!marked (o));
 24938|             }
 24939|         }
 24940|         else
 24941|         {
 24942|             while (o < heap_segment_allocated (seg) && !marked (o))
 24943|             {
 24944|                 o = o + AlignQword (size (o));
 24945|             }
 24946|         }
 24947|     }
 24948| #ifdef FEATURE_EVENT_TRACE
 24949|     if (informational_event_enabled_p)
 24950|     {
 24951|         end_time = GetHighPrecisionTimeStamp();
 24952|         loh_compact_info[heap_number].time_compact = limit_time_to_uint32 (end_time - start_time);
 24953|     }
 24954| #endif //FEATURE_EVENT_TRACE
 24955|     assert (loh_pinned_plug_que_empty_p());
 24956|     dprintf (1235, ("after GC LOH size: %zd, free list: %zd, free obj: %zd\n\n",
 24957|         generation_size (loh_generation),
 24958|         generation_free_list_space (gen),
 24959|         generation_free_obj_space (gen)));
 24960| }
 24961| #ifdef FEATURE_EVENT_TRACE
 24962| inline
 24963| void gc_heap::loh_reloc_survivor_helper (uint8_t** pval, size_t& total_refs, size_t& zero_refs)
 24964| {
 24965|     uint8_t* val = *pval;
 24966|     if (!val)
 24967|         zero_refs++;
 24968|     total_refs++;
 24969|     reloc_survivor_helper (pval);
 24970| }
 24971| #endif //FEATURE_EVENT_TRACE
 24972| void gc_heap::relocate_in_loh_compact()
 24973| {
 24974|     generation* gen        = large_object_generation;
 24975|     heap_segment* seg      = heap_segment_rw (generation_start_segment (gen));
 24976|     uint8_t* o              = get_uoh_start_object (seg, gen);
 24977| #ifdef FEATURE_EVENT_TRACE
 24978|     size_t total_refs = 0;
 24979|     size_t zero_refs = 0;
 24980|     uint64_t start_time = 0, end_time;
 24981|     if (informational_event_enabled_p)
 24982|     {
 24983|         start_time = GetHighPrecisionTimeStamp();
 24984|     }
 24985| #endif //FEATURE_EVENT_TRACE
 24986|     while (1)
 24987|     {
 24988|         if (o >= heap_segment_allocated (seg))
 24989|         {
 24990|             seg = heap_segment_next (seg);
 24991|             if (seg == 0)
 24992|             {
 24993|                 break;
 24994|             }
 24995|             o = heap_segment_mem (seg);
 24996|         }
 24997|         if (marked (o))
 24998|         {
 24999|             size_t size = AlignQword (size (o));
 25000|             check_class_object_demotion (o);
 25001|             if (contain_pointers (o))
 25002|             {
 25003| #ifdef FEATURE_EVENT_TRACE
 25004|                 if (informational_event_enabled_p)
 25005|                 {
 25006|                     go_through_object_nostart (method_table (o), o, size(o), pval,
 25007|                     {
 25008|                         loh_reloc_survivor_helper (pval, total_refs, zero_refs);
 25009|                     });
 25010|                 }
 25011|                 else
 25012| #endif //FEATURE_EVENT_TRACE
 25013|                 {
 25014|                     go_through_object_nostart (method_table (o), o, size(o), pval,
 25015|                     {
 25016|                         reloc_survivor_helper (pval);
 25017|                     });
 25018|                 }
 25019|             }
 25020|             o = o + size;
 25021|             if (o < heap_segment_allocated (seg))
 25022|             {
 25023|                 assert (!marked (o));
 25024|             }
 25025|         }
 25026|         else
 25027|         {
 25028|             while (o < heap_segment_allocated (seg) && !marked (o))
 25029|             {
 25030|                 o = o + AlignQword (size (o));
 25031|             }
 25032|         }
 25033|     }
 25034| #ifdef FEATURE_EVENT_TRACE
 25035|     if (informational_event_enabled_p)
 25036|     {
 25037|         end_time = GetHighPrecisionTimeStamp();
 25038|         loh_compact_info[heap_number].time_relocate = limit_time_to_uint32 (end_time - start_time);
 25039|         loh_compact_info[heap_number].total_refs = total_refs;
 25040|         loh_compact_info[heap_number].zero_refs = zero_refs;
 25041|     }
 25042| #endif //FEATURE_EVENT_TRACE
 25043|     dprintf (1235, ("after GC LOH size: %zd, free list: %zd, free obj: %zd\n\n",
 25044|         generation_size (loh_generation),
 25045|         generation_free_list_space (gen),
 25046|         generation_free_obj_space (gen)));
 25047| }
 25048| void gc_heap::walk_relocation_for_loh (void* profiling_context, record_surv_fn fn)
 25049| {
 25050|     generation* gen        = large_object_generation;
 25051|     heap_segment* seg      = heap_segment_rw (generation_start_segment (gen));
 25052|     uint8_t* o             = get_uoh_start_object (seg, gen);
 25053|     while (1)
 25054|     {
 25055|         if (o >= heap_segment_allocated (seg))
 25056|         {
 25057|             seg = heap_segment_next (seg);
 25058|             if (seg == 0)
 25059|             {
 25060|                 break;
 25061|             }
 25062|             o = heap_segment_mem (seg);
 25063|         }
 25064|         if (marked (o))
 25065|         {
 25066|             size_t size = AlignQword (size (o));
 25067|             ptrdiff_t reloc = loh_node_relocation_distance (o);
 25068|             STRESS_LOG_PLUG_MOVE(o, (o + size), -reloc);
 25069|             fn (o, (o + size), reloc, profiling_context, !!settings.compaction, false);
 25070|             o = o + size;
 25071|             if (o < heap_segment_allocated (seg))
 25072|             {
 25073|                 assert (!marked (o));
 25074|             }
 25075|         }
 25076|         else
 25077|         {
 25078|             while (o < heap_segment_allocated (seg) && !marked (o))
 25079|             {
 25080|                 o = o + AlignQword (size (o));
 25081|             }
 25082|         }
 25083|     }
 25084| }
 25085| BOOL gc_heap::loh_object_p (uint8_t* o)
 25086| {
 25087| #ifdef MULTIPLE_HEAPS
 25088|     gc_heap* hp = gc_heap::g_heaps [0];
 25089|     int brick_entry = hp->brick_table[hp->brick_of (o)];
 25090| #else //MULTIPLE_HEAPS
 25091|     int brick_entry = brick_table[brick_of (o)];
 25092| #endif //MULTIPLE_HEAPS
 25093|     return (brick_entry == 0);
 25094| }
 25095| #endif //FEATURE_LOH_COMPACTION
 25096| void gc_heap::convert_to_pinned_plug (BOOL& last_npinned_plug_p,
 25097|                                       BOOL& last_pinned_plug_p,
 25098|                                       BOOL& pinned_plug_p,
 25099|                                       size_t ps,
 25100|                                       size_t& artificial_pinned_size)
 25101| {
 25102|     last_npinned_plug_p = FALSE;
 25103|     last_pinned_plug_p = TRUE;
 25104|     pinned_plug_p = TRUE;
 25105|     artificial_pinned_size = ps;
 25106| }
 25107| void gc_heap::store_plug_gap_info (uint8_t* plug_start,
 25108|                                    uint8_t* plug_end,
 25109|                                    BOOL& last_npinned_plug_p,
 25110|                                    BOOL& last_pinned_plug_p,
 25111|                                    uint8_t*& last_pinned_plug,
 25112|                                    BOOL& pinned_plug_p,
 25113|                                    uint8_t* last_object_in_last_plug,
 25114|                                    BOOL& merge_with_last_pin_p,
 25115|                                    size_t last_plug_len)
 25116| {
 25117|     UNREFERENCED_PARAMETER(last_plug_len);
 25118|     if (!last_npinned_plug_p && !last_pinned_plug_p)
 25119|     {
 25120|         dprintf (3, ("Free: %zx", (plug_start - plug_end)));
 25121|         assert ((plug_start == plug_end) || ((size_t)(plug_start - plug_end) >= Align (min_obj_size)));
 25122|         set_gap_size (plug_start, plug_start - plug_end);
 25123|     }
 25124|     if (pinned (plug_start))
 25125|     {
 25126|         BOOL save_pre_plug_info_p = FALSE;
 25127|         if (last_npinned_plug_p || last_pinned_plug_p)
 25128|         {
 25129|             save_pre_plug_info_p = TRUE;
 25130|         }
 25131|         pinned_plug_p = TRUE;
 25132|         last_npinned_plug_p = FALSE;
 25133|         if (last_pinned_plug_p)
 25134|         {
 25135|             dprintf (3, ("last plug %p was also pinned, should merge", last_pinned_plug));
 25136|             merge_with_last_pin_p = TRUE;
 25137|         }
 25138|         else
 25139|         {
 25140|             last_pinned_plug_p = TRUE;
 25141|             last_pinned_plug = plug_start;
 25142|             enque_pinned_plug (last_pinned_plug, save_pre_plug_info_p, last_object_in_last_plug);
 25143|             if (save_pre_plug_info_p)
 25144|             {
 25145| #ifdef DOUBLY_LINKED_FL
 25146|                 if (last_object_in_last_plug == generation_last_free_list_allocated(generation_of(max_generation)))
 25147|                 {
 25148|                     saved_pinned_plug_index = mark_stack_tos;
 25149|                 }
 25150| #endif //DOUBLY_LINKED_FL
 25151|                 set_gap_size (plug_start, sizeof (gap_reloc_pair));
 25152|             }
 25153|         }
 25154|     }
 25155|     else
 25156|     {
 25157|         if (last_pinned_plug_p)
 25158|         {
 25159|             save_post_plug_info (last_pinned_plug, last_object_in_last_plug, plug_start);
 25160|             set_gap_size (plug_start, sizeof (gap_reloc_pair));
 25161|             verify_pins_with_post_plug_info("after saving post plug info");
 25162|         }
 25163|         last_npinned_plug_p = TRUE;
 25164|         last_pinned_plug_p = FALSE;
 25165|     }
 25166| }
 25167| void gc_heap::record_interesting_data_point (interesting_data_point idp)
 25168| {
 25169| #ifdef GC_CONFIG_DRIVEN
 25170|     (interesting_data_per_gc[idp])++;
 25171| #else
 25172|     UNREFERENCED_PARAMETER(idp);
 25173| #endif //GC_CONFIG_DRIVEN
 25174| }
 25175| #ifdef USE_REGIONS
 25176| void gc_heap::skip_pins_in_alloc_region (generation* consing_gen, int plan_gen_num)
 25177| {
 25178|     heap_segment* alloc_region = generation_allocation_segment (consing_gen);
 25179|     while (!pinned_plug_que_empty_p())
 25180|     {
 25181|         uint8_t* oldest_plug = pinned_plug (oldest_pin());
 25182|         if ((oldest_plug >= generation_allocation_pointer (consing_gen)) &&
 25183|             (oldest_plug < heap_segment_allocated (alloc_region)))
 25184|         {
 25185|             mark* m =       pinned_plug_of (deque_pinned_plug());
 25186|             uint8_t* plug = pinned_plug (m);
 25187|             size_t len =    pinned_len (m);
 25188|             set_new_pin_info (m, generation_allocation_pointer (consing_gen));
 25189|             dprintf (REGIONS_LOG, ("pin %p b: %zx->%zx", plug, brick_of (plug),
 25190|                 (size_t)(brick_table[brick_of (plug)])));
 25191|             generation_allocation_pointer (consing_gen) = plug + len;
 25192|         }
 25193|         else
 25194|         {
 25195|             break;
 25196|         }
 25197|     }
 25198|     dprintf (REGIONS_LOG, ("finished with alloc region %p, (%s) plan gen -> %d",
 25199|         heap_segment_mem (alloc_region),
 25200|         (heap_segment_swept_in_plan (alloc_region) ? "SIP" : "non SIP"),
 25201|         (heap_segment_swept_in_plan (alloc_region) ?
 25202|             heap_segment_plan_gen_num (alloc_region) : plan_gen_num)));
 25203|     set_region_plan_gen_num_sip (alloc_region, plan_gen_num);
 25204|     heap_segment_plan_allocated (alloc_region) = generation_allocation_pointer (consing_gen);
 25205| }
 25206| void gc_heap::decide_on_demotion_pin_surv (heap_segment* region, int* no_pinned_surv_region_count)
 25207| {
 25208|     int new_gen_num = 0;
 25209|     int pinned_surv = heap_segment_pinned_survived (region);
 25210|     if (pinned_surv == 0)
 25211|     {
 25212|         (*no_pinned_surv_region_count)++;
 25213|         dprintf (REGIONS_LOG, ("region %Ix will be empty", heap_segment_mem (region)));
 25214|     }
 25215|     size_t basic_region_size = (size_t)1 << min_segment_size_shr;
 25216|     int pinned_ratio = (int)(((double)pinned_surv * 100.0) / (double)basic_region_size);
 25217|     dprintf (REGIONS_LOG, ("h%d g%d region %Ix(%Ix) ps: %d (%d) (%s)", heap_number,
 25218|         heap_segment_gen_num (region), (size_t)region, heap_segment_mem (region), pinned_surv, pinned_ratio,
 25219|         ((pinned_ratio >= demotion_pinned_ratio_th) ? "ND" : "D")));
 25220|     if (pinned_ratio >= demotion_pinned_ratio_th)
 25221|     {
 25222|         if (settings.promotion)
 25223|         {
 25224|             new_gen_num = get_plan_gen_num (heap_segment_gen_num (region));
 25225|         }
 25226|     }
 25227|     set_region_plan_gen_num (region, new_gen_num);
 25228| }
 25229| void gc_heap::process_last_np_surv_region (generation* consing_gen,
 25230|                                            int current_plan_gen_num,
 25231|                                            int next_plan_gen_num)
 25232| {
 25233|     heap_segment* alloc_region = generation_allocation_segment (consing_gen);
 25234|     uint8_t* consing_gen_alloc_ptr = generation_allocation_pointer (consing_gen);
 25235|     assert ((consing_gen_alloc_ptr >= heap_segment_mem (alloc_region)) &&
 25236|             (consing_gen_alloc_ptr <= heap_segment_reserved (alloc_region)));
 25237|     dprintf (REGIONS_LOG, ("h%d PLN: (%s) plan gen%d->%d, consing alloc region: %p, ptr: %p (%Id) (consing gen: %d)",
 25238|         heap_number, (settings.promotion ? "promotion" : "no promotion"), current_plan_gen_num, next_plan_gen_num,
 25239|         heap_segment_mem (alloc_region),
 25240|         generation_allocation_pointer (consing_gen),
 25241|         (generation_allocation_pointer (consing_gen) - heap_segment_mem (alloc_region)),
 25242|         consing_gen->gen_num));
 25243|     if (current_plan_gen_num != next_plan_gen_num)
 25244|     {
 25245|         if (generation_allocation_pointer (consing_gen) == heap_segment_mem (alloc_region))
 25246|         {
 25247|             dprintf (REGIONS_LOG, ("h%d alloc region %p unused, using it to plan %d",
 25248|                 heap_number, heap_segment_mem (alloc_region), next_plan_gen_num));
 25249|             return;
 25250|         }
 25251|         skip_pins_in_alloc_region (consing_gen, current_plan_gen_num);
 25252|         heap_segment* next_region = heap_segment_next_non_sip (alloc_region);
 25253|         if (!next_region)
 25254|         {
 25255|             int gen_num = heap_segment_gen_num (alloc_region);
 25256|             if (gen_num > 0)
 25257|             {
 25258|                 next_region = generation_start_segment (generation_of (gen_num - 1));
 25259|                 dprintf (REGIONS_LOG, ("h%d consing switching to next gen%d seg %p",
 25260|                     heap_number, heap_segment_gen_num (next_region), heap_segment_mem (next_region)));
 25261|             }
 25262|             else
 25263|             {
 25264|                 if (settings.promotion)
 25265|                 {
 25266|                     assert (next_plan_gen_num == 0);
 25267|                     next_region = get_new_region (0);
 25268|                     if (next_region)
 25269|                     {
 25270|                         dprintf (REGIONS_LOG, ("h%d getting a new region for gen0 plan start seg to %p",
 25271|                             heap_number, heap_segment_mem (next_region)));
 25272|                         regions_per_gen[0]++;
 25273|                         new_gen0_regions_in_plns++;
 25274|                     }
 25275|                     else
 25276|                     {
 25277|                         dprintf (REGIONS_LOG, ("h%d couldn't get a region to plan gen0, special sweep on",
 25278|                             heap_number));
 25279|                         special_sweep_p = true;
 25280|                     }
 25281|                 }
 25282|                 else
 25283|                 {
 25284|                     assert (!"ran out of regions for non promotion case??");
 25285|                 }
 25286|             }
 25287|         }
 25288|         else
 25289|         {
 25290|             dprintf (REGIONS_LOG, ("h%d consing switching to next seg %p in gen%d to alloc in",
 25291|                 heap_number, heap_segment_mem (next_region), heap_segment_gen_num (next_region)));
 25292|         }
 25293|         if (next_region)
 25294|         {
 25295|             init_alloc_info (consing_gen, next_region);
 25296|             dprintf (REGIONS_LOG, ("h%d consing(%d) alloc seg: %p(%p, %p), ptr: %p, planning gen%d",
 25297|                 heap_number, consing_gen->gen_num,
 25298|                 heap_segment_mem (generation_allocation_segment (consing_gen)),
 25299|                 heap_segment_allocated (generation_allocation_segment (consing_gen)),
 25300|                 heap_segment_plan_allocated (generation_allocation_segment (consing_gen)),
 25301|                 generation_allocation_pointer (consing_gen), next_plan_gen_num));
 25302|         }
 25303|         else
 25304|         {
 25305|             assert (special_sweep_p);
 25306|         }
 25307|     }
 25308| }
 25309| void gc_heap::process_remaining_regions (int current_plan_gen_num, generation* consing_gen)
 25310| {
 25311|     assert ((current_plan_gen_num == 0) || (!settings.promotion && (current_plan_gen_num == -1)));
 25312|     if (special_sweep_p)
 25313|     {
 25314|         assert (pinned_plug_que_empty_p());
 25315|     }
 25316|     dprintf (REGIONS_LOG, ("h%d PRR: (%s) plan %d: consing alloc seg: %p, ptr: %p",
 25317|         heap_number, (settings.promotion ? "promotion" : "no promotion"), current_plan_gen_num,
 25318|         heap_segment_mem (generation_allocation_segment (consing_gen)),
 25319|         generation_allocation_pointer (consing_gen)));
 25320|     if (current_plan_gen_num == -1)
 25321|     {
 25322|         assert (!settings.promotion);
 25323|         current_plan_gen_num = 0;
 25324|         heap_segment* alloc_region = generation_allocation_segment (consing_gen);
 25325|         if (generation_allocation_pointer (consing_gen) > heap_segment_mem (alloc_region))
 25326|         {
 25327|             skip_pins_in_alloc_region (consing_gen, current_plan_gen_num);
 25328|             heap_segment* next_region = heap_segment_next_non_sip (alloc_region);
 25329|             if ((next_region == 0) && (heap_segment_gen_num (alloc_region) > 0))
 25330|             {
 25331|                 next_region = generation_start_segment (generation_of (heap_segment_gen_num (alloc_region) - 1));
 25332|             }
 25333|             if (next_region)
 25334|             {
 25335|                 init_alloc_info (consing_gen, next_region);
 25336|             }
 25337|             else
 25338|             {
 25339|                 assert (pinned_plug_que_empty_p ());
 25340|                 if (!pinned_plug_que_empty_p ())
 25341|                 {
 25342|                     dprintf (REGIONS_LOG, ("we still have a pin at %Ix but no more regions!?", pinned_plug (oldest_pin ())));
 25343|                     GCToOSInterface::DebugBreak ();
 25344|                 }
 25345|                 generation_allocation_segment (consing_gen) = 0;
 25346|                 generation_allocation_pointer (consing_gen) = 0;
 25347|                 generation_allocation_limit (consing_gen) = 0;
 25348|             }
 25349|         }
 25350|     }
 25351|     dprintf (REGIONS_LOG, ("h%d regions in g2: %d, g1: %d, g0: %d, before processing remaining regions",
 25352|         heap_number, planned_regions_per_gen[2], planned_regions_per_gen[1], planned_regions_per_gen[0]));
 25353|     dprintf (REGIONS_LOG, ("h%d g2: surv %Id(p: %Id, %.2f%%), g1: surv %Id(p: %Id, %.2f%%), g0: surv %Id(p: %Id, %.2f%%)",
 25354|         heap_number,
 25355|         dd_survived_size (dynamic_data_of (2)), dd_pinned_survived_size (dynamic_data_of (2)),
 25356|         (dd_survived_size (dynamic_data_of (2)) ? ((double)dd_pinned_survived_size (dynamic_data_of (2)) * 100.0 / (double)dd_survived_size (dynamic_data_of (2))) : 0),
 25357|         dd_survived_size (dynamic_data_of (1)), dd_pinned_survived_size (dynamic_data_of (1)),
 25358|         (dd_survived_size (dynamic_data_of (2)) ? ((double)dd_pinned_survived_size (dynamic_data_of (1)) * 100.0 / (double)dd_survived_size (dynamic_data_of (1))) : 0),
 25359|         dd_survived_size (dynamic_data_of (0)), dd_pinned_survived_size (dynamic_data_of (0)),
 25360|         (dd_survived_size (dynamic_data_of (2)) ? ((double)dd_pinned_survived_size (dynamic_data_of (0)) * 100.0 / (double)dd_survived_size (dynamic_data_of (0))) : 0)));
 25361|     int to_be_empty_regions = 0;
 25362|     while (!pinned_plug_que_empty_p())
 25363|     {
 25364|         uint8_t* oldest_plug = pinned_plug (oldest_pin());
 25365|         heap_segment* nseg = heap_segment_rw (generation_allocation_segment (consing_gen));
 25366|         dprintf (3, ("h%d oldest pin: %p, consing alloc %p, ptr %p, limit %p",
 25367|             heap_number, oldest_plug, heap_segment_mem (nseg),
 25368|             generation_allocation_pointer (consing_gen),
 25369|             generation_allocation_limit (consing_gen)));
 25370|         while ((oldest_plug < generation_allocation_pointer (consing_gen)) ||
 25371|                (oldest_plug >= heap_segment_allocated (nseg)))
 25372|         {
 25373|             assert ((oldest_plug < heap_segment_mem (nseg)) ||
 25374|                     (oldest_plug > heap_segment_reserved (nseg)));
 25375|             assert (generation_allocation_pointer (consing_gen)>=
 25376|                     heap_segment_mem (nseg));
 25377|             assert (generation_allocation_pointer (consing_gen)<=
 25378|                     heap_segment_committed (nseg));
 25379|             dprintf (3, ("h%d PRR: in loop, seg %p pa %p -> alloc ptr %p, plan gen %d->%d",
 25380|                 heap_number, heap_segment_mem (nseg),
 25381|                 heap_segment_plan_allocated (nseg),
 25382|                 generation_allocation_pointer (consing_gen),
 25383|                 heap_segment_plan_gen_num (nseg),
 25384|                 current_plan_gen_num));
 25385|             assert (!heap_segment_swept_in_plan (nseg));
 25386|             heap_segment_plan_allocated (nseg) = generation_allocation_pointer (consing_gen);
 25387|             decide_on_demotion_pin_surv (nseg, &to_be_empty_regions);
 25388|             heap_segment* next_seg = heap_segment_next_non_sip (nseg);
 25389|             if ((next_seg == 0) && (heap_segment_gen_num (nseg) > 0))
 25390|             {
 25391|                 next_seg = generation_start_segment (generation_of (heap_segment_gen_num (nseg) - 1));
 25392|                 dprintf (3, ("h%d PRR: switching to next gen%d start %zx",
 25393|                     heap_number, heap_segment_gen_num (next_seg), (size_t)next_seg));
 25394|             }
 25395|             assert (next_seg != 0);
 25396|             nseg = next_seg;
 25397|             generation_allocation_segment (consing_gen) = nseg;
 25398|             generation_allocation_pointer (consing_gen) = heap_segment_mem (nseg);
 25399|         }
 25400|         mark* m = pinned_plug_of (deque_pinned_plug());
 25401|         uint8_t* plug = pinned_plug (m);
 25402|         size_t len = pinned_len (m);
 25403|         set_new_pin_info (m, generation_allocation_pointer (consing_gen));
 25404|         size_t free_size = pinned_len (m);
 25405|         update_planned_gen0_free_space (free_size, plug);
 25406|         dprintf (2, ("h%d plug %p-%p(%zu), free space before %p-%p(%zu)",
 25407|             heap_number, plug, (plug + len), len,
 25408|             generation_allocation_pointer (consing_gen), plug, free_size));
 25409|         generation_allocation_pointer (consing_gen) = plug + len;
 25410|         generation_allocation_limit (consing_gen) =
 25411|             generation_allocation_pointer (consing_gen);
 25412|     }
 25413|     heap_segment* current_region = generation_allocation_segment (consing_gen);
 25414|     if (special_sweep_p)
 25415|     {
 25416|         assert ((current_region == 0) || (heap_segment_next_rw (current_region) == 0));
 25417|         return;
 25418|     }
 25419|     dprintf (REGIONS_LOG, ("after going through the rest of regions - regions in g2: %d, g1: %d, g0: %d, to be empty %d now",
 25420|         planned_regions_per_gen[2], planned_regions_per_gen[1], planned_regions_per_gen[0], to_be_empty_regions));
 25421|     if (current_region)
 25422|     {
 25423|         decide_on_demotion_pin_surv (current_region, &to_be_empty_regions);
 25424|         if (!heap_segment_swept_in_plan (current_region))
 25425|         {
 25426|             heap_segment_plan_allocated (current_region) = generation_allocation_pointer (consing_gen);
 25427|             dprintf (REGIONS_LOG, ("h%d setting alloc seg %p plan alloc to %p",
 25428|                 heap_number, heap_segment_mem (current_region),
 25429|                 heap_segment_plan_allocated (current_region)));
 25430|         }
 25431|         dprintf (REGIONS_LOG, ("before going through the rest of empty regions - regions in g2: %d, g1: %d, g0: %d, to be empty %d now",
 25432|             planned_regions_per_gen[2], planned_regions_per_gen[1], planned_regions_per_gen[0], to_be_empty_regions));
 25433|         heap_segment* region_no_pins = heap_segment_next (current_region);
 25434|         int region_no_pins_gen_num = heap_segment_gen_num (current_region);
 25435|         do
 25436|         {
 25437|             region_no_pins = heap_segment_non_sip (region_no_pins);
 25438|             if (region_no_pins)
 25439|             {
 25440|                 set_region_plan_gen_num (region_no_pins, current_plan_gen_num);
 25441|                 to_be_empty_regions++;
 25442|                 heap_segment_plan_allocated (region_no_pins) = heap_segment_mem (region_no_pins);
 25443|                 dprintf (REGIONS_LOG, ("h%d setting empty seg %p(no pins) plan gen to 0, plan alloc to %p",
 25444|                     heap_number, heap_segment_mem (region_no_pins),
 25445|                     heap_segment_plan_allocated (region_no_pins)));
 25446|                 region_no_pins = heap_segment_next (region_no_pins);
 25447|             }
 25448|             if (!region_no_pins)
 25449|             {
 25450|                 if (region_no_pins_gen_num > 0)
 25451|                 {
 25452|                     region_no_pins_gen_num--;
 25453|                     region_no_pins = generation_start_segment (generation_of (region_no_pins_gen_num));
 25454|                 }
 25455|                 else
 25456|                     break;
 25457|             }
 25458|         } while (region_no_pins);
 25459|     }
 25460|     if (to_be_empty_regions)
 25461|     {
 25462|         if (planned_regions_per_gen[0] == 0)
 25463|         {
 25464|             dprintf (REGIONS_LOG, ("we didn't seem to find any gen to plan gen0 yet we have empty regions?!"));
 25465|         }
 25466|         assert (planned_regions_per_gen[0]);
 25467|     }
 25468|     int saved_planned_regions_per_gen[max_generation + 1];
 25469|     memcpy (saved_planned_regions_per_gen, planned_regions_per_gen, sizeof (saved_planned_regions_per_gen));
 25470|     assert (saved_planned_regions_per_gen[0] >= to_be_empty_regions);
 25471|     saved_planned_regions_per_gen[0] -= to_be_empty_regions;
 25472|     int plan_regions_needed = 0;
 25473|     for (int gen_idx = settings.condemned_generation; gen_idx >= 0; gen_idx--)
 25474|     {
 25475|         if (saved_planned_regions_per_gen[gen_idx] == 0)
 25476|         {
 25477|             dprintf (REGIONS_LOG, ("g%d has 0 planned regions!!!", gen_idx));
 25478|             plan_regions_needed++;
 25479|         }
 25480|     }
 25481|     dprintf (1, ("we still need %d regions, %d will be empty", plan_regions_needed, to_be_empty_regions));
 25482|     if (plan_regions_needed > to_be_empty_regions)
 25483|     {
 25484|         dprintf (REGIONS_LOG, ("h%d %d regions will be empty but we still need %d regions!!", heap_number, to_be_empty_regions, plan_regions_needed));
 25485|         plan_regions_needed -= to_be_empty_regions;
 25486|         while (plan_regions_needed && get_new_region (0))
 25487|         {
 25488|             new_regions_in_prr++;
 25489|             plan_regions_needed--;
 25490|         }
 25491|         if (plan_regions_needed > 0)
 25492|         {
 25493|             dprintf (REGIONS_LOG, ("h%d %d regions short for having at least one region per gen, special sweep on",
 25494|                 heap_number));
 25495|             special_sweep_p = true;
 25496|         }
 25497|     }
 25498| #ifdef _DEBUG
 25499|     {
 25500|         dprintf (REGIONS_LOG, ("regions in g2: %d[%d], g1: %d[%d], g0: %d[%d]",
 25501|             planned_regions_per_gen[2], regions_per_gen[2],
 25502|             planned_regions_per_gen[1], regions_per_gen[1],
 25503|             planned_regions_per_gen[0], regions_per_gen[0]));
 25504|         int total_regions = 0;
 25505|         int total_planned_regions = 0;
 25506|         for (int i = max_generation; i >= 0; i--)
 25507|         {
 25508|             total_regions += regions_per_gen[i];
 25509|             total_planned_regions += planned_regions_per_gen[i];
 25510|         }
 25511|         if (total_regions != total_planned_regions)
 25512|         {
 25513|             dprintf (REGIONS_LOG, ("planned %d regions, saw %d total",
 25514|                 total_planned_regions, total_regions));
 25515|         }
 25516|     }
 25517| #endif //_DEBUG
 25518| }
 25519| void gc_heap::grow_mark_list_piece()
 25520| {
 25521|     if (g_mark_list_piece_total_size < region_count * 2 * get_num_heaps())
 25522|     {
 25523|         delete[] g_mark_list_piece;
 25524|         size_t alloc_count = max ((g_mark_list_piece_size * 2), region_count);
 25525|         g_mark_list_piece = new (nothrow) uint8_t * *[alloc_count * 2 * get_num_heaps()];
 25526|         if (g_mark_list_piece != nullptr)
 25527|         {
 25528|             g_mark_list_piece_size = alloc_count;
 25529|         }
 25530|         else
 25531|         {
 25532|             g_mark_list_piece_size = 0;
 25533|         }
 25534|         g_mark_list_piece_total_size = g_mark_list_piece_size * 2 * get_num_heaps();
 25535|     }
 25536|     g_mark_list_piece_size = g_mark_list_piece_total_size / (2 * get_num_heaps());
 25537| }
 25538| void gc_heap::save_current_survived()
 25539| {
 25540|     if (!survived_per_region) return;
 25541|     size_t region_info_to_copy = region_count * sizeof (size_t);
 25542|     memcpy (old_card_survived_per_region, survived_per_region, region_info_to_copy);
 25543| #ifdef _DEBUG
 25544|     for (size_t region_index = 0; region_index < region_count; region_index++)
 25545|     {
 25546|         if (survived_per_region[region_index] != 0)
 25547|         {
 25548|             dprintf (REGIONS_LOG, ("region#[%3zd]: %zd", region_index, survived_per_region[region_index]));
 25549|         }
 25550|     }
 25551|     dprintf (REGIONS_LOG, ("global reported %zd", promoted_bytes (heap_number)));
 25552| #endif //_DEBUG
 25553| }
 25554| void gc_heap::update_old_card_survived()
 25555| {
 25556|     if (!survived_per_region) return;
 25557|     for (size_t region_index = 0; region_index < region_count; region_index++)
 25558|     {
 25559|         old_card_survived_per_region[region_index] = survived_per_region[region_index] -
 25560|                                                      old_card_survived_per_region[region_index];
 25561|         if (survived_per_region[region_index] != 0)
 25562|         {
 25563|             dprintf (REGIONS_LOG, ("region#[%3zd]: %zd (card: %zd)",
 25564|                 region_index, survived_per_region[region_index], old_card_survived_per_region[region_index]));
 25565|         }
 25566|     }
 25567| }
 25568| void gc_heap::update_planned_gen0_free_space (size_t free_size, uint8_t* plug)
 25569| {
 25570|     gen0_pinned_free_space += free_size;
 25571|     if (!gen0_large_chunk_found)
 25572|     {
 25573|         gen0_large_chunk_found = (free_size >= END_SPACE_AFTER_GC_FL);
 25574|         if (gen0_large_chunk_found)
 25575|         {
 25576|             dprintf (3, ("h%d found large pin free space: %zd at %p",
 25577|                 heap_number, free_size, plug));
 25578|         }
 25579|     }
 25580| }
 25581| void gc_heap::get_gen0_end_plan_space()
 25582| {
 25583|     end_gen0_region_space = 0;
 25584|     for (int gen_idx = settings.condemned_generation; gen_idx >= 0; gen_idx--)
 25585|     {
 25586|         generation* gen = generation_of (gen_idx);
 25587|         heap_segment* region = heap_segment_rw (generation_start_segment (gen));
 25588|         while (region)
 25589|         {
 25590|             if (heap_segment_plan_gen_num (region) == 0)
 25591|             {
 25592|                 size_t end_plan_space = heap_segment_reserved (region) - heap_segment_plan_allocated (region);
 25593|                 if (!gen0_large_chunk_found)
 25594|                 {
 25595|                     gen0_large_chunk_found = (end_plan_space >= END_SPACE_AFTER_GC_FL);
 25596|                     if (gen0_large_chunk_found)
 25597|                     {
 25598|                         dprintf (REGIONS_LOG, ("h%d found large end space: %zd in region %p",
 25599|                             heap_number, end_plan_space, heap_segment_mem (region)));
 25600|                     }
 25601|                 }
 25602|                 dprintf (REGIONS_LOG, ("h%d found end space: %zd in region %p, total %zd->%zd",
 25603|                     heap_number, end_plan_space, heap_segment_mem (region), end_gen0_region_space,
 25604|                     (end_gen0_region_space + end_plan_space)));
 25605|                 end_gen0_region_space += end_plan_space;
 25606|             }
 25607|             region = heap_segment_next (region);
 25608|         }
 25609|     }
 25610| }
 25611| size_t gc_heap::get_gen0_end_space(memory_type type)
 25612| {
 25613|     size_t end_space = 0;
 25614|     heap_segment* seg = generation_start_segment (generation_of (0));
 25615|     while (seg)
 25616|     {
 25617|         uint8_t* allocated = heap_segment_allocated (seg);
 25618|         uint8_t* end = (type == memory_type_reserved) ? heap_segment_reserved (seg) : heap_segment_committed (seg);
 25619|         end_space += end - allocated;
 25620|         dprintf (REGIONS_LOG, ("h%d gen0 seg %p, end %p-%p=%zx, end_space->%zd",
 25621|             heap_number, heap_segment_mem (seg),
 25622|             end, allocated,
 25623|             (end - allocated),
 25624|             end_space));
 25625|         seg = heap_segment_next (seg);
 25626|     }
 25627|     return end_space;
 25628| }
 25629| #endif //USE_REGIONS
 25630| inline
 25631| uint8_t* gc_heap::find_next_marked (uint8_t* x, uint8_t* end,
 25632|                                     BOOL use_mark_list,
 25633|                                     uint8_t**& mark_list_next,
 25634|                                     uint8_t** mark_list_index)
 25635| {
 25636|     if (use_mark_list)
 25637|     {
 25638|         uint8_t* old_x = x;
 25639|         while ((mark_list_next < mark_list_index) &&
 25640|             (*mark_list_next <= x))
 25641|         {
 25642|             mark_list_next++;
 25643|         }
 25644|         x = end;
 25645|         if ((mark_list_next < mark_list_index)
 25646| #ifdef MULTIPLE_HEAPS
 25647|             && (*mark_list_next < end) //for multiple segments
 25648| #endif //MULTIPLE_HEAPS
 25649|             )
 25650|         x = *mark_list_next;
 25651| #ifdef BACKGROUND_GC
 25652|         if (current_c_gc_state == c_gc_state_marking)
 25653|         {
 25654|             assert(gc_heap::background_running_p());
 25655|             bgc_clear_batch_mark_array_bits (old_x, x);
 25656|         }
 25657| #endif //BACKGROUND_GC
 25658|     }
 25659|     else
 25660|     {
 25661|         uint8_t* xl = x;
 25662| #ifdef BACKGROUND_GC
 25663|         if (current_c_gc_state == c_gc_state_marking)
 25664|         {
 25665|             assert (gc_heap::background_running_p());
 25666|             while ((xl < end) && !marked (xl))
 25667|             {
 25668|                 dprintf (4, ("-%zx-", (size_t)xl));
 25669|                 assert ((size (xl) > 0));
 25670|                 background_object_marked (xl, TRUE);
 25671|                 xl = xl + Align (size (xl));
 25672|                 Prefetch (xl);
 25673|             }
 25674|         }
 25675|         else
 25676| #endif //BACKGROUND_GC
 25677|         {
 25678|             while ((xl < end) && !marked (xl))
 25679|             {
 25680|                 dprintf (4, ("-%zx-", (size_t)xl));
 25681|                 assert ((size (xl) > 0));
 25682|                 xl = xl + Align (size (xl));
 25683|                 Prefetch (xl);
 25684|             }
 25685|         }
 25686|         assert (xl <= end);
 25687|         x = xl;
 25688|     }
 25689|     return x;
 25690| }
 25691| #ifdef FEATURE_EVENT_TRACE
 25692| void gc_heap::init_bucket_info()
 25693| {
 25694|     memset (bucket_info, 0, sizeof (bucket_info));
 25695| }
 25696| void gc_heap::add_plug_in_condemned_info (generation* gen, size_t plug_size)
 25697| {
 25698|     uint32_t bucket_index = generation_allocator (gen)->first_suitable_bucket (plug_size);
 25699|     (bucket_info[bucket_index].count)++;
 25700|     bucket_info[bucket_index].size += plug_size;
 25701| }
 25702| #endif //FEATURE_EVENT_TRACE
 25703| inline void save_allocated(heap_segment* seg)
 25704| {
 25705| #ifndef MULTIPLE_HEAPS
 25706|     if (!heap_segment_saved_allocated(seg))
 25707| #endif // !MULTIPLE_HEAPS
 25708|     {
 25709|         heap_segment_saved_allocated (seg) = heap_segment_allocated (seg);
 25710|     }
 25711| }
 25712| #ifdef _PREFAST_
 25713| #pragma warning(push)
 25714| #pragma warning(disable:21000) // Suppress PREFast warning about overly large function
 25715| #endif //_PREFAST_
 25716| void gc_heap::plan_phase (int condemned_gen_number)
 25717| {
 25718|     size_t old_gen2_allocated = 0;
 25719|     size_t old_gen2_size = 0;
 25720|     if (condemned_gen_number == (max_generation - 1))
 25721|     {
 25722|         old_gen2_allocated = generation_free_list_allocated (generation_of (max_generation));
 25723|         old_gen2_size = generation_size (max_generation);
 25724|     }
 25725|     assert (settings.concurrent == FALSE);
 25726|     dprintf (2,(ThreadStressLog::gcStartPlanMsg(), heap_number,
 25727|                 condemned_gen_number, settings.promotion ? 1 : 0));
 25728|     generation*  condemned_gen1 = generation_of (condemned_gen_number);
 25729|     BOOL use_mark_list = FALSE;
 25730| #ifdef GC_CONFIG_DRIVEN
 25731|     dprintf (3, ("total number of marked objects: %zd (%zd)",
 25732|                  (mark_list_index - &mark_list[0]), (mark_list_end - &mark_list[0])));
 25733|     if (mark_list_index >= (mark_list_end + 1))
 25734|     {
 25735|         mark_list_index = mark_list_end + 1;
 25736| #ifndef MULTIPLE_HEAPS // in Server GC, we check for mark list overflow in sort_mark_list
 25737|         mark_list_overflow = true;
 25738| #endif
 25739|     }
 25740| #else //GC_CONFIG_DRIVEN
 25741|     dprintf (3, ("mark_list length: %zd",
 25742|                  (mark_list_index - &mark_list[0])));
 25743| #endif //GC_CONFIG_DRIVEN
 25744|     if ((condemned_gen_number < max_generation) &&
 25745|         (mark_list_index <= mark_list_end))
 25746|     {
 25747| #ifndef MULTIPLE_HEAPS
 25748| #ifdef USE_VXSORT
 25749|         do_vxsort (mark_list, mark_list_index - mark_list, slow, shigh);
 25750| #else //USE_VXSORT
 25751|         _sort (&mark_list[0], mark_list_index - 1, 0);
 25752| #endif //USE_VXSORT
 25753|         dprintf (3, ("using mark list at GC #%zd", (size_t)settings.gc_index));
 25754| #endif //!MULTIPLE_HEAPS
 25755|         use_mark_list = TRUE;
 25756|         get_gc_data_per_heap()->set_mechanism_bit(gc_mark_list_bit);
 25757|     }
 25758|     else
 25759|     {
 25760|         dprintf (3, ("mark_list not used"));
 25761|     }
 25762| #ifdef FEATURE_BASICFREEZE
 25763|     sweep_ro_segments();
 25764| #endif //FEATURE_BASICFREEZE
 25765| #ifndef MULTIPLE_HEAPS
 25766|     int condemned_gen_index = get_stop_generation_index (condemned_gen_number);
 25767|     for (; condemned_gen_index <= condemned_gen_number; condemned_gen_index++)
 25768|     {
 25769|         generation* current_gen = generation_of (condemned_gen_index);
 25770|         if (shigh != (uint8_t*)0)
 25771|         {
 25772|             heap_segment* seg = heap_segment_rw (generation_start_segment (current_gen));
 25773|             PREFIX_ASSUME(seg != NULL);
 25774|             heap_segment* fseg = seg;
 25775|             do
 25776|             {
 25777|                 heap_segment_saved_allocated(seg) = 0;
 25778|                 if (in_range_for_segment (slow, seg))
 25779|                 {
 25780|                     uint8_t* start_unmarked = 0;
 25781| #ifdef USE_REGIONS
 25782|                     start_unmarked = heap_segment_mem (seg);
 25783| #else //USE_REGIONS
 25784|                     if (seg == fseg)
 25785|                     {
 25786|                         uint8_t* o = generation_allocation_start (current_gen);
 25787|                         o += get_soh_start_obj_len (o);
 25788|                         if (slow > o)
 25789|                         {
 25790|                             start_unmarked = o;
 25791|                             assert ((slow - o) >= (int)Align (min_obj_size));
 25792|                         }
 25793|                     }
 25794|                     else
 25795|                     {
 25796|                         assert (condemned_gen_number == max_generation);
 25797|                         start_unmarked = heap_segment_mem (seg);
 25798|                     }
 25799| #endif //USE_REGIONS
 25800|                     if (start_unmarked)
 25801|                     {
 25802|                         size_t unmarked_size = slow - start_unmarked;
 25803|                         if (unmarked_size > 0)
 25804|                         {
 25805| #ifdef BACKGROUND_GC
 25806|                             if (current_c_gc_state == c_gc_state_marking)
 25807|                             {
 25808|                                 bgc_clear_batch_mark_array_bits (start_unmarked, slow);
 25809|                             }
 25810| #endif //BACKGROUND_GC
 25811|                             make_unused_array (start_unmarked, unmarked_size);
 25812|                         }
 25813|                     }
 25814|                 }
 25815|                 if (in_range_for_segment (shigh, seg))
 25816|                 {
 25817| #ifdef BACKGROUND_GC
 25818|                     if (current_c_gc_state == c_gc_state_marking)
 25819|                     {
 25820|                         bgc_clear_batch_mark_array_bits ((shigh + Align (size (shigh))), heap_segment_allocated (seg));
 25821|                     }
 25822| #endif //BACKGROUND_GC
 25823|                     save_allocated(seg);
 25824|                     heap_segment_allocated (seg) = shigh + Align (size (shigh));
 25825|                 }
 25826|                 if (!((heap_segment_reserved (seg) >= slow) &&
 25827|                     (heap_segment_mem (seg) <= shigh)))
 25828|                 {
 25829| #ifdef BACKGROUND_GC
 25830|                     if (current_c_gc_state == c_gc_state_marking)
 25831|                     {
 25832| #ifdef USE_REGIONS
 25833|                         bgc_clear_batch_mark_array_bits (heap_segment_mem (seg), heap_segment_allocated (seg));
 25834| #else //USE_REGIONS
 25835|                         assert (!"cannot happen with segments");
 25836| #endif //USE_REGIONS
 25837|                     }
 25838| #endif //BACKGROUND_GC
 25839|                     save_allocated(seg);
 25840|                     heap_segment_allocated (seg) =  heap_segment_mem (seg);
 25841|                 }
 25842|                 seg = heap_segment_next_rw (seg);
 25843|             } while (seg);
 25844|         }
 25845|         else
 25846|         {
 25847|             heap_segment* seg = heap_segment_rw (generation_start_segment (current_gen));
 25848|             PREFIX_ASSUME(seg != NULL);
 25849|             heap_segment* sseg = seg;
 25850|             do
 25851|             {
 25852|                 heap_segment_saved_allocated(seg) = 0;
 25853|                 uint8_t* start_unmarked = heap_segment_mem (seg);
 25854| #ifndef USE_REGIONS
 25855|                 if (seg == sseg)
 25856|                 {
 25857|                     uint8_t* o = generation_allocation_start (current_gen);
 25858|                     o += get_soh_start_obj_len (o);
 25859|                     start_unmarked = o;
 25860|                 }
 25861| #endif //!USE_REGIONS
 25862| #ifdef BACKGROUND_GC
 25863|                 if (current_c_gc_state == c_gc_state_marking)
 25864|                 {
 25865|                     bgc_clear_batch_mark_array_bits (start_unmarked, heap_segment_allocated (seg));
 25866|                 }
 25867| #endif //BACKGROUND_GC
 25868|                 save_allocated(seg);
 25869|                 heap_segment_allocated (seg) = start_unmarked;
 25870|                 seg = heap_segment_next_rw (seg);
 25871|             } while (seg);
 25872|         }
 25873|     }
 25874| #endif //MULTIPLE_HEAPS
 25875|     heap_segment*  seg1 = heap_segment_rw (generation_start_segment (condemned_gen1));
 25876|     PREFIX_ASSUME(seg1 != NULL);
 25877|     uint8_t*  end = heap_segment_allocated (seg1);
 25878|     uint8_t*  first_condemned_address = get_soh_start_object (seg1, condemned_gen1);
 25879|     uint8_t*  x = first_condemned_address;
 25880| #ifdef USE_REGIONS
 25881|     memset (regions_per_gen, 0, sizeof (regions_per_gen));
 25882|     memset (planned_regions_per_gen, 0, sizeof (planned_regions_per_gen));
 25883|     memset (sip_maxgen_regions_per_gen, 0, sizeof (sip_maxgen_regions_per_gen));
 25884|     memset (reserved_free_regions_sip, 0, sizeof (reserved_free_regions_sip));
 25885|     int pinned_survived_region = 0;
 25886|     uint8_t** mark_list_index = nullptr;
 25887|     uint8_t** mark_list_next = nullptr;
 25888|     if (use_mark_list)
 25889|         mark_list_next = get_region_mark_list (use_mark_list, x, end, &mark_list_index);
 25890| #else // USE_REGIONS
 25891|     assert (!marked (x));
 25892|     uint8_t** mark_list_next = &mark_list[0];
 25893| #endif //USE_REGIONS
 25894|     uint8_t*  plug_end = x;
 25895|     uint8_t*  tree = 0;
 25896|     size_t  sequence_number = 0;
 25897|     uint8_t*  last_node = 0;
 25898|     size_t  current_brick = brick_of (x);
 25899|     BOOL  allocate_in_condemned = ((condemned_gen_number == max_generation)||
 25900|                                    (settings.promotion == FALSE));
 25901|     int  active_old_gen_number = condemned_gen_number;
 25902|     int  active_new_gen_number = (allocate_in_condemned ? condemned_gen_number:
 25903|                                   (1 + condemned_gen_number));
 25904|     generation*  older_gen = 0;
 25905|     generation* consing_gen = condemned_gen1;
 25906|     alloc_list  r_free_list [MAX_SOH_BUCKET_COUNT];
 25907|     size_t r_free_list_space = 0;
 25908|     size_t r_free_obj_space = 0;
 25909|     size_t r_older_gen_free_list_allocated = 0;
 25910|     size_t r_older_gen_condemned_allocated = 0;
 25911|     size_t r_older_gen_end_seg_allocated = 0;
 25912|     uint8_t*  r_allocation_pointer = 0;
 25913|     uint8_t*  r_allocation_limit = 0;
 25914|     uint8_t* r_allocation_start_region = 0;
 25915|     heap_segment*  r_allocation_segment = 0;
 25916| #ifdef FREE_USAGE_STATS
 25917|     size_t r_older_gen_free_space[NUM_GEN_POWER2];
 25918| #endif //FREE_USAGE_STATS
 25919|     if ((condemned_gen_number < max_generation))
 25920|     {
 25921|         older_gen = generation_of (min (max_generation, 1 + condemned_gen_number));
 25922|         generation_allocator (older_gen)->copy_to_alloc_list (r_free_list);
 25923|         r_free_list_space = generation_free_list_space (older_gen);
 25924|         r_free_obj_space = generation_free_obj_space (older_gen);
 25925| #ifdef FREE_USAGE_STATS
 25926|         memcpy (r_older_gen_free_space, older_gen->gen_free_spaces, sizeof (r_older_gen_free_space));
 25927| #endif //FREE_USAGE_STATS
 25928|         generation_allocate_end_seg_p (older_gen) = FALSE;
 25929| #ifdef DOUBLY_LINKED_FL
 25930|         if (older_gen->gen_num == max_generation)
 25931|         {
 25932|             generation_set_bgc_mark_bit_p (older_gen) = FALSE;
 25933|             generation_last_free_list_allocated (older_gen) = 0;
 25934|         }
 25935| #endif //DOUBLY_LINKED_FL
 25936|         r_older_gen_free_list_allocated = generation_free_list_allocated (older_gen);
 25937|         r_older_gen_condemned_allocated = generation_condemned_allocated (older_gen);
 25938|         r_older_gen_end_seg_allocated = generation_end_seg_allocated (older_gen);
 25939|         r_allocation_limit = generation_allocation_limit (older_gen);
 25940|         r_allocation_pointer = generation_allocation_pointer (older_gen);
 25941|         r_allocation_start_region = generation_allocation_context_start_region (older_gen);
 25942|         r_allocation_segment = generation_allocation_segment (older_gen);
 25943| #ifdef USE_REGIONS
 25944|         if (older_gen->gen_num == max_generation)
 25945|         {
 25946|             check_seg_gen_num (r_allocation_segment);
 25947|         }
 25948| #endif //USE_REGIONS
 25949|         heap_segment* start_seg = heap_segment_rw (generation_start_segment (older_gen));
 25950|         PREFIX_ASSUME(start_seg != NULL);
 25951| #ifdef USE_REGIONS
 25952|         heap_segment* skip_seg = 0;
 25953|         assert (generation_allocation_pointer (older_gen) == 0);
 25954|         assert (generation_allocation_limit (older_gen) == 0);
 25955| #else //USE_REGIONS
 25956|         heap_segment* skip_seg = ephemeral_heap_segment;
 25957|         if (start_seg != ephemeral_heap_segment)
 25958|         {
 25959|             assert (condemned_gen_number == (max_generation - 1));
 25960|         }
 25961| #endif //USE_REGIONS
 25962|         if (start_seg != skip_seg)
 25963|         {
 25964|             while (start_seg && (start_seg != skip_seg))
 25965|             {
 25966|                 assert (heap_segment_allocated (start_seg) >=
 25967|                         heap_segment_mem (start_seg));
 25968|                 assert (heap_segment_allocated (start_seg) <=
 25969|                         heap_segment_reserved (start_seg));
 25970|                 heap_segment_plan_allocated (start_seg) =
 25971|                     heap_segment_allocated (start_seg);
 25972|                 start_seg = heap_segment_next_rw (start_seg);
 25973|             }
 25974|         }
 25975|     }
 25976|     {
 25977|         int condemned_gen_index1 = get_stop_generation_index (condemned_gen_number);
 25978|         for (; condemned_gen_index1 <= condemned_gen_number; condemned_gen_index1++)
 25979|         {
 25980|             generation* current_gen = generation_of (condemned_gen_index1);
 25981|             heap_segment*  seg2 = heap_segment_rw (generation_start_segment (current_gen));
 25982|             PREFIX_ASSUME(seg2 != NULL);
 25983|             while (seg2)
 25984|             {
 25985| #ifdef USE_REGIONS
 25986|                 regions_per_gen[condemned_gen_index1]++;
 25987|                 dprintf (REGIONS_LOG, ("h%d PS: gen%d %p-%p (%d, surv: %d), %d regions",
 25988|                     heap_number, condemned_gen_index1,
 25989|                     heap_segment_mem (seg2), heap_segment_allocated (seg2),
 25990|                     (heap_segment_allocated (seg2) - heap_segment_mem (seg2)),
 25991|                     (int)heap_segment_survived (seg2), regions_per_gen[condemned_gen_index1]));
 25992| #endif //USE_REGIONS
 25993|                 heap_segment_plan_allocated (seg2) =
 25994|                     heap_segment_mem (seg2);
 25995|                 seg2 = heap_segment_next_rw (seg2);
 25996|             }
 25997|         }
 25998|     }
 25999|     int  condemned_gn = condemned_gen_number;
 26000|     int bottom_gen = 0;
 26001|     init_free_and_plug();
 26002|     while (condemned_gn >= bottom_gen)
 26003|     {
 26004|         generation*  condemned_gen2 = generation_of (condemned_gn);
 26005|         generation_allocator (condemned_gen2)->clear();
 26006|         generation_free_list_space (condemned_gen2) = 0;
 26007|         generation_free_obj_space (condemned_gen2) = 0;
 26008|         generation_allocation_size (condemned_gen2) = 0;
 26009|         generation_condemned_allocated (condemned_gen2) = 0;
 26010|         generation_sweep_allocated (condemned_gen2) = 0;
 26011|         generation_free_list_allocated(condemned_gen2) = 0;
 26012|         generation_end_seg_allocated (condemned_gen2) = 0;
 26013|         generation_pinned_allocation_sweep_size (condemned_gen2) = 0;
 26014|         generation_pinned_allocation_compact_size (condemned_gen2) = 0;
 26015| #ifdef FREE_USAGE_STATS
 26016|         generation_pinned_free_obj_space (condemned_gen2) = 0;
 26017|         generation_allocated_in_pinned_free (condemned_gen2) = 0;
 26018|         generation_allocated_since_last_pin (condemned_gen2) = 0;
 26019| #endif //FREE_USAGE_STATS
 26020| #ifndef USE_REGIONS
 26021|         generation_plan_allocation_start (condemned_gen2) = 0;
 26022| #endif //!USE_REGIONS
 26023|         generation_allocation_segment (condemned_gen2) =
 26024|             heap_segment_rw (generation_start_segment (condemned_gen2));
 26025|         PREFIX_ASSUME(generation_allocation_segment(condemned_gen2) != NULL);
 26026| #ifdef USE_REGIONS
 26027|         generation_allocation_pointer (condemned_gen2) =
 26028|             heap_segment_mem (generation_allocation_segment (condemned_gen2));
 26029| #else //USE_REGIONS
 26030|         if (generation_start_segment (condemned_gen2) != ephemeral_heap_segment)
 26031|         {
 26032|             generation_allocation_pointer (condemned_gen2) =
 26033|                 heap_segment_mem (generation_allocation_segment (condemned_gen2));
 26034|         }
 26035|         else
 26036|         {
 26037|             generation_allocation_pointer (condemned_gen2) = generation_allocation_start (condemned_gen2);
 26038|         }
 26039| #endif //USE_REGIONS
 26040|         generation_allocation_limit (condemned_gen2) = generation_allocation_pointer (condemned_gen2);
 26041|         generation_allocation_context_start_region (condemned_gen2) = generation_allocation_pointer (condemned_gen2);
 26042|         condemned_gn--;
 26043|     }
 26044|     BOOL allocate_first_generation_start = FALSE;
 26045|     if (allocate_in_condemned)
 26046|     {
 26047|         allocate_first_generation_start = TRUE;
 26048|     }
 26049|     dprintf(3,( " From %zx to %zx", (size_t)x, (size_t)end));
 26050| #ifdef USE_REGIONS
 26051|     if (should_sweep_in_plan (seg1))
 26052|     {
 26053|         sweep_region_in_plan (seg1, use_mark_list, mark_list_next, mark_list_index);
 26054|         x = end;
 26055|     }
 26056| #else
 26057|     demotion_low = MAX_PTR;
 26058|     demotion_high = heap_segment_allocated (ephemeral_heap_segment);
 26059|     demote_gen1_p = !(settings.promotion &&
 26060|         (settings.condemned_generation == (max_generation - 1)) &&
 26061|         gen_to_condemn_reasons.is_only_condition(gen_low_card_p));
 26062|     total_ephemeral_size = 0;
 26063| #endif //!USE_REGIONS
 26064|     print_free_and_plug ("BP");
 26065| #ifndef USE_REGIONS
 26066|     for (int gen_idx = 0; gen_idx <= max_generation; gen_idx++)
 26067|     {
 26068|         generation* temp_gen = generation_of (gen_idx);
 26069|         dprintf (2, ("gen%d start %p, plan start %p",
 26070|             gen_idx,
 26071|             generation_allocation_start (temp_gen),
 26072|             generation_plan_allocation_start (temp_gen)));
 26073|     }
 26074| #endif //!USE_REGIONS
 26075| #ifdef FEATURE_EVENT_TRACE
 26076|     bool record_fl_info_p = (EVENT_ENABLED (GCFitBucketInfo) && (condemned_gen_number == (max_generation - 1)));
 26077|     size_t recorded_fl_info_size = 0;
 26078|     if (record_fl_info_p)
 26079|         init_bucket_info();
 26080|     bool fire_pinned_plug_events_p = EVENT_ENABLED(PinPlugAtGCTime);
 26081| #endif //FEATURE_EVENT_TRACE
 26082|     size_t last_plug_len = 0;
 26083| #ifdef DOUBLY_LINKED_FL
 26084|     gen2_removed_no_undo = 0;
 26085|     saved_pinned_plug_index = INVALID_SAVED_PINNED_PLUG_INDEX;
 26086| #endif //DOUBLY_LINKED_FL
 26087|     while (1)
 26088|     {
 26089|         if (x >= end)
 26090|         {
 26091|             if (!use_mark_list)
 26092|             {
 26093|                 assert (x == end);
 26094|             }
 26095| #ifdef USE_REGIONS
 26096|             if (heap_segment_swept_in_plan (seg1))
 26097|             {
 26098|                 assert (heap_segment_gen_num (seg1) == active_old_gen_number);
 26099|                 dynamic_data* dd_active_old = dynamic_data_of (active_old_gen_number);
 26100|                 dd_survived_size (dd_active_old) += heap_segment_survived (seg1);
 26101|                 dprintf (REGIONS_LOG, ("region %p-%p SIP",
 26102|                     heap_segment_mem (seg1), heap_segment_allocated (seg1)));
 26103|             }
 26104|             else
 26105| #endif //USE_REGIONS
 26106|             {
 26107|                 assert (heap_segment_allocated (seg1) == end);
 26108|                 save_allocated(seg1);
 26109|                 heap_segment_allocated (seg1) = plug_end;
 26110|                 current_brick = update_brick_table (tree, current_brick, x, plug_end);
 26111|                 dprintf (REGIONS_LOG, ("region %p-%p(%p) non SIP",
 26112|                     heap_segment_mem (seg1), heap_segment_allocated (seg1),
 26113|                     heap_segment_plan_allocated (seg1)));
 26114|                 dprintf (3, ("end of seg: new tree, sequence# 0"));
 26115|                 sequence_number = 0;
 26116|                 tree = 0;
 26117|             }
 26118| #ifdef USE_REGIONS
 26119|             heap_segment_pinned_survived (seg1) = pinned_survived_region;
 26120|             dprintf (REGIONS_LOG, ("h%d setting seg %p pin surv: %d",
 26121|                 heap_number, heap_segment_mem (seg1), pinned_survived_region));
 26122|             pinned_survived_region = 0;
 26123|             if (heap_segment_mem (seg1) == heap_segment_allocated (seg1))
 26124|             {
 26125|                 num_regions_freed_in_sweep++;
 26126|             }
 26127| #endif //USE_REGIONS
 26128|             if (heap_segment_next_rw (seg1))
 26129|             {
 26130|                 seg1 = heap_segment_next_rw (seg1);
 26131|                 end = heap_segment_allocated (seg1);
 26132|                 plug_end = x = heap_segment_mem (seg1);
 26133|                 current_brick = brick_of (x);
 26134| #ifdef USE_REGIONS
 26135|                 if (use_mark_list)
 26136|                     mark_list_next = get_region_mark_list (use_mark_list, x, end, &mark_list_index);
 26137|                 if (should_sweep_in_plan (seg1))
 26138|                 {
 26139|                     sweep_region_in_plan (seg1, use_mark_list, mark_list_next, mark_list_index);
 26140|                     x = end;
 26141|                 }
 26142| #endif //USE_REGIONS
 26143|                 dprintf(3,( " From %zx to %zx", (size_t)x, (size_t)end));
 26144|                 continue;
 26145|             }
 26146|             else
 26147|             {
 26148| #ifdef USE_REGIONS
 26149|                 int saved_active_new_gen_number = active_new_gen_number;
 26150|                 BOOL saved_allocate_in_condemned = allocate_in_condemned;
 26151|                 dprintf (REGIONS_LOG, ("h%d finished planning gen%d regions into gen%d, alloc_in_condemned: %d",
 26152|                     heap_number, active_old_gen_number, active_new_gen_number, allocate_in_condemned));
 26153|                 if (active_old_gen_number <= (settings.promotion ? (max_generation - 1) : max_generation))
 26154|                 {
 26155|                     dprintf (REGIONS_LOG, ("h%d active old: %d, new: %d->%d, allocate_in_condemned %d->1",
 26156|                         heap_number, active_old_gen_number,
 26157|                         active_new_gen_number, (active_new_gen_number - 1),
 26158|                         allocate_in_condemned));
 26159|                     active_new_gen_number--;
 26160|                     allocate_in_condemned = TRUE;
 26161|                 }
 26162|                 if (active_new_gen_number >= 0)
 26163|                 {
 26164|                     process_last_np_surv_region (consing_gen, saved_active_new_gen_number, active_new_gen_number);
 26165|                 }
 26166|                 if (active_old_gen_number == 0)
 26167|                 {
 26168|                     process_remaining_regions (active_new_gen_number, consing_gen);
 26169|                     break;
 26170|                 }
 26171|                 else
 26172|                 {
 26173|                     active_old_gen_number--;
 26174|                     seg1 = heap_segment_rw (generation_start_segment (generation_of (active_old_gen_number)));
 26175|                     end = heap_segment_allocated (seg1);
 26176|                     plug_end = x = heap_segment_mem (seg1);
 26177|                     current_brick = brick_of (x);
 26178|                     if (use_mark_list)
 26179|                         mark_list_next = get_region_mark_list (use_mark_list, x, end, &mark_list_index);
 26180|                     if (should_sweep_in_plan (seg1))
 26181|                     {
 26182|                         sweep_region_in_plan (seg1, use_mark_list, mark_list_next, mark_list_index);
 26183|                         x = end;
 26184|                     }
 26185|                     dprintf (REGIONS_LOG,("h%d switching to gen%d start region %p, %p-%p",
 26186|                         heap_number, active_old_gen_number, heap_segment_mem (seg1), x, end));
 26187|                     continue;
 26188|                 }
 26189| #else //USE_REGIONS
 26190|                 break;
 26191| #endif //USE_REGIONS
 26192|             }
 26193|         }
 26194|         BOOL last_npinned_plug_p = FALSE;
 26195|         BOOL last_pinned_plug_p = FALSE;
 26196|         uint8_t* last_pinned_plug = 0;
 26197|         size_t num_pinned_plugs_in_plug = 0;
 26198|         uint8_t* last_object_in_plug = 0;
 26199|         while ((x < end) && marked (x))
 26200|         {
 26201|             uint8_t*  plug_start = x;
 26202|             uint8_t*  saved_plug_end = plug_end;
 26203|             BOOL   pinned_plug_p = FALSE;
 26204|             BOOL   npin_before_pin_p = FALSE;
 26205|             BOOL   saved_last_npinned_plug_p = last_npinned_plug_p;
 26206|             uint8_t*  saved_last_object_in_plug = last_object_in_plug;
 26207|             BOOL   merge_with_last_pin_p = FALSE;
 26208|             size_t added_pinning_size = 0;
 26209|             size_t artificial_pinned_size = 0;
 26210|             store_plug_gap_info (plug_start, plug_end, last_npinned_plug_p, last_pinned_plug_p,
 26211|                                  last_pinned_plug, pinned_plug_p, last_object_in_plug,
 26212|                                  merge_with_last_pin_p, last_plug_len);
 26213| #ifdef FEATURE_STRUCTALIGN
 26214|             int requiredAlignment = ((CObjectHeader*)plug_start)->GetRequiredAlignment();
 26215|             size_t alignmentOffset = OBJECT_ALIGNMENT_OFFSET;
 26216| #endif // FEATURE_STRUCTALIGN
 26217|             {
 26218|                 uint8_t* xl = x;
 26219|                 while ((xl < end) && marked (xl) && (pinned (xl) == pinned_plug_p))
 26220|                 {
 26221|                     assert (xl < end);
 26222|                     if (pinned(xl))
 26223|                     {
 26224|                         clear_pinned (xl);
 26225|                     }
 26226| #ifdef FEATURE_STRUCTALIGN
 26227|                     else
 26228|                     {
 26229|                         int obj_requiredAlignment = ((CObjectHeader*)xl)->GetRequiredAlignment();
 26230|                         if (obj_requiredAlignment > requiredAlignment)
 26231|                         {
 26232|                             requiredAlignment = obj_requiredAlignment;
 26233|                             alignmentOffset = xl - plug_start + OBJECT_ALIGNMENT_OFFSET;
 26234|                         }
 26235|                     }
 26236| #endif // FEATURE_STRUCTALIGN
 26237|                     clear_marked (xl);
 26238|                     dprintf(4, ("+%zx+", (size_t)xl));
 26239|                     assert ((size (xl) > 0));
 26240|                     assert ((size (xl) <= loh_size_threshold));
 26241|                     last_object_in_plug = xl;
 26242|                     xl = xl + Align (size (xl));
 26243|                     Prefetch (xl);
 26244|                 }
 26245|                 BOOL next_object_marked_p = ((xl < end) && marked (xl));
 26246|                 if (pinned_plug_p)
 26247|                 {
 26248|                     if (next_object_marked_p)
 26249|                     {
 26250|                         clear_marked (xl);
 26251|                         last_object_in_plug = xl;
 26252|                         size_t extra_size = Align (size (xl));
 26253|                         xl = xl + extra_size;
 26254|                         added_pinning_size = extra_size;
 26255|                     }
 26256|                 }
 26257|                 else
 26258|                 {
 26259|                     if (next_object_marked_p)
 26260|                         npin_before_pin_p = TRUE;
 26261|                 }
 26262|                 assert (xl <= end);
 26263|                 x = xl;
 26264|             }
 26265|             dprintf (3, ( "%zx[", (size_t)plug_start));
 26266|             plug_end = x;
 26267|             size_t ps = plug_end - plug_start;
 26268|             last_plug_len = ps;
 26269|             dprintf (3, ( "%zx[(%zx)", (size_t)x, ps));
 26270|             uint8_t*  new_address = 0;
 26271|             if (!pinned_plug_p)
 26272|             {
 26273|                 if (allocate_in_condemned &&
 26274|                     (settings.condemned_generation == max_generation) &&
 26275|                     (ps > OS_PAGE_SIZE))
 26276|                 {
 26277|                     ptrdiff_t reloc = plug_start - generation_allocation_pointer (consing_gen);
 26278|                     if ((ps > (8*OS_PAGE_SIZE)) &&
 26279|                         (reloc > 0) &&
 26280|                         ((size_t)reloc < (ps/16)))
 26281|                     {
 26282|                         dprintf (3, ("Pinning %zx; reloc would have been: %zx",
 26283|                                      (size_t)plug_start, reloc));
 26284|                         assert (!saved_last_npinned_plug_p);
 26285|                         if (last_pinned_plug)
 26286|                         {
 26287|                             dprintf (3, ("artificially pinned plug merged with last pinned plug"));
 26288|                             merge_with_last_pin_p = TRUE;
 26289|                         }
 26290|                         else
 26291|                         {
 26292|                             enque_pinned_plug (plug_start, FALSE, 0);
 26293|                             last_pinned_plug = plug_start;
 26294|                         }
 26295|                         convert_to_pinned_plug (last_npinned_plug_p, last_pinned_plug_p, pinned_plug_p,
 26296|                                                 ps, artificial_pinned_size);
 26297|                     }
 26298|                 }
 26299|             }
 26300| #ifndef USE_REGIONS
 26301|             if (allocate_first_generation_start)
 26302|             {
 26303|                 allocate_first_generation_start = FALSE;
 26304|                 plan_generation_start (condemned_gen1, consing_gen, plug_start);
 26305|                 assert (generation_plan_allocation_start (condemned_gen1));
 26306|             }
 26307|             if (seg1 == ephemeral_heap_segment)
 26308|             {
 26309|                 process_ephemeral_boundaries (plug_start, active_new_gen_number,
 26310|                                               active_old_gen_number,
 26311|                                               consing_gen,
 26312|                                               allocate_in_condemned);
 26313|             }
 26314| #endif //!USE_REGIONS
 26315|             dprintf (3, ("adding %zd to gen%d surv", ps, active_old_gen_number));
 26316|             dynamic_data* dd_active_old = dynamic_data_of (active_old_gen_number);
 26317|             dd_survived_size (dd_active_old) += ps;
 26318|             BOOL convert_to_pinned_p = FALSE;
 26319|             BOOL allocated_in_older_p = FALSE;
 26320|             if (!pinned_plug_p)
 26321|             {
 26322| #if defined (RESPECT_LARGE_ALIGNMENT) || defined (FEATURE_STRUCTALIGN)
 26323|                 dd_num_npinned_plugs (dd_active_old)++;
 26324| #endif //RESPECT_LARGE_ALIGNMENT || FEATURE_STRUCTALIGN
 26325|                 add_gen_plug (active_old_gen_number, ps);
 26326|                 if (allocate_in_condemned)
 26327|                 {
 26328|                     verify_pins_with_post_plug_info("before aic");
 26329|                     new_address =
 26330|                         allocate_in_condemned_generations (consing_gen,
 26331|                                                            ps,
 26332|                                                            active_old_gen_number,
 26333| #ifdef SHORT_PLUGS
 26334|                                                            &convert_to_pinned_p,
 26335|                                                            (npin_before_pin_p ? plug_end : 0),
 26336|                                                            seg1,
 26337| #endif //SHORT_PLUGS
 26338|                                                            plug_start REQD_ALIGN_AND_OFFSET_ARG);
 26339|                     verify_pins_with_post_plug_info("after aic");
 26340|                 }
 26341|                 else
 26342|                 {
 26343|                     new_address = allocate_in_older_generation (older_gen, ps, active_old_gen_number, plug_start REQD_ALIGN_AND_OFFSET_ARG);
 26344|                     if (new_address != 0)
 26345|                     {
 26346|                         allocated_in_older_p = TRUE;
 26347|                         if (settings.condemned_generation == (max_generation - 1))
 26348|                         {
 26349|                             dprintf (3, (" NA: %p-%p -> %zx, %zx (%zx)",
 26350|                                 plug_start, plug_end,
 26351|                                 (size_t)new_address, (size_t)new_address + (plug_end - plug_start),
 26352|                                 (size_t)(plug_end - plug_start)));
 26353|                         }
 26354|                     }
 26355|                     else
 26356|                     {
 26357|                         if (generation_allocator(older_gen)->discard_if_no_fit_p())
 26358|                         {
 26359|                             allocate_in_condemned = TRUE;
 26360|                         }
 26361|                         new_address = allocate_in_condemned_generations (consing_gen, ps, active_old_gen_number,
 26362| #ifdef SHORT_PLUGS
 26363|                                                                          &convert_to_pinned_p,
 26364|                                                                          (npin_before_pin_p ? plug_end : 0),
 26365|                                                                          seg1,
 26366| #endif //SHORT_PLUGS
 26367|                                                                          plug_start REQD_ALIGN_AND_OFFSET_ARG);
 26368|                     }
 26369|                 }
 26370| #ifdef FEATURE_EVENT_TRACE
 26371|                 if (record_fl_info_p && !allocated_in_older_p)
 26372|                 {
 26373|                     add_plug_in_condemned_info (older_gen, ps);
 26374|                     recorded_fl_info_size += ps;
 26375|                 }
 26376| #endif //FEATURE_EVENT_TRACE
 26377|                 if (convert_to_pinned_p)
 26378|                 {
 26379|                     assert (last_npinned_plug_p != FALSE);
 26380|                     assert (last_pinned_plug_p == FALSE);
 26381|                     convert_to_pinned_plug (last_npinned_plug_p, last_pinned_plug_p, pinned_plug_p,
 26382|                                             ps, artificial_pinned_size);
 26383|                     enque_pinned_plug (plug_start, FALSE, 0);
 26384|                     last_pinned_plug = plug_start;
 26385|                 }
 26386|                 else
 26387|                 {
 26388|                     if (!new_address)
 26389|                     {
 26390|                         assert (generation_allocation_segment (consing_gen) ==
 26391|                                 ephemeral_heap_segment);
 26392|                         assert ((generation_allocation_pointer (consing_gen) + Align (ps)) <
 26393|                                 heap_segment_allocated (ephemeral_heap_segment));
 26394|                         assert ((generation_allocation_pointer (consing_gen) + Align (ps)) >
 26395|                                 (heap_segment_allocated (ephemeral_heap_segment) + Align (min_obj_size)));
 26396|                     }
 26397|                     else
 26398|                     {
 26399|                         dprintf (3, (ThreadStressLog::gcPlanPlugMsg(),
 26400|                             (size_t)(node_gap_size (plug_start)),
 26401|                             plug_start, plug_end, (size_t)new_address, (size_t)(plug_start - new_address),
 26402|                                 (size_t)new_address + ps, ps,
 26403|                                 (is_plug_padded (plug_start) ? 1 : 0), x,
 26404|                                 (allocated_in_older_p ? "O" : "C")));
 26405| #ifdef SHORT_PLUGS
 26406|                         if (is_plug_padded (plug_start))
 26407|                         {
 26408|                             dprintf (3, ("%p was padded", plug_start));
 26409|                             dd_padding_size (dd_active_old) += Align (min_obj_size);
 26410|                         }
 26411| #endif //SHORT_PLUGS
 26412|                     }
 26413|                 }
 26414|             }
 26415|             if (pinned_plug_p)
 26416|             {
 26417| #ifdef FEATURE_EVENT_TRACE
 26418|                 if (fire_pinned_plug_events_p)
 26419|                 {
 26420|                     FIRE_EVENT(PinPlugAtGCTime, plug_start, plug_end,
 26421|                                (merge_with_last_pin_p ? 0 : (uint8_t*)node_gap_size (plug_start)));
 26422|                 }
 26423| #endif //FEATURE_EVENT_TRACE
 26424|                 if (merge_with_last_pin_p)
 26425|                 {
 26426|                     merge_with_last_pinned_plug (last_pinned_plug, ps);
 26427|                 }
 26428|                 else
 26429|                 {
 26430|                     assert (last_pinned_plug == plug_start);
 26431|                     set_pinned_info (plug_start, ps, consing_gen);
 26432|                 }
 26433|                 new_address = plug_start;
 26434|                 dprintf (3, (ThreadStressLog::gcPlanPinnedPlugMsg(),
 26435|                             (size_t)(node_gap_size (plug_start)), (size_t)plug_start,
 26436|                             (size_t)plug_end, ps,
 26437|                             (merge_with_last_pin_p ? 1 : 0)));
 26438|                 dprintf (3, ("adding %zd to gen%d pinned surv", plug_end - plug_start, active_old_gen_number));
 26439|                 size_t pinned_plug_size = plug_end - plug_start;
 26440| #ifdef USE_REGIONS
 26441|                 pinned_survived_region += (int)pinned_plug_size;
 26442| #endif //USE_REGIONS
 26443|                 dd_pinned_survived_size (dd_active_old) += pinned_plug_size;
 26444|                 dd_added_pinned_size (dd_active_old) += added_pinning_size;
 26445|                 dd_artificial_pinned_survived_size (dd_active_old) += artificial_pinned_size;
 26446| #ifndef USE_REGIONS
 26447|                 if (!demote_gen1_p && (active_old_gen_number == (max_generation - 1)))
 26448|                 {
 26449|                     last_gen1_pin_end = plug_end;
 26450|                 }
 26451| #endif //!USE_REGIONS
 26452|             }
 26453| #ifdef _DEBUG
 26454|             assert (!((new_address > plug_start) &&
 26455|                 (new_address < heap_segment_reserved (seg1))));
 26456| #endif //_DEBUG
 26457|             if (!merge_with_last_pin_p)
 26458|             {
 26459|                 if (current_brick != brick_of (plug_start))
 26460|                 {
 26461|                     current_brick = update_brick_table (tree, current_brick, plug_start, saved_plug_end);
 26462|                     sequence_number = 0;
 26463|                     tree = 0;
 26464|                 }
 26465|                 set_node_relocation_distance (plug_start, (new_address - plug_start));
 26466|                 if (last_node && (node_relocation_distance (last_node) ==
 26467|                                   (node_relocation_distance (plug_start) +
 26468|                                    (ptrdiff_t)node_gap_size (plug_start))))
 26469|                 {
 26470|                     dprintf (3, ("%p Lb", plug_start));
 26471|                     set_node_left (plug_start);
 26472|                 }
 26473|                 if (0 == sequence_number)
 26474|                 {
 26475|                     dprintf (2, ("sn: 0, tree is set to %p", plug_start));
 26476|                     tree = plug_start;
 26477|                 }
 26478|                 verify_pins_with_post_plug_info("before insert node");
 26479|                 tree = insert_node (plug_start, ++sequence_number, tree, last_node);
 26480|                 dprintf (3, ("tree is %p (b: %zx) after insert_node(lc: %p, rc: %p)",
 26481|                     tree, brick_of (tree),
 26482|                     (tree + node_left_child (tree)), (tree + node_right_child (tree))));
 26483|                 last_node = plug_start;
 26484| #ifdef _DEBUG
 26485|                 if (!pinned_plug_p)
 26486|                 {
 26487|                     if (mark_stack_tos > 0)
 26488|                     {
 26489|                         mark& m = mark_stack_array[mark_stack_tos - 1];
 26490|                         if (m.has_post_plug_info())
 26491|                         {
 26492|                             uint8_t* post_plug_info_start = m.saved_post_plug_info_start;
 26493|                             size_t* current_plug_gap_start = (size_t*)(plug_start - sizeof (plug_and_gap));
 26494|                             if ((uint8_t*)current_plug_gap_start == post_plug_info_start)
 26495|                             {
 26496|                                 dprintf (3, ("Ginfo: %zx, %zx, %zx",
 26497|                                     *current_plug_gap_start, *(current_plug_gap_start + 1),
 26498|                                     *(current_plug_gap_start + 2)));
 26499|                                 memcpy (&(m.saved_post_plug_debug), current_plug_gap_start, sizeof (gap_reloc_pair));
 26500|                             }
 26501|                         }
 26502|                     }
 26503|                 }
 26504| #endif //_DEBUG
 26505|                 verify_pins_with_post_plug_info("after insert node");
 26506|             }
 26507|         }
 26508|         if (num_pinned_plugs_in_plug > 1)
 26509|         {
 26510|             dprintf (3, ("more than %zd pinned plugs in this plug", num_pinned_plugs_in_plug));
 26511|         }
 26512|         x = find_next_marked (x, end, use_mark_list, mark_list_next, mark_list_index);
 26513|     }
 26514| #ifndef USE_REGIONS
 26515|     while (!pinned_plug_que_empty_p())
 26516|     {
 26517|         if (settings.promotion)
 26518|         {
 26519|             uint8_t* pplug = pinned_plug (oldest_pin());
 26520|             if (in_range_for_segment (pplug, ephemeral_heap_segment))
 26521|             {
 26522|                 consing_gen = ensure_ephemeral_heap_segment (consing_gen);
 26523|                 while (active_new_gen_number > 0)
 26524|                 {
 26525|                     active_new_gen_number--;
 26526|                     if (active_new_gen_number == (max_generation - 1))
 26527|                     {
 26528|                         maxgen_pinned_compact_before_advance = generation_pinned_allocation_compact_size (generation_of (max_generation));
 26529|                         if (!demote_gen1_p)
 26530|                             advance_pins_for_demotion (consing_gen);
 26531|                     }
 26532|                     generation* gen = generation_of (active_new_gen_number);
 26533|                     plan_generation_start (gen, consing_gen, 0);
 26534|                     if (demotion_low == MAX_PTR)
 26535|                     {
 26536|                         demotion_low = pplug;
 26537|                         dprintf (3, ("end plan: dlow->%p", demotion_low));
 26538|                     }
 26539|                     dprintf (2, ("(%d)gen%d plan start: %zx",
 26540|                                   heap_number, active_new_gen_number, (size_t)generation_plan_allocation_start (gen)));
 26541|                     assert (generation_plan_allocation_start (gen));
 26542|                 }
 26543|             }
 26544|         }
 26545|         if (pinned_plug_que_empty_p())
 26546|             break;
 26547|         size_t  entry = deque_pinned_plug();
 26548|         mark*  m = pinned_plug_of (entry);
 26549|         uint8_t*  plug = pinned_plug (m);
 26550|         size_t  len = pinned_len (m);
 26551|         heap_segment* nseg = heap_segment_rw (generation_allocation_segment (consing_gen));
 26552|         while ((plug < generation_allocation_pointer (consing_gen)) ||
 26553|                (plug >= heap_segment_allocated (nseg)))
 26554|         {
 26555|             assert ((plug < heap_segment_mem (nseg)) ||
 26556|                     (plug > heap_segment_reserved (nseg)));
 26557|             assert (generation_allocation_pointer (consing_gen)>=
 26558|                     heap_segment_mem (nseg));
 26559|             assert (generation_allocation_pointer (consing_gen)<=
 26560|                     heap_segment_committed (nseg));
 26561|             heap_segment_plan_allocated (nseg) =
 26562|                 generation_allocation_pointer (consing_gen);
 26563|             nseg = heap_segment_next_rw (nseg);
 26564|             generation_allocation_segment (consing_gen) = nseg;
 26565|             generation_allocation_pointer (consing_gen) =
 26566|                 heap_segment_mem (nseg);
 26567|         }
 26568|         set_new_pin_info (m, generation_allocation_pointer (consing_gen));
 26569|         dprintf (2, ("pin %p b: %zx->%zx", plug, brick_of (plug),
 26570|             (size_t)(brick_table[brick_of (plug)])));
 26571|         generation_allocation_pointer (consing_gen) = plug + len;
 26572|         generation_allocation_limit (consing_gen) =
 26573|             generation_allocation_pointer (consing_gen);
 26574|         int frgn = object_gennum (plug);
 26575|         if ((frgn != (int)max_generation) && settings.promotion)
 26576|         {
 26577|             generation_pinned_allocation_sweep_size ((generation_of (frgn +1))) += len;
 26578|         }
 26579|     }
 26580|     plan_generation_starts (consing_gen);
 26581| #endif //!USE_REGIONS
 26582|     descr_generations ("AP");
 26583|     print_free_and_plug ("AP");
 26584|     {
 26585| #ifdef SIMPLE_DPRINTF
 26586|         for (int gen_idx = 0; gen_idx <= max_generation; gen_idx++)
 26587|         {
 26588|             generation* temp_gen = generation_of (gen_idx);
 26589|             dynamic_data* temp_dd = dynamic_data_of (gen_idx);
 26590|             int added_pinning_ratio = 0;
 26591|             int artificial_pinned_ratio = 0;
 26592|             if (dd_pinned_survived_size (temp_dd) != 0)
 26593|             {
 26594|                 added_pinning_ratio = (int)((float)dd_added_pinned_size (temp_dd) * 100 / (float)dd_pinned_survived_size (temp_dd));
 26595|                 artificial_pinned_ratio = (int)((float)dd_artificial_pinned_survived_size (temp_dd) * 100 / (float)dd_pinned_survived_size (temp_dd));
 26596|             }
 26597|             size_t padding_size =
 26598| #ifdef SHORT_PLUGS
 26599|                 dd_padding_size (temp_dd);
 26600| #else
 26601|                 0;
 26602| #endif //SHORT_PLUGS
 26603|             dprintf (1, ("gen%d: NON PIN alloc: %zd, pin com: %zd, sweep: %zd, surv: %zd, pinsurv: %zd(%d%% added, %d%% art), np surv: %zd, pad: %zd",
 26604|                 gen_idx,
 26605|                 generation_allocation_size (temp_gen),
 26606|                 generation_pinned_allocation_compact_size (temp_gen),
 26607|                 generation_pinned_allocation_sweep_size (temp_gen),
 26608|                 dd_survived_size (temp_dd),
 26609|                 dd_pinned_survived_size (temp_dd),
 26610|                 added_pinning_ratio,
 26611|                 artificial_pinned_ratio,
 26612|                 (dd_survived_size (temp_dd) - dd_pinned_survived_size (temp_dd)),
 26613|                 padding_size));
 26614| #ifndef USE_REGIONS
 26615|             dprintf (1, ("gen%d: %p, %p(%zd)",
 26616|                 gen_idx,
 26617|                 generation_allocation_start (temp_gen),
 26618|                 generation_plan_allocation_start (temp_gen),
 26619|                 (size_t)(generation_plan_allocation_start (temp_gen) - generation_allocation_start (temp_gen))));
 26620| #endif //USE_REGIONS
 26621|         }
 26622| #endif //SIMPLE_DPRINTF
 26623|     }
 26624|     if (settings.condemned_generation == (max_generation - 1 ))
 26625|     {
 26626|         generation* older_gen = generation_of (settings.condemned_generation + 1);
 26627|         size_t rejected_free_space = generation_free_obj_space (older_gen) - r_free_obj_space;
 26628|         size_t free_list_allocated = generation_free_list_allocated (older_gen) - r_older_gen_free_list_allocated;
 26629|         size_t end_seg_allocated = generation_end_seg_allocated (older_gen) - r_older_gen_end_seg_allocated;
 26630|         size_t condemned_allocated = generation_condemned_allocated (older_gen) - r_older_gen_condemned_allocated;
 26631|         size_t growth = end_seg_allocated + condemned_allocated;
 26632|         if (growth > 0)
 26633|         {
 26634|             dprintf (1, ("gen2 grew %zd (end seg alloc: %zd, condemned alloc: %zd",
 26635|                          growth, end_seg_allocated, condemned_allocated));
 26636|             maxgen_size_inc_p = true;
 26637|         }
 26638|         else
 26639|         {
 26640|             dprintf (1, ("gen2 didn't grow (end seg alloc: %zd, , condemned alloc: %zd, gen1 c alloc: %zd",
 26641|                          end_seg_allocated, condemned_allocated,
 26642|                          generation_condemned_allocated (generation_of (max_generation - 1))));
 26643|         }
 26644|         dprintf (2, ("older gen's free alloc: %zd->%zd, seg alloc: %zd->%zd, condemned alloc: %zd->%zd",
 26645|                     r_older_gen_free_list_allocated, generation_free_list_allocated (older_gen),
 26646|                     r_older_gen_end_seg_allocated, generation_end_seg_allocated (older_gen),
 26647|                     r_older_gen_condemned_allocated, generation_condemned_allocated (older_gen)));
 26648|         dprintf (2, ("this GC did %zd free list alloc(%zd bytes free space rejected)",
 26649|             free_list_allocated, rejected_free_space));
 26650|         maxgen_size_increase* maxgen_size_info = &(get_gc_data_per_heap()->maxgen_size_info);
 26651|         maxgen_size_info->free_list_allocated = free_list_allocated;
 26652|         maxgen_size_info->free_list_rejected = rejected_free_space;
 26653|         maxgen_size_info->end_seg_allocated = end_seg_allocated;
 26654|         maxgen_size_info->condemned_allocated = condemned_allocated;
 26655|         maxgen_size_info->pinned_allocated = maxgen_pinned_compact_before_advance;
 26656|         maxgen_size_info->pinned_allocated_advance = generation_pinned_allocation_compact_size (generation_of (max_generation)) - maxgen_pinned_compact_before_advance;
 26657| #ifdef FREE_USAGE_STATS
 26658|         int free_list_efficiency = 0;
 26659|         if ((free_list_allocated + rejected_free_space) != 0)
 26660|             free_list_efficiency = (int)(((float) (free_list_allocated) / (float)(free_list_allocated + rejected_free_space)) * (float)100);
 26661|         int running_free_list_efficiency = (int)(generation_allocator_efficiency(older_gen)*100);
 26662|         dprintf (1, ("gen%d free list alloc effi: %d%%, current effi: %d%%",
 26663|                     older_gen->gen_num,
 26664|                     free_list_efficiency, running_free_list_efficiency));
 26665|         dprintf (1, ("gen2 free list change"));
 26666|         for (int j = 0; j < NUM_GEN_POWER2; j++)
 26667|         {
 26668|             dprintf (1, ("[h%d][#%zd]: 2^%d: F: %zd->%zd(%zd), P: %zd",
 26669|                 heap_number,
 26670|                 settings.gc_index,
 26671|                 (j + 10), r_older_gen_free_space[j], older_gen->gen_free_spaces[j],
 26672|                 (ptrdiff_t)(r_older_gen_free_space[j] - older_gen->gen_free_spaces[j]),
 26673|                 (generation_of(max_generation - 1))->gen_plugs[j]));
 26674|         }
 26675| #endif //FREE_USAGE_STATS
 26676|     }
 26677|     size_t fragmentation =
 26678|         generation_fragmentation (generation_of (condemned_gen_number),
 26679|                                   consing_gen,
 26680|                                   heap_segment_allocated (ephemeral_heap_segment));
 26681|     dprintf (2,("Fragmentation: %zd", fragmentation));
 26682|     dprintf (2,("---- End of Plan phase ----"));
 26683|     assert(IsGCInProgress());
 26684|     BOOL should_expand = FALSE;
 26685|     BOOL should_compact= FALSE;
 26686| #ifndef USE_REGIONS
 26687|     ephemeral_promotion = FALSE;
 26688| #endif //!USE_REGIONS
 26689| #ifdef HOST_64BIT
 26690|     if ((!settings.concurrent) &&
 26691| #ifdef USE_REGIONS
 26692|         !special_sweep_p &&
 26693| #endif //USE_REGIONS
 26694|         !provisional_mode_triggered &&
 26695|         ((condemned_gen_number < max_generation) &&
 26696|          ((settings.gen0_reduction_count > 0) || (settings.entry_memory_load >= 95))))
 26697|     {
 26698|         dprintf (GTC_LOG, ("gen0 reduction count is %d, condemning %d, mem load %d",
 26699|                      settings.gen0_reduction_count,
 26700|                      condemned_gen_number,
 26701|                      settings.entry_memory_load));
 26702|         should_compact = TRUE;
 26703|         get_gc_data_per_heap()->set_mechanism (gc_heap_compact,
 26704|             ((settings.gen0_reduction_count > 0) ? compact_fragmented_gen0 : compact_high_mem_load));
 26705| #ifndef USE_REGIONS
 26706|         if ((condemned_gen_number >= (max_generation - 1)) &&
 26707|             dt_low_ephemeral_space_p (tuning_deciding_expansion))
 26708|         {
 26709|             dprintf (GTC_LOG, ("Not enough space for all ephemeral generations with compaction"));
 26710|             should_expand = TRUE;
 26711|         }
 26712| #endif //!USE_REGIONS
 26713|     }
 26714|     else
 26715| #endif // HOST_64BIT
 26716|     {
 26717|         should_compact = decide_on_compacting (condemned_gen_number, fragmentation, should_expand);
 26718|     }
 26719|     if (condemned_gen_number == max_generation)
 26720|     {
 26721| #ifdef FEATURE_LOH_COMPACTION
 26722|         if (settings.loh_compaction)
 26723|         {
 26724|             should_compact = TRUE;
 26725|             get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_loh_forced);
 26726|         }
 26727|         else
 26728| #endif //FEATURE_LOH_COMPACTION
 26729|         {
 26730|             GCToEEInterface::DiagWalkUOHSurvivors(__this, loh_generation);
 26731|             sweep_uoh_objects (loh_generation);
 26732|         }
 26733|         GCToEEInterface::DiagWalkUOHSurvivors(__this, poh_generation);
 26734|         sweep_uoh_objects (poh_generation);
 26735|     }
 26736|     else
 26737|     {
 26738|         settings.loh_compaction = FALSE;
 26739|     }
 26740| #ifdef MULTIPLE_HEAPS
 26741| #ifndef USE_REGIONS
 26742|     new_heap_segment = NULL;
 26743| #endif //!USE_REGIONS
 26744|     if (should_compact && should_expand)
 26745|         gc_policy = policy_expand;
 26746|     else if (should_compact)
 26747|         gc_policy = policy_compact;
 26748|     else
 26749|         gc_policy = policy_sweep;
 26750|     dprintf (3, ("Joining for compaction decision"));
 26751|     gc_t_join.join(this, gc_join_decide_on_compaction);
 26752|     if (gc_t_join.joined())
 26753|     {
 26754| #ifndef USE_REGIONS
 26755|         if (condemned_gen_number == max_generation)
 26756|         {
 26757|             for (int i = 0; i < n_heaps; i++)
 26758|             {
 26759|                 g_heaps [i]->rearrange_uoh_segments ();
 26760|             }
 26761|         }
 26762| #endif //!USE_REGIONS
 26763|         if (maxgen_size_inc_p && provisional_mode_triggered
 26764| #ifdef BACKGROUND_GC
 26765|             && !is_bgc_in_progress()
 26766| #endif //BACKGROUND_GC
 26767|             )
 26768|         {
 26769|             pm_trigger_full_gc = true;
 26770|             dprintf (GTC_LOG, ("in PM: maxgen size inc, doing a sweeping gen1 and trigger NGC2"));
 26771|         }
 26772|         else
 26773|         {
 26774| #ifdef USE_REGIONS
 26775|             bool joined_special_sweep_p = false;
 26776| #else
 26777|             settings.demotion = FALSE;
 26778| #endif //USE_REGIONS
 26779|             int pol_max = policy_sweep;
 26780| #ifdef GC_CONFIG_DRIVEN
 26781|             BOOL is_compaction_mandatory = FALSE;
 26782| #endif //GC_CONFIG_DRIVEN
 26783|             int i;
 26784|             for (i = 0; i < n_heaps; i++)
 26785|             {
 26786|                 if (pol_max < g_heaps[i]->gc_policy)
 26787|                     pol_max = policy_compact;
 26788| #ifdef USE_REGIONS
 26789|                 joined_special_sweep_p |= g_heaps[i]->special_sweep_p;
 26790| #else
 26791|                 if (g_heaps[i]->demotion_high >= g_heaps[i]->demotion_low)
 26792|                 {
 26793|                     (g_heaps[i]->get_gc_data_per_heap())->set_mechanism_bit (gc_demotion_bit);
 26794|                     settings.demotion = TRUE;
 26795|                 }
 26796| #endif //USE_REGIONS
 26797| #ifdef GC_CONFIG_DRIVEN
 26798|                 if (!is_compaction_mandatory)
 26799|                 {
 26800|                     int compact_reason = (g_heaps[i]->get_gc_data_per_heap())->get_mechanism (gc_heap_compact);
 26801|                     if (compact_reason >= 0)
 26802|                     {
 26803|                         if (gc_heap_compact_reason_mandatory_p[compact_reason])
 26804|                             is_compaction_mandatory = TRUE;
 26805|                     }
 26806|                 }
 26807| #endif //GC_CONFIG_DRIVEN
 26808|             }
 26809| #ifdef GC_CONFIG_DRIVEN
 26810|             if (!is_compaction_mandatory)
 26811|             {
 26812|                 if (should_do_sweeping_gc (pol_max >= policy_compact))
 26813|                 {
 26814|                     pol_max = policy_sweep;
 26815|                 }
 26816|                 else
 26817|                 {
 26818|                     if (pol_max == policy_sweep)
 26819|                         pol_max = policy_compact;
 26820|                 }
 26821|             }
 26822| #endif //GC_CONFIG_DRIVEN
 26823|             for (i = 0; i < n_heaps; i++)
 26824|             {
 26825| #ifdef USE_REGIONS
 26826|                 g_heaps[i]->special_sweep_p = joined_special_sweep_p;
 26827|                 if (joined_special_sweep_p)
 26828|                 {
 26829|                     g_heaps[i]->gc_policy = policy_sweep;
 26830|                 }
 26831|                 else
 26832| #endif //USE_REGIONS
 26833|                 if (pol_max > g_heaps[i]->gc_policy)
 26834|                     g_heaps[i]->gc_policy = pol_max;
 26835| #ifndef USE_REGIONS
 26836|                 if (g_heaps[i]->gc_policy == policy_expand)
 26837|                 {
 26838|                     g_heaps[i]->new_heap_segment =
 26839|                         g_heaps[i]->soh_get_segment_to_expand();
 26840|                     if (!g_heaps[i]->new_heap_segment)
 26841|                     {
 26842|                         set_expand_in_full_gc (condemned_gen_number);
 26843|                         g_heaps[i]->gc_policy = policy_compact;
 26844|                     }
 26845|                 }
 26846| #endif //!USE_REGIONS
 26847|             }
 26848|             BOOL is_full_compacting_gc = FALSE;
 26849|             if ((gc_policy >= policy_compact) && (condemned_gen_number == max_generation))
 26850|             {
 26851|                 full_gc_counts[gc_type_compacting]++;
 26852|                 is_full_compacting_gc = TRUE;
 26853|             }
 26854|             for (i = 0; i < n_heaps; i++)
 26855|             {
 26856|                 if (g_gc_card_table!= g_heaps[i]->card_table)
 26857|                 {
 26858|                     g_heaps[i]->copy_brick_card_table();
 26859|                 }
 26860|                 if (is_full_compacting_gc)
 26861|                 {
 26862|                     g_heaps[i]->loh_alloc_since_cg = 0;
 26863|                 }
 26864|             }
 26865|         }
 26866| #ifdef FEATURE_EVENT_TRACE
 26867|         if (informational_event_enabled_p)
 26868|         {
 26869|             gc_time_info[time_sweep] = GetHighPrecisionTimeStamp();
 26870|             gc_time_info[time_plan] = gc_time_info[time_sweep] - gc_time_info[time_plan];
 26871|         }
 26872| #endif //FEATURE_EVENT_TRACE
 26873|         dprintf(3, ("Starting all gc threads after compaction decision"));
 26874|         gc_t_join.restart();
 26875|     }
 26876|     should_compact = (gc_policy >= policy_compact);
 26877|     should_expand  = (gc_policy >= policy_expand);
 26878| #else //MULTIPLE_HEAPS
 26879| #ifndef USE_REGIONS
 26880|     if (condemned_gen_number == max_generation)
 26881|     {
 26882|         rearrange_uoh_segments ();
 26883|     }
 26884| #endif //!USE_REGIONS
 26885|     if (maxgen_size_inc_p && provisional_mode_triggered
 26886| #ifdef BACKGROUND_GC
 26887|         && !is_bgc_in_progress()
 26888| #endif //BACKGROUND_GC
 26889|         )
 26890|     {
 26891|         pm_trigger_full_gc = true;
 26892|         dprintf (GTC_LOG, ("in PM: maxgen size inc, doing a sweeping gen1 and trigger NGC2"));
 26893|     }
 26894|     else
 26895|     {
 26896| #ifndef USE_REGIONS
 26897|         settings.demotion = ((demotion_high >= demotion_low) ? TRUE : FALSE);
 26898|         if (settings.demotion)
 26899|             get_gc_data_per_heap()->set_mechanism_bit (gc_demotion_bit);
 26900| #endif //!USE_REGIONS
 26901| #ifdef GC_CONFIG_DRIVEN
 26902|         BOOL is_compaction_mandatory = FALSE;
 26903|         int compact_reason = get_gc_data_per_heap()->get_mechanism (gc_heap_compact);
 26904|         if (compact_reason >= 0)
 26905|             is_compaction_mandatory = gc_heap_compact_reason_mandatory_p[compact_reason];
 26906|         if (!is_compaction_mandatory)
 26907|         {
 26908|             if (should_do_sweeping_gc (should_compact))
 26909|                 should_compact = FALSE;
 26910|             else
 26911|                 should_compact = TRUE;
 26912|         }
 26913| #endif //GC_CONFIG_DRIVEN
 26914|         if (should_compact && (condemned_gen_number == max_generation))
 26915|         {
 26916|             full_gc_counts[gc_type_compacting]++;
 26917|             loh_alloc_since_cg = 0;
 26918|         }
 26919|     }
 26920| #ifdef FEATURE_EVENT_TRACE
 26921|     if (informational_event_enabled_p)
 26922|     {
 26923|         gc_time_info[time_sweep] = GetHighPrecisionTimeStamp();
 26924|         gc_time_info[time_plan] = gc_time_info[time_sweep] - gc_time_info[time_plan];
 26925|     }
 26926| #endif //FEATURE_EVENT_TRACE
 26927| #ifdef USE_REGIONS
 26928|     if (special_sweep_p)
 26929|     {
 26930|         should_compact = FALSE;
 26931|     }
 26932| #endif //!USE_REGIONS
 26933| #endif //MULTIPLE_HEAPS
 26934| #ifdef FEATURE_LOH_COMPACTION
 26935|     loh_compacted_p = FALSE;
 26936| #endif //FEATURE_LOH_COMPACTION
 26937|     if (condemned_gen_number == max_generation)
 26938|     {
 26939| #ifdef FEATURE_LOH_COMPACTION
 26940|         if (settings.loh_compaction)
 26941|         {
 26942|             if (should_compact && plan_loh())
 26943|             {
 26944|                 loh_compacted_p = TRUE;
 26945|             }
 26946|             else
 26947|             {
 26948|                 GCToEEInterface::DiagWalkUOHSurvivors(__this, loh_generation);
 26949|                 sweep_uoh_objects (loh_generation);
 26950|             }
 26951|         }
 26952|         else
 26953|         {
 26954|             if (loh_pinned_queue)
 26955|             {
 26956|                 loh_pinned_queue_decay--;
 26957|                 if (!loh_pinned_queue_decay)
 26958|                 {
 26959|                     delete loh_pinned_queue;
 26960|                     loh_pinned_queue = 0;
 26961|                 }
 26962|             }
 26963|         }
 26964| #endif //FEATURE_LOH_COMPACTION
 26965|     }
 26966|     if (!pm_trigger_full_gc && pm_stress_on && provisional_mode_triggered)
 26967|     {
 26968|         if ((settings.condemned_generation == (max_generation - 1)) &&
 26969|             ((settings.gc_index % 5) == 0)
 26970| #ifdef BACKGROUND_GC
 26971|             && !is_bgc_in_progress()
 26972| #endif //BACKGROUND_GC
 26973|             )
 26974|         {
 26975|             pm_trigger_full_gc = true;
 26976|         }
 26977|     }
 26978|     if (settings.condemned_generation == (max_generation - 1))
 26979|     {
 26980|         if (provisional_mode_triggered)
 26981|         {
 26982|             if (should_expand)
 26983|             {
 26984|                 should_expand = FALSE;
 26985|                 dprintf (GTC_LOG, ("h%d in PM cannot expand", heap_number));
 26986|             }
 26987|         }
 26988|         if (pm_trigger_full_gc)
 26989|         {
 26990|             should_compact = FALSE;
 26991|             dprintf (GTC_LOG, ("h%d PM doing sweeping", heap_number));
 26992|         }
 26993|     }
 26994|     if (should_compact)
 26995|     {
 26996|         dprintf (2,( "**** Doing Compacting GC ****"));
 26997| #if defined(USE_REGIONS) && defined(BACKGROUND_GC)
 26998|         if (should_update_end_mark_size())
 26999|         {
 27000|             background_soh_size_end_mark += generation_end_seg_allocated (older_gen) -
 27001|                                             r_older_gen_end_seg_allocated;
 27002|         }
 27003| #endif //USE_REGIONS && BACKGROUND_GC
 27004| #ifndef USE_REGIONS
 27005|         if (should_expand)
 27006|         {
 27007| #ifndef MULTIPLE_HEAPS
 27008|             heap_segment* new_heap_segment = soh_get_segment_to_expand();
 27009| #endif //!MULTIPLE_HEAPS
 27010|             if (new_heap_segment)
 27011|             {
 27012|                 consing_gen = expand_heap(condemned_gen_number,
 27013|                                           consing_gen,
 27014|                                           new_heap_segment);
 27015|             }
 27016|             if (ephemeral_heap_segment != new_heap_segment)
 27017|             {
 27018|                 set_expand_in_full_gc (condemned_gen_number);
 27019|                 should_expand = FALSE;
 27020|             }
 27021|         }
 27022| #endif //!USE_REGIONS
 27023|         generation_allocation_limit (condemned_gen1) =
 27024|             generation_allocation_pointer (condemned_gen1);
 27025|         if ((condemned_gen_number < max_generation))
 27026|         {
 27027|             generation_allocator (older_gen)->commit_alloc_list_changes();
 27028|             fix_older_allocation_area (older_gen);
 27029| #ifdef FEATURE_EVENT_TRACE
 27030|             if (record_fl_info_p)
 27031|             {
 27032|                 uint16_t non_zero_buckets = 0;
 27033|                 for (uint16_t bucket_index = 0; bucket_index < NUM_GEN2_ALIST; bucket_index++)
 27034|                 {
 27035|                     if (bucket_info[bucket_index].count != 0)
 27036|                     {
 27037|                         if (bucket_index != non_zero_buckets)
 27038|                         {
 27039|                             bucket_info[non_zero_buckets].set (bucket_index,
 27040|                                                             bucket_info[bucket_index].count,
 27041|                                                             bucket_info[bucket_index].size);
 27042|                         }
 27043|                         else
 27044|                         {
 27045|                             bucket_info[bucket_index].index = bucket_index;
 27046|                         }
 27047|                         non_zero_buckets++;
 27048|                     }
 27049|                 }
 27050|                 if (non_zero_buckets)
 27051|                 {
 27052|                     FIRE_EVENT(GCFitBucketInfo,
 27053|                             (uint16_t)etw_bucket_kind::plugs_in_condemned,
 27054|                             recorded_fl_info_size,
 27055|                             non_zero_buckets,
 27056|                             (uint32_t)(sizeof (etw_bucket_info)),
 27057|                             (void *)bucket_info);
 27058|                     init_bucket_info();
 27059|                 }
 27060|                 size_t max_size_to_count = generation_free_list_space (older_gen) / 4;
 27061|                 non_zero_buckets =
 27062|                     generation_allocator (older_gen)->count_largest_items (bucket_info,
 27063|                                                                         max_size_to_count,
 27064|                                                                         max_etw_item_count,
 27065|                                                                         &recorded_fl_info_size);
 27066|                 if (non_zero_buckets)
 27067|                 {
 27068|                     FIRE_EVENT(GCFitBucketInfo,
 27069|                             (uint16_t)etw_bucket_kind::largest_fl_items,
 27070|                             recorded_fl_info_size,
 27071|                             non_zero_buckets,
 27072|                             (uint32_t)(sizeof (etw_bucket_info)),
 27073|                             (void *)bucket_info);
 27074|                 }
 27075|             }
 27076| #endif //FEATURE_EVENT_TRACE
 27077|         }
 27078| #ifndef USE_REGIONS
 27079|         assert (generation_allocation_segment (consing_gen) ==
 27080|                 ephemeral_heap_segment);
 27081| #endif //!USE_REGIONS
 27082|         GCToEEInterface::DiagWalkSurvivors(__this, true);
 27083|         relocate_phase (condemned_gen_number, first_condemned_address);
 27084|         compact_phase (condemned_gen_number, first_condemned_address,
 27085|                        (!settings.demotion && settings.promotion));
 27086|         fix_generation_bounds (condemned_gen_number, consing_gen);
 27087|         assert (generation_allocation_limit (youngest_generation) ==
 27088|                 generation_allocation_pointer (youngest_generation));
 27089| #ifndef USE_REGIONS
 27090|         if (condemned_gen_number >= (max_generation -1))
 27091|         {
 27092| #ifdef MULTIPLE_HEAPS
 27093|             gc_t_join.join(this, gc_join_rearrange_segs_compaction);
 27094|             if (gc_t_join.joined())
 27095| #endif //MULTIPLE_HEAPS
 27096|             {
 27097| #ifdef FEATURE_EVENT_TRACE
 27098|                 if (informational_event_enabled_p)
 27099|                 {
 27100|                     uint64_t current_time = GetHighPrecisionTimeStamp();
 27101|                     gc_time_info[time_compact] = current_time - gc_time_info[time_compact];
 27102|                 }
 27103| #endif //FEATURE_EVENT_TRACE
 27104| #ifdef MULTIPLE_HEAPS
 27105|                 for (int i = 0; i < n_heaps; i++)
 27106|                 {
 27107| #ifdef USE_REGIONS
 27108|                     g_heaps [i]->rearrange_uoh_segments();
 27109| #endif //USE_REGIONS
 27110|                     g_heaps [i]->rearrange_heap_segments (TRUE);
 27111|                 }
 27112| #else //MULTIPLE_HEAPS
 27113| #ifdef USE_REGIONS
 27114|                 rearrange_uoh_segments();
 27115| #endif //USE_REGIONS
 27116|                 rearrange_heap_segments (TRUE);
 27117| #endif //MULTIPLE_HEAPS
 27118| #ifdef MULTIPLE_HEAPS
 27119|                 gc_t_join.restart();
 27120| #endif //MULTIPLE_HEAPS
 27121|             }
 27122|             if (should_expand)
 27123|             {
 27124|                 for (int i = 0; i < max_generation; i++)
 27125|                 {
 27126|                     generation* gen = generation_of (i);
 27127|                     generation_start_segment (gen) = ephemeral_heap_segment;
 27128|                     generation_allocation_segment (gen) = ephemeral_heap_segment;
 27129|                 }
 27130|             }
 27131|         }
 27132| #endif //!USE_REGIONS
 27133|         {
 27134| #ifdef USE_REGIONS
 27135|             end_gen0_region_committed_space = get_gen0_end_space (memory_type_committed);
 27136|             dprintf(REGIONS_LOG, ("h%d computed the end_gen0_region_committed_space value to be %zd", heap_number, end_gen0_region_committed_space));
 27137| #endif //USE_REGIONS
 27138| #ifdef MULTIPLE_HEAPS
 27139|             dprintf(3, ("Joining after end of compaction"));
 27140|             gc_t_join.join(this, gc_join_adjust_handle_age_compact);
 27141|             if (gc_t_join.joined())
 27142|             {
 27143| #endif //MULTIPLE_HEAPS
 27144| #ifdef FEATURE_EVENT_TRACE
 27145|                 if (informational_event_enabled_p && (condemned_gen_number < (max_generation -1)))
 27146|                 {
 27147|                     uint64_t current_time = GetHighPrecisionTimeStamp();
 27148|                     gc_time_info[time_compact] = current_time - gc_time_info[time_compact];
 27149|                 }
 27150| #endif //FEATURE_EVENT_TRACE
 27151| #if defined(_DEBUG) && defined(USE_REGIONS)
 27152|                 if (heap_hard_limit)
 27153|                 {
 27154|                     size_t committed = 0;
 27155|                     for (int i = 0; i < total_generation_count; i++)
 27156|                     {
 27157|                         int oh = i - max_generation;
 27158| #ifdef MULTIPLE_HEAPS
 27159|                         for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 27160|                         {
 27161|                             gc_heap* hp = gc_heap::g_heaps[hn];
 27162| #else
 27163|                         {
 27164|                             gc_heap* hp = pGenGCHeap;
 27165| #endif // MULTIPLE_HEAPS
 27166|                             heap_segment* region = generation_start_segment (hp->generation_of (i));
 27167|                             while (region)
 27168|                             {
 27169|                                 if (!heap_segment_read_only_p (region))
 27170|                                 {
 27171|                                     committed += heap_segment_committed (region) - get_region_start (region);
 27172|                                 }
 27173|                                 region = heap_segment_next (region);
 27174|                             }
 27175| #ifdef BACKGROUND_GC
 27176|                             if (oh == soh)
 27177|                             {
 27178|                                 heap_segment* freeable = hp->freeable_soh_segment;
 27179|                                 while (freeable)
 27180|                                 {
 27181|                                     committed += (heap_segment_committed (freeable) - get_region_start (freeable));
 27182|                                     freeable = heap_segment_next (freeable);
 27183|                                 }
 27184|                             }
 27185|                             else
 27186|                             {
 27187|                                 heap_segment* freeable = hp->freeable_uoh_segment;
 27188|                                 while (freeable)
 27189|                                 {
 27190|                                     if (heap_segment_oh (freeable) == oh)
 27191|                                     {
 27192|                                         committed += (heap_segment_committed (freeable) - get_region_start (freeable));
 27193|                                     }
 27194|                                     freeable = heap_segment_next (freeable);
 27195|                                 }
 27196|                             }
 27197| #endif //BACKGROUND_GC
 27198|                         }
 27199|                         if (i >= max_generation)
 27200|                         {
 27201|                             assert (committed_by_oh[oh] == committed);
 27202|                             committed = 0;
 27203|                         }
 27204|                     }
 27205|                 }
 27206| #endif // _DEBUG && USE_REGIONS
 27207| #ifdef MULTIPLE_HEAPS
 27208|                 dprintf(3, ("Restarting after Promotion granted"));
 27209|                 gc_t_join.restart();
 27210|             }
 27211| #endif //MULTIPLE_HEAPS
 27212| #ifdef FEATURE_PREMORTEM_FINALIZATION
 27213|             finalize_queue->UpdatePromotedGenerations (condemned_gen_number,
 27214|                                                        (!settings.demotion && settings.promotion));
 27215| #endif // FEATURE_PREMORTEM_FINALIZATION
 27216|             ScanContext sc;
 27217|             sc.thread_number = heap_number;
 27218|             sc.thread_count = n_heaps;
 27219|             sc.promotion = FALSE;
 27220|             sc.concurrent = FALSE;
 27221|             if (settings.promotion && !settings.demotion)
 27222|             {
 27223|                 dprintf (2, ("Promoting EE roots for gen %d",
 27224|                              condemned_gen_number));
 27225|                 GCScan::GcPromotionsGranted(condemned_gen_number,
 27226|                                                 max_generation, &sc);
 27227|             }
 27228|             else if (settings.demotion)
 27229|             {
 27230|                 dprintf (2, ("Demoting EE roots for gen %d",
 27231|                              condemned_gen_number));
 27232|                 GCScan::GcDemote (condemned_gen_number, max_generation, &sc);
 27233|             }
 27234|         }
 27235|         {
 27236|             reset_pinned_queue_bos();
 27237| #ifndef USE_REGIONS
 27238|             unsigned int  gen_number = min (max_generation, 1 + condemned_gen_number);
 27239|             generation*  gen = generation_of (gen_number);
 27240|             uint8_t*  low = generation_allocation_start (generation_of (gen_number-1));
 27241|             uint8_t*  high =  heap_segment_allocated (ephemeral_heap_segment);
 27242| #endif //!USE_REGIONS
 27243|             while (!pinned_plug_que_empty_p())
 27244|             {
 27245|                 mark*  m = pinned_plug_of (deque_pinned_plug());
 27246|                 size_t len = pinned_len (m);
 27247|                 uint8_t*  arr = (pinned_plug (m) - len);
 27248|                 dprintf(3,("free [%zx %zx[ pin",
 27249|                             (size_t)arr, (size_t)arr + len));
 27250|                 if (len != 0)
 27251|                 {
 27252|                     assert (len >= Align (min_obj_size));
 27253|                     make_unused_array (arr, len);
 27254|                     size_t start_brick = brick_of (arr);
 27255|                     size_t end_brick = brick_of (arr + len);
 27256|                     if (end_brick != start_brick)
 27257|                     {
 27258|                         dprintf (3,
 27259|                                     ("Fixing bricks [%zx, %zx[ to point to unused array %zx",
 27260|                                     start_brick, end_brick, (size_t)arr));
 27261|                         set_brick (start_brick,
 27262|                                     arr - brick_address (start_brick));
 27263|                         size_t brick = start_brick+1;
 27264|                         while (brick < end_brick)
 27265|                         {
 27266|                             set_brick (brick, start_brick - brick);
 27267|                             brick++;
 27268|                         }
 27269|                     }
 27270| #ifdef USE_REGIONS
 27271|                     int gen_number = object_gennum_plan (arr);
 27272|                     generation* gen = generation_of (gen_number);
 27273| #else
 27274|                     if ((heap_segment_mem (ephemeral_heap_segment) <= arr) &&
 27275|                         (heap_segment_reserved (ephemeral_heap_segment) > arr))
 27276|                     {
 27277|                         while ((low <= arr) && (high > arr))
 27278|                         {
 27279|                             gen_number--;
 27280|                             assert ((gen_number >= 1) || (demotion_low != MAX_PTR) ||
 27281|                                     settings.demotion || !settings.promotion);
 27282|                             dprintf (3, ("new free list generation %d", gen_number));
 27283|                             gen = generation_of (gen_number);
 27284|                             if (gen_number >= 1)
 27285|                                 low = generation_allocation_start (generation_of (gen_number-1));
 27286|                             else
 27287|                                 low = high;
 27288|                         }
 27289|                     }
 27290|                     else
 27291|                     {
 27292|                         dprintf (3, ("new free list generation %d", max_generation));
 27293|                         gen_number = max_generation;
 27294|                         gen = generation_of (gen_number);
 27295|                     }
 27296| #endif //USE_REGIONS
 27297|                     dprintf(3,("h%d threading %p (%zd) before pin in gen %d",
 27298|                         heap_number, arr, len, gen_number));
 27299|                     thread_gap (arr, len, gen);
 27300|                     add_gen_free (gen_number, len);
 27301|                 }
 27302|             }
 27303|         }
 27304|         clear_gen1_cards();
 27305|     }
 27306|     else
 27307|     {
 27308|         settings.promotion = TRUE;
 27309|         settings.compaction = FALSE;
 27310| #ifdef USE_REGIONS
 27311|         settings.demotion = FALSE;
 27312| #endif //USE_REGIONS
 27313|         ScanContext sc;
 27314|         sc.thread_number = heap_number;
 27315|         sc.thread_count = n_heaps;
 27316|         sc.promotion = FALSE;
 27317|         sc.concurrent = FALSE;
 27318|         dprintf (2, ("**** Doing Mark and Sweep GC****"));
 27319|         if ((condemned_gen_number < max_generation))
 27320|         {
 27321| #ifdef FREE_USAGE_STATS
 27322|             memcpy (older_gen->gen_free_spaces, r_older_gen_free_space, sizeof (r_older_gen_free_space));
 27323| #endif //FREE_USAGE_STATS
 27324|             generation_allocator (older_gen)->copy_from_alloc_list (r_free_list);
 27325|             generation_free_list_space (older_gen) = r_free_list_space;
 27326|             generation_free_obj_space (older_gen) = r_free_obj_space;
 27327| #ifdef DOUBLY_LINKED_FL
 27328|             if (condemned_gen_number == (max_generation - 1))
 27329|             {
 27330|                 dprintf (2, ("[h%d] no undo, FL %zd-%zd -> %zd, FO %zd+%zd=%zd",
 27331|                     heap_number,
 27332|                     generation_free_list_space (older_gen), gen2_removed_no_undo,
 27333|                     (generation_free_list_space (older_gen) - gen2_removed_no_undo),
 27334|                     generation_free_obj_space (older_gen), gen2_removed_no_undo,
 27335|                     (generation_free_obj_space (older_gen) + gen2_removed_no_undo)));
 27336|                 generation_free_list_space (older_gen) -= gen2_removed_no_undo;
 27337|                 generation_free_obj_space (older_gen) += gen2_removed_no_undo;
 27338|             }
 27339| #endif //DOUBLY_LINKED_FL
 27340|             generation_free_list_allocated (older_gen) = r_older_gen_free_list_allocated;
 27341|             generation_end_seg_allocated (older_gen) = r_older_gen_end_seg_allocated;
 27342|             generation_condemned_allocated (older_gen) = r_older_gen_condemned_allocated;
 27343|             generation_sweep_allocated (older_gen) += dd_survived_size (dynamic_data_of (condemned_gen_number));
 27344|             generation_allocation_limit (older_gen) = r_allocation_limit;
 27345|             generation_allocation_pointer (older_gen) = r_allocation_pointer;
 27346|             generation_allocation_context_start_region (older_gen) = r_allocation_start_region;
 27347|             generation_allocation_segment (older_gen) = r_allocation_segment;
 27348| #ifdef USE_REGIONS
 27349|             if (older_gen->gen_num == max_generation)
 27350|             {
 27351|                 check_seg_gen_num (r_allocation_segment);
 27352|             }
 27353| #endif //USE_REGIONS
 27354|         }
 27355|         if ((condemned_gen_number < max_generation))
 27356|         {
 27357|             fix_older_allocation_area (older_gen);
 27358|         }
 27359|         GCToEEInterface::DiagWalkSurvivors(__this, false);
 27360|         make_free_lists (condemned_gen_number);
 27361|         size_t total_recovered_sweep_size = recover_saved_pinned_info();
 27362|         if (total_recovered_sweep_size > 0)
 27363|         {
 27364|             generation_free_obj_space (generation_of (max_generation)) -= total_recovered_sweep_size;
 27365|             dprintf (2, ("h%d: deduct %zd for pin, fo->%zd",
 27366|                 heap_number, total_recovered_sweep_size,
 27367|                 generation_free_obj_space (generation_of (max_generation))));
 27368|         }
 27369| #ifdef USE_REGIONS
 27370|         end_gen0_region_committed_space = get_gen0_end_space (memory_type_committed);
 27371|         dprintf(REGIONS_LOG, ("h%d computed the end_gen0_region_committed_space value to be %zd", heap_number, end_gen0_region_committed_space));
 27372| #endif //USE_REGIONS
 27373| #ifdef MULTIPLE_HEAPS
 27374|         dprintf(3, ("Joining after end of sweep"));
 27375|         gc_t_join.join(this, gc_join_adjust_handle_age_sweep);
 27376|         if (gc_t_join.joined())
 27377| #endif //MULTIPLE_HEAPS
 27378|         {
 27379| #ifdef FEATURE_EVENT_TRACE
 27380|             if (informational_event_enabled_p)
 27381|             {
 27382|                 uint64_t current_time = GetHighPrecisionTimeStamp();
 27383|                 gc_time_info[time_sweep] = current_time - gc_time_info[time_sweep];
 27384|             }
 27385| #endif //FEATURE_EVENT_TRACE
 27386| #ifdef USE_REGIONS
 27387|             if (!special_sweep_p)
 27388| #endif //USE_REGIONS
 27389|             {
 27390|                 GCScan::GcPromotionsGranted(condemned_gen_number,
 27391|                                                 max_generation, &sc);
 27392|             }
 27393| #ifndef USE_REGIONS
 27394|             if (condemned_gen_number >= (max_generation -1))
 27395|             {
 27396| #ifdef MULTIPLE_HEAPS
 27397|                 for (int i = 0; i < n_heaps; i++)
 27398|                 {
 27399|                     g_heaps[i]->rearrange_heap_segments(FALSE);
 27400|                 }
 27401| #else
 27402|                 rearrange_heap_segments(FALSE);
 27403| #endif //MULTIPLE_HEAPS
 27404|             }
 27405| #endif //!USE_REGIONS
 27406| #ifdef USE_REGIONS
 27407|             verify_region_to_generation_map ();
 27408| #endif //USE_REGIONS
 27409| #ifdef MULTIPLE_HEAPS
 27410|             dprintf(3, ("Restarting after Promotion granted"));
 27411|             gc_t_join.restart();
 27412| #endif //MULTIPLE_HEAPS
 27413|         }
 27414| #ifdef FEATURE_PREMORTEM_FINALIZATION
 27415| #ifdef USE_REGIONS
 27416|         if (!special_sweep_p)
 27417| #endif //USE_REGIONS
 27418|         {
 27419|             finalize_queue->UpdatePromotedGenerations (condemned_gen_number, TRUE);
 27420|         }
 27421| #endif // FEATURE_PREMORTEM_FINALIZATION
 27422| #ifdef USE_REGIONS
 27423|         if (!special_sweep_p)
 27424| #endif //USE_REGIONS
 27425|         {
 27426|             clear_gen1_cards();
 27427|         }
 27428|     }
 27429| }
 27430| #ifdef _PREFAST_
 27431| #pragma warning(pop)
 27432| #endif //_PREFAST_
 27433| /*****************************
 27434| Called after compact phase to fix all generation gaps
 27435| ********************************/
 27436| void gc_heap::fix_generation_bounds (int condemned_gen_number,
 27437|                                      generation* consing_gen)
 27438| {
 27439| #ifndef _DEBUG
 27440|     UNREFERENCED_PARAMETER(consing_gen);
 27441| #endif //_DEBUG
 27442|     int gen_number = condemned_gen_number;
 27443|     dprintf (2, ("---- thread regions gen%d GC ----", gen_number));
 27444| #ifdef USE_REGIONS
 27445|     if (settings.promotion && (condemned_gen_number < max_generation))
 27446|     {
 27447|         int older_gen_number = condemned_gen_number + 1;
 27448|         generation* older_gen = generation_of (older_gen_number);
 27449|         heap_segment* last_alloc_region = generation_allocation_segment (older_gen);
 27450|         dprintf (REGIONS_LOG, ("fix till we see alloc region which is %p", heap_segment_mem (last_alloc_region)));
 27451|         heap_segment* region = heap_segment_rw (generation_start_segment (older_gen));
 27452|         while (region)
 27453|         {
 27454|             heap_segment_allocated (region) = heap_segment_plan_allocated (region);
 27455|             if (region == last_alloc_region)
 27456|                 break;
 27457|             region = heap_segment_next (region);
 27458|         }
 27459|     }
 27460|     thread_final_regions (true);
 27461|     ephemeral_heap_segment = generation_start_segment (generation_of (0));
 27462|     alloc_allocated = heap_segment_allocated (ephemeral_heap_segment);
 27463| #else //USE_REGIONS
 27464|     assert (generation_allocation_segment (consing_gen) ==
 27465|             ephemeral_heap_segment);
 27466|     int bottom_gen = 0;
 27467|     while (gen_number >= bottom_gen)
 27468|     {
 27469|         generation*  gen = generation_of (gen_number);
 27470|         dprintf(3,("Fixing generation pointers for %d", gen_number));
 27471|         if ((gen_number < max_generation) && ephemeral_promotion)
 27472|         {
 27473|             size_t saved_eph_start_size = saved_ephemeral_plan_start_size[gen_number];
 27474|             make_unused_array (saved_ephemeral_plan_start[gen_number],
 27475|                                saved_eph_start_size);
 27476|             generation_free_obj_space (generation_of (max_generation)) += saved_eph_start_size;
 27477|             dprintf (2, ("[h%d] EP %p(%zd)", heap_number, saved_ephemeral_plan_start[gen_number],
 27478|                 saved_ephemeral_plan_start_size[gen_number]));
 27479|         }
 27480|         reset_allocation_pointers (gen, generation_plan_allocation_start (gen));
 27481|         make_unused_array (generation_allocation_start (gen), generation_plan_allocation_start_size (gen));
 27482|         dprintf(3,(" start %zx", (size_t)generation_allocation_start (gen)));
 27483|         gen_number--;
 27484|     }
 27485| #ifdef MULTIPLE_HEAPS
 27486|     if (ephemeral_promotion)
 27487|     {
 27488|         ptrdiff_t delta = 0;
 27489|         heap_segment* old_ephemeral_seg = seg_mapping_table_segment_of (saved_ephemeral_plan_start[max_generation-1]);
 27490|         assert (in_range_for_segment (saved_ephemeral_plan_start[max_generation-1], old_ephemeral_seg));
 27491|         size_t end_card = card_of (align_on_card (heap_segment_plan_allocated (old_ephemeral_seg)));
 27492|         size_t card = card_of (saved_ephemeral_plan_start[max_generation-1]);
 27493|         while (card != end_card)
 27494|         {
 27495|             set_card (card);
 27496|             card++;
 27497|         }
 27498|     }
 27499| #endif //MULTIPLE_HEAPS
 27500| #ifdef BACKGROUND_GC
 27501|     if (should_update_end_mark_size())
 27502|     {
 27503|         background_soh_size_end_mark = generation_size (max_generation);
 27504|     }
 27505| #endif //BACKGROUND_GC
 27506| #endif //!USE_REGIONS
 27507|     {
 27508|         alloc_allocated = heap_segment_plan_allocated(ephemeral_heap_segment);
 27509| #ifdef _DEBUG
 27510|         uint8_t* start = get_soh_start_object (ephemeral_heap_segment, youngest_generation);
 27511|         if (settings.promotion && !settings.demotion)
 27512|         {
 27513|             assert ((start + get_soh_start_obj_len (start)) ==
 27514|                     heap_segment_plan_allocated(ephemeral_heap_segment));
 27515|         }
 27516| #endif //_DEBUG
 27517|         heap_segment_allocated(ephemeral_heap_segment)=
 27518|             heap_segment_plan_allocated(ephemeral_heap_segment);
 27519|     }
 27520| }
 27521| #ifndef USE_REGIONS
 27522| uint8_t* gc_heap::generation_limit (int gen_number)
 27523| {
 27524|     if (settings.promotion)
 27525|     {
 27526|         if (gen_number <= 1)
 27527|             return heap_segment_reserved (ephemeral_heap_segment);
 27528|         else
 27529|             return generation_allocation_start (generation_of ((gen_number - 2)));
 27530|     }
 27531|     else
 27532|     {
 27533|         if (gen_number <= 0)
 27534|             return heap_segment_reserved (ephemeral_heap_segment);
 27535|         else
 27536|             return generation_allocation_start (generation_of ((gen_number - 1)));
 27537|     }
 27538| }
 27539| #endif //!USE_REGIONS
 27540| BOOL gc_heap::ensure_gap_allocation (int condemned_gen_number)
 27541| {
 27542| #ifndef USE_REGIONS
 27543|     uint8_t* start = heap_segment_allocated (ephemeral_heap_segment);
 27544|     size_t size = Align (min_obj_size)*(condemned_gen_number+1);
 27545|     assert ((start + size) <=
 27546|             heap_segment_reserved (ephemeral_heap_segment));
 27547|     if ((start + size) >
 27548|         heap_segment_committed (ephemeral_heap_segment))
 27549|     {
 27550|         if (!grow_heap_segment (ephemeral_heap_segment, start + size))
 27551|         {
 27552|             return FALSE;
 27553|         }
 27554|     }
 27555| #endif //USE_REGIONS
 27556|     return TRUE;
 27557| }
 27558| uint8_t* gc_heap::allocate_at_end (size_t size)
 27559| {
 27560|     uint8_t* start = heap_segment_allocated (ephemeral_heap_segment);
 27561|     size = Align (size);
 27562|     uint8_t* result = start;
 27563|     assert ((start + size) <=
 27564|             heap_segment_reserved (ephemeral_heap_segment));
 27565|     assert ((start + size) <=
 27566|             heap_segment_committed (ephemeral_heap_segment));
 27567|     heap_segment_allocated (ephemeral_heap_segment) += size;
 27568|     return result;
 27569| }
 27570| #ifdef USE_REGIONS
 27571| heap_segment* gc_heap::find_first_valid_region (heap_segment* region, bool compact_p, int* num_returned_regions)
 27572| {
 27573|     check_seg_gen_num (generation_allocation_segment (generation_of (max_generation)));
 27574|     dprintf (REGIONS_LOG, ("  FFVR region %zx(%p), gen%d",
 27575|         (size_t)region, (region ? heap_segment_mem (region) : 0),
 27576|         (region ? heap_segment_gen_num (region) : 0)));
 27577|     if (!region)
 27578|         return 0;
 27579|     heap_segment* current_region = region;
 27580|     do
 27581|     {
 27582|         int gen_num = heap_segment_gen_num (current_region);
 27583|         int plan_gen_num = -1;
 27584|         if (compact_p)
 27585|         {
 27586|             assert (settings.compaction);
 27587|             plan_gen_num = heap_segment_plan_gen_num (current_region);
 27588|             dprintf (REGIONS_LOG, ("  gen%d->%d", gen_num, plan_gen_num));
 27589|         }
 27590|         else
 27591|         {
 27592|             plan_gen_num = (special_sweep_p ? gen_num : get_plan_gen_num (gen_num));
 27593|             dprintf (REGIONS_LOG, ("  gen%d->%d, special_sweep_p %d, swept_in_plan %d",
 27594|                 gen_num, plan_gen_num, (int)special_sweep_p,
 27595|                 (int)heap_segment_swept_in_plan (current_region)));
 27596|         }
 27597|         uint8_t* allocated = (compact_p ?
 27598|                               heap_segment_plan_allocated (current_region) :
 27599|                               heap_segment_allocated (current_region));
 27600|         if (heap_segment_mem (current_region) == allocated)
 27601|         {
 27602|             heap_segment* region_to_delete = current_region;
 27603|             current_region = heap_segment_next (current_region);
 27604|             return_free_region (region_to_delete);
 27605|             (*num_returned_regions)++;
 27606|             dprintf (REGIONS_LOG, ("  h%d gen%d return region %p to free, current->%p(%p)",
 27607|                 heap_number, gen_num, heap_segment_mem (region_to_delete),
 27608|                 current_region, (current_region ? heap_segment_mem (current_region) : 0)));
 27609|             if (!current_region)
 27610|                 return 0;
 27611|         }
 27612|         else
 27613|         {
 27614|             if (compact_p)
 27615|             {
 27616|                 dprintf (REGIONS_LOG, ("  gen%d setting region %p alloc %p to plan %p",
 27617|                     gen_num, heap_segment_mem (current_region),
 27618|                     heap_segment_allocated (current_region),
 27619|                     heap_segment_plan_allocated (current_region)));
 27620|                 if (heap_segment_swept_in_plan (current_region))
 27621|                 {
 27622|                     assert (heap_segment_allocated (current_region) ==
 27623|                             heap_segment_plan_allocated (current_region));
 27624|                 }
 27625|                 else
 27626|                 {
 27627|                     heap_segment_allocated (current_region) = heap_segment_plan_allocated (current_region);
 27628|                 }
 27629|             }
 27630|             else
 27631|             {
 27632|                 set_region_plan_gen_num (current_region, plan_gen_num);
 27633|             }
 27634|             if (gen_num >= soh_gen2)
 27635|             {
 27636|                 dprintf (REGIONS_LOG, ("  gen%d decommit end of region %p(%p)",
 27637|                     gen_num, current_region, heap_segment_mem (current_region)));
 27638|                 decommit_heap_segment_pages (current_region, 0);
 27639|             }
 27640|             dprintf (REGIONS_LOG, ("  set region %p(%p) gen num to %d",
 27641|                 current_region, heap_segment_mem (current_region), plan_gen_num));
 27642|             set_region_gen_num (current_region, plan_gen_num);
 27643|             break;
 27644|         }
 27645|     } while (current_region);
 27646|     assert (current_region);
 27647|     if (heap_segment_swept_in_plan (current_region))
 27648|     {
 27649|         int gen_num = heap_segment_gen_num (current_region);
 27650|         dprintf (REGIONS_LOG, ("threading SIP region %p surv %zd onto gen%d",
 27651|             heap_segment_mem (current_region), heap_segment_survived (current_region), gen_num));
 27652|         generation* gen = generation_of (gen_num);
 27653|         generation_allocator (gen)->thread_sip_fl (current_region);
 27654|         generation_free_list_space (gen) += heap_segment_free_list_size (current_region);
 27655|         generation_free_obj_space (gen) += heap_segment_free_obj_size (current_region);
 27656|     }
 27657|     clear_region_sweep_in_plan (current_region);
 27658|     clear_region_demoted (current_region);
 27659|     return current_region;
 27660| }
 27661| void gc_heap::thread_final_regions (bool compact_p)
 27662| {
 27663|     int num_returned_regions = 0;
 27664|     int num_new_regions = 0;
 27665|     for (int i = 0; i < max_generation; i++)
 27666|     {
 27667|         if (reserved_free_regions_sip[i])
 27668|         {
 27669|             return_free_region (reserved_free_regions_sip[i]);
 27670|         }
 27671|     }
 27672|     int condemned_gen_number = settings.condemned_generation;
 27673|     generation_region_info generation_final_regions[max_generation + 1];
 27674|     memset (generation_final_regions, 0, sizeof (generation_final_regions));
 27675|     for (int gen_idx = max_generation; gen_idx > condemned_gen_number; gen_idx--)
 27676|     {
 27677|         generation* gen = generation_of (gen_idx);
 27678|         generation_final_regions[gen_idx].head = heap_segment_rw (generation_start_segment (gen));
 27679|         generation_final_regions[gen_idx].tail = generation_tail_region (gen);
 27680|     }
 27681| #ifdef BACKGROUND_GC
 27682|     heap_segment* max_gen_tail_region = 0;
 27683|     if (should_update_end_mark_size())
 27684|     {
 27685|         max_gen_tail_region = generation_final_regions[max_generation].tail;
 27686|     }
 27687| #endif //BACKGROUND_GC
 27688|     for (int gen_idx = condemned_gen_number; gen_idx >= 0; gen_idx--)
 27689|     {
 27690|         heap_segment* current_region = heap_segment_rw (generation_start_segment (generation_of (gen_idx)));
 27691|         dprintf (REGIONS_LOG, ("gen%d start from %p", gen_idx, heap_segment_mem (current_region)));
 27692|         while ((current_region = find_first_valid_region (current_region, compact_p, &num_returned_regions)))
 27693|         {
 27694|             assert (!compact_p ||
 27695|                     (heap_segment_plan_gen_num (current_region) == heap_segment_gen_num (current_region)));
 27696|             int new_gen_num = heap_segment_plan_gen_num (current_region);
 27697|             generation* new_gen = generation_of (new_gen_num);
 27698|             heap_segment* next_region = heap_segment_next (current_region);
 27699|             if (generation_final_regions[new_gen_num].head)
 27700|             {
 27701|                 assert (generation_final_regions[new_gen_num].tail);
 27702|                 dprintf (REGIONS_LOG, ("gen%d exists, tail region %p next -> %p",
 27703|                     new_gen_num, heap_segment_mem (generation_final_regions[new_gen_num].tail),
 27704|                     heap_segment_mem (current_region)));
 27705|                 heap_segment_next (generation_final_regions[new_gen_num].tail) = current_region;
 27706|                 generation_final_regions[new_gen_num].tail = current_region;
 27707|             }
 27708|             else
 27709|             {
 27710|                 generation_final_regions[new_gen_num].head = current_region;
 27711|                 generation_final_regions[new_gen_num].tail = current_region;
 27712|             }
 27713|             current_region = next_region;
 27714|         }
 27715|     }
 27716|     for (int gen_idx = 0; gen_idx <= max_generation; gen_idx++)
 27717|     {
 27718|         generation* gen = generation_of (gen_idx);
 27719|         if (generation_final_regions[gen_idx].tail)
 27720|         {
 27721|             heap_segment_next (generation_final_regions[gen_idx].tail) = 0;
 27722|         }
 27723|     }
 27724| #ifdef BACKGROUND_GC
 27725|     if (max_gen_tail_region)
 27726|     {
 27727|         max_gen_tail_region = heap_segment_next (max_gen_tail_region);
 27728|         while (max_gen_tail_region)
 27729|         {
 27730|             background_soh_size_end_mark += heap_segment_allocated (max_gen_tail_region) -
 27731|                                             heap_segment_mem (max_gen_tail_region);
 27732|             max_gen_tail_region = heap_segment_next (max_gen_tail_region);
 27733|         }
 27734|     }
 27735| #endif //BACKGROUND_GC
 27736|     for (int gen_idx = 0; gen_idx <= max_generation; gen_idx++)
 27737|     {
 27738|         bool condemned_p = (gen_idx <= condemned_gen_number);
 27739|         assert (condemned_p || generation_final_regions[gen_idx].head);
 27740|         generation* gen = generation_of (gen_idx);
 27741|         heap_segment* start_region = 0;
 27742|         if (generation_final_regions[gen_idx].head)
 27743|         {
 27744|             if (condemned_p)
 27745|             {
 27746|                 start_region = generation_final_regions[gen_idx].head;
 27747|                 thread_start_region (gen, start_region);
 27748|             }
 27749|             generation_tail_region (gen) = generation_final_regions[gen_idx].tail;
 27750|             dprintf (REGIONS_LOG, ("setting gen%d start %p, tail %p",
 27751|                 gen_idx,
 27752|                 heap_segment_mem (heap_segment_rw (generation_start_segment (gen))),
 27753|                 heap_segment_mem (generation_tail_region (gen))));
 27754|         }
 27755|         else
 27756|         {
 27757|             start_region = get_free_region (gen_idx);
 27758|             assert (start_region);
 27759|             num_new_regions++;
 27760|             thread_start_region (gen, start_region);
 27761|             dprintf (REGIONS_LOG, ("creating new gen%d at %p", gen_idx, heap_segment_mem (start_region)));
 27762|         }
 27763|         if (condemned_p)
 27764|         {
 27765|             uint8_t* gen_start = heap_segment_mem (start_region);
 27766|             reset_allocation_pointers (gen, gen_start);
 27767|         }
 27768|     }
 27769|     int net_added_regions = num_new_regions - num_returned_regions;
 27770|     dprintf (REGIONS_LOG, ("TFR: added %d, returned %d, net %d", num_new_regions, num_returned_regions, net_added_regions));
 27771|     if ((settings.compaction || special_sweep_p) && (net_added_regions > 0))
 27772|     {
 27773|         new_regions_in_threading += net_added_regions;
 27774|         assert (!"we shouldn't be getting new regions in TFR!");
 27775|     }
 27776|     verify_regions (true, false);
 27777| }
 27778| void gc_heap::thread_start_region (generation* gen, heap_segment* region)
 27779| {
 27780|     heap_segment* prev_region = generation_tail_ro_region (gen);
 27781|     if (prev_region)
 27782|     {
 27783|         heap_segment_next (prev_region) = region;
 27784|         dprintf (REGIONS_LOG,("gen%d tail ro %zx(%p) next -> %zx(%p)",
 27785|             gen->gen_num, (size_t)prev_region, heap_segment_mem (prev_region),
 27786|             (size_t)region, heap_segment_mem (region)));
 27787|     }
 27788|     else
 27789|     {
 27790|         generation_start_segment (gen) = region;
 27791|         dprintf (REGIONS_LOG, ("start region of gen%d -> %zx(%p)", gen->gen_num,
 27792|             (size_t)region, heap_segment_mem (region)));
 27793|     }
 27794|     dprintf (REGIONS_LOG, ("tail region of gen%d -> %zx(%p)", gen->gen_num,
 27795|         (size_t)region, heap_segment_mem (region)));
 27796|     generation_tail_region (gen) = region;
 27797| }
 27798| heap_segment* gc_heap::get_new_region (int gen_number, size_t size)
 27799| {
 27800|     heap_segment* new_region = get_free_region (gen_number, size);
 27801|     if (new_region)
 27802|     {
 27803|         switch (gen_number)
 27804|         {
 27805|         default:
 27806|             assert ((new_region->flags & (heap_segment_flags_loh | heap_segment_flags_poh)) == 0);
 27807|             break;
 27808|         case    loh_generation:
 27809|             new_region->flags |= heap_segment_flags_loh;
 27810|             break;
 27811|         case    poh_generation:
 27812|             new_region->flags |= heap_segment_flags_poh;
 27813|             break;
 27814|         }
 27815|         generation* gen = generation_of (gen_number);
 27816|         heap_segment_next (generation_tail_region (gen)) = new_region;
 27817|         generation_tail_region (gen) = new_region;
 27818|         verify_regions (gen_number, false, settings.concurrent);
 27819|     }
 27820|     return new_region;
 27821| }
 27822| heap_segment* gc_heap::allocate_new_region (gc_heap* hp, int gen_num, bool uoh_p, size_t size)
 27823| {
 27824|     uint8_t* start = 0;
 27825|     uint8_t* end = 0;
 27826|     assert (uoh_p || size == 0);
 27827|     bool allocated_p = (uoh_p ?
 27828|         global_region_allocator.allocate_large_region (gen_num, &start, &end, allocate_forward, size, on_used_changed) :
 27829|         global_region_allocator.allocate_basic_region (gen_num, &start, &end, on_used_changed));
 27830|     if (!allocated_p)
 27831|     {
 27832|         return 0;
 27833|     }
 27834|     heap_segment* res = make_heap_segment (start, (end - start), hp, gen_num);
 27835|     dprintf (REGIONS_LOG, ("got a new region %zx %p->%p", (size_t)res, start, end));
 27836|     if (res == nullptr)
 27837|     {
 27838|         global_region_allocator.delete_region (start);
 27839|     }
 27840|     return res;
 27841| }
 27842| void gc_heap::update_start_tail_regions (generation* gen,
 27843|                                          heap_segment* region_to_delete,
 27844|                                          heap_segment* prev_region,
 27845|                                          heap_segment* next_region)
 27846| {
 27847|     if (region_to_delete == heap_segment_rw (generation_start_segment (gen)))
 27848|     {
 27849|         assert (!prev_region);
 27850|         heap_segment* tail_ro_region = generation_tail_ro_region (gen);
 27851|         if (tail_ro_region)
 27852|         {
 27853|             heap_segment_next (tail_ro_region) = next_region;
 27854|             dprintf (REGIONS_LOG, ("gen%d tail ro %zx(%p) next updated to %zx(%p)",
 27855|                 gen->gen_num, (size_t)tail_ro_region, heap_segment_mem (tail_ro_region),
 27856|                 (size_t)next_region, heap_segment_mem (next_region)));
 27857|         }
 27858|         else
 27859|         {
 27860|             generation_start_segment (gen) = next_region;
 27861|             dprintf (REGIONS_LOG, ("start region of gen%d updated to %zx(%p)", gen->gen_num,
 27862|                 (size_t)next_region, heap_segment_mem (next_region)));
 27863|         }
 27864|     }
 27865|     if (region_to_delete == generation_tail_region (gen))
 27866|     {
 27867|         assert (!next_region);
 27868|         generation_tail_region (gen) = prev_region;
 27869|         dprintf (REGIONS_LOG, ("tail region of gen%d updated to %zx(%p)", gen->gen_num,
 27870|             (size_t)prev_region, heap_segment_mem (prev_region)));
 27871|     }
 27872|     verify_regions (false, settings.concurrent);
 27873| }
 27874| inline
 27875| bool gc_heap::should_sweep_in_plan (heap_segment* region)
 27876| {
 27877|     if (!enable_special_regions_p)
 27878|     {
 27879|         return false;
 27880|     }
 27881|     if (settings.reason == reason_induced_aggressive)
 27882|     {
 27883|         return false;
 27884|     }
 27885|     bool sip_p = false;
 27886|     int gen_num = get_region_gen_num (region);
 27887|     int new_gen_num = get_plan_gen_num (gen_num);
 27888|     heap_segment_swept_in_plan (region) = false;
 27889|     dprintf (REGIONS_LOG, ("checking if region %p should be SIP", heap_segment_mem (region)));
 27890| #ifdef STRESS_REGIONS
 27891|     if (0)
 27892|     {
 27893|         num_condemned_regions++;
 27894|         if ((num_condemned_regions % sip_seg_interval) == 0)
 27895|         {
 27896|             set_region_plan_gen_num (region, new_gen_num);
 27897|             sip_p = true;
 27898|         }
 27899|         if ((num_condemned_regions % sip_seg_maxgen_interval) == 0)
 27900|         {
 27901|             set_region_plan_gen_num (region, max_generation);
 27902|             sip_maxgen_regions_per_gen[gen_num]++;
 27903|             sip_p = true;
 27904|         }
 27905|     }
 27906|     else
 27907| #endif //STRESS_REGIONS
 27908|     {
 27909|         size_t basic_region_size = (size_t)1 << min_segment_size_shr;
 27910|         assert (heap_segment_gen_num (region) == heap_segment_plan_gen_num (region));
 27911|         uint8_t surv_ratio = (uint8_t)(((double)heap_segment_survived (region) * 100.0) / (double)basic_region_size);
 27912|         dprintf (2222, ("SSIP: region %p surv %hu / %zd = %d%%(%d)",
 27913|             heap_segment_mem (region),
 27914|             heap_segment_survived (region),
 27915|             basic_region_size,
 27916|             surv_ratio, sip_surv_ratio_th));
 27917|         if (surv_ratio >= sip_surv_ratio_th)
 27918|         {
 27919|             set_region_plan_gen_num (region, new_gen_num);
 27920|             sip_p = true;
 27921|         }
 27922|         if (settings.promotion && (new_gen_num < max_generation))
 27923|         {
 27924|             int old_card_surv_ratio =
 27925|                 (int)(((double)heap_segment_old_card_survived (region) * 100.0) / (double)basic_region_size);
 27926|             dprintf (2222, ("SSIP: region %p old card surv %d / %zd = %d%%(%d)",
 27927|                 heap_segment_mem (region),
 27928|                 heap_segment_old_card_survived (region),
 27929|                 basic_region_size,
 27930|                 old_card_surv_ratio, sip_surv_ratio_th));
 27931|             if (old_card_surv_ratio >= sip_old_card_surv_ratio_th)
 27932|             {
 27933|                 set_region_plan_gen_num (region, max_generation, true);
 27934|                 sip_maxgen_regions_per_gen[gen_num]++;
 27935|                 sip_p = true;
 27936|             }
 27937|         }
 27938|     }
 27939|     if (sip_p)
 27940|     {
 27941|         if ((new_gen_num < max_generation) &&
 27942|             (sip_maxgen_regions_per_gen[gen_num] == regions_per_gen[gen_num]))
 27943|         {
 27944|             assert (get_region_gen_num (region) == 0);
 27945|             assert (new_gen_num < max_generation);
 27946|             heap_segment* reserved_free_region = get_free_region (gen_num);
 27947|             if (reserved_free_region)
 27948|             {
 27949|                 dprintf (REGIONS_LOG, ("all regions in gen%d -> SIP 2, get a new region for it %p",
 27950|                     gen_num, heap_segment_mem (reserved_free_region)));
 27951|                 reserved_free_regions_sip[gen_num] = reserved_free_region;
 27952|             }
 27953|             else
 27954|             {
 27955|                 sip_maxgen_regions_per_gen[gen_num]--;
 27956|                 set_region_plan_gen_num (region, new_gen_num, true);
 27957|             }
 27958|         }
 27959|     }
 27960|     dprintf (REGIONS_LOG, ("region %p %s SIP", heap_segment_mem (region),
 27961|         (sip_p ? "is" : "is not")));
 27962|     return sip_p;
 27963| }
 27964| void heap_segment::thread_free_obj (uint8_t* obj, size_t s)
 27965| {
 27966|     if (s >= min_free_list)
 27967|     {
 27968|         free_list_slot (obj) = 0;
 27969|         if (free_list_head)
 27970|         {
 27971|             assert (free_list_tail);
 27972|             free_list_slot (free_list_tail) = obj;
 27973|         }
 27974|         else
 27975|         {
 27976|             free_list_head = obj;
 27977|         }
 27978|         free_list_tail = obj;
 27979|         free_list_size += s;
 27980|     }
 27981|     else
 27982|     {
 27983|         free_obj_size += s;
 27984|     }
 27985| }
 27986| void gc_heap::sweep_region_in_plan (heap_segment* region,
 27987|                                     BOOL use_mark_list,
 27988|                                     uint8_t**& mark_list_next,
 27989|                                     uint8_t** mark_list_index)
 27990| {
 27991|     set_region_sweep_in_plan (region);
 27992|     region->init_free_list();
 27993|     uint8_t* x = heap_segment_mem (region);
 27994|     uint8_t* last_marked_obj_start = 0;
 27995|     uint8_t* last_marked_obj_end = 0;
 27996|     uint8_t* end = heap_segment_allocated (region);
 27997|     dprintf (2222, ("h%d region %p->%p SIP, gen %d->%d, %s mark list(%p->%p, %p->%p)",
 27998|         heap_number, x, end, heap_segment_gen_num (region), heap_segment_plan_gen_num (region),
 27999|         (use_mark_list ? "using" : "not using"),
 28000|         (uint8_t*)mark_list_next, (mark_list_next ? *mark_list_next : 0),
 28001|         (uint8_t*)mark_list_index, (mark_list_index ? *mark_list_index : 0)));
 28002| #ifdef _DEBUG
 28003|     size_t survived = 0;
 28004|     uint8_t* saved_last_unmarked_obj_start = 0;
 28005|     uint8_t* saved_last_unmarked_obj_end = 0;
 28006|     size_t saved_obj_brick = 0;
 28007|     size_t saved_next_obj_brick = 0;
 28008| #endif //_DEBUG
 28009|     while (x < end)
 28010|     {
 28011|         uint8_t* obj = x;
 28012|         size_t obj_brick = (size_t)obj / brick_size;
 28013|         uint8_t* next_obj = 0;
 28014|         if (marked (obj))
 28015|         {
 28016|             if (pinned(obj))
 28017|             {
 28018|                 clear_pinned (obj);
 28019|             }
 28020|             clear_marked (obj);
 28021|             size_t s = size (obj);
 28022|             next_obj = obj + Align (s);
 28023|             last_marked_obj_start = obj;
 28024|             last_marked_obj_end = next_obj;
 28025| #ifdef _DEBUG
 28026|             survived += s;
 28027| #endif //_DEBUG
 28028|             dprintf (4444, ("M: %p-%p(%zd)", obj, next_obj, s));
 28029|         }
 28030|         else
 28031|         {
 28032|             next_obj = find_next_marked (x, end, use_mark_list, mark_list_next, mark_list_index);
 28033| #ifdef _DEBUG
 28034|             saved_last_unmarked_obj_start = obj;
 28035|             saved_last_unmarked_obj_end = next_obj;
 28036| #endif //_DEBUG
 28037|             if ((next_obj > obj) && (next_obj != end))
 28038|             {
 28039|                 size_t free_obj_size = next_obj - obj;
 28040|                 make_unused_array (obj, free_obj_size);
 28041|                 region->thread_free_obj (obj, free_obj_size);
 28042|                 dprintf (4444, ("UM threading: %p-%p(%zd)", obj, next_obj, (next_obj - obj)));
 28043|             }
 28044|         }
 28045|         size_t next_obj_brick = (size_t)next_obj / brick_size;
 28046| #ifdef _DEBUG
 28047|         saved_obj_brick = obj_brick;
 28048|         saved_next_obj_brick = next_obj_brick;
 28049| #endif //_DEBUG
 28050|         if (next_obj_brick != obj_brick)
 28051|         {
 28052|             fix_brick_to_highest (obj, next_obj);
 28053|         }
 28054|         x = next_obj;
 28055|     }
 28056|     if (last_marked_obj_start)
 28057|     {
 28058|         size_t last_marked_obj_start_b = brick_of (last_marked_obj_start);
 28059|         size_t last_marked_obj_end_b = brick_of (last_marked_obj_end - 1);
 28060|         dprintf (REGIONS_LOG, ("last live obj %p(%p)-%p, fixing its brick(s) %zx-%zx",
 28061|             last_marked_obj_start, method_table (last_marked_obj_start), last_marked_obj_end,
 28062|             last_marked_obj_start_b, last_marked_obj_end_b));
 28063|         if (last_marked_obj_start_b == last_marked_obj_end_b)
 28064|         {
 28065|             set_brick (last_marked_obj_start_b,
 28066|                     (last_marked_obj_start - brick_address (last_marked_obj_start_b)));
 28067|         }
 28068|         else
 28069|         {
 28070|             set_brick (last_marked_obj_end_b,
 28071|                     (last_marked_obj_start_b - last_marked_obj_end_b));
 28072|         }
 28073|     }
 28074|     else
 28075|     {
 28076|         last_marked_obj_end = heap_segment_mem (region);
 28077|     }
 28078| #ifdef _DEBUG
 28079|     size_t region_index = get_basic_region_index_for_address (heap_segment_mem (region));
 28080|     dprintf (REGIONS_LOG, ("region #%zd %p survived %zd, %s recorded %d",
 28081|         region_index, heap_segment_mem (region), survived,
 28082|         ((survived == heap_segment_survived (region)) ? "same as" : "diff from"),
 28083|         heap_segment_survived (region)));
 28084| #ifdef MULTIPLE_HEAPS
 28085|     assert (survived <= heap_segment_survived (region));
 28086| #else
 28087|     assert (survived == heap_segment_survived (region));
 28088| #endif //MULTIPLE_HEAPS
 28089| #endif //_DEBUG
 28090|     assert (last_marked_obj_end);
 28091|     save_allocated(region);
 28092|     heap_segment_allocated (region) = last_marked_obj_end;
 28093|     heap_segment_plan_allocated (region) = heap_segment_allocated (region);
 28094|     int plan_gen_num = heap_segment_plan_gen_num (region);
 28095|     if (plan_gen_num < heap_segment_gen_num (region))
 28096|     {
 28097|         generation_allocation_size (generation_of (plan_gen_num)) += heap_segment_survived (region);
 28098|         dprintf (REGIONS_LOG, ("sip: g%d alloc size is now %zd", plan_gen_num,
 28099|             generation_allocation_size (generation_of (plan_gen_num))));
 28100|     }
 28101| }
 28102| inline
 28103| void gc_heap::check_demotion_helper_sip (uint8_t** pval, int parent_gen_num, uint8_t* parent_loc)
 28104| {
 28105|     uint8_t* child_object = *pval;
 28106|     if (!is_in_heap_range (child_object))
 28107|         return;
 28108|     assert (child_object != nullptr);
 28109|     int child_object_plan_gen = get_region_plan_gen_num (child_object);
 28110|     if (child_object_plan_gen < parent_gen_num)
 28111|     {
 28112|         set_card (card_of (parent_loc));
 28113|     }
 28114|     dprintf (3, ("SCS %d, %d", child_object_plan_gen, parent_gen_num));
 28115| }
 28116| heap_segment* gc_heap::relocate_advance_to_non_sip (heap_segment* region)
 28117| {
 28118|     THREAD_FROM_HEAP;
 28119|     heap_segment* current_region = region;
 28120|     dprintf (REGIONS_LOG, ("Relocate searching for next non SIP, starting from %p",
 28121|         (region ? heap_segment_mem (region) : 0)));
 28122|     while (current_region)
 28123|     {
 28124|         if (heap_segment_swept_in_plan (current_region))
 28125|         {
 28126|             int gen_num = heap_segment_gen_num (current_region);
 28127|             int plan_gen_num = heap_segment_plan_gen_num (current_region);
 28128|             bool use_sip_demotion = (plan_gen_num > get_plan_gen_num (gen_num));
 28129|             dprintf (REGIONS_LOG, ("region %p is SIP, relocating, gen %d, plan gen: %d(supposed to be %d) %s",
 28130|                 heap_segment_mem (current_region), gen_num, plan_gen_num, get_plan_gen_num (gen_num),
 28131|                 (use_sip_demotion ? "Sd" : "d")));
 28132|             uint8_t* x = heap_segment_mem (current_region);
 28133|             uint8_t* end = heap_segment_allocated (current_region);
 28134|             while (x < end)
 28135|             {
 28136|                 size_t s = size (x);
 28137|                 assert (s > 0);
 28138|                 uint8_t* next_obj = x + Align (s);
 28139|                 Prefetch (next_obj);
 28140|                 if (!(((CObjectHeader*)x)->IsFree()))
 28141|                 {
 28142|                     if (contain_pointers (x))
 28143|                     {
 28144|                         dprintf (3, ("$%zx$", (size_t)x));
 28145|                         go_through_object_nostart (method_table(x), x, s, pval,
 28146|                         {
 28147|                             uint8_t* child = *pval;
 28148|                             relocate_address (pval THREAD_NUMBER_ARG);
 28149|                             if (use_sip_demotion)
 28150|                                 check_demotion_helper_sip (pval, plan_gen_num, (uint8_t*)pval);
 28151|                             else
 28152|                                 check_demotion_helper (pval, (uint8_t*)pval);
 28153|                             if (child)
 28154|                             {
 28155|                                 dprintf (4444, ("SIP %p(%p)->%p->%p(%p)",
 28156|                                     x, (uint8_t*)pval, child, *pval, method_table (child)));
 28157|                             }
 28158|                         });
 28159|                     }
 28160|                     check_class_object_demotion (x);
 28161|                 }
 28162|                 x = next_obj;
 28163|             }
 28164|         }
 28165|         else
 28166|         {
 28167|             int gen_num = heap_segment_gen_num (current_region);
 28168|             int plan_gen_num = heap_segment_plan_gen_num (current_region);
 28169|             dprintf (REGIONS_LOG, ("region %p is not SIP, relocating, gen %d, plan gen: %d",
 28170|                 heap_segment_mem (current_region), gen_num, plan_gen_num));
 28171|             return current_region;
 28172|         }
 28173|         current_region = heap_segment_next (current_region);
 28174|     }
 28175|     return 0;
 28176| }
 28177| #ifdef STRESS_REGIONS
 28178| void gc_heap::pin_by_gc (uint8_t* object)
 28179| {
 28180|     heap_segment* region = region_of (object);
 28181|     HndAssignHandleGC(pinning_handles_for_alloc[ph_index_per_heap], object);
 28182|     dprintf (REGIONS_LOG, ("h%d pinning object at %zx on eph seg %zx (ph#%d)",
 28183|         heap_number, object, heap_segment_mem (region), ph_index_per_heap));
 28184|     ph_index_per_heap++;
 28185|     if (ph_index_per_heap == PINNING_HANDLE_INITIAL_LENGTH)
 28186|     {
 28187|         ph_index_per_heap = 0;
 28188|     }
 28189| }
 28190| #endif //STRESS_REGIONS
 28191| #endif //USE_REGIONS
 28192| void gc_heap::make_free_lists (int condemned_gen_number)
 28193| {
 28194|     assert (settings.promotion);
 28195|     make_free_args args = {};
 28196|     int stop_gen_idx = get_stop_generation_index (condemned_gen_number);
 28197|     for (int i = condemned_gen_number; i >= stop_gen_idx; i--)
 28198|     {
 28199|         generation* condemned_gen = generation_of (i);
 28200|         heap_segment* current_heap_segment = get_start_segment (condemned_gen);
 28201| #ifdef USE_REGIONS
 28202|     if (!current_heap_segment)
 28203|         continue;
 28204| #endif //USE_REGIONS
 28205|         uint8_t* start_address = get_soh_start_object (current_heap_segment, condemned_gen);
 28206|         size_t current_brick = brick_of (start_address);
 28207|         PREFIX_ASSUME(current_heap_segment != NULL);
 28208|         uint8_t* end_address = heap_segment_allocated (current_heap_segment);
 28209|         size_t  end_brick = brick_of (end_address-1);
 28210|         int current_gen_num = i;
 28211| #ifdef USE_REGIONS
 28212|         args.free_list_gen_number = (special_sweep_p ? current_gen_num : get_plan_gen_num (current_gen_num));
 28213| #else
 28214|         args.free_list_gen_number = get_plan_gen_num (current_gen_num);
 28215| #endif //USE_REGIONS
 28216|         args.free_list_gen = generation_of (args.free_list_gen_number);
 28217|         args.highest_plug = 0;
 28218| #ifdef USE_REGIONS
 28219|         dprintf (REGIONS_LOG, ("starting at gen%d %p -> %p", i, start_address, end_address));
 28220| #else
 28221|         args.current_gen_limit = (((current_gen_num == max_generation)) ?
 28222|                                   MAX_PTR :
 28223|                                   (generation_limit (args.free_list_gen_number)));
 28224| #endif //USE_REGIONS
 28225| #ifndef USE_REGIONS
 28226|         if ((start_address >= end_address) && (condemned_gen_number < max_generation))
 28227|         {
 28228|             break;
 28229|         }
 28230| #endif //!USE_REGIONS
 28231|         while (1)
 28232|         {
 28233|             if ((current_brick > end_brick))
 28234|             {
 28235| #ifndef USE_REGIONS
 28236|                 if (args.current_gen_limit == MAX_PTR)
 28237|                 {
 28238|                     generation* gen = generation_of (max_generation);
 28239|                     heap_segment* start_seg = heap_segment_rw (generation_start_segment (gen));
 28240|                     PREFIX_ASSUME(start_seg != NULL);
 28241|                     uint8_t* gap = heap_segment_mem (start_seg);
 28242|                     generation_allocation_start (gen) = gap;
 28243|                     heap_segment_allocated (start_seg) = gap + Align (min_obj_size);
 28244|                     make_unused_array (gap, Align (min_obj_size));
 28245|                     reset_allocation_pointers (gen, gap);
 28246|                     dprintf (3, ("Start segment empty, fixing generation start of %d to: %zx",
 28247|                                 max_generation, (size_t)gap));
 28248|                     args.current_gen_limit = generation_limit (args.free_list_gen_number);
 28249|                 }
 28250| #endif //!USE_REGIONS
 28251|                 if (heap_segment_next_non_sip (current_heap_segment))
 28252|                 {
 28253|                     current_heap_segment = heap_segment_next_non_sip (current_heap_segment);
 28254|                 }
 28255|                 else
 28256|                 {
 28257|                     break;
 28258|                 }
 28259|                 current_brick = brick_of (heap_segment_mem (current_heap_segment));
 28260|                 end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
 28261|                 continue;
 28262|             }
 28263|             {
 28264|                 int brick_entry =  brick_table [ current_brick ];
 28265|                 if ((brick_entry >= 0))
 28266|                 {
 28267|                     make_free_list_in_brick (brick_address (current_brick) + brick_entry-1, &args);
 28268|                     dprintf(3,("Fixing brick entry %zx to %zx",
 28269|                             current_brick, (size_t)args.highest_plug));
 28270|                     set_brick (current_brick,
 28271|                             (args.highest_plug - brick_address (current_brick)));
 28272|                 }
 28273|                 else
 28274|                 {
 28275|                     if ((brick_entry > -32768))
 28276|                     {
 28277| #ifdef _DEBUG
 28278|                         ptrdiff_t offset = brick_of (args.highest_plug) - current_brick;
 28279|                         if ((brick_entry != -32767) && (! ((offset == brick_entry))))
 28280|                         {
 28281|                             assert ((brick_entry == -1));
 28282|                         }
 28283| #endif //_DEBUG
 28284|                         set_brick (current_brick, -1);
 28285|                     }
 28286|                 }
 28287|             }
 28288|             current_brick++;
 28289|         }
 28290|     }
 28291|     {
 28292| #ifdef USE_REGIONS
 28293|         check_seg_gen_num (generation_allocation_segment (generation_of (max_generation)));
 28294|         thread_final_regions (false);
 28295|         generation* gen_gen0 = generation_of (0);
 28296|         ephemeral_heap_segment = generation_start_segment (gen_gen0);
 28297|         alloc_allocated = heap_segment_allocated (ephemeral_heap_segment);
 28298| #else //USE_REGIONS
 28299|         int bottom_gen = 0;
 28300|         args.free_list_gen_number--;
 28301|         while (args.free_list_gen_number >= bottom_gen)
 28302|         {
 28303|             uint8_t*  gap = 0;
 28304|             generation* gen2 = generation_of (args.free_list_gen_number);
 28305|             gap = allocate_at_end (Align(min_obj_size));
 28306|             generation_allocation_start (gen2) = gap;
 28307|             reset_allocation_pointers (gen2, gap);
 28308|             dprintf(3,("Fixing generation start of %d to: %zx",
 28309|                        args.free_list_gen_number, (size_t)gap));
 28310|             PREFIX_ASSUME(gap != NULL);
 28311|             make_unused_array (gap, Align (min_obj_size));
 28312|             args.free_list_gen_number--;
 28313|         }
 28314|         uint8_t* start2 = generation_allocation_start (youngest_generation);
 28315|         alloc_allocated = start2 + Align (size (start2));
 28316| #endif //USE_REGIONS
 28317|     }
 28318| }
 28319| void gc_heap::make_free_list_in_brick (uint8_t* tree, make_free_args* args)
 28320| {
 28321|     assert ((tree != NULL));
 28322|     {
 28323|         int  right_node = node_right_child (tree);
 28324|         int left_node = node_left_child (tree);
 28325|         args->highest_plug = 0;
 28326|         if (! (0 == tree))
 28327|         {
 28328|             if (! (0 == left_node))
 28329|             {
 28330|                 make_free_list_in_brick (tree + left_node, args);
 28331|             }
 28332|             {
 28333|                 uint8_t*  plug = tree;
 28334|                 size_t  gap_size = node_gap_size (tree);
 28335|                 uint8_t*  gap = (plug - gap_size);
 28336|                 args->highest_plug = tree;
 28337|                 dprintf (3,("plug: %p (highest p: %p), free %zx len %zd in %d",
 28338|                         plug, args->highest_plug, (size_t)gap, gap_size, args->free_list_gen_number));
 28339| #ifdef SHORT_PLUGS
 28340|                 if (is_plug_padded (plug))
 28341|                 {
 28342|                     dprintf (3, ("%p padded", plug));
 28343|                     clear_plug_padded (plug);
 28344|                 }
 28345| #endif //SHORT_PLUGS
 28346| #ifdef DOUBLY_LINKED_FL
 28347|                 if (is_plug_bgc_mark_bit_set (plug))
 28348|                 {
 28349|                     dprintf (3333, ("cbgcm: %p", plug));
 28350|                     clear_plug_bgc_mark_bit (plug);
 28351|                 }
 28352|                 if (is_free_obj_in_compact_bit_set (plug))
 28353|                 {
 28354|                     dprintf (3333, ("cfoc: %p", plug));
 28355|                     clear_free_obj_in_compact_bit (plug);
 28356|                 }
 28357| #endif //DOUBLY_LINKED_FL
 28358| #ifndef USE_REGIONS
 28359|             gen_crossing:
 28360|                 {
 28361|                     if ((args->current_gen_limit == MAX_PTR) ||
 28362|                         ((plug >= args->current_gen_limit) &&
 28363|                          ephemeral_pointer_p (plug)))
 28364|                     {
 28365|                         dprintf(3,(" Crossing Generation boundary at %zx",
 28366|                                (size_t)args->current_gen_limit));
 28367|                         if (!(args->current_gen_limit == MAX_PTR))
 28368|                         {
 28369|                             args->free_list_gen_number--;
 28370|                             args->free_list_gen = generation_of (args->free_list_gen_number);
 28371|                         }
 28372|                         dprintf(3,( " Fixing generation start of %d to: %zx",
 28373|                                 args->free_list_gen_number, (size_t)gap));
 28374|                         reset_allocation_pointers (args->free_list_gen, gap);
 28375|                         args->current_gen_limit = generation_limit (args->free_list_gen_number);
 28376|                         if ((gap_size >= (2*Align (min_obj_size))))
 28377|                         {
 28378|                             dprintf(3,(" Splitting the gap in two %zd left",
 28379|                                    gap_size));
 28380|                             make_unused_array (gap, Align(min_obj_size));
 28381|                             gap_size = (gap_size - Align(min_obj_size));
 28382|                             gap = (gap + Align(min_obj_size));
 28383|                         }
 28384|                         else
 28385|                         {
 28386|                             make_unused_array (gap, gap_size);
 28387|                             gap_size = 0;
 28388|                         }
 28389|                         goto gen_crossing;
 28390|                     }
 28391|                 }
 28392| #endif //!USE_REGIONS
 28393|                 thread_gap (gap, gap_size, args->free_list_gen);
 28394|                 add_gen_free (args->free_list_gen->gen_num, gap_size);
 28395|             }
 28396|             if (! (0 == right_node))
 28397|             {
 28398|                 make_free_list_in_brick (tree + right_node, args);
 28399|             }
 28400|         }
 28401|     }
 28402| }
 28403| void gc_heap::thread_gap (uint8_t* gap_start, size_t size, generation*  gen)
 28404| {
 28405| #ifndef USE_REGIONS
 28406|     assert (generation_allocation_start (gen));
 28407| #endif
 28408|     if ((size > 0))
 28409|     {
 28410| #ifndef USE_REGIONS
 28411|         assert ((heap_segment_rw (generation_start_segment (gen)) != ephemeral_heap_segment) ||
 28412|                 (gap_start > generation_allocation_start (gen)));
 28413| #endif //USE_REGIONS
 28414|         assert (size >= Align (min_obj_size));
 28415|         make_unused_array (gap_start, size,
 28416|                           (!settings.concurrent && (gen != youngest_generation)),
 28417|                           (gen->gen_num == max_generation));
 28418|         dprintf (3, ("fr: [%zx, %zx[", (size_t)gap_start, (size_t)gap_start+size));
 28419|         if ((size >= min_free_list))
 28420|         {
 28421|             generation_free_list_space (gen) += size;
 28422|             generation_allocator (gen)->thread_item (gap_start, size);
 28423|         }
 28424|         else
 28425|         {
 28426|             generation_free_obj_space (gen) += size;
 28427|         }
 28428|     }
 28429| }
 28430| void gc_heap::uoh_thread_gap_front (uint8_t* gap_start, size_t size, generation*  gen)
 28431| {
 28432| #ifndef USE_REGIONS
 28433|     assert (generation_allocation_start (gen));
 28434| #endif
 28435|     if (size >= min_free_list)
 28436|     {
 28437|         generation_free_list_space (gen) += size;
 28438|         generation_allocator (gen)->thread_item_front (gap_start, size);
 28439|     }
 28440| }
 28441| void gc_heap::make_unused_array (uint8_t* x, size_t size, BOOL clearp, BOOL resetp)
 28442| {
 28443|     dprintf (3, (ThreadStressLog::gcMakeUnusedArrayMsg(),
 28444|         (size_t)x, (size_t)(x+size)));
 28445|     assert (size >= Align (min_obj_size));
 28446|     if (resetp)
 28447|     {
 28448| #ifdef BGC_SERVO_TUNING
 28449|         if (!(bgc_tuning::enable_fl_tuning && bgc_tuning::fl_tuning_triggered))
 28450| #endif //BGC_SERVO_TUNING
 28451|         {
 28452|             reset_memory (x, size);
 28453|         }
 28454|     }
 28455|     ((CObjectHeader*)x)->SetFree(size);
 28456| #ifdef HOST_64BIT
 28457| #if BIGENDIAN
 28458| #error "This won't work on big endian platforms"
 28459| #endif
 28460|     size_t size_as_object = (uint32_t)(size - free_object_base_size) + free_object_base_size;
 28461|     if (size_as_object < size)
 28462|     {
 28463|         uint8_t * tmp = x + size_as_object;
 28464|         size_t remaining_size = size - size_as_object;
 28465|         while (remaining_size > UINT32_MAX)
 28466|         {
 28467|             size_t current_size = UINT32_MAX - get_alignment_constant (FALSE)
 28468|                 - Align (min_obj_size, get_alignment_constant (FALSE));
 28469|             ((CObjectHeader*)tmp)->SetFree(current_size);
 28470|             remaining_size -= current_size;
 28471|             tmp += current_size;
 28472|         }
 28473|         ((CObjectHeader*)tmp)->SetFree(remaining_size);
 28474|     }
 28475| #endif
 28476|     if (clearp)
 28477|         clear_card_for_addresses (x, x + Align(size));
 28478| }
 28479| void gc_heap::clear_unused_array (uint8_t* x, size_t size)
 28480| {
 28481|     *(((PTR_PTR)x)-1) = 0;
 28482|     ((CObjectHeader*)x)->UnsetFree();
 28483| #ifdef HOST_64BIT
 28484| #if BIGENDIAN
 28485| #error "This won't work on big endian platforms"
 28486| #endif
 28487|     size_t size_as_object = (uint32_t)(size - free_object_base_size) + free_object_base_size;
 28488|     if (size_as_object < size)
 28489|     {
 28490|         uint8_t * tmp = x + size_as_object;
 28491|         size_t remaining_size = size - size_as_object;
 28492|         while (remaining_size > UINT32_MAX)
 28493|         {
 28494|             size_t current_size = UINT32_MAX - get_alignment_constant (FALSE)
 28495|                 - Align (min_obj_size, get_alignment_constant (FALSE));
 28496|             ((CObjectHeader*)tmp)->UnsetFree();
 28497|             remaining_size -= current_size;
 28498|             tmp += current_size;
 28499|         }
 28500|         ((CObjectHeader*)tmp)->UnsetFree();
 28501|     }
 28502| #else
 28503|     UNREFERENCED_PARAMETER(size);
 28504| #endif
 28505| }
 28506| inline
 28507| uint8_t* tree_search (uint8_t* tree, uint8_t* old_address)
 28508| {
 28509|     uint8_t* candidate = 0;
 28510|     int cn;
 28511|     while (1)
 28512|     {
 28513|         if (tree < old_address)
 28514|         {
 28515|             if ((cn = node_right_child (tree)) != 0)
 28516|             {
 28517|                 assert (candidate < tree);
 28518|                 candidate = tree;
 28519|                 tree = tree + cn;
 28520|                 Prefetch (&((plug_and_pair*)tree)[-1].m_pair.left);
 28521|                 continue;
 28522|             }
 28523|             else
 28524|                 break;
 28525|         }
 28526|         else if (tree > old_address)
 28527|         {
 28528|             if ((cn = node_left_child (tree)) != 0)
 28529|             {
 28530|                 tree = tree + cn;
 28531|                 Prefetch (&((plug_and_pair*)tree)[-1].m_pair.left);
 28532|                 continue;
 28533|             }
 28534|             else
 28535|                 break;
 28536|         } else
 28537|             break;
 28538|     }
 28539|     if (tree <= old_address)
 28540|         return tree;
 28541|     else if (candidate)
 28542|         return candidate;
 28543|     else
 28544|         return tree;
 28545| }
 28546| void gc_heap::relocate_address (uint8_t** pold_address THREAD_NUMBER_DCL)
 28547| {
 28548|     uint8_t* old_address = *pold_address;
 28549| #ifdef USE_REGIONS
 28550|     if (!is_in_gc_range (old_address) || !should_check_brick_for_reloc (old_address))
 28551|     {
 28552|         return;
 28553|     }
 28554| #else //USE_REGIONS
 28555|     if (!((old_address >= gc_low) && (old_address < gc_high)))
 28556| #ifdef MULTIPLE_HEAPS
 28557|     {
 28558|         UNREFERENCED_PARAMETER(thread);
 28559|         if (old_address == 0)
 28560|             return;
 28561|         gc_heap* hp = heap_of (old_address);
 28562|         if ((hp == this) ||
 28563|             !((old_address >= hp->gc_low) && (old_address < hp->gc_high)))
 28564|             return;
 28565|     }
 28566| #else //MULTIPLE_HEAPS
 28567|         return ;
 28568| #endif //MULTIPLE_HEAPS
 28569| #endif //USE_REGIONS
 28570|     size_t  brick = brick_of (old_address);
 28571|     int    brick_entry =  brick_table [ brick ];
 28572|     uint8_t*  new_address = old_address;
 28573|     if (! ((brick_entry == 0)))
 28574|     {
 28575|     retry:
 28576|         {
 28577|             while (brick_entry < 0)
 28578|             {
 28579|                 brick = (brick + brick_entry);
 28580|                 brick_entry =  brick_table [ brick ];
 28581|             }
 28582|             uint8_t* old_loc = old_address;
 28583|             uint8_t* node = tree_search ((brick_address (brick) + brick_entry-1),
 28584|                                       old_loc);
 28585|             if ((node <= old_loc))
 28586|                 new_address = (old_address + node_relocation_distance (node));
 28587|             else
 28588|             {
 28589|                 if (node_left_p (node))
 28590|                 {
 28591|                     dprintf(3,(" L: %zx", (size_t)node));
 28592|                     new_address = (old_address +
 28593|                                    (node_relocation_distance (node) +
 28594|                                     node_gap_size (node)));
 28595|                 }
 28596|                 else
 28597|                 {
 28598|                     brick = brick - 1;
 28599|                     brick_entry =  brick_table [ brick ];
 28600|                     goto retry;
 28601|                 }
 28602|             }
 28603|         }
 28604|         dprintf (4, (ThreadStressLog::gcRelocateReferenceMsg(), pold_address, old_address, new_address));
 28605|         *pold_address = new_address;
 28606|         return;
 28607|     }
 28608| #ifdef FEATURE_LOH_COMPACTION
 28609|     if (settings.loh_compaction)
 28610|     {
 28611|         heap_segment* pSegment = seg_mapping_table_segment_of ((uint8_t*)old_address);
 28612| #ifdef USE_REGIONS
 28613|         if (!pSegment)
 28614|         {
 28615|             return;
 28616|         }
 28617| #endif //USE_REGIONS
 28618| #ifdef MULTIPLE_HEAPS
 28619|         if (heap_segment_heap (pSegment)->loh_compacted_p)
 28620| #else
 28621|         if (loh_compacted_p)
 28622| #endif
 28623|         {
 28624|             size_t flags = pSegment->flags;
 28625|             if ((flags & heap_segment_flags_loh)
 28626| #ifdef FEATURE_BASICFREEZE
 28627|                 && !(flags & heap_segment_flags_readonly)
 28628| #endif
 28629|                 )
 28630|             {
 28631|                 new_address = old_address + loh_node_relocation_distance (old_address);
 28632|                 dprintf (4, (ThreadStressLog::gcRelocateReferenceMsg(), pold_address, old_address, new_address));
 28633|                 *pold_address = new_address;
 28634|             }
 28635|         }
 28636|     }
 28637| #endif //FEATURE_LOH_COMPACTION
 28638| }
 28639| inline void
 28640| gc_heap::check_class_object_demotion (uint8_t* obj)
 28641| {
 28642| #ifdef COLLECTIBLE_CLASS
 28643|     if (is_collectible(obj))
 28644|     {
 28645|         check_class_object_demotion_internal (obj);
 28646|     }
 28647| #else
 28648|     UNREFERENCED_PARAMETER(obj);
 28649| #endif //COLLECTIBLE_CLASS
 28650| }
 28651| #ifdef COLLECTIBLE_CLASS
 28652| NOINLINE void
 28653| gc_heap::check_class_object_demotion_internal (uint8_t* obj)
 28654| {
 28655|     if (settings.demotion)
 28656|     {
 28657| #ifdef MULTIPLE_HEAPS
 28658|         set_card (card_of (obj));
 28659| #else
 28660|         THREAD_FROM_HEAP;
 28661|         uint8_t* class_obj = get_class_object (obj);
 28662|         dprintf (3, ("%p: got classobj %p", obj, class_obj));
 28663|         uint8_t* temp_class_obj = class_obj;
 28664|         uint8_t** temp = &temp_class_obj;
 28665|         relocate_address (temp THREAD_NUMBER_ARG);
 28666|         check_demotion_helper (temp, obj);
 28667| #endif //MULTIPLE_HEAPS
 28668|     }
 28669| }
 28670| #endif //COLLECTIBLE_CLASS
 28671| inline void
 28672| gc_heap::check_demotion_helper (uint8_t** pval, uint8_t* parent_obj)
 28673| {
 28674| #ifdef USE_REGIONS
 28675|     uint8_t* child_object = *pval;
 28676|     if (!is_in_heap_range (child_object))
 28677|         return;
 28678|     int child_object_plan_gen = get_region_plan_gen_num (child_object);
 28679|     bool child_obj_demoted_p = is_region_demoted (child_object);
 28680|     if (child_obj_demoted_p)
 28681|     {
 28682|         set_card (card_of (parent_obj));
 28683|     }
 28684|     dprintf (3, ("SC %d (%s)", child_object_plan_gen, (child_obj_demoted_p ? "D" : "ND")));
 28685| #else //USE_REGIONS
 28686|     if ((*pval < demotion_high) &&
 28687|         (*pval >= demotion_low))
 28688|     {
 28689|         dprintf(3, ("setting card %zx:%zx",
 28690|                     card_of((uint8_t*)pval),
 28691|                     (size_t)pval));
 28692|         set_card (card_of (parent_obj));
 28693|     }
 28694| #ifdef MULTIPLE_HEAPS
 28695|     else if (settings.demotion)
 28696|     {
 28697|         dprintf (4, ("Demotion active, computing heap_of object"));
 28698|         gc_heap* hp = heap_of (*pval);
 28699|         if ((*pval < hp->demotion_high) &&
 28700|             (*pval >= hp->demotion_low))
 28701|         {
 28702|             dprintf(3, ("setting card %zx:%zx",
 28703|                         card_of((uint8_t*)pval),
 28704|                         (size_t)pval));
 28705|             set_card (card_of (parent_obj));
 28706|         }
 28707|     }
 28708| #endif //MULTIPLE_HEAPS
 28709| #endif //USE_REGIONS
 28710| }
 28711| inline void
 28712| gc_heap::reloc_survivor_helper (uint8_t** pval)
 28713| {
 28714|     THREAD_FROM_HEAP;
 28715|     relocate_address (pval THREAD_NUMBER_ARG);
 28716|     check_demotion_helper (pval, (uint8_t*)pval);
 28717| }
 28718| inline void
 28719| gc_heap::relocate_obj_helper (uint8_t* x, size_t s)
 28720| {
 28721|     THREAD_FROM_HEAP;
 28722|     if (contain_pointers (x))
 28723|     {
 28724|         dprintf (3, ("o$%zx$", (size_t)x));
 28725|         go_through_object_nostart (method_table(x), x, s, pval,
 28726|                             {
 28727|                                 uint8_t* child = *pval;
 28728|                                 reloc_survivor_helper (pval);
 28729|                                 if (child)
 28730|                                 {
 28731|                                     dprintf (3, ("%p->%p->%p", (uint8_t*)pval, child, *pval));
 28732|                                 }
 28733|                             });
 28734|     }
 28735|     check_class_object_demotion (x);
 28736| }
 28737| inline
 28738| void gc_heap::reloc_ref_in_shortened_obj (uint8_t** address_to_set_card, uint8_t** address_to_reloc)
 28739| {
 28740|     THREAD_FROM_HEAP;
 28741|     uint8_t* old_val = (address_to_reloc ? *address_to_reloc : 0);
 28742|     relocate_address (address_to_reloc THREAD_NUMBER_ARG);
 28743|     if (address_to_reloc)
 28744|     {
 28745|         dprintf (3, ("SR %p: %p->%p", (uint8_t*)address_to_reloc, old_val, *address_to_reloc));
 28746|     }
 28747|     check_demotion_helper (address_to_reloc, (uint8_t*)address_to_set_card);
 28748| }
 28749| void gc_heap::relocate_pre_plug_info (mark* pinned_plug_entry)
 28750| {
 28751|     THREAD_FROM_HEAP;
 28752|     uint8_t* plug = pinned_plug (pinned_plug_entry);
 28753|     uint8_t* pre_plug_start = plug - sizeof (plug_and_gap);
 28754|     pre_plug_start += sizeof (uint8_t*);
 28755|     uint8_t** old_address = &pre_plug_start;
 28756|     uint8_t* old_val = (old_address ? *old_address : 0);
 28757|     relocate_address (old_address THREAD_NUMBER_ARG);
 28758|     if (old_address)
 28759|     {
 28760|         dprintf (3, ("PreR %p: %p->%p, set reloc: %p",
 28761|             (uint8_t*)old_address, old_val, *old_address, (pre_plug_start - sizeof (uint8_t*))));
 28762|     }
 28763|     pinned_plug_entry->set_pre_plug_info_reloc_start (pre_plug_start - sizeof (uint8_t*));
 28764| }
 28765| inline
 28766| void gc_heap::relocate_shortened_obj_helper (uint8_t* x, size_t s, uint8_t* end, mark* pinned_plug_entry, BOOL is_pinned)
 28767| {
 28768|     THREAD_FROM_HEAP;
 28769|     uint8_t* plug = pinned_plug (pinned_plug_entry);
 28770|     if (!is_pinned)
 28771|     {
 28772|         relocate_pre_plug_info (pinned_plug_entry);
 28773|     }
 28774|     verify_pins_with_post_plug_info("after relocate_pre_plug_info");
 28775|     uint8_t* saved_plug_info_start = 0;
 28776|     uint8_t** saved_info_to_relocate = 0;
 28777|     if (is_pinned)
 28778|     {
 28779|         saved_plug_info_start = (uint8_t*)(pinned_plug_entry->get_post_plug_info_start());
 28780|         saved_info_to_relocate = (uint8_t**)(pinned_plug_entry->get_post_plug_reloc_info());
 28781|     }
 28782|     else
 28783|     {
 28784|         saved_plug_info_start = (plug - sizeof (plug_and_gap));
 28785|         saved_info_to_relocate = (uint8_t**)(pinned_plug_entry->get_pre_plug_reloc_info());
 28786|     }
 28787|     uint8_t** current_saved_info_to_relocate = 0;
 28788|     uint8_t* child = 0;
 28789|     dprintf (3, ("x: %p, pp: %p, end: %p", x, plug, end));
 28790|     if (contain_pointers (x))
 28791|     {
 28792|         dprintf (3,("s$%zx$", (size_t)x));
 28793|         go_through_object_nostart (method_table(x), x, s, pval,
 28794|         {
 28795|             dprintf (3, ("obj %p, member: %p->%p", x, (uint8_t*)pval, *pval));
 28796|             if ((uint8_t*)pval >= end)
 28797|             {
 28798|                 current_saved_info_to_relocate = saved_info_to_relocate + ((uint8_t*)pval - saved_plug_info_start) / sizeof (uint8_t**);
 28799|                 child = *current_saved_info_to_relocate;
 28800|                 reloc_ref_in_shortened_obj (pval, current_saved_info_to_relocate);
 28801|                 dprintf (3, ("last part: R-%p(saved: %p)->%p ->%p",
 28802|                     (uint8_t*)pval, current_saved_info_to_relocate, child, *current_saved_info_to_relocate));
 28803|             }
 28804|             else
 28805|             {
 28806|                 reloc_survivor_helper (pval);
 28807|             }
 28808|         });
 28809|     }
 28810|     check_class_object_demotion (x);
 28811| }
 28812| void gc_heap::relocate_survivor_helper (uint8_t* plug, uint8_t* plug_end)
 28813| {
 28814|     uint8_t*  x = plug;
 28815|     while (x < plug_end)
 28816|     {
 28817|         size_t s = size (x);
 28818|         uint8_t* next_obj = x + Align (s);
 28819|         Prefetch (next_obj);
 28820|         relocate_obj_helper (x, s);
 28821|         assert (s > 0);
 28822|         x = next_obj;
 28823|     }
 28824| }
 28825| void gc_heap::verify_pins_with_post_plug_info (const char* msg)
 28826| {
 28827| #if defined (_DEBUG) && defined (VERIFY_HEAP)
 28828|     if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
 28829|     {
 28830|         if (!verify_pinned_queue_p)
 28831|             return;
 28832|         if (settings.heap_expansion)
 28833|             return;
 28834|         for (size_t i = 0; i < mark_stack_tos; i++)
 28835|         {
 28836|             mark& m = mark_stack_array[i];
 28837|             mark* pinned_plug_entry = pinned_plug_of(i);
 28838|             if (pinned_plug_entry->has_post_plug_info() &&
 28839|                 pinned_plug_entry->post_short_p() &&
 28840|                 (pinned_plug_entry->saved_post_plug_debug.gap != 1))
 28841|             {
 28842|                 uint8_t* next_obj = pinned_plug_entry->get_post_plug_info_start() + sizeof (plug_and_gap);
 28843|                 dprintf (3, ("OFP: %p, G: %zx, R: %zx, LC: %d, RC: %d",
 28844|                     next_obj, node_gap_size (next_obj), node_relocation_distance (next_obj),
 28845|                     (int)node_left_child (next_obj), (int)node_right_child (next_obj)));
 28846|                 size_t* post_plug_debug = (size_t*)(&m.saved_post_plug_debug);
 28847|                 if (node_gap_size (next_obj) != *post_plug_debug)
 28848|                 {
 28849|                     dprintf (1, ("obj: %p gap should be %zx but it is %zx",
 28850|                         next_obj, *post_plug_debug, (size_t)(node_gap_size (next_obj))));
 28851|                     FATAL_GC_ERROR();
 28852|                 }
 28853|                 post_plug_debug++;
 28854|                 if (*((size_t*)(next_obj - 3 * sizeof (size_t))) != *post_plug_debug)
 28855|                 {
 28856|                     dprintf (1, ("obj: %p reloc should be %zx but it is %zx",
 28857|                         next_obj, *post_plug_debug, (size_t)(node_relocation_distance (next_obj))));
 28858|                     FATAL_GC_ERROR();
 28859|                 }
 28860|                 if (node_left_child (next_obj) > 0)
 28861|                 {
 28862|                     dprintf (1, ("obj: %p, vLC: %d\n", next_obj, (int)(node_left_child (next_obj))));
 28863|                     FATAL_GC_ERROR();
 28864|                 }
 28865|             }
 28866|         }
 28867|         dprintf (3, ("%s verified", msg));
 28868|     }
 28869| #else
 28870|     UNREFERENCED_PARAMETER(msg);
 28871| #endif // _DEBUG && VERIFY_HEAP
 28872| }
 28873| #ifdef COLLECTIBLE_CLASS
 28874| inline void
 28875| gc_heap::unconditional_set_card_collectible (uint8_t* obj)
 28876| {
 28877|     if (settings.demotion)
 28878|     {
 28879|         set_card (card_of (obj));
 28880|     }
 28881| }
 28882| #endif //COLLECTIBLE_CLASS
 28883| void gc_heap::relocate_shortened_survivor_helper (uint8_t* plug, uint8_t* plug_end, mark* pinned_plug_entry)
 28884| {
 28885|     uint8_t*  x = plug;
 28886|     uint8_t* p_plug = pinned_plug (pinned_plug_entry);
 28887|     BOOL is_pinned = (plug == p_plug);
 28888|     BOOL check_short_obj_p = (is_pinned ? pinned_plug_entry->post_short_p() : pinned_plug_entry->pre_short_p());
 28889|     plug_end += sizeof (gap_reloc_pair);
 28890|     dprintf (3, ("%s %p-%p short, LO: %s OW", (is_pinned ? "PP" : "NP"), plug, plug_end, (check_short_obj_p ? "is" : "is not")));
 28891|     verify_pins_with_post_plug_info("begin reloc short surv");
 28892|     while (x < plug_end)
 28893|     {
 28894|         if (check_short_obj_p && ((DWORD)(plug_end - x) < (DWORD)min_pre_pin_obj_size))
 28895|         {
 28896|             dprintf (3, ("last obj %p is short", x));
 28897|             if (is_pinned)
 28898|             {
 28899| #ifdef COLLECTIBLE_CLASS
 28900|                 if (pinned_plug_entry->post_short_collectible_p())
 28901|                     unconditional_set_card_collectible (x);
 28902| #endif //COLLECTIBLE_CLASS
 28903|                 uint8_t** saved_plug_info_start = (uint8_t**)(pinned_plug_entry->get_post_plug_info_start());
 28904|                 uint8_t** saved_info_to_relocate = (uint8_t**)(pinned_plug_entry->get_post_plug_reloc_info());
 28905|                 for (size_t i = 0; i < pinned_plug_entry->get_max_short_bits(); i++)
 28906|                 {
 28907|                     if (pinned_plug_entry->post_short_bit_p (i))
 28908|                     {
 28909|                         reloc_ref_in_shortened_obj ((saved_plug_info_start + i), (saved_info_to_relocate + i));
 28910|                     }
 28911|                 }
 28912|             }
 28913|             else
 28914|             {
 28915| #ifdef COLLECTIBLE_CLASS
 28916|                 if (pinned_plug_entry->pre_short_collectible_p())
 28917|                     unconditional_set_card_collectible (x);
 28918| #endif //COLLECTIBLE_CLASS
 28919|                 relocate_pre_plug_info (pinned_plug_entry);
 28920|                 uint8_t** saved_plug_info_start = (uint8_t**)(p_plug - sizeof (plug_and_gap));
 28921|                 uint8_t** saved_info_to_relocate = (uint8_t**)(pinned_plug_entry->get_pre_plug_reloc_info());
 28922|                 for (size_t i = 0; i < pinned_plug_entry->get_max_short_bits(); i++)
 28923|                 {
 28924|                     if (pinned_plug_entry->pre_short_bit_p (i))
 28925|                     {
 28926|                         reloc_ref_in_shortened_obj ((saved_plug_info_start + i), (saved_info_to_relocate + i));
 28927|                     }
 28928|                 }
 28929|             }
 28930|             break;
 28931|         }
 28932|         size_t s = size (x);
 28933|         uint8_t* next_obj = x + Align (s);
 28934|         Prefetch (next_obj);
 28935|         if (next_obj >= plug_end)
 28936|         {
 28937|             dprintf (3, ("object %p is at the end of the plug %p->%p",
 28938|                 next_obj, plug, plug_end));
 28939|             verify_pins_with_post_plug_info("before reloc short obj");
 28940|             relocate_shortened_obj_helper (x, s, (x + Align (s) - sizeof (plug_and_gap)), pinned_plug_entry, is_pinned);
 28941|         }
 28942|         else
 28943|         {
 28944|             relocate_obj_helper (x, s);
 28945|         }
 28946|         assert (s > 0);
 28947|         x = next_obj;
 28948|     }
 28949|     verify_pins_with_post_plug_info("end reloc short surv");
 28950| }
 28951| void gc_heap::relocate_survivors_in_plug (uint8_t* plug, uint8_t* plug_end,
 28952|                                           BOOL check_last_object_p,
 28953|                                           mark* pinned_plug_entry)
 28954| {
 28955|     dprintf (3,("RP: [%zx(%zx->%zx),%zx(%zx->%zx)[",
 28956|         (size_t)plug, brick_of (plug), (size_t)brick_table[brick_of (plug)],
 28957|         (size_t)plug_end, brick_of (plug_end), (size_t)brick_table[brick_of (plug_end)]));
 28958|     if (check_last_object_p)
 28959|     {
 28960|         relocate_shortened_survivor_helper (plug, plug_end, pinned_plug_entry);
 28961|     }
 28962|     else
 28963|     {
 28964|         relocate_survivor_helper (plug, plug_end);
 28965|     }
 28966| }
 28967| void gc_heap::relocate_survivors_in_brick (uint8_t* tree, relocate_args* args)
 28968| {
 28969|     assert ((tree != NULL));
 28970|     dprintf (3, ("tree: %p, args->last_plug: %p, left: %p, right: %p, gap(t): %zx",
 28971|         tree, args->last_plug,
 28972|         (tree + node_left_child (tree)),
 28973|         (tree + node_right_child (tree)),
 28974|         node_gap_size (tree)));
 28975|     if (node_left_child (tree))
 28976|     {
 28977|         relocate_survivors_in_brick (tree + node_left_child (tree), args);
 28978|     }
 28979|     {
 28980|         uint8_t*  plug = tree;
 28981|         BOOL   has_post_plug_info_p = FALSE;
 28982|         BOOL   has_pre_plug_info_p = FALSE;
 28983|         if (tree == oldest_pinned_plug)
 28984|         {
 28985|             args->pinned_plug_entry = get_oldest_pinned_entry (&has_pre_plug_info_p,
 28986|                                                                &has_post_plug_info_p);
 28987|             assert (tree == pinned_plug (args->pinned_plug_entry));
 28988|             dprintf (3, ("tree is the oldest pin: %p", tree));
 28989|         }
 28990|         if (args->last_plug)
 28991|         {
 28992|             size_t  gap_size = node_gap_size (tree);
 28993|             uint8_t*  gap = (plug - gap_size);
 28994|             dprintf (3, ("tree: %p, gap: %p (%zx)", tree, gap, gap_size));
 28995|             assert (gap_size >= Align (min_obj_size));
 28996|             uint8_t*  last_plug_end = gap;
 28997|             BOOL check_last_object_p = (args->is_shortened || has_pre_plug_info_p);
 28998|             {
 28999|                 relocate_survivors_in_plug (args->last_plug, last_plug_end, check_last_object_p, args->pinned_plug_entry);
 29000|             }
 29001|         }
 29002|         else
 29003|         {
 29004|             assert (!has_pre_plug_info_p);
 29005|         }
 29006|         args->last_plug = plug;
 29007|         args->is_shortened = has_post_plug_info_p;
 29008|         if (has_post_plug_info_p)
 29009|         {
 29010|             dprintf (3, ("setting %p as shortened", plug));
 29011|         }
 29012|         dprintf (3, ("last_plug: %p(shortened: %d)", plug, (args->is_shortened ? 1 : 0)));
 29013|     }
 29014|     if (node_right_child (tree))
 29015|     {
 29016|         relocate_survivors_in_brick (tree + node_right_child (tree), args);
 29017|     }
 29018| }
 29019| inline
 29020| void gc_heap::update_oldest_pinned_plug()
 29021| {
 29022|     oldest_pinned_plug = (pinned_plug_que_empty_p() ? 0 : pinned_plug (oldest_pin()));
 29023| }
 29024| heap_segment* gc_heap::get_start_segment (generation* gen)
 29025| {
 29026|     heap_segment* start_heap_segment = heap_segment_rw (generation_start_segment (gen));
 29027| #ifdef USE_REGIONS
 29028|     heap_segment* current_heap_segment = heap_segment_non_sip (start_heap_segment);
 29029|     if (current_heap_segment != start_heap_segment)
 29030|     {
 29031|         dprintf (REGIONS_LOG, ("h%d skipped gen%d SIP regions, start %p->%p",
 29032|             heap_number,
 29033|             (current_heap_segment ? heap_segment_gen_num (current_heap_segment) : -1),
 29034|             heap_segment_mem (start_heap_segment),
 29035|             (current_heap_segment ? heap_segment_mem (current_heap_segment) : 0)));
 29036|     }
 29037|     start_heap_segment = current_heap_segment;
 29038| #endif //USE_REGIONS
 29039|     return start_heap_segment;
 29040| }
 29041| void gc_heap::relocate_survivors (int condemned_gen_number,
 29042|                                   uint8_t* first_condemned_address)
 29043| {
 29044|     reset_pinned_queue_bos();
 29045|     update_oldest_pinned_plug();
 29046|     int stop_gen_idx = get_stop_generation_index (condemned_gen_number);
 29047| #ifndef USE_REGIONS
 29048|     assert (first_condemned_address == generation_allocation_start (generation_of (condemned_gen_number)));
 29049| #endif //!USE_REGIONS
 29050|     for (int i = condemned_gen_number; i >= stop_gen_idx; i--)
 29051|     {
 29052|         generation* condemned_gen = generation_of (i);
 29053|         heap_segment* current_heap_segment = heap_segment_rw (generation_start_segment (condemned_gen));
 29054| #ifdef USE_REGIONS
 29055|         current_heap_segment = relocate_advance_to_non_sip (current_heap_segment);
 29056|         if (!current_heap_segment)
 29057|             continue;
 29058| #endif //USE_REGIONS
 29059|         uint8_t*  start_address = get_soh_start_object (current_heap_segment, condemned_gen);
 29060|         size_t  current_brick = brick_of (start_address);
 29061|         PREFIX_ASSUME(current_heap_segment != NULL);
 29062|         uint8_t*  end_address = heap_segment_allocated (current_heap_segment);
 29063|         size_t  end_brick = brick_of (end_address - 1);
 29064|         relocate_args args;
 29065|         args.is_shortened = FALSE;
 29066|         args.pinned_plug_entry = 0;
 29067|         args.last_plug = 0;
 29068|         while (1)
 29069|         {
 29070|             if (current_brick > end_brick)
 29071|             {
 29072|                 if (args.last_plug)
 29073|                 {
 29074|                     {
 29075|                         assert (!(args.is_shortened));
 29076|                         relocate_survivors_in_plug (args.last_plug,
 29077|                                                     heap_segment_allocated (current_heap_segment),
 29078|                                                     args.is_shortened,
 29079|                                                     args.pinned_plug_entry);
 29080|                     }
 29081|                     args.last_plug = 0;
 29082|                 }
 29083|                 heap_segment* next_heap_segment = heap_segment_next (current_heap_segment);
 29084|                 if (next_heap_segment)
 29085|                 {
 29086| #ifdef USE_REGIONS
 29087|                     next_heap_segment = relocate_advance_to_non_sip (next_heap_segment);
 29088| #endif //USE_REGIONS
 29089|                     if (next_heap_segment)
 29090|                     {
 29091|                         current_heap_segment = next_heap_segment;
 29092|                         current_brick = brick_of (heap_segment_mem (current_heap_segment));
 29093|                         end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
 29094|                         continue;
 29095|                     }
 29096|                     else
 29097|                         break;
 29098|                 }
 29099|                 else
 29100|                 {
 29101|                     break;
 29102|                 }
 29103|             }
 29104|             {
 29105|                 int brick_entry =  brick_table [ current_brick ];
 29106|                 if (brick_entry >= 0)
 29107|                 {
 29108|                     relocate_survivors_in_brick (brick_address (current_brick) +
 29109|                                                 brick_entry -1,
 29110|                                                 &args);
 29111|                 }
 29112|             }
 29113|             current_brick++;
 29114|         }
 29115|     }
 29116| }
 29117| void gc_heap::walk_plug (uint8_t* plug, size_t size, BOOL check_last_object_p, walk_relocate_args* args)
 29118| {
 29119|     if (check_last_object_p)
 29120|     {
 29121|         size += sizeof (gap_reloc_pair);
 29122|         mark* entry = args->pinned_plug_entry;
 29123|         if (args->is_shortened)
 29124|         {
 29125|             assert (entry->has_post_plug_info());
 29126|             entry->swap_post_plug_and_saved_for_profiler();
 29127|         }
 29128|         else
 29129|         {
 29130|             assert (entry->has_pre_plug_info());
 29131|             entry->swap_pre_plug_and_saved_for_profiler();
 29132|         }
 29133|     }
 29134|     ptrdiff_t last_plug_relocation = node_relocation_distance (plug);
 29135|     STRESS_LOG_PLUG_MOVE(plug, (plug + size), -last_plug_relocation);
 29136|     ptrdiff_t reloc = settings.compaction ? last_plug_relocation : 0;
 29137|     (args->fn) (plug, (plug + size), reloc, args->profiling_context, !!settings.compaction, false);
 29138|     if (check_last_object_p)
 29139|     {
 29140|         mark* entry = args->pinned_plug_entry;
 29141|         if (args->is_shortened)
 29142|         {
 29143|             entry->swap_post_plug_and_saved_for_profiler();
 29144|         }
 29145|         else
 29146|         {
 29147|             entry->swap_pre_plug_and_saved_for_profiler();
 29148|         }
 29149|     }
 29150| }
 29151| void gc_heap::walk_relocation_in_brick (uint8_t* tree, walk_relocate_args* args)
 29152| {
 29153|     assert ((tree != NULL));
 29154|     if (node_left_child (tree))
 29155|     {
 29156|         walk_relocation_in_brick (tree + node_left_child (tree), args);
 29157|     }
 29158|     uint8_t*  plug = tree;
 29159|     BOOL   has_pre_plug_info_p = FALSE;
 29160|     BOOL   has_post_plug_info_p = FALSE;
 29161|     if (tree == oldest_pinned_plug)
 29162|     {
 29163|         args->pinned_plug_entry = get_oldest_pinned_entry (&has_pre_plug_info_p,
 29164|                                                            &has_post_plug_info_p);
 29165|         assert (tree == pinned_plug (args->pinned_plug_entry));
 29166|     }
 29167|     if (args->last_plug != 0)
 29168|     {
 29169|         size_t gap_size = node_gap_size (tree);
 29170|         uint8_t*  gap = (plug - gap_size);
 29171|         uint8_t*  last_plug_end = gap;
 29172|         size_t last_plug_size = (last_plug_end - args->last_plug);
 29173|         dprintf (3, ("tree: %p, last_plug: %p, gap: %p(%zx), last_plug_end: %p, size: %zx",
 29174|             tree, args->last_plug, gap, gap_size, last_plug_end, last_plug_size));
 29175|         BOOL check_last_object_p = (args->is_shortened || has_pre_plug_info_p);
 29176|         if (!check_last_object_p)
 29177|         {
 29178|             assert (last_plug_size >= Align (min_obj_size));
 29179|         }
 29180|         walk_plug (args->last_plug, last_plug_size, check_last_object_p, args);
 29181|     }
 29182|     else
 29183|     {
 29184|         assert (!has_pre_plug_info_p);
 29185|     }
 29186|     dprintf (3, ("set args last plug to plug: %p", plug));
 29187|     args->last_plug = plug;
 29188|     args->is_shortened = has_post_plug_info_p;
 29189|     if (node_right_child (tree))
 29190|     {
 29191|         walk_relocation_in_brick (tree + node_right_child (tree), args);
 29192|     }
 29193| }
 29194| void gc_heap::walk_relocation (void* profiling_context, record_surv_fn fn)
 29195| {
 29196|     int condemned_gen_number = settings.condemned_generation;
 29197|     int stop_gen_idx = get_stop_generation_index (condemned_gen_number);
 29198|     reset_pinned_queue_bos();
 29199|     update_oldest_pinned_plug();
 29200|     for (int i = condemned_gen_number; i >= stop_gen_idx; i--)
 29201|     {
 29202|         generation* condemned_gen = generation_of (i);
 29203|         heap_segment*  current_heap_segment = heap_segment_rw (generation_start_segment (condemned_gen));
 29204| #ifdef USE_REGIONS
 29205|         current_heap_segment = walk_relocation_sip (current_heap_segment, profiling_context, fn);
 29206|         if (!current_heap_segment)
 29207|             continue;
 29208| #endif // USE_REGIONS
 29209|         uint8_t*  start_address = get_soh_start_object (current_heap_segment, condemned_gen);
 29210|         size_t  current_brick = brick_of (start_address);
 29211|         PREFIX_ASSUME(current_heap_segment != NULL);
 29212|         size_t end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
 29213|         walk_relocate_args args;
 29214|         args.is_shortened = FALSE;
 29215|         args.pinned_plug_entry = 0;
 29216|         args.last_plug = 0;
 29217|         args.profiling_context = profiling_context;
 29218|         args.fn = fn;
 29219|         while (1)
 29220|         {
 29221|             if (current_brick > end_brick)
 29222|             {
 29223|                 if (args.last_plug)
 29224|                 {
 29225|                     walk_plug (args.last_plug,
 29226|                             (heap_segment_allocated (current_heap_segment) - args.last_plug),
 29227|                             args.is_shortened,
 29228|                             &args);
 29229|                     args.last_plug = 0;
 29230|                 }
 29231|                 current_heap_segment = heap_segment_next_rw (current_heap_segment);
 29232| #ifdef USE_REGIONS
 29233|                 current_heap_segment = walk_relocation_sip (current_heap_segment, profiling_context, fn);
 29234| #endif // USE_REGIONS
 29235|                 if (current_heap_segment)
 29236|                 {
 29237|                     current_brick = brick_of (heap_segment_mem (current_heap_segment));
 29238|                     end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
 29239|                     continue;
 29240|                 }
 29241|                 else
 29242|                 {
 29243|                     break;
 29244|                 }
 29245|             }
 29246|             {
 29247|                 int brick_entry =  brick_table [ current_brick ];
 29248|                 if (brick_entry >= 0)
 29249|                 {
 29250|                     walk_relocation_in_brick (brick_address (current_brick) +
 29251|                                             brick_entry - 1,
 29252|                                             &args);
 29253|                 }
 29254|             }
 29255|             current_brick++;
 29256|         }
 29257|     }
 29258| }
 29259| #ifdef USE_REGIONS
 29260| heap_segment* gc_heap::walk_relocation_sip (heap_segment* current_heap_segment, void* profiling_context, record_surv_fn fn)
 29261| {
 29262|     while (current_heap_segment && heap_segment_swept_in_plan (current_heap_segment))
 29263|     {
 29264|         uint8_t* start = heap_segment_mem (current_heap_segment);
 29265|         uint8_t* end = heap_segment_allocated (current_heap_segment);
 29266|         uint8_t* obj = start;
 29267|         uint8_t* plug_start = nullptr;
 29268|         while (obj < end)
 29269|         {
 29270|             if (((CObjectHeader*)obj)->IsFree())
 29271|             {
 29272|                 if (plug_start)
 29273|                 {
 29274|                     fn (plug_start, obj, 0, profiling_context, false, false);
 29275|                     plug_start = nullptr;
 29276|                 }
 29277|             }
 29278|             else
 29279|             {
 29280|                 if (!plug_start)
 29281|                 {
 29282|                     plug_start = obj;
 29283|                 }
 29284|             }
 29285|             obj += Align (size (obj));
 29286|         }
 29287|         if (plug_start)
 29288|         {
 29289|             fn (plug_start, end, 0, profiling_context, false, false);
 29290|         }
 29291|         current_heap_segment = heap_segment_next_rw (current_heap_segment);
 29292|     }
 29293|     return current_heap_segment;
 29294| }
 29295| #endif // USE_REGIONS
 29296| void gc_heap::walk_survivors (record_surv_fn fn, void* context, walk_surv_type type)
 29297| {
 29298|     if (type == walk_for_gc)
 29299|         walk_survivors_relocation (context, fn);
 29300| #if defined(BACKGROUND_GC) && defined(FEATURE_EVENT_TRACE)
 29301|     else if (type == walk_for_bgc)
 29302|         walk_survivors_for_bgc (context, fn);
 29303| #endif //BACKGROUND_GC && FEATURE_EVENT_TRACE
 29304|     else
 29305|         assert (!"unknown type!");
 29306| }
 29307| #if defined(BACKGROUND_GC) && defined(FEATURE_EVENT_TRACE)
 29308| void gc_heap::walk_survivors_for_bgc (void* profiling_context, record_surv_fn fn)
 29309| {
 29310|     assert(settings.concurrent);
 29311|     for (int i = get_start_generation_index(); i < total_generation_count; i++)
 29312|     {
 29313|         int align_const = get_alignment_constant (i == max_generation);
 29314|         heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (i)));
 29315|         while (seg)
 29316|         {
 29317|             uint8_t* o = heap_segment_mem (seg);
 29318|             uint8_t* end = heap_segment_allocated (seg);
 29319|             while (o < end)
 29320|             {
 29321|                 if (method_table(o) == g_gc_pFreeObjectMethodTable)
 29322|                 {
 29323|                     o += Align (size (o), align_const);
 29324|                     continue;
 29325|                 }
 29326|                 uint8_t* plug_start = o;
 29327|                 while (method_table(o) != g_gc_pFreeObjectMethodTable)
 29328|                 {
 29329|                     o += Align (size (o), align_const);
 29330|                     if (o >= end)
 29331|                     {
 29332|                         break;
 29333|                     }
 29334|                 }
 29335|                 uint8_t* plug_end = o;
 29336|                 fn (plug_start,
 29337|                     plug_end,
 29338|                     0,              // Reloc distance == 0 as this is non-compacting
 29339|                     profiling_context,
 29340|                     false,          // Non-compacting
 29341|                     true);          // BGC
 29342|             }
 29343|             seg = heap_segment_next (seg);
 29344|         }
 29345|     }
 29346| }
 29347| #endif //BACKGROUND_GC && FEATURE_EVENT_TRACE
 29348| void gc_heap::relocate_phase (int condemned_gen_number,
 29349|                               uint8_t* first_condemned_address)
 29350| {
 29351|     ScanContext sc;
 29352|     sc.thread_number = heap_number;
 29353|     sc.thread_count = n_heaps;
 29354|     sc.promotion = FALSE;
 29355|     sc.concurrent = FALSE;
 29356| #ifdef MULTIPLE_HEAPS
 29357|     dprintf(3, ("Joining after end of plan"));
 29358|     gc_t_join.join(this, gc_join_begin_relocate_phase);
 29359|     if (gc_t_join.joined())
 29360|     {
 29361| #endif //MULTIPLE_HEAPS
 29362| #ifdef FEATURE_EVENT_TRACE
 29363|         if (informational_event_enabled_p)
 29364|         {
 29365|             gc_time_info[time_relocate] = GetHighPrecisionTimeStamp();
 29366|         }
 29367| #endif //FEATURE_EVENT_TRACE
 29368| #ifdef USE_REGIONS
 29369|         verify_region_to_generation_map();
 29370| #endif //USE_REGIONS
 29371| #ifdef MULTIPLE_HEAPS
 29372|         dprintf(3, ("Restarting for relocation"));
 29373|         gc_t_join.restart();
 29374|     }
 29375| #endif //MULTIPLE_HEAPS
 29376|     dprintf (2, (ThreadStressLog::gcStartRelocateMsg(), heap_number));
 29377|     dprintf(3,("Relocating roots"));
 29378|     GCScan::GcScanRoots(GCHeap::Relocate,
 29379|                             condemned_gen_number, max_generation, &sc);
 29380|     verify_pins_with_post_plug_info("after reloc stack");
 29381| #ifdef BACKGROUND_GC
 29382|     if (gc_heap::background_running_p())
 29383|     {
 29384|         scan_background_roots (GCHeap::Relocate, heap_number, &sc);
 29385|     }
 29386| #endif //BACKGROUND_GC
 29387| #ifdef FEATURE_CARD_MARKING_STEALING
 29388|     {
 29389|         dprintf(3, ("Relocating survivors"));
 29390|         relocate_survivors(condemned_gen_number,
 29391|             first_condemned_address);
 29392|     }
 29393| #ifdef FEATURE_PREMORTEM_FINALIZATION
 29394|     dprintf(3, ("Relocating finalization data"));
 29395|     finalize_queue->RelocateFinalizationData(condemned_gen_number,
 29396|         __this);
 29397| #endif // FEATURE_PREMORTEM_FINALIZATION
 29398|     {
 29399|         dprintf(3, ("Relocating handle table"));
 29400|         GCScan::GcScanHandles(GCHeap::Relocate,
 29401|             condemned_gen_number, max_generation, &sc);
 29402|     }
 29403| #endif // FEATURE_CARD_MARKING_STEALING
 29404|     if (condemned_gen_number != max_generation)
 29405|     {
 29406| #if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
 29407|         if (!card_mark_done_soh)
 29408| #endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
 29409|         {
 29410|             dprintf (3, ("Relocating cross generation pointers on heap %d", heap_number));
 29411|             mark_through_cards_for_segments(&gc_heap::relocate_address, TRUE THIS_ARG);
 29412|             verify_pins_with_post_plug_info("after reloc cards");
 29413| #if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
 29414|             card_mark_done_soh = true;
 29415| #endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
 29416|         }
 29417|     }
 29418|     if (condemned_gen_number != max_generation)
 29419|     {
 29420| #if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
 29421|         if (!card_mark_done_uoh)
 29422| #endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
 29423|         {
 29424|             dprintf (3, ("Relocating cross generation pointers for uoh objects on heap %d", heap_number));
 29425|             for (int i = uoh_start_generation; i < total_generation_count; i++)
 29426|             {
 29427| #ifndef ALLOW_REFERENCES_IN_POH
 29428|                 if (i != poh_generation)
 29429| #endif //ALLOW_REFERENCES_IN_POH
 29430|                     mark_through_cards_for_uoh_objects(&gc_heap::relocate_address, i, TRUE THIS_ARG);
 29431|             }
 29432| #if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
 29433|             card_mark_done_uoh = true;
 29434| #endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
 29435|         }
 29436|     }
 29437|     else
 29438|     {
 29439| #ifdef FEATURE_LOH_COMPACTION
 29440|         if (loh_compacted_p)
 29441|         {
 29442|             assert (settings.condemned_generation == max_generation);
 29443|             relocate_in_loh_compact();
 29444|         }
 29445|         else
 29446| #endif //FEATURE_LOH_COMPACTION
 29447|         {
 29448|             relocate_in_uoh_objects (loh_generation);
 29449|         }
 29450| #ifdef ALLOW_REFERENCES_IN_POH
 29451|         relocate_in_uoh_objects (poh_generation);
 29452| #endif
 29453|     }
 29454| #ifndef FEATURE_CARD_MARKING_STEALING
 29455|     {
 29456|         dprintf(3,("Relocating survivors"));
 29457|         relocate_survivors (condemned_gen_number,
 29458|                             first_condemned_address);
 29459|     }
 29460| #ifdef FEATURE_PREMORTEM_FINALIZATION
 29461|         dprintf(3,("Relocating finalization data"));
 29462|         finalize_queue->RelocateFinalizationData (condemned_gen_number,
 29463|                                                        __this);
 29464| #endif // FEATURE_PREMORTEM_FINALIZATION
 29465|     {
 29466|         dprintf(3,("Relocating handle table"));
 29467|         GCScan::GcScanHandles(GCHeap::Relocate,
 29468|                                   condemned_gen_number, max_generation, &sc);
 29469|     }
 29470| #endif // !FEATURE_CARD_MARKING_STEALING
 29471| #if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
 29472|     if (condemned_gen_number != max_generation)
 29473|     {
 29474|         for (int i = 0; i < gc_heap::n_heaps; i++)
 29475|         {
 29476|             int heap_number_to_look_at = (i + heap_number) % gc_heap::n_heaps;
 29477|             gc_heap* hp = gc_heap::g_heaps[heap_number_to_look_at];
 29478|             if (!hp->card_mark_done_soh)
 29479|             {
 29480|                 dprintf(3, ("Relocating cross generation pointers on heap %d", hp->heap_number));
 29481|                 hp->mark_through_cards_for_segments(&gc_heap::relocate_address, TRUE THIS_ARG);
 29482|                 hp->card_mark_done_soh = true;
 29483|             }
 29484|             if (!hp->card_mark_done_uoh)
 29485|             {
 29486|                 dprintf(3, ("Relocating cross generation pointers for uoh objects on heap %d", hp->heap_number));
 29487|                 for (int i = uoh_start_generation; i < total_generation_count; i++)
 29488|                 {
 29489| #ifndef ALLOW_REFERENCES_IN_POH
 29490|                     if (i != poh_generation)
 29491| #endif //ALLOW_REFERENCES_IN_POH
 29492|                         hp->mark_through_cards_for_uoh_objects(&gc_heap::relocate_address, i, TRUE THIS_ARG);
 29493|                 }
 29494|                 hp->card_mark_done_uoh = true;
 29495|             }
 29496|         }
 29497|     }
 29498| #endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
 29499|     dprintf(2, (ThreadStressLog::gcEndRelocateMsg(), heap_number));
 29500| }
 29501| mark* gc_heap::get_next_pinned_entry (uint8_t* tree,
 29502|                                       BOOL* has_pre_plug_info_p,
 29503|                                       BOOL* has_post_plug_info_p,
 29504|                                       BOOL deque_p)
 29505| {
 29506|     if (!pinned_plug_que_empty_p())
 29507|     {
 29508|         mark* oldest_entry = oldest_pin();
 29509|         uint8_t* oldest_plug = pinned_plug (oldest_entry);
 29510|         if (tree == oldest_plug)
 29511|         {
 29512|             *has_pre_plug_info_p =  oldest_entry->has_pre_plug_info();
 29513|             *has_post_plug_info_p = oldest_entry->has_post_plug_info();
 29514|             if (deque_p)
 29515|             {
 29516|                 deque_pinned_plug();
 29517|             }
 29518|             dprintf (3, ("found a pinned plug %p, pre: %d, post: %d",
 29519|                 tree,
 29520|                 (*has_pre_plug_info_p ? 1 : 0),
 29521|                 (*has_post_plug_info_p ? 1 : 0)));
 29522|             return oldest_entry;
 29523|         }
 29524|     }
 29525|     return NULL;
 29526| }
 29527| mark* gc_heap::get_oldest_pinned_entry (BOOL* has_pre_plug_info_p,
 29528|                                         BOOL* has_post_plug_info_p)
 29529| {
 29530|     mark* oldest_entry = oldest_pin();
 29531|     *has_pre_plug_info_p =  oldest_entry->has_pre_plug_info();
 29532|     *has_post_plug_info_p = oldest_entry->has_post_plug_info();
 29533|     deque_pinned_plug();
 29534|     update_oldest_pinned_plug();
 29535|     return oldest_entry;
 29536| }
 29537| inline
 29538| void gc_heap::copy_cards_range (uint8_t* dest, uint8_t* src, size_t len, BOOL copy_cards_p)
 29539| {
 29540|     if (copy_cards_p)
 29541|         copy_cards_for_addresses (dest, src, len);
 29542|     else
 29543|         clear_card_for_addresses (dest, dest + len);
 29544| }
 29545| inline
 29546| void  gc_heap::gcmemcopy (uint8_t* dest, uint8_t* src, size_t len, BOOL copy_cards_p)
 29547| {
 29548|     if (dest != src)
 29549|     {
 29550| #ifdef BACKGROUND_GC
 29551|         if (current_c_gc_state == c_gc_state_marking)
 29552|         {
 29553|             copy_mark_bits_for_addresses (dest, src, len);
 29554|         }
 29555| #endif //BACKGROUND_GC
 29556| #ifdef DOUBLY_LINKED_FL
 29557|         BOOL set_bgc_mark_bits_p = is_plug_bgc_mark_bit_set (src);
 29558|         if (set_bgc_mark_bits_p)
 29559|         {
 29560|             clear_plug_bgc_mark_bit (src);
 29561|         }
 29562|         BOOL make_free_obj_p = FALSE;
 29563|         if (len <= min_free_item_no_prev)
 29564|         {
 29565|             make_free_obj_p = is_free_obj_in_compact_bit_set (src);
 29566|             if (make_free_obj_p)
 29567|             {
 29568|                 clear_free_obj_in_compact_bit (src);
 29569|             }
 29570|         }
 29571| #endif //DOUBLY_LINKED_FL
 29572|         dprintf(3,(ThreadStressLog::gcMemCopyMsg(), (size_t)src, (size_t)dest, (size_t)src+len, (size_t)dest+len));
 29573|         memcopy (dest - plug_skew, src - plug_skew, len);
 29574| #ifdef DOUBLY_LINKED_FL
 29575|         if (set_bgc_mark_bits_p)
 29576|         {
 29577|             uint8_t* dest_o = dest;
 29578|             uint8_t* dest_end_o = dest + len;
 29579|             while (dest_o < dest_end_o)
 29580|             {
 29581|                 uint8_t* next_o = dest_o + Align (size (dest_o));
 29582|                 background_mark (dest_o, background_saved_lowest_address, background_saved_highest_address);
 29583|                 dest_o = next_o;
 29584|             }
 29585|             dprintf (3333, ("[h%d] GM: %p(%zx-%zx)->%p(%zx-%zx)",
 29586|                 heap_number, dest,
 29587|                 (size_t)(&mark_array [mark_word_of (dest)]),
 29588|                 (size_t)(mark_array [mark_word_of (dest)]),
 29589|                 dest_end_o,
 29590|                 (size_t)(&mark_array [mark_word_of (dest_o)]),
 29591|                 (size_t)(mark_array [mark_word_of (dest_o)])));
 29592|         }
 29593|         if (make_free_obj_p)
 29594|         {
 29595|             size_t* filler_free_obj_size_location = (size_t*)(dest + min_free_item_no_prev);
 29596|             size_t filler_free_obj_size = *filler_free_obj_size_location;
 29597|             make_unused_array ((dest + len), filler_free_obj_size);
 29598|             dprintf (3333, ("[h%d] smallobj, %p(%zd): %p->%p", heap_number,
 29599|                 filler_free_obj_size_location, filler_free_obj_size, (dest + len), (dest + len + filler_free_obj_size)));
 29600|         }
 29601| #endif //DOUBLY_LINKED_FL
 29602| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 29603|         if (SoftwareWriteWatch::IsEnabledForGCHeap())
 29604|         {
 29605|             SoftwareWriteWatch::SetDirtyRegion(dest, len - plug_skew);
 29606|         }
 29607| #endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 29608|         copy_cards_range (dest, src, len, copy_cards_p);
 29609|     }
 29610| }
 29611| void gc_heap::compact_plug (uint8_t* plug, size_t size, BOOL check_last_object_p, compact_args* args)
 29612| {
 29613|     args->print();
 29614|     uint8_t* reloc_plug = plug + args->last_plug_relocation;
 29615|     if (check_last_object_p)
 29616|     {
 29617|         size += sizeof (gap_reloc_pair);
 29618|         mark* entry = args->pinned_plug_entry;
 29619|         if (args->is_shortened)
 29620|         {
 29621|             assert (entry->has_post_plug_info());
 29622|             entry->swap_post_plug_and_saved();
 29623|         }
 29624|         else
 29625|         {
 29626|             assert (entry->has_pre_plug_info());
 29627|             entry->swap_pre_plug_and_saved();
 29628|         }
 29629|     }
 29630|     int  old_brick_entry =  brick_table [brick_of (plug)];
 29631|     assert (node_relocation_distance (plug) == args->last_plug_relocation);
 29632| #ifdef FEATURE_STRUCTALIGN
 29633|     ptrdiff_t alignpad = node_alignpad(plug);
 29634|     if (alignpad)
 29635|     {
 29636|         make_unused_array (reloc_plug - alignpad, alignpad);
 29637|         if (brick_of (reloc_plug - alignpad) != brick_of (reloc_plug))
 29638|         {
 29639|             fix_brick_to_highest (reloc_plug - alignpad, reloc_plug);
 29640|         }
 29641|     }
 29642| #else // FEATURE_STRUCTALIGN
 29643|     size_t unused_arr_size = 0;
 29644|     BOOL  already_padded_p = FALSE;
 29645| #ifdef SHORT_PLUGS
 29646|     if (is_plug_padded (plug))
 29647|     {
 29648|         already_padded_p = TRUE;
 29649|         clear_plug_padded (plug);
 29650|         unused_arr_size = Align (min_obj_size);
 29651|     }
 29652| #endif //SHORT_PLUGS
 29653|     if (node_realigned (plug))
 29654|     {
 29655|         unused_arr_size += switch_alignment_size (already_padded_p);
 29656|     }
 29657|     if (unused_arr_size != 0)
 29658|     {
 29659|         make_unused_array (reloc_plug - unused_arr_size, unused_arr_size);
 29660|         if (brick_of (reloc_plug - unused_arr_size) != brick_of (reloc_plug))
 29661|         {
 29662|             dprintf (3, ("fix B for padding: %zd: %p->%p",
 29663|                 unused_arr_size, (reloc_plug - unused_arr_size), reloc_plug));
 29664|             fix_brick_to_highest (reloc_plug - unused_arr_size, reloc_plug);
 29665|         }
 29666|     }
 29667| #endif // FEATURE_STRUCTALIGN
 29668| #ifdef SHORT_PLUGS
 29669|     if (is_plug_padded (plug))
 29670|     {
 29671|         make_unused_array (reloc_plug - Align (min_obj_size), Align (min_obj_size));
 29672|         if (brick_of (reloc_plug - Align (min_obj_size)) != brick_of (reloc_plug))
 29673|         {
 29674|             fix_brick_to_highest (reloc_plug - Align (min_obj_size), reloc_plug);
 29675|         }
 29676|     }
 29677| #endif //SHORT_PLUGS
 29678|     gcmemcopy (reloc_plug, plug, size, args->copy_cards_p);
 29679|     if (args->check_gennum_p)
 29680|     {
 29681|         int src_gennum = args->src_gennum;
 29682|         if (src_gennum == -1)
 29683|         {
 29684|             src_gennum = object_gennum (plug);
 29685|         }
 29686|         int dest_gennum = object_gennum_plan (reloc_plug);
 29687|         if (src_gennum < dest_gennum)
 29688|         {
 29689|             generation_allocation_size (generation_of (dest_gennum)) += size;
 29690|         }
 29691|     }
 29692|     size_t current_reloc_brick = args->current_compacted_brick;
 29693|     if (brick_of (reloc_plug) != current_reloc_brick)
 29694|     {
 29695|         dprintf (3, ("last reloc B: %zx, current reloc B: %zx",
 29696|             current_reloc_brick, brick_of (reloc_plug)));
 29697|         if (args->before_last_plug)
 29698|         {
 29699|             dprintf (3,(" fixing last brick %zx to point to last plug %p(%zx)",
 29700|                      current_reloc_brick,
 29701|                      args->before_last_plug,
 29702|                      (args->before_last_plug - brick_address (current_reloc_brick))));
 29703|             {
 29704|                 set_brick (current_reloc_brick,
 29705|                         args->before_last_plug - brick_address (current_reloc_brick));
 29706|             }
 29707|         }
 29708|         current_reloc_brick = brick_of (reloc_plug);
 29709|     }
 29710|     size_t end_brick = brick_of (reloc_plug + size-1);
 29711|     if (end_brick != current_reloc_brick)
 29712|     {
 29713|         dprintf (3,("plug spanning multiple bricks, fixing first brick %zx to %zx(%zx)",
 29714|                  current_reloc_brick, (size_t)reloc_plug,
 29715|                  (reloc_plug - brick_address (current_reloc_brick))));
 29716|         {
 29717|             set_brick (current_reloc_brick,
 29718|                     reloc_plug - brick_address (current_reloc_brick));
 29719|         }
 29720|         size_t brick = current_reloc_brick + 1;
 29721|         dprintf (3,("setting intervening bricks %zu->%zu to -1",
 29722|             brick, (end_brick - 1)));
 29723|         while (brick < end_brick)
 29724|         {
 29725|             set_brick (brick, -1);
 29726|             brick++;
 29727|         }
 29728|         args->before_last_plug = brick_address (end_brick) -1;
 29729|         current_reloc_brick = end_brick;
 29730|         dprintf (3, ("setting before last to %p, last brick to %zx",
 29731|             args->before_last_plug, current_reloc_brick));
 29732|     }
 29733|     else
 29734|     {
 29735|         dprintf (3, ("still in the same brick: %zx", end_brick));
 29736|         args->before_last_plug = reloc_plug;
 29737|     }
 29738|     args->current_compacted_brick = current_reloc_brick;
 29739|     if (check_last_object_p)
 29740|     {
 29741|         mark* entry = args->pinned_plug_entry;
 29742|         if (args->is_shortened)
 29743|         {
 29744|             entry->swap_post_plug_and_saved();
 29745|         }
 29746|         else
 29747|         {
 29748|             entry->swap_pre_plug_and_saved();
 29749|         }
 29750|     }
 29751| }
 29752| void gc_heap::compact_in_brick (uint8_t* tree, compact_args* args)
 29753| {
 29754|     assert (tree != NULL);
 29755|     int   left_node = node_left_child (tree);
 29756|     int   right_node = node_right_child (tree);
 29757|     ptrdiff_t relocation = node_relocation_distance (tree);
 29758|     args->print();
 29759|     if (left_node)
 29760|     {
 29761|         dprintf (3, ("B: L: %d->%p", left_node, (tree + left_node)));
 29762|         compact_in_brick ((tree + left_node), args);
 29763|     }
 29764|     uint8_t*  plug = tree;
 29765|     BOOL   has_pre_plug_info_p = FALSE;
 29766|     BOOL   has_post_plug_info_p = FALSE;
 29767|     if (tree == oldest_pinned_plug)
 29768|     {
 29769|         args->pinned_plug_entry = get_oldest_pinned_entry (&has_pre_plug_info_p,
 29770|                                                            &has_post_plug_info_p);
 29771|         assert (tree == pinned_plug (args->pinned_plug_entry));
 29772|     }
 29773|     if (args->last_plug != 0)
 29774|     {
 29775|         size_t gap_size = node_gap_size (tree);
 29776|         uint8_t*  gap = (plug - gap_size);
 29777|         uint8_t*  last_plug_end = gap;
 29778|         size_t last_plug_size = (last_plug_end - args->last_plug);
 29779|         assert ((last_plug_size & (sizeof(PTR_PTR) - 1)) == 0);
 29780|         dprintf (3, ("tree: %p, last_plug: %p, gap: %p(%zx), last_plug_end: %p, size: %zx",
 29781|             tree, args->last_plug, gap, gap_size, last_plug_end, last_plug_size));
 29782|         BOOL check_last_object_p = (args->is_shortened || has_pre_plug_info_p);
 29783|         if (!check_last_object_p)
 29784|         {
 29785|             assert (last_plug_size >= Align (min_obj_size));
 29786|         }
 29787|         compact_plug (args->last_plug, last_plug_size, check_last_object_p, args);
 29788|     }
 29789|     else
 29790|     {
 29791|         assert (!has_pre_plug_info_p);
 29792|     }
 29793|     dprintf (3, ("set args last plug to plug: %p, reloc: %zx", plug, relocation));
 29794|     args->last_plug = plug;
 29795|     args->last_plug_relocation = relocation;
 29796|     args->is_shortened = has_post_plug_info_p;
 29797|     if (right_node)
 29798|     {
 29799|         dprintf (3, ("B: R: %d->%p", right_node, (tree + right_node)));
 29800|         compact_in_brick ((tree + right_node), args);
 29801|     }
 29802| }
 29803| size_t gc_heap::recover_saved_pinned_info()
 29804| {
 29805|     reset_pinned_queue_bos();
 29806|     size_t total_recovered_sweep_size = 0;
 29807|     while (!(pinned_plug_que_empty_p()))
 29808|     {
 29809|         mark* oldest_entry = oldest_pin();
 29810|         size_t recovered_sweep_size = oldest_entry->recover_plug_info();
 29811|         if (recovered_sweep_size > 0)
 29812|         {
 29813|             uint8_t* plug = pinned_plug (oldest_entry);
 29814|             if (object_gennum (plug) == max_generation)
 29815|             {
 29816|                 dprintf (3, ("recovered %p(%zd) from pin", plug, recovered_sweep_size));
 29817|                 total_recovered_sweep_size += recovered_sweep_size;
 29818|             }
 29819|         }
 29820| #ifdef GC_CONFIG_DRIVEN
 29821|         if (oldest_entry->has_pre_plug_info() && oldest_entry->has_post_plug_info())
 29822|             record_interesting_data_point (idp_pre_and_post_pin);
 29823|         else if (oldest_entry->has_pre_plug_info())
 29824|             record_interesting_data_point (idp_pre_pin);
 29825|         else if (oldest_entry->has_post_plug_info())
 29826|             record_interesting_data_point (idp_post_pin);
 29827| #endif //GC_CONFIG_DRIVEN
 29828|         deque_pinned_plug();
 29829|     }
 29830|     return total_recovered_sweep_size;
 29831| }
 29832| void gc_heap::compact_phase (int condemned_gen_number,
 29833|                              uint8_t*  first_condemned_address,
 29834|                              BOOL clear_cards)
 29835| {
 29836| #ifdef MULTIPLE_HEAPS
 29837|     dprintf(3, ("Joining after end of relocation"));
 29838|     gc_t_join.join(this, gc_join_relocate_phase_done);
 29839|     if (gc_t_join.joined())
 29840| #endif //MULTIPLE_HEAPS
 29841|     {
 29842| #ifdef FEATURE_EVENT_TRACE
 29843|         if (informational_event_enabled_p)
 29844|         {
 29845|             gc_time_info[time_compact] = GetHighPrecisionTimeStamp();
 29846|             gc_time_info[time_relocate] = gc_time_info[time_compact] - gc_time_info[time_relocate];
 29847|         }
 29848| #endif //FEATURE_EVENT_TRACE
 29849| #ifdef MULTIPLE_HEAPS
 29850|         dprintf(3, ("Restarting for compaction"));
 29851|         gc_t_join.restart();
 29852| #endif //MULTIPLE_HEAPS
 29853|     }
 29854|     dprintf (2, (ThreadStressLog::gcStartCompactMsg(), heap_number,
 29855|         first_condemned_address, brick_of (first_condemned_address)));
 29856| #ifdef FEATURE_LOH_COMPACTION
 29857|     if (loh_compacted_p)
 29858|     {
 29859|         compact_loh();
 29860|     }
 29861| #endif //FEATURE_LOH_COMPACTION
 29862|     reset_pinned_queue_bos();
 29863|     update_oldest_pinned_plug();
 29864|     BOOL reused_seg = expand_reused_seg_p();
 29865|     if (reused_seg)
 29866|     {
 29867|         for (int i = 1; i <= max_generation; i++)
 29868|         {
 29869|             generation_allocation_size (generation_of (i)) = 0;
 29870|         }
 29871|     }
 29872|     int stop_gen_idx = get_stop_generation_index (condemned_gen_number);
 29873|     for (int i = condemned_gen_number; i >= stop_gen_idx; i--)
 29874|     {
 29875|         generation* condemned_gen = generation_of (i);
 29876|         heap_segment* current_heap_segment = get_start_segment (condemned_gen);
 29877| #ifdef USE_REGIONS
 29878|         if (!current_heap_segment)
 29879|             continue;
 29880|         size_t   current_brick = brick_of (heap_segment_mem (current_heap_segment));
 29881| #else
 29882|         size_t   current_brick = brick_of (first_condemned_address);
 29883| #endif //USE_REGIONS
 29884|         uint8_t*  end_address = heap_segment_allocated (current_heap_segment);
 29885| #ifndef USE_REGIONS
 29886|         if ((first_condemned_address >= end_address) && (condemned_gen_number < max_generation))
 29887|         {
 29888|             return;
 29889|         }
 29890| #endif //!USE_REGIONS
 29891|         size_t  end_brick = brick_of (end_address-1);
 29892|         compact_args args;
 29893|         args.last_plug = 0;
 29894|         args.before_last_plug = 0;
 29895|         args.current_compacted_brick = ~((size_t)1);
 29896|         args.is_shortened = FALSE;
 29897|         args.pinned_plug_entry = 0;
 29898|         args.copy_cards_p =  (condemned_gen_number >= 1) || !clear_cards;
 29899|         args.check_gennum_p = reused_seg;
 29900|         if (args.check_gennum_p)
 29901|         {
 29902|             args.src_gennum = ((current_heap_segment == ephemeral_heap_segment) ? -1 : 2);
 29903|         }
 29904| #ifdef USE_REGIONS
 29905|         assert (!args.check_gennum_p);
 29906| #endif //USE_REGIONS
 29907|         while (1)
 29908|         {
 29909|             if (current_brick > end_brick)
 29910|             {
 29911|                 if (args.last_plug != 0)
 29912|                 {
 29913|                     dprintf (3, ("compacting last plug: %p", args.last_plug))
 29914|                     compact_plug (args.last_plug,
 29915|                                   (heap_segment_allocated (current_heap_segment) - args.last_plug),
 29916|                                   args.is_shortened,
 29917|                                   &args);
 29918|                 }
 29919|                 heap_segment* next_heap_segment = heap_segment_next_non_sip (current_heap_segment);
 29920|                 if (next_heap_segment)
 29921|                 {
 29922|                     current_heap_segment = next_heap_segment;
 29923|                     current_brick = brick_of (heap_segment_mem (current_heap_segment));
 29924|                     end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
 29925|                     args.last_plug = 0;
 29926|                     if (args.check_gennum_p)
 29927|                     {
 29928|                         args.src_gennum = ((current_heap_segment == ephemeral_heap_segment) ? -1 : 2);
 29929|                     }
 29930|                     continue;
 29931|                 }
 29932|                 else
 29933|                 {
 29934|                     if (args.before_last_plug !=0)
 29935|                     {
 29936|                         dprintf (3, ("Fixing last brick %zx to point to plug %zx",
 29937|                                     args.current_compacted_brick, (size_t)args.before_last_plug));
 29938|                         assert (args.current_compacted_brick != ~1u);
 29939|                         set_brick (args.current_compacted_brick,
 29940|                                    args.before_last_plug - brick_address (args.current_compacted_brick));
 29941|                     }
 29942|                     break;
 29943|                 }
 29944|             }
 29945|             {
 29946|                 int  brick_entry =  brick_table [ current_brick ];
 29947|                 dprintf (3, ("B: %zx(%zx)->%p",
 29948|                     current_brick, (size_t)brick_entry, (brick_address (current_brick) + brick_entry - 1)));
 29949|                 if (brick_entry >= 0)
 29950|                 {
 29951|                     compact_in_brick ((brick_address (current_brick) + brick_entry -1),
 29952|                                       &args);
 29953|                 }
 29954|             }
 29955|             current_brick++;
 29956|         }
 29957|     }
 29958|     recover_saved_pinned_info();
 29959|     concurrent_print_time_delta ("compact end");
 29960|     dprintf (2, (ThreadStressLog::gcEndCompactMsg(), heap_number));
 29961| }
 29962| #ifdef MULTIPLE_HEAPS
 29963| #ifdef _MSC_VER
 29964| #pragma warning(push)
 29965| #pragma warning(disable:4702) // C4702: unreachable code: gc_thread_function may not return
 29966| #endif //_MSC_VER
 29967| void gc_heap::gc_thread_stub (void* arg)
 29968| {
 29969|     gc_heap* heap = (gc_heap*)arg;
 29970|     if (!gc_thread_no_affinitize_p)
 29971|     {
 29972|         set_thread_affinity_for_heap (heap->heap_number, heap_select::find_proc_no_from_heap_no (heap->heap_number));
 29973|     }
 29974|     GCToOSInterface::BoostThreadPriority();
 29975|     void* tmp = _alloca (256*heap->heap_number);
 29976|     heap->gc_thread_function();
 29977| }
 29978| #ifdef _MSC_VER
 29979| #pragma warning(pop)
 29980| #endif //_MSC_VER
 29981| #endif //MULTIPLE_HEAPS
 29982| #ifdef BACKGROUND_GC
 29983| #ifdef _MSC_VER
 29984| #pragma warning(push)
 29985| #pragma warning(disable:4702) // C4702: unreachable code: gc_thread_function may not return
 29986| #endif //_MSC_VER
 29987| void gc_heap::bgc_thread_stub (void* arg)
 29988| {
 29989|     gc_heap* heap = (gc_heap*)arg;
 29990|     heap->bgc_thread = GCToEEInterface::GetThread();
 29991|     assert(heap->bgc_thread != nullptr);
 29992|     heap->bgc_thread_function();
 29993| }
 29994| #ifdef _MSC_VER
 29995| #pragma warning(pop)
 29996| #endif //_MSC_VER
 29997| void gc_heap::background_drain_mark_list (int thread)
 29998| {
 29999| #ifndef MULTIPLE_HEAPS
 30000|     UNREFERENCED_PARAMETER(thread);
 30001| #endif //!MULTIPLE_HEAPS
 30002|     size_t saved_c_mark_list_index = c_mark_list_index;
 30003|     if (saved_c_mark_list_index)
 30004|     {
 30005|         concurrent_print_time_delta ("SML");
 30006|     }
 30007|     while (c_mark_list_index != 0)
 30008|     {
 30009|         size_t current_index = c_mark_list_index - 1;
 30010|         uint8_t* o = c_mark_list [current_index];
 30011|         background_mark_object (o THREAD_NUMBER_ARG);
 30012|         c_mark_list_index--;
 30013|     }
 30014|     if (saved_c_mark_list_index)
 30015|     {
 30016|         concurrent_print_time_delta ("EML");
 30017|     }
 30018|     fire_drain_mark_list_event (saved_c_mark_list_index);
 30019| }
 30020| #ifdef MULTIPLE_HEAPS
 30021| void gc_heap::background_scan_dependent_handles (ScanContext *sc)
 30022| {
 30023|     s_fUnscannedPromotions = TRUE;
 30024|     while (true)
 30025|     {
 30026|         if (GCScan::GcDhUnpromotedHandlesExist(sc))
 30027|             s_fUnpromotedHandles = TRUE;
 30028|         bgc_t_join.join(this, gc_join_scan_dependent_handles);
 30029|         if (bgc_t_join.joined())
 30030|         {
 30031|             s_fScanRequired = s_fUnscannedPromotions && s_fUnpromotedHandles;
 30032|             s_fUnscannedPromotions = FALSE;
 30033|             s_fUnpromotedHandles = FALSE;
 30034|             if (!s_fScanRequired)
 30035|             {
 30036| #ifdef USE_REGIONS
 30037|                 BOOL all_heaps_background_overflow_p = FALSE;
 30038| #else //USE_REGIONS
 30039|                 uint8_t* all_heaps_max = 0;
 30040|                 uint8_t* all_heaps_min = MAX_PTR;
 30041| #endif //USE_REGIONS
 30042|                 int i;
 30043|                 for (i = 0; i < n_heaps; i++)
 30044|                 {
 30045| #ifdef USE_REGIONS
 30046|                     if (g_heaps[i]->background_overflow_p)
 30047|                         all_heaps_background_overflow_p = TRUE;
 30048| #else //USE_REGIONS
 30049|                     if (all_heaps_max < g_heaps[i]->background_max_overflow_address)
 30050|                         all_heaps_max = g_heaps[i]->background_max_overflow_address;
 30051|                     if (all_heaps_min > g_heaps[i]->background_min_overflow_address)
 30052|                         all_heaps_min = g_heaps[i]->background_min_overflow_address;
 30053| #endif //USE_REGIONS
 30054|                 }
 30055|                 for (i = 0; i < n_heaps; i++)
 30056|                 {
 30057| #ifdef USE_REGIONS
 30058|                     g_heaps[i]->background_overflow_p = all_heaps_background_overflow_p;
 30059| #else //USE_REGIONS
 30060|                     g_heaps[i]->background_max_overflow_address = all_heaps_max;
 30061|                     g_heaps[i]->background_min_overflow_address = all_heaps_min;
 30062| #endif //USE_REGIONS
 30063|                 }
 30064|             }
 30065|             dprintf(2, ("Starting all gc thread mark stack overflow processing"));
 30066|             bgc_t_join.restart();
 30067|         }
 30068|         if (background_process_mark_overflow (sc->concurrent))
 30069|             s_fUnscannedPromotions = TRUE;
 30070|         if (!s_fScanRequired)
 30071|             break;
 30072|         bgc_t_join.join(this, gc_join_rescan_dependent_handles);
 30073|         if (bgc_t_join.joined())
 30074|         {
 30075|             dprintf(3, ("Starting all gc thread for dependent handle promotion"));
 30076|             bgc_t_join.restart();
 30077|         }
 30078|         if (GCScan::GcDhUnpromotedHandlesExist(sc))
 30079|             if (GCScan::GcDhReScan(sc))
 30080|                 s_fUnscannedPromotions = TRUE;
 30081|     }
 30082| }
 30083| #else
 30084| void gc_heap::background_scan_dependent_handles (ScanContext *sc)
 30085| {
 30086|     bool fUnscannedPromotions = true;
 30087|     while (GCScan::GcDhUnpromotedHandlesExist(sc) && fUnscannedPromotions)
 30088|     {
 30089|         fUnscannedPromotions = false;
 30090|         if (background_process_mark_overflow (sc->concurrent))
 30091|             fUnscannedPromotions = true;
 30092|         if (GCScan::GcDhReScan (sc))
 30093|             fUnscannedPromotions = true;
 30094|     }
 30095|     background_process_mark_overflow (sc->concurrent);
 30096| }
 30097| #endif //MULTIPLE_HEAPS
 30098| void gc_heap::recover_bgc_settings()
 30099| {
 30100|     if ((settings.condemned_generation < max_generation) && gc_heap::background_running_p())
 30101|     {
 30102|         dprintf (2, ("restoring bgc settings"));
 30103|         settings = saved_bgc_settings;
 30104|         GCHeap::GcCondemnedGeneration = gc_heap::settings.condemned_generation;
 30105|     }
 30106| }
 30107| void gc_heap::allow_fgc()
 30108| {
 30109|     assert (bgc_thread == GCToEEInterface::GetThread());
 30110|     bool bToggleGC = false;
 30111|     if (g_fSuspensionPending > 0)
 30112|     {
 30113|         bToggleGC = GCToEEInterface::EnablePreemptiveGC();
 30114|         if (bToggleGC)
 30115|         {
 30116|             GCToEEInterface::DisablePreemptiveGC();
 30117|         }
 30118|     }
 30119| }
 30120| BOOL gc_heap::is_bgc_in_progress()
 30121| {
 30122|     return (background_running_p() || (current_bgc_state == bgc_initialized));
 30123| }
 30124| void gc_heap::clear_commit_flag()
 30125| {
 30126|     for (int i = get_start_generation_index(); i < total_generation_count; i++)
 30127|     {
 30128|         generation* gen = generation_of (i);
 30129|         heap_segment* seg = heap_segment_in_range (generation_start_segment (gen));
 30130|         while (seg)
 30131|         {
 30132|             if (seg->flags & heap_segment_flags_ma_committed)
 30133|             {
 30134|                 seg->flags &= ~heap_segment_flags_ma_committed;
 30135|             }
 30136|             if (seg->flags & heap_segment_flags_ma_pcommitted)
 30137|             {
 30138|                 seg->flags &= ~heap_segment_flags_ma_pcommitted;
 30139|             }
 30140|             seg = heap_segment_next (seg);
 30141|         }
 30142|     }
 30143| }
 30144| void gc_heap::clear_commit_flag_global()
 30145| {
 30146| #ifdef MULTIPLE_HEAPS
 30147|     for (int i = 0; i < n_heaps; i++)
 30148|     {
 30149|         g_heaps[i]->clear_commit_flag();
 30150|     }
 30151| #else
 30152|     clear_commit_flag();
 30153| #endif //MULTIPLE_HEAPS
 30154| }
 30155| void gc_heap::verify_mark_array_cleared (uint8_t* begin, uint8_t* end, uint32_t* mark_array_addr)
 30156| {
 30157| #ifdef _DEBUG
 30158|     size_t  markw = mark_word_of (begin);
 30159|     size_t  markw_end = mark_word_of (end);
 30160|     while (markw < markw_end)
 30161|     {
 30162|         if (mark_array_addr[markw])
 30163|         {
 30164|             uint8_t* addr = mark_word_address (markw);
 30165| #ifdef USE_REGIONS
 30166|             heap_segment* region = region_of (addr);
 30167|             dprintf (1, ("The mark bits at 0x%zx:0x%x(addr: 0x%p, r: %zx(%p)) were not cleared",
 30168|                             markw, mark_array_addr[markw], addr,
 30169|                             (size_t)region, heap_segment_mem (region)));
 30170| #else
 30171|             dprintf (1, ("The mark bits at 0x%zx:0x%x(addr: 0x%p) were not cleared",
 30172|                             markw, mark_array_addr[markw], addr));
 30173| #endif //USE_REGIONS
 30174|             FATAL_GC_ERROR();
 30175|         }
 30176|         markw++;
 30177|     }
 30178| #else // _DEBUG
 30179|     UNREFERENCED_PARAMETER(begin);
 30180|     UNREFERENCED_PARAMETER(end);
 30181|     UNREFERENCED_PARAMETER(mark_array_addr);
 30182| #endif //_DEBUG
 30183| }
 30184| uint8_t* gc_heap::get_start_address (heap_segment* seg)
 30185| {
 30186|     uint8_t* start =
 30187| #ifdef USE_REGIONS
 30188|         heap_segment_mem (seg);
 30189| #else
 30190|         (heap_segment_read_only_p(seg) ? heap_segment_mem (seg) : (uint8_t*)seg);
 30191| #endif //USE_REGIONS
 30192|     return start;
 30193| }
 30194| BOOL gc_heap::commit_mark_array_new_seg (gc_heap* hp,
 30195|                                          heap_segment* seg,
 30196|                                          uint32_t* new_card_table,
 30197|                                          uint8_t* new_lowest_address)
 30198| {
 30199|     uint8_t* start = get_start_address (seg);
 30200|     uint8_t* end = heap_segment_reserved (seg);
 30201|     uint8_t* lowest = hp->background_saved_lowest_address;
 30202|     uint8_t* highest = hp->background_saved_highest_address;
 30203|     uint8_t* commit_start = NULL;
 30204|     uint8_t* commit_end = NULL;
 30205|     size_t commit_flag = 0;
 30206|     if ((highest >= start) &&
 30207|         (lowest <= end))
 30208|     {
 30209|         if ((start >= lowest) && (end <= highest))
 30210|         {
 30211|             dprintf (GC_TABLE_LOG, ("completely in bgc range: seg %p-%p, bgc: %p-%p",
 30212|                                     start, end, lowest, highest));
 30213|             commit_flag = heap_segment_flags_ma_committed;
 30214|         }
 30215|         else
 30216|         {
 30217|             dprintf (GC_TABLE_LOG, ("partially in bgc range: seg %p-%p, bgc: %p-%p",
 30218|                                     start, end, lowest, highest));
 30219|             commit_flag = heap_segment_flags_ma_pcommitted;
 30220| #ifdef USE_REGIONS
 30221|             assert (!"Region should not have its mark array partially committed.");
 30222| #endif
 30223|         }
 30224|         commit_start = max (lowest, start);
 30225|         commit_end = min (highest, end);
 30226|         if (!commit_mark_array_by_range (commit_start, commit_end, hp->mark_array))
 30227|         {
 30228|             return FALSE;
 30229|         }
 30230|         if (new_card_table == 0)
 30231|         {
 30232|             new_card_table = g_gc_card_table;
 30233|         }
 30234|         if (hp->card_table != new_card_table)
 30235|         {
 30236|             if (new_lowest_address == 0)
 30237|             {
 30238|                 new_lowest_address = g_gc_lowest_address;
 30239|             }
 30240|             uint32_t* ct = &new_card_table[card_word (gcard_of (new_lowest_address))];
 30241|             uint32_t* ma = (uint32_t*)((uint8_t*)card_table_mark_array (ct) - size_mark_array_of (0, new_lowest_address));
 30242|             dprintf (GC_TABLE_LOG, ("table realloc-ed: %p->%p, MA: %p->%p",
 30243|                                     hp->card_table, new_card_table,
 30244|                                     hp->mark_array, ma));
 30245|             if (!commit_mark_array_by_range (commit_start, commit_end, ma))
 30246|             {
 30247|                 return FALSE;
 30248|             }
 30249|         }
 30250|         seg->flags |= commit_flag;
 30251|     }
 30252|     return TRUE;
 30253| }
 30254| BOOL gc_heap::commit_mark_array_by_range (uint8_t* begin, uint8_t* end, uint32_t* mark_array_addr)
 30255| {
 30256|     size_t beg_word = mark_word_of (begin);
 30257|     size_t end_word = mark_word_of (align_on_mark_word (end));
 30258|     uint8_t* commit_start = align_lower_page ((uint8_t*)&mark_array_addr[beg_word]);
 30259|     uint8_t* commit_end = align_on_page ((uint8_t*)&mark_array_addr[end_word]);
 30260|     size_t size = (size_t)(commit_end - commit_start);
 30261| #ifdef SIMPLE_DPRINTF
 30262|     dprintf (GC_TABLE_LOG, ("range: %p->%p mark word: %zx->%zx(%zd), mark array: %p->%p(%zd), commit %p->%p(%zd)",
 30263|                             begin, end,
 30264|                             beg_word, end_word,
 30265|                             (end_word - beg_word) * sizeof (uint32_t),
 30266|                             &mark_array_addr[beg_word],
 30267|                             &mark_array_addr[end_word],
 30268|                             (size_t)(&mark_array_addr[end_word] - &mark_array_addr[beg_word]),
 30269|                             commit_start, commit_end,
 30270|                             size));
 30271| #endif //SIMPLE_DPRINTF
 30272|     if (virtual_commit (commit_start, size, recorded_committed_bookkeeping_bucket))
 30273|     {
 30274|         verify_mark_array_cleared (begin, end, mark_array_addr);
 30275|         return TRUE;
 30276|     }
 30277|     else
 30278|     {
 30279|         dprintf (GC_TABLE_LOG, ("failed to commit %zd bytes", (end_word - beg_word) * sizeof (uint32_t)));
 30280|         return FALSE;
 30281|     }
 30282| }
 30283| BOOL gc_heap::commit_mark_array_with_check (heap_segment* seg, uint32_t* new_mark_array_addr)
 30284| {
 30285|     uint8_t* start = get_start_address (seg);
 30286|     uint8_t* end = heap_segment_reserved (seg);
 30287| #ifdef MULTIPLE_HEAPS
 30288|     uint8_t* lowest = heap_segment_heap (seg)->background_saved_lowest_address;
 30289|     uint8_t* highest = heap_segment_heap (seg)->background_saved_highest_address;
 30290| #else
 30291|     uint8_t* lowest = background_saved_lowest_address;
 30292|     uint8_t* highest = background_saved_highest_address;
 30293| #endif //MULTIPLE_HEAPS
 30294|     if ((highest >= start) &&
 30295|         (lowest <= end))
 30296|     {
 30297|         start = max (lowest, start);
 30298|         end = min (highest, end);
 30299|         if (!commit_mark_array_by_range (start, end, new_mark_array_addr))
 30300|         {
 30301|             return FALSE;
 30302|         }
 30303|     }
 30304|     return TRUE;
 30305| }
 30306| BOOL gc_heap::commit_mark_array_by_seg (heap_segment* seg, uint32_t* mark_array_addr)
 30307| {
 30308|     dprintf (GC_TABLE_LOG, ("seg: %p->%p; MA: %p",
 30309|         seg,
 30310|         heap_segment_reserved (seg),
 30311|         mark_array_addr));
 30312|     uint8_t* start = get_start_address (seg);
 30313|     return commit_mark_array_by_range (start, heap_segment_reserved (seg), mark_array_addr);
 30314| }
 30315| BOOL gc_heap::commit_mark_array_bgc_init()
 30316| {
 30317|     dprintf (GC_TABLE_LOG, ("BGC init commit: lowest: %p, highest: %p, mark_array: %p",
 30318|                             lowest_address, highest_address, mark_array));
 30319|     for (int i = get_start_generation_index(); i < total_generation_count; i++)
 30320|     {
 30321|         generation* gen = generation_of (i);
 30322|         heap_segment* seg = heap_segment_in_range (generation_start_segment (gen));
 30323|         while (seg)
 30324|         {
 30325|             dprintf (GC_TABLE_LOG, ("h%d gen%d seg: %p(%p-%p), flags: %zd",
 30326|                 heap_number, i, seg, heap_segment_mem (seg), heap_segment_allocated (seg), seg->flags));
 30327|             if (!(seg->flags & heap_segment_flags_ma_committed))
 30328|             {
 30329|                 if (heap_segment_read_only_p (seg))
 30330|                 {
 30331|                     if ((heap_segment_mem (seg) >= lowest_address) &&
 30332|                         (heap_segment_reserved (seg) <= highest_address))
 30333|                     {
 30334|                         if (commit_mark_array_by_seg (seg, mark_array))
 30335|                         {
 30336|                             seg->flags |= heap_segment_flags_ma_committed;
 30337|                         }
 30338|                         else
 30339|                         {
 30340|                             return FALSE;
 30341|                         }
 30342|                     }
 30343|                     else
 30344|                     {
 30345|                         uint8_t* start = max (lowest_address, heap_segment_mem (seg));
 30346|                         uint8_t* end = min (highest_address, heap_segment_reserved (seg));
 30347|                         if (commit_mark_array_by_range (start, end, mark_array))
 30348|                         {
 30349|                             seg->flags |= heap_segment_flags_ma_pcommitted;
 30350|                         }
 30351|                         else
 30352|                         {
 30353|                             return FALSE;
 30354|                         }
 30355|                     }
 30356|                 }
 30357|                 else
 30358|                 {
 30359|                     if (commit_mark_array_by_seg (seg, mark_array))
 30360|                     {
 30361|                         if (seg->flags & heap_segment_flags_ma_pcommitted)
 30362|                         {
 30363|                             seg->flags &= ~heap_segment_flags_ma_pcommitted;
 30364|                         }
 30365|                         seg->flags |= heap_segment_flags_ma_committed;
 30366|                     }
 30367|                     else
 30368|                     {
 30369|                         return FALSE;
 30370|                     }
 30371|                 }
 30372|             }
 30373|             seg = heap_segment_next (seg);
 30374|         }
 30375|     }
 30376|     return TRUE;
 30377| }
 30378| BOOL gc_heap::commit_new_mark_array (uint32_t* new_mark_array_addr)
 30379| {
 30380|     dprintf (GC_TABLE_LOG, ("committing existing segs on MA %p", new_mark_array_addr));
 30381|     for (int i = get_start_generation_index(); i < total_generation_count; i++)
 30382|     {
 30383|         generation* gen = generation_of (i);
 30384|         heap_segment* seg = heap_segment_in_range (generation_start_segment (gen));
 30385|         while (seg)
 30386|         {
 30387|             if (!commit_mark_array_with_check (seg, new_mark_array_addr))
 30388|             {
 30389|                 return FALSE;
 30390|             }
 30391|             seg = heap_segment_next (seg);
 30392|         }
 30393|     }
 30394| #if defined(MULTIPLE_HEAPS) && !defined(USE_REGIONS)
 30395|     if (new_heap_segment)
 30396|     {
 30397|         if (!commit_mark_array_with_check (new_heap_segment, new_mark_array_addr))
 30398|         {
 30399|             return FALSE;
 30400|         }
 30401|     }
 30402| #endif //MULTIPLE_HEAPS && !USE_REGIONS
 30403|     return TRUE;
 30404| }
 30405| BOOL gc_heap::commit_new_mark_array_global (uint32_t* new_mark_array)
 30406| {
 30407| #ifdef MULTIPLE_HEAPS
 30408|     for (int i = 0; i < n_heaps; i++)
 30409|     {
 30410|         if (!g_heaps[i]->commit_new_mark_array (new_mark_array))
 30411|         {
 30412|             return FALSE;
 30413|         }
 30414|     }
 30415| #else
 30416|     if (!commit_new_mark_array (new_mark_array))
 30417|     {
 30418|         return FALSE;
 30419|     }
 30420| #endif //MULTIPLE_HEAPS
 30421|     return TRUE;
 30422| }
 30423| void gc_heap::decommit_mark_array_by_seg (heap_segment* seg)
 30424| {
 30425|     if (mark_array == NULL)
 30426|     {
 30427|         return;
 30428|     }
 30429|     dprintf (GC_TABLE_LOG, ("decommitting seg %p(%zx), MA: %p", seg, seg->flags, mark_array));
 30430|     size_t flags = seg->flags;
 30431|     if ((flags & heap_segment_flags_ma_committed) ||
 30432|         (flags & heap_segment_flags_ma_pcommitted))
 30433|     {
 30434|         uint8_t* start = get_start_address (seg);
 30435|         uint8_t* end = heap_segment_reserved (seg);
 30436|         if (flags & heap_segment_flags_ma_pcommitted)
 30437|         {
 30438|             start = max (lowest_address, start);
 30439|             end = min (highest_address, end);
 30440|         }
 30441|         size_t beg_word = mark_word_of (start);
 30442|         size_t end_word = mark_word_of (align_on_mark_word (end));
 30443|         uint8_t* decommit_start = align_on_page ((uint8_t*)&mark_array[beg_word]);
 30444|         uint8_t* decommit_end = align_lower_page ((uint8_t*)&mark_array[end_word]);
 30445|         size_t size = (size_t)(decommit_end - decommit_start);
 30446| #ifdef SIMPLE_DPRINTF
 30447|         dprintf (GC_TABLE_LOG, ("seg: %p mark word: %zx->%zx(%zd), mark array: %p->%p(%zd), decommit %p->%p(%zd)",
 30448|                                 seg,
 30449|                                 beg_word, end_word,
 30450|                                 (end_word - beg_word) * sizeof (uint32_t),
 30451|                                 &mark_array[beg_word],
 30452|                                 &mark_array[end_word],
 30453|                                 (size_t)(&mark_array[end_word] - &mark_array[beg_word]),
 30454|                                 decommit_start, decommit_end,
 30455|                                 size));
 30456| #endif //SIMPLE_DPRINTF
 30457|         if (decommit_start < decommit_end)
 30458|         {
 30459|             if (!virtual_decommit (decommit_start, size, recorded_committed_bookkeeping_bucket))
 30460|             {
 30461|                 dprintf (GC_TABLE_LOG, ("decommit on %p for %zd bytes failed",
 30462|                                         decommit_start, size));
 30463|                 assert (!"decommit failed");
 30464|             }
 30465|         }
 30466|         dprintf (GC_TABLE_LOG, ("decommitted [%zx for address [%p", beg_word, seg));
 30467|     }
 30468| }
 30469| bool gc_heap::should_update_end_mark_size()
 30470| {
 30471|     return ((settings.condemned_generation == (max_generation - 1)) && (current_c_gc_state == c_gc_state_planning));
 30472| }
 30473| void gc_heap::background_mark_phase ()
 30474| {
 30475|     verify_mark_array_cleared();
 30476|     ScanContext sc;
 30477|     sc.thread_number = heap_number;
 30478|     sc.thread_count = n_heaps;
 30479|     sc.promotion = TRUE;
 30480|     sc.concurrent = FALSE;
 30481|     THREAD_FROM_HEAP;
 30482|     BOOL cooperative_mode = TRUE;
 30483| #ifndef MULTIPLE_HEAPS
 30484|     const int thread = heap_number;
 30485| #endif //!MULTIPLE_HEAPS
 30486|     dprintf(2,("-(GC%zu)BMark-", VolatileLoad(&settings.gc_index)));
 30487|     assert (settings.concurrent);
 30488|     if (gen0_must_clear_bricks > 0)
 30489|         gen0_must_clear_bricks--;
 30490|     background_soh_alloc_count = 0;
 30491|     background_uoh_alloc_count = 0;
 30492|     bgc_overflow_count = 0;
 30493|     bpromoted_bytes (heap_number) = 0;
 30494|     static uint32_t num_sizedrefs = 0;
 30495| #ifdef USE_REGIONS
 30496|     background_overflow_p = FALSE;
 30497| #else
 30498|     background_min_overflow_address = MAX_PTR;
 30499|     background_max_overflow_address = 0;
 30500|     background_min_soh_overflow_address = MAX_PTR;
 30501|     background_max_soh_overflow_address = 0;
 30502| #endif //USE_REGIONS
 30503|     processed_eph_overflow_p = FALSE;
 30504|     assert (g_mark_list);
 30505|     mark_list = g_mark_list;
 30506|     mark_list_end = &mark_list [0];
 30507|     mark_list_index = &mark_list [0];
 30508|     c_mark_list_index = 0;
 30509| #ifndef MULTIPLE_HEAPS
 30510|     shigh = (uint8_t*) 0;
 30511|     slow  = MAX_PTR;
 30512| #endif //MULTIPLE_HEAPS
 30513|     generation*   gen = generation_of (max_generation);
 30514|     dprintf(3,("BGC: stack marking"));
 30515|     sc.concurrent = TRUE;
 30516|     GCScan::GcScanRoots(background_promote_callback,
 30517|                             max_generation, max_generation,
 30518|                             &sc);
 30519|     dprintf(3,("BGC: finalization marking"));
 30520|     finalize_queue->GcScanRoots(background_promote_callback, heap_number, 0);
 30521|     size_t total_soh_size = generation_sizes (generation_of (max_generation));
 30522|     size_t total_loh_size = generation_size (loh_generation);
 30523|     size_t total_poh_size = generation_size (poh_generation);
 30524|     bgc_begin_loh_size = total_loh_size;
 30525|     bgc_begin_poh_size = total_poh_size;
 30526|     bgc_loh_size_increased = 0;
 30527|     bgc_poh_size_increased = 0;
 30528|     background_soh_size_end_mark = 0;
 30529|     dprintf (GTC_LOG, ("BM: h%d: loh: %zd, soh: %zd, poh: %zd", heap_number, total_loh_size, total_soh_size, total_poh_size));
 30530|     concurrent_print_time_delta ("CS");
 30531|     FIRE_EVENT(BGC1stNonConEnd);
 30532| #ifndef USE_REGIONS
 30533|     saved_overflow_ephemeral_seg = 0;
 30534| #endif //!USE_REGIONS
 30535|     current_bgc_state = bgc_reset_ww;
 30536| #ifdef MULTIPLE_HEAPS
 30537|     bgc_t_join.join(this, gc_join_restart_ee);
 30538|     if (bgc_t_join.joined())
 30539| #endif //MULTIPLE_HEAPS
 30540|     {
 30541| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 30542|         concurrent_print_time_delta ("CRWW begin");
 30543| #ifdef MULTIPLE_HEAPS
 30544|         for (int i = 0; i < n_heaps; i++)
 30545|         {
 30546|             g_heaps[i]->reset_write_watch (FALSE);
 30547|         }
 30548| #else
 30549|         reset_write_watch (FALSE);
 30550| #endif //MULTIPLE_HEAPS
 30551|         concurrent_print_time_delta ("CRWW");
 30552| #endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 30553|         num_sizedrefs = GCToEEInterface::GetTotalNumSizedRefHandles();
 30554|         dprintf (GTC_LOG, ("setting cm_in_progress"));
 30555|         c_write (cm_in_progress, TRUE);
 30556|         assert (dont_restart_ee_p);
 30557|         dont_restart_ee_p = FALSE;
 30558|         restart_vm();
 30559|         GCToOSInterface::YieldThread (0);
 30560| #ifdef MULTIPLE_HEAPS
 30561|         dprintf(3, ("Starting all gc threads for gc"));
 30562|         bgc_t_join.restart();
 30563| #endif //MULTIPLE_HEAPS
 30564|     }
 30565| #ifdef MULTIPLE_HEAPS
 30566|     bgc_t_join.join(this, gc_join_after_reset);
 30567|     if (bgc_t_join.joined())
 30568| #endif //MULTIPLE_HEAPS
 30569|     {
 30570|         disable_preemptive (true);
 30571| #ifndef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 30572| #ifdef WRITE_WATCH
 30573|         concurrent_print_time_delta ("CRWW begin");
 30574| #ifdef MULTIPLE_HEAPS
 30575|         for (int i = 0; i < n_heaps; i++)
 30576|         {
 30577|             g_heaps[i]->reset_write_watch (TRUE);
 30578|         }
 30579| #else
 30580|         reset_write_watch (TRUE);
 30581| #endif //MULTIPLE_HEAPS
 30582|         concurrent_print_time_delta ("CRWW");
 30583| #endif //WRITE_WATCH
 30584| #ifdef MULTIPLE_HEAPS
 30585|         for (int i = 0; i < n_heaps; i++)
 30586|         {
 30587|             g_heaps[i]->revisit_written_pages (TRUE, TRUE);
 30588|         }
 30589| #else
 30590|         revisit_written_pages (TRUE, TRUE);
 30591| #endif //MULTIPLE_HEAPS
 30592|         concurrent_print_time_delta ("CRW");
 30593| #endif // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 30594| #ifdef MULTIPLE_HEAPS
 30595|         for (int i = 0; i < n_heaps; i++)
 30596|         {
 30597|             g_heaps[i]->current_bgc_state = bgc_mark_handles;
 30598|         }
 30599| #else
 30600|         current_bgc_state = bgc_mark_handles;
 30601| #endif //MULTIPLE_HEAPS
 30602|         current_c_gc_state = c_gc_state_marking;
 30603|         enable_preemptive ();
 30604| #ifdef MULTIPLE_HEAPS
 30605|         dprintf(3, ("Joining BGC threads after resetting writewatch"));
 30606|         bgc_t_join.restart();
 30607| #endif //MULTIPLE_HEAPS
 30608|     }
 30609|     disable_preemptive (true);
 30610|     if (num_sizedrefs > 0)
 30611|     {
 30612|         GCScan::GcScanSizedRefs(background_promote, max_generation, max_generation, &sc);
 30613|         enable_preemptive ();
 30614| #ifdef MULTIPLE_HEAPS
 30615|         bgc_t_join.join(this, gc_join_scan_sizedref_done);
 30616|         if (bgc_t_join.joined())
 30617|         {
 30618|             dprintf(3, ("Done with marking all sized refs. Starting all bgc thread for marking other strong roots"));
 30619|             bgc_t_join.restart();
 30620|         }
 30621| #endif //MULTIPLE_HEAPS
 30622|         disable_preemptive (true);
 30623|     }
 30624|     dprintf (3,("BGC: handle table marking"));
 30625|     GCScan::GcScanHandles(background_promote,
 30626|                                 max_generation, max_generation,
 30627|                                 &sc);
 30628|     concurrent_print_time_delta ("CRH");
 30629|     current_bgc_state = bgc_mark_stack;
 30630|     dprintf (2,("concurrent draining mark list"));
 30631|     background_drain_mark_list (thread);
 30632|     concurrent_print_time_delta ("CRS");
 30633|     dprintf (2,("concurrent revisiting dirtied pages"));
 30634|     revisit_written_pages (TRUE);
 30635|     revisit_written_pages (TRUE);
 30636|     concurrent_print_time_delta ("CRre");
 30637|     enable_preemptive ();
 30638| #if defined(MULTIPLE_HEAPS)
 30639|     bgc_t_join.join(this, gc_join_concurrent_overflow);
 30640|     if (bgc_t_join.joined())
 30641|     {
 30642| #ifdef USE_REGIONS
 30643|         BOOL all_heaps_background_overflow_p = FALSE;
 30644| #else //USE_REGIONS
 30645|         uint8_t* all_heaps_max = 0;
 30646|         uint8_t* all_heaps_min = MAX_PTR;
 30647| #endif //USE_REGIONS
 30648|         int i;
 30649|         for (i = 0; i < n_heaps; i++)
 30650|         {
 30651| #ifdef USE_REGIONS
 30652|             if (g_heaps[i]->background_overflow_p)
 30653|                 all_heaps_background_overflow_p = TRUE;
 30654| #else //USE_REGIONS
 30655|             dprintf (3, ("heap %d overflow max is %p, min is %p",
 30656|                 i,
 30657|                 g_heaps[i]->background_max_overflow_address,
 30658|                 g_heaps[i]->background_min_overflow_address));
 30659|             if (all_heaps_max < g_heaps[i]->background_max_overflow_address)
 30660|                 all_heaps_max = g_heaps[i]->background_max_overflow_address;
 30661|             if (all_heaps_min > g_heaps[i]->background_min_overflow_address)
 30662|                 all_heaps_min = g_heaps[i]->background_min_overflow_address;
 30663| #endif //USE_REGIONS
 30664|         }
 30665|         for (i = 0; i < n_heaps; i++)
 30666|         {
 30667| #ifdef USE_REGIONS
 30668|             g_heaps[i]->background_overflow_p = all_heaps_background_overflow_p;
 30669| #else //USE_REGIONS
 30670|             g_heaps[i]->background_max_overflow_address = all_heaps_max;
 30671|             g_heaps[i]->background_min_overflow_address = all_heaps_min;
 30672| #endif //USE_REGIONS
 30673|         }
 30674|         dprintf(3, ("Starting all bgc threads after updating the overflow info"));
 30675|         bgc_t_join.restart();
 30676|     }
 30677| #endif //MULTIPLE_HEAPS
 30678|     disable_preemptive (true);
 30679|     dprintf (2, ("before CRov count: %zu", bgc_overflow_count));
 30680|     bgc_overflow_count = 0;
 30681|     background_process_mark_overflow (TRUE);
 30682|     dprintf (2, ("after CRov count: %zu", bgc_overflow_count));
 30683|     bgc_overflow_count = 0;
 30684|     concurrent_print_time_delta ("CRov");
 30685|     FIRE_EVENT(BGC1stConEnd);
 30686|     dprintf (2, ("Stopping the EE"));
 30687|     enable_preemptive ();
 30688| #ifdef MULTIPLE_HEAPS
 30689|     bgc_t_join.join(this, gc_join_suspend_ee);
 30690|     if (bgc_t_join.joined())
 30691|     {
 30692|         bgc_threads_sync_event.Reset();
 30693|         dprintf(3, ("Joining BGC threads for non concurrent final marking"));
 30694|         bgc_t_join.restart();
 30695|     }
 30696| #endif //MULTIPLE_HEAPS
 30697|     if (heap_number == 0)
 30698|     {
 30699|         enter_spin_lock (&gc_lock);
 30700|         suspended_start_time = GetHighPrecisionTimeStamp();
 30701|         bgc_suspend_EE ();
 30702|         bgc_threads_sync_event.Set();
 30703|     }
 30704|     else
 30705|     {
 30706|         bgc_threads_sync_event.Wait(INFINITE, FALSE);
 30707|         dprintf (2, ("bgc_threads_sync_event is signalled"));
 30708|     }
 30709|     assert (settings.concurrent);
 30710|     assert (settings.condemned_generation == max_generation);
 30711|     dprintf (2, ("clearing cm_in_progress"));
 30712|     c_write (cm_in_progress, FALSE);
 30713|     bgc_alloc_lock->check();
 30714|     current_bgc_state = bgc_final_marking;
 30715|     concurrent_print_time_delta ("CR");
 30716|     FIRE_EVENT(BGC2ndNonConBegin);
 30717|     mark_absorb_new_alloc();
 30718| #ifdef FEATURE_EVENT_TRACE
 30719|     static uint64_t current_mark_time = 0;
 30720|     static uint64_t last_mark_time = 0;
 30721| #endif //FEATURE_EVENT_TRACE
 30722| #ifdef MULTIPLE_HEAPS
 30723|     bgc_t_join.join(this, gc_join_after_absorb);
 30724|     if (bgc_t_join.joined())
 30725| #endif //MULTIPLE_HEAPS
 30726|     {
 30727| #ifdef BGC_SERVO_TUNING
 30728|         bgc_tuning::record_bgc_sweep_start();
 30729| #endif //BGC_SERVO_TUNING
 30730|         GCToEEInterface::BeforeGcScanRoots(max_generation, /* is_bgc */ true, /* is_concurrent */ false);
 30731| #ifdef FEATURE_EVENT_TRACE
 30732|         informational_event_enabled_p = EVENT_ENABLED (GCMarkWithType);
 30733|         if (informational_event_enabled_p)
 30734|             last_mark_time = GetHighPrecisionTimeStamp();
 30735| #endif //FEATURE_EVENT_TRACE
 30736| #ifdef MULTIPLE_HEAPS
 30737|         dprintf(3, ("Joining BGC threads after absorb"));
 30738|         bgc_t_join.restart();
 30739| #endif //MULTIPLE_HEAPS
 30740|     }
 30741|     sc.concurrent = FALSE;
 30742|     total_soh_size = generation_sizes (generation_of (max_generation));
 30743|     total_loh_size = generation_size (loh_generation);
 30744|     total_poh_size = generation_size (poh_generation);
 30745|     dprintf (GTC_LOG, ("FM: h%d: loh: %zd, soh: %zd, poh: %zd", heap_number, total_loh_size, total_soh_size, total_poh_size));
 30746| #ifdef FEATURE_BASICFREEZE
 30747| #ifdef USE_REGIONS
 30748|     assert (!ro_segments_in_range);
 30749| #else //USE_REGIONS
 30750|     if (ro_segments_in_range)
 30751|     {
 30752|         dprintf (2, ("nonconcurrent marking in range ro segments"));
 30753|         mark_ro_segments();
 30754|         concurrent_print_time_delta ("NRRO");
 30755|     }
 30756| #endif //USE_REGIONS
 30757| #endif //FEATURE_BASICFREEZE
 30758|     dprintf (2, ("nonconcurrent marking stack roots"));
 30759|     GCScan::GcScanRoots(background_promote,
 30760|                             max_generation, max_generation,
 30761|                             &sc);
 30762|     concurrent_print_time_delta ("NRS");
 30763|     finalize_queue->GcScanRoots(background_promote, heap_number, 0);
 30764|     dprintf (2, ("nonconcurrent marking handle table"));
 30765|     GCScan::GcScanHandles(background_promote,
 30766|                                 max_generation, max_generation,
 30767|                                 &sc);
 30768|     concurrent_print_time_delta ("NRH");
 30769|     dprintf (2,("---- (GC%zu)final going through written pages ----", VolatileLoad(&settings.gc_index)));
 30770|     revisit_written_pages (FALSE);
 30771|     concurrent_print_time_delta ("NRre LOH");
 30772|     dprintf (2, ("before NR 1st Hov count: %zu", bgc_overflow_count));
 30773|     bgc_overflow_count = 0;
 30774|     dprintf (2, ("1st dependent handle scan and process mark overflow"));
 30775|     GCScan::GcDhInitialScan(background_promote, max_generation, max_generation, &sc);
 30776|     background_scan_dependent_handles (&sc);
 30777|     concurrent_print_time_delta ("NR 1st Hov");
 30778|     dprintf (2, ("after NR 1st Hov count: %zu", bgc_overflow_count));
 30779|     bgc_overflow_count = 0;
 30780| #ifdef MULTIPLE_HEAPS
 30781|     bgc_t_join.join(this, gc_join_null_dead_short_weak);
 30782|     if (bgc_t_join.joined())
 30783| #endif //MULTIPLE_HEAPS
 30784|     {
 30785| #ifdef FEATURE_EVENT_TRACE
 30786|         bgc_time_info[time_mark_sizedref] = 0;
 30787|         record_mark_time (bgc_time_info[time_mark_roots], current_mark_time, last_mark_time);
 30788| #endif //FEATURE_EVENT_TRACE
 30789| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 30790|         SoftwareWriteWatch::DisableForGCHeap();
 30791| #endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 30792|         GCToEEInterface::AfterGcScanRoots (max_generation, max_generation, &sc);
 30793| #ifdef MULTIPLE_HEAPS
 30794|         dprintf(3, ("Joining BGC threads for short weak handle scan"));
 30795|         bgc_t_join.restart();
 30796| #endif //MULTIPLE_HEAPS
 30797|     }
 30798|     GCScan::GcShortWeakPtrScan(max_generation, max_generation, &sc);
 30799|     concurrent_print_time_delta ("NR GcShortWeakPtrScan");
 30800|     {
 30801| #ifdef MULTIPLE_HEAPS
 30802|         bgc_t_join.join(this, gc_join_scan_finalization);
 30803|         if (bgc_t_join.joined())
 30804|         {
 30805| #endif //MULTIPLE_HEAPS
 30806| #ifdef FEATURE_EVENT_TRACE
 30807|             record_mark_time (bgc_time_info[time_mark_short_weak], current_mark_time, last_mark_time);
 30808| #endif //FEATURE_EVENT_TRACE
 30809| #ifdef MULTIPLE_HEAPS
 30810|             dprintf(3, ("Joining BGC threads for finalization"));
 30811|             bgc_t_join.restart();
 30812|         }
 30813| #endif //MULTIPLE_HEAPS
 30814|         dprintf(3,("Marking finalization data"));
 30815|         concurrent_print_time_delta ("NRj");
 30816|         finalize_queue->ScanForFinalization (background_promote, max_generation, FALSE, __this);
 30817|         concurrent_print_time_delta ("NRF");
 30818|     }
 30819|     dprintf (2, ("before NR 2nd Hov count: %zu", bgc_overflow_count));
 30820|     bgc_overflow_count = 0;
 30821|     dprintf (2, ("2nd dependent handle scan and process mark overflow"));
 30822|     background_scan_dependent_handles (&sc);
 30823|     concurrent_print_time_delta ("NR 2nd Hov");
 30824| #ifdef MULTIPLE_HEAPS
 30825|     bgc_t_join.join(this, gc_join_null_dead_long_weak);
 30826|     if (bgc_t_join.joined())
 30827| #endif //MULTIPLE_HEAPS
 30828|     {
 30829| #ifdef FEATURE_EVENT_TRACE
 30830|         record_mark_time (bgc_time_info[time_mark_scan_finalization], current_mark_time, last_mark_time);
 30831| #endif //FEATURE_EVENT_TRACE
 30832| #ifdef MULTIPLE_HEAPS
 30833|         dprintf(2, ("Joining BGC threads for weak pointer deletion"));
 30834|         bgc_t_join.restart();
 30835| #endif //MULTIPLE_HEAPS
 30836|     }
 30837|     GCScan::GcWeakPtrScan (max_generation, max_generation, &sc);
 30838|     concurrent_print_time_delta ("NR GcWeakPtrScan");
 30839| #ifdef MULTIPLE_HEAPS
 30840|     bgc_t_join.join(this, gc_join_null_dead_syncblk);
 30841|     if (bgc_t_join.joined())
 30842| #endif //MULTIPLE_HEAPS
 30843|     {
 30844|         dprintf (2, ("calling GcWeakPtrScanBySingleThread"));
 30845|         GCScan::GcWeakPtrScanBySingleThread (max_generation, max_generation, &sc);
 30846| #ifdef FEATURE_EVENT_TRACE
 30847|         record_mark_time (bgc_time_info[time_mark_long_weak], current_mark_time, last_mark_time);
 30848| #endif //FEATURE_EVENT_TRACE
 30849|         concurrent_print_time_delta ("NR GcWeakPtrScanBySingleThread");
 30850| #ifdef MULTIPLE_HEAPS
 30851|         dprintf(2, ("Starting BGC threads for end of background mark phase"));
 30852|         bgc_t_join.restart();
 30853| #endif //MULTIPLE_HEAPS
 30854|     }
 30855|     dprintf (2, ("end of bgc mark: loh: %zu, poh: %zu, soh: %zu",
 30856|                  generation_size (loh_generation),
 30857|                  generation_size (poh_generation),
 30858|                  generation_sizes (generation_of (max_generation))));
 30859|     for (int gen_idx = max_generation; gen_idx < total_generation_count; gen_idx++)
 30860|     {
 30861|         generation* gen = generation_of (gen_idx);
 30862|         dynamic_data* dd = dynamic_data_of (gen_idx);
 30863|         dd_begin_data_size (dd) = generation_size (gen_idx) -
 30864|                                   (generation_free_list_space (gen) + generation_free_obj_space (gen)) -
 30865|                                    get_generation_start_size (gen_idx);
 30866|         dd_survived_size (dd) = 0;
 30867|         dd_pinned_survived_size (dd) = 0;
 30868|         dd_artificial_pinned_survived_size (dd) = 0;
 30869|         dd_added_pinned_size (dd) = 0;
 30870|     }
 30871|     for (int i = get_start_generation_index(); i < uoh_start_generation; i++)
 30872|     {
 30873|         heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (i)));
 30874|         PREFIX_ASSUME(seg != NULL);
 30875|         while (seg)
 30876|         {
 30877|             seg->flags &= ~heap_segment_flags_swept;
 30878| #ifndef USE_REGIONS
 30879|             if (heap_segment_allocated (seg) == heap_segment_mem (seg))
 30880|             {
 30881|                 FATAL_GC_ERROR();
 30882|             }
 30883|             if (seg == ephemeral_heap_segment)
 30884|             {
 30885|                 heap_segment_background_allocated (seg) = generation_allocation_start (generation_of (max_generation - 1));
 30886|             }
 30887|             else
 30888| #endif //!USE_REGIONS
 30889|             {
 30890|                 heap_segment_background_allocated (seg) = heap_segment_allocated (seg);
 30891|             }
 30892|             background_soh_size_end_mark += heap_segment_background_allocated (seg) - heap_segment_mem (seg);
 30893|             dprintf (3333, ("h%d gen%d seg %zx (%p) background allocated is %p",
 30894|                             heap_number, i, (size_t)(seg), heap_segment_mem (seg),
 30895|                             heap_segment_background_allocated (seg)));
 30896|             seg = heap_segment_next_rw (seg);
 30897|         }
 30898|     }
 30899|     repair_allocation_contexts (FALSE);
 30900|     dprintf (2, ("end of bgc mark: gen2 free list space: %zu, free obj space: %zu",
 30901|         generation_free_list_space (generation_of (max_generation)),
 30902|         generation_free_obj_space (generation_of (max_generation))));
 30903|     dprintf(2,("---- (GC%zu)End of background mark phase ----", VolatileLoad(&settings.gc_index)));
 30904| }
 30905| void
 30906| gc_heap::suspend_EE ()
 30907| {
 30908|     dprintf (2, ("suspend_EE"));
 30909| #ifdef MULTIPLE_HEAPS
 30910|     gc_heap* hp = gc_heap::g_heaps[0];
 30911|     GCToEEInterface::SuspendEE(SUSPEND_FOR_GC_PREP);
 30912| #else
 30913|     GCToEEInterface::SuspendEE(SUSPEND_FOR_GC_PREP);
 30914| #endif //MULTIPLE_HEAPS
 30915| }
 30916| #ifdef MULTIPLE_HEAPS
 30917| void
 30918| gc_heap::bgc_suspend_EE ()
 30919| {
 30920|     for (int i = 0; i < n_heaps; i++)
 30921|     {
 30922|         gc_heap::g_heaps[i]->reset_gc_done();
 30923|     }
 30924|     gc_started = TRUE;
 30925|     dprintf (2, ("bgc_suspend_EE"));
 30926|     GCToEEInterface::SuspendEE(SUSPEND_FOR_GC_PREP);
 30927|     gc_started = FALSE;
 30928|     for (int i = 0; i < n_heaps; i++)
 30929|     {
 30930|         gc_heap::g_heaps[i]->set_gc_done();
 30931|     }
 30932| }
 30933| #else
 30934| void
 30935| gc_heap::bgc_suspend_EE ()
 30936| {
 30937|     reset_gc_done();
 30938|     gc_started = TRUE;
 30939|     dprintf (2, ("bgc_suspend_EE"));
 30940|     GCToEEInterface::SuspendEE(SUSPEND_FOR_GC_PREP);
 30941|     gc_started = FALSE;
 30942|     set_gc_done();
 30943| }
 30944| #endif //MULTIPLE_HEAPS
 30945| void
 30946| gc_heap::restart_EE ()
 30947| {
 30948|     dprintf (2, ("restart_EE"));
 30949| #ifdef MULTIPLE_HEAPS
 30950|     GCToEEInterface::RestartEE(FALSE);
 30951| #else
 30952|     GCToEEInterface::RestartEE(FALSE);
 30953| #endif //MULTIPLE_HEAPS
 30954| }
 30955| inline uint8_t* gc_heap::high_page (heap_segment* seg, BOOL concurrent_p)
 30956| {
 30957| #ifdef USE_REGIONS
 30958|     assert (!concurrent_p || (heap_segment_gen_num (seg) >= max_generation));
 30959| #else
 30960|     if (concurrent_p)
 30961|     {
 30962|         uint8_t* end = ((seg == ephemeral_heap_segment) ?
 30963|                      generation_allocation_start (generation_of (max_generation - 1)) :
 30964|                      heap_segment_allocated (seg));
 30965|         return align_lower_page (end);
 30966|     }
 30967|     else
 30968| #endif //USE_REGIONS
 30969|     {
 30970|         return heap_segment_allocated (seg);
 30971|     }
 30972| }
 30973| void gc_heap::revisit_written_page (uint8_t* page,
 30974|                                     uint8_t* end,
 30975|                                     BOOL concurrent_p,
 30976|                                     uint8_t*& last_page,
 30977|                                     uint8_t*& last_object,
 30978|                                     BOOL large_objects_p,
 30979|                                     size_t& num_marked_objects)
 30980| {
 30981|     uint8_t*   start_address = page;
 30982|     uint8_t*   o             = 0;
 30983|     int align_const = get_alignment_constant (!large_objects_p);
 30984|     uint8_t* high_address = end;
 30985|     uint8_t* current_lowest_address = background_saved_lowest_address;
 30986|     uint8_t* current_highest_address = background_saved_highest_address;
 30987|     BOOL no_more_loop_p = FALSE;
 30988|     THREAD_FROM_HEAP;
 30989| #ifndef MULTIPLE_HEAPS
 30990|     const int thread = heap_number;
 30991| #endif //!MULTIPLE_HEAPS
 30992|     if (large_objects_p)
 30993|     {
 30994|         o = last_object;
 30995|     }
 30996|     else
 30997|     {
 30998|         if (((last_page + WRITE_WATCH_UNIT_SIZE) == page)
 30999|             || (start_address <= last_object))
 31000|         {
 31001|             o = last_object;
 31002|         }
 31003|         else
 31004|         {
 31005|             o = find_first_object (start_address, last_object);
 31006|             assert (o >= last_object);
 31007|         }
 31008|     }
 31009|     dprintf (3,("page %zx start: %zx, %zx[ ",
 31010|                (size_t)page, (size_t)o,
 31011|                (size_t)(min (high_address, page + WRITE_WATCH_UNIT_SIZE))));
 31012|     while (o < (min (high_address, page + WRITE_WATCH_UNIT_SIZE)))
 31013|     {
 31014|         size_t s;
 31015|         if (concurrent_p && large_objects_p)
 31016|         {
 31017|             bgc_alloc_lock->bgc_mark_set (o);
 31018|             if (((CObjectHeader*)o)->IsFree())
 31019|             {
 31020|                 s = unused_array_size (o);
 31021|             }
 31022|             else
 31023|             {
 31024|                 s = size (o);
 31025|             }
 31026|         }
 31027|         else
 31028|         {
 31029|             s = size (o);
 31030|         }
 31031|         dprintf (3,("Considering object %zx(%s)", (size_t)o, (background_object_marked (o, FALSE) ? "bm" : "nbm")));
 31032|         assert (Align (s) >= Align (min_obj_size));
 31033|         uint8_t* next_o =  o + Align (s, align_const);
 31034|         if (next_o >= start_address)
 31035|         {
 31036| #ifdef MULTIPLE_HEAPS
 31037|             if (concurrent_p)
 31038|             {
 31039|                 last_object = o;
 31040|             }
 31041| #endif //MULTIPLE_HEAPS
 31042|             if (contain_pointers (o) &&
 31043|                 (!((o >= current_lowest_address) && (o < current_highest_address)) ||
 31044|                 background_marked (o)))
 31045|             {
 31046|                 dprintf (3, ("going through %zx", (size_t)o));
 31047|                 go_through_object (method_table(o), o, s, poo, start_address, use_start, (o + s),
 31048|                                     if ((uint8_t*)poo >= min (high_address, page + WRITE_WATCH_UNIT_SIZE))
 31049|                                     {
 31050|                                         no_more_loop_p = TRUE;
 31051|                                         goto end_limit;
 31052|                                     }
 31053|                                     uint8_t* oo = VolatileLoadWithoutBarrier(poo);
 31054|                                     num_marked_objects++;
 31055|                                     background_mark_object (oo THREAD_NUMBER_ARG);
 31056|                                 );
 31057|             }
 31058|             else if (
 31059|                 concurrent_p &&
 31060|                 ((CObjectHeader*)o)->IsFree() &&
 31061|                 (next_o > min (high_address, page + WRITE_WATCH_UNIT_SIZE)))
 31062|             {
 31063|                 no_more_loop_p = TRUE;
 31064|                 goto end_limit;
 31065|             }
 31066|         }
 31067| end_limit:
 31068|         if (concurrent_p && large_objects_p)
 31069|         {
 31070|             bgc_alloc_lock->bgc_mark_done ();
 31071|         }
 31072|         if (no_more_loop_p)
 31073|         {
 31074|             break;
 31075|         }
 31076|         o = next_o;
 31077|     }
 31078| #ifdef MULTIPLE_HEAPS
 31079|     if (concurrent_p)
 31080|     {
 31081|         assert (last_object < (min (high_address, page + WRITE_WATCH_UNIT_SIZE)));
 31082|     }
 31083|     else
 31084| #endif //MULTIPLE_HEAPS
 31085|     {
 31086|         last_object = o;
 31087|     }
 31088|     dprintf (3,("Last object: %zx", (size_t)last_object));
 31089|     last_page = align_write_watch_lower_page (o);
 31090|     if (concurrent_p)
 31091|     {
 31092|         allow_fgc();
 31093|     }
 31094| }
 31095| void gc_heap::revisit_written_pages (BOOL concurrent_p, BOOL reset_only_p)
 31096| {
 31097|     if (concurrent_p && !reset_only_p)
 31098|     {
 31099|         current_bgc_state = bgc_revisit_soh;
 31100|     }
 31101|     size_t total_dirtied_pages = 0;
 31102|     size_t total_marked_objects = 0;
 31103|     bool reset_watch_state = !!concurrent_p;
 31104|     bool is_runtime_suspended = !concurrent_p;
 31105|     BOOL small_object_segments = TRUE;
 31106|     int start_gen_idx = get_start_generation_index();
 31107| #ifdef USE_REGIONS
 31108|     if (concurrent_p && !reset_only_p)
 31109|     {
 31110|         start_gen_idx = max_generation;
 31111|     }
 31112| #endif //USE_REGIONS
 31113|     for (int i = start_gen_idx; i < total_generation_count; i++)
 31114|     {
 31115|         heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (i)));
 31116|         PREFIX_ASSUME(seg != NULL);
 31117|         while (seg)
 31118|         {
 31119|             uint8_t* base_address = (uint8_t*)heap_segment_mem (seg);
 31120|             uintptr_t bcount = array_size;
 31121|             uint8_t* last_page = 0;
 31122|             uint8_t* last_object = heap_segment_mem (seg);
 31123|             uint8_t* high_address = 0;
 31124|             BOOL skip_seg_p = FALSE;
 31125|             if (reset_only_p)
 31126|             {
 31127|                 if ((heap_segment_mem (seg) >= background_saved_lowest_address) ||
 31128|                     (heap_segment_reserved (seg) <= background_saved_highest_address))
 31129|                 {
 31130|                     dprintf (3, ("h%d: sseg: %p(-%p)", heap_number,
 31131|                         heap_segment_mem (seg), heap_segment_reserved (seg)));
 31132|                     skip_seg_p = TRUE;
 31133|                 }
 31134|             }
 31135|             if (!skip_seg_p)
 31136|             {
 31137|                 dprintf (3, ("looking at seg %zx", (size_t)last_object));
 31138|                 if (reset_only_p)
 31139|                 {
 31140|                     base_address = max (base_address, background_saved_lowest_address);
 31141|                     dprintf (3, ("h%d: reset only starting %p", heap_number, base_address));
 31142|                 }
 31143|                 dprintf (3, ("h%d: starting: %p, seg %p-%p", heap_number, base_address,
 31144|                     heap_segment_mem (seg), heap_segment_reserved (seg)));
 31145|                 while (1)
 31146|                 {
 31147|                     if (reset_only_p)
 31148|                     {
 31149|                         high_address = ((seg == ephemeral_heap_segment) ? alloc_allocated : heap_segment_allocated (seg));
 31150|                         high_address = min (high_address, background_saved_highest_address);
 31151|                     }
 31152|                     else
 31153|                     {
 31154|                         high_address = high_page (seg, concurrent_p);
 31155|                     }
 31156|                     if ((base_address < high_address) &&
 31157|                         (bcount >= array_size))
 31158|                     {
 31159|                         ptrdiff_t region_size = high_address - base_address;
 31160|                         dprintf (3, ("h%d: gw: [%zx(%zd)", heap_number, (size_t)base_address, (size_t)region_size));
 31161| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 31162|                         if (!is_runtime_suspended)
 31163|                         {
 31164|                             enter_spin_lock(&gc_lock);
 31165|                         }
 31166| #endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 31167|                         get_write_watch_for_gc_heap (reset_watch_state, base_address, region_size,
 31168|                                                      (void**)background_written_addresses,
 31169|                                                      &bcount, is_runtime_suspended);
 31170| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 31171|                         if (!is_runtime_suspended)
 31172|                         {
 31173|                             leave_spin_lock(&gc_lock);
 31174|                         }
 31175| #endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 31176|                         if (bcount != 0)
 31177|                         {
 31178|                             total_dirtied_pages += bcount;
 31179|                             dprintf (3, ("Found %zu pages [%zx, %zx[",
 31180|                                             bcount, (size_t)base_address, (size_t)high_address));
 31181|                         }
 31182|                         if (!reset_only_p)
 31183|                         {
 31184|                             high_address = high_page (seg, concurrent_p);
 31185|                             for (unsigned i = 0; i < bcount; i++)
 31186|                             {
 31187|                                 uint8_t* page = (uint8_t*)background_written_addresses[i];
 31188|                                 dprintf (3, ("looking at page %d at %zx(h: %zx)", i,
 31189|                                     (size_t)page, (size_t)high_address));
 31190|                                 if (page < high_address)
 31191|                                 {
 31192|                                     revisit_written_page (page, high_address, concurrent_p,
 31193|                                                           last_page, last_object,
 31194|                                                           !small_object_segments,
 31195|                                                           total_marked_objects);
 31196|                                 }
 31197|                                 else
 31198|                                 {
 31199|                                     dprintf (3, ("page %d at %zx is >= %zx!", i, (size_t)page, (size_t)high_address));
 31200|                                     assert (!"page shouldn't have exceeded limit");
 31201|                                 }
 31202|                             }
 31203|                         }
 31204|                         if (bcount >= array_size){
 31205|                             base_address = background_written_addresses [array_size-1] + WRITE_WATCH_UNIT_SIZE;
 31206|                             bcount = array_size;
 31207|                         }
 31208|                     }
 31209|                     else
 31210|                     {
 31211|                         break;
 31212|                     }
 31213|                 }
 31214|             }
 31215|             seg = heap_segment_next_rw (seg);
 31216|         }
 31217|         if (i == soh_gen2)
 31218|         {
 31219|             if (!reset_only_p)
 31220|             {
 31221|                 dprintf (GTC_LOG, ("h%d: SOH: dp:%zd; mo: %zd", heap_number, total_dirtied_pages, total_marked_objects));
 31222|                 fire_revisit_event (total_dirtied_pages, total_marked_objects, FALSE);
 31223|                 concurrent_print_time_delta (concurrent_p ? "CR SOH" : "NR SOH");
 31224|                 total_dirtied_pages = 0;
 31225|                 total_marked_objects = 0;
 31226|             }
 31227|             if (concurrent_p && !reset_only_p)
 31228|             {
 31229|                 current_bgc_state = bgc_revisit_uoh;
 31230|             }
 31231|             small_object_segments = FALSE;
 31232|             dprintf (3, ("now revisiting large object segments"));
 31233|         }
 31234|         else
 31235|         {
 31236|             if (reset_only_p)
 31237|             {
 31238|                 dprintf (GTC_LOG, ("h%d: tdp: %zd", heap_number, total_dirtied_pages));
 31239|             }
 31240|             else
 31241|             {
 31242|                 dprintf (GTC_LOG, ("h%d: LOH: dp:%zd; mo: %zd", heap_number, total_dirtied_pages, total_marked_objects));
 31243|                 fire_revisit_event (total_dirtied_pages, total_marked_objects, TRUE);
 31244|             }
 31245|         }
 31246|     }
 31247| }
 31248| void gc_heap::background_grow_c_mark_list()
 31249| {
 31250|     assert (c_mark_list_index >= c_mark_list_length);
 31251|     BOOL should_drain_p = FALSE;
 31252|     THREAD_FROM_HEAP;
 31253| #ifndef MULTIPLE_HEAPS
 31254|     const int thread = heap_number;
 31255| #endif //!MULTIPLE_HEAPS
 31256|     dprintf (2, ("stack copy buffer overflow"));
 31257|     uint8_t** new_c_mark_list = 0;
 31258|     {
 31259|         FAULT_NOT_FATAL();
 31260|         if (c_mark_list_length >= (SIZE_T_MAX / (2 * sizeof (uint8_t*))))
 31261|         {
 31262|             should_drain_p = TRUE;
 31263|         }
 31264|         else
 31265|         {
 31266|             new_c_mark_list = new (nothrow) uint8_t*[c_mark_list_length*2];
 31267|             if (new_c_mark_list == 0)
 31268|             {
 31269|                 should_drain_p = TRUE;
 31270|             }
 31271|         }
 31272|     }
 31273|     if (should_drain_p)
 31274|     {
 31275|         dprintf (2, ("No more memory for the stacks copy, draining.."));
 31276|         background_drain_mark_list (thread);
 31277|     }
 31278|     else
 31279|     {
 31280|         assert (new_c_mark_list);
 31281|         memcpy (new_c_mark_list, c_mark_list, c_mark_list_length*sizeof(uint8_t*));
 31282|         c_mark_list_length = c_mark_list_length*2;
 31283|         dprintf (5555, ("h%d replacing mark list at %Ix with %Ix", heap_number, (size_t)c_mark_list, (size_t)new_c_mark_list));
 31284|         delete[] c_mark_list;
 31285|         c_mark_list = new_c_mark_list;
 31286|     }
 31287| }
 31288| void gc_heap::background_promote_callback (Object** ppObject, ScanContext* sc,
 31289|                                   uint32_t flags)
 31290| {
 31291|     UNREFERENCED_PARAMETER(sc);
 31292|     assert (settings.concurrent);
 31293|     THREAD_NUMBER_FROM_CONTEXT;
 31294| #ifndef MULTIPLE_HEAPS
 31295|     const int thread = 0;
 31296| #endif //!MULTIPLE_HEAPS
 31297|     uint8_t* o = (uint8_t*)*ppObject;
 31298|     if (!is_in_find_object_range (o))
 31299|     {
 31300|         return;
 31301|     }
 31302|     HEAP_FROM_THREAD;
 31303|     gc_heap* hp = gc_heap::heap_of (o);
 31304|     if ((o < hp->background_saved_lowest_address) || (o >= hp->background_saved_highest_address))
 31305|     {
 31306|         return;
 31307|     }
 31308|     if (flags & GC_CALL_INTERIOR)
 31309|     {
 31310|         o = hp->find_object (o);
 31311|         if (o == 0)
 31312|             return;
 31313|     }
 31314| #ifdef FEATURE_CONSERVATIVE_GC
 31315|     if (GCConfig::GetConservativeGC() && ((CObjectHeader*)o)->IsFree())
 31316|     {
 31317|         return;
 31318|     }
 31319| #endif //FEATURE_CONSERVATIVE_GC
 31320| #ifdef _DEBUG
 31321|     ((CObjectHeader*)o)->Validate();
 31322| #endif //_DEBUG
 31323|     dprintf (3, ("Concurrent Background Promote %zx", (size_t)o));
 31324|     if (o && (size (o) > loh_size_threshold))
 31325|     {
 31326|         dprintf (3, ("Brc %zx", (size_t)o));
 31327|     }
 31328|     if (hpt->c_mark_list_index >= hpt->c_mark_list_length)
 31329|     {
 31330|         hpt->background_grow_c_mark_list();
 31331|     }
 31332|     dprintf (3, ("pushing %zx into mark_list", (size_t)o));
 31333|     hpt->c_mark_list [hpt->c_mark_list_index++] = o;
 31334|     STRESS_LOG3(LF_GC|LF_GCROOTS, LL_INFO1000000, "    GCHeap::Background Promote: Promote GC Root *%p = %p MT = %pT", ppObject, o, o ? ((Object*) o)->GetGCSafeMethodTable() : NULL);
 31335| }
 31336| void gc_heap::mark_absorb_new_alloc()
 31337| {
 31338|     fix_allocation_contexts (FALSE);
 31339|     gen0_bricks_cleared = FALSE;
 31340|     clear_gen0_bricks();
 31341| }
 31342| BOOL gc_heap::prepare_bgc_thread(gc_heap* gh)
 31343| {
 31344|     BOOL success = FALSE;
 31345|     BOOL thread_created = FALSE;
 31346|     dprintf (2, ("Preparing gc thread"));
 31347|     gh->bgc_threads_timeout_cs.Enter();
 31348|     if (!(gh->bgc_thread_running))
 31349|     {
 31350|         dprintf (2, ("GC thread not running"));
 31351|         if ((gh->bgc_thread == 0) && create_bgc_thread(gh))
 31352|         {
 31353|             success = TRUE;
 31354|             thread_created = TRUE;
 31355|         }
 31356|     }
 31357|     else
 31358|     {
 31359|         dprintf (3, ("GC thread already running"));
 31360|         success = TRUE;
 31361|     }
 31362|     gh->bgc_threads_timeout_cs.Leave();
 31363|     if(thread_created)
 31364|         FIRE_EVENT(GCCreateConcurrentThread_V1);
 31365|     return success;
 31366| }
 31367| BOOL gc_heap::create_bgc_thread(gc_heap* gh)
 31368| {
 31369|     assert (background_gc_done_event.IsValid());
 31370|     gh->bgc_thread_running = GCToEEInterface::CreateThread(gh->bgc_thread_stub, gh, true, ".NET BGC");
 31371|     return gh->bgc_thread_running;
 31372| }
 31373| BOOL gc_heap::create_bgc_threads_support (int number_of_heaps)
 31374| {
 31375|     BOOL ret = FALSE;
 31376|     dprintf (3, ("Creating concurrent GC thread for the first time"));
 31377|     if (!background_gc_done_event.CreateManualEventNoThrow(TRUE))
 31378|     {
 31379|         goto cleanup;
 31380|     }
 31381|     if (!bgc_threads_sync_event.CreateManualEventNoThrow(FALSE))
 31382|     {
 31383|         goto cleanup;
 31384|     }
 31385|     if (!ee_proceed_event.CreateAutoEventNoThrow(FALSE))
 31386|     {
 31387|         goto cleanup;
 31388|     }
 31389|     if (!bgc_start_event.CreateManualEventNoThrow(FALSE))
 31390|     {
 31391|         goto cleanup;
 31392|     }
 31393| #ifdef MULTIPLE_HEAPS
 31394|     bgc_t_join.init (number_of_heaps, join_flavor_bgc);
 31395| #else
 31396|     UNREFERENCED_PARAMETER(number_of_heaps);
 31397| #endif //MULTIPLE_HEAPS
 31398|     ret = TRUE;
 31399| cleanup:
 31400|     if (!ret)
 31401|     {
 31402|         if (background_gc_done_event.IsValid())
 31403|         {
 31404|             background_gc_done_event.CloseEvent();
 31405|         }
 31406|         if (bgc_threads_sync_event.IsValid())
 31407|         {
 31408|             bgc_threads_sync_event.CloseEvent();
 31409|         }
 31410|         if (ee_proceed_event.IsValid())
 31411|         {
 31412|             ee_proceed_event.CloseEvent();
 31413|         }
 31414|         if (bgc_start_event.IsValid())
 31415|         {
 31416|             bgc_start_event.CloseEvent();
 31417|         }
 31418|     }
 31419|     return ret;
 31420| }
 31421| BOOL gc_heap::create_bgc_thread_support()
 31422| {
 31423|     uint8_t** parr;
 31424|     parr = new (nothrow) uint8_t*[1 + OS_PAGE_SIZE / MIN_OBJECT_SIZE];
 31425|     if (!parr)
 31426|     {
 31427|         return FALSE;
 31428|     }
 31429|     make_c_mark_list (parr);
 31430|     return TRUE;
 31431| }
 31432| int gc_heap::check_for_ephemeral_alloc()
 31433| {
 31434|     int gen = ((settings.reason == reason_oos_soh) ? (max_generation - 1) : -1);
 31435|     if (gen == -1)
 31436|     {
 31437| #ifdef MULTIPLE_HEAPS
 31438|         for (int heap_index = 0; heap_index < n_heaps; heap_index++)
 31439| #endif //MULTIPLE_HEAPS
 31440|         {
 31441|             for (int i = 0; i < max_generation; i++)
 31442|             {
 31443| #ifdef MULTIPLE_HEAPS
 31444|                 if (g_heaps[heap_index]->get_new_allocation (i) <= 0)
 31445| #else
 31446|                 if (get_new_allocation (i) <= 0)
 31447| #endif //MULTIPLE_HEAPS
 31448|                 {
 31449|                     gen = max (gen, i);
 31450|                 }
 31451|                 else
 31452|                     break;
 31453|             }
 31454|         }
 31455|     }
 31456|     return gen;
 31457| }
 31458| void gc_heap::wait_to_proceed()
 31459| {
 31460|     assert (background_gc_done_event.IsValid());
 31461|     assert (bgc_start_event.IsValid());
 31462|     user_thread_wait(&ee_proceed_event, FALSE);
 31463| }
 31464| void gc_heap::start_c_gc()
 31465| {
 31466|     assert (background_gc_done_event.IsValid());
 31467|     assert (bgc_start_event.IsValid());
 31468|     background_gc_done_event.Wait(INFINITE, FALSE);
 31469|     background_gc_done_event.Reset();
 31470|     bgc_start_event.Set();
 31471| }
 31472| void gc_heap::do_background_gc()
 31473| {
 31474|     dprintf (2, ("starting a BGC"));
 31475| #ifdef MULTIPLE_HEAPS
 31476|     for (int i = 0; i < n_heaps; i++)
 31477|     {
 31478|         g_heaps[i]->init_background_gc();
 31479|     }
 31480| #else
 31481|     init_background_gc();
 31482| #endif //MULTIPLE_HEAPS
 31483| #ifdef BGC_SERVO_TUNING
 31484|     bgc_tuning::record_bgc_start();
 31485| #endif //BGC_SERVO_TUNING
 31486|     start_c_gc ();
 31487|     wait_to_proceed();
 31488| }
 31489| void gc_heap::kill_gc_thread()
 31490| {
 31491|     background_gc_done_event.CloseEvent();
 31492|     bgc_start_event.CloseEvent();
 31493|     bgc_threads_timeout_cs.Destroy();
 31494|     bgc_thread = 0;
 31495| }
 31496| void gc_heap::bgc_thread_function()
 31497| {
 31498|     assert (background_gc_done_event.IsValid());
 31499|     assert (bgc_start_event.IsValid());
 31500|     dprintf (3, ("gc_thread thread starting..."));
 31501|     BOOL do_exit = FALSE;
 31502|     bool cooperative_mode = true;
 31503|     bgc_thread_id.SetToCurrentThread();
 31504|     dprintf (1, ("bgc_thread_id is set to %x", (uint32_t)GCToOSInterface::GetCurrentThreadIdForLogging()));
 31505|     while (1)
 31506|     {
 31507|         dprintf (3, ("bgc thread: waiting..."));
 31508|         cooperative_mode = enable_preemptive ();
 31509|         uint32_t result = bgc_start_event.Wait(
 31510| #ifdef _DEBUG
 31511| #ifdef MULTIPLE_HEAPS
 31512|                                              INFINITE,
 31513| #else
 31514|                                              2000,
 31515| #endif //MULTIPLE_HEAPS
 31516| #else //_DEBUG
 31517| #ifdef MULTIPLE_HEAPS
 31518|                                              INFINITE,
 31519| #else
 31520|                                              20000,
 31521| #endif //MULTIPLE_HEAPS
 31522| #endif //_DEBUG
 31523|             FALSE);
 31524|         dprintf (2, ("gc thread: finished waiting"));
 31525|         if (result == WAIT_TIMEOUT)
 31526|         {
 31527|             dprintf (1, ("GC thread timeout"));
 31528|             bgc_threads_timeout_cs.Enter();
 31529|             if (!keep_bgc_threads_p)
 31530|             {
 31531|                 dprintf (2, ("GC thread exiting"));
 31532|                 bgc_thread_running = FALSE;
 31533|                 bgc_thread = 0;
 31534|                 bgc_thread_id.Clear();
 31535|                 do_exit = TRUE;
 31536|             }
 31537|             bgc_threads_timeout_cs.Leave();
 31538|             if (do_exit)
 31539|                 break;
 31540|             else
 31541|             {
 31542|                 dprintf (3, ("GC thread needed, not exiting"));
 31543|                 continue;
 31544|             }
 31545|         }
 31546|         if (!settings.concurrent)
 31547|         {
 31548|             dprintf (3, ("no concurrent GC needed, exiting"));
 31549|             break;
 31550|         }
 31551|         gc_background_running = TRUE;
 31552|         dprintf (2, (ThreadStressLog::gcStartBgcThread(), heap_number,
 31553|             generation_free_list_space (generation_of (max_generation)),
 31554|             generation_free_obj_space (generation_of (max_generation)),
 31555|             dd_fragmentation (dynamic_data_of (max_generation))));
 31556| #ifdef DYNAMIC_HEAP_COUNT
 31557|         if (n_heaps <= heap_number)
 31558|         {
 31559|             dprintf (9999, ("BGC thread %d idle (%d heaps) (gc%Id)", heap_number, n_heaps, VolatileLoadWithoutBarrier (&settings.gc_index)));
 31560|             bgc_idle_thread_event.Wait(INFINITE, FALSE);
 31561|             dprintf (9999, ("BGC thread %d waking from idle (%d heaps) (gc%Id)", heap_number, n_heaps, VolatileLoadWithoutBarrier (&settings.gc_index)));
 31562|             continue;
 31563|         }
 31564| #endif //DYNAMIC_HEAP_COUNT
 31565|         gc1();
 31566| #ifndef DOUBLY_LINKED_FL
 31567|         current_bgc_state = bgc_not_in_process;
 31568| #endif //!DOUBLY_LINKED_FL
 31569|         enable_preemptive ();
 31570| #ifdef MULTIPLE_HEAPS
 31571|         bgc_t_join.join(this, gc_join_done);
 31572|         if (bgc_t_join.joined())
 31573| #endif //MULTIPLE_HEAPS
 31574|         {
 31575|             enter_spin_lock (&gc_lock);
 31576|             dprintf (SPINLOCK_LOG, ("bgc Egc"));
 31577|             bgc_start_event.Reset();
 31578|             do_post_gc();
 31579| #ifdef MULTIPLE_HEAPS
 31580|             for (int gen = max_generation; gen < total_generation_count; gen++)
 31581|             {
 31582|                 size_t desired_per_heap = 0;
 31583|                 size_t total_desired = 0;
 31584|                 gc_heap* hp = 0;
 31585|                 dynamic_data* dd;
 31586|                 for (int i = 0; i < n_heaps; i++)
 31587|                 {
 31588|                     hp = g_heaps[i];
 31589|                     dd = hp->dynamic_data_of (gen);
 31590|                     size_t temp_total_desired = total_desired + dd_desired_allocation (dd);
 31591|                     if (temp_total_desired < total_desired)
 31592|                     {
 31593|                         total_desired = (size_t)MAX_PTR;
 31594|                         break;
 31595|                     }
 31596|                     total_desired = temp_total_desired;
 31597|                 }
 31598|                 desired_per_heap = Align ((total_desired/n_heaps), get_alignment_constant (FALSE));
 31599|                 if (gen >= loh_generation)
 31600|                 {
 31601|                     desired_per_heap = exponential_smoothing (gen, dd_collection_count (dynamic_data_of (max_generation)), desired_per_heap);
 31602|                 }
 31603|                 for (int i = 0; i < n_heaps; i++)
 31604|                 {
 31605|                     hp = gc_heap::g_heaps[i];
 31606|                     dd = hp->dynamic_data_of (gen);
 31607|                     dd_desired_allocation (dd) = desired_per_heap;
 31608|                     dd_gc_new_allocation (dd) = desired_per_heap;
 31609|                     dd_new_allocation (dd) = desired_per_heap;
 31610|                 }
 31611|             }
 31612| #endif //MULTIPLE_HEAPS
 31613| #ifdef MULTIPLE_HEAPS
 31614|             fire_pevents();
 31615| #endif //MULTIPLE_HEAPS
 31616|             c_write (settings.concurrent, FALSE);
 31617|             gc_background_running = FALSE;
 31618|             keep_bgc_threads_p = FALSE;
 31619|             background_gc_done_event.Set();
 31620|             dprintf (SPINLOCK_LOG, ("bgc Lgc"));
 31621|             leave_spin_lock (&gc_lock);
 31622| #ifdef MULTIPLE_HEAPS
 31623|             dprintf(1, ("End of BGC"));
 31624|             bgc_t_join.restart();
 31625| #endif //MULTIPLE_HEAPS
 31626|         }
 31627|     }
 31628|     FIRE_EVENT(GCTerminateConcurrentThread_V1);
 31629|     dprintf (3, ("bgc_thread thread exiting"));
 31630|     return;
 31631| }
 31632| #ifdef BGC_SERVO_TUNING
 31633| bool gc_heap::bgc_tuning::stepping_trigger (uint32_t current_memory_load, size_t current_gen2_count)
 31634| {
 31635|     if (!bgc_tuning::enable_fl_tuning)
 31636|     {
 31637|         return false;
 31638|     }
 31639|     bool stepping_trigger_p = false;
 31640|     if (use_stepping_trigger_p)
 31641|     {
 31642|         dprintf (BGC_TUNING_LOG, ("current ml: %d, goal: %d",
 31643|             current_memory_load, memory_load_goal));
 31644|         if ((current_memory_load <= (memory_load_goal * 2 / 3)) ||
 31645|             ((memory_load_goal > current_memory_load) &&
 31646|              ((memory_load_goal - current_memory_load) > (stepping_interval * 3))))
 31647|         {
 31648|             int memory_load_delta = (int)current_memory_load - (int)last_stepping_mem_load;
 31649|             if (memory_load_delta >= (int)stepping_interval)
 31650|             {
 31651|                 stepping_trigger_p = (current_gen2_count == last_stepping_bgc_count);
 31652|                 if (stepping_trigger_p)
 31653|                 {
 31654|                     current_gen2_count++;
 31655|                 }
 31656|                 dprintf (BGC_TUNING_LOG, ("current ml: %u - %u = %d (>= %u), gen2 count: %zu->%zu, stepping trigger: %s ",
 31657|                     current_memory_load, last_stepping_mem_load, memory_load_delta, stepping_interval,
 31658|                     last_stepping_bgc_count, current_gen2_count,
 31659|                     (stepping_trigger_p ? "yes" : "no")));
 31660|                 last_stepping_mem_load = current_memory_load;
 31661|                 last_stepping_bgc_count = current_gen2_count;
 31662|             }
 31663|         }
 31664|         else
 31665|         {
 31666|             use_stepping_trigger_p = false;
 31667|         }
 31668|     }
 31669|     return stepping_trigger_p;
 31670| }
 31671| bool gc_heap::bgc_tuning::should_trigger_bgc_loh()
 31672| {
 31673|     if (fl_tuning_triggered)
 31674|     {
 31675| #ifdef MULTIPLE_HEAPS
 31676|         gc_heap* hp = g_heaps[0];
 31677| #else
 31678|         gc_heap* hp = pGenGCHeap;
 31679| #endif //MULTIPLE_HEAPS
 31680|         if (!(gc_heap::background_running_p()))
 31681|         {
 31682|             size_t current_alloc = get_total_servo_alloc (loh_generation);
 31683|             tuning_calculation* current_gen_calc = &gen_calc[loh_generation - max_generation];
 31684|             if (current_alloc < current_gen_calc->last_bgc_end_alloc)
 31685|             {
 31686|                 dprintf (BGC_TUNING_LOG, ("BTL: current alloc: %zd, last alloc: %zd?",
 31687|                     current_alloc, current_gen_calc->last_bgc_end_alloc));
 31688|             }
 31689|             bool trigger_p = ((current_alloc - current_gen_calc->last_bgc_end_alloc) >= current_gen_calc->alloc_to_trigger);
 31690|             dprintf (2, ("BTL3: LOH a %zd, la: %zd(%zd), %zd",
 31691|                     current_alloc, current_gen_calc->last_bgc_end_alloc,
 31692|                     (current_alloc - current_gen_calc->last_bgc_end_alloc),
 31693|                     current_gen_calc->alloc_to_trigger));
 31694|             if (trigger_p)
 31695|             {
 31696|                 dprintf (BGC_TUNING_LOG, ("BTL3: LOH detected (%zd - %zd) >= %zd, TRIGGER",
 31697|                         current_alloc, current_gen_calc->last_bgc_end_alloc, current_gen_calc->alloc_to_trigger));
 31698|                 return true;
 31699|             }
 31700|         }
 31701|     }
 31702|     return false;
 31703| }
 31704| bool gc_heap::bgc_tuning::should_trigger_bgc()
 31705| {
 31706|     if (!bgc_tuning::enable_fl_tuning || gc_heap::background_running_p())
 31707|     {
 31708|         return false;
 31709|     }
 31710|     if (settings.reason == reason_bgc_tuning_loh)
 31711|     {
 31712|         bgc_tuning::next_bgc_p = true;
 31713|         dprintf (BGC_TUNING_LOG, ("BTL LOH triggered"));
 31714|         return true;
 31715|     }
 31716|     if (!bgc_tuning::next_bgc_p &&
 31717|         !fl_tuning_triggered &&
 31718|         (gc_heap::settings.entry_memory_load >= (memory_load_goal * 2 / 3)) &&
 31719|         (gc_heap::full_gc_counts[gc_type_background] >= 2))
 31720|     {
 31721|         next_bgc_p = true;
 31722|         gen_calc[0].first_alloc_to_trigger = gc_heap::get_total_servo_alloc (max_generation);
 31723|         gen_calc[1].first_alloc_to_trigger = gc_heap::get_total_servo_alloc (loh_generation);
 31724|         dprintf (BGC_TUNING_LOG, ("BTL[GTC] mem high enough: %d(goal: %d), %zd BGCs done, g2a=%zd, g3a=%zd, trigger FL tuning!",
 31725|             gc_heap::settings.entry_memory_load, memory_load_goal,
 31726|             gc_heap::full_gc_counts[gc_type_background],
 31727|             gen_calc[0].first_alloc_to_trigger,
 31728|             gen_calc[1].first_alloc_to_trigger));
 31729|     }
 31730|     if (bgc_tuning::next_bgc_p)
 31731|     {
 31732|         dprintf (BGC_TUNING_LOG, ("BTL started FL tuning"));
 31733|         return true;
 31734|     }
 31735|     if (!fl_tuning_triggered)
 31736|     {
 31737|         return false;
 31738|     }
 31739|     int index = 0;
 31740|     bgc_tuning::tuning_calculation* current_gen_calc = 0;
 31741|     index = 0;
 31742|     current_gen_calc = &bgc_tuning::gen_calc[index];
 31743| #ifdef MULTIPLE_HEAPS
 31744|     gc_heap* hp = g_heaps[0];
 31745| #else
 31746|     gc_heap* hp = pGenGCHeap;
 31747| #endif //MULTIPLE_HEAPS
 31748|     size_t current_gen1_index = dd_collection_count (hp->dynamic_data_of (max_generation - 1));
 31749|     size_t gen1_so_far = current_gen1_index - gen1_index_last_bgc_end;
 31750|     if (current_gen_calc->alloc_to_trigger > 0)
 31751|     {
 31752|         size_t current_alloc = get_total_servo_alloc (max_generation);
 31753|         if ((current_alloc - current_gen_calc->last_bgc_end_alloc) >= current_gen_calc->alloc_to_trigger)
 31754|         {
 31755|             dprintf (BGC_TUNING_LOG, ("BTL2: SOH detected (%zd - %zd) >= %zd, TRIGGER",
 31756|                     current_alloc, current_gen_calc->last_bgc_end_alloc, current_gen_calc->alloc_to_trigger));
 31757|             settings.reason = reason_bgc_tuning_soh;
 31758|             return true;
 31759|         }
 31760|     }
 31761|     return false;
 31762| }
 31763| bool gc_heap::bgc_tuning::should_delay_alloc (int gen_number)
 31764| {
 31765|     if ((gen_number != max_generation) || !bgc_tuning::enable_fl_tuning)
 31766|         return false;
 31767|     if (current_c_gc_state == c_gc_state_planning)
 31768|     {
 31769|         int i = 0;
 31770| #ifdef MULTIPLE_HEAPS
 31771|         for (; i < gc_heap::n_heaps; i++)
 31772|         {
 31773|             gc_heap* hp = gc_heap::g_heaps[i];
 31774|             size_t current_fl_size = generation_free_list_space (hp->generation_of (max_generation));
 31775|             size_t last_bgc_fl_size = hp->bgc_maxgen_end_fl_size;
 31776| #else
 31777|         {
 31778|             size_t current_fl_size = generation_free_list_space (generation_of (max_generation));
 31779|             size_t last_bgc_fl_size = bgc_maxgen_end_fl_size;
 31780| #endif //MULTIPLE_HEAPS
 31781|             if (last_bgc_fl_size)
 31782|             {
 31783|                 float current_flr = (float) current_fl_size / (float)last_bgc_fl_size;
 31784|                 if (current_flr < 0.4)
 31785|                 {
 31786|                     dprintf (BGC_TUNING_LOG, ("BTL%d h%d last fl %zd, curr fl %zd (%.3f) d1",
 31787|                             gen_number, i, last_bgc_fl_size, current_fl_size, current_flr));
 31788|                     return true;
 31789|                 }
 31790|             }
 31791|         }
 31792|     }
 31793|     return false;
 31794| }
 31795| void gc_heap::bgc_tuning::update_bgc_start (int gen_number, size_t num_gen1s_since_end)
 31796| {
 31797|     int tuning_data_index = gen_number - max_generation;
 31798|     tuning_calculation* current_gen_calc = &gen_calc[tuning_data_index];
 31799|     tuning_stats* current_gen_stats = &gen_stats[tuning_data_index];
 31800|     size_t total_generation_size = get_total_generation_size (gen_number);
 31801|     ptrdiff_t current_bgc_fl_size = get_total_generation_fl_size (gen_number);
 31802|     double physical_gen_flr = (double)current_bgc_fl_size * 100.0 / (double)total_generation_size;
 31803|     ptrdiff_t artificial_additional_fl = 0;
 31804|     if (fl_tuning_triggered)
 31805|     {
 31806|         artificial_additional_fl = ((current_gen_calc->end_gen_size_goal > total_generation_size) ? (current_gen_calc->end_gen_size_goal - total_generation_size) : 0);
 31807|         total_generation_size += artificial_additional_fl;
 31808|         current_bgc_fl_size += artificial_additional_fl;
 31809|     }
 31810|     current_gen_calc->current_bgc_start_flr = (double)current_bgc_fl_size * 100.0 / (double)total_generation_size;
 31811|     size_t current_alloc = get_total_servo_alloc (gen_number);
 31812|     dprintf (BGC_TUNING_LOG, ("BTL%d: st a: %zd, la: %zd",
 31813|         gen_number, current_alloc, current_gen_stats->last_alloc));
 31814|     current_gen_stats->last_alloc_end_to_start = current_alloc - current_gen_stats->last_alloc;
 31815|     current_gen_stats->last_alloc = current_alloc;
 31816|     current_gen_calc->actual_alloc_to_trigger = current_alloc - current_gen_calc->last_bgc_end_alloc;
 31817|     dprintf (BGC_TUNING_LOG, ("BTL%d: st: %zd g1s (%zd->%zd/gen1) since end, flr: %.3f(afl: %zd, %.3f)",
 31818|              gen_number, actual_num_gen1s_to_trigger,
 31819|              current_gen_stats->last_alloc_end_to_start,
 31820|              (num_gen1s_since_end ? (current_gen_stats->last_alloc_end_to_start / num_gen1s_since_end) : 0),
 31821|              current_gen_calc->current_bgc_start_flr, artificial_additional_fl, physical_gen_flr));
 31822| }
 31823| void gc_heap::bgc_tuning::record_bgc_start()
 31824| {
 31825|     if (!bgc_tuning::enable_fl_tuning)
 31826|         return;
 31827|     uint64_t elapsed_time_so_far = GetHighPrecisionTimeStamp() - process_start_time;
 31828|     size_t current_gen1_index = get_current_gc_index (max_generation - 1);
 31829|     dprintf (BGC_TUNING_LOG, ("BTL: g2t[st][g1 %zd]: %0.3f minutes",
 31830|         current_gen1_index,
 31831|         (double)elapsed_time_so_far / (double)1000000 / (double)60));
 31832|     actual_num_gen1s_to_trigger = current_gen1_index - gen1_index_last_bgc_end;
 31833|     gen1_index_last_bgc_start = current_gen1_index;
 31834|     update_bgc_start (max_generation, actual_num_gen1s_to_trigger);
 31835|     update_bgc_start (loh_generation, actual_num_gen1s_to_trigger);
 31836| }
 31837| double convert_range (double lower, double upper, double num, double percentage)
 31838| {
 31839|     double d = num - lower;
 31840|     if (d < 0.0)
 31841|         return 0.0;
 31842|     else
 31843|     {
 31844|         d = min ((upper - lower), d);
 31845|         return (d * percentage);
 31846|     }
 31847| }
 31848| double calculate_gradual_d (double delta_double, double step)
 31849| {
 31850|     bool changed_sign = false;
 31851|     if (delta_double < 0.0)
 31852|     {
 31853|         delta_double = -delta_double;
 31854|         changed_sign = true;
 31855|     }
 31856|     double res = 0;
 31857|     double current_lower_limit = 0;
 31858|     double current_ratio = 1.0;
 31859|     while (current_ratio > 0.22)
 31860|     {
 31861|         res += convert_range (current_lower_limit, (current_lower_limit + step), delta_double, current_ratio);
 31862|         current_lower_limit += step;
 31863|         current_ratio *= 0.6;
 31864|     }
 31865|     if (changed_sign)
 31866|         res = -res;
 31867|     return res;
 31868| }
 31869| void gc_heap::bgc_tuning::update_bgc_sweep_start (int gen_number, size_t num_gen1s_since_start)
 31870| {
 31871|     int tuning_data_index = gen_number - max_generation;
 31872|     tuning_calculation* current_gen_calc = &gen_calc[tuning_data_index];
 31873|     tuning_stats* current_gen_stats = &gen_stats[tuning_data_index];
 31874|     size_t total_generation_size = 0;
 31875|     ptrdiff_t current_bgc_fl_size = 0;
 31876|     total_generation_size = get_total_generation_size (gen_number);
 31877|     current_bgc_fl_size = get_total_generation_fl_size (gen_number);
 31878|     double physical_gen_flr = (double)current_bgc_fl_size * 100.0 / (double)total_generation_size;
 31879|     ptrdiff_t artificial_additional_fl = 0;
 31880|     if (fl_tuning_triggered)
 31881|     {
 31882|         artificial_additional_fl = ((current_gen_calc->end_gen_size_goal > total_generation_size) ? (current_gen_calc->end_gen_size_goal - total_generation_size) : 0);
 31883|         total_generation_size += artificial_additional_fl;
 31884|         current_bgc_fl_size += artificial_additional_fl;
 31885|     }
 31886|     current_gen_calc->current_bgc_sweep_flr = (double)current_bgc_fl_size * 100.0 / (double)total_generation_size;
 31887|     size_t current_alloc = get_total_servo_alloc (gen_number);
 31888|     dprintf (BGC_TUNING_LOG, ("BTL%d: sw a: %zd, la: %zd",
 31889|         gen_number, current_alloc, current_gen_stats->last_alloc));
 31890|     current_gen_stats->last_alloc_start_to_sweep = current_alloc - current_gen_stats->last_alloc;
 31891|     current_gen_stats->last_alloc = 0;
 31892| #ifdef SIMPLE_DPRINTF
 31893|     dprintf (BGC_TUNING_LOG, ("BTL%d: sflr: %.3f%%->%.3f%% (%zd->%zd, %zd->%zd) (%zd:%zd-%zd/gen1) since start (afl: %zd, %.3f)",
 31894|              gen_number,
 31895|              current_gen_calc->last_bgc_flr, current_gen_calc->current_bgc_sweep_flr,
 31896|              current_gen_calc->last_bgc_size, total_generation_size,
 31897|              current_gen_stats->last_bgc_fl_size, current_bgc_fl_size,
 31898|              num_gen1s_since_start, current_gen_stats->last_alloc_start_to_sweep,
 31899|              (num_gen1s_since_start? (current_gen_stats->last_alloc_start_to_sweep / num_gen1s_since_start) : 0),
 31900|              artificial_additional_fl, physical_gen_flr));
 31901| #endif //SIMPLE_DPRINTF
 31902| }
 31903| void gc_heap::bgc_tuning::record_bgc_sweep_start()
 31904| {
 31905|     if (!bgc_tuning::enable_fl_tuning)
 31906|         return;
 31907|     size_t current_gen1_index = get_current_gc_index (max_generation - 1);
 31908|     size_t num_gen1s_since_start = current_gen1_index - gen1_index_last_bgc_start;
 31909|     gen1_index_last_bgc_sweep = current_gen1_index;
 31910|     uint64_t elapsed_time_so_far = GetHighPrecisionTimeStamp() - process_start_time;
 31911|     dprintf (BGC_TUNING_LOG, ("BTL: g2t[sw][g1 %zd]: %0.3f minutes",
 31912|         current_gen1_index,
 31913|         (double)elapsed_time_so_far / (double)1000000 / (double)60));
 31914|     update_bgc_sweep_start (max_generation, num_gen1s_since_start);
 31915|     update_bgc_sweep_start (loh_generation, num_gen1s_since_start);
 31916| }
 31917| void gc_heap::bgc_tuning::calculate_tuning (int gen_number, bool use_this_loop_p)
 31918| {
 31919|     BOOL use_kd_p = enable_kd;
 31920|     BOOL use_ki_p = enable_ki;
 31921|     BOOL use_smooth_p = enable_smooth;
 31922|     BOOL use_tbh_p = enable_tbh;
 31923|     BOOL use_ff_p = enable_ff;
 31924|     int tuning_data_index = gen_number - max_generation;
 31925|     tuning_calculation* current_gen_calc = &gen_calc[tuning_data_index];
 31926|     tuning_stats* current_gen_stats = &gen_stats[tuning_data_index];
 31927|     bgc_size_data* data = &current_bgc_end_data[tuning_data_index];
 31928|     size_t total_generation_size = data->gen_size;
 31929|     size_t current_bgc_fl = data->gen_fl_size;
 31930|     size_t current_bgc_surv_size = get_total_surv_size (gen_number);
 31931|     size_t current_bgc_begin_data_size = get_total_begin_data_size (gen_number);
 31932|     size_t current_alloc = get_total_servo_alloc (gen_number);
 31933|     dprintf (BGC_TUNING_LOG, ("BTL%d: en a: %zd, la: %zd, lbgca: %zd",
 31934|         gen_number, current_alloc, current_gen_stats->last_alloc, current_gen_calc->last_bgc_end_alloc));
 31935|     double current_bgc_surv_rate = (current_bgc_begin_data_size == 0) ?
 31936|                                     0 : ((double)current_bgc_surv_size * 100.0 / (double)current_bgc_begin_data_size);
 31937|     current_gen_stats->last_alloc_sweep_to_end = current_alloc - current_gen_stats->last_alloc;
 31938|     size_t gen1_index = get_current_gc_index (max_generation - 1);
 31939|     size_t gen2_index = get_current_gc_index (max_generation);
 31940|     size_t num_gen1s_since_sweep = gen1_index - gen1_index_last_bgc_sweep;
 31941|     size_t num_gen1s_bgc_end = gen1_index - gen1_index_last_bgc_end;
 31942|     size_t gen_end_size_goal = current_gen_calc->end_gen_size_goal;
 31943|     double gen_sweep_flr_goal = current_gen_calc->sweep_flr_goal;
 31944|     size_t last_gen_alloc_to_trigger = current_gen_calc->alloc_to_trigger;
 31945|     size_t gen_actual_alloc_to_trigger = current_gen_calc->actual_alloc_to_trigger;
 31946|     size_t last_gen_alloc_to_trigger_0 = current_gen_calc->alloc_to_trigger_0;
 31947|     double current_end_to_sweep_flr = current_gen_calc->last_bgc_flr - current_gen_calc->current_bgc_sweep_flr;
 31948|     bool current_sweep_above_p = (current_gen_calc->current_bgc_sweep_flr > gen_sweep_flr_goal);
 31949| #ifdef SIMPLE_DPRINTF
 31950|     dprintf (BGC_TUNING_LOG, ("BTL%d: sflr: c %.3f (%s), p %s, palloc: %zd, aalloc %zd(%s)",
 31951|         gen_number,
 31952|         current_gen_calc->current_bgc_sweep_flr,
 31953|         (current_sweep_above_p ? "above" : "below"),
 31954|         (current_gen_calc->last_sweep_above_p ? "above" : "below"),
 31955|         last_gen_alloc_to_trigger,
 31956|         current_gen_calc->actual_alloc_to_trigger,
 31957|         (use_this_loop_p ? "this" : "last")));
 31958|     dprintf (BGC_TUNING_LOG, ("BTL%d-en[g1: %zd, g2: %zd]: end fl: %zd (%zd: S-%zd, %.3f%%->%.3f%%)",
 31959|             gen_number,
 31960|             gen1_index, gen2_index, current_bgc_fl,
 31961|             total_generation_size, current_bgc_surv_size,
 31962|             current_gen_stats->last_bgc_surv_rate, current_bgc_surv_rate));
 31963|     dprintf (BGC_TUNING_LOG, ("BTLS%d sflr: %.3f, end-start: %zd(%zd), start-sweep: %zd(%zd), sweep-end: %zd(%zd)",
 31964|             gen_number,
 31965|             current_gen_calc->current_bgc_sweep_flr,
 31966|             (gen1_index_last_bgc_start - gen1_index_last_bgc_end), current_gen_stats->last_alloc_end_to_start,
 31967|             (gen1_index_last_bgc_sweep - gen1_index_last_bgc_start), current_gen_stats->last_alloc_start_to_sweep,
 31968|             num_gen1s_since_sweep, current_gen_stats->last_alloc_sweep_to_end));
 31969| #endif //SIMPLE_DPRINTF
 31970|     size_t saved_alloc_to_trigger = 0;
 31971|     double current_alloc_to_trigger = 0.0;
 31972|     if (!fl_tuning_triggered && use_tbh_p)
 31973|     {
 31974|         current_gen_calc->alloc_to_trigger_0 = current_gen_calc->actual_alloc_to_trigger;
 31975|         dprintf (BGC_TUNING_LOG, ("BTL%d[g1: %zd]: not in FL tuning yet, setting alloc_to_trigger_0 to %zd",
 31976|                  gen_number,
 31977|                  gen1_index, current_gen_calc->alloc_to_trigger_0));
 31978|     }
 31979|     if (fl_tuning_triggered)
 31980|     {
 31981|         BOOL tuning_kd_finished_p = FALSE;
 31982|         double max_alloc_to_trigger = ((double)current_bgc_fl * (100 - gen_sweep_flr_goal) / 100.0);
 31983|         double min_alloc_to_trigger = (double)current_bgc_fl * 0.05;
 31984|         {
 31985|             if (current_gen_calc->current_bgc_sweep_flr < 0.0)
 31986|             {
 31987|                 dprintf (BGC_TUNING_LOG, ("BTL%d: sflr is %.3f!!! < 0, make it 0", gen_number, current_gen_calc->current_bgc_sweep_flr));
 31988|                 current_gen_calc->current_bgc_sweep_flr = 0.0;
 31989|             }
 31990|             double adjusted_above_goal_kp = above_goal_kp;
 31991|             double above_goal_distance = current_gen_calc->current_bgc_sweep_flr - gen_sweep_flr_goal;
 31992|             if (use_ki_p)
 31993|             {
 31994|                 if (current_gen_calc->above_goal_accu_error > max_alloc_to_trigger)
 31995|                 {
 31996|                     dprintf (BGC_TUNING_LOG, ("g%d: ae TB! %.1f->%.1f", gen_number, current_gen_calc->above_goal_accu_error, max_alloc_to_trigger));
 31997|                 }
 31998|                 else if (current_gen_calc->above_goal_accu_error < min_alloc_to_trigger)
 31999|                 {
 32000|                     dprintf (BGC_TUNING_LOG, ("g%d: ae TS! %.1f->%.1f", gen_number, current_gen_calc->above_goal_accu_error, min_alloc_to_trigger));
 32001|                 }
 32002|                 current_gen_calc->above_goal_accu_error = min (max_alloc_to_trigger, current_gen_calc->above_goal_accu_error);
 32003|                 current_gen_calc->above_goal_accu_error = max (min_alloc_to_trigger, current_gen_calc->above_goal_accu_error);
 32004|                 double above_goal_ki_gain = above_goal_ki * above_goal_distance * current_bgc_fl;
 32005|                 double temp_accu_error = current_gen_calc->above_goal_accu_error + above_goal_ki_gain;
 32006|                 if ((temp_accu_error > min_alloc_to_trigger) &&
 32007|                     (temp_accu_error < max_alloc_to_trigger))
 32008|                 {
 32009|                     current_gen_calc->above_goal_accu_error = temp_accu_error;
 32010|                 }
 32011|                 else
 32012|                 {
 32013|                     dprintf (BGC_TUNING_LOG, ("g%d: aae + %.1f=%.1f, exc", gen_number,
 32014|                             above_goal_ki_gain,
 32015|                             temp_accu_error));
 32016|                 }
 32017|             }
 32018|             {
 32019|                 saved_alloc_to_trigger = current_gen_calc->alloc_to_trigger;
 32020|                 current_alloc_to_trigger = adjusted_above_goal_kp * above_goal_distance * current_bgc_fl;
 32021|                 dprintf (BGC_TUNING_LOG, ("BTL%d: sflr %.3f above * %.4f * %zd = %zd bytes in alloc, la: %zd(+%zd), laa: %zd(+%zd)",
 32022|                         gen_number,
 32023|                         (current_gen_calc->current_bgc_sweep_flr - (double)gen_sweep_flr_goal),
 32024|                         adjusted_above_goal_kp,
 32025|                         current_bgc_fl,
 32026|                         (size_t)current_alloc_to_trigger,
 32027|                         saved_alloc_to_trigger,
 32028|                         (size_t)(current_alloc_to_trigger - (double)saved_alloc_to_trigger),
 32029|                         gen_actual_alloc_to_trigger,
 32030|                         (gen_actual_alloc_to_trigger - saved_alloc_to_trigger)));
 32031|                 if (use_ki_p)
 32032|                 {
 32033|                     current_alloc_to_trigger += current_gen_calc->above_goal_accu_error;
 32034|                     dprintf (BGC_TUNING_LOG, ("BTL%d: +accu err %zd=%zd",
 32035|                             gen_number,
 32036|                             (size_t)(current_gen_calc->above_goal_accu_error),
 32037|                             (size_t)current_alloc_to_trigger));
 32038|                 }
 32039|             }
 32040|             if (use_tbh_p)
 32041|             {
 32042|                 if (current_gen_calc->last_sweep_above_p != current_sweep_above_p)
 32043|                 {
 32044|                     size_t new_alloc_to_trigger_0 = (last_gen_alloc_to_trigger + last_gen_alloc_to_trigger_0) / 2;
 32045|                     dprintf (BGC_TUNING_LOG, ("BTL%d: tbh crossed SP, setting both to %zd", gen_number, new_alloc_to_trigger_0));
 32046|                     current_gen_calc->alloc_to_trigger_0 = new_alloc_to_trigger_0;
 32047|                     current_gen_calc->alloc_to_trigger = new_alloc_to_trigger_0;
 32048|                 }
 32049|                 tuning_kd_finished_p = TRUE;
 32050|             }
 32051|         }
 32052|         if (!tuning_kd_finished_p)
 32053|         {
 32054|             if (use_kd_p)
 32055|             {
 32056|                 saved_alloc_to_trigger = last_gen_alloc_to_trigger;
 32057|                 size_t alloc_delta = saved_alloc_to_trigger - gen_actual_alloc_to_trigger;
 32058|                 double adjust_ratio = (double)alloc_delta / (double)gen_actual_alloc_to_trigger;
 32059|                 double saved_adjust_ratio = adjust_ratio;
 32060|                 if (enable_gradual_d)
 32061|                 {
 32062|                     adjust_ratio = calculate_gradual_d (adjust_ratio, above_goal_kd);
 32063|                     dprintf (BGC_TUNING_LOG, ("BTL%d: gradual kd - reduced from %.3f to %.3f",
 32064|                             gen_number, saved_adjust_ratio, adjust_ratio));
 32065|                 }
 32066|                 else
 32067|                 {
 32068|                     double kd = above_goal_kd;
 32069|                     double neg_kd = 0 - kd;
 32070|                     if (adjust_ratio > kd) adjust_ratio = kd;
 32071|                     if (adjust_ratio < neg_kd) adjust_ratio = neg_kd;
 32072|                     dprintf (BGC_TUNING_LOG, ("BTL%d: kd - reduced from %.3f to %.3f",
 32073|                             gen_number, saved_adjust_ratio, adjust_ratio));
 32074|                 }
 32075|                 current_gen_calc->alloc_to_trigger = (size_t)((double)gen_actual_alloc_to_trigger * (1 + adjust_ratio));
 32076|                 dprintf (BGC_TUNING_LOG, ("BTL%d: kd %.3f, reduced it to %.3f * %zd, adjust %zd->%zd",
 32077|                         gen_number, saved_adjust_ratio,
 32078|                         adjust_ratio, gen_actual_alloc_to_trigger,
 32079|                         saved_alloc_to_trigger, current_gen_calc->alloc_to_trigger));
 32080|             }
 32081|             if (use_smooth_p && use_this_loop_p)
 32082|             {
 32083|                 saved_alloc_to_trigger = current_gen_calc->alloc_to_trigger;
 32084|                 size_t gen_smoothed_alloc_to_trigger = current_gen_calc->smoothed_alloc_to_trigger;
 32085|                 double current_num_gen1s_smooth_factor = (num_gen1s_smooth_factor > (double)num_bgcs_since_tuning_trigger) ?
 32086|                                                         (double)num_bgcs_since_tuning_trigger : num_gen1s_smooth_factor;
 32087|                 current_gen_calc->smoothed_alloc_to_trigger = (size_t)((double)saved_alloc_to_trigger / current_num_gen1s_smooth_factor +
 32088|                     ((double)gen_smoothed_alloc_to_trigger / current_num_gen1s_smooth_factor) * (current_num_gen1s_smooth_factor - 1.0));
 32089|                 dprintf (BGC_TUNING_LOG, ("BTL%d: smoothed %zd / %.3f + %zd / %.3f * %.3f adjust %zd->%zd",
 32090|                     gen_number, saved_alloc_to_trigger, current_num_gen1s_smooth_factor,
 32091|                     gen_smoothed_alloc_to_trigger, current_num_gen1s_smooth_factor,
 32092|                     (current_num_gen1s_smooth_factor - 1.0),
 32093|                     saved_alloc_to_trigger, current_gen_calc->smoothed_alloc_to_trigger));
 32094|                 current_gen_calc->alloc_to_trigger = current_gen_calc->smoothed_alloc_to_trigger;
 32095|             }
 32096|         }
 32097|         if (use_ff_p)
 32098|         {
 32099|             double next_end_to_sweep_flr = data->gen_flr - gen_sweep_flr_goal;
 32100|             if (next_end_to_sweep_flr > 0.0)
 32101|             {
 32102|                 saved_alloc_to_trigger = current_gen_calc->alloc_to_trigger;
 32103|                 double ff_ratio = next_end_to_sweep_flr / current_end_to_sweep_flr - 1;
 32104|                 if (use_this_loop_p)
 32105|                 {
 32106|                     double ff_step = above_goal_ff * 0.5;
 32107|                     double adjusted_above_goal_ff = above_goal_ff;
 32108|                     if (ff_ratio > 0)
 32109|                         adjusted_above_goal_ff -= ff_step;
 32110|                     else
 32111|                         adjusted_above_goal_ff += ff_step;
 32112|                     double adjusted_ff_ratio = ff_ratio * adjusted_above_goal_ff;
 32113|                     current_gen_calc->alloc_to_trigger = saved_alloc_to_trigger + (size_t)((double)saved_alloc_to_trigger * adjusted_ff_ratio);
 32114|                     dprintf (BGC_TUNING_LOG, ("BTL%d: ff (%.3f / %.3f - 1) * %.3f = %.3f adjust %zd->%zd",
 32115|                         gen_number, next_end_to_sweep_flr, current_end_to_sweep_flr, adjusted_above_goal_ff, adjusted_ff_ratio,
 32116|                         saved_alloc_to_trigger, current_gen_calc->alloc_to_trigger));
 32117|                 }
 32118|             }
 32119|         }
 32120|         if (use_this_loop_p)
 32121|         {
 32122|             if (current_alloc_to_trigger > max_alloc_to_trigger)
 32123|             {
 32124|                 dprintf (BGC_TUNING_LOG, ("BTL%d: TB! %.1f -> %.1f",
 32125|                     gen_number, current_alloc_to_trigger, max_alloc_to_trigger));
 32126|                 current_alloc_to_trigger = max_alloc_to_trigger;
 32127|             }
 32128|             if (current_alloc_to_trigger < min_alloc_to_trigger)
 32129|             {
 32130|                 dprintf (BGC_TUNING_LOG, ("BTL%d: TS! %zd -> %zd",
 32131|                         gen_number, (ptrdiff_t)current_alloc_to_trigger, (size_t)min_alloc_to_trigger));
 32132|                 current_alloc_to_trigger = min_alloc_to_trigger;
 32133|             }
 32134|             current_gen_calc->alloc_to_trigger = (size_t)current_alloc_to_trigger;
 32135|         }
 32136|         else
 32137|         {
 32138|             dprintf (BGC_TUNING_LOG, ("BTL%d: ag, revert %zd->%zd",
 32139|                 gen_number, current_gen_calc->alloc_to_trigger, last_gen_alloc_to_trigger));
 32140|             current_gen_calc->alloc_to_trigger = last_gen_alloc_to_trigger;
 32141|         }
 32142|     }
 32143|     if (next_bgc_p)
 32144|     {
 32145|         size_t first_alloc = (size_t)((double)current_gen_calc->first_alloc_to_trigger * 0.75);
 32146|         size_t min_first_alloc = current_bgc_fl / 20;
 32147|         current_gen_calc->alloc_to_trigger = max (first_alloc, min_first_alloc);
 32148|         dprintf (BGC_TUNING_LOG, ("BTL%d[g1: %zd]: BGC end, trigger FL, set gen%d alloc to max (0.75 of first: %zd, 5%% fl: %zd), actual alloc: %zd",
 32149|             gen_number, gen1_index, gen_number,
 32150|             first_alloc, min_first_alloc,
 32151|             current_gen_calc->actual_alloc_to_trigger));
 32152|     }
 32153|     dprintf (BGC_TUNING_LOG, ("BTL%d* %zd, %.3f, %.3f, %.3f, %.3f, %.3f, %zd, %zd, %zd, %zd",
 32154|                               gen_number,
 32155|                               total_generation_size,
 32156|                               current_gen_calc->current_bgc_start_flr,
 32157|                               current_gen_calc->current_bgc_sweep_flr,
 32158|                               current_bgc_end_data[tuning_data_index].gen_flr,
 32159|                               current_gen_stats->last_gen_increase_flr,
 32160|                               current_bgc_surv_rate,
 32161|                               actual_num_gen1s_to_trigger,
 32162|                               num_gen1s_bgc_end,
 32163|                               gen_actual_alloc_to_trigger,
 32164|                               current_gen_calc->alloc_to_trigger));
 32165|     gen1_index_last_bgc_end = gen1_index;
 32166|     current_gen_calc->last_bgc_size = total_generation_size;
 32167|     current_gen_calc->last_bgc_flr = current_bgc_end_data[tuning_data_index].gen_flr;
 32168|     current_gen_calc->last_sweep_above_p = current_sweep_above_p;
 32169|     current_gen_calc->last_bgc_end_alloc = current_alloc;
 32170|     current_gen_stats->last_bgc_physical_size = data->gen_physical_size;
 32171|     current_gen_stats->last_alloc_end_to_start = 0;
 32172|     current_gen_stats->last_alloc_start_to_sweep = 0;
 32173|     current_gen_stats->last_alloc_sweep_to_end = 0;
 32174|     current_gen_stats->last_alloc = current_alloc;
 32175|     current_gen_stats->last_bgc_fl_size = current_bgc_end_data[tuning_data_index].gen_fl_size;
 32176|     current_gen_stats->last_bgc_surv_rate = current_bgc_surv_rate;
 32177|     current_gen_stats->last_gen_increase_flr = 0;
 32178| }
 32179| void gc_heap::bgc_tuning::init_bgc_end_data (int gen_number, bool use_this_loop_p)
 32180| {
 32181|     int index = gen_number - max_generation;
 32182|     bgc_size_data* data = &current_bgc_end_data[index];
 32183|     size_t physical_size = get_total_generation_size (gen_number);
 32184|     ptrdiff_t physical_fl_size = get_total_generation_fl_size (gen_number);
 32185|     data->gen_actual_phys_fl_size = physical_fl_size;
 32186|     if (fl_tuning_triggered && !use_this_loop_p)
 32187|     {
 32188|         tuning_calculation* current_gen_calc = &gen_calc[gen_number - max_generation];
 32189|         if (current_gen_calc->actual_alloc_to_trigger > current_gen_calc->alloc_to_trigger)
 32190|         {
 32191|             dprintf (BGC_TUNING_LOG, ("BTL%d: gen alloc also exceeded %zd (la: %zd), no action",
 32192|                 gen_number, current_gen_calc->actual_alloc_to_trigger, current_gen_calc->alloc_to_trigger));
 32193|         }
 32194|         else
 32195|         {
 32196|             size_t remaining_alloc = current_gen_calc->alloc_to_trigger -
 32197|                                      current_gen_calc->actual_alloc_to_trigger;
 32198|             size_t gen_size = current_gen_calc->end_gen_size_goal;
 32199|             double sweep_flr = current_gen_calc->current_bgc_sweep_flr;
 32200|             size_t sweep_fl_size = (size_t)((double)gen_size * sweep_flr / 100.0);
 32201|             if (sweep_fl_size < remaining_alloc)
 32202|             {
 32203|                 dprintf (BGC_TUNING_LOG, ("BTL%d: sweep fl %zd < remain alloc %zd", gen_number, sweep_fl_size, remaining_alloc));
 32204|                 remaining_alloc = sweep_fl_size - (10 * 1024);
 32205|             }
 32206|             size_t new_sweep_fl_size = sweep_fl_size - remaining_alloc;
 32207|             ptrdiff_t signed_new_sweep_fl_size = sweep_fl_size - remaining_alloc;
 32208|             double new_current_bgc_sweep_flr = (double)new_sweep_fl_size * 100.0 / (double)gen_size;
 32209|             double signed_new_current_bgc_sweep_flr = (double)signed_new_sweep_fl_size * 100.0 / (double)gen_size;
 32210|             dprintf (BGC_TUNING_LOG, ("BTL%d: sg: %zd(%zd), sfl: %zd->%zd(%zd)(%.3f->%.3f(%.3f)), la: %zd, aa: %zd",
 32211|                 gen_number, gen_size, physical_size, sweep_fl_size,
 32212|                 new_sweep_fl_size, signed_new_sweep_fl_size,
 32213|                 sweep_flr, new_current_bgc_sweep_flr, signed_new_current_bgc_sweep_flr,
 32214|                 current_gen_calc->alloc_to_trigger, current_gen_calc->actual_alloc_to_trigger));
 32215|             current_gen_calc->actual_alloc_to_trigger = current_gen_calc->alloc_to_trigger;
 32216|             current_gen_calc->current_bgc_sweep_flr = new_current_bgc_sweep_flr;
 32217|             size_t current_bgc_surv_size = get_total_surv_size (gen_number);
 32218|             size_t current_bgc_begin_data_size = get_total_begin_data_size (gen_number);
 32219|             double current_bgc_surv_rate = (current_bgc_begin_data_size == 0) ?
 32220|                                             0 : ((double)current_bgc_surv_size / (double)current_bgc_begin_data_size);
 32221|             size_t remaining_alloc_surv = (size_t)((double)remaining_alloc * current_bgc_surv_rate);
 32222|             physical_fl_size -= remaining_alloc_surv;
 32223|             dprintf (BGC_TUNING_LOG, ("BTL%d: asfl %zd-%zd=%zd, flr %.3f->%.3f, %.3f%% s, fl %zd-%zd->%zd",
 32224|                 gen_number, sweep_fl_size, remaining_alloc, new_sweep_fl_size,
 32225|                 sweep_flr, current_gen_calc->current_bgc_sweep_flr,
 32226|                 (current_bgc_surv_rate * 100.0),
 32227|                 (physical_fl_size + remaining_alloc_surv),
 32228|                 remaining_alloc_surv, physical_fl_size));
 32229|         }
 32230|     }
 32231|     double physical_gen_flr = (double)physical_fl_size * 100.0 / (double)physical_size;
 32232|     data->gen_physical_size = physical_size;
 32233|     data->gen_physical_fl_size = physical_fl_size;
 32234|     data->gen_physical_flr = physical_gen_flr;
 32235| }
 32236| void gc_heap::bgc_tuning::calc_end_bgc_fl (int gen_number)
 32237| {
 32238|     int index = gen_number - max_generation;
 32239|     bgc_size_data* data = &current_bgc_end_data[index];
 32240|     tuning_calculation* current_gen_calc = &gen_calc[gen_number - max_generation];
 32241|     size_t virtual_size = current_gen_calc->end_gen_size_goal;
 32242|     size_t physical_size = data->gen_physical_size;
 32243|     ptrdiff_t physical_fl_size = data->gen_physical_fl_size;
 32244|     ptrdiff_t virtual_fl_size = (ptrdiff_t)virtual_size - (ptrdiff_t)physical_size;
 32245|     ptrdiff_t end_gen_fl_size = physical_fl_size + virtual_fl_size;
 32246|     if (end_gen_fl_size < 0)
 32247|     {
 32248|         end_gen_fl_size = 0;
 32249|     }
 32250|     data->gen_size = virtual_size;
 32251|     data->gen_fl_size = end_gen_fl_size;
 32252|     data->gen_flr = (double)(data->gen_fl_size) * 100.0 / (double)(data->gen_size);
 32253|     dprintf (BGC_TUNING_LOG, ("BTL%d: vfl: %zd, size %zd->%zd, fl %zd->%zd, flr %.3f->%.3f",
 32254|         gen_number, virtual_fl_size,
 32255|         data->gen_physical_size, data->gen_size,
 32256|         data->gen_physical_fl_size, data->gen_fl_size,
 32257|         data->gen_physical_flr, data->gen_flr));
 32258| }
 32259| double gc_heap::bgc_tuning::calculate_ml_tuning (uint64_t current_available_physical, bool reduce_p,
 32260|                                                  ptrdiff_t* _vfl_from_kp, ptrdiff_t* _vfl_from_ki)
 32261| {
 32262|     ptrdiff_t error = (ptrdiff_t)(current_available_physical - available_memory_goal);
 32263|     size_t gen2_physical_size = current_bgc_end_data[0].gen_physical_size;
 32264|     size_t gen3_physical_size = current_bgc_end_data[1].gen_physical_size;
 32265|     double max_output = (double)(total_physical_mem - available_memory_goal -
 32266|                                  gen2_physical_size - gen3_physical_size);
 32267|     double error_ratio = (double)error / (double)total_physical_mem;
 32268|     bool include_in_i_p = ((error_ratio > 0.005) || (error_ratio < -0.005));
 32269|     dprintf (BGC_TUNING_LOG, ("total phy %zd, mem goal: %zd, curr phy: %zd, g2 phy: %zd, g3 phy: %zd",
 32270|             (size_t)total_physical_mem, (size_t)available_memory_goal,
 32271|             (size_t)current_available_physical,
 32272|             gen2_physical_size, gen3_physical_size));
 32273|     dprintf (BGC_TUNING_LOG, ("BTL: Max output: %zd, ER %zd / %zd = %.3f, %s",
 32274|             (size_t)max_output,
 32275|             error, available_memory_goal, error_ratio,
 32276|             (include_in_i_p ? "inc" : "exc")));
 32277|     if (include_in_i_p)
 32278|     {
 32279|         double error_ki = ml_ki * (double)error;
 32280|         double temp_accu_error = accu_error + error_ki;
 32281|         if ((temp_accu_error > 0) && (temp_accu_error < max_output))
 32282|             accu_error = temp_accu_error;
 32283|         else
 32284|         {
 32285|             dprintf (BGC_TUNING_LOG, ("mae + %zd=%zd, exc",
 32286|                     (size_t)error_ki, (size_t)temp_accu_error));
 32287|         }
 32288|     }
 32289|     if (reduce_p)
 32290|     {
 32291|         double saved_accu_error = accu_error;
 32292|         accu_error = accu_error * 2.0 / 3.0;
 32293|         panic_activated_p = false;
 32294|         accu_error_panic = 0;
 32295|         dprintf (BGC_TUNING_LOG, ("BTL reduced accu ki %zd->%zd", (ptrdiff_t)saved_accu_error, (ptrdiff_t)accu_error));
 32296|     }
 32297|     if (panic_activated_p)
 32298|         accu_error_panic += (double)error;
 32299|     else
 32300|         accu_error_panic = 0.0;
 32301|     double vfl_from_kp = (double)error * ml_kp;
 32302|     double total_virtual_fl_size = vfl_from_kp + accu_error;
 32303|     if (total_virtual_fl_size < 0)
 32304|     {
 32305|         dprintf (BGC_TUNING_LOG, ("BTL vfl %zd < 0", (size_t)total_virtual_fl_size));
 32306|         total_virtual_fl_size = 0;
 32307|     }
 32308|     else if (total_virtual_fl_size > max_output)
 32309|     {
 32310|         dprintf (BGC_TUNING_LOG, ("BTL vfl %zd > max", (size_t)total_virtual_fl_size));
 32311|         total_virtual_fl_size = max_output;
 32312|     }
 32313|     *_vfl_from_kp = (ptrdiff_t)vfl_from_kp;
 32314|     *_vfl_from_ki = (ptrdiff_t)accu_error;
 32315|     return total_virtual_fl_size;
 32316| }
 32317| void gc_heap::bgc_tuning::set_total_gen_sizes (bool use_gen2_loop_p, bool use_gen3_loop_p)
 32318| {
 32319|     size_t gen2_physical_size = current_bgc_end_data[0].gen_physical_size;
 32320|     size_t gen3_physical_size = 0;
 32321|     ptrdiff_t gen3_virtual_fl_size = 0;
 32322|     gen3_physical_size = current_bgc_end_data[1].gen_physical_size;
 32323|     double gen2_size_ratio = (double)gen2_physical_size / ((double)gen2_physical_size + (double)gen3_physical_size);
 32324|     uint32_t current_memory_load = settings.entry_memory_load;
 32325|     uint64_t current_available_physical = settings.entry_available_physical_mem;
 32326|     panic_activated_p = (current_memory_load >= (memory_load_goal + memory_load_goal_slack));
 32327|     if (panic_activated_p)
 32328|     {
 32329|         dprintf (BGC_TUNING_LOG, ("BTL: exceeded slack %zd >= (%zd + %zd)",
 32330|             (size_t)current_memory_load, (size_t)memory_load_goal,
 32331|             (size_t)memory_load_goal_slack));
 32332|     }
 32333|     ptrdiff_t vfl_from_kp = 0;
 32334|     ptrdiff_t vfl_from_ki = 0;
 32335|     double total_virtual_fl_size = calculate_ml_tuning (current_available_physical, false, &vfl_from_kp, &vfl_from_ki);
 32336|     if (use_gen2_loop_p || use_gen3_loop_p)
 32337|     {
 32338|         if (use_gen2_loop_p)
 32339|         {
 32340|             gen2_ratio_correction += ratio_correction_step;
 32341|         }
 32342|         else
 32343|         {
 32344|             gen2_ratio_correction -= ratio_correction_step;
 32345|         }
 32346|         dprintf (BGC_TUNING_LOG, ("BTL: rc: g2 ratio %.3f%% + %d%% = %.3f%%",
 32347|             (gen2_size_ratio * 100.0), (int)(gen2_ratio_correction * 100.0), ((gen2_size_ratio + gen2_ratio_correction) * 100.0)));
 32348|         gen2_ratio_correction = min (0.99, gen2_ratio_correction);
 32349|         gen2_ratio_correction = max (-0.99, gen2_ratio_correction);
 32350|         dprintf (BGC_TUNING_LOG, ("BTL: rc again: g2 ratio %.3f%% + %d%% = %.3f%%",
 32351|             (gen2_size_ratio * 100.0), (int)(gen2_ratio_correction * 100.0), ((gen2_size_ratio + gen2_ratio_correction) * 100.0)));
 32352|         gen2_size_ratio += gen2_ratio_correction;
 32353|         if (gen2_size_ratio <= 0.0)
 32354|         {
 32355|             gen2_size_ratio = 0.01;
 32356|             dprintf (BGC_TUNING_LOG, ("BTL: rc: g2 ratio->0.01"));
 32357|         }
 32358|         if (gen2_size_ratio >= 1.0)
 32359|         {
 32360|             gen2_size_ratio = 0.99;
 32361|             dprintf (BGC_TUNING_LOG, ("BTL: rc: g2 ratio->0.99"));
 32362|         }
 32363|     }
 32364|     ptrdiff_t gen2_virtual_fl_size = (ptrdiff_t)(total_virtual_fl_size * gen2_size_ratio);
 32365|     gen3_virtual_fl_size = (ptrdiff_t)(total_virtual_fl_size * (1.0 - gen2_size_ratio));
 32366|     if (gen2_virtual_fl_size < 0)
 32367|     {
 32368|         ptrdiff_t saved_gen2_virtual_fl_size = gen2_virtual_fl_size;
 32369|         ptrdiff_t half_gen2_physical_size = (ptrdiff_t)((double)gen2_physical_size * 0.5);
 32370|         if (-gen2_virtual_fl_size > half_gen2_physical_size)
 32371|         {
 32372|             gen2_virtual_fl_size = -half_gen2_physical_size;
 32373|         }
 32374|         dprintf (BGC_TUNING_LOG, ("BTL2: n_vfl %zd(%zd)->%zd", saved_gen2_virtual_fl_size, half_gen2_physical_size, gen2_virtual_fl_size));
 32375|         gen2_virtual_fl_size = 0;
 32376|     }
 32377|     if (gen3_virtual_fl_size < 0)
 32378|     {
 32379|         ptrdiff_t saved_gen3_virtual_fl_size = gen3_virtual_fl_size;
 32380|         ptrdiff_t half_gen3_physical_size = (ptrdiff_t)((double)gen3_physical_size * 0.5);
 32381|         if (-gen3_virtual_fl_size > half_gen3_physical_size)
 32382|         {
 32383|             gen3_virtual_fl_size = -half_gen3_physical_size;
 32384|         }
 32385|         dprintf (BGC_TUNING_LOG, ("BTL3: n_vfl %zd(%zd)->%zd", saved_gen3_virtual_fl_size, half_gen3_physical_size, gen3_virtual_fl_size));
 32386|         gen3_virtual_fl_size = 0;
 32387|     }
 32388|     gen_calc[0].end_gen_size_goal = gen2_physical_size + gen2_virtual_fl_size;
 32389|     gen_calc[1].end_gen_size_goal = gen3_physical_size + gen3_virtual_fl_size;
 32390|     calc_end_bgc_fl (max_generation);
 32391|     calc_end_bgc_fl (loh_generation);
 32392| #ifdef SIMPLE_DPRINTF
 32393|     dprintf (BGC_TUNING_LOG, ("BTL: ml: %d (g: %d)(%s), a: %zd (g: %zd, elg: %zd+%zd=%zd, %zd+%zd=%zd, pi=%zd), vfl: %zd=%zd+%zd",
 32394|         current_memory_load, memory_load_goal,
 32395|         ((current_available_physical > available_memory_goal) ? "above" : "below"),
 32396|         current_available_physical, available_memory_goal,
 32397|         gen2_physical_size, gen2_virtual_fl_size, gen_calc[0].end_gen_size_goal,
 32398|         gen3_physical_size, gen3_virtual_fl_size, gen_calc[1].end_gen_size_goal,
 32399|         (ptrdiff_t)accu_error_panic,
 32400|         (ptrdiff_t)total_virtual_fl_size, vfl_from_kp, vfl_from_ki));
 32401| #endif //SIMPLE_DPRINTF
 32402| }
 32403| bool gc_heap::bgc_tuning::should_trigger_ngc2()
 32404| {
 32405|     return panic_activated_p;
 32406| }
 32407| void gc_heap::bgc_tuning::convert_to_fl (bool use_gen2_loop_p, bool use_gen3_loop_p)
 32408| {
 32409|     size_t current_bgc_count = full_gc_counts[gc_type_background];
 32410| #ifdef MULTIPLE_HEAPS
 32411|     for (int i = 0; i < gc_heap::n_heaps; i++)
 32412|     {
 32413|         gc_heap* hp = gc_heap::g_heaps[i];
 32414|         hp->bgc_maxgen_end_fl_size = generation_free_list_space (hp->generation_of (max_generation));
 32415|     }
 32416| #else
 32417|     bgc_maxgen_end_fl_size = generation_free_list_space (generation_of (max_generation));
 32418| #endif //MULTIPLE_HEAPS
 32419|     init_bgc_end_data (max_generation, use_gen2_loop_p);
 32420|     init_bgc_end_data (loh_generation, use_gen3_loop_p);
 32421|     set_total_gen_sizes (use_gen2_loop_p, use_gen3_loop_p);
 32422|     dprintf (BGC_TUNING_LOG, ("BTL: gen2 %zd, fl %zd(%.3f)->%zd; gen3 %zd, fl %zd(%.3f)->%zd, %zd BGCs",
 32423|         current_bgc_end_data[0].gen_size, current_bgc_end_data[0].gen_fl_size,
 32424|         current_bgc_end_data[0].gen_flr, gen_calc[0].end_gen_size_goal,
 32425|         current_bgc_end_data[1].gen_size, current_bgc_end_data[1].gen_fl_size,
 32426|         current_bgc_end_data[1].gen_flr, gen_calc[1].end_gen_size_goal,
 32427|         current_bgc_count));
 32428| }
 32429| void gc_heap::bgc_tuning::record_and_adjust_bgc_end()
 32430| {
 32431|     if (!bgc_tuning::enable_fl_tuning)
 32432|         return;
 32433|     uint64_t elapsed_time_so_far = GetHighPrecisionTimeStamp() - process_start_time;
 32434|     size_t current_gen1_index = get_current_gc_index (max_generation - 1);
 32435|     dprintf (BGC_TUNING_LOG, ("BTL: g2t[en][g1 %zd]: %0.3f minutes",
 32436|         current_gen1_index,
 32437|         (double)elapsed_time_so_far / (double)1000000 / (double)60));
 32438|     if (fl_tuning_triggered)
 32439|     {
 32440|         num_bgcs_since_tuning_trigger++;
 32441|     }
 32442|     bool use_gen2_loop_p = (settings.reason == reason_bgc_tuning_soh);
 32443|     bool use_gen3_loop_p = (settings.reason == reason_bgc_tuning_loh);
 32444|     dprintf (BGC_TUNING_LOG, ("BTL: reason: %d, gen2 loop: %s; gen3 loop: %s, promoted %zd bytes",
 32445|         (((settings.reason != reason_bgc_tuning_soh) && (settings.reason != reason_bgc_tuning_loh)) ?
 32446|             saved_bgc_tuning_reason : settings.reason),
 32447|         (use_gen2_loop_p ? "yes" : "no"),
 32448|         (use_gen3_loop_p ? "yes" : "no"),
 32449|         get_total_bgc_promoted()));
 32450|     convert_to_fl (use_gen2_loop_p, use_gen3_loop_p);
 32451|     calculate_tuning (max_generation, true);
 32452|     if (total_loh_a_last_bgc > 0)
 32453|     {
 32454|         calculate_tuning (loh_generation, true);
 32455|     }
 32456|     else
 32457|     {
 32458|         dprintf (BGC_TUNING_LOG, ("BTL: gen3 not allocated"));
 32459|     }
 32460|     if (next_bgc_p)
 32461|     {
 32462|         next_bgc_p = false;
 32463|         fl_tuning_triggered = true;
 32464|         dprintf (BGC_TUNING_LOG, ("BTL: FL tuning ENABLED!!!"));
 32465|     }
 32466|     saved_bgc_tuning_reason = -1;
 32467| }
 32468| #endif //BGC_SERVO_TUNING
 32469| #endif //BACKGROUND_GC
 32470| void gc_heap::clear_cards (size_t start_card, size_t end_card)
 32471| {
 32472|     if (start_card < end_card)
 32473|     {
 32474|         size_t start_word = card_word (start_card);
 32475|         size_t end_word = card_word (end_card);
 32476|         if (start_word < end_word)
 32477|         {
 32478|             unsigned bits = card_bit (start_card);
 32479|             card_table [start_word] &= lowbits (~0, bits);
 32480|             for (size_t i = start_word+1; i < end_word; i++)
 32481|                 card_table [i] = 0;
 32482|             bits = card_bit (end_card);
 32483|             if (bits != 0)
 32484|             {
 32485|                 card_table [end_word] &= highbits (~0, bits);
 32486|             }
 32487|         }
 32488|         else
 32489|         {
 32490|             card_table [start_word] &= (lowbits (~0, card_bit (start_card)) |
 32491|                                         highbits (~0, card_bit (end_card)));
 32492|         }
 32493| #if defined(_DEBUG) && defined(VERIFY_HEAP)
 32494|         if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
 32495|         {
 32496|             size_t  card = start_card;
 32497|             while (card < end_card)
 32498|             {
 32499|                 assert (!(card_set_p (card)));
 32500|                 card++;
 32501|             }
 32502|         }
 32503| #endif //_DEBUG && VERIFY_HEAP
 32504|         dprintf (3,("Cleared cards [%zx:%zx, %zx:%zx[",
 32505|                   start_card, (size_t)card_address (start_card),
 32506|                   end_card, (size_t)card_address (end_card)));
 32507|     }
 32508| }
 32509| void gc_heap::clear_card_for_addresses (uint8_t* start_address, uint8_t* end_address)
 32510| {
 32511|     size_t   start_card = card_of (align_on_card (start_address));
 32512|     size_t   end_card = card_of (align_lower_card (end_address));
 32513|     clear_cards (start_card, end_card);
 32514| }
 32515| inline
 32516| void gc_heap::copy_cards (size_t dst_card,
 32517|                           size_t src_card,
 32518|                           size_t end_card,
 32519|                           BOOL nextp)
 32520| {
 32521|     if (!(dst_card < end_card))
 32522|         return;
 32523|     unsigned int srcbit = card_bit (src_card);
 32524|     unsigned int dstbit = card_bit (dst_card);
 32525|     size_t srcwrd = card_word (src_card);
 32526|     size_t dstwrd = card_word (dst_card);
 32527|     unsigned int srctmp = card_table[srcwrd];
 32528|     unsigned int dsttmp = card_table[dstwrd];
 32529|     for (size_t card = dst_card; card < end_card; card++)
 32530|     {
 32531|         if (srctmp & (1 << srcbit))
 32532|             dsttmp |= 1 << dstbit;
 32533|         else
 32534|             dsttmp &= ~(1 << dstbit);
 32535|         if (!(++srcbit % 32))
 32536|         {
 32537|             srctmp = card_table[++srcwrd];
 32538|             srcbit = 0;
 32539|         }
 32540|         if (nextp)
 32541|         {
 32542|             if (srctmp & (1 << srcbit))
 32543|                 dsttmp |= 1 << dstbit;
 32544|         }
 32545|         if (!(++dstbit % 32))
 32546|         {
 32547|             card_table[dstwrd] = dsttmp;
 32548| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
 32549|             if (dsttmp != 0)
 32550|             {
 32551|                 card_bundle_set(cardw_card_bundle(dstwrd));
 32552|             }
 32553| #endif
 32554|             dstwrd++;
 32555|             dsttmp = card_table[dstwrd];
 32556|             dstbit = 0;
 32557|         }
 32558|     }
 32559|     card_table[dstwrd] = dsttmp;
 32560| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
 32561|     if (dsttmp != 0)
 32562|     {
 32563|         card_bundle_set(cardw_card_bundle(dstwrd));
 32564|     }
 32565| #endif
 32566| }
 32567| void gc_heap::copy_cards_for_addresses (uint8_t* dest, uint8_t* src, size_t len)
 32568| {
 32569|     ptrdiff_t relocation_distance = src - dest;
 32570|     size_t start_dest_card = card_of (align_on_card (dest));
 32571|     size_t end_dest_card = card_of (dest + len - 1);
 32572|     size_t dest_card = start_dest_card;
 32573|     size_t src_card = card_of (card_address (dest_card)+relocation_distance);
 32574|     dprintf (3,("Copying cards [%zx:%zx->%zx:%zx, ",
 32575|                  src_card, (size_t)src, dest_card, (size_t)dest));
 32576|     dprintf (3,(" %zx->%zx:%zx[",
 32577|               (size_t)src+len, end_dest_card, (size_t)dest+len));
 32578|     dprintf (3, ("dest: %p, src: %p, len: %zx, reloc: %zx, align_on_card(dest) is %p",
 32579|         dest, src, len, relocation_distance, (align_on_card (dest))));
 32580|     dprintf (3, ("start_dest_card: %zx (address: %p), end_dest_card: %zx(addr: %p), card_of (dest): %zx",
 32581|         start_dest_card, card_address (start_dest_card), end_dest_card, card_address (end_dest_card), card_of (dest)));
 32582|     if (start_dest_card != card_of (dest))
 32583|     {
 32584|         if ((card_of (card_address (start_dest_card) + relocation_distance) <= card_of (src + len - 1))&&
 32585|             card_set_p (card_of (card_address (start_dest_card) + relocation_distance)))
 32586|         {
 32587|             dprintf (3, ("card_address (start_dest_card) + reloc is %p, card: %zx(set), src+len-1: %p, card: %zx",
 32588|                     (card_address (start_dest_card) + relocation_distance),
 32589|                     card_of (card_address (start_dest_card) + relocation_distance),
 32590|                     (src + len - 1),
 32591|                     card_of (src + len - 1)));
 32592|             dprintf (3, ("setting card: %zx", card_of (dest)));
 32593|             set_card (card_of (dest));
 32594|         }
 32595|     }
 32596|     if (card_set_p (card_of (src)))
 32597|         set_card (card_of (dest));
 32598|     copy_cards (dest_card, src_card, end_dest_card,
 32599|                 ((dest - align_lower_card (dest)) != (src - align_lower_card (src))));
 32600|     if ((card_of (card_address (end_dest_card) + relocation_distance) >= card_of (src)) &&
 32601|         card_set_p (card_of (card_address (end_dest_card) + relocation_distance)))
 32602|     {
 32603|         dprintf (3, ("card_address (end_dest_card) + reloc is %p, card: %zx(set), src: %p, card: %zx",
 32604|                 (card_address (end_dest_card) + relocation_distance),
 32605|                 card_of (card_address (end_dest_card) + relocation_distance),
 32606|                 src,
 32607|                 card_of (src)));
 32608|         dprintf (3, ("setting card: %zx", end_dest_card));
 32609|         set_card (end_dest_card);
 32610|     }
 32611|     if (card_set_p (card_of (src + len - 1)))
 32612|         set_card (end_dest_card);
 32613| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
 32614|     card_bundles_set(cardw_card_bundle(card_word(card_of(dest))), cardw_card_bundle(align_cardw_on_bundle(card_word(end_dest_card))));
 32615| #endif
 32616| }
 32617| #ifdef BACKGROUND_GC
 32618| void gc_heap::copy_mark_bits_for_addresses (uint8_t* dest, uint8_t* src, size_t len)
 32619| {
 32620|     dprintf (3, ("Copying mark_bits for addresses [%zx->%zx, %zx->%zx[",
 32621|                  (size_t)src, (size_t)dest,
 32622|                  (size_t)src+len, (size_t)dest+len));
 32623|     uint8_t* src_o = src;
 32624|     uint8_t* dest_o;
 32625|     uint8_t* src_end = src + len;
 32626|     int align_const = get_alignment_constant (TRUE);
 32627|     ptrdiff_t reloc = dest - src;
 32628|     while (src_o < src_end)
 32629|     {
 32630|         uint8_t*  next_o = src_o + Align (size (src_o), align_const);
 32631|         if (background_object_marked (src_o, TRUE))
 32632|         {
 32633|             dest_o = src_o + reloc;
 32634|             background_mark (dest_o,
 32635|                              background_saved_lowest_address,
 32636|                              background_saved_highest_address);
 32637|             dprintf (3, ("bc*%zx*bc, b*%zx*b", (size_t)src_o, (size_t)(dest_o)));
 32638|         }
 32639|         src_o = next_o;
 32640|     }
 32641| }
 32642| #endif //BACKGROUND_GC
 32643| void gc_heap::fix_brick_to_highest (uint8_t* o, uint8_t* next_o)
 32644| {
 32645|     size_t new_current_brick = brick_of (o);
 32646|     set_brick (new_current_brick,
 32647|                (o - brick_address (new_current_brick)));
 32648|     size_t b = 1 + new_current_brick;
 32649|     size_t limit = brick_of (next_o);
 32650|     dprintf(3,("b:%zx->%zx-%zx",
 32651|                new_current_brick, (size_t)o, (size_t)next_o));
 32652|     while (b < limit)
 32653|     {
 32654|         set_brick (b,(new_current_brick - b));
 32655|         b++;
 32656|     }
 32657| }
 32658| uint8_t* gc_heap::find_first_object (uint8_t* start, uint8_t* first_object)
 32659| {
 32660|     size_t brick = brick_of (start);
 32661|     uint8_t* o = 0;
 32662|     if ((brick == brick_of (first_object) || (start <= first_object)))
 32663|     {
 32664|         o = first_object;
 32665|     }
 32666|     else
 32667|     {
 32668|         ptrdiff_t  min_brick = (ptrdiff_t)brick_of (first_object);
 32669|         ptrdiff_t  prev_brick = (ptrdiff_t)brick - 1;
 32670|         int         brick_entry = 0;
 32671|         while (1)
 32672|         {
 32673|             if (prev_brick < min_brick)
 32674|             {
 32675|                 break;
 32676|             }
 32677|             if ((brick_entry = get_brick_entry(prev_brick)) >= 0)
 32678|             {
 32679|                 break;
 32680|             }
 32681|             assert (! ((brick_entry == 0)));
 32682|             prev_brick = (brick_entry + prev_brick);
 32683|         }
 32684|         o = ((prev_brick < min_brick) ? first_object :
 32685|                       brick_address (prev_brick) + brick_entry - 1);
 32686|         assert (o <= start);
 32687|     }
 32688|     assert (Align (size (o)) >= Align (min_obj_size));
 32689|     uint8_t*  next_o = o + Align (size (o));
 32690|     size_t curr_cl = (size_t)next_o / brick_size;
 32691|     size_t min_cl = (size_t)first_object / brick_size;
 32692| #ifdef TRACE_GC
 32693|     unsigned int n_o = 1;
 32694| #endif //TRACE_GC
 32695|     uint8_t* next_b = min (align_lower_brick (next_o) + brick_size, start+1);
 32696|     while (next_o <= start)
 32697|     {
 32698|         do
 32699|         {
 32700| #ifdef TRACE_GC
 32701|             n_o++;
 32702| #endif //TRACE_GC
 32703|             o = next_o;
 32704|             assert (Align (size (o)) >= Align (min_obj_size));
 32705|             next_o = o + Align (size (o));
 32706|             Prefetch (next_o);
 32707|         }while (next_o < next_b);
 32708|         if (((size_t)next_o / brick_size) != curr_cl)
 32709|         {
 32710|             if (curr_cl >= min_cl)
 32711|             {
 32712|                 fix_brick_to_highest (o, next_o);
 32713|             }
 32714|             curr_cl = (size_t) next_o / brick_size;
 32715|         }
 32716|         next_b = min (align_lower_brick (next_o) + brick_size, start+1);
 32717|     }
 32718|     size_t bo = brick_of (o);
 32719|     dprintf (3, ("%u o, [%zx-[%zx",
 32720|         n_o, bo, brick));
 32721|     if (bo < brick)
 32722|     {
 32723|         set_brick (bo, (o - brick_address(bo)));
 32724|         size_t b = 1 + bo;
 32725|         int x = -1;
 32726|         while (b < brick)
 32727|         {
 32728|             set_brick (b,x--);
 32729|             b++;
 32730|         }
 32731|     }
 32732|     return o;
 32733| }
 32734| #ifdef CARD_BUNDLE
 32735| BOOL gc_heap::find_card_dword (size_t& cardw, size_t cardw_end)
 32736| {
 32737|     dprintf (3, ("gc: %zd, find_card_dword cardw: %zx, cardw_end: %zx",
 32738|                  dd_collection_count (dynamic_data_of (0)), cardw, cardw_end));
 32739|     if (card_bundles_enabled())
 32740|     {
 32741|         size_t cardb = cardw_card_bundle (cardw);
 32742|         size_t end_cardb = cardw_card_bundle (align_cardw_on_bundle (cardw_end));
 32743|         while (1)
 32744|         {
 32745|             while (cardb < end_cardb)
 32746|             {
 32747|                 uint32_t cbw = card_bundle_table[card_bundle_word(cardb)] >> card_bundle_bit (cardb);
 32748|                 DWORD bit_index;
 32749|                 if (BitScanForward (&bit_index, cbw))
 32750|                 {
 32751|                     cardb += bit_index;
 32752|                     break;
 32753|                 }
 32754|                 else
 32755|                 {
 32756|                     cardb += sizeof(cbw)*8 - card_bundle_bit (cardb);
 32757|                 }
 32758|             }
 32759|             if (cardb >= end_cardb)
 32760|                 return FALSE;
 32761|             uint32_t* card_word = &card_table[max(card_bundle_cardw (cardb),cardw)];
 32762|             uint32_t* card_word_end = &card_table[min(card_bundle_cardw (cardb+1),cardw_end)];
 32763|             while ((card_word < card_word_end) && !(*card_word))
 32764|             {
 32765|                 card_word++;
 32766|             }
 32767|             if (card_word != card_word_end)
 32768|             {
 32769|                 cardw = (card_word - &card_table[0]);
 32770|                 return TRUE;
 32771|             }
 32772|             if (cardw == (card_bundle_cardw (cardb) + 1) && !card_table[cardw-1])
 32773|             {
 32774|                 cardw--;
 32775|             }
 32776|             card_word_end = &card_table[card_bundle_cardw (cardb+1)];
 32777|             while ((card_word < card_word_end) && !(*card_word))
 32778|             {
 32779|                 card_word++;
 32780|             }
 32781|             if ((cardw <= card_bundle_cardw (cardb)) &&
 32782|                 (card_word == card_word_end))
 32783|             {
 32784|                 dprintf  (3, ("gc: %zd, find_card_dword clear bundle: %zx cardw:[%zx,%zx[",
 32785|                         dd_collection_count (dynamic_data_of (0)),
 32786|                         cardb, card_bundle_cardw (cardb),
 32787|                         card_bundle_cardw (cardb+1)));
 32788|                 card_bundle_clear (cardb);
 32789|             }
 32790|             cardb++;
 32791|         }
 32792|     }
 32793|     else
 32794|     {
 32795|         uint32_t* card_word = &card_table[cardw];
 32796|         uint32_t* card_word_end = &card_table [cardw_end];
 32797|         while (card_word < card_word_end)
 32798|         {
 32799|             if ((*card_word) != 0)
 32800|             {
 32801|                 cardw = (card_word - &card_table [0]);
 32802|                 return TRUE;
 32803|             }
 32804|             card_word++;
 32805|         }
 32806|         return FALSE;
 32807|     }
 32808| }
 32809| #endif //CARD_BUNDLE
 32810| BOOL gc_heap::find_card(uint32_t* card_table,
 32811|                         size_t&   card,
 32812|                         size_t    card_word_end,
 32813|                         size_t&   end_card)
 32814| {
 32815|     uint32_t* last_card_word;
 32816|     uint32_t card_word_value;
 32817|     uint32_t bit_position;
 32818|     if (card_word (card) >= card_word_end)
 32819|         return FALSE;
 32820|     last_card_word = &card_table [card_word (card)];
 32821|     bit_position = card_bit (card);
 32822| #ifdef CARD_BUNDLE
 32823|     if (bit_position == 0)
 32824|     {
 32825|         card_word_value = 0;
 32826|     }
 32827|     else
 32828| #endif
 32829|     {
 32830|         card_word_value = (*last_card_word) >> bit_position;
 32831|     }
 32832|     if (!card_word_value)
 32833|     {
 32834| #ifdef CARD_BUNDLE
 32835|         size_t lcw = card_word(card) + (bit_position != 0);
 32836|         if (gc_heap::find_card_dword (lcw, card_word_end) == FALSE)
 32837|         {
 32838|             return FALSE;
 32839|         }
 32840|         else
 32841|         {
 32842|             last_card_word = &card_table [lcw];
 32843|             card_word_value = *last_card_word;
 32844|         }
 32845|         bit_position = 0;
 32846| #else //CARD_BUNDLE
 32847|         do
 32848|         {
 32849|             ++last_card_word;
 32850|         }
 32851|         while ((last_card_word < &card_table [card_word_end]) && !(*last_card_word));
 32852|         if (last_card_word < &card_table [card_word_end])
 32853|         {
 32854|             card_word_value = *last_card_word;
 32855|         }
 32856|         else
 32857|         {
 32858|             return FALSE;
 32859|         }
 32860| #endif //CARD_BUNDLE
 32861|     }
 32862|     if (card_word_value)
 32863|     {
 32864|         DWORD bit_index;
 32865|         uint8_t res = BitScanForward (&bit_index, card_word_value);
 32866|         assert (res != 0);
 32867|         card_word_value >>= bit_index;
 32868|         bit_position += bit_index;
 32869|     }
 32870|     card = (last_card_word - &card_table[0]) * card_word_width + bit_position;
 32871|     do
 32872|     {
 32873|         bit_position++;
 32874|         card_word_value = card_word_value / 2;
 32875|         if ((bit_position == card_word_width) && (last_card_word < &card_table [card_word_end-1]))
 32876|         {
 32877|             do
 32878|             {
 32879|                 card_word_value = *(++last_card_word);
 32880|             } while ((last_card_word < &card_table [card_word_end-1]) &&
 32881|                      (card_word_value == ~0u /* (1 << card_word_width)-1 */));
 32882|             bit_position = 0;
 32883|         }
 32884|     } while (card_word_value & 1);
 32885|     end_card = (last_card_word - &card_table [0])* card_word_width + bit_position;
 32886|     dprintf (3, ("fc: [%zx, %zx[", card, end_card));
 32887|     return TRUE;
 32888| }
 32889| uint8_t* compute_next_end (heap_segment* seg, uint8_t* low)
 32890| {
 32891|     if ((low >=  heap_segment_mem (seg)) &&
 32892|         (low < heap_segment_allocated (seg)))
 32893|         return low;
 32894|     else
 32895|         return heap_segment_allocated (seg);
 32896| }
 32897| #ifndef USE_REGIONS
 32898| uint8_t*
 32899| gc_heap::compute_next_boundary (int gen_number,
 32900|                                 BOOL relocating)
 32901| {
 32902|     if (relocating && (gen_number == (settings.condemned_generation + 1)))
 32903|     {
 32904|         generation* gen = generation_of (gen_number - 1);
 32905|         uint8_t* gen_alloc = generation_plan_allocation_start (gen);
 32906|         assert (gen_alloc);
 32907|         return gen_alloc;
 32908|     }
 32909|     else
 32910|     {
 32911|         assert (gen_number > settings.condemned_generation);
 32912|         return generation_allocation_start (generation_of (gen_number - 1 ));
 32913|     }
 32914| }
 32915| #endif //!USE_REGIONS
 32916| inline void
 32917| gc_heap::mark_through_cards_helper (uint8_t** poo, size_t& n_gen,
 32918|                                     size_t& cg_pointers_found,
 32919|                                     card_fn fn, uint8_t* nhigh,
 32920|                                     uint8_t* next_boundary,
 32921|                                     int condemned_gen,
 32922|                                     int current_gen
 32923|                                     CARD_MARKING_STEALING_ARG(gc_heap* hpt))
 32924| {
 32925| #if defined(FEATURE_CARD_MARKING_STEALING) && defined(MULTIPLE_HEAPS)
 32926|     int thread = hpt->heap_number;
 32927| #else
 32928|     THREAD_FROM_HEAP;
 32929| #ifdef MULTIPLE_HEAPS
 32930|     gc_heap* hpt = this;
 32931| #endif //MULTIPLE_HEAPS
 32932| #endif //FEATURE_CARD_MARKING_STEALING && MULTIPLE_HEAPS
 32933| #ifdef USE_REGIONS
 32934|     assert (nhigh == 0);
 32935|     assert (next_boundary == 0);
 32936|     uint8_t* child_object = *poo;
 32937|     if ((child_object < ephemeral_low) || (ephemeral_high <= child_object))
 32938|         return;
 32939|     int child_object_gen = get_region_gen_num (child_object);
 32940|     int saved_child_object_gen = child_object_gen;
 32941|     uint8_t* saved_child_object = child_object;
 32942|     if (child_object_gen <= condemned_gen)
 32943|     {
 32944|         n_gen++;
 32945|         call_fn(hpt,fn) (poo THREAD_NUMBER_ARG);
 32946|     }
 32947|     if (fn == &gc_heap::relocate_address)
 32948|     {
 32949|         child_object_gen = get_region_plan_gen_num (*poo);
 32950|     }
 32951|     if (child_object_gen < current_gen)
 32952|     {
 32953|         cg_pointers_found++;
 32954|         dprintf (4, ("cg pointer %zx found, %zd so far",
 32955|                         (size_t)*poo, cg_pointers_found ));
 32956|     }
 32957| #else //USE_REGIONS
 32958|     assert (condemned_gen == -1);
 32959|     if ((gc_low <= *poo) && (gc_high > *poo))
 32960|     {
 32961|         n_gen++;
 32962|         call_fn(hpt,fn) (poo THREAD_NUMBER_ARG);
 32963|     }
 32964| #ifdef MULTIPLE_HEAPS
 32965|     else if (*poo)
 32966|     {
 32967|         gc_heap* hp = heap_of_gc (*poo);
 32968|         if (hp != this)
 32969|         {
 32970|             if ((hp->gc_low <= *poo) &&
 32971|                 (hp->gc_high > *poo))
 32972|             {
 32973|                 n_gen++;
 32974|                 call_fn(hpt,fn) (poo THREAD_NUMBER_ARG);
 32975|             }
 32976|             if ((fn == &gc_heap::relocate_address) ||
 32977|                 ((hp->ephemeral_low <= *poo) &&
 32978|                  (hp->ephemeral_high > *poo)))
 32979|             {
 32980|                 cg_pointers_found++;
 32981|             }
 32982|         }
 32983|     }
 32984| #endif //MULTIPLE_HEAPS
 32985|     if ((next_boundary <= *poo) && (nhigh > *poo))
 32986|     {
 32987|         cg_pointers_found ++;
 32988|         dprintf (4, ("cg pointer %zx found, %zd so far",
 32989|                      (size_t)*poo, cg_pointers_found ));
 32990|     }
 32991| #endif //USE_REGIONS
 32992| }
 32993| BOOL gc_heap::card_transition (uint8_t* po, uint8_t* end, size_t card_word_end,
 32994|                                size_t& cg_pointers_found,
 32995|                                size_t& n_eph, size_t& n_card_set,
 32996|                                size_t& card, size_t& end_card,
 32997|                                BOOL& foundp, uint8_t*& start_address,
 32998|                                uint8_t*& limit, size_t& n_cards_cleared
 32999|                                CARD_MARKING_STEALING_ARGS(card_marking_enumerator& card_mark_enumerator, heap_segment* seg, size_t &card_word_end_out))
 33000| {
 33001|     dprintf (3, ("pointer %zx past card %zx, cg %zd", (size_t)po, (size_t)card, cg_pointers_found));
 33002|     BOOL passed_end_card_p = FALSE;
 33003|     foundp = FALSE;
 33004|     if (cg_pointers_found == 0)
 33005|     {
 33006|         dprintf(3,(" CC [%zx, %zx[ ",
 33007|                 (size_t)card_address(card), (size_t)po));
 33008|         clear_cards (card, card_of(po));
 33009|         n_card_set -= (card_of (po) - card);
 33010|         n_cards_cleared += (card_of (po) - card);
 33011|     }
 33012|     n_eph +=cg_pointers_found;
 33013|     cg_pointers_found = 0;
 33014|     card = card_of (po);
 33015|     if (card >= end_card)
 33016|     {
 33017|         passed_end_card_p = TRUE;
 33018|         dprintf (3, ("card %zx exceeding end_card %zx",
 33019|                     (size_t)card, (size_t)end_card));
 33020|         foundp = find_card (card_table, card, card_word_end, end_card);
 33021|         if (foundp)
 33022|         {
 33023|             n_card_set+= end_card - card;
 33024|             start_address = card_address (card);
 33025|             dprintf (3, ("NewC: %zx, start: %zx, end: %zx",
 33026|                         (size_t)card, (size_t)start_address,
 33027|                         (size_t)card_address (end_card)));
 33028|         }
 33029|         limit = min (end, card_address (end_card));
 33030| #ifdef FEATURE_CARD_MARKING_STEALING
 33031|         assert(!((card_word(end_card) < card_word_end) &&
 33032|             card_set_p(end_card)));
 33033|         if (!foundp)
 33034|         {
 33035|             card_word_end_out = 0;
 33036|             foundp = find_next_chunk(card_mark_enumerator, seg, n_card_set, start_address, limit, card, end_card, card_word_end_out);
 33037|         }
 33038| #else
 33039|         assert (!((limit < end) &&
 33040|                 card_set_p (end_card)));
 33041| #endif
 33042|     }
 33043|     return passed_end_card_p;
 33044| }
 33045| #ifdef FEATURE_CARD_MARKING_STEALING
 33046| bool card_marking_enumerator::move_next(heap_segment* seg, uint8_t*& low, uint8_t*& high)
 33047| {
 33048|     if (segment == nullptr)
 33049|         return false;
 33050|     uint32_t chunk_index = old_chunk_index;
 33051|     old_chunk_index = INVALID_CHUNK_INDEX;
 33052|     if (chunk_index == INVALID_CHUNK_INDEX)
 33053|         chunk_index = Interlocked::Increment((volatile int32_t *)chunk_index_counter);
 33054|     while (true)
 33055|     {
 33056|         uint32_t chunk_index_within_seg = chunk_index - segment_start_chunk_index;
 33057|         uint8_t* start = heap_segment_mem(segment);
 33058|         uint8_t* end = compute_next_end(segment, gc_low);
 33059|         uint8_t* aligned_start = (uint8_t*)((size_t)start & ~(CARD_MARKING_STEALING_GRANULARITY - 1));
 33060|         size_t seg_size = end - aligned_start;
 33061|         uint32_t chunk_count_within_seg = (uint32_t)((seg_size + (CARD_MARKING_STEALING_GRANULARITY - 1)) / CARD_MARKING_STEALING_GRANULARITY);
 33062|         if (chunk_index_within_seg < chunk_count_within_seg)
 33063|         {
 33064|             if (seg == segment)
 33065|             {
 33066|                 low = (chunk_index_within_seg == 0) ? start : (aligned_start + (size_t)chunk_index_within_seg * CARD_MARKING_STEALING_GRANULARITY);
 33067|                 high = (chunk_index_within_seg + 1 == chunk_count_within_seg) ? end : (aligned_start + (size_t)(chunk_index_within_seg + 1) * CARD_MARKING_STEALING_GRANULARITY);
 33068|                 chunk_high = high;
 33069|                 dprintf (3, ("cme:mn ci: %u, low: %p, high: %p", chunk_index, low, high));
 33070|                 return true;
 33071|             }
 33072|             else
 33073|             {
 33074| #ifdef _DEBUG
 33075|                 for (heap_segment* cur_seg = seg; cur_seg != segment; cur_seg = heap_segment_next_in_range(cur_seg))
 33076|                 {
 33077|                     assert(cur_seg);
 33078|                 }
 33079| #endif //_DEBUG
 33080|                 old_chunk_index = chunk_index;
 33081|                 dprintf (3, ("cme:mn oci: %u, seg mismatch seg: %p, segment: %p", old_chunk_index, heap_segment_mem (segment), heap_segment_mem (seg)));
 33082|                 return false;
 33083|             }
 33084|         }
 33085|         segment = heap_segment_next_in_range(segment);
 33086|         segment_start_chunk_index += chunk_count_within_seg;
 33087|         if (segment == nullptr)
 33088|         {
 33089|             old_chunk_index = chunk_index;
 33090|             dprintf (3, ("cme:mn oci: %u no more segments", old_chunk_index));
 33091|             return false;
 33092|         }
 33093|     }
 33094| }
 33095| bool gc_heap::find_next_chunk(card_marking_enumerator& card_mark_enumerator, heap_segment* seg, size_t& n_card_set,
 33096|     uint8_t*& start_address, uint8_t*& limit,
 33097|     size_t& card, size_t& end_card, size_t& card_word_end)
 33098| {
 33099|     while (true)
 33100|     {
 33101|         if (card_word_end != 0 && find_card(card_table, card, card_word_end, end_card))
 33102|         {
 33103|             assert(end_card <= card_word_end * card_word_width);
 33104|             n_card_set += end_card - card;
 33105|             start_address = card_address(card);
 33106|             dprintf(3, ("NewC: %zx, start: %zx, end: %zx",
 33107|                 (size_t)card, (size_t)start_address,
 33108|                 (size_t)card_address(end_card)));
 33109|             limit = min(card_mark_enumerator.get_chunk_high(), card_address(end_card));
 33110|             dprintf (3, ("New run of cards on heap %d: [%zx,%zx[", heap_number, (size_t)start_address, (size_t)limit));
 33111|             return true;
 33112|         }
 33113|         uint8_t* chunk_low = nullptr;
 33114|         uint8_t* chunk_high = nullptr;
 33115|         if (!card_mark_enumerator.move_next(seg, chunk_low, chunk_high))
 33116|         {
 33117|             dprintf (3, ("No more chunks on heap %d\n", heap_number));
 33118|             return false;
 33119|         }
 33120|         card = max(card, card_of(chunk_low));
 33121|         card_word_end = (card_of(align_on_card_word(chunk_high)) / card_word_width);
 33122|         dprintf (3, ("Moved to next chunk on heap %d: [%zx,%zx[", heap_number, (size_t)chunk_low, (size_t)chunk_high));
 33123|     }
 33124| }
 33125| #endif // FEATURE_CARD_MARKING_STEALING
 33126| void gc_heap::mark_through_cards_for_segments (card_fn fn, BOOL relocating CARD_MARKING_STEALING_ARG(gc_heap* hpt))
 33127| {
 33128| #ifdef BACKGROUND_GC
 33129| #ifdef USE_REGIONS
 33130|     dprintf (3, ("current_sweep_pos is %p", current_sweep_pos));
 33131| #else
 33132|     dprintf (3, ("current_sweep_pos is %p, saved_sweep_ephemeral_seg is %p(%p)",
 33133|                  current_sweep_pos, saved_sweep_ephemeral_seg, saved_sweep_ephemeral_start));
 33134| #endif //USE_REGIONS
 33135|     for (int i = get_start_generation_index(); i < max_generation; i++)
 33136|     {
 33137|         heap_segment* soh_seg = heap_segment_rw (generation_start_segment (generation_of (i)));
 33138|         PREFIX_ASSUME(soh_seg != NULL);
 33139|         while (soh_seg)
 33140|         {
 33141|             dprintf (3, ("seg %p, bgc_alloc: %p, alloc: %p",
 33142|                 soh_seg,
 33143|                 heap_segment_background_allocated (soh_seg),
 33144|                 heap_segment_allocated (soh_seg)));
 33145|             soh_seg = heap_segment_next_rw (soh_seg);
 33146|         }
 33147|     }
 33148| #endif //BACKGROUND_GC
 33149|     size_t end_card = 0;
 33150|     generation*   oldest_gen        = generation_of (max_generation);
 33151|     int           curr_gen_number   = max_generation;
 33152| #ifdef USE_REGIONS
 33153|     uint8_t* low = 0;
 33154|     uint8_t*      gen_boundary      = 0;
 33155|     uint8_t*      next_boundary     = 0;
 33156|     int condemned_gen               = settings.condemned_generation;
 33157|     uint8_t*      nhigh             = 0;
 33158| #else
 33159|     uint8_t* low = gc_low;
 33160|     uint8_t* high = gc_high;
 33161|     uint8_t*      gen_boundary      = generation_allocation_start(generation_of(curr_gen_number - 1));
 33162|     uint8_t*      next_boundary     = compute_next_boundary(curr_gen_number, relocating);
 33163|     int condemned_gen = -1;
 33164|     uint8_t*      nhigh             = (relocating ?
 33165|                                        heap_segment_plan_allocated (ephemeral_heap_segment) : high);
 33166| #endif //USE_REGIONS
 33167|     heap_segment* seg               = heap_segment_rw (generation_start_segment (oldest_gen));
 33168|     PREFIX_ASSUME(seg != NULL);
 33169|     uint8_t*      beg               = get_soh_start_object (seg, oldest_gen);
 33170|     uint8_t*      end               = compute_next_end (seg, low);
 33171|     uint8_t*      last_object       = beg;
 33172|     size_t  cg_pointers_found = 0;
 33173|     size_t  card_word_end = (card_of (align_on_card_word (end)) / card_word_width);
 33174|     size_t        n_eph             = 0;
 33175|     size_t        n_gen             = 0;
 33176|     size_t        n_card_set        = 0;
 33177|     BOOL          foundp            = FALSE;
 33178|     uint8_t*      start_address     = 0;
 33179|     uint8_t*      limit             = 0;
 33180|     size_t        card              = card_of (beg);
 33181| #ifdef BACKGROUND_GC
 33182|     BOOL consider_bgc_mark_p        = FALSE;
 33183|     BOOL check_current_sweep_p      = FALSE;
 33184|     BOOL check_saved_sweep_p        = FALSE;
 33185|     should_check_bgc_mark (seg, &consider_bgc_mark_p, &check_current_sweep_p, &check_saved_sweep_p);
 33186| #endif //BACKGROUND_GC
 33187|     dprintf(3, ("CMs: %zx->%zx", (size_t)beg, (size_t)end));
 33188|     size_t total_cards_cleared = 0;
 33189| #ifdef FEATURE_CARD_MARKING_STEALING
 33190|     card_marking_enumerator card_mark_enumerator (seg, low, (VOLATILE(uint32_t)*)&card_mark_chunk_index_soh);
 33191|     card_word_end = 0;
 33192| #endif // FEATURE_CARD_MARKING_STEALING
 33193|     while (1)
 33194|     {
 33195|         if (card_of(last_object) > card)
 33196|         {
 33197|             dprintf (3, ("Found %zd cg pointers", cg_pointers_found));
 33198|             if (cg_pointers_found == 0)
 33199|             {
 33200|                 uint8_t* last_object_processed = last_object;
 33201| #ifdef FEATURE_CARD_MARKING_STEALING
 33202|                 last_object_processed = min(limit, last_object);
 33203| #endif // FEATURE_CARD_MARKING_STEALING
 33204|                 dprintf (3, (" Clearing cards [%zx, %zx[ ", (size_t)card_address(card), (size_t)last_object_processed));
 33205|                 clear_cards(card, card_of(last_object_processed));
 33206|                 n_card_set -= (card_of(last_object_processed) - card);
 33207|                 total_cards_cleared += (card_of(last_object_processed) - card);
 33208|             }
 33209|             n_eph += cg_pointers_found;
 33210|             cg_pointers_found = 0;
 33211|             card = card_of (last_object);
 33212|         }
 33213|         if (card >= end_card)
 33214|         {
 33215| #ifdef FEATURE_CARD_MARKING_STEALING
 33216|             foundp = find_next_chunk(card_mark_enumerator, seg, n_card_set, start_address, limit, card, end_card, card_word_end);
 33217| #else // FEATURE_CARD_MARKING_STEALING
 33218|             foundp = find_card(card_table, card, card_word_end, end_card);
 33219|             if (foundp)
 33220|             {
 33221|                 n_card_set += end_card - card;
 33222|                 start_address = max (beg, card_address (card));
 33223|             }
 33224|             limit = min (end, card_address (end_card));
 33225| #endif // FEATURE_CARD_MARKING_STEALING
 33226|         }
 33227|         if (!foundp || (last_object >= end) || (card_address (card) >= end))
 33228|         {
 33229|             if (foundp && (cg_pointers_found == 0))
 33230|             {
 33231| #ifndef USE_REGIONS
 33232|                 end_card = card_of (end);
 33233| #endif
 33234|                 dprintf(3,(" Clearing cards [%zx, %zx[ ", (size_t)card_address(card),
 33235|                             (size_t)card_address(end_card)));
 33236|                 clear_cards (card, end_card);
 33237|                 n_card_set -= (end_card - card);
 33238|                 total_cards_cleared += (end_card - card);
 33239|             }
 33240|             n_eph += cg_pointers_found;
 33241|             cg_pointers_found = 0;
 33242| #ifdef FEATURE_CARD_MARKING_STEALING
 33243|             card_mark_enumerator.exhaust_segment(seg);
 33244| #endif // FEATURE_CARD_MARKING_STEALING
 33245|             seg = heap_segment_next_in_range (seg);
 33246| #ifdef USE_REGIONS
 33247|             if (!seg)
 33248|             {
 33249|                 curr_gen_number--;
 33250|                 if (curr_gen_number > condemned_gen)
 33251|                 {
 33252|                     seg = generation_start_segment (generation_of (curr_gen_number));
 33253| #ifdef FEATURE_CARD_MARKING_STEALING
 33254|                     card_mark_enumerator.switch_to_segment(seg);
 33255| #endif // FEATURE_CARD_MARKING_STEALING
 33256|                     dprintf (REGIONS_LOG, ("h%d switching to gen%d start seg %zx",
 33257|                         heap_number, curr_gen_number, (size_t)seg));
 33258|                 }
 33259|             }
 33260| #endif //USE_REGIONS
 33261|             if (seg)
 33262|             {
 33263| #ifdef BACKGROUND_GC
 33264|                 should_check_bgc_mark (seg, &consider_bgc_mark_p, &check_current_sweep_p, &check_saved_sweep_p);
 33265| #endif //BACKGROUND_GC
 33266|                 beg = heap_segment_mem (seg);
 33267| #ifdef USE_REGIONS
 33268|                 end = heap_segment_allocated (seg);
 33269| #else
 33270|                 end = compute_next_end (seg, low);
 33271| #endif //USE_REGIONS
 33272| #ifdef FEATURE_CARD_MARKING_STEALING
 33273|                 card_word_end = 0;
 33274| #else // FEATURE_CARD_MARKING_STEALING
 33275|                 card_word_end = card_of (align_on_card_word (end)) / card_word_width;
 33276| #endif // FEATURE_CARD_MARKING_STEALING
 33277|                 card = card_of (beg);
 33278|                 last_object = beg;
 33279|                 end_card = 0;
 33280|                 continue;
 33281|             }
 33282|             else
 33283|             {
 33284|                 break;
 33285|             }
 33286|         }
 33287|         assert (card_set_p (card));
 33288|         {
 33289|             uint8_t* o = last_object;
 33290|             o = find_first_object (start_address, last_object);
 33291|             assert (o >= last_object);
 33292| #ifndef USE_REGIONS
 33293|             dprintf(3, ("c: %zx, o: %zx, l: %zx[ boundary: %zx",
 33294|                    card, (size_t)o, (size_t)limit, (size_t)gen_boundary));
 33295| #endif //USE_REGIONS
 33296|             while (o < limit)
 33297|             {
 33298|                 assert (Align (size (o)) >= Align (min_obj_size));
 33299|                 size_t s = size (o);
 33300|                 uint8_t* next_o =  o + Align (s);
 33301|                 uint8_t* cont_o = next_o;
 33302|                 Prefetch (next_o);
 33303| #ifndef USE_REGIONS
 33304|                 if ((o >= gen_boundary) &&
 33305|                     (seg == ephemeral_heap_segment))
 33306|                 {
 33307|                     dprintf (3, ("switching gen boundary %zx", (size_t)gen_boundary));
 33308|                     curr_gen_number--;
 33309|                     assert ((curr_gen_number > 0));
 33310|                     gen_boundary = generation_allocation_start
 33311|                         (generation_of (curr_gen_number - 1));
 33312|                     next_boundary = (compute_next_boundary
 33313|                                      (curr_gen_number, relocating));
 33314|                 }
 33315| #endif //!USE_REGIONS
 33316|                 dprintf (4, ("|%zx|", (size_t)o));
 33317|                 if (next_o < start_address)
 33318|                 {
 33319|                     goto end_object;
 33320|                 }
 33321| #ifdef BACKGROUND_GC
 33322|                 if (!fgc_should_consider_object (o, seg, consider_bgc_mark_p, check_current_sweep_p, check_saved_sweep_p))
 33323|                 {
 33324|                     goto end_object;
 33325|                 }
 33326| #endif //BACKGROUND_GC
 33327| #ifdef COLLECTIBLE_CLASS
 33328|                 if (is_collectible(o))
 33329|                 {
 33330|                     BOOL passed_end_card_p = FALSE;
 33331|                     if (card_of (o) > card)
 33332|                     {
 33333|                         passed_end_card_p = card_transition (o, end, card_word_end,
 33334|                             cg_pointers_found,
 33335|                             n_eph, n_card_set,
 33336|                             card, end_card,
 33337|                             foundp, start_address,
 33338|                             limit, total_cards_cleared
 33339|                             CARD_MARKING_STEALING_ARGS(card_mark_enumerator, seg, card_word_end));
 33340|                     }
 33341|                     if ((!passed_end_card_p || foundp) && (card_of (o) == card))
 33342|                     {
 33343|                         if (fn == &gc_heap::relocate_address)
 33344|                         {
 33345|                             cg_pointers_found++;
 33346|                         }
 33347|                         else
 33348|                         {
 33349|                             uint8_t* class_obj = get_class_object (o);
 33350|                             mark_through_cards_helper (&class_obj, n_gen,
 33351|                                                        cg_pointers_found, fn,
 33352|                                                        nhigh, next_boundary,
 33353|                                                        condemned_gen, curr_gen_number CARD_MARKING_STEALING_ARG(hpt));
 33354|                         }
 33355|                     }
 33356|                     if (passed_end_card_p)
 33357|                     {
 33358|                         if (foundp && (card_address (card) < next_o))
 33359|                         {
 33360|                             goto go_through_refs;
 33361|                         }
 33362|                         else if (foundp && (start_address < limit))
 33363|                         {
 33364|                             cont_o = find_first_object (start_address, o);
 33365|                             goto end_object;
 33366|                         }
 33367|                         else
 33368|                             goto end_limit;
 33369|                     }
 33370|                 }
 33371| go_through_refs:
 33372| #endif //COLLECTIBLE_CLASS
 33373|                 if (contain_pointers (o))
 33374|                 {
 33375|                     dprintf(3,("Going through %zx start_address: %zx", (size_t)o, (size_t)start_address));
 33376|                     {
 33377|                         dprintf (4, ("normal object path"));
 33378|                         go_through_object
 33379|                             (method_table(o), o, s, poo,
 33380|                              start_address, use_start, (o + s),
 33381|                              {
 33382|                                  dprintf (4, ("<%zx>:%zx", (size_t)poo, (size_t)*poo));
 33383|                                  if (card_of ((uint8_t*)poo) > card)
 33384|                                  {
 33385|                                      BOOL passed_end_card_p  = card_transition ((uint8_t*)poo, end,
 33386|                                             card_word_end,
 33387|                                             cg_pointers_found,
 33388|                                             n_eph, n_card_set,
 33389|                                             card, end_card,
 33390|                                             foundp, start_address,
 33391|                                             limit, total_cards_cleared
 33392|                                             CARD_MARKING_STEALING_ARGS(card_mark_enumerator, seg, card_word_end));
 33393|                                      if (passed_end_card_p)
 33394|                                      {
 33395|                                          if (foundp && (card_address (card) < next_o))
 33396|                                          {
 33397|                                              {
 33398|                                                  if (ppstop <= (uint8_t**)start_address)
 33399|                                                      {break;}
 33400|                                                  else if (poo < (uint8_t**)start_address)
 33401|                                                      {poo = (uint8_t**)start_address;}
 33402|                                              }
 33403|                                          }
 33404|                                          else if (foundp && (start_address < limit))
 33405|                                          {
 33406|                                              cont_o = find_first_object (start_address, o);
 33407|                                              goto end_object;
 33408|                                          }
 33409|                                          else
 33410|                                              goto end_limit;
 33411|                                      }
 33412|                                  }
 33413|                                  mark_through_cards_helper (poo, n_gen,
 33414|                                                             cg_pointers_found, fn,
 33415|                                                             nhigh, next_boundary,
 33416|                                                             condemned_gen, curr_gen_number CARD_MARKING_STEALING_ARG(hpt));
 33417|                              }
 33418|                             );
 33419|                     }
 33420|                 }
 33421|             end_object:
 33422|                 if (((size_t)next_o / brick_size) != ((size_t) o / brick_size))
 33423|                 {
 33424|                     if (brick_table [brick_of (o)] <0)
 33425|                         fix_brick_to_highest (o, next_o);
 33426|                 }
 33427|                 o = cont_o;
 33428|             }
 33429|         end_limit:
 33430|             last_object = o;
 33431|         }
 33432|     }
 33433|     if (!relocating)
 33434|     {
 33435| #ifdef FEATURE_CARD_MARKING_STEALING
 33436|         Interlocked::ExchangeAddPtr(&n_eph_soh, n_eph);
 33437|         Interlocked::ExchangeAddPtr(&n_gen_soh, n_gen);
 33438|         dprintf (3, ("h%d marking h%d Msoh: cross: %zd, useful: %zd, cards set: %zd, cards cleared: %zd, ratio: %d",
 33439|             hpt->heap_number, heap_number, n_eph, n_gen, n_card_set, total_cards_cleared,
 33440|             (n_eph ? (int)(((float)n_gen / (float)n_eph) * 100) : 0)));
 33441|         dprintf (3, ("h%d marking h%d Msoh: total cross %zd, useful: %zd, running ratio: %d",
 33442|             hpt->heap_number, heap_number, (size_t)n_eph_soh, (size_t)n_gen_soh,
 33443|             (n_eph_soh ? (int)(((float)n_gen_soh / (float)n_eph_soh) * 100) : 0)));
 33444| #else
 33445|         generation_skip_ratio = ((n_eph > MIN_SOH_CROSS_GEN_REFS) ? (int)(((float)n_gen / (float)n_eph) * 100) : 100);
 33446|         dprintf (3, ("marking h%d Msoh: cross: %zd, useful: %zd, cards set: %zd, cards cleared: %zd, ratio: %d",
 33447|             heap_number, n_eph, n_gen, n_card_set, total_cards_cleared, generation_skip_ratio));
 33448| #endif //FEATURE_CARD_MARKING_STEALING
 33449|     }
 33450|     else
 33451|     {
 33452|         dprintf (3, ("R: Msoh: cross: %zd, useful: %zd, cards set: %zd, cards cleared: %zd, ratio: %d",
 33453|             n_gen, n_eph, n_card_set, total_cards_cleared, generation_skip_ratio));
 33454|     }
 33455| }
 33456| #ifndef USE_REGIONS
 33457| #ifdef SEG_REUSE_STATS
 33458| size_t gc_heap::dump_buckets (size_t* ordered_indices, int count, size_t* total_size)
 33459| {
 33460|     size_t total_items = 0;
 33461|     *total_size = 0;
 33462|     for (int i = 0; i < count; i++)
 33463|     {
 33464|         total_items += ordered_indices[i];
 33465|         *total_size += ordered_indices[i] << (MIN_INDEX_POWER2 + i);
 33466|         dprintf (SEG_REUSE_LOG_0, ("[%d]%4d 2^%2d", heap_number, ordered_indices[i], (MIN_INDEX_POWER2 + i)));
 33467|     }
 33468|     dprintf (SEG_REUSE_LOG_0, ("[%d]Total %d items, total size is 0x%zx", heap_number, total_items, *total_size));
 33469|     return total_items;
 33470| }
 33471| #endif // SEG_REUSE_STATS
 33472| void gc_heap::count_plug (size_t last_plug_size, uint8_t*& last_plug)
 33473| {
 33474|     if (!pinned_plug_que_empty_p() && (last_plug == pinned_plug (oldest_pin())))
 33475|     {
 33476|         deque_pinned_plug();
 33477|         update_oldest_pinned_plug();
 33478|         dprintf (3, ("deque pin,now oldest pin is %p", pinned_plug (oldest_pin())));
 33479|     }
 33480|     else
 33481|     {
 33482|         size_t plug_size = last_plug_size + Align(min_obj_size);
 33483|         BOOL is_padded = FALSE;
 33484| #ifdef SHORT_PLUGS
 33485|         plug_size += Align (min_obj_size);
 33486|         is_padded = TRUE;
 33487| #endif //SHORT_PLUGS
 33488| #ifdef RESPECT_LARGE_ALIGNMENT
 33489|         plug_size += switch_alignment_size (is_padded);
 33490| #endif //RESPECT_LARGE_ALIGNMENT
 33491|         total_ephemeral_plugs += plug_size;
 33492|         size_t plug_size_power2 = round_up_power2 (plug_size);
 33493|         ordered_plug_indices[relative_index_power2_plug (plug_size_power2)]++;
 33494|         dprintf (SEG_REUSE_LOG_1, ("[%d]count_plug: adding 0x%p - %zd (2^%d) to ordered plug array",
 33495|             heap_number,
 33496|             last_plug,
 33497|             plug_size,
 33498|             (relative_index_power2_plug (plug_size_power2) + MIN_INDEX_POWER2)));
 33499|     }
 33500| }
 33501| void gc_heap::count_plugs_in_brick (uint8_t* tree, uint8_t*& last_plug)
 33502| {
 33503|     assert ((tree != NULL));
 33504|     if (node_left_child (tree))
 33505|     {
 33506|         count_plugs_in_brick (tree + node_left_child (tree), last_plug);
 33507|     }
 33508|     if (last_plug != 0)
 33509|     {
 33510|         uint8_t*  plug = tree;
 33511|         size_t gap_size = node_gap_size (plug);
 33512|         uint8_t*   gap = (plug - gap_size);
 33513|         uint8_t*  last_plug_end = gap;
 33514|         size_t  last_plug_size = (last_plug_end - last_plug);
 33515|         dprintf (3, ("tree: %p, last plug: %p, gap size: %zx, gap: %p, last plug size: %zx",
 33516|             tree, last_plug, gap_size, gap, last_plug_size));
 33517|         if (tree == oldest_pinned_plug)
 33518|         {
 33519|             dprintf (3, ("tree %p is pinned, last plug is %p, size is %zx",
 33520|                 tree, last_plug, last_plug_size));
 33521|             mark* m = oldest_pin();
 33522|             if (m->has_pre_plug_info())
 33523|             {
 33524|                 last_plug_size += sizeof (gap_reloc_pair);
 33525|                 dprintf (3, ("pin %p has pre plug, adjusting plug size to %zx", tree, last_plug_size));
 33526|             }
 33527|         }
 33528|         count_plug (last_plug_size, last_plug);
 33529|     }
 33530|     last_plug = tree;
 33531|     if (node_right_child (tree))
 33532|     {
 33533|         count_plugs_in_brick (tree + node_right_child (tree), last_plug);
 33534|     }
 33535| }
 33536| void gc_heap::build_ordered_plug_indices ()
 33537| {
 33538|     memset (ordered_plug_indices, 0, sizeof(ordered_plug_indices));
 33539|     memset (saved_ordered_plug_indices, 0, sizeof(saved_ordered_plug_indices));
 33540|     uint8_t*  start_address = generation_limit (max_generation);
 33541|     uint8_t* end_address = heap_segment_allocated (ephemeral_heap_segment);
 33542|     size_t  current_brick = brick_of (start_address);
 33543|     size_t  end_brick = brick_of (end_address - 1);
 33544|     uint8_t* last_plug = 0;
 33545|     reset_pinned_queue_bos();
 33546|     while (!pinned_plug_que_empty_p())
 33547|     {
 33548|         mark* m = oldest_pin();
 33549|         if ((m->first >= start_address) && (m->first < end_address))
 33550|         {
 33551|             dprintf (3, ("found a pin %p between %p and %p", m->first, start_address, end_address));
 33552|             break;
 33553|         }
 33554|         else
 33555|             deque_pinned_plug();
 33556|     }
 33557|     update_oldest_pinned_plug();
 33558|     while (current_brick <= end_brick)
 33559|     {
 33560|         int brick_entry =  brick_table [ current_brick ];
 33561|         if (brick_entry >= 0)
 33562|         {
 33563|             count_plugs_in_brick (brick_address (current_brick) + brick_entry -1, last_plug);
 33564|         }
 33565|         current_brick++;
 33566|     }
 33567|     if (last_plug !=0)
 33568|     {
 33569|         count_plug (end_address - last_plug, last_plug);
 33570|     }
 33571|     size_t extra_size = END_SPACE_AFTER_GC_FL;
 33572|     total_ephemeral_plugs += extra_size;
 33573|     dprintf (SEG_REUSE_LOG_0, ("Making sure we can fit a large object after fitting all plugs"));
 33574|     ordered_plug_indices[relative_index_power2_plug (round_up_power2 (extra_size))]++;
 33575|     memcpy (saved_ordered_plug_indices, ordered_plug_indices, sizeof(ordered_plug_indices));
 33576| #ifdef SEG_REUSE_STATS
 33577|     dprintf (SEG_REUSE_LOG_0, ("Plugs:"));
 33578|     size_t total_plug_power2 = 0;
 33579|     dump_buckets (ordered_plug_indices, MAX_NUM_BUCKETS, &total_plug_power2);
 33580|     dprintf (SEG_REUSE_LOG_0, ("plugs: 0x%zx (rounded up to 0x%zx (%d%%))",
 33581|                 total_ephemeral_plugs,
 33582|                 total_plug_power2,
 33583|                 (total_ephemeral_plugs ?
 33584|                     (total_plug_power2 * 100 / total_ephemeral_plugs) :
 33585|                     0)));
 33586|     dprintf (SEG_REUSE_LOG_0, ("-------------------"));
 33587| #endif // SEG_REUSE_STATS
 33588| }
 33589| void gc_heap::init_ordered_free_space_indices ()
 33590| {
 33591|     memset (ordered_free_space_indices, 0, sizeof(ordered_free_space_indices));
 33592|     memset (saved_ordered_free_space_indices, 0, sizeof(saved_ordered_free_space_indices));
 33593| }
 33594| void gc_heap::trim_free_spaces_indices ()
 33595| {
 33596|     trimmed_free_space_index = -1;
 33597|     size_t max_count = max_free_space_items - 1;
 33598|     size_t count = 0;
 33599|     int i = 0;
 33600|     for (i = (MAX_NUM_BUCKETS - 1); i >= 0; i--)
 33601|     {
 33602|         count += ordered_free_space_indices[i];
 33603|         if (count >= max_count)
 33604|         {
 33605|             break;
 33606|         }
 33607|     }
 33608|     ptrdiff_t extra_free_space_items = count - max_count;
 33609|     if (extra_free_space_items > 0)
 33610|     {
 33611|         ordered_free_space_indices[i] -= extra_free_space_items;
 33612|         free_space_items = max_count;
 33613|         trimmed_free_space_index = i;
 33614|     }
 33615|     else
 33616|     {
 33617|         free_space_items = count;
 33618|     }
 33619|     if (i == -1)
 33620|     {
 33621|         i = 0;
 33622|     }
 33623|     free_space_buckets = MAX_NUM_BUCKETS - i;
 33624|     for (--i; i >= 0; i--)
 33625|     {
 33626|         ordered_free_space_indices[i] = 0;
 33627|     }
 33628|     memcpy (saved_ordered_free_space_indices,
 33629|             ordered_free_space_indices,
 33630|             sizeof(ordered_free_space_indices));
 33631| }
 33632| BOOL gc_heap::can_fit_in_spaces_p (size_t* ordered_blocks, int small_index, size_t* ordered_spaces, int big_index)
 33633| {
 33634|     assert (small_index <= big_index);
 33635|     assert (big_index < MAX_NUM_BUCKETS);
 33636|     size_t small_blocks = ordered_blocks[small_index];
 33637|     if (small_blocks == 0)
 33638|     {
 33639|         return TRUE;
 33640|     }
 33641|     size_t big_spaces = ordered_spaces[big_index];
 33642|     if (big_spaces == 0)
 33643|     {
 33644|         return FALSE;
 33645|     }
 33646|     dprintf (SEG_REUSE_LOG_1, ("[%d]Fitting %zu 2^%d plugs into %zu 2^%d free spaces",
 33647|         heap_number,
 33648|         small_blocks, (small_index + MIN_INDEX_POWER2),
 33649|         big_spaces, (big_index + MIN_INDEX_POWER2)));
 33650|     size_t big_to_small = big_spaces << (big_index - small_index);
 33651|     ptrdiff_t extra_small_spaces = big_to_small - small_blocks;
 33652|     dprintf (SEG_REUSE_LOG_1, ("[%d]%zu 2^%d spaces can fit %zu 2^%d blocks",
 33653|         heap_number,
 33654|         big_spaces, (big_index + MIN_INDEX_POWER2), big_to_small, (small_index + MIN_INDEX_POWER2)));
 33655|     BOOL can_fit = (extra_small_spaces >= 0);
 33656|     if (can_fit)
 33657|     {
 33658|         dprintf (SEG_REUSE_LOG_1, ("[%d]Can fit with %zd 2^%d extras blocks",
 33659|             heap_number,
 33660|             extra_small_spaces, (small_index + MIN_INDEX_POWER2)));
 33661|     }
 33662|     int i = 0;
 33663|     dprintf (SEG_REUSE_LOG_1, ("[%d]Setting # of 2^%d spaces to 0", heap_number, (big_index + MIN_INDEX_POWER2)));
 33664|     ordered_spaces[big_index] = 0;
 33665|     if (extra_small_spaces > 0)
 33666|     {
 33667|         dprintf (SEG_REUSE_LOG_1, ("[%d]Setting # of 2^%d blocks to 0", heap_number, (small_index + MIN_INDEX_POWER2)));
 33668|         ordered_blocks[small_index] = 0;
 33669|         for (i = small_index; i < big_index; i++)
 33670|         {
 33671|             if (extra_small_spaces & 1)
 33672|             {
 33673|                 dprintf (SEG_REUSE_LOG_1, ("[%d]Increasing # of 2^%d spaces from %zu to %zu",
 33674|                     heap_number,
 33675|                     (i + MIN_INDEX_POWER2), ordered_spaces[i], (ordered_spaces[i] + 1)));
 33676|                 ordered_spaces[i] += 1;
 33677|             }
 33678|             extra_small_spaces >>= 1;
 33679|         }
 33680|         dprintf (SEG_REUSE_LOG_1, ("[%d]Finally increasing # of 2^%d spaces from %zu to %zu",
 33681|             heap_number,
 33682|             (i + MIN_INDEX_POWER2), ordered_spaces[i], (ordered_spaces[i] + extra_small_spaces)));
 33683|         ordered_spaces[i] += extra_small_spaces;
 33684|     }
 33685|     else
 33686|     {
 33687|         dprintf (SEG_REUSE_LOG_1, ("[%d]Decreasing # of 2^%d blocks from %zu to %zu",
 33688|             heap_number,
 33689|             (small_index + MIN_INDEX_POWER2),
 33690|             ordered_blocks[small_index],
 33691|             (ordered_blocks[small_index] - big_to_small)));
 33692|         ordered_blocks[small_index] -= big_to_small;
 33693|     }
 33694| #ifdef SEG_REUSE_STATS
 33695|     size_t temp;
 33696|     dprintf (SEG_REUSE_LOG_1, ("[%d]Plugs became:", heap_number));
 33697|     dump_buckets (ordered_blocks, MAX_NUM_BUCKETS, &temp);
 33698|     dprintf (SEG_REUSE_LOG_1, ("[%d]Free spaces became:", heap_number));
 33699|     dump_buckets (ordered_spaces, MAX_NUM_BUCKETS, &temp);
 33700| #endif //SEG_REUSE_STATS
 33701|     return can_fit;
 33702| }
 33703| BOOL gc_heap::can_fit_blocks_p (size_t* ordered_blocks, int block_index, size_t* ordered_spaces, int* space_index)
 33704| {
 33705|     assert (*space_index >= block_index);
 33706|     while (!can_fit_in_spaces_p (ordered_blocks, block_index, ordered_spaces, *space_index))
 33707|     {
 33708|         (*space_index)--;
 33709|         if (*space_index < block_index)
 33710|         {
 33711|             return FALSE;
 33712|         }
 33713|     }
 33714|     return TRUE;
 33715| }
 33716| BOOL gc_heap::can_fit_all_blocks_p (size_t* ordered_blocks, size_t* ordered_spaces, int count)
 33717| {
 33718| #ifdef FEATURE_STRUCTALIGN
 33719|     return FALSE;
 33720| #endif // FEATURE_STRUCTALIGN
 33721|     int space_index = count - 1;
 33722|     for (int block_index = (count - 1); block_index >= 0; block_index--)
 33723|     {
 33724|         if (!can_fit_blocks_p (ordered_blocks, block_index, ordered_spaces, &space_index))
 33725|         {
 33726|             return FALSE;
 33727|         }
 33728|     }
 33729|     return TRUE;
 33730| }
 33731| void gc_heap::build_ordered_free_spaces (heap_segment* seg)
 33732| {
 33733|     assert (bestfit_seg);
 33734|     bestfit_seg->add_buckets (MIN_INDEX_POWER2,
 33735|                         ordered_free_space_indices,
 33736|                         MAX_NUM_BUCKETS,
 33737|                         free_space_items);
 33738|     assert (settings.condemned_generation == max_generation);
 33739|     uint8_t* first_address = heap_segment_mem (seg);
 33740|     uint8_t* end_address   = heap_segment_reserved (seg);
 33741|     reset_pinned_queue_bos();
 33742|     mark* m = 0;
 33743|     size_t eph_gen_starts = eph_gen_starts_size + Align (min_obj_size);
 33744|     BOOL has_fit_gen_starts = FALSE;
 33745|     while (!pinned_plug_que_empty_p())
 33746|     {
 33747|         m = oldest_pin();
 33748|         if ((pinned_plug (m) >= first_address) &&
 33749|             (pinned_plug (m) < end_address) &&
 33750|             (pinned_len (m) >= eph_gen_starts))
 33751|         {
 33752|             assert ((pinned_plug (m) - pinned_len (m)) == bestfit_first_pin);
 33753|             break;
 33754|         }
 33755|         else
 33756|         {
 33757|             deque_pinned_plug();
 33758|         }
 33759|     }
 33760|     if (!pinned_plug_que_empty_p())
 33761|     {
 33762|         bestfit_seg->add ((void*)m, TRUE, TRUE);
 33763|         deque_pinned_plug();
 33764|         m = oldest_pin();
 33765|         has_fit_gen_starts = TRUE;
 33766|     }
 33767|     while (!pinned_plug_que_empty_p() &&
 33768|             ((pinned_plug (m) >= first_address) && (pinned_plug (m) < end_address)))
 33769|     {
 33770|         bestfit_seg->add ((void*)m, TRUE, FALSE);
 33771|         deque_pinned_plug();
 33772|         m = oldest_pin();
 33773|     }
 33774|     if (commit_end_of_seg)
 33775|     {
 33776|         if (!has_fit_gen_starts)
 33777|         {
 33778|             assert (bestfit_first_pin == heap_segment_plan_allocated (seg));
 33779|         }
 33780|         bestfit_seg->add ((void*)seg, FALSE, (!has_fit_gen_starts));
 33781|     }
 33782| #ifdef _DEBUG
 33783|     bestfit_seg->check();
 33784| #endif //_DEBUG
 33785| }
 33786| BOOL gc_heap::try_best_fit (BOOL end_of_segment_p)
 33787| {
 33788|     if (!end_of_segment_p)
 33789|     {
 33790|         trim_free_spaces_indices ();
 33791|     }
 33792|     BOOL can_bestfit = can_fit_all_blocks_p (ordered_plug_indices,
 33793|                                              ordered_free_space_indices,
 33794|                                              MAX_NUM_BUCKETS);
 33795|     return can_bestfit;
 33796| }
 33797| BOOL gc_heap::best_fit (size_t free_space,
 33798|                         size_t largest_free_space,
 33799|                         size_t additional_space,
 33800|                         BOOL* use_additional_space)
 33801| {
 33802|     dprintf (SEG_REUSE_LOG_0, ("gen%d: trying best fit mechanism", settings.condemned_generation));
 33803|     assert (!additional_space || (additional_space && use_additional_space));
 33804|     if (use_additional_space)
 33805|     {
 33806|         *use_additional_space = FALSE;
 33807|     }
 33808|     if (ordered_plug_indices_init == FALSE)
 33809|     {
 33810|         total_ephemeral_plugs = 0;
 33811|         build_ordered_plug_indices();
 33812|         ordered_plug_indices_init = TRUE;
 33813|     }
 33814|     else
 33815|     {
 33816|         memcpy (ordered_plug_indices, saved_ordered_plug_indices, sizeof(ordered_plug_indices));
 33817|     }
 33818|     if (total_ephemeral_plugs == END_SPACE_AFTER_GC_FL)
 33819|     {
 33820|         dprintf (SEG_REUSE_LOG_0, ("No ephemeral plugs to realloc, done"));
 33821|         size_t empty_eph = (END_SPACE_AFTER_GC_FL + (Align (min_obj_size)) * (max_generation + 1));
 33822|         BOOL can_fit_empty_eph = (largest_free_space >= empty_eph);
 33823|         if (!can_fit_empty_eph)
 33824|         {
 33825|             can_fit_empty_eph = (additional_space >= empty_eph);
 33826|             if (can_fit_empty_eph)
 33827|             {
 33828|                 *use_additional_space = TRUE;
 33829|             }
 33830|         }
 33831|         return can_fit_empty_eph;
 33832|     }
 33833|     if ((total_ephemeral_plugs + approximate_new_allocation()) >= (free_space + additional_space))
 33834|     {
 33835|         dprintf (SEG_REUSE_LOG_0, ("We won't have enough free space left in this segment after fitting, done"));
 33836|         return FALSE;
 33837|     }
 33838|     if ((free_space + additional_space) == 0)
 33839|     {
 33840|         dprintf (SEG_REUSE_LOG_0, ("No free space in this segment, done"));
 33841|         return FALSE;
 33842|     }
 33843| #ifdef SEG_REUSE_STATS
 33844|     dprintf (SEG_REUSE_LOG_0, ("Free spaces:"));
 33845|     size_t total_free_space_power2 = 0;
 33846|     size_t total_free_space_items =
 33847|         dump_buckets (ordered_free_space_indices,
 33848|                       MAX_NUM_BUCKETS,
 33849|                       &total_free_space_power2);
 33850|     dprintf (SEG_REUSE_LOG_0, ("currently max free spaces is %zd", max_free_space_items));
 33851|     dprintf (SEG_REUSE_LOG_0, ("Ephemeral plugs: 0x%zx, free space: 0x%zx (rounded down to 0x%zx (%zd%%)), additional free_space: 0x%zx",
 33852|                 total_ephemeral_plugs,
 33853|                 free_space,
 33854|                 total_free_space_power2,
 33855|                 (free_space ? (total_free_space_power2 * 100 / free_space) : 0),
 33856|                 additional_space));
 33857|     size_t saved_all_free_space_indices[MAX_NUM_BUCKETS];
 33858|     memcpy (saved_all_free_space_indices,
 33859|             ordered_free_space_indices,
 33860|             sizeof(saved_all_free_space_indices));
 33861| #endif // SEG_REUSE_STATS
 33862|     if (total_ephemeral_plugs > (free_space + additional_space))
 33863|     {
 33864|         return FALSE;
 33865|     }
 33866|     use_bestfit = try_best_fit(FALSE);
 33867|     if (!use_bestfit && additional_space)
 33868|     {
 33869|         int relative_free_space_index = relative_index_power2_free_space (round_down_power2 (additional_space));
 33870|         if (relative_free_space_index != -1)
 33871|         {
 33872|             int relative_plug_index = 0;
 33873|             size_t plugs_to_fit = 0;
 33874|             for (relative_plug_index = (MAX_NUM_BUCKETS - 1); relative_plug_index >= 0; relative_plug_index--)
 33875|             {
 33876|                 plugs_to_fit = ordered_plug_indices[relative_plug_index];
 33877|                 if (plugs_to_fit != 0)
 33878|                 {
 33879|                     break;
 33880|                 }
 33881|             }
 33882|             if ((relative_plug_index > relative_free_space_index) ||
 33883|                 ((relative_plug_index == relative_free_space_index) &&
 33884|                 (plugs_to_fit > 1)))
 33885|             {
 33886| #ifdef SEG_REUSE_STATS
 33887|                 dprintf (SEG_REUSE_LOG_0, ("additional space is 2^%d but we stopped at %d 2^%d plug(s)",
 33888|                             (relative_free_space_index + MIN_INDEX_POWER2),
 33889|                             plugs_to_fit,
 33890|                             (relative_plug_index + MIN_INDEX_POWER2)));
 33891| #endif // SEG_REUSE_STATS
 33892|                 goto adjust;
 33893|             }
 33894|             dprintf (SEG_REUSE_LOG_0, ("Adding end of segment (2^%d)", (relative_free_space_index + MIN_INDEX_POWER2)));
 33895|             ordered_free_space_indices[relative_free_space_index]++;
 33896|             use_bestfit = try_best_fit(TRUE);
 33897|             if (use_bestfit)
 33898|             {
 33899|                 free_space_items++;
 33900|                 if (relative_free_space_index > trimmed_free_space_index)
 33901|                 {
 33902|                     *use_additional_space = TRUE;
 33903|                 }
 33904|                 else
 33905|                 {
 33906|                     saved_ordered_free_space_indices[trimmed_free_space_index]++;
 33907|                 }
 33908|             }
 33909|         }
 33910|     }
 33911| adjust:
 33912|     if (!use_bestfit)
 33913|     {
 33914|         dprintf (SEG_REUSE_LOG_0, ("couldn't fit..."));
 33915| #ifdef SEG_REUSE_STATS
 33916|         size_t saved_max = max_free_space_items;
 33917|         BOOL temp_bestfit = FALSE;
 33918|         dprintf (SEG_REUSE_LOG_0, ("----Starting experiment process----"));
 33919|         dprintf (SEG_REUSE_LOG_0, ("----Couldn't fit with max free items %zd", max_free_space_items));
 33920|         while (max_free_space_items <= total_free_space_items)
 33921|         {
 33922|             max_free_space_items += max_free_space_items / 2;
 33923|             dprintf (SEG_REUSE_LOG_0, ("----Temporarily increasing max free spaces to %zd", max_free_space_items));
 33924|             memcpy (ordered_free_space_indices,
 33925|                     saved_all_free_space_indices,
 33926|                     sizeof(ordered_free_space_indices));
 33927|             if (try_best_fit(FALSE))
 33928|             {
 33929|                 temp_bestfit = TRUE;
 33930|                 break;
 33931|             }
 33932|         }
 33933|         if (temp_bestfit)
 33934|         {
 33935|             dprintf (SEG_REUSE_LOG_0, ("----With %zd max free spaces we could fit", max_free_space_items));
 33936|         }
 33937|         else
 33938|         {
 33939|             dprintf (SEG_REUSE_LOG_0, ("----Tried all free spaces and still couldn't fit, lost too much space"));
 33940|         }
 33941|         dprintf (SEG_REUSE_LOG_0, ("----Restoring max free spaces to %zd", saved_max));
 33942|         max_free_space_items = saved_max;
 33943| #endif // SEG_REUSE_STATS
 33944|         if (free_space_items)
 33945|         {
 33946|             max_free_space_items = min (MAX_NUM_FREE_SPACES, free_space_items * 2);
 33947|             max_free_space_items = max (max_free_space_items, MIN_NUM_FREE_SPACES);
 33948|         }
 33949|         else
 33950|         {
 33951|             max_free_space_items = MAX_NUM_FREE_SPACES;
 33952|         }
 33953|     }
 33954|     dprintf (SEG_REUSE_LOG_0, ("Adjusted number of max free spaces to %zd", max_free_space_items));
 33955|     dprintf (SEG_REUSE_LOG_0, ("------End of best fitting process------\n"));
 33956|     return use_bestfit;
 33957| }
 33958| BOOL gc_heap::process_free_space (heap_segment* seg,
 33959|                                   size_t free_space,
 33960|                                   size_t min_free_size,
 33961|                                   size_t min_cont_size,
 33962|                                   size_t* total_free_space,
 33963|                                   size_t* largest_free_space)
 33964| {
 33965|     *total_free_space += free_space;
 33966|     *largest_free_space = max (*largest_free_space, free_space);
 33967| #ifdef SIMPLE_DPRINTF
 33968|     dprintf (SEG_REUSE_LOG_1, ("free space len: %zx, total free space: %zx, largest free space: %zx",
 33969|                 free_space, *total_free_space, *largest_free_space));
 33970| #endif //SIMPLE_DPRINTF
 33971|     if ((*total_free_space >= min_free_size) && (*largest_free_space >= min_cont_size))
 33972|     {
 33973| #ifdef SIMPLE_DPRINTF
 33974|         dprintf (SEG_REUSE_LOG_0, ("(gen%d)total free: %zx(min: %zx), largest free: %zx(min: %zx). Found segment %zx to reuse without bestfit",
 33975|             settings.condemned_generation,
 33976|             *total_free_space, min_free_size, *largest_free_space, min_cont_size,
 33977|             (size_t)seg));
 33978| #else
 33979|         UNREFERENCED_PARAMETER(seg);
 33980| #endif //SIMPLE_DPRINTF
 33981|         return TRUE;
 33982|     }
 33983|     int free_space_index = relative_index_power2_free_space (round_down_power2 (free_space));
 33984|     if (free_space_index != -1)
 33985|     {
 33986|         ordered_free_space_indices[free_space_index]++;
 33987|     }
 33988|     return FALSE;
 33989| }
 33990| BOOL gc_heap::can_expand_into_p (heap_segment* seg, size_t min_free_size, size_t min_cont_size,
 33991|                                  allocator* gen_allocator)
 33992| {
 33993|     min_cont_size += END_SPACE_AFTER_GC;
 33994|     use_bestfit = FALSE;
 33995|     commit_end_of_seg = FALSE;
 33996|     bestfit_first_pin = 0;
 33997|     uint8_t* first_address = heap_segment_mem (seg);
 33998|     uint8_t* end_address   = heap_segment_reserved (seg);
 33999|     size_t end_extra_space = end_space_after_gc();
 34000|     if ((heap_segment_reserved (seg) - end_extra_space) <= heap_segment_plan_allocated (seg))
 34001|     {
 34002|         dprintf (SEG_REUSE_LOG_0, ("can_expand_into_p: can't use segment [%p %p, has less than %zu bytes at the end",
 34003|                                    first_address, end_address, end_extra_space));
 34004|         return FALSE;
 34005|     }
 34006|     end_address -= end_extra_space;
 34007|     dprintf (SEG_REUSE_LOG_0, ("can_expand_into_p(gen%d): min free: %zx, min continuous: %zx",
 34008|         settings.condemned_generation, min_free_size, min_cont_size));
 34009|     size_t eph_gen_starts = eph_gen_starts_size;
 34010|     if (settings.condemned_generation == max_generation)
 34011|     {
 34012|         size_t free_space = 0;
 34013|         size_t largest_free_space = free_space;
 34014|         dprintf (SEG_REUSE_LOG_0, ("can_expand_into_p: gen2: testing segment [%p %p", first_address, end_address));
 34015|         reset_pinned_queue_bos();
 34016|         mark* m = 0;
 34017|         BOOL has_fit_gen_starts = FALSE;
 34018|         init_ordered_free_space_indices ();
 34019|         while (!pinned_plug_que_empty_p())
 34020|         {
 34021|             m = oldest_pin();
 34022|             if ((pinned_plug (m) >= first_address) &&
 34023|                 (pinned_plug (m) < end_address) &&
 34024|                 (pinned_len (m) >= (eph_gen_starts + Align (min_obj_size))))
 34025|             {
 34026|                 break;
 34027|             }
 34028|             else
 34029|             {
 34030|                 deque_pinned_plug();
 34031|             }
 34032|         }
 34033|         if (!pinned_plug_que_empty_p())
 34034|         {
 34035|             bestfit_first_pin = pinned_plug (m) - pinned_len (m);
 34036|             if (process_free_space (seg,
 34037|                                     pinned_len (m) - eph_gen_starts,
 34038|                                     min_free_size, min_cont_size,
 34039|                                     &free_space, &largest_free_space))
 34040|             {
 34041|                 return TRUE;
 34042|             }
 34043|             deque_pinned_plug();
 34044|             m = oldest_pin();
 34045|             has_fit_gen_starts = TRUE;
 34046|         }
 34047|         dprintf (3, ("first pin is %p", pinned_plug (m)));
 34048|         while (!pinned_plug_que_empty_p() &&
 34049|                ((pinned_plug (m) >= first_address) && (pinned_plug (m) < end_address)))
 34050|         {
 34051|             dprintf (3, ("looking at pin %p", pinned_plug (m)));
 34052|             if (process_free_space (seg,
 34053|                                     pinned_len (m),
 34054|                                     min_free_size, min_cont_size,
 34055|                                     &free_space, &largest_free_space))
 34056|             {
 34057|                 return TRUE;
 34058|             }
 34059|             deque_pinned_plug();
 34060|             m = oldest_pin();
 34061|         }
 34062|         size_t end_space = (end_address - heap_segment_plan_allocated (seg));
 34063|         size_t additional_space = ((min_free_size > free_space) ? (min_free_size - free_space) : 0);
 34064|         dprintf (SEG_REUSE_LOG_0, ("end space: %zx; additional: %zx", end_space, additional_space));
 34065|         if (end_space >= additional_space)
 34066|         {
 34067|             BOOL can_fit = TRUE;
 34068|             commit_end_of_seg = TRUE;
 34069|             if (largest_free_space < min_cont_size)
 34070|             {
 34071|                 if (end_space >= min_cont_size)
 34072|                 {
 34073|                     additional_space = max (min_cont_size, additional_space);
 34074|                     dprintf (SEG_REUSE_LOG_0, ("(gen2)Found segment %p to reuse without bestfit, with committing end of seg for eph",
 34075|                         seg));
 34076|                 }
 34077|                 else
 34078|                 {
 34079|                     if (settings.concurrent)
 34080|                     {
 34081|                         can_fit = FALSE;
 34082|                         commit_end_of_seg = FALSE;
 34083|                     }
 34084|                     else
 34085|                     {
 34086|                         size_t additional_space_bestfit = additional_space;
 34087|                         if (!has_fit_gen_starts)
 34088|                         {
 34089|                             if (additional_space_bestfit < (eph_gen_starts + Align (min_obj_size)))
 34090|                             {
 34091|                                 dprintf (SEG_REUSE_LOG_0, ("(gen2)Couldn't fit, gen starts not allocated yet and end space is too small: %zd",
 34092|                                         additional_space_bestfit));
 34093|                                 return FALSE;
 34094|                             }
 34095|                             bestfit_first_pin = heap_segment_plan_allocated (seg);
 34096|                             additional_space_bestfit -= eph_gen_starts;
 34097|                         }
 34098|                         can_fit = best_fit (free_space,
 34099|                                             largest_free_space,
 34100|                                             additional_space_bestfit,
 34101|                                             &commit_end_of_seg);
 34102|                         if (can_fit)
 34103|                         {
 34104|                             dprintf (SEG_REUSE_LOG_0, ("(gen2)Found segment %p to reuse with bestfit, %s committing end of seg",
 34105|                                 seg, (commit_end_of_seg ? "with" : "without")));
 34106|                         }
 34107|                         else
 34108|                         {
 34109|                             dprintf (SEG_REUSE_LOG_0, ("(gen2)Couldn't fit, total free space is %zx", (free_space + end_space)));
 34110|                         }
 34111|                     }
 34112|                 }
 34113|             }
 34114|             else
 34115|             {
 34116|                 dprintf (SEG_REUSE_LOG_0, ("(gen2)Found segment %p to reuse without bestfit, with committing end of seg", seg));
 34117|             }
 34118|             assert (additional_space <= end_space);
 34119|             if (commit_end_of_seg)
 34120|             {
 34121|                 if (!grow_heap_segment (seg, heap_segment_plan_allocated (seg) + additional_space))
 34122|                 {
 34123|                     dprintf (2, ("Couldn't commit end of segment?!"));
 34124|                     use_bestfit = FALSE;
 34125|                     return FALSE;
 34126|                 }
 34127|                 if (use_bestfit)
 34128|                 {
 34129|                     size_t free_space_end_of_seg =
 34130|                         heap_segment_committed (seg) - heap_segment_plan_allocated (seg);
 34131|                     int relative_free_space_index = relative_index_power2_free_space (round_down_power2 (free_space_end_of_seg));
 34132|                     saved_ordered_free_space_indices[relative_free_space_index]++;
 34133|                 }
 34134|             }
 34135|             if (use_bestfit)
 34136|             {
 34137|                 memcpy (ordered_free_space_indices,
 34138|                         saved_ordered_free_space_indices,
 34139|                         sizeof(ordered_free_space_indices));
 34140|                 max_free_space_items = max (MIN_NUM_FREE_SPACES, free_space_items * 3 / 2);
 34141|                 max_free_space_items = min (MAX_NUM_FREE_SPACES, max_free_space_items);
 34142|                 dprintf (SEG_REUSE_LOG_0, ("could fit! %zd free spaces, %zd max", free_space_items, max_free_space_items));
 34143|             }
 34144|             return can_fit;
 34145|         }
 34146|         dprintf (SEG_REUSE_LOG_0, ("(gen2)Couldn't fit, total free space is %zx", (free_space + end_space)));
 34147|         return FALSE;
 34148|     }
 34149|     else
 34150|     {
 34151|         assert (settings.condemned_generation == (max_generation-1));
 34152|         size_t free_space = (end_address - heap_segment_plan_allocated (seg));
 34153|         size_t largest_free_space = free_space;
 34154|         dprintf (SEG_REUSE_LOG_0, ("can_expand_into_p: gen1: testing segment [%p %p", first_address, end_address));
 34155|         uint8_t* free_list = 0;
 34156|         unsigned int a_l_idx = gen_allocator->first_suitable_bucket(eph_gen_starts);
 34157|         for (; a_l_idx < gen_allocator->number_of_buckets(); a_l_idx++)
 34158|         {
 34159|             free_list = gen_allocator->alloc_list_head_of (a_l_idx);
 34160|             while (free_list)
 34161|             {
 34162|                 if ((free_list >= first_address) &&
 34163|                     (free_list < end_address) &&
 34164|                     (unused_array_size (free_list) >= eph_gen_starts))
 34165|                 {
 34166|                     goto next;
 34167|                 }
 34168|                 else
 34169|                 {
 34170|                     free_list = free_list_slot (free_list);
 34171|                 }
 34172|             }
 34173|         }
 34174| next:
 34175|         if (free_list)
 34176|         {
 34177|             init_ordered_free_space_indices ();
 34178|             if (process_free_space (seg,
 34179|                                     unused_array_size (free_list) - eph_gen_starts + Align (min_obj_size),
 34180|                                     min_free_size, min_cont_size,
 34181|                                     &free_space, &largest_free_space))
 34182|             {
 34183|                 return TRUE;
 34184|             }
 34185|             free_list = free_list_slot (free_list);
 34186|         }
 34187|         else
 34188|         {
 34189|             dprintf (SEG_REUSE_LOG_0, ("(gen1)Couldn't fit, no free list"));
 34190|             return FALSE;
 34191|         }
 34192|         while (1)
 34193|         {
 34194|             while (free_list)
 34195|             {
 34196|                 if ((free_list >= first_address) && (free_list < end_address) &&
 34197|                     process_free_space (seg,
 34198|                                         unused_array_size (free_list),
 34199|                                         min_free_size, min_cont_size,
 34200|                                         &free_space, &largest_free_space))
 34201|                 {
 34202|                     return TRUE;
 34203|                 }
 34204|                 free_list = free_list_slot (free_list);
 34205|             }
 34206|             a_l_idx++;
 34207|             if (a_l_idx < gen_allocator->number_of_buckets())
 34208|             {
 34209|                 free_list = gen_allocator->alloc_list_head_of (a_l_idx);
 34210|             }
 34211|             else
 34212|                 break;
 34213|         }
 34214|         dprintf (SEG_REUSE_LOG_0, ("(gen1)Couldn't fit, total free space is %zx", free_space));
 34215|         return FALSE;
 34216|         /*
 34217|         BOOL can_fit = best_fit (free_space, 0, NULL);
 34218|         if (can_fit)
 34219|         {
 34220|             dprintf (SEG_REUSE_LOG_0, ("(gen1)Found segment %zx to reuse with bestfit", seg));
 34221|         }
 34222|         else
 34223|         {
 34224|             dprintf (SEG_REUSE_LOG_0, ("(gen1)Couldn't fit, total free space is %zx", free_space));
 34225|         }
 34226|         return can_fit;
 34227|         */
 34228|     }
 34229| }
 34230| void gc_heap::realloc_plug (size_t last_plug_size, uint8_t*& last_plug,
 34231|                             generation* gen, uint8_t* start_address,
 34232|                             unsigned int& active_new_gen_number,
 34233|                             uint8_t*& last_pinned_gap, BOOL& leftp,
 34234|                             BOOL shortened_p
 34235| #ifdef SHORT_PLUGS
 34236|                             , mark* pinned_plug_entry
 34237| #endif //SHORT_PLUGS
 34238|                             )
 34239| {
 34240|     if (!use_bestfit)
 34241|     {
 34242|         if ((active_new_gen_number > 1) &&
 34243|             (last_plug >= generation_limit (active_new_gen_number)))
 34244|         {
 34245|             assert (last_plug >= start_address);
 34246|             active_new_gen_number--;
 34247|             realloc_plan_generation_start (generation_of (active_new_gen_number), gen);
 34248|             assert (generation_plan_allocation_start (generation_of (active_new_gen_number)));
 34249|             leftp = FALSE;
 34250|         }
 34251|     }
 34252|     if (!pinned_plug_que_empty_p() && (last_plug == pinned_plug (oldest_pin())))
 34253|     {
 34254|         size_t  entry = deque_pinned_plug();
 34255|         mark*  m = pinned_plug_of (entry);
 34256|         size_t saved_pinned_len = pinned_len(m);
 34257|         pinned_len(m) = last_plug - last_pinned_gap;
 34258|         if (m->has_post_plug_info())
 34259|         {
 34260|             last_plug_size += sizeof (gap_reloc_pair);
 34261|             dprintf (3, ("ra pinned %p was shortened, adjusting plug size to %zx", last_plug, last_plug_size))
 34262|         }
 34263|         last_pinned_gap = last_plug + last_plug_size;
 34264|         dprintf (3, ("ra found pin %p, len: %zx->%zx, last_p: %p, last_p_size: %zx",
 34265|             pinned_plug (m), saved_pinned_len, pinned_len (m), last_plug, last_plug_size));
 34266|         leftp = FALSE;
 34267|         {
 34268|             size_t end_card = card_of (align_on_card (last_plug + last_plug_size));
 34269|             size_t card = card_of (last_plug);
 34270|             while (card != end_card)
 34271|             {
 34272|                 set_card (card);
 34273|                 card++;
 34274|             }
 34275|         }
 34276|     }
 34277|     else if (last_plug >= start_address)
 34278|     {
 34279| #ifdef FEATURE_STRUCTALIGN
 34280|         int requiredAlignment;
 34281|         ptrdiff_t pad;
 34282|         node_aligninfo (last_plug, requiredAlignment, pad);
 34283|         uint8_t* reloc_plug = last_plug + node_relocation_distance (last_plug);
 34284|         ptrdiff_t alignmentOffset = ComputeStructAlignPad(reloc_plug, requiredAlignment, 0);
 34285|         if (!alignmentOffset)
 34286|         {
 34287|             alignmentOffset = requiredAlignment;
 34288|         }
 34289|         clear_node_aligninfo (last_plug);
 34290| #else // FEATURE_STRUCTALIGN
 34291|         clear_node_realigned (last_plug);
 34292| #endif // FEATURE_STRUCTALIGN
 34293|         BOOL adjacentp = FALSE;
 34294|         BOOL set_padding_on_saved_p = FALSE;
 34295|         if (shortened_p)
 34296|         {
 34297|             last_plug_size += sizeof (gap_reloc_pair);
 34298| #ifdef SHORT_PLUGS
 34299|             assert (pinned_plug_entry != NULL);
 34300|             if (last_plug_size <= sizeof (plug_and_gap))
 34301|             {
 34302|                 set_padding_on_saved_p = TRUE;
 34303|             }
 34304| #endif //SHORT_PLUGS
 34305|             dprintf (3, ("ra plug %p was shortened, adjusting plug size to %zx", last_plug, last_plug_size))
 34306|         }
 34307| #ifdef SHORT_PLUGS
 34308|         clear_padding_in_expand (last_plug, set_padding_on_saved_p, pinned_plug_entry);
 34309| #endif //SHORT_PLUGS
 34310|         uint8_t* new_address = allocate_in_expanded_heap(gen, last_plug_size, adjacentp, last_plug,
 34311| #ifdef SHORT_PLUGS
 34312|                                      set_padding_on_saved_p,
 34313|                                      pinned_plug_entry,
 34314| #endif //SHORT_PLUGS
 34315|                                      TRUE, active_new_gen_number REQD_ALIGN_AND_OFFSET_ARG);
 34316|         dprintf (3, ("ra NA: [%p, %p[: %zx", new_address, (new_address + last_plug_size), last_plug_size));
 34317|         assert (new_address);
 34318|         set_node_relocation_distance (last_plug, new_address - last_plug);
 34319| #ifdef FEATURE_STRUCTALIGN
 34320|         if (leftp && node_alignpad (last_plug) == 0)
 34321| #else // FEATURE_STRUCTALIGN
 34322|         if (leftp && !node_realigned (last_plug))
 34323| #endif // FEATURE_STRUCTALIGN
 34324|         {
 34325|         }
 34326|         dprintf (3,(" Re-allocating %zx->%zx len %zd", (size_t)last_plug, (size_t)new_address, last_plug_size));
 34327|         leftp = adjacentp;
 34328|     }
 34329| }
 34330| void gc_heap::realloc_in_brick (uint8_t* tree, uint8_t*& last_plug,
 34331|                                 uint8_t* start_address,
 34332|                                 generation* gen,
 34333|                                 unsigned int& active_new_gen_number,
 34334|                                 uint8_t*& last_pinned_gap, BOOL& leftp)
 34335| {
 34336|     assert (tree != NULL);
 34337|     int   left_node = node_left_child (tree);
 34338|     int   right_node = node_right_child (tree);
 34339|     dprintf (3, ("ra: tree: %p, last_pin_gap: %p, last_p: %p, L: %d, R: %d",
 34340|         tree, last_pinned_gap, last_plug, left_node, right_node));
 34341|     if (left_node)
 34342|     {
 34343|         dprintf (3, ("LN: realloc %p(%p)", (tree + left_node), last_plug));
 34344|         realloc_in_brick ((tree + left_node), last_plug, start_address,
 34345|                           gen, active_new_gen_number, last_pinned_gap,
 34346|                           leftp);
 34347|     }
 34348|     if (last_plug != 0)
 34349|     {
 34350|         uint8_t*  plug = tree;
 34351|         BOOL has_pre_plug_info_p = FALSE;
 34352|         BOOL has_post_plug_info_p = FALSE;
 34353|         mark* pinned_plug_entry = get_next_pinned_entry (tree,
 34354|                                                          &has_pre_plug_info_p,
 34355|                                                          &has_post_plug_info_p,
 34356|                                                          FALSE);
 34357|         size_t gap_size = node_gap_size (plug);
 34358|         uint8_t*   gap = (plug - gap_size);
 34359|         uint8_t*  last_plug_end = gap;
 34360|         size_t  last_plug_size = (last_plug_end - last_plug);
 34361|         dprintf (3, ("ra: plug %p, gap size: %zd, last_pin_gap: %p, last_p: %p, last_p_end: %p, shortened: %d",
 34362|             plug, gap_size, last_pinned_gap, last_plug, last_plug_end, (has_pre_plug_info_p ? 1 : 0)));
 34363|         realloc_plug (last_plug_size, last_plug, gen, start_address,
 34364|                       active_new_gen_number, last_pinned_gap,
 34365|                       leftp, has_pre_plug_info_p
 34366| #ifdef SHORT_PLUGS
 34367|                       , pinned_plug_entry
 34368| #endif //SHORT_PLUGS
 34369|                       );
 34370|     }
 34371|     last_plug = tree;
 34372|     if (right_node)
 34373|     {
 34374|         dprintf (3, ("RN: realloc %p(%p)", (tree + right_node), last_plug));
 34375|         realloc_in_brick ((tree + right_node), last_plug, start_address,
 34376|                           gen, active_new_gen_number, last_pinned_gap,
 34377|                           leftp);
 34378|     }
 34379| }
 34380| void
 34381| gc_heap::realloc_plugs (generation* consing_gen, heap_segment* seg,
 34382|                         uint8_t* start_address, uint8_t* end_address,
 34383|                         unsigned active_new_gen_number)
 34384| {
 34385|     dprintf (3, ("--- Reallocing ---"));
 34386|     if (use_bestfit)
 34387|     {
 34388|         int  gen_number = max_generation - 1;
 34389|         while (gen_number >= 0)
 34390|         {
 34391|             generation* gen = generation_of (gen_number);
 34392|             if (0 == generation_plan_allocation_start (gen))
 34393|             {
 34394|                 generation_plan_allocation_start (gen) =
 34395|                     bestfit_first_pin + (max_generation - gen_number - 1) * Align (min_obj_size);
 34396|                 generation_plan_allocation_start_size (gen) = Align (min_obj_size);
 34397|                 assert (generation_plan_allocation_start (gen));
 34398|             }
 34399|             gen_number--;
 34400|         }
 34401|     }
 34402|     uint8_t* first_address = start_address;
 34403|     reset_pinned_queue_bos();
 34404|     uint8_t* planned_ephemeral_seg_end = heap_segment_plan_allocated (seg);
 34405|     while (!pinned_plug_que_empty_p())
 34406|     {
 34407|         mark* m = oldest_pin();
 34408|         if ((pinned_plug (m) >= planned_ephemeral_seg_end) && (pinned_plug (m) < end_address))
 34409|         {
 34410|             if (pinned_plug (m) < first_address)
 34411|             {
 34412|                 first_address = pinned_plug (m);
 34413|             }
 34414|             break;
 34415|         }
 34416|         else
 34417|             deque_pinned_plug();
 34418|     }
 34419|     size_t  current_brick = brick_of (first_address);
 34420|     size_t  end_brick = brick_of (end_address-1);
 34421|     uint8_t*  last_plug = 0;
 34422|     uint8_t* last_pinned_gap = heap_segment_plan_allocated (seg);
 34423|     BOOL leftp = FALSE;
 34424|     dprintf (3, ("start addr: %p, first addr: %p, current oldest pin: %p",
 34425|         start_address, first_address, pinned_plug (oldest_pin())));
 34426|     while (current_brick <= end_brick)
 34427|     {
 34428|         int   brick_entry =  brick_table [ current_brick ];
 34429|         if (brick_entry >= 0)
 34430|         {
 34431|             realloc_in_brick ((brick_address (current_brick) + brick_entry - 1),
 34432|                               last_plug, start_address, consing_gen,
 34433|                               active_new_gen_number, last_pinned_gap,
 34434|                               leftp);
 34435|         }
 34436|         current_brick++;
 34437|     }
 34438|     if (last_plug != 0)
 34439|     {
 34440|         realloc_plug (end_address - last_plug, last_plug, consing_gen,
 34441|                       start_address,
 34442|                       active_new_gen_number, last_pinned_gap,
 34443|                       leftp, FALSE
 34444| #ifdef SHORT_PLUGS
 34445|                       , NULL
 34446| #endif //SHORT_PLUGS
 34447|                       );
 34448|     }
 34449|     assert (last_pinned_gap >= heap_segment_mem (seg));
 34450|     assert (last_pinned_gap <= heap_segment_committed (seg));
 34451|     heap_segment_plan_allocated (seg) = last_pinned_gap;
 34452| }
 34453| void gc_heap::set_expand_in_full_gc (int condemned_gen_number)
 34454| {
 34455|     if (!should_expand_in_full_gc)
 34456|     {
 34457|         if ((condemned_gen_number != max_generation) &&
 34458|             (settings.pause_mode != pause_low_latency) &&
 34459|             (settings.pause_mode != pause_sustained_low_latency))
 34460|         {
 34461|             should_expand_in_full_gc = TRUE;
 34462|         }
 34463|     }
 34464| }
 34465| void gc_heap::save_ephemeral_generation_starts()
 34466| {
 34467|     for (int ephemeral_generation = 0; ephemeral_generation < max_generation; ephemeral_generation++)
 34468|     {
 34469|         saved_ephemeral_plan_start[ephemeral_generation] =
 34470|             generation_plan_allocation_start (generation_of (ephemeral_generation));
 34471|         saved_ephemeral_plan_start_size[ephemeral_generation] =
 34472|             generation_plan_allocation_start_size (generation_of (ephemeral_generation));
 34473|     }
 34474| }
 34475| generation* gc_heap::expand_heap (int condemned_generation,
 34476|                                   generation* consing_gen,
 34477|                                   heap_segment* new_heap_segment)
 34478| {
 34479| #ifndef _DEBUG
 34480|     UNREFERENCED_PARAMETER(condemned_generation);
 34481| #endif //!_DEBUG
 34482|     assert (condemned_generation >= (max_generation -1));
 34483|     unsigned int active_new_gen_number = max_generation; //Set one too high to get generation gap
 34484|     uint8_t*  start_address = generation_limit (max_generation);
 34485|     uint8_t*  end_address = heap_segment_allocated (ephemeral_heap_segment);
 34486|     BOOL should_promote_ephemeral = FALSE;
 34487|     ptrdiff_t eph_size = total_ephemeral_size;
 34488| #ifdef BACKGROUND_GC
 34489|     dprintf(2,("%s: ---- Heap Expansion ----", (gc_heap::background_running_p() ? "FGC" : "NGC")));
 34490| #endif //BACKGROUND_GC
 34491|     settings.heap_expansion = TRUE;
 34492|     dprintf (2, ("Elevation: elevation = el_none"));
 34493|     if (settings.should_lock_elevation && !expand_reused_seg_p())
 34494|         settings.should_lock_elevation = FALSE;
 34495|     heap_segment* new_seg = new_heap_segment;
 34496|     if (!new_seg)
 34497|         return consing_gen;
 34498|     if (g_gc_card_table!= card_table)
 34499|         copy_brick_card_table();
 34500|     BOOL new_segment_p = (heap_segment_next (new_seg) == 0);
 34501|     dprintf (2, ("new_segment_p %zx", (size_t)new_segment_p));
 34502|     assert (generation_plan_allocation_start (generation_of (max_generation-1)));
 34503|     assert (generation_plan_allocation_start (generation_of (max_generation-1)) >=
 34504|             heap_segment_mem (ephemeral_heap_segment));
 34505|     assert (generation_plan_allocation_start (generation_of (max_generation-1)) <=
 34506|             heap_segment_committed (ephemeral_heap_segment));
 34507|     assert (generation_plan_allocation_start (youngest_generation));
 34508|     assert (generation_plan_allocation_start (youngest_generation) <
 34509|             heap_segment_plan_allocated (ephemeral_heap_segment));
 34510|     if (settings.pause_mode == pause_no_gc)
 34511|     {
 34512|         if ((size_t)(heap_segment_reserved (new_seg) - heap_segment_mem (new_seg)) < (eph_size + soh_allocation_no_gc))
 34513|             should_promote_ephemeral = TRUE;
 34514|     }
 34515|     else
 34516|     {
 34517|         if (!use_bestfit)
 34518|         {
 34519|             should_promote_ephemeral = dt_low_ephemeral_space_p (tuning_deciding_promote_ephemeral);
 34520|         }
 34521|     }
 34522|     if (should_promote_ephemeral)
 34523|     {
 34524|         ephemeral_promotion = TRUE;
 34525|         get_gc_data_per_heap()->set_mechanism (gc_heap_expand, expand_new_seg_ep);
 34526|         dprintf (2, ("promoting ephemeral"));
 34527|         save_ephemeral_generation_starts();
 34528|         generation* max_gen = generation_of (max_generation);
 34529|         for (int i = 1; i < max_generation; i++)
 34530|         {
 34531|             generation_free_obj_space (max_gen) +=
 34532|                 generation_free_obj_space (generation_of (i));
 34533|             dprintf (2, ("[h%d] maxgen freeobj + %zd=%zd",
 34534|                 heap_number, generation_free_obj_space (generation_of (i)),
 34535|                 generation_free_obj_space (max_gen)));
 34536|         }
 34537|         heap_segment_used (new_seg) = heap_segment_committed (new_seg);
 34538|     }
 34539|     else
 34540|     {
 34541|         if ((eph_size > 0) && new_segment_p)
 34542|         {
 34543| #ifdef FEATURE_STRUCTALIGN
 34544|             eph_size += ComputeStructAlignPad(heap_segment_mem (new_seg), MAX_STRUCTALIGN, OBJECT_ALIGNMENT_OFFSET);
 34545| #endif // FEATURE_STRUCTALIGN
 34546| #ifdef RESPECT_LARGE_ALIGNMENT
 34547|             eph_size += switch_alignment_size(FALSE);
 34548| #endif //RESPECT_LARGE_ALIGNMENT
 34549|             if (grow_heap_segment (new_seg, heap_segment_mem (new_seg) + eph_size) == 0)
 34550|             {
 34551|                 fgm_result.set_fgm (fgm_commit_eph_segment, eph_size, FALSE);
 34552|                 return consing_gen;
 34553|             }
 34554|             heap_segment_used (new_seg) = heap_segment_committed (new_seg);
 34555|         }
 34556|         heap_segment_plan_allocated (ephemeral_heap_segment) =
 34557|             generation_plan_allocation_start (generation_of (max_generation-1));
 34558|         dprintf (3, ("Old ephemeral allocated set to %zx",
 34559|                     (size_t)heap_segment_plan_allocated (ephemeral_heap_segment)));
 34560|     }
 34561|     if (new_segment_p)
 34562|     {
 34563|         size_t first_brick = brick_of (heap_segment_mem (new_seg));
 34564|         set_brick (first_brick,
 34565|                 heap_segment_mem (new_seg) - brick_address (first_brick));
 34566|     }
 34567|     generation_allocation_limit (consing_gen) =
 34568|         heap_segment_plan_allocated (ephemeral_heap_segment);
 34569|     generation_allocation_pointer (consing_gen) = generation_allocation_limit (consing_gen);
 34570|     generation_allocation_segment (consing_gen) = ephemeral_heap_segment;
 34571|     {
 34572|         int generation_num = max_generation-1;
 34573|         while (generation_num >= 0)
 34574|         {
 34575|             generation* gen = generation_of (generation_num);
 34576|             generation_plan_allocation_start (gen) = 0;
 34577|             generation_num--;
 34578|         }
 34579|     }
 34580|     heap_segment* old_seg = ephemeral_heap_segment;
 34581|     ephemeral_heap_segment = new_seg;
 34582|     consing_gen = ensure_ephemeral_heap_segment (consing_gen);
 34583|     if (!should_promote_ephemeral)
 34584|     {
 34585|         realloc_plugs (consing_gen, old_seg, start_address, end_address,
 34586|                     active_new_gen_number);
 34587|     }
 34588|     if (!use_bestfit)
 34589|     {
 34590|         repair_allocation_in_expanded_heap (consing_gen);
 34591|     }
 34592| #ifdef _DEBUG
 34593|     {
 34594|         int generation_num = max_generation-1;
 34595|         while (generation_num >= 0)
 34596|         {
 34597|             generation* gen = generation_of (generation_num);
 34598|             assert (generation_plan_allocation_start (gen));
 34599|             generation_num--;
 34600|         }
 34601|     }
 34602| #endif // _DEBUG
 34603|     if (!new_segment_p)
 34604|     {
 34605|         dprintf (2, ("Demoting ephemeral segment"));
 34606|         settings.demotion = TRUE;
 34607|         get_gc_data_per_heap()->set_mechanism_bit (gc_demotion_bit);
 34608|         demotion_low = heap_segment_mem (ephemeral_heap_segment);
 34609|         demotion_high = heap_segment_reserved (ephemeral_heap_segment);
 34610|     }
 34611|     else
 34612|     {
 34613|         demotion_low = MAX_PTR;
 34614|         demotion_high = 0;
 34615| #ifndef MULTIPLE_HEAPS
 34616|         settings.demotion = FALSE;
 34617|         get_gc_data_per_heap()->clear_mechanism_bit (gc_demotion_bit);
 34618| #endif //!MULTIPLE_HEAPS
 34619|     }
 34620|     if (!should_promote_ephemeral && new_segment_p)
 34621|     {
 34622|         assert ((ptrdiff_t)total_ephemeral_size <= eph_size);
 34623|     }
 34624|     if (heap_segment_mem (old_seg) == heap_segment_plan_allocated (old_seg))
 34625|     {
 34626|         verify_no_pins (heap_segment_mem (old_seg), heap_segment_reserved (old_seg));
 34627|     }
 34628|     verify_no_pins (heap_segment_plan_allocated (old_seg), heap_segment_reserved(old_seg));
 34629|     dprintf(2,("---- End of Heap Expansion ----"));
 34630|     return consing_gen;
 34631| }
 34632| #endif //!USE_REGIONS
 34633| BOOL gc_heap::expand_reused_seg_p()
 34634| {
 34635| #ifdef USE_REGIONS
 34636|     return FALSE;
 34637| #else
 34638|     BOOL reused_seg = FALSE;
 34639|     int heap_expand_mechanism = gc_data_per_heap.get_mechanism (gc_heap_expand);
 34640|     if ((heap_expand_mechanism == expand_reuse_bestfit) ||
 34641|         (heap_expand_mechanism == expand_reuse_normal))
 34642|     {
 34643|         reused_seg = TRUE;
 34644|     }
 34645|     return reused_seg;
 34646| #endif //USE_REGIONS
 34647| }
 34648| void gc_heap::verify_no_pins (uint8_t* start, uint8_t* end)
 34649| {
 34650| #ifdef VERIFY_HEAP
 34651|     if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
 34652|     {
 34653|         BOOL contains_pinned_plugs = FALSE;
 34654|         size_t mi = 0;
 34655|         mark* m = 0;
 34656|         while (mi != mark_stack_tos)
 34657|         {
 34658|             m = pinned_plug_of (mi);
 34659|             if ((pinned_plug (m) >= start) && (pinned_plug (m) < end))
 34660|             {
 34661|                 contains_pinned_plugs = TRUE;
 34662|                 break;
 34663|             }
 34664|             else
 34665|                 mi++;
 34666|         }
 34667|         if (contains_pinned_plugs)
 34668|         {
 34669|             FATAL_GC_ERROR();
 34670|         }
 34671|     }
 34672| #endif //VERIFY_HEAP
 34673| }
 34674| void gc_heap::set_static_data()
 34675| {
 34676|     static_data* pause_mode_sdata = static_data_table[latency_level];
 34677|     for (int i = 0; i < total_generation_count; i++)
 34678|     {
 34679|         dynamic_data* dd = dynamic_data_of (i);
 34680|         static_data* sdata = &pause_mode_sdata[i];
 34681|         dd->sdata = sdata;
 34682|         dd->min_size = sdata->min_size;
 34683|         dprintf (GTC_LOG, ("PM: %d, gen%d:  min: %zd, max: %zd, fr_l: %zd, fr_b: %d%%",
 34684|             settings.pause_mode,i,
 34685|             dd->min_size, dd_max_size (dd),
 34686|             sdata->fragmentation_limit, (int)(sdata->fragmentation_burden_limit * 100)));
 34687|     }
 34688| }
 34689| void gc_heap::init_static_data()
 34690| {
 34691|     size_t gen0_min_size = get_gen0_min_size();
 34692|     size_t gen0_max_size =
 34693| #ifdef MULTIPLE_HEAPS
 34694|         max (6*1024*1024, min ( Align(soh_segment_size/2), 200*1024*1024));
 34695| #else //MULTIPLE_HEAPS
 34696|         (
 34697| #ifdef BACKGROUND_GC
 34698|             gc_can_use_concurrent ?
 34699|             6*1024*1024 :
 34700| #endif //BACKGROUND_GC
 34701|             max (6*1024*1024,  min ( Align(soh_segment_size/2), 200*1024*1024))
 34702|         );
 34703| #endif //MULTIPLE_HEAPS
 34704|     gen0_max_size = max (gen0_min_size, gen0_max_size);
 34705|     if (heap_hard_limit)
 34706|     {
 34707|         size_t gen0_max_size_seg = soh_segment_size / 4;
 34708|         dprintf (GTC_LOG, ("limit gen0 max %zd->%zd", gen0_max_size, gen0_max_size_seg));
 34709|         gen0_max_size = min (gen0_max_size, gen0_max_size_seg);
 34710|     }
 34711|     size_t gen0_max_size_config = (size_t)GCConfig::GetGCGen0MaxBudget();
 34712|     if (gen0_max_size_config)
 34713|     {
 34714|         gen0_max_size = min (gen0_max_size, gen0_max_size_config);
 34715| #ifdef FEATURE_EVENT_TRACE
 34716|         gen0_max_budget_from_config = gen0_max_size;
 34717| #endif //FEATURE_EVENT_TRACE
 34718|     }
 34719|     gen0_max_size = Align (gen0_max_size);
 34720|     gen0_min_size = min (gen0_min_size, gen0_max_size);
 34721|     size_t gen1_max_size = (size_t)
 34722| #ifdef MULTIPLE_HEAPS
 34723|         max (6*1024*1024, Align(soh_segment_size/2));
 34724| #else //MULTIPLE_HEAPS
 34725|         (
 34726| #ifdef BACKGROUND_GC
 34727|             gc_can_use_concurrent ?
 34728|             6*1024*1024 :
 34729| #endif //BACKGROUND_GC
 34730|             max (6*1024*1024, Align(soh_segment_size/2))
 34731|         );
 34732| #endif //MULTIPLE_HEAPS
 34733|     size_t gen1_max_size_config = (size_t)GCConfig::GetGCGen1MaxBudget();
 34734|     if (gen1_max_size_config)
 34735|     {
 34736|         gen1_max_size = min (gen1_max_size, gen1_max_size_config);
 34737|     }
 34738|     gen1_max_size = Align (gen1_max_size);
 34739|     dprintf (GTC_LOG, ("gen0 min: %zd, max: %zd, gen1 max: %zd",
 34740|         gen0_min_size, gen0_max_size, gen1_max_size));
 34741|     for (int i = latency_level_first; i <= latency_level_last; i++)
 34742|     {
 34743|         static_data_table[i][0].min_size = gen0_min_size;
 34744|         static_data_table[i][0].max_size = gen0_max_size;
 34745|         static_data_table[i][1].max_size = gen1_max_size;
 34746|     }
 34747| }
 34748| bool gc_heap::init_dynamic_data()
 34749| {
 34750|     uint64_t now_raw_ts = RawGetHighPrecisionTimeStamp ();
 34751| #ifdef HEAP_BALANCE_INSTRUMENTATION
 34752|     start_raw_ts = now_raw_ts;
 34753| #endif //HEAP_BALANCE_INSTRUMENTATION
 34754|     uint64_t now = (uint64_t)((double)now_raw_ts * qpf_us);
 34755|     set_static_data();
 34756|     if (heap_number == 0)
 34757|     {
 34758|         process_start_time = now;
 34759|         smoothed_desired_total[0] = dynamic_data_of (0)->min_size * n_heaps;
 34760| #ifdef DYNAMIC_HEAP_COUNT
 34761|         last_suspended_end_time = now;
 34762| #endif //DYNAMIC_HEAP_COUNT
 34763| #ifdef HEAP_BALANCE_INSTRUMENTATION
 34764|         last_gc_end_time_us = now;
 34765|         dprintf (HEAP_BALANCE_LOG, ("qpf=%zd, start: %zd(%d)", qpf, start_raw_ts, now));
 34766| #endif //HEAP_BALANCE_INSTRUMENTATION
 34767|     }
 34768|     for (int i = 0; i < total_generation_count; i++)
 34769|     {
 34770|         dynamic_data* dd = dynamic_data_of (i);
 34771|         dd->gc_clock = 0;
 34772|         dd->time_clock = now;
 34773|         dd->previous_time_clock = now;
 34774|         dd->current_size = 0;
 34775|         dd->promoted_size = 0;
 34776|         dd->collection_count = 0;
 34777|         dd->new_allocation = dd->min_size;
 34778|         dd->gc_new_allocation = dd->new_allocation;
 34779|         dd->desired_allocation = dd->new_allocation;
 34780|         dd->fragmentation = 0;
 34781|     }
 34782|     return true;
 34783| }
 34784| float gc_heap::surv_to_growth (float cst, float limit, float max_limit)
 34785| {
 34786|     if (cst < ((max_limit - limit ) / (limit * (max_limit-1.0f))))
 34787|         return ((limit - limit*cst) / (1.0f - (cst * limit)));
 34788|     else
 34789|         return max_limit;
 34790| }
 34791| static size_t linear_allocation_model (float allocation_fraction, size_t new_allocation,
 34792|                                        size_t previous_desired_allocation, float time_since_previous_collection_secs)
 34793| {
 34794|     if ((allocation_fraction < 0.95) && (allocation_fraction > 0.0))
 34795|     {
 34796|         const float decay_time = 5*60.0f; // previous desired allocation expires over 5 minutes
 34797|         float decay_factor = (decay_time <= time_since_previous_collection_secs) ?
 34798|                                 0 :
 34799|                                 ((decay_time - time_since_previous_collection_secs) / decay_time);
 34800|         float previous_allocation_factor = (1.0f - allocation_fraction) * decay_factor;
 34801|         dprintf (2, ("allocation fraction: %d, decay factor: %d, previous allocation factor: %d",
 34802|             (int)(allocation_fraction*100.0), (int)(decay_factor*100.0), (int)(previous_allocation_factor*100.0)));
 34803|         new_allocation = (size_t)((1.0 - previous_allocation_factor)*new_allocation + previous_allocation_factor * previous_desired_allocation);
 34804|     }
 34805|     return new_allocation;
 34806| }
 34807| size_t gc_heap::desired_new_allocation (dynamic_data* dd,
 34808|                                         size_t out, int gen_number,
 34809|                                         int pass)
 34810| {
 34811|     gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
 34812|     if (dd_begin_data_size (dd) == 0)
 34813|     {
 34814|         size_t new_allocation = dd_min_size (dd);
 34815|         current_gc_data_per_heap->gen_data[gen_number].new_allocation = new_allocation;
 34816|         return new_allocation;
 34817|     }
 34818|     else
 34819|     {
 34820|         float     cst;
 34821|         size_t    previous_desired_allocation = dd_desired_allocation (dd);
 34822|         size_t    current_size = dd_current_size (dd);
 34823|         float     max_limit = dd_max_limit (dd);
 34824|         float     limit = dd_limit (dd);
 34825|         size_t    min_gc_size = dd_min_size (dd);
 34826|         float     f = 0;
 34827|         size_t    max_size = dd_max_size (dd);
 34828|         size_t    new_allocation = 0;
 34829|         float     time_since_previous_collection_secs = (dd_time_clock (dd) - dd_previous_time_clock (dd))*1e-6f;
 34830|         float allocation_fraction = (float) (dd_desired_allocation (dd) - dd_gc_new_allocation (dd)) / (float) (dd_desired_allocation (dd));
 34831|         if (gen_number >= max_generation)
 34832|         {
 34833|             size_t    new_size = 0;
 34834|             cst = min (1.0f, float (out) / float (dd_begin_data_size (dd)));
 34835|             f = surv_to_growth (cst, limit, max_limit);
 34836|             if (conserve_mem_setting != 0)
 34837|             {
 34838|                 float f_conserve = ((10.0f / conserve_mem_setting) - 1) * 0.5f + 1.0f;
 34839|                 f = min (f, f_conserve);
 34840|             }
 34841|             size_t max_growth_size = (size_t)(max_size / f);
 34842|             if (current_size >= max_growth_size)
 34843|             {
 34844|                 new_size = max_size;
 34845|             }
 34846|             else
 34847|             {
 34848|                 new_size = (size_t) min (max ( (f * current_size), min_gc_size), max_size);
 34849|             }
 34850|             assert ((new_size >= current_size) || (new_size == max_size));
 34851|             if (gen_number == max_generation)
 34852|             {
 34853|                 new_allocation  =  max((new_size - current_size), min_gc_size);
 34854|                 new_allocation = linear_allocation_model (allocation_fraction, new_allocation,
 34855|                                                           dd_desired_allocation (dd), time_since_previous_collection_secs);
 34856|                 if (
 34857| #ifdef BGC_SERVO_TUNING
 34858|                     !bgc_tuning::fl_tuning_triggered &&
 34859| #endif //BGC_SERVO_TUNING
 34860|                     (conserve_mem_setting == 0) &&
 34861|                     (dd_fragmentation (dd) > ((size_t)((f-1)*current_size))))
 34862|                 {
 34863|                     size_t new_allocation1 = max (min_gc_size,
 34864|                                                   (size_t)((float)new_allocation * current_size /
 34865|                                                            ((float)current_size + 2*dd_fragmentation (dd))));
 34866|                     dprintf (2, ("Reducing max_gen allocation due to fragmentation from %zd to %zd",
 34867|                                  new_allocation, new_allocation1));
 34868|                     new_allocation = new_allocation1;
 34869|                 }
 34870|             }
 34871|             else // not a SOH generation
 34872|             {
 34873|                 uint32_t memory_load = 0;
 34874|                 uint64_t available_physical = 0;
 34875|                 get_memory_info (&memory_load, &available_physical);
 34876| #ifdef TRACE_GC
 34877|                 if (heap_hard_limit)
 34878|                 {
 34879|                     size_t allocated = 0;
 34880|                     size_t committed = uoh_committed_size (gen_number, &allocated);
 34881|                     dprintf (1, ("GC#%zd h%d, GMI: UOH budget, UOH commit %zd (obj %zd, frag %zd), total commit: %zd (recorded: %zd)",
 34882|                         (size_t)settings.gc_index, heap_number,
 34883|                         committed, allocated,
 34884|                         dd_fragmentation (dynamic_data_of (gen_number)),
 34885|                         get_total_committed_size(), (current_total_committed - current_total_committed_bookkeeping)));
 34886|                 }
 34887| #endif //TRACE_GC
 34888|                 if (heap_number == 0)
 34889|                     settings.exit_memory_load = memory_load;
 34890|                 if (available_physical > 1024*1024)
 34891|                     available_physical -= 1024*1024;
 34892|                 uint64_t available_free = available_physical + (uint64_t)generation_free_list_space (generation_of (gen_number));
 34893|                 if (available_free > (uint64_t)MAX_PTR)
 34894|                 {
 34895|                     available_free = (uint64_t)MAX_PTR;
 34896|                 }
 34897|                 new_allocation = max (min(max((new_size - current_size), dd_desired_allocation (dynamic_data_of (max_generation))),
 34898|                                           (size_t)available_free),
 34899|                                       max ((current_size/4), min_gc_size));
 34900|                 new_allocation = linear_allocation_model (allocation_fraction, new_allocation,
 34901|                                                           dd_desired_allocation (dd), time_since_previous_collection_secs);
 34902|             }
 34903|         }
 34904|         else
 34905|         {
 34906|             size_t survivors = out;
 34907|             cst = float (survivors) / float (dd_begin_data_size (dd));
 34908|             f = surv_to_growth (cst, limit, max_limit);
 34909|             new_allocation = (size_t) min (max ((f * (survivors)), min_gc_size), max_size);
 34910|             new_allocation = linear_allocation_model (allocation_fraction, new_allocation,
 34911|                                                       dd_desired_allocation (dd), time_since_previous_collection_secs);
 34912|             if (gen_number == 0)
 34913|             {
 34914|                 if (pass == 0)
 34915|                 {
 34916|                     size_t free_space = generation_free_list_space (generation_of (gen_number));
 34917|                     dprintf (GTC_LOG, ("frag: %zd, min: %zd", free_space, min_gc_size));
 34918|                     if (free_space > min_gc_size)
 34919|                     {
 34920|                         settings.gen0_reduction_count = 2;
 34921|                     }
 34922|                     else
 34923|                     {
 34924|                         if (settings.gen0_reduction_count > 0)
 34925|                             settings.gen0_reduction_count--;
 34926|                     }
 34927|                 }
 34928|                 if (settings.gen0_reduction_count > 0)
 34929|                 {
 34930|                     dprintf (2, ("Reducing new allocation based on fragmentation"));
 34931|                     new_allocation = min (new_allocation,
 34932|                                           max (min_gc_size, (max_size/3)));
 34933|                 }
 34934| #ifdef DYNAMIC_HEAP_COUNT
 34935|                 if (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes)
 34936|                 {
 34937|                     float f_older_gen = ((10.0f / conserve_mem_setting) - 1) * 0.5f;
 34938|                     size_t older_size = 0;
 34939|                     for (int gen_index_older = 1; gen_index_older < total_generation_count; gen_index_older++)
 34940|                     {
 34941|                         dynamic_data* dd_older = dynamic_data_of (gen_index_older);
 34942|                         older_size += dd_current_size (dd_older);
 34943|                     }
 34944|                     size_t new_allocation_from_older = (size_t)(older_size*f_older_gen);
 34945|                     new_allocation = min (new_allocation, new_allocation_from_older);
 34946|                     new_allocation = max (new_allocation, min_gc_size);
 34947|                     dprintf (2, ("f_older_gen: %d%% older_size: %zd new_allocation: %zd",
 34948|                         (int)(f_older_gen*100),
 34949|                         older_size,
 34950|                         new_allocation));
 34951|                 }
 34952| #endif //DYNAMIC_HEAP_COUNT
 34953|             }
 34954|         }
 34955|         size_t new_allocation_ret = Align (new_allocation, get_alignment_constant (gen_number <= max_generation));
 34956|         int gen_data_index = gen_number;
 34957|         gc_generation_data* gen_data = &(current_gc_data_per_heap->gen_data[gen_data_index]);
 34958|         gen_data->new_allocation = new_allocation_ret;
 34959|         dd_surv (dd) = cst;
 34960|         dprintf (1, (ThreadStressLog::gcDesiredNewAllocationMsg(),
 34961|                     heap_number, gen_number, out, current_size, (dd_desired_allocation (dd) - dd_gc_new_allocation (dd)),
 34962|                     (int)(cst*100), (int)(f*100), current_size + new_allocation, new_allocation));
 34963|         return new_allocation_ret;
 34964|     }
 34965| }
 34966| size_t gc_heap::generation_plan_size (int gen_number)
 34967| {
 34968| #ifdef USE_REGIONS
 34969|     size_t result = 0;
 34970|     heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (gen_number)));
 34971|     while (seg)
 34972|     {
 34973|         uint8_t* end = heap_segment_plan_allocated (seg);
 34974|         result += end - heap_segment_mem (seg);
 34975|         dprintf (REGIONS_LOG, ("h%d size + %zd (%p - %p) -> %zd",
 34976|             heap_number, (end - heap_segment_mem (seg)),
 34977|             heap_segment_mem (seg), end, result));
 34978|         seg = heap_segment_next (seg);
 34979|     }
 34980|     return result;
 34981| #else //USE_REGIONS
 34982|     if (0 == gen_number)
 34983|         return max((heap_segment_plan_allocated (ephemeral_heap_segment) -
 34984|                     generation_plan_allocation_start (generation_of (gen_number))),
 34985|                    (int)Align (min_obj_size));
 34986|     else
 34987|     {
 34988|         generation* gen = generation_of (gen_number);
 34989|         if (heap_segment_rw (generation_start_segment (gen)) == ephemeral_heap_segment)
 34990|             return (generation_plan_allocation_start (generation_of (gen_number - 1)) -
 34991|                     generation_plan_allocation_start (generation_of (gen_number)));
 34992|         else
 34993|         {
 34994|             size_t gensize = 0;
 34995|             heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 34996|             PREFIX_ASSUME(seg != NULL);
 34997|             while (seg && (seg != ephemeral_heap_segment))
 34998|             {
 34999|                 gensize += heap_segment_plan_allocated (seg) -
 35000|                            heap_segment_mem (seg);
 35001|                 seg = heap_segment_next_rw (seg);
 35002|             }
 35003|             if (seg)
 35004|             {
 35005|                 gensize += (generation_plan_allocation_start (generation_of (gen_number - 1)) -
 35006|                             heap_segment_mem (ephemeral_heap_segment));
 35007|             }
 35008|             return gensize;
 35009|         }
 35010|     }
 35011| #endif //USE_REGIONS
 35012| }
 35013| size_t gc_heap::generation_size (int gen_number)
 35014| {
 35015| #ifdef USE_REGIONS
 35016|     size_t result = 0;
 35017|     heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (gen_number)));
 35018|     while (seg)
 35019|     {
 35020|         uint8_t* end = heap_segment_allocated (seg);
 35021|         result += end - heap_segment_mem (seg);
 35022|         dprintf (2, ("h%d size + %zd (%p - %p) -> %zd",
 35023|             heap_number, (end - heap_segment_mem (seg)),
 35024|             heap_segment_mem (seg), end, result));
 35025|         seg = heap_segment_next (seg);
 35026|     }
 35027|     return result;
 35028| #else //USE_REGIONS
 35029|     if (0 == gen_number)
 35030|         return max((heap_segment_allocated (ephemeral_heap_segment) -
 35031|                     generation_allocation_start (generation_of (gen_number))),
 35032|                    (int)Align (min_obj_size));
 35033|     else
 35034|     {
 35035|         generation* gen = generation_of (gen_number);
 35036|         if (heap_segment_rw (generation_start_segment (gen)) == ephemeral_heap_segment)
 35037|             return (generation_allocation_start (generation_of (gen_number - 1)) -
 35038|                     generation_allocation_start (generation_of (gen_number)));
 35039|         else
 35040|         {
 35041|             size_t gensize = 0;
 35042|             heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 35043|             PREFIX_ASSUME(seg != NULL);
 35044|             while (seg && (seg != ephemeral_heap_segment))
 35045|             {
 35046|                 gensize += heap_segment_allocated (seg) -
 35047|                            heap_segment_mem (seg);
 35048|                 seg = heap_segment_next_rw (seg);
 35049|             }
 35050|             if (seg)
 35051|             {
 35052|                 gensize += (generation_allocation_start (generation_of (gen_number - 1)) -
 35053|                             heap_segment_mem (ephemeral_heap_segment));
 35054|             }
 35055|             return gensize;
 35056|         }
 35057|     }
 35058| #endif //USE_REGIONS
 35059| }
 35060| size_t  gc_heap::compute_in (int gen_number)
 35061| {
 35062|     assert (gen_number != 0);
 35063|     dynamic_data* dd = dynamic_data_of (gen_number);
 35064|     size_t in = generation_allocation_size (generation_of (gen_number));
 35065| #ifndef USE_REGIONS
 35066|     if (gen_number == max_generation && ephemeral_promotion)
 35067|     {
 35068|         in = 0;
 35069|         for (int i = 0; i <= max_generation; i++)
 35070|         {
 35071|             dynamic_data* dd = dynamic_data_of (i);
 35072|             in += dd_survived_size (dd);
 35073|             if (i != max_generation)
 35074|             {
 35075|                 generation_condemned_allocated (generation_of (gen_number)) += dd_survived_size (dd);
 35076|             }
 35077|         }
 35078|     }
 35079| #endif //!USE_REGIONS
 35080|     dd_gc_new_allocation (dd) -= in;
 35081|     dd_new_allocation (dd) = dd_gc_new_allocation (dd);
 35082|     gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
 35083|     gc_generation_data* gen_data = &(current_gc_data_per_heap->gen_data[gen_number]);
 35084|     gen_data->in = in;
 35085|     generation_allocation_size (generation_of (gen_number)) = 0;
 35086|     return in;
 35087| }
 35088| void  gc_heap::compute_promoted_allocation (int gen_number)
 35089| {
 35090|     compute_in (gen_number);
 35091| }
 35092| #ifdef HOST_64BIT
 35093| inline
 35094| size_t gc_heap::trim_youngest_desired (uint32_t memory_load,
 35095|                                        size_t total_new_allocation,
 35096|                                        size_t total_min_allocation)
 35097| {
 35098|     if (memory_load < MAX_ALLOWED_MEM_LOAD)
 35099|     {
 35100|         size_t remain_memory_load = (MAX_ALLOWED_MEM_LOAD - memory_load) * mem_one_percent;
 35101|         return min (total_new_allocation, remain_memory_load);
 35102|     }
 35103|     else
 35104|     {
 35105|         size_t total_max_allocation = max (mem_one_percent, total_min_allocation);
 35106|         return min (total_new_allocation, total_max_allocation);
 35107|     }
 35108| }
 35109| size_t gc_heap::joined_youngest_desired (size_t new_allocation)
 35110| {
 35111|     dprintf (2, ("Entry memory load: %d; gen0 new_alloc: %zd", settings.entry_memory_load, new_allocation));
 35112|     size_t final_new_allocation = new_allocation;
 35113|     if (new_allocation > MIN_YOUNGEST_GEN_DESIRED)
 35114|     {
 35115|         uint32_t num_heaps = 1;
 35116| #ifdef MULTIPLE_HEAPS
 35117|         num_heaps = gc_heap::n_heaps;
 35118| #endif //MULTIPLE_HEAPS
 35119|         size_t total_new_allocation = new_allocation * num_heaps;
 35120|         size_t total_min_allocation = MIN_YOUNGEST_GEN_DESIRED * num_heaps;
 35121|         if ((settings.entry_memory_load >= MAX_ALLOWED_MEM_LOAD) ||
 35122|             (total_new_allocation > max (youngest_gen_desired_th, total_min_allocation)))
 35123|         {
 35124|             uint32_t memory_load = 0;
 35125|             get_memory_info (&memory_load);
 35126|             settings.exit_memory_load = memory_load;
 35127|             dprintf (2, ("Current memory load: %d", memory_load));
 35128|             size_t final_total =
 35129|                 trim_youngest_desired (memory_load, total_new_allocation, total_min_allocation);
 35130|             size_t max_new_allocation =
 35131| #ifdef MULTIPLE_HEAPS
 35132|                                          dd_max_size (g_heaps[0]->dynamic_data_of (0));
 35133| #else //MULTIPLE_HEAPS
 35134|                                          dd_max_size (dynamic_data_of (0));
 35135| #endif //MULTIPLE_HEAPS
 35136|             final_new_allocation  = min (Align ((final_total / num_heaps), get_alignment_constant (TRUE)), max_new_allocation);
 35137|         }
 35138|     }
 35139|     if (final_new_allocation < new_allocation)
 35140|     {
 35141|         settings.gen0_reduction_count = 2;
 35142|     }
 35143|     return final_new_allocation;
 35144| }
 35145| #endif // HOST_64BIT
 35146| inline
 35147| gc_history_global* gc_heap::get_gc_data_global()
 35148| {
 35149| #ifdef BACKGROUND_GC
 35150|     return (settings.concurrent ? &bgc_data_global : &gc_data_global);
 35151| #else
 35152|     return &gc_data_global;
 35153| #endif //BACKGROUND_GC
 35154| }
 35155| inline
 35156| gc_history_per_heap* gc_heap::get_gc_data_per_heap()
 35157| {
 35158| #ifdef BACKGROUND_GC
 35159|     return (settings.concurrent ? &bgc_data_per_heap : &gc_data_per_heap);
 35160| #else
 35161|     return &gc_data_per_heap;
 35162| #endif //BACKGROUND_GC
 35163| }
 35164| void gc_heap::compute_new_dynamic_data (int gen_number)
 35165| {
 35166|     PREFIX_ASSUME(gen_number >= 0);
 35167|     PREFIX_ASSUME(gen_number <= max_generation);
 35168|     dynamic_data* dd = dynamic_data_of (gen_number);
 35169|     generation*   gen = generation_of (gen_number);
 35170|     size_t        in = (gen_number==0) ? 0 : compute_in (gen_number);
 35171|     size_t total_gen_size = generation_size (gen_number);
 35172|     dd_fragmentation (dd) = generation_free_list_space (gen) + generation_free_obj_space (gen);
 35173|     if (dd_fragmentation (dd) <= total_gen_size)
 35174|         dd_current_size (dd) = total_gen_size - dd_fragmentation (dd);
 35175|     else
 35176|         dd_current_size (dd) = 0;
 35177|     gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
 35178|     size_t out = dd_survived_size (dd);
 35179|     gc_generation_data* gen_data = &(current_gc_data_per_heap->gen_data[gen_number]);
 35180|     gen_data->size_after = total_gen_size;
 35181|     gen_data->free_list_space_after = generation_free_list_space (gen);
 35182|     gen_data->free_obj_space_after = generation_free_obj_space (gen);
 35183|     if ((settings.pause_mode == pause_low_latency) && (gen_number <= 1))
 35184|     {
 35185|         dd_desired_allocation (dd) = low_latency_alloc;
 35186|         dd_gc_new_allocation (dd) = dd_desired_allocation (dd);
 35187|         dd_new_allocation (dd) = dd_gc_new_allocation (dd);
 35188|     }
 35189|     else
 35190|     {
 35191|         if (gen_number == 0)
 35192|         {
 35193|             size_t final_promoted = 0;
 35194|             final_promoted = min (finalization_promoted_bytes, out);
 35195|             PREFIX_ASSUME(final_promoted <= out);
 35196|             dprintf (2, ("gen: %d final promoted: %zd", gen_number, final_promoted));
 35197|             dd_freach_previous_promotion (dd) = final_promoted;
 35198|             size_t lower_bound = desired_new_allocation  (dd, out-final_promoted, gen_number, 0);
 35199|             if (settings.condemned_generation == 0)
 35200|             {
 35201|                 dd_desired_allocation (dd) = lower_bound;
 35202|             }
 35203|             else
 35204|             {
 35205|                 size_t higher_bound = desired_new_allocation (dd, out, gen_number, 1);
 35206|                 if (dd_desired_allocation (dd) < lower_bound)
 35207|                 {
 35208|                     dd_desired_allocation (dd) = lower_bound;
 35209|                 }
 35210|                 else if (dd_desired_allocation (dd) > higher_bound)
 35211|                 {
 35212|                     dd_desired_allocation (dd) = higher_bound;
 35213|                 }
 35214| #if defined (HOST_64BIT) && !defined (MULTIPLE_HEAPS)
 35215|                 dd_desired_allocation (dd) = joined_youngest_desired (dd_desired_allocation (dd));
 35216| #endif // HOST_64BIT && !MULTIPLE_HEAPS
 35217|                 trim_youngest_desired_low_memory();
 35218|                 dprintf (2, ("final gen0 new_alloc: %zd", dd_desired_allocation (dd)));
 35219|             }
 35220|         }
 35221|         else
 35222|         {
 35223|             dd_desired_allocation (dd) = desired_new_allocation (dd, out, gen_number, 0);
 35224|         }
 35225|         dd_gc_new_allocation (dd) = dd_desired_allocation (dd);
 35226| #ifdef USE_REGIONS
 35227|         dd_new_allocation (dd) = dd_gc_new_allocation (dd) - in;
 35228| #else //USE_REGIONS
 35229|         dd_new_allocation (dd) = dd_gc_new_allocation (dd);
 35230| #endif //USE_REGIONS
 35231|     }
 35232|     gen_data->pinned_surv = dd_pinned_survived_size (dd);
 35233|     gen_data->npinned_surv = dd_survived_size (dd) - dd_pinned_survived_size (dd);
 35234|     dd_promoted_size (dd) = out;
 35235|     if (gen_number == max_generation)
 35236|     {
 35237|         for (int i = (gen_number + 1); i < total_generation_count; i++)
 35238|         {
 35239|             dd = dynamic_data_of (i);
 35240|             total_gen_size = generation_size (i);
 35241|             generation* gen = generation_of (i);
 35242|             dd_fragmentation (dd) = generation_free_list_space (gen) +
 35243|                 generation_free_obj_space (gen);
 35244|             dd_current_size (dd) = total_gen_size - dd_fragmentation (dd);
 35245|             dd_survived_size (dd) = dd_current_size (dd);
 35246|             in = 0;
 35247|             out = dd_current_size (dd);
 35248|             dd_desired_allocation (dd) = desired_new_allocation (dd, out, i, 0);
 35249|             dd_gc_new_allocation (dd) = Align (dd_desired_allocation (dd),
 35250|                 get_alignment_constant (FALSE));
 35251|             dd_new_allocation (dd) = dd_gc_new_allocation (dd);
 35252|             gen_data = &(current_gc_data_per_heap->gen_data[i]);
 35253|             gen_data->size_after = total_gen_size;
 35254|             gen_data->free_list_space_after = generation_free_list_space (gen);
 35255|             gen_data->free_obj_space_after = generation_free_obj_space (gen);
 35256|             gen_data->npinned_surv = out;
 35257| #ifdef BACKGROUND_GC
 35258|             if (i == loh_generation)
 35259|                 end_loh_size = total_gen_size;
 35260|             if (i == poh_generation)
 35261|                 end_poh_size = total_gen_size;
 35262| #endif //BACKGROUND_GC
 35263|             dd_promoted_size (dd) = out;
 35264|         }
 35265|     }
 35266| }
 35267| void gc_heap::trim_youngest_desired_low_memory()
 35268| {
 35269|     if (g_low_memory_status)
 35270|     {
 35271|         size_t committed_mem = committed_size();
 35272|         dynamic_data* dd = dynamic_data_of (0);
 35273|         size_t current = dd_desired_allocation (dd);
 35274|         size_t candidate = max (Align ((committed_mem / 10), get_alignment_constant(FALSE)), dd_min_size (dd));
 35275|         dd_desired_allocation (dd) = min (current, candidate);
 35276|     }
 35277| }
 35278| ptrdiff_t gc_heap::estimate_gen_growth (int gen_number)
 35279| {
 35280|     dynamic_data* dd_gen = dynamic_data_of (gen_number);
 35281|     generation *gen = generation_of (gen_number);
 35282|     ptrdiff_t new_allocation_gen = dd_new_allocation (dd_gen);
 35283|     ptrdiff_t free_list_space_gen = generation_free_list_space (gen);
 35284| #ifdef USE_REGIONS
 35285|     ptrdiff_t reserved_not_in_use = 0;
 35286|     ptrdiff_t allocated_gen = 0;
 35287|     for (heap_segment* region = generation_start_segment_rw (gen); region != nullptr; region = heap_segment_next (region))
 35288|     {
 35289|         allocated_gen += heap_segment_allocated (region) - heap_segment_mem (region);
 35290|         reserved_not_in_use += heap_segment_reserved (region) - heap_segment_allocated (region);
 35291|     }
 35292|     double free_list_fraction_gen = (allocated_gen == 0) ? 0.0 : (double)(free_list_space_gen) / (double)allocated_gen;
 35293|     ptrdiff_t usable_free_space = (ptrdiff_t)(free_list_fraction_gen * free_list_space_gen);
 35294|     ptrdiff_t budget_gen = new_allocation_gen - usable_free_space - reserved_not_in_use;
 35295|     dprintf (REGIONS_LOG, ("h%2d gen %d budget %zd allocated: %zd, FL: %zd, reserved_not_in_use %zd budget_gen %zd",
 35296|         heap_number, gen_number, new_allocation_gen, allocated_gen, free_list_space_gen, reserved_not_in_use, budget_gen));
 35297| #else  //USE_REGIONS
 35298|     ptrdiff_t budget_gen = new_allocation_gen - (free_list_space_gen / 2);
 35299|     dprintf (REGIONS_LOG, ("budget for gen %d on heap %d is %zd (new %zd, free %zd)",
 35300|         gen_number, heap_number, budget_gen, new_allocation_gen, free_list_space_gen));
 35301| #endif //USE_REGIONS
 35302|     return budget_gen;
 35303| }
 35304| void gc_heap::decommit_ephemeral_segment_pages()
 35305| {
 35306|     if (settings.concurrent || use_large_pages_p || (settings.pause_mode == pause_no_gc))
 35307|     {
 35308|         return;
 35309|     }
 35310| #if defined(MULTIPLE_HEAPS) && defined(USE_REGIONS)
 35311|     for (int gen_number = soh_gen0; gen_number <= soh_gen1; gen_number++)
 35312|     {
 35313|         generation *gen = generation_of (gen_number);
 35314|         heap_segment* tail_region = generation_tail_region (gen);
 35315|         uint8_t* previous_decommit_target = heap_segment_decommit_target (tail_region);
 35316|         for (heap_segment* region = generation_start_segment_rw (gen); region != nullptr; region = heap_segment_next (region))
 35317|         {
 35318|             heap_segment_decommit_target (region) = heap_segment_reserved (region);
 35319|         }
 35320|         ptrdiff_t budget_gen = estimate_gen_growth (gen_number) + loh_size_threshold;
 35321|         if (budget_gen >= 0)
 35322|         {
 35323|             continue;
 35324|         }
 35325|         ptrdiff_t tail_region_size = heap_segment_reserved (tail_region) - heap_segment_mem (tail_region);
 35326|         ptrdiff_t unneeded_tail_size = min (-budget_gen, tail_region_size);
 35327|         uint8_t *decommit_target = heap_segment_reserved (tail_region) - unneeded_tail_size;
 35328|         decommit_target = max (decommit_target, heap_segment_allocated (tail_region));
 35329|         if (decommit_target < previous_decommit_target)
 35330|         {
 35331|             ptrdiff_t target_decrease = previous_decommit_target - decommit_target;
 35332|             decommit_target += target_decrease * 2 / 3;
 35333|         }
 35334| #ifdef STRESS_DECOMMIT
 35335|         decommit_target = heap_segment_mem (tail_region) + gc_rand::get_rand (heap_segment_reserved (tail_region) - heap_segment_mem (tail_region));
 35336| #endif //STRESS_DECOMMIT
 35337|         heap_segment_decommit_target (tail_region) = decommit_target;
 35338|         if (decommit_target < heap_segment_committed (tail_region))
 35339|         {
 35340|             gradual_decommit_in_progress_p = TRUE;
 35341|             dprintf (1, ("h%2d gen %d region %p allocated %zdkB committed %zdkB reduce_commit by %zdkB",
 35342|                 heap_number,
 35343|                 gen_number,
 35344|                 get_region_start (tail_region),
 35345|                 (heap_segment_allocated (tail_region) - get_region_start (tail_region))/1024,
 35346|                 (heap_segment_committed (tail_region) - get_region_start (tail_region))/1024,
 35347|                 (heap_segment_committed (tail_region) - decommit_target)/1024));
 35348|         }
 35349|         dprintf(3, ("h%2d gen %d allocated: %zdkB committed: %zdkB target: %zdkB",
 35350|             heap_number,
 35351|             gen_number,
 35352|             (heap_segment_allocated (tail_region) - heap_segment_mem (tail_region))/1024,
 35353|             (heap_segment_committed (tail_region) - heap_segment_mem (tail_region))/1024,
 35354|             (decommit_target                      - heap_segment_mem (tail_region))/1024));
 35355|     }
 35356| #elif !defined(USE_REGIONS)
 35357|     dynamic_data* dd0 = dynamic_data_of (0);
 35358|     ptrdiff_t desired_allocation = dd_new_allocation (dd0) +
 35359|                                    max (estimate_gen_growth (soh_gen1), 0) +
 35360|                                    loh_size_threshold;
 35361|     size_t slack_space =
 35362| #ifdef HOST_64BIT
 35363|                 max(min(min(soh_segment_size/32, dd_max_size (dd0)), (generation_size (max_generation) / 10)), (size_t)desired_allocation);
 35364| #else
 35365| #ifdef FEATURE_CORECLR
 35366|                 desired_allocation;
 35367| #else
 35368|                 dd_max_size (dd0);
 35369| #endif //FEATURE_CORECLR
 35370| #endif // HOST_64BIT
 35371|     uint8_t *decommit_target = heap_segment_allocated (ephemeral_heap_segment) + slack_space;
 35372|     if (decommit_target < heap_segment_decommit_target (ephemeral_heap_segment))
 35373|     {
 35374|         ptrdiff_t target_decrease = heap_segment_decommit_target (ephemeral_heap_segment) - decommit_target;
 35375|         decommit_target += target_decrease * 2 / 3;
 35376|     }
 35377|     heap_segment_decommit_target (ephemeral_heap_segment) = decommit_target;
 35378| #ifdef MULTIPLE_HEAPS
 35379|     if (decommit_target < heap_segment_committed (ephemeral_heap_segment))
 35380|     {
 35381|         gradual_decommit_in_progress_p = TRUE;
 35382|     }
 35383| #ifdef _DEBUG
 35384|     ephemeral_heap_segment->saved_committed = heap_segment_committed (ephemeral_heap_segment);
 35385|     ephemeral_heap_segment->saved_desired_allocation = dd_desired_allocation (dd0);
 35386| #endif // _DEBUG
 35387| #endif // MULTIPLE_HEAPS
 35388| #ifndef MULTIPLE_HEAPS
 35389|     size_t ephemeral_elapsed = (size_t)((dd_time_clock (dd0) - gc_last_ephemeral_decommit_time) / 1000);
 35390|     gc_last_ephemeral_decommit_time = dd_time_clock (dd0);
 35391|     ptrdiff_t decommit_size = heap_segment_committed (ephemeral_heap_segment) - decommit_target;
 35392|     ptrdiff_t max_decommit_size = min (ephemeral_elapsed, (10*1000)) * DECOMMIT_SIZE_PER_MILLISECOND;
 35393|     decommit_size = min (decommit_size, max_decommit_size);
 35394|     slack_space = heap_segment_committed (ephemeral_heap_segment) - heap_segment_allocated (ephemeral_heap_segment) - decommit_size;
 35395|     decommit_heap_segment_pages (ephemeral_heap_segment, slack_space);
 35396| #endif // !MULTIPLE_HEAPS
 35397|     gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
 35398|     current_gc_data_per_heap->extra_gen0_committed = heap_segment_committed (ephemeral_heap_segment) - heap_segment_allocated (ephemeral_heap_segment);
 35399| #endif //MULTIPLE_HEAPS && USE_REGIONS
 35400| }
 35401| bool gc_heap::decommit_step (uint64_t step_milliseconds)
 35402| {
 35403|     if (settings.pause_mode == pause_no_gc)
 35404|     {
 35405|         return false;
 35406|     }
 35407|     size_t decommit_size = 0;
 35408| #ifdef USE_REGIONS
 35409|     const size_t max_decommit_step_size = DECOMMIT_SIZE_PER_MILLISECOND * step_milliseconds;
 35410|     for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
 35411|     {
 35412|         dprintf (REGIONS_LOG, ("decommit_step %d, regions_to_decommit = %zd",
 35413|             kind, global_regions_to_decommit[kind].get_num_free_regions()));
 35414|         while (global_regions_to_decommit[kind].get_num_free_regions() > 0)
 35415|         {
 35416|             heap_segment* region = global_regions_to_decommit[kind].unlink_region_front();
 35417|             size_t size = decommit_region (region, recorded_committed_free_bucket, -1);
 35418|             decommit_size += size;
 35419|             if (decommit_size >= max_decommit_step_size)
 35420|             {
 35421|                 return true;
 35422|             }
 35423|         }
 35424|     }
 35425|     if (use_large_pages_p)
 35426|     {
 35427|         return (decommit_size != 0);
 35428|     }
 35429| #endif //USE_REGIONS
 35430| #ifdef MULTIPLE_HEAPS
 35431|     assert(!use_large_pages_p);
 35432|     for (int i = 0; i < n_heaps; i++)
 35433|     {
 35434|         gc_heap* hp = gc_heap::g_heaps[i];
 35435|         decommit_size += hp->decommit_ephemeral_segment_pages_step ();
 35436|     }
 35437| #endif //MULTIPLE_HEAPS
 35438|     return (decommit_size != 0);
 35439| }
 35440| #ifdef USE_REGIONS
 35441| size_t gc_heap::decommit_region (heap_segment* region, int bucket, int h_number)
 35442| {
 35443|     uint8_t* page_start = align_lower_page (get_region_start (region));
 35444|     uint8_t* decommit_end = heap_segment_committed (region);
 35445|     size_t decommit_size = decommit_end - page_start;
 35446|     bool decommit_succeeded_p = virtual_decommit (page_start, decommit_size, bucket, h_number);
 35447|     bool require_clearing_memory_p = !decommit_succeeded_p || use_large_pages_p;
 35448|     dprintf (REGIONS_LOG, ("decommitted region %p(%p-%p) (%zu bytes) - success: %d",
 35449|         region,
 35450|         page_start,
 35451|         decommit_end,
 35452|         decommit_size,
 35453|         decommit_succeeded_p));
 35454|     if (require_clearing_memory_p)
 35455|     {
 35456|         uint8_t* clear_end = use_large_pages_p ? heap_segment_used (region) : heap_segment_committed (region);
 35457|         size_t clear_size = clear_end - page_start;
 35458|         memclr (page_start, clear_size);
 35459|         heap_segment_used (region) = heap_segment_mem (region);
 35460|         dprintf(REGIONS_LOG, ("cleared region %p(%p-%p) (%zu bytes)",
 35461|             region,
 35462|             page_start,
 35463|             clear_end,
 35464|             clear_size));
 35465|     }
 35466|     else
 35467|     {
 35468|         heap_segment_committed (region) = heap_segment_mem (region);
 35469|     }
 35470|     if ((region->flags & heap_segment_flags_ma_committed) != 0)
 35471|     {
 35472| #ifdef MULTIPLE_HEAPS
 35473|         gc_heap* hp = g_heaps [0];
 35474| #else
 35475|         gc_heap* hp = pGenGCHeap;
 35476| #endif
 35477|         hp->decommit_mark_array_by_seg (region);
 35478|         region->flags &= ~(heap_segment_flags_ma_committed);
 35479|     }
 35480|     if (use_large_pages_p)
 35481|     {
 35482|         assert (heap_segment_used (region) == heap_segment_mem (region));
 35483|     }
 35484|     else
 35485|     {
 35486|         assert (heap_segment_committed (region) == heap_segment_mem (region));
 35487|     }
 35488|     assert ((region->flags & heap_segment_flags_ma_committed) == 0);
 35489|     global_region_allocator.delete_region (get_region_start (region));
 35490|     return decommit_size;
 35491| }
 35492| #endif //USE_REGIONS
 35493| #ifdef MULTIPLE_HEAPS
 35494| size_t gc_heap::decommit_ephemeral_segment_pages_step ()
 35495| {
 35496|     size_t size = 0;
 35497| #ifdef USE_REGIONS
 35498|     for (int gen_number = soh_gen0; gen_number <= soh_gen1; gen_number++)
 35499|     {
 35500|         generation* gen = generation_of (gen_number);
 35501|         heap_segment* seg = generation_tail_region (gen);
 35502| #else // USE_REGIONS
 35503|     {
 35504|         heap_segment* seg = ephemeral_heap_segment;
 35505|         assert (seg->saved_desired_allocation == dd_desired_allocation (dynamic_data_of (0)));
 35506| #endif // USE_REGIONS
 35507|         uint8_t* decommit_target = heap_segment_decommit_target (seg);
 35508|         size_t EXTRA_SPACE = 2 * OS_PAGE_SIZE;
 35509|         decommit_target += EXTRA_SPACE;
 35510| #ifdef STRESS_DECOMMIT
 35511|         decommit_target = heap_segment_mem (seg) + gc_rand::get_rand (heap_segment_reserved (seg) - heap_segment_mem (seg));
 35512| #endif //STRESS_DECOMMIT
 35513|         uint8_t* committed = heap_segment_committed (seg);
 35514|         uint8_t* allocated = (seg == ephemeral_heap_segment) ? alloc_allocated : heap_segment_allocated (seg);
 35515|         if ((allocated <= decommit_target) && (decommit_target < committed))
 35516|         {
 35517| #ifdef USE_REGIONS
 35518|             if (gen_number == soh_gen0)
 35519|             {
 35520|                 if (!try_enter_spin_lock (&more_space_lock_soh))
 35521|                 {
 35522|                     continue;
 35523|                 }
 35524|                 add_saved_spinlock_info (false, me_acquire, mt_decommit_step, msl_entered);
 35525|                 seg = generation_tail_region (gen);
 35526| #ifndef STRESS_DECOMMIT
 35527|                 decommit_target = heap_segment_decommit_target (seg);
 35528|                 decommit_target += EXTRA_SPACE;
 35529| #endif
 35530|                 committed = heap_segment_committed (seg);
 35531|                 allocated = (seg == ephemeral_heap_segment) ? alloc_allocated : heap_segment_allocated (seg);
 35532|             }
 35533|             if ((allocated <= decommit_target) && (decommit_target < committed))
 35534| #else // USE_REGIONS
 35535|             assert (seg->saved_committed == heap_segment_committed (seg));
 35536| #endif // USE_REGIONS
 35537|             {
 35538|                 size_t full_decommit_size = (committed - decommit_target);
 35539|                 size_t decommit_size = min (max_decommit_step_size, full_decommit_size);
 35540|                 uint8_t* new_committed = (committed - decommit_size);
 35541|                 size += decommit_heap_segment_pages_worker (seg, new_committed);
 35542| #ifdef _DEBUG
 35543|                 seg->saved_committed = committed - size;
 35544| #endif // _DEBUG
 35545|             }
 35546| #ifdef USE_REGIONS
 35547|             if (gen_number == soh_gen0)
 35548|             {
 35549|                 add_saved_spinlock_info (false, me_release, mt_decommit_step, msl_entered);
 35550|                 leave_spin_lock (&more_space_lock_soh);
 35551|             }
 35552| #endif // USE_REGIONS
 35553|         }
 35554|     }
 35555|     return size;
 35556| }
 35557| #endif //MULTIPLE_HEAPS
 35558| size_t gc_heap::generation_fragmentation (generation* gen,
 35559|                                           generation* consing_gen,
 35560|                                           uint8_t* end)
 35561| {
 35562|     ptrdiff_t frag = 0;
 35563| #ifdef USE_REGIONS
 35564|     for (int gen_num = 0; gen_num <= gen->gen_num; gen_num++)
 35565|     {
 35566|         generation* gen = generation_of (gen_num);
 35567|         heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 35568|         while (seg)
 35569|         {
 35570|             frag += (heap_segment_saved_allocated (seg) -
 35571|                  heap_segment_plan_allocated (seg));
 35572|             dprintf (3, ("h%d g%d adding seg plan frag: %p-%p=%zd -> %zd",
 35573|                 heap_number, gen_num,
 35574|                 heap_segment_saved_allocated (seg),
 35575|                 heap_segment_plan_allocated (seg),
 35576|                 (heap_segment_saved_allocated (seg) - heap_segment_plan_allocated (seg)),
 35577|                 frag));
 35578|             seg = heap_segment_next_rw (seg);
 35579|         }
 35580|     }
 35581| #else //USE_REGIONS
 35582|     uint8_t* alloc = generation_allocation_pointer (consing_gen);
 35583|     if (in_range_for_segment (alloc, ephemeral_heap_segment))
 35584|     {
 35585|         if (alloc <= heap_segment_allocated(ephemeral_heap_segment))
 35586|             frag = end - alloc;
 35587|         else
 35588|         {
 35589|             frag = 0;
 35590|         }
 35591|         dprintf (3, ("ephemeral frag: %zd", frag));
 35592|     }
 35593|     else
 35594|         frag = (heap_segment_allocated (ephemeral_heap_segment) -
 35595|                 heap_segment_mem (ephemeral_heap_segment));
 35596|     heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 35597|     PREFIX_ASSUME(seg != NULL);
 35598|     while (seg != ephemeral_heap_segment)
 35599|     {
 35600|         frag += (heap_segment_allocated (seg) -
 35601|                  heap_segment_plan_allocated (seg));
 35602|         dprintf (3, ("seg: %zx, frag: %zd", (size_t)seg,
 35603|                      (heap_segment_allocated (seg) -
 35604|                       heap_segment_plan_allocated (seg))));
 35605|         seg = heap_segment_next_rw (seg);
 35606|         assert (seg);
 35607|     }
 35608| #endif //USE_REGIONS
 35609|     dprintf (3, ("frag: %zd discounting pinned plugs", frag));
 35610|     size_t bos = 0;
 35611|     while (bos < mark_stack_bos)
 35612|     {
 35613|         frag += (pinned_len (pinned_plug_of (bos)));
 35614|         dprintf (3, ("adding pinned len %zd to frag ->%zd",
 35615|             pinned_len (pinned_plug_of (bos)), frag));
 35616|         bos++;
 35617|     }
 35618|     return frag;
 35619| }
 35620| size_t gc_heap::generation_sizes (generation* gen, bool use_saved_p)
 35621| {
 35622|     size_t result = 0;
 35623| #ifdef USE_REGIONS
 35624|     int gen_num = gen->gen_num;
 35625|     int start_gen_index = ((gen_num > max_generation) ? gen_num : 0);
 35626|     for (int i = start_gen_index; i <= gen_num; i++)
 35627|     {
 35628|         heap_segment* seg = heap_segment_in_range (generation_start_segment (generation_of (i)));
 35629|         while (seg)
 35630|         {
 35631|             uint8_t* end = (use_saved_p ?
 35632|                 heap_segment_saved_allocated (seg) : heap_segment_allocated (seg));
 35633|             result += end - heap_segment_mem (seg);
 35634|             dprintf (3, ("h%d gen%d size + %zd (%p - %p) -> %zd",
 35635|                 heap_number, i, (end - heap_segment_mem (seg)),
 35636|                 heap_segment_mem (seg), end, result));
 35637|             seg = heap_segment_next (seg);
 35638|         }
 35639|     }
 35640| #else //USE_REGIONS
 35641|     if (generation_start_segment (gen ) == ephemeral_heap_segment)
 35642|         result = (heap_segment_allocated (ephemeral_heap_segment) -
 35643|                   generation_allocation_start (gen));
 35644|     else
 35645|     {
 35646|         heap_segment* seg = heap_segment_in_range (generation_start_segment (gen));
 35647|         PREFIX_ASSUME(seg != NULL);
 35648|         while (seg)
 35649|         {
 35650|             result += (heap_segment_allocated (seg) -
 35651|                        heap_segment_mem (seg));
 35652|             seg = heap_segment_next_in_range (seg);
 35653|         }
 35654|     }
 35655| #endif //USE_REGIONS
 35656|     return result;
 35657| }
 35658| #ifdef USE_REGIONS
 35659| bool gc_heap::decide_on_compaction_space()
 35660| {
 35661|     size_t gen0size = approximate_new_allocation();
 35662|     dprintf (REGIONS_LOG, ("gen0size: %zd, free: %zd",
 35663|         gen0size, (num_regions_freed_in_sweep * ((size_t)1 << min_segment_size_shr))));
 35664|     if (sufficient_space_regions ((num_regions_freed_in_sweep * ((size_t)1 << min_segment_size_shr)),
 35665|                                   gen0size))
 35666|     {
 35667|         dprintf (REGIONS_LOG, ("it is sufficient!"));
 35668|         return false;
 35669|     }
 35670|     get_gen0_end_plan_space();
 35671|     if (!gen0_large_chunk_found)
 35672|     {
 35673|         gen0_large_chunk_found = (free_regions[basic_free_region].get_num_free_regions() > 0);
 35674|     }
 35675|     dprintf (REGIONS_LOG, ("gen0_pinned_free_space: %zd, end_gen0_region_space: %zd, gen0size: %zd",
 35676|             gen0_pinned_free_space, end_gen0_region_space, gen0size));
 35677|     if (sufficient_space_regions ((gen0_pinned_free_space + end_gen0_region_space), gen0size) &&
 35678|         gen0_large_chunk_found)
 35679|     {
 35680|         sufficient_gen0_space_p = TRUE;
 35681|     }
 35682|     return true;
 35683| }
 35684| #endif //USE_REGIONS
 35685| size_t gc_heap::estimated_reclaim (int gen_number)
 35686| {
 35687|     dynamic_data* dd = dynamic_data_of (gen_number);
 35688|     size_t gen_allocated = (dd_desired_allocation (dd) - dd_new_allocation (dd));
 35689|     size_t gen_total_size = gen_allocated + dd_current_size (dd);
 35690|     size_t est_gen_surv = (size_t)((float) (gen_total_size) * dd_surv (dd));
 35691|     size_t est_gen_free = gen_total_size - est_gen_surv + dd_fragmentation (dd);
 35692|     dprintf (GTC_LOG, ("h%d gen%d total size: %zd, est dead space: %zd (s: %d, allocated: %zd), frag: %zd",
 35693|                 heap_number, gen_number,
 35694|                 gen_total_size,
 35695|                 est_gen_free,
 35696|                 (int)(dd_surv (dd) * 100),
 35697|                 gen_allocated,
 35698|                 dd_fragmentation (dd)));
 35699|     return est_gen_free;
 35700| }
 35701| bool gc_heap::is_full_compacting_gc_productive()
 35702| {
 35703| #ifdef USE_REGIONS
 35704|     heap_segment* gen1_start_region = generation_start_segment (generation_of (max_generation - 1));
 35705|     if (heap_segment_plan_gen_num (gen1_start_region) == max_generation)
 35706|     {
 35707|         dprintf (REGIONS_LOG, ("gen1 start region %p is now part of gen2, unproductive",
 35708|             heap_segment_mem (gen1_start_region)));
 35709|         return false;
 35710|     }
 35711|     else
 35712|     {
 35713|         heap_segment* gen2_tail_region = generation_tail_region (generation_of (max_generation));
 35714|         if (heap_segment_plan_allocated (gen2_tail_region) >= heap_segment_allocated (gen2_tail_region))
 35715|         {
 35716|             dprintf (REGIONS_LOG, ("last gen2 region extended %p->%p, unproductive",
 35717|                 heap_segment_allocated (gen2_tail_region), heap_segment_plan_allocated (gen2_tail_region)));
 35718|             return false;
 35719|         }
 35720|     }
 35721|     return true;
 35722| #else //USE_REGIONS
 35723|     if (generation_plan_allocation_start (generation_of (max_generation - 1)) >=
 35724|         generation_allocation_start (generation_of (max_generation - 1)))
 35725|     {
 35726|         dprintf (1, ("gen1 start %p->%p, gen2 size %zd->%zd, lock elevation",
 35727|                 generation_allocation_start (generation_of (max_generation - 1)),
 35728|                 generation_plan_allocation_start (generation_of (max_generation - 1)),
 35729|                     generation_size (max_generation),
 35730|                     generation_plan_size (max_generation)));
 35731|         return false;
 35732|     }
 35733|     else
 35734|         return true;
 35735| #endif //USE_REGIONS
 35736| }
 35737| BOOL gc_heap::decide_on_compacting (int condemned_gen_number,
 35738|                                     size_t fragmentation,
 35739|                                     BOOL& should_expand)
 35740| {
 35741|     BOOL should_compact = FALSE;
 35742|     should_expand = FALSE;
 35743|     generation*   gen = generation_of (condemned_gen_number);
 35744|     dynamic_data* dd = dynamic_data_of (condemned_gen_number);
 35745|     size_t gen_sizes     = generation_sizes(gen, true);
 35746|     float  fragmentation_burden = ( ((0 == fragmentation) || (0 == gen_sizes)) ? (0.0f) :
 35747|                                     (float (fragmentation) / gen_sizes) );
 35748|     dprintf (GTC_LOG, ("h%d g%d fragmentation: %zd (%d%%), gen_sizes: %zd",
 35749|         heap_number, settings.condemned_generation,
 35750|         fragmentation, (int)(fragmentation_burden * 100.0),
 35751|         gen_sizes));
 35752| #ifdef USE_REGIONS
 35753|     if (special_sweep_p)
 35754|     {
 35755|         return FALSE;
 35756|     }
 35757| #endif //USE_REGIONS
 35758| #if defined(STRESS_HEAP) && !defined(FEATURE_NATIVEAOT)
 35759|     if (GCStress<cfg_any>::IsEnabled() && !settings.concurrent)
 35760|         should_compact = TRUE;
 35761| #endif //defined(STRESS_HEAP) && !defined(FEATURE_NATIVEAOT)
 35762|     if (GCConfig::GetForceCompact())
 35763|         should_compact = TRUE;
 35764|     if ((condemned_gen_number == max_generation) && last_gc_before_oom)
 35765|     {
 35766|         should_compact = TRUE;
 35767| #ifndef USE_REGIONS
 35768|         last_gc_before_oom = FALSE;
 35769| #endif //!USE_REGIONS
 35770|         get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_last_gc);
 35771|     }
 35772|     if (settings.reason == reason_induced_compacting)
 35773|     {
 35774|         dprintf (2, ("induced compacting GC"));
 35775|         should_compact = TRUE;
 35776|         get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_induced_compacting);
 35777|     }
 35778|     if (settings.reason == reason_induced_aggressive)
 35779|     {
 35780|         dprintf (2, ("aggressive compacting GC"));
 35781|         should_compact = TRUE;
 35782|         get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_aggressive_compacting);
 35783|     }
 35784|     if (settings.reason == reason_pm_full_gc)
 35785|     {
 35786|         assert (condemned_gen_number == max_generation);
 35787|         if (heap_number == 0)
 35788|         {
 35789|             dprintf (GTC_LOG, ("PM doing compacting full GC after a gen1"));
 35790|         }
 35791|         should_compact = TRUE;
 35792|     }
 35793|     dprintf (2, ("Fragmentation: %zu Fragmentation burden %d%%",
 35794|                 fragmentation, (int) (100*fragmentation_burden)));
 35795|     if (provisional_mode_triggered && (condemned_gen_number == (max_generation - 1)))
 35796|     {
 35797|         dprintf (GTC_LOG, ("gen1 in PM always compact"));
 35798|         should_compact = TRUE;
 35799|     }
 35800| #ifdef USE_REGIONS
 35801|     if (!should_compact)
 35802|     {
 35803|         should_compact = !!decide_on_compaction_space();
 35804|     }
 35805| #else //USE_REGIONS
 35806|     if (!should_compact)
 35807|     {
 35808|         if (dt_low_ephemeral_space_p (tuning_deciding_compaction))
 35809|         {
 35810|             dprintf(GTC_LOG, ("compacting due to low ephemeral"));
 35811|             should_compact = TRUE;
 35812|             get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_low_ephemeral);
 35813|         }
 35814|     }
 35815|     if (should_compact)
 35816|     {
 35817|         if ((condemned_gen_number >= (max_generation - 1)))
 35818|         {
 35819|             if (dt_low_ephemeral_space_p (tuning_deciding_expansion))
 35820|             {
 35821|                 dprintf (GTC_LOG,("Not enough space for all ephemeral generations with compaction"));
 35822|                 should_expand = TRUE;
 35823|             }
 35824|         }
 35825|     }
 35826| #endif //USE_REGIONS
 35827| #ifdef HOST_64BIT
 35828|     BOOL high_memory = FALSE;
 35829| #endif // HOST_64BIT
 35830|     if (!should_compact)
 35831|     {
 35832|         dprintf (REGIONS_LOG, ("frag: %zd, fragmentation_burden: %.3f",
 35833|             fragmentation, fragmentation_burden));
 35834|         BOOL frag_exceeded = ((fragmentation >= dd_fragmentation_limit (dd)) &&
 35835|                                 (fragmentation_burden >= dd_fragmentation_burden_limit (dd)));
 35836|         if (frag_exceeded)
 35837|         {
 35838| #ifdef BACKGROUND_GC
 35839|             IN_STRESS_HEAP(if (!settings.stress_induced))
 35840|             {
 35841| #endif // BACKGROUND_GC
 35842|             assert (settings.concurrent == FALSE);
 35843|             should_compact = TRUE;
 35844|             get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_high_frag);
 35845| #ifdef BACKGROUND_GC
 35846|             }
 35847| #endif // BACKGROUND_GC
 35848|         }
 35849| #ifdef HOST_64BIT
 35850|         if(!should_compact)
 35851|         {
 35852|             uint32_t num_heaps = 1;
 35853| #ifdef MULTIPLE_HEAPS
 35854|             num_heaps = gc_heap::n_heaps;
 35855| #endif // MULTIPLE_HEAPS
 35856|             ptrdiff_t reclaim_space = generation_size(max_generation) - generation_plan_size(max_generation);
 35857|             if((settings.entry_memory_load >= high_memory_load_th) && (settings.entry_memory_load < v_high_memory_load_th))
 35858|             {
 35859|                 if(reclaim_space > (int64_t)(min_high_fragmentation_threshold (entry_available_physical_mem, num_heaps)))
 35860|                 {
 35861|                     dprintf(GTC_LOG,("compacting due to fragmentation in high memory"));
 35862|                     should_compact = TRUE;
 35863|                     get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_high_mem_frag);
 35864|                 }
 35865|                 high_memory = TRUE;
 35866|             }
 35867|             else if(settings.entry_memory_load >= v_high_memory_load_th)
 35868|             {
 35869|                 if(reclaim_space > (ptrdiff_t)(min_reclaim_fragmentation_threshold (num_heaps)))
 35870|                 {
 35871|                     dprintf(GTC_LOG,("compacting due to fragmentation in very high memory"));
 35872|                     should_compact = TRUE;
 35873|                     get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_vhigh_mem_frag);
 35874|                 }
 35875|                 high_memory = TRUE;
 35876|             }
 35877|         }
 35878| #endif // HOST_64BIT
 35879|     }
 35880|     if ((should_compact == FALSE) &&
 35881|         (ensure_gap_allocation (condemned_gen_number) == FALSE))
 35882|     {
 35883|         should_compact = TRUE;
 35884|         get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_no_gaps);
 35885|     }
 35886|     if (settings.condemned_generation == max_generation)
 35887|     {
 35888|         if (
 35889| #ifdef HOST_64BIT
 35890|             (high_memory && !should_compact) ||
 35891| #endif // HOST_64BIT
 35892|             !is_full_compacting_gc_productive())
 35893|         {
 35894|             settings.should_lock_elevation = TRUE;
 35895|         }
 35896|     }
 35897|     if (settings.pause_mode == pause_no_gc)
 35898|     {
 35899|         should_compact = TRUE;
 35900|         if ((size_t)(heap_segment_reserved (ephemeral_heap_segment) - heap_segment_plan_allocated (ephemeral_heap_segment))
 35901|             < soh_allocation_no_gc)
 35902|         {
 35903|             should_expand = TRUE;
 35904|         }
 35905|     }
 35906|     dprintf (2, ("will %s(%s)", (should_compact ? "compact" : "sweep"), (should_expand ? "ex" : "")));
 35907|     return should_compact;
 35908| }
 35909| size_t align_lower_good_size_allocation (size_t size)
 35910| {
 35911|     return (size/64)*64;
 35912| }
 35913| size_t gc_heap::approximate_new_allocation()
 35914| {
 35915|     dynamic_data* dd0 = dynamic_data_of (0);
 35916|     return max (2*dd_min_size (dd0), ((dd_desired_allocation (dd0)*2)/3));
 35917| }
 35918| bool gc_heap::check_against_hard_limit (size_t space_required)
 35919| {
 35920|     bool can_fit = TRUE;
 35921|     if (heap_hard_limit)
 35922|     {
 35923|         size_t left_in_commit = heap_hard_limit - current_total_committed;
 35924|         int num_heaps = get_num_heaps();
 35925|         left_in_commit /= num_heaps;
 35926|         if (left_in_commit < space_required)
 35927|         {
 35928|             can_fit = FALSE;
 35929|         }
 35930|         dprintf (2, ("h%d end seg %zd, but only %zd left in HARD LIMIT commit, required: %zd %s on eph",
 35931|             heap_number, space_required,
 35932|             left_in_commit, space_required,
 35933|             (can_fit ? "ok" : "short")));
 35934|     }
 35935|     return can_fit;
 35936| }
 35937| #ifdef USE_REGIONS
 35938| bool gc_heap::sufficient_space_regions_for_allocation (size_t end_space, size_t end_space_required)
 35939| {
 35940|     size_t free_regions_space = (free_regions[basic_free_region].get_num_free_regions() * ((size_t)1 << min_segment_size_shr)) +
 35941|                                 global_region_allocator.get_free();
 35942|     size_t total_alloc_space = end_space + free_regions_space;
 35943|     dprintf (REGIONS_LOG, ("h%d required %zd, end %zd + free %zd=%zd",
 35944|         heap_number, end_space_required, end_space, free_regions_space, total_alloc_space));
 35945|     size_t total_commit_space = end_gen0_region_committed_space + free_regions[basic_free_region].get_size_committed_in_free();
 35946|     if (total_alloc_space > end_space_required)
 35947|     {
 35948|         if (end_space_required > total_commit_space)
 35949|         {
 35950|             return check_against_hard_limit (end_space_required - total_commit_space);
 35951|         }
 35952|         else
 35953|         {
 35954|             return true;
 35955|         }
 35956|     }
 35957|     else
 35958|     {
 35959|         return false;
 35960|     }
 35961| }
 35962| bool gc_heap::sufficient_space_regions (size_t end_space, size_t end_space_required)
 35963| {
 35964|     size_t free_regions_space = (free_regions[basic_free_region].get_num_free_regions() * ((size_t)1 << min_segment_size_shr)) +
 35965|                                 global_region_allocator.get_free();
 35966|     size_t total_alloc_space = end_space + free_regions_space;
 35967|     dprintf (REGIONS_LOG, ("h%d required %zd, end %zd + free %zd=%zd",
 35968|         heap_number, end_space_required, end_space, free_regions_space, total_alloc_space));
 35969|     if (total_alloc_space > end_space_required)
 35970|     {
 35971|         return check_against_hard_limit (end_space_required);
 35972|     }
 35973|     else
 35974|     {
 35975|         return false;
 35976|     }
 35977| }
 35978| #else //USE_REGIONS
 35979| BOOL gc_heap::sufficient_space_end_seg (uint8_t* start, uint8_t* committed, uint8_t* reserved, size_t end_space_required)
 35980| {
 35981|     BOOL can_fit = FALSE;
 35982|     size_t committed_space = (size_t)(committed - start);
 35983|     size_t end_seg_space = (size_t)(reserved - start);
 35984|     if (committed_space > end_space_required)
 35985|     {
 35986|         return true;
 35987|     }
 35988|     else if (end_seg_space > end_space_required)
 35989|     {
 35990|         return check_against_hard_limit (end_space_required - committed_space);
 35991|     }
 35992|     else
 35993|         return false;
 35994| }
 35995| #endif //USE_REGIONS
 35996| size_t gc_heap::end_space_after_gc()
 35997| {
 35998|     return max ((dd_min_size (dynamic_data_of (0))/2), (END_SPACE_AFTER_GC_FL));
 35999| }
 36000| BOOL gc_heap::ephemeral_gen_fit_p (gc_tuning_point tp)
 36001| {
 36002|     uint8_t* start = 0;
 36003| #ifdef USE_REGIONS
 36004|     assert ((tp == tuning_deciding_condemned_gen) || (tp == tuning_deciding_full_gc));
 36005| #else//USE_REGIONS
 36006|     if ((tp == tuning_deciding_condemned_gen) ||
 36007|         (tp == tuning_deciding_compaction))
 36008|     {
 36009|         start = (settings.concurrent ? alloc_allocated : heap_segment_allocated (ephemeral_heap_segment));
 36010|         if (settings.concurrent)
 36011|         {
 36012|             dprintf (2, ("%zd left at the end of ephemeral segment (alloc_allocated)",
 36013|                 (size_t)(heap_segment_reserved (ephemeral_heap_segment) - alloc_allocated)));
 36014|         }
 36015|         else
 36016|         {
 36017|             dprintf (2, ("%zd left at the end of ephemeral segment (allocated)",
 36018|                 (size_t)(heap_segment_reserved (ephemeral_heap_segment) - heap_segment_allocated (ephemeral_heap_segment))));
 36019|         }
 36020|     }
 36021|     else if (tp == tuning_deciding_expansion)
 36022|     {
 36023|         start = heap_segment_plan_allocated (ephemeral_heap_segment);
 36024|         dprintf (2, ("%zd left at the end of ephemeral segment based on plan",
 36025|             (size_t)(heap_segment_reserved (ephemeral_heap_segment) - start)));
 36026|     }
 36027|     else
 36028|     {
 36029|         assert (tp == tuning_deciding_full_gc);
 36030|         dprintf (2, ("FGC: %zd left at the end of ephemeral segment (alloc_allocated)",
 36031|             (size_t)(heap_segment_reserved (ephemeral_heap_segment) - alloc_allocated)));
 36032|         start = alloc_allocated;
 36033|     }
 36034|     if (start == 0) // empty ephemeral generations
 36035|     {
 36036|         assert (tp == tuning_deciding_expansion);
 36037|         start = generation_allocation_pointer (generation_of (max_generation));
 36038|         assert (start == heap_segment_mem (ephemeral_heap_segment));
 36039|     }
 36040|     if (tp == tuning_deciding_expansion)
 36041|     {
 36042|         assert (settings.condemned_generation >= (max_generation-1));
 36043|         size_t gen0size = approximate_new_allocation();
 36044|         size_t eph_size = gen0size;
 36045|         size_t gen_min_sizes = 0;
 36046|         for (int j = 1; j <= max_generation-1; j++)
 36047|         {
 36048|             gen_min_sizes += 2*dd_min_size (dynamic_data_of(j));
 36049|         }
 36050|         eph_size += gen_min_sizes;
 36051|         dprintf (3, ("h%d deciding on expansion, need %zd (gen0: %zd, 2*min: %zd)",
 36052|             heap_number, gen0size, gen_min_sizes, eph_size));
 36053|         if ((size_t)(heap_segment_reserved (ephemeral_heap_segment) - start) > eph_size)
 36054|         {
 36055|             dprintf (3, ("Enough room before end of segment"));
 36056|             return TRUE;
 36057|         }
 36058|         else
 36059|         {
 36060|             size_t room = align_lower_good_size_allocation
 36061|                 (heap_segment_reserved (ephemeral_heap_segment) - start);
 36062|             size_t end_seg = room;
 36063|             size_t largest_alloc = END_SPACE_AFTER_GC_FL;
 36064|             bool large_chunk_found = FALSE;
 36065|             size_t bos = 0;
 36066|             uint8_t* gen0start = generation_plan_allocation_start (youngest_generation);
 36067|             dprintf (3, ("ephemeral_gen_fit_p: gen0 plan start: %zx", (size_t)gen0start));
 36068|             if (gen0start == 0)
 36069|                 return FALSE;
 36070|             dprintf (3, ("ephemeral_gen_fit_p: room before free list search %zd, needed: %zd",
 36071|                          room, gen0size));
 36072|             while ((bos < mark_stack_bos) &&
 36073|                    !((room >= gen0size) && large_chunk_found))
 36074|             {
 36075|                 uint8_t* plug = pinned_plug (pinned_plug_of (bos));
 36076|                 if (in_range_for_segment (plug, ephemeral_heap_segment))
 36077|                 {
 36078|                     if (plug >= gen0start)
 36079|                     {
 36080|                         size_t chunk = align_lower_good_size_allocation (pinned_len (pinned_plug_of (bos)));
 36081|                         room += chunk;
 36082|                         if (!large_chunk_found)
 36083|                         {
 36084|                             large_chunk_found = (chunk >= largest_alloc);
 36085|                         }
 36086|                         dprintf (3, ("ephemeral_gen_fit_p: room now %zd, large chunk: %d",
 36087|                                      room, large_chunk_found));
 36088|                     }
 36089|                 }
 36090|                 bos++;
 36091|             }
 36092|             if (room >= gen0size)
 36093|             {
 36094|                 if (large_chunk_found)
 36095|                 {
 36096|                     sufficient_gen0_space_p = TRUE;
 36097|                     dprintf (3, ("Enough room"));
 36098|                     return TRUE;
 36099|                 }
 36100|                 else
 36101|                 {
 36102|                     if (end_seg >= end_space_after_gc())
 36103|                     {
 36104|                         dprintf (3, ("Enough room (may need end of seg)"));
 36105|                         return TRUE;
 36106|                     }
 36107|                 }
 36108|             }
 36109|             dprintf (3, ("Not enough room"));
 36110|                 return FALSE;
 36111|         }
 36112|     }
 36113|     else
 36114| #endif //USE_REGIONS
 36115|     {
 36116|         size_t end_space = 0;
 36117|         dynamic_data* dd = dynamic_data_of (0);
 36118|         if ((tp == tuning_deciding_condemned_gen) ||
 36119|             (tp == tuning_deciding_full_gc))
 36120|         {
 36121|             end_space = max (2*dd_min_size (dd), end_space_after_gc());
 36122|         }
 36123|         else
 36124|         {
 36125|             assert (tp == tuning_deciding_compaction);
 36126|             end_space = approximate_new_allocation();
 36127|         }
 36128| #ifdef USE_REGIONS
 36129|         size_t gen0_end_space = get_gen0_end_space (memory_type_reserved);
 36130|         BOOL can_fit = sufficient_space_regions (gen0_end_space, end_space);
 36131| #else //USE_REGIONS
 36132|         BOOL can_fit = sufficient_space_end_seg (start, heap_segment_committed (ephemeral_heap_segment), heap_segment_reserved (ephemeral_heap_segment), end_space);
 36133| #endif //USE_REGIONS
 36134|         return can_fit;
 36135|     }
 36136| }
 36137| CObjectHeader* gc_heap::allocate_uoh_object (size_t jsize, uint32_t flags, int gen_number, int64_t& alloc_bytes)
 36138| {
 36139|     alloc_context acontext;
 36140|     acontext.init();
 36141| #if HOST_64BIT
 36142|     size_t maxObjectSize = (INT64_MAX - 7 - Align(min_obj_size));
 36143| #else
 36144|     size_t maxObjectSize = (INT32_MAX - 7 - Align(min_obj_size));
 36145| #endif
 36146|     if (jsize >= maxObjectSize)
 36147|     {
 36148|         if (GCConfig::GetBreakOnOOM())
 36149|         {
 36150|             GCToOSInterface::DebugBreak();
 36151|         }
 36152|         return NULL;
 36153|     }
 36154|     size_t size = AlignQword (jsize);
 36155|     int align_const = get_alignment_constant (FALSE);
 36156|     size_t pad = 0;
 36157| #ifdef FEATURE_LOH_COMPACTION
 36158|     if (gen_number == loh_generation)
 36159|     {
 36160|         pad = Align (loh_padding_obj_size, align_const);
 36161|     }
 36162| #endif //FEATURE_LOH_COMPACTION
 36163|     assert (size >= Align (min_obj_size, align_const));
 36164| #ifdef _MSC_VER
 36165| #pragma inline_depth(0)
 36166| #endif //_MSC_VER
 36167|     if (! allocate_more_space (&acontext, (size + pad), flags, gen_number))
 36168|     {
 36169|         return 0;
 36170|     }
 36171| #ifdef _MSC_VER
 36172| #pragma inline_depth(20)
 36173| #endif //_MSC_VER
 36174| #ifdef FEATURE_LOH_COMPACTION
 36175| #endif //FEATURE_LOH_COMPACTION
 36176|     uint8_t*  result = acontext.alloc_ptr;
 36177|     assert ((size_t)(acontext.alloc_limit - acontext.alloc_ptr) == size);
 36178|     alloc_bytes += size;
 36179|     CObjectHeader* obj = (CObjectHeader*)result;
 36180|     assert (obj != 0);
 36181|     assert ((size_t)obj == Align ((size_t)obj, align_const));
 36182|     return obj;
 36183| }
 36184| void gc_heap::reset_memory (uint8_t* o, size_t sizeo)
 36185| {
 36186|     if (gc_heap::use_large_pages_p)
 36187|         return;
 36188|     if (sizeo > 128 * 1024)
 36189|     {
 36190|         size_t size_to_skip = min_free_list - plug_skew;
 36191|         size_t page_start = align_on_page ((size_t)(o + size_to_skip));
 36192|         size_t size = align_lower_page ((size_t)o + sizeo - size_to_skip - plug_skew) - page_start;
 36193|         if (reset_mm_p && gc_heap::dt_high_memory_load_p())
 36194|         {
 36195| #ifdef MULTIPLE_HEAPS
 36196|             bool unlock_p = true;
 36197| #else
 36198|             bool unlock_p = false;
 36199| #endif //MULTIPLE_HEAPS
 36200|             reset_mm_p = GCToOSInterface::VirtualReset((void*)page_start, size, unlock_p);
 36201|         }
 36202|     }
 36203| }
 36204| BOOL gc_heap::uoh_object_marked (uint8_t* o, BOOL clearp)
 36205| {
 36206|     BOOL m = FALSE;
 36207|     if ((o >= lowest_address) && (o < highest_address))
 36208|     {
 36209|         if (marked (o))
 36210|         {
 36211|             if (clearp)
 36212|             {
 36213|                 clear_marked (o);
 36214|                 if (pinned (o))
 36215|                     clear_pinned(o);
 36216|             }
 36217|             m = TRUE;
 36218|         }
 36219|         else
 36220|             m = FALSE;
 36221|     }
 36222|     else
 36223|         m = TRUE;
 36224|     return m;
 36225| }
 36226| void gc_heap::walk_survivors_relocation (void* profiling_context, record_surv_fn fn)
 36227| {
 36228|     walk_relocation (profiling_context, fn);
 36229| #ifdef FEATURE_LOH_COMPACTION
 36230|     if (loh_compacted_p)
 36231|     {
 36232|         walk_relocation_for_loh (profiling_context, fn);
 36233|     }
 36234| #endif //FEATURE_LOH_COMPACTION
 36235| }
 36236| void gc_heap::walk_survivors_for_uoh (void* profiling_context, record_surv_fn fn, int gen_number)
 36237| {
 36238|     generation* gen        = generation_of (gen_number);
 36239|     heap_segment* seg      = heap_segment_rw (generation_start_segment (gen));;
 36240|     PREFIX_ASSUME(seg != NULL);
 36241|     uint8_t* o                = get_uoh_start_object (seg, gen);
 36242|     uint8_t* plug_end         = o;
 36243|     uint8_t* plug_start       = o;
 36244|     while (1)
 36245|     {
 36246|         if (o >= heap_segment_allocated (seg))
 36247|         {
 36248|             seg = heap_segment_next (seg);
 36249|             if (seg == 0)
 36250|                 break;
 36251|             else
 36252|                 o = heap_segment_mem (seg);
 36253|         }
 36254|         if (uoh_object_marked(o, FALSE))
 36255|         {
 36256|             plug_start = o;
 36257|             BOOL m = TRUE;
 36258|             while (m)
 36259|             {
 36260|                 o = o + AlignQword (size (o));
 36261|                 if (o >= heap_segment_allocated (seg))
 36262|                 {
 36263|                     break;
 36264|                 }
 36265|                 m = uoh_object_marked (o, FALSE);
 36266|             }
 36267|             plug_end = o;
 36268|             fn (plug_start, plug_end, 0, profiling_context, false, false);
 36269|         }
 36270|         else
 36271|         {
 36272|             while (o < heap_segment_allocated (seg) && !uoh_object_marked(o, FALSE))
 36273|             {
 36274|                 o = o + AlignQword (size (o));
 36275|             }
 36276|         }
 36277|     }
 36278| }
 36279| #ifdef BACKGROUND_GC
 36280| BOOL gc_heap::background_object_marked (uint8_t* o, BOOL clearp)
 36281| {
 36282|     BOOL m = FALSE;
 36283|     if ((o >= background_saved_lowest_address) && (o < background_saved_highest_address))
 36284|     {
 36285|         if (mark_array_marked (o))
 36286|         {
 36287|             if (clearp)
 36288|             {
 36289|                 mark_array_clear_marked (o);
 36290|                 dprintf (3, ("CM: %p", o));
 36291|             }
 36292|             m = TRUE;
 36293|         }
 36294|         else
 36295|             m = FALSE;
 36296|     }
 36297|     else
 36298|         m = TRUE;
 36299|     dprintf (3, ("o %p(%zu) %s", o, size(o), (m ? "was bm" : "was NOT bm")));
 36300|     return m;
 36301| }
 36302| void gc_heap::background_delay_delete_uoh_segments()
 36303| {
 36304|     for (int i = uoh_start_generation; i < total_generation_count; i++)
 36305|     {
 36306|         generation* gen = generation_of (i);
 36307|         heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 36308|         heap_segment* prev_seg = 0;
 36309| #ifdef USE_REGIONS
 36310|         heap_segment* first_remaining_region = 0;
 36311| #endif //USE_REGIONS
 36312|         while (seg)
 36313|         {
 36314|             heap_segment* next_seg = heap_segment_next (seg);
 36315|             if (seg->flags & heap_segment_flags_uoh_delete)
 36316|             {
 36317|                 dprintf (3, ("deleting %zx-%p-%p", (size_t)seg, heap_segment_allocated (seg), heap_segment_reserved (seg)));
 36318|                 delete_heap_segment (seg, (GCConfig::GetRetainVM() != 0));
 36319|                 heap_segment_next (prev_seg) = next_seg;
 36320| #ifdef USE_REGIONS
 36321|                 update_start_tail_regions (gen, seg, prev_seg, next_seg);
 36322| #endif //USE_REGIONS
 36323|             }
 36324|             else
 36325|             {
 36326| #ifdef USE_REGIONS
 36327|                 if (!first_remaining_region)
 36328|                     first_remaining_region = seg;
 36329| #endif //USE_REGIONS
 36330|                 prev_seg = seg;
 36331|             }
 36332|             seg = next_seg;
 36333|         }
 36334| #ifdef USE_REGIONS
 36335|         assert (heap_segment_rw (generation_start_segment (gen)) == generation_start_segment (gen));
 36336|         if (generation_start_segment (gen) != first_remaining_region)
 36337|         {
 36338|             dprintf (REGIONS_LOG, ("h%d gen%d start %p -> %p",
 36339|                 heap_number, gen->gen_num,
 36340|                 heap_segment_mem (generation_start_segment (gen)),
 36341|                 heap_segment_mem (first_remaining_region)));
 36342|             generation_start_segment (gen) = first_remaining_region;
 36343|         }
 36344|         if (generation_tail_region (gen) != prev_seg)
 36345|         {
 36346|             dprintf (REGIONS_LOG, ("h%d gen%d start %p -> %p",
 36347|                 heap_number, gen->gen_num,
 36348|                 heap_segment_mem (generation_tail_region (gen)),
 36349|                 heap_segment_mem (prev_seg)));
 36350|             generation_tail_region (gen) = prev_seg;
 36351|         }
 36352| #endif //USE_REGIONS
 36353|     }
 36354| }
 36355| uint8_t* gc_heap::background_next_end (heap_segment* seg, BOOL uoh_objects_p)
 36356| {
 36357|     return
 36358|         (uoh_objects_p ? heap_segment_allocated (seg) : heap_segment_background_allocated (seg));
 36359| }
 36360| void gc_heap::set_mem_verify (uint8_t* start, uint8_t* end, uint8_t b)
 36361| {
 36362| #ifdef VERIFY_HEAP
 36363|     if (end > start)
 36364|     {
 36365|         if ((GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC) &&
 36366|            !(GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_NO_MEM_FILL))
 36367|         {
 36368|             dprintf (3, ("setting mem to %c [%p, [%p", b, start, end));
 36369|             memset (start, b, (end - start));
 36370|         }
 36371|     }
 36372| #endif //VERIFY_HEAP
 36373| }
 36374| void gc_heap::generation_delete_heap_segment (generation* gen,
 36375|                                               heap_segment* seg,
 36376|                                               heap_segment* prev_seg,
 36377|                                               heap_segment* next_seg)
 36378| {
 36379|     dprintf (3, ("bgc sweep: deleting seg %zx(%p), next %zx(%p), prev %zx(%p)",
 36380|         (size_t)seg, heap_segment_mem (seg),
 36381|         (size_t)next_seg, (next_seg ? heap_segment_mem (next_seg) : 0),
 36382|         (size_t)prev_seg, (prev_seg ? heap_segment_mem (prev_seg) : 0)));
 36383|     if (gen->gen_num > max_generation)
 36384|     {
 36385|         dprintf (3, ("Preparing empty large segment %zx for deletion", (size_t)seg));
 36386|         seg->flags |= heap_segment_flags_uoh_delete;
 36387|         heap_segment_allocated (seg) = heap_segment_mem (seg);
 36388|     }
 36389|     else
 36390|     {
 36391|         assert (seg != ephemeral_heap_segment);
 36392| #ifdef DOUBLY_LINKED_FL
 36393|         heap_segment_next (prev_seg) = next_seg;
 36394| #else //DOUBLY_LINKED_FL
 36395|         heap_segment_next (next_seg) = prev_seg;
 36396| #endif //DOUBLY_LINKED_FL
 36397|         dprintf (3, ("Preparing empty small segment %zx for deletion", (size_t)seg));
 36398|         heap_segment_next (seg) = freeable_soh_segment;
 36399|         freeable_soh_segment = seg;
 36400| #ifdef USE_REGIONS
 36401| #ifdef DOUBLY_LINKED_FL
 36402|         heap_segment* next_region = next_seg;
 36403|         heap_segment* prev_region = prev_seg;
 36404| #else //DOUBLY_LINKED_FL
 36405|         heap_segment* next_region = prev_seg;
 36406|         heap_segment* prev_region = next_seg;
 36407| #endif //DOUBLY_LINKED_FL
 36408|         update_start_tail_regions (gen, seg, prev_region, next_region);
 36409| #endif //USE_REGIONS
 36410|     }
 36411|     decommit_heap_segment (seg);
 36412|     seg->flags |= heap_segment_flags_decommitted;
 36413|     set_mem_verify (heap_segment_allocated (seg) - plug_skew, heap_segment_used (seg), 0xbb);
 36414| }
 36415| void gc_heap::process_background_segment_end (heap_segment* seg,
 36416|                                               generation* gen,
 36417|                                               uint8_t* last_plug_end,
 36418|                                               heap_segment* start_seg,
 36419|                                               BOOL* delete_p,
 36420|                                               size_t free_obj_size_last_gap)
 36421| {
 36422|     *delete_p = FALSE;
 36423|     uint8_t* allocated = heap_segment_allocated (seg);
 36424|     uint8_t* background_allocated = heap_segment_background_allocated (seg);
 36425|     BOOL uoh_p = heap_segment_uoh_p (seg);
 36426|     dprintf (3, ("EoS [%zx, %p[(%p[), last: %p(%zu)",
 36427|                 (size_t)heap_segment_mem (seg), background_allocated, allocated, last_plug_end, free_obj_size_last_gap));
 36428|     if (!uoh_p && (allocated != background_allocated))
 36429|     {
 36430|         assert (gen->gen_num <= max_generation);
 36431|         dprintf (3, ("Make a free object before newly promoted objects [%zx, %p[",
 36432|                     (size_t)last_plug_end, background_allocated));
 36433|         size_t last_gap = background_allocated - last_plug_end;
 36434|         if (last_gap > 0)
 36435|         {
 36436|             thread_gap (last_plug_end, last_gap, generation_of (max_generation));
 36437|             add_gen_free (max_generation, last_gap);
 36438|             fix_brick_to_highest (last_plug_end, background_allocated);
 36439|             fix_brick_to_highest (background_allocated, background_allocated);
 36440|         }
 36441|     }
 36442|     else
 36443|     {
 36444|         if (seg == ephemeral_heap_segment)
 36445|         {
 36446|             FATAL_GC_ERROR();
 36447|         }
 36448| #ifndef USE_REGIONS
 36449|         if (allocated == heap_segment_mem (seg))
 36450|         {
 36451|             assert (gen->gen_num > max_generation);
 36452|         }
 36453| #endif //!USE_REGIONS
 36454|         if (last_plug_end == heap_segment_mem (seg))
 36455|         {
 36456|             if (seg != start_seg)
 36457|             {
 36458|                 *delete_p = TRUE;
 36459|             }
 36460|             dprintf (3, ("h%d seg %p %s be deleted", heap_number,
 36461|                         heap_segment_mem (seg), (*delete_p ? "should" : "should not")));
 36462|         }
 36463|         if (!*delete_p)
 36464|         {
 36465|             dprintf (3, ("[h%d] seg %zx alloc %p->%zx",
 36466|                 heap_number, (size_t)seg,
 36467|                 heap_segment_allocated (seg),
 36468|                 (size_t)last_plug_end));
 36469|             heap_segment_allocated (seg) = last_plug_end;
 36470|             set_mem_verify (heap_segment_allocated (seg) - plug_skew, heap_segment_used (seg), 0xbb);
 36471|             decommit_heap_segment_pages (seg, 0);
 36472|         }
 36473|     }
 36474|     if (free_obj_size_last_gap)
 36475|     {
 36476|         generation_free_obj_space (gen) -= free_obj_size_last_gap;
 36477|         dprintf (2, ("[h%d] PS: gen2FO-: %zd->%zd",
 36478|             heap_number, free_obj_size_last_gap, generation_free_obj_space (gen)));
 36479|     }
 36480|     dprintf (3, ("verifying seg %p's mark array was completely cleared", seg));
 36481|     bgc_verify_mark_array_cleared (seg);
 36482| }
 36483| inline
 36484| BOOL gc_heap::fgc_should_consider_object (uint8_t* o,
 36485|                                           heap_segment* seg,
 36486|                                           BOOL consider_bgc_mark_p,
 36487|                                           BOOL check_current_sweep_p,
 36488|                                           BOOL check_saved_sweep_p)
 36489| {
 36490| #ifdef USE_REGIONS
 36491|     assert (!check_saved_sweep_p);
 36492| #endif //USE_REGIONS
 36493|     BOOL no_bgc_mark_p = FALSE;
 36494|     if (consider_bgc_mark_p)
 36495|     {
 36496|         if (check_current_sweep_p && (o < current_sweep_pos))
 36497|         {
 36498|             dprintf (3, ("no bgc mark - o: %p < cs: %p", o, current_sweep_pos));
 36499|             no_bgc_mark_p = TRUE;
 36500|         }
 36501|         if (!no_bgc_mark_p)
 36502|         {
 36503| #ifndef USE_REGIONS
 36504|             if(check_saved_sweep_p && (o >= saved_sweep_ephemeral_start))
 36505|             {
 36506|                 dprintf (3, ("no bgc mark - o: %p >= ss: %p", o, saved_sweep_ephemeral_start));
 36507|                 no_bgc_mark_p = TRUE;
 36508|             }
 36509| #endif //!USE_REGIONS
 36510|             if (!check_saved_sweep_p)
 36511|             {
 36512|                 uint8_t* background_allocated = heap_segment_background_allocated (seg);
 36513| #ifndef USE_REGIONS
 36514|                 assert (heap_segment_background_allocated (seg) != saved_sweep_ephemeral_start);
 36515| #endif //!USE_REGIONS
 36516|                 if (o >= background_allocated)
 36517|                 {
 36518|                     dprintf (3, ("no bgc mark - o: %p >= ba: %p", o, background_allocated));
 36519|                     no_bgc_mark_p = TRUE;
 36520|                 }
 36521|             }
 36522|         }
 36523|     }
 36524|     else
 36525|     {
 36526|         no_bgc_mark_p = TRUE;
 36527|     }
 36528|     dprintf (3, ("bgc mark %p: %s (bm: %s)", o, (no_bgc_mark_p ? "no" : "yes"), ((no_bgc_mark_p || background_object_marked (o, FALSE)) ? "yes" : "no")));
 36529|     return (no_bgc_mark_p ? TRUE : background_object_marked (o, FALSE));
 36530| }
 36531| void gc_heap::should_check_bgc_mark (heap_segment* seg,
 36532|                                      BOOL* consider_bgc_mark_p,
 36533|                                      BOOL* check_current_sweep_p,
 36534|                                      BOOL* check_saved_sweep_p)
 36535| {
 36536|     *consider_bgc_mark_p = FALSE;
 36537|     *check_current_sweep_p = FALSE;
 36538|     *check_saved_sweep_p = FALSE;
 36539|     if (current_c_gc_state == c_gc_state_planning)
 36540|     {
 36541|         if ((seg->flags & heap_segment_flags_swept) || (current_sweep_pos == heap_segment_reserved (seg)))
 36542|         {
 36543|             dprintf (3, ("seg %p is already swept by bgc", seg));
 36544|         }
 36545|         else if (heap_segment_background_allocated (seg) == 0)
 36546|         {
 36547|             dprintf (3, ("seg %p newly alloc during bgc", seg));
 36548|         }
 36549|         else
 36550|         {
 36551|             *consider_bgc_mark_p = TRUE;
 36552|             dprintf (3, ("seg %p hasn't been swept by bgc", seg));
 36553| #ifndef USE_REGIONS
 36554|             if (seg == saved_sweep_ephemeral_seg)
 36555|             {
 36556|                 dprintf (3, ("seg %p is the saved ephemeral seg", seg));
 36557|                 *check_saved_sweep_p = TRUE;
 36558|             }
 36559| #endif //!USE_REGIONS
 36560|             if (in_range_for_segment (current_sweep_pos, seg))
 36561|             {
 36562|                 dprintf (3, ("current sweep pos is %p and within seg %p",
 36563|                               current_sweep_pos, seg));
 36564|                 *check_current_sweep_p = TRUE;
 36565|             }
 36566|         }
 36567|     }
 36568| }
 36569| void gc_heap::background_ephemeral_sweep()
 36570| {
 36571|     dprintf (3, ("bgc ephemeral sweep"));
 36572|     int align_const = get_alignment_constant (TRUE);
 36573| #ifndef USE_REGIONS
 36574|     saved_sweep_ephemeral_seg = ephemeral_heap_segment;
 36575|     saved_sweep_ephemeral_start = generation_allocation_start (generation_of (max_generation - 1));
 36576| #endif //!USE_REGIONS
 36577|     allocator youngest_free_list;
 36578|     size_t youngest_free_list_space = 0;
 36579|     size_t youngest_free_obj_space = 0;
 36580|     youngest_free_list.clear();
 36581|     for (int i = 0; i <= (max_generation - 1); i++)
 36582|     {
 36583|         generation* gen_to_reset = generation_of (i);
 36584|         assert (generation_free_list_space (gen_to_reset) == 0);
 36585|     }
 36586|     for (int i = (max_generation - 1); i >= 0; i--)
 36587|     {
 36588|         generation* current_gen = generation_of (i);
 36589| #ifdef USE_REGIONS
 36590|         heap_segment* ephemeral_region = heap_segment_rw (generation_start_segment (current_gen));
 36591|         while (ephemeral_region)
 36592| #endif //USE_REGIONS
 36593|         {
 36594| #ifdef USE_REGIONS
 36595|             uint8_t* o = heap_segment_mem (ephemeral_region);
 36596|             uint8_t* end = heap_segment_background_allocated (ephemeral_region);
 36597|             dprintf (3, ("bgc eph: gen%d seg %p(%p-%p)",
 36598|                 heap_segment_gen_num (ephemeral_region),
 36599|                 heap_segment_mem (ephemeral_region),
 36600|                 heap_segment_allocated (ephemeral_region),
 36601|                 heap_segment_background_allocated (ephemeral_region)));
 36602|             if (!end)
 36603|             {
 36604|                 ephemeral_region->flags |= heap_segment_flags_swept;
 36605|                 ephemeral_region = heap_segment_next (ephemeral_region);
 36606|                 continue;
 36607|             }
 36608| #else //USE_REGIONS
 36609|             uint8_t* o = generation_allocation_start (current_gen);
 36610|             o = o + Align(size (o), align_const);
 36611|             uint8_t* end = ((i > 0) ?
 36612|                         generation_allocation_start (generation_of (i - 1)) :
 36613|                         heap_segment_allocated (ephemeral_heap_segment));
 36614| #endif //USE_REGIONS
 36615|             uint8_t* plug_end = o;
 36616|             uint8_t* plug_start = o;
 36617|             BOOL marked_p = FALSE;
 36618|             while (o < end)
 36619|             {
 36620|                 marked_p = background_object_marked (o, TRUE);
 36621|                 if (marked_p)
 36622|                 {
 36623|                     plug_start = o;
 36624|                     size_t plug_size = plug_start - plug_end;
 36625|                     if (i >= 1)
 36626|                     {
 36627|                         thread_gap (plug_end, plug_size, current_gen);
 36628|                     }
 36629|                     else
 36630|                     {
 36631|                         if (plug_size > 0)
 36632|                         {
 36633|                             make_unused_array (plug_end, plug_size);
 36634|                             if (plug_size >= min_free_list)
 36635|                             {
 36636|                                 youngest_free_list_space += plug_size;
 36637|                                 youngest_free_list.thread_item (plug_end, plug_size);
 36638|                             }
 36639|                             else
 36640|                             {
 36641|                                 youngest_free_obj_space += plug_size;
 36642|                             }
 36643|                         }
 36644|                     }
 36645|                     fix_brick_to_highest (plug_end, plug_start);
 36646|                     fix_brick_to_highest (plug_start, plug_start);
 36647|                     BOOL m = TRUE;
 36648|                     while (m)
 36649|                     {
 36650|                         o = o + Align (size (o), align_const);
 36651|                         if (o >= end)
 36652|                         {
 36653|                             break;
 36654|                         }
 36655|                         m = background_object_marked (o, TRUE);
 36656|                     }
 36657|                     plug_end = o;
 36658|                     dprintf (3, ("bgs: plug [%zx, %zx[", (size_t)plug_start, (size_t)plug_end));
 36659|                 }
 36660|                 else
 36661|                 {
 36662|                     while ((o < end) && !background_object_marked (o, FALSE))
 36663|                     {
 36664|                         o = o + Align (size (o), align_const);
 36665|                     }
 36666|                 }
 36667|             }
 36668|             if (plug_end != end)
 36669|             {
 36670|                 if (i >= 1)
 36671|                 {
 36672|                     thread_gap (plug_end, end - plug_end, current_gen);
 36673|                 }
 36674|                 else
 36675|                 {
 36676| #ifndef USE_REGIONS
 36677|                     heap_segment_allocated (ephemeral_heap_segment) = plug_end;
 36678|                     heap_segment_saved_bg_allocated (ephemeral_heap_segment) = plug_end;
 36679| #endif //!USE_REGIONS
 36680|                     make_unused_array (plug_end, (end - plug_end));
 36681|                 }
 36682|                 fix_brick_to_highest (plug_end, end);
 36683|             }
 36684| #ifdef USE_REGIONS
 36685|             ephemeral_region->flags |= heap_segment_flags_swept;
 36686|             heap_segment_background_allocated (ephemeral_region) = 0;
 36687|             ephemeral_region = heap_segment_next (ephemeral_region);
 36688| #endif //USE_REGIONS
 36689|         }
 36690|         dd_fragmentation (dynamic_data_of (i)) =
 36691|             generation_free_list_space (current_gen) + generation_free_obj_space (current_gen);
 36692|     }
 36693|     generation* youngest_gen = generation_of (0);
 36694|     generation_free_list_space (youngest_gen) = youngest_free_list_space;
 36695|     generation_free_obj_space (youngest_gen) = youngest_free_obj_space;
 36696|     dd_fragmentation (dynamic_data_of (0)) = youngest_free_list_space + youngest_free_obj_space;
 36697|     generation_allocator (youngest_gen)->copy_with_no_repair (&youngest_free_list);
 36698| }
 36699| void gc_heap::background_sweep()
 36700| {
 36701|     concurrent_print_time_delta ("Sw");
 36702|     dprintf (2, ("---- (GC%zu)Background Sweep Phase ----", VolatileLoad(&settings.gc_index)));
 36703|     dprintf (3, ("lh state: planning"));
 36704|     for (int i = 0; i <= max_generation; i++)
 36705|     {
 36706|         generation* gen_to_reset = generation_of (i);
 36707| #ifdef DOUBLY_LINKED_FL
 36708|         if (i == max_generation)
 36709|         {
 36710|             dprintf (2, ("h%d: gen2 still has FL: %zd, FO: %zd",
 36711|                 heap_number,
 36712|                 generation_free_list_space (gen_to_reset),
 36713|                 generation_free_obj_space (gen_to_reset)));
 36714|         }
 36715|         else
 36716| #endif //DOUBLY_LINKED_FL
 36717|         {
 36718|             generation_allocator (gen_to_reset)->clear();
 36719|             generation_free_list_space (gen_to_reset) = 0;
 36720|             generation_free_obj_space (gen_to_reset) = 0;
 36721|         }
 36722|         generation_free_list_allocated (gen_to_reset) = 0;
 36723|         generation_end_seg_allocated (gen_to_reset) = 0;
 36724|         generation_condemned_allocated (gen_to_reset) = 0;
 36725|         generation_sweep_allocated (gen_to_reset) = 0;
 36726|         generation_allocation_pointer (gen_to_reset)= 0;
 36727|         generation_allocation_limit (gen_to_reset) = 0;
 36728|         generation_allocation_segment (gen_to_reset) = heap_segment_rw (generation_start_segment (gen_to_reset));
 36729|     }
 36730|     FIRE_EVENT(BGC2ndNonConEnd);
 36731|     uoh_alloc_thread_count = 0;
 36732|     init_free_and_plug();
 36733|     current_bgc_state = bgc_sweep_soh;
 36734|     verify_soh_segment_list();
 36735| #ifdef DOUBLY_LINKED_FL
 36736|     current_sweep_seg = heap_segment_rw (generation_start_segment (generation_of (max_generation)));
 36737|     current_sweep_pos = 0;
 36738| #endif //DOUBLY_LINKED_FL
 36739| #ifdef FEATURE_BASICFREEZE
 36740|     sweep_ro_segments();
 36741| #endif //FEATURE_BASICFREEZE
 36742|     if (current_c_gc_state != c_gc_state_planning)
 36743|     {
 36744|         current_c_gc_state = c_gc_state_planning;
 36745|     }
 36746|     concurrent_print_time_delta ("Swe");
 36747|     for (int i = uoh_start_generation; i < total_generation_count; i++)
 36748|     {
 36749|         heap_segment* uoh_seg = heap_segment_rw (generation_start_segment (generation_of (i)));
 36750|         PREFIX_ASSUME(uoh_seg  != NULL);
 36751|         while (uoh_seg)
 36752|         {
 36753|             uoh_seg->flags &= ~heap_segment_flags_swept;
 36754|             heap_segment_background_allocated (uoh_seg) = heap_segment_allocated (uoh_seg);
 36755|             uoh_seg = heap_segment_next_rw (uoh_seg);
 36756|         }
 36757|     }
 36758| #ifdef MULTIPLE_HEAPS
 36759|     bgc_t_join.join(this, gc_join_restart_ee);
 36760|     if (bgc_t_join.joined())
 36761|     {
 36762|         dprintf(2, ("Starting BGC threads for resuming EE"));
 36763|         bgc_t_join.restart();
 36764|     }
 36765| #endif //MULTIPLE_HEAPS
 36766|     if (heap_number == 0)
 36767|     {
 36768| #ifdef BGC_SERVO_TUNING
 36769|         get_and_reset_loh_alloc_info();
 36770| #endif //BGC_SERVO_TUNING
 36771|         uint64_t suspended_end_ts = GetHighPrecisionTimeStamp();
 36772|         last_bgc_info[last_bgc_info_index].pause_durations[1] = (size_t)(suspended_end_ts - suspended_start_time);
 36773|         total_suspended_time += last_bgc_info[last_bgc_info_index].pause_durations[1];
 36774|         restart_EE ();
 36775|     }
 36776|     FIRE_EVENT(BGC2ndConBegin);
 36777|     background_ephemeral_sweep();
 36778|     concurrent_print_time_delta ("Swe eph");
 36779| #ifdef MULTIPLE_HEAPS
 36780|     bgc_t_join.join(this, gc_join_after_ephemeral_sweep);
 36781|     if (bgc_t_join.joined())
 36782| #endif //MULTIPLE_HEAPS
 36783|     {
 36784| #ifdef FEATURE_EVENT_TRACE
 36785|         bgc_heap_walk_for_etw_p = GCEventStatus::IsEnabled(GCEventProvider_Default,
 36786|                                                            GCEventKeyword_GCHeapSurvivalAndMovement,
 36787|                                                            GCEventLevel_Information);
 36788| #endif //FEATURE_EVENT_TRACE
 36789|         leave_spin_lock (&gc_lock);
 36790| #ifdef MULTIPLE_HEAPS
 36791|         dprintf(2, ("Starting BGC threads for BGC sweeping"));
 36792|         bgc_t_join.restart();
 36793| #endif //MULTIPLE_HEAPS
 36794|     }
 36795|     disable_preemptive (true);
 36796|     dynamic_data* dd     = dynamic_data_of (max_generation);
 36797|     const int num_objs   = 256;
 36798|     int current_num_objs = 0;
 36799|     for (int i = max_generation; i < total_generation_count; i++)
 36800|     {
 36801|         generation* gen = generation_of (i);
 36802|         heap_segment* gen_start_seg = heap_segment_rw (generation_start_segment(gen));
 36803|         heap_segment* next_seg = 0;
 36804|         heap_segment* prev_seg;
 36805|         heap_segment* start_seg;
 36806|         int align_const = get_alignment_constant (i == max_generation);
 36807| #ifndef DOUBLY_LINKED_FL
 36808|         if (i == max_generation)
 36809|         {
 36810| #ifdef USE_REGIONS
 36811|             start_seg = generation_tail_region (gen);
 36812| #else
 36813|             start_seg = saved_sweep_ephemeral_seg;
 36814| #endif //USE_REGIONS
 36815|             prev_seg = heap_segment_next(start_seg);
 36816|         }
 36817|         else
 36818| #endif //!DOUBLY_LINKED_FL
 36819|         {
 36820|             start_seg = gen_start_seg;
 36821|             prev_seg = NULL;
 36822|             if (i > max_generation)
 36823|             {
 36824|                 generation_allocator (gen)->clear();
 36825|                 generation_free_list_space (gen) = 0;
 36826|                 generation_free_obj_space (gen) = 0;
 36827|                 generation_free_list_allocated (gen) = 0;
 36828|                 generation_end_seg_allocated (gen) = 0;
 36829|                 generation_condemned_allocated (gen) = 0;
 36830|                 generation_sweep_allocated (gen) = 0;
 36831|                 generation_allocation_pointer (gen)= 0;
 36832|                 generation_allocation_limit (gen) = 0;
 36833|                 generation_allocation_segment (gen) = heap_segment_rw (generation_start_segment (gen));
 36834|             }
 36835|             else
 36836|             {
 36837|                 dprintf (3333, ("h%d: SOH sweep start on seg %zx: total FL: %zd, FO: %zd",
 36838|                     heap_number, (size_t)start_seg,
 36839|                     generation_free_list_space (gen),
 36840|                     generation_free_obj_space (gen)));
 36841|             }
 36842|         }
 36843|         PREFIX_ASSUME(start_seg != NULL);
 36844|         heap_segment* seg = start_seg;
 36845|         dprintf (2, ("bgs: sweeping gen %d seg %p->%p(%p)", gen->gen_num,
 36846|             heap_segment_mem (seg),
 36847|             heap_segment_allocated (seg),
 36848|             heap_segment_background_allocated (seg)));
 36849|         while (seg
 36850| #ifdef DOUBLY_LINKED_FL
 36851|                && !((heap_segment_background_allocated (seg) == 0) && (gen != large_object_generation))
 36852| #endif //DOUBLY_LINKED_FL
 36853|                 )
 36854|         {
 36855|             uint8_t* o = heap_segment_mem (seg);
 36856|             if (seg == gen_start_seg)
 36857|             {
 36858| #ifndef USE_REGIONS
 36859|                 assert (o == generation_allocation_start (gen));
 36860|                 assert (method_table (o) == g_gc_pFreeObjectMethodTable);
 36861|                 o = o + Align (size (o), align_const);
 36862| #endif //!USE_REGIONS
 36863|             }
 36864|             uint8_t* plug_end = o;
 36865|             current_sweep_pos = o;
 36866|             next_sweep_obj = o;
 36867| #ifdef DOUBLY_LINKED_FL
 36868|             current_sweep_seg = seg;
 36869| #endif //DOUBLY_LINKED_FL
 36870|             size_t free_obj_size_last_gap = 0;
 36871|             allow_fgc();
 36872|             uint8_t* end = background_next_end (seg, (i > max_generation));
 36873|             dprintf (3333, ("bgs: seg: %zx, [%zx, %zx[%zx", (size_t)seg,
 36874|                             (size_t)heap_segment_mem (seg),
 36875|                             (size_t)heap_segment_allocated (seg),
 36876|                             (size_t)heap_segment_background_allocated (seg)));
 36877|             while (o < end)
 36878|             {
 36879|                 if (background_object_marked (o, TRUE))
 36880|                 {
 36881|                     uint8_t* plug_start = o;
 36882|                     if (i > max_generation)
 36883|                     {
 36884|                         dprintf (2, ("uoh fr: [%p-%p[(%zd)", plug_end, plug_start, plug_start-plug_end));
 36885|                     }
 36886|                     thread_gap (plug_end, plug_start-plug_end, gen);
 36887|                     if (i == max_generation)
 36888|                     {
 36889|                         add_gen_free (max_generation, plug_start-plug_end);
 36890| #ifdef DOUBLY_LINKED_FL
 36891|                         if (free_obj_size_last_gap)
 36892|                         {
 36893|                             generation_free_obj_space (gen) -= free_obj_size_last_gap;
 36894|                             dprintf (3333, ("[h%d] LG: gen2FO-: %zd->%zd",
 36895|                                 heap_number, free_obj_size_last_gap, generation_free_obj_space (gen)));
 36896|                             free_obj_size_last_gap = 0;
 36897|                         }
 36898| #endif //DOUBLY_LINKED_FL
 36899|                         fix_brick_to_highest (plug_end, plug_start);
 36900|                         fix_brick_to_highest (plug_start, plug_start);
 36901|                     }
 36902|                     do
 36903|                     {
 36904|                         next_sweep_obj = o + Align (size (o), align_const);
 36905|                         current_num_objs++;
 36906|                         if (current_num_objs >= num_objs)
 36907|                         {
 36908|                             current_sweep_pos = next_sweep_obj;
 36909|                             allow_fgc();
 36910|                             current_num_objs = 0;
 36911|                         }
 36912|                         o = next_sweep_obj;
 36913|                     } while ((o < end) && background_object_marked(o, TRUE));
 36914|                     plug_end = o;
 36915|                     if (i == max_generation)
 36916|                     {
 36917|                         add_gen_plug (max_generation, plug_end-plug_start);
 36918|                         dd_survived_size (dd) += (plug_end - plug_start);
 36919|                     }
 36920|                     dprintf (3, ("bgs: plug [%zx, %zx[", (size_t)plug_start, (size_t)plug_end));
 36921|                 }
 36922|                 while ((o < end) && !background_object_marked (o, FALSE))
 36923|                 {
 36924|                     size_t size_o = Align(size (o), align_const);
 36925|                     next_sweep_obj = o + size_o;
 36926| #ifdef DOUBLY_LINKED_FL
 36927|                     if (gen != large_object_generation)
 36928|                     {
 36929|                         if (method_table (o) == g_gc_pFreeObjectMethodTable)
 36930|                         {
 36931|                             free_obj_size_last_gap += size_o;
 36932|                             if (is_on_free_list (o, size_o))
 36933|                             {
 36934| #ifdef MULTIPLE_HEAPS
 36935|                                 assert (heap_of (o) == this);
 36936| #endif //MULTIPLE_HEAPS
 36937|                                 generation_allocator (gen)->unlink_item_no_undo (o, size_o);
 36938|                                 generation_free_list_space (gen) -= size_o;
 36939|                                 assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
 36940|                                 generation_free_obj_space (gen) += size_o;
 36941|                                 dprintf (3333, ("[h%d] gen2F-: %p->%p(%zd) FL: %zd",
 36942|                                     heap_number, o, (o + size_o), size_o,
 36943|                                     generation_free_list_space (gen)));
 36944|                                 dprintf (3333, ("h%d: gen2FO+: %p(%zx)->%zd (g: %zd)",
 36945|                                     heap_number, o, size_o,
 36946|                                     generation_free_obj_space (gen),
 36947|                                     free_obj_size_last_gap));
 36948|                                 remove_gen_free (max_generation, size_o);
 36949|                             }
 36950|                             else
 36951|                             {
 36952|                                 dprintf (3333, ("h%d: gen2FO: %p(%zd)->%zd (g: %zd)",
 36953|                                     heap_number, o, size_o,
 36954|                                     generation_free_obj_space (gen), free_obj_size_last_gap));
 36955|                             }
 36956|                             dprintf (3333, ("h%d: total FO: %p->%p FL: %zd, FO: %zd (g: %zd)",
 36957|                                 heap_number, plug_end, next_sweep_obj,
 36958|                                 generation_free_list_space (gen),
 36959|                                 generation_free_obj_space (gen),
 36960|                                 free_obj_size_last_gap));
 36961|                         }
 36962|                     }
 36963| #endif //DOUBLY_LINKED_FL
 36964|                     current_num_objs++;
 36965|                     if (current_num_objs >= num_objs)
 36966|                     {
 36967|                         current_sweep_pos = plug_end;
 36968|                         dprintf (1234, ("f: swept till %p", current_sweep_pos));
 36969|                         allow_fgc();
 36970|                         current_num_objs = 0;
 36971|                     }
 36972|                     o = next_sweep_obj;
 36973|                 }
 36974|             }
 36975| #ifdef DOUBLY_LINKED_FL
 36976|             next_seg = heap_segment_next (seg);
 36977| #else //DOUBLY_LINKED_FL
 36978|             if (i > max_generation)
 36979|             {
 36980|                 next_seg = heap_segment_next (seg);
 36981|             }
 36982|             else
 36983|             {
 36984|                 next_seg = heap_segment_prev (gen_start_seg, seg);
 36985|             }
 36986| #endif //DOUBLY_LINKED_FL
 36987|             BOOL delete_p = FALSE;
 36988|             if (!heap_segment_read_only_p (seg))
 36989|             {
 36990|                 if (i > max_generation)
 36991|                 {
 36992|                     process_background_segment_end (seg, gen, plug_end,
 36993|                                                     start_seg, &delete_p, 0);
 36994|                 }
 36995|                 else
 36996|                 {
 36997|                     assert (heap_segment_background_allocated (seg) != 0);
 36998|                     process_background_segment_end (seg, gen, plug_end,
 36999|                                                     start_seg, &delete_p, free_obj_size_last_gap);
 37000| #ifndef USE_REGIONS
 37001|                     assert (next_seg || !delete_p);
 37002| #endif //!USE_REGIONS
 37003|                 }
 37004|             }
 37005|             heap_segment* saved_prev_seg = prev_seg;
 37006|             if (delete_p)
 37007|             {
 37008|                 generation_delete_heap_segment (gen, seg, prev_seg, next_seg);
 37009|             }
 37010|             else
 37011|             {
 37012|                 prev_seg = seg;
 37013|                 dprintf (2, ("seg %p (%p) has been swept", seg, heap_segment_mem (seg)));
 37014|                 seg->flags |= heap_segment_flags_swept;
 37015|                 current_sweep_pos = end;
 37016|             }
 37017|             verify_soh_segment_list();
 37018| #ifdef DOUBLY_LINKED_FL
 37019|             while (next_seg && heap_segment_background_allocated (next_seg) == 0)
 37020|             {
 37021|                 dprintf (2, ("[h%d] skip new %p ", heap_number, next_seg));
 37022|                 next_seg = heap_segment_next (next_seg);
 37023|             }
 37024| #endif //DOUBLY_LINKED_FL
 37025|             dprintf (GTC_LOG, ("seg: %p(%p), next_seg: %p(%p), prev_seg: %p(%p), delete_p %d",
 37026|                 seg, (seg ? heap_segment_mem (seg) : 0),
 37027|                 next_seg, (next_seg ? heap_segment_mem (next_seg) : 0),
 37028|                 saved_prev_seg, (saved_prev_seg ? heap_segment_mem (saved_prev_seg) : 0),
 37029|                 (delete_p ? 1 : 0)));
 37030|             seg = next_seg;
 37031|         }
 37032|         generation_allocation_segment (gen) = heap_segment_rw (generation_start_segment (gen));
 37033|         PREFIX_ASSUME(generation_allocation_segment(gen) != NULL);
 37034|         if (i == max_generation)
 37035|         {
 37036|             dprintf (2, ("bgs: sweeping uoh objects"));
 37037|             concurrent_print_time_delta ("Swe SOH");
 37038|             FIRE_EVENT(BGC1stSweepEnd, 0);
 37039|             enter_spin_lock (&more_space_lock_uoh);
 37040|             add_saved_spinlock_info (true, me_acquire, mt_bgc_uoh_sweep, msl_entered);
 37041|             concurrent_print_time_delta ("Swe UOH took msl");
 37042|             int spin_count = yp_spin_count_unit;
 37043|             while (uoh_alloc_thread_count)
 37044|             {
 37045|                 spin_and_switch (spin_count, (uoh_alloc_thread_count == 0));
 37046|             }
 37047|             current_bgc_state = bgc_sweep_uoh;
 37048|         }
 37049|     }
 37050|     size_t total_soh_size = generation_sizes (generation_of (max_generation));
 37051|     size_t total_loh_size = generation_size (loh_generation);
 37052|     size_t total_poh_size = generation_size (poh_generation);
 37053|     dprintf (GTC_LOG, ("h%d: S: poh: %zd, loh: %zd, soh: %zd", heap_number, total_poh_size, total_loh_size, total_soh_size));
 37054|     dprintf (GTC_LOG, ("end of bgc sweep: gen2 FL: %zd, FO: %zd",
 37055|         generation_free_list_space (generation_of (max_generation)),
 37056|         generation_free_obj_space (generation_of (max_generation))));
 37057|     dprintf (GTC_LOG, ("h%d: end of bgc sweep: loh FL: %zd, FO: %zd",
 37058|         heap_number,
 37059|         generation_free_list_space (generation_of (loh_generation)),
 37060|         generation_free_obj_space (generation_of (loh_generation))));
 37061|     dprintf (GTC_LOG, ("h%d: end of bgc sweep: poh FL: %zd, FO: %zd",
 37062|         heap_number,
 37063|         generation_free_list_space (generation_of (poh_generation)),
 37064|         generation_free_obj_space (generation_of (poh_generation))));
 37065|     FIRE_EVENT(BGC2ndConEnd);
 37066|     concurrent_print_time_delta ("background sweep");
 37067|     heap_segment* reset_seg = heap_segment_rw (generation_start_segment (generation_of (max_generation)));
 37068|     PREFIX_ASSUME(reset_seg != NULL);
 37069|     while (reset_seg)
 37070|     {
 37071|         heap_segment_saved_bg_allocated (reset_seg) = heap_segment_background_allocated (reset_seg);
 37072|         heap_segment_background_allocated (reset_seg) = 0;
 37073|         reset_seg = heap_segment_next_rw (reset_seg);
 37074|     }
 37075|     compute_new_dynamic_data (max_generation);
 37076| #ifdef DOUBLY_LINKED_FL
 37077|     current_bgc_state = bgc_not_in_process;
 37078|     current_sweep_seg = 0;
 37079| #endif //DOUBLY_LINKED_FL
 37080|     enable_preemptive ();
 37081| #ifdef MULTIPLE_HEAPS
 37082|     bgc_t_join.join(this, gc_join_set_state_free);
 37083|     if (bgc_t_join.joined())
 37084| #endif //MULTIPLE_HEAPS
 37085|     {
 37086|         current_c_gc_state = c_gc_state_free;
 37087| #ifdef BGC_SERVO_TUNING
 37088|         if (bgc_tuning::enable_fl_tuning)
 37089|         {
 37090|             enter_spin_lock (&gc_lock);
 37091|             bgc_tuning::record_and_adjust_bgc_end();
 37092|             leave_spin_lock (&gc_lock);
 37093|         }
 37094| #endif //BGC_SERVO_TUNING
 37095| #ifdef MULTIPLE_HEAPS
 37096|         dprintf(2, ("Starting BGC threads after background sweep phase"));
 37097|         bgc_t_join.restart();
 37098| #endif //MULTIPLE_HEAPS
 37099|     }
 37100|     disable_preemptive (true);
 37101|     add_saved_spinlock_info (true, me_release, mt_bgc_uoh_sweep, msl_entered);
 37102|     leave_spin_lock (&more_space_lock_uoh);
 37103|     dprintf (GTC_LOG, ("---- (GC%zu)ESw ----", VolatileLoad(&settings.gc_index)));
 37104| }
 37105| #endif //BACKGROUND_GC
 37106| void gc_heap::sweep_uoh_objects (int gen_num)
 37107| {
 37108|     generation* gen        = generation_of (gen_num);
 37109|     heap_segment* start_seg = heap_segment_rw (generation_start_segment (gen));
 37110|     PREFIX_ASSUME(start_seg != NULL);
 37111|     heap_segment* seg      = start_seg;
 37112|     heap_segment* prev_seg = 0;
 37113|     uint8_t* o             = get_uoh_start_object (seg, gen);
 37114|     uint8_t* plug_end         = o;
 37115|     uint8_t* plug_start       = o;
 37116|     generation_allocator (gen)->clear();
 37117|     generation_free_list_space (gen) = 0;
 37118|     generation_free_obj_space (gen) = 0;
 37119|     generation_free_list_allocated (gen) = 0;
 37120|     dprintf (3, ("sweeping uoh objects"));
 37121|     dprintf (3, ("seg: %zx, [%zx, %zx[, starting from %p",
 37122|                  (size_t)seg,
 37123|                  (size_t)heap_segment_mem (seg),
 37124|                  (size_t)heap_segment_allocated (seg),
 37125|                  o));
 37126|     while (1)
 37127|     {
 37128|         if (o >= heap_segment_allocated (seg))
 37129|         {
 37130|             heap_segment* next_seg = heap_segment_next (seg);
 37131|             if ((plug_end == heap_segment_mem (seg)) &&
 37132|                 (seg != start_seg) && !heap_segment_read_only_p (seg))
 37133|             {
 37134|                 dprintf (3, ("Preparing empty large segment %zx", (size_t)seg));
 37135|                 assert (prev_seg);
 37136|                 heap_segment_next (prev_seg) = next_seg;
 37137|                 heap_segment_next (seg) = freeable_uoh_segment;
 37138|                 freeable_uoh_segment = seg;
 37139| #ifdef USE_REGIONS
 37140|                 update_start_tail_regions (gen, seg, prev_seg, next_seg);
 37141| #endif //USE_REGIONS
 37142|             }
 37143|             else
 37144|             {
 37145|                 if (!heap_segment_read_only_p (seg))
 37146|                 {
 37147|                     dprintf (3, ("Trimming seg to %zx[", (size_t)plug_end));
 37148|                     heap_segment_allocated (seg) = plug_end;
 37149|                     decommit_heap_segment_pages (seg, 0);
 37150|                 }
 37151|                 prev_seg = seg;
 37152|             }
 37153|             seg = next_seg;
 37154|             if (seg == 0)
 37155|                 break;
 37156|             else
 37157|             {
 37158|                 o = heap_segment_mem (seg);
 37159|                 plug_end = o;
 37160|                 dprintf (3, ("seg: %zx, [%zx, %zx[", (size_t)seg,
 37161|                              (size_t)heap_segment_mem (seg),
 37162|                              (size_t)heap_segment_allocated (seg)));
 37163| #ifdef USE_REGIONS
 37164|                 continue;
 37165| #endif //USE_REGIONS
 37166|             }
 37167|         }
 37168|         if (uoh_object_marked(o, TRUE))
 37169|         {
 37170|             plug_start = o;
 37171|             thread_gap (plug_end, plug_start-plug_end, gen);
 37172|             BOOL m = TRUE;
 37173|             while (m)
 37174|             {
 37175|                 o = o + AlignQword (size (o));
 37176|                 if (o >= heap_segment_allocated (seg))
 37177|                 {
 37178|                     break;
 37179|                 }
 37180|                 m = uoh_object_marked (o, TRUE);
 37181|             }
 37182|             plug_end = o;
 37183|             dprintf (3, ("plug [%zx, %zx[", (size_t)plug_start, (size_t)plug_end));
 37184|         }
 37185|         else
 37186|         {
 37187|             while (o < heap_segment_allocated (seg) && !uoh_object_marked(o, FALSE))
 37188|             {
 37189|                 o = o + AlignQword (size (o));
 37190|             }
 37191|         }
 37192|     }
 37193|     generation_allocation_segment (gen) = heap_segment_rw (generation_start_segment (gen));
 37194|     PREFIX_ASSUME(generation_allocation_segment(gen) != NULL);
 37195| }
 37196| void gc_heap::relocate_in_uoh_objects (int gen_num)
 37197| {
 37198|     generation* gen = generation_of (gen_num);
 37199|     heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 37200|     PREFIX_ASSUME(seg != NULL);
 37201|     uint8_t* o = get_uoh_start_object (seg, gen);
 37202|     while (1)
 37203|     {
 37204|         if (o >= heap_segment_allocated (seg))
 37205|         {
 37206|             seg = heap_segment_next_rw (seg);
 37207|             if (seg == 0)
 37208|                 break;
 37209|             else
 37210|             {
 37211|                 o = heap_segment_mem (seg);
 37212|             }
 37213|         }
 37214|         while (o < heap_segment_allocated (seg))
 37215|         {
 37216|             check_class_object_demotion (o);
 37217|             if (contain_pointers (o))
 37218|             {
 37219|                 dprintf(3, ("Relocating through uoh object %zx", (size_t)o));
 37220|                 go_through_object_nostart (method_table (o), o, size(o), pval,
 37221|                         {
 37222|                             reloc_survivor_helper (pval);
 37223|                         });
 37224|             }
 37225|             o = o + AlignQword (size (o));
 37226|         }
 37227|     }
 37228| }
 37229| void gc_heap::mark_through_cards_for_uoh_objects (card_fn fn,
 37230|                                                   int gen_num,
 37231|                                                   BOOL relocating
 37232|                                                   CARD_MARKING_STEALING_ARG(gc_heap* hpt))
 37233| {
 37234| #ifdef USE_REGIONS
 37235|     uint8_t*      low               = 0;
 37236| #else
 37237|     uint8_t*      low               = gc_low;
 37238| #endif //USE_REGIONS
 37239|     size_t        end_card          = 0;
 37240|     generation*   oldest_gen        = generation_of (gen_num);
 37241|     heap_segment* seg               = heap_segment_rw (generation_start_segment (oldest_gen));
 37242|     PREFIX_ASSUME(seg != NULL);
 37243|     uint8_t*      beg               = get_uoh_start_object (seg, oldest_gen);
 37244|     uint8_t*      end               = heap_segment_allocated (seg);
 37245|     size_t  cg_pointers_found = 0;
 37246|     size_t  card_word_end = (card_of (align_on_card_word (end)) /
 37247|                              card_word_width);
 37248|     size_t      n_eph             = 0;
 37249|     size_t      n_gen             = 0;
 37250|     size_t      n_card_set        = 0;
 37251| #ifdef USE_REGIONS
 37252|     uint8_t*    next_boundary = 0;
 37253|     uint8_t*    nhigh         = 0;
 37254| #else
 37255|     uint8_t*    next_boundary = (relocating ?
 37256|                               generation_plan_allocation_start (generation_of (max_generation -1)) :
 37257|                               ephemeral_low);
 37258|     uint8_t*    nhigh         = (relocating ?
 37259|                               heap_segment_plan_allocated (ephemeral_heap_segment) :
 37260|                               ephemeral_high);
 37261| #endif //USE_REGIONS
 37262|     BOOL          foundp            = FALSE;
 37263|     uint8_t*      start_address     = 0;
 37264|     uint8_t*      limit             = 0;
 37265|     size_t        card              = card_of (beg);
 37266|     uint8_t*      o                 = beg;
 37267| #ifdef BACKGROUND_GC
 37268|     BOOL consider_bgc_mark_p        = FALSE;
 37269|     BOOL check_current_sweep_p      = FALSE;
 37270|     BOOL check_saved_sweep_p        = FALSE;
 37271|     should_check_bgc_mark (seg, &consider_bgc_mark_p, &check_current_sweep_p, &check_saved_sweep_p);
 37272| #endif //BACKGROUND_GC
 37273|     size_t total_cards_cleared = 0;
 37274| #ifdef FEATURE_CARD_MARKING_STEALING
 37275|     VOLATILE(uint32_t)* chunk_index = (VOLATILE(uint32_t)*) &(gen_num == loh_generation ?
 37276|         card_mark_chunk_index_loh :
 37277|         card_mark_chunk_index_poh);
 37278|     card_marking_enumerator card_mark_enumerator(seg, low, chunk_index);
 37279|     card_word_end = 0;
 37280| #endif // FEATURE_CARD_MARKING_STEALING
 37281| #ifdef USE_REGIONS
 37282|     int condemned_gen = settings.condemned_generation;
 37283| #else
 37284|     int condemned_gen = -1;
 37285| #endif //USE_REGIONS
 37286|     dprintf(3, ("CMl: %zx->%zx", (size_t)beg, (size_t)end));
 37287|     while (1)
 37288|     {
 37289|         if ((o < end) && (card_of(o) > card))
 37290|         {
 37291|             dprintf (3, ("Found %zd cg pointers", cg_pointers_found));
 37292|             if (cg_pointers_found == 0)
 37293|             {
 37294|                 uint8_t* last_object_processed = o;
 37295| #ifdef FEATURE_CARD_MARKING_STEALING
 37296|                 last_object_processed = min(limit, o);
 37297| #endif // FEATURE_CARD_MARKING_STEALING
 37298|                 dprintf (3, (" Clearing cards [%zx, %zx[ ", (size_t)card_address(card), (size_t)last_object_processed));
 37299|                 clear_cards (card, card_of((uint8_t*)last_object_processed));
 37300|                 total_cards_cleared += (card_of((uint8_t*)last_object_processed) - card);
 37301|             }
 37302|             n_eph +=cg_pointers_found;
 37303|             cg_pointers_found = 0;
 37304|             card = card_of ((uint8_t*)o);
 37305|         }
 37306|         if ((o < end) &&(card >= end_card))
 37307|         {
 37308| #ifdef FEATURE_CARD_MARKING_STEALING
 37309|             foundp = find_next_chunk(card_mark_enumerator, seg, n_card_set, start_address, limit, card, end_card, card_word_end);
 37310| #else // FEATURE_CARD_MARKING_STEALING
 37311|             foundp = find_card (card_table, card, card_word_end, end_card);
 37312|             if (foundp)
 37313|             {
 37314|                 n_card_set+= end_card - card;
 37315|                 start_address = max (beg, card_address (card));
 37316|             }
 37317|             limit = min (end, card_address (end_card));
 37318| #endif  // FEATURE_CARD_MARKING_STEALING
 37319|         }
 37320|         if ((!foundp) || (o >= end) || (card_address (card) >= end))
 37321|         {
 37322|             if ((foundp) && (cg_pointers_found == 0))
 37323|             {
 37324|                 dprintf(3,(" Clearing cards [%zx, %zx[ ", (size_t)card_address(card),
 37325|                            (size_t)card_address(card+1)));
 37326|                 clear_cards (card, card+1);
 37327|                 total_cards_cleared += 1;
 37328|             }
 37329|             n_eph +=cg_pointers_found;
 37330|             cg_pointers_found = 0;
 37331| #ifdef FEATURE_CARD_MARKING_STEALING
 37332|             card_mark_enumerator.exhaust_segment(seg);
 37333| #endif // FEATURE_CARD_MARKING_STEALING
 37334|             if ((seg = heap_segment_next_rw (seg)) != 0)
 37335|             {
 37336| #ifdef BACKGROUND_GC
 37337|                 should_check_bgc_mark (seg, &consider_bgc_mark_p, &check_current_sweep_p, &check_saved_sweep_p);
 37338| #endif //BACKGROUND_GC
 37339|                 beg = heap_segment_mem (seg);
 37340|                 end = compute_next_end (seg, low);
 37341| #ifdef FEATURE_CARD_MARKING_STEALING
 37342|                 card_word_end = 0;
 37343| #else // FEATURE_CARD_MARKING_STEALING
 37344|                 card_word_end = card_of (align_on_card_word (end)) / card_word_width;
 37345| #endif // FEATURE_CARD_MARKING_STEALING
 37346|                 card = card_of (beg);
 37347|                 o  = beg;
 37348|                 end_card = 0;
 37349|                 continue;
 37350|             }
 37351|             else
 37352|             {
 37353|                 break;
 37354|             }
 37355|         }
 37356|         assert (card_set_p (card));
 37357|         {
 37358|             dprintf(3,("card %zx: o: %zx, l: %zx[ ",
 37359|                        card, (size_t)o, (size_t)limit));
 37360|             assert (Align (size (o)) >= Align (min_obj_size));
 37361|             size_t s = size (o);
 37362|             uint8_t* next_o =  o + AlignQword (s);
 37363|             Prefetch (next_o);
 37364|             while (o < limit)
 37365|             {
 37366|                 s = size (o);
 37367|                 assert (Align (s) >= Align (min_obj_size));
 37368|                 next_o =  o + AlignQword (s);
 37369|                 Prefetch (next_o);
 37370|                 dprintf (4, ("|%zx|", (size_t)o));
 37371|                 if (next_o < start_address)
 37372|                 {
 37373|                     goto end_object;
 37374|                 }
 37375| #ifdef BACKGROUND_GC
 37376|                 if (!fgc_should_consider_object (o, seg, consider_bgc_mark_p, check_current_sweep_p, check_saved_sweep_p))
 37377|                 {
 37378|                     goto end_object;
 37379|                 }
 37380| #endif //BACKGROUND_GC
 37381| #ifdef COLLECTIBLE_CLASS
 37382|                 if (is_collectible(o))
 37383|                 {
 37384|                     BOOL passed_end_card_p = FALSE;
 37385|                     if (card_of (o) > card)
 37386|                     {
 37387|                         passed_end_card_p = card_transition (o, end, card_word_end,
 37388|                             cg_pointers_found,
 37389|                             n_eph, n_card_set,
 37390|                             card, end_card,
 37391|                             foundp, start_address,
 37392|                             limit, total_cards_cleared
 37393|                             CARD_MARKING_STEALING_ARGS(card_mark_enumerator, seg, card_word_end));
 37394|                     }
 37395|                     if ((!passed_end_card_p || foundp) && (card_of (o) == card))
 37396|                     {
 37397|                         if (fn == &gc_heap::relocate_address)
 37398|                         {
 37399|                             cg_pointers_found++;
 37400|                         }
 37401|                         else
 37402|                         {
 37403|                             uint8_t* class_obj = get_class_object (o);
 37404|                             mark_through_cards_helper (&class_obj, n_gen,
 37405|                                                        cg_pointers_found, fn,
 37406|                                                        nhigh, next_boundary,
 37407|                                                        condemned_gen, max_generation CARD_MARKING_STEALING_ARG(hpt));
 37408|                         }
 37409|                     }
 37410|                     if (passed_end_card_p)
 37411|                     {
 37412|                         if (foundp && (card_address (card) < next_o))
 37413|                         {
 37414|                             goto go_through_refs;
 37415|                         }
 37416|                         else
 37417|                         {
 37418|                             goto end_object;
 37419|                         }
 37420|                     }
 37421|                 }
 37422| go_through_refs:
 37423| #endif //COLLECTIBLE_CLASS
 37424|                 if (contain_pointers (o))
 37425|                 {
 37426|                     dprintf(3,("Going through %zx", (size_t)o));
 37427|                     go_through_object (method_table(o), o, s, poo,
 37428|                                        start_address, use_start, (o + s),
 37429|                        {
 37430|                            if (card_of ((uint8_t*)poo) > card)
 37431|                            {
 37432|                                 BOOL passed_end_card_p  = card_transition ((uint8_t*)poo, end,
 37433|                                         card_word_end,
 37434|                                         cg_pointers_found,
 37435|                                         n_eph, n_card_set,
 37436|                                         card, end_card,
 37437|                                         foundp, start_address,
 37438|                                         limit, total_cards_cleared
 37439|                                         CARD_MARKING_STEALING_ARGS(card_mark_enumerator, seg, card_word_end));
 37440|                                 if (passed_end_card_p)
 37441|                                 {
 37442|                                     if (foundp && (card_address (card) < next_o))
 37443|                                     {
 37444|                                         {
 37445|                                             if (ppstop <= (uint8_t**)start_address)
 37446|                                             {break;}
 37447|                                             else if (poo < (uint8_t**)start_address)
 37448|                                             {poo = (uint8_t**)start_address;}
 37449|                                         }
 37450|                                     }
 37451|                                     else
 37452|                                     {
 37453|                                         goto end_object;
 37454|                                     }
 37455|                                 }
 37456|                             }
 37457|                            mark_through_cards_helper (poo, n_gen,
 37458|                                                       cg_pointers_found, fn,
 37459|                                                       nhigh, next_boundary,
 37460|                                                       condemned_gen, max_generation CARD_MARKING_STEALING_ARG(hpt));
 37461|                        }
 37462|                         );
 37463|                 }
 37464|             end_object:
 37465|                 o = next_o;
 37466|             }
 37467|         }
 37468|     }
 37469|     if (!relocating)
 37470|     {
 37471| #ifdef FEATURE_CARD_MARKING_STEALING
 37472|         Interlocked::ExchangeAddPtr(&n_eph_loh, n_eph);
 37473|         Interlocked::ExchangeAddPtr(&n_gen_loh, n_gen);
 37474|         dprintf (3, ("h%d marking h%d Mloh: cross: %zd, useful: %zd, cards set: %zd, cards cleared: %zd, ratio: %d",
 37475|             hpt->heap_number, heap_number, n_eph, n_gen, n_card_set, total_cards_cleared,
 37476|             (n_eph ? (int)(((float)n_gen / (float)n_eph) * 100) : 0)));
 37477|         dprintf (3, ("h%d marking h%d Mloh: total cross %zd, useful: %zd, running ratio: %d",
 37478|             hpt->heap_number, heap_number, (size_t)n_eph_loh, (size_t)n_gen_loh,
 37479|             (n_eph_loh ? (int)(((float)n_gen_loh / (float)n_eph_loh) * 100) : 0)));
 37480| #else
 37481|         generation_skip_ratio = min (((n_eph > MIN_LOH_CROSS_GEN_REFS) ?
 37482|             (int)(((float)n_gen / (float)n_eph) * 100) : 100),
 37483|             generation_skip_ratio);
 37484|         dprintf (3, ("marking h%d Mloh: cross: %zd, useful: %zd, cards cleared: %zd, cards set: %zd, ratio: %d",
 37485|             heap_number, n_eph, n_gen, total_cards_cleared, n_card_set, generation_skip_ratio));
 37486| #endif //FEATURE_CARD_MARKING_STEALING
 37487|     }
 37488|     else
 37489|     {
 37490|         dprintf (3, ("R: Mloh: cross: %zd, useful: %zd, cards set: %zd, ratio: %d",
 37491|              n_eph, n_gen, n_card_set, generation_skip_ratio));
 37492|     }
 37493| }
 37494| void gc_heap::descr_generations_to_profiler (gen_walk_fn fn, void *context)
 37495| {
 37496| #ifdef MULTIPLE_HEAPS
 37497|     for (int i = 0; i < n_heaps; i++)
 37498|     {
 37499|         gc_heap* hp = g_heaps[i];
 37500| #else //MULTIPLE_HEAPS
 37501|     {
 37502|         gc_heap* hp = NULL;
 37503| #ifdef _PREFAST_
 37504|         PREFIX_ASSUME(hp != NULL);
 37505| #endif // _PREFAST_
 37506| #endif //MULTIPLE_HEAPS
 37507|         for (int curr_gen_number = total_generation_count-1; curr_gen_number >= 0; curr_gen_number--)
 37508|         {
 37509|             generation* gen = hp->generation_of (curr_gen_number);
 37510|             heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 37511| #ifdef USE_REGIONS
 37512|             while (seg)
 37513|             {
 37514|                 fn(context, curr_gen_number, heap_segment_mem (seg),
 37515|                                               heap_segment_allocated (seg),
 37516|                                               heap_segment_reserved (seg));
 37517|                 seg = heap_segment_next_rw (seg);
 37518|             }
 37519| #else
 37520|             while (seg && (seg != hp->ephemeral_heap_segment))
 37521|             {
 37522|                 assert (curr_gen_number > 0);
 37523|                 fn(context, curr_gen_number, heap_segment_mem (seg),
 37524|                                               heap_segment_allocated (seg),
 37525|                                               (curr_gen_number > max_generation) ?
 37526|                                                 heap_segment_reserved (seg) : heap_segment_allocated (seg));
 37527|                 seg = heap_segment_next_rw (seg);
 37528|             }
 37529|             if (seg)
 37530|             {
 37531|                 assert (seg == hp->ephemeral_heap_segment);
 37532|                 assert (curr_gen_number <= max_generation);
 37533|                 if (curr_gen_number == max_generation)
 37534|                 {
 37535|                     if (heap_segment_mem (seg) < generation_allocation_start (hp->generation_of (max_generation-1)))
 37536|                     {
 37537|                         fn(context, curr_gen_number, heap_segment_mem (seg),
 37538|                                                       generation_allocation_start (hp->generation_of (max_generation-1)),
 37539|                                                       generation_allocation_start (hp->generation_of (max_generation-1)) );
 37540|                     }
 37541|                 }
 37542|                 else if (curr_gen_number != 0)
 37543|                 {
 37544|                     fn(context, curr_gen_number, generation_allocation_start (hp->generation_of (curr_gen_number)),
 37545|                                                   generation_allocation_start (hp->generation_of (curr_gen_number-1)),
 37546|                                                   generation_allocation_start (hp->generation_of (curr_gen_number-1)));
 37547|                 }
 37548|                 else
 37549|                 {
 37550|                     fn(context, curr_gen_number, generation_allocation_start (hp->generation_of (curr_gen_number)),
 37551|                                                   heap_segment_allocated (hp->ephemeral_heap_segment),
 37552|                                                   heap_segment_reserved (hp->ephemeral_heap_segment) );
 37553|                 }
 37554|             }
 37555| #endif //USE_REGIONS
 37556|         }
 37557|     }
 37558| }
 37559| #ifdef TRACE_GC
 37560| void gc_heap::print_free_list (int gen, heap_segment* seg)
 37561| {
 37562|     UNREFERENCED_PARAMETER(gen);
 37563|     UNREFERENCED_PARAMETER(seg);
 37564| /*
 37565|     if (settings.concurrent == FALSE)
 37566|     {
 37567|         uint8_t* seg_start = heap_segment_mem (seg);
 37568|         uint8_t* seg_end = heap_segment_allocated (seg);
 37569|         dprintf (3, ("Free list in seg %zx:", seg_start));
 37570|         size_t total_free_item = 0;
 37571|         allocator* gen_allocator = generation_allocator (generation_of (gen));
 37572|         for (unsigned int b = 0; b < gen_allocator->number_of_buckets(); b++)
 37573|         {
 37574|             uint8_t* fo = gen_allocator->alloc_list_head_of (b);
 37575|             while (fo)
 37576|             {
 37577|                 if (fo >= seg_start && fo < seg_end)
 37578|                 {
 37579|                     total_free_item++;
 37580|                     size_t free_item_len = size(fo);
 37581|                     dprintf (3, ("[%zx, %zx[:%zd",
 37582|                                  (size_t)fo,
 37583|                                  (size_t)(fo + free_item_len),
 37584|                                  free_item_len));
 37585|                 }
 37586|                 fo = free_list_slot (fo);
 37587|             }
 37588|         }
 37589|         dprintf (3, ("total %zd free items", total_free_item));
 37590|     }
 37591| */
 37592| }
 37593| #endif //TRACE_GC
 37594| void gc_heap::descr_generations (const char* msg)
 37595| {
 37596| #ifndef TRACE_GC
 37597|     UNREFERENCED_PARAMETER(msg);
 37598| #endif //!TRACE_GC
 37599| #ifdef STRESS_LOG
 37600|     if (StressLog::StressLogOn(LF_GC, LL_INFO10))
 37601|     {
 37602|         gc_heap* hp = 0;
 37603| #ifdef MULTIPLE_HEAPS
 37604|         hp= this;
 37605| #endif //MULTIPLE_HEAPS
 37606|         STRESS_LOG1(LF_GC, LL_INFO10, "GC Heap %p\n", hp);
 37607|         for (int n = max_generation; n >= 0; --n)
 37608|         {
 37609| #ifndef USE_REGIONS
 37610|             STRESS_LOG4(LF_GC, LL_INFO10, "    Generation %d [%p, %p] cur = %p\n",
 37611|                     n,
 37612|                     generation_allocation_start(generation_of(n)),
 37613|                     generation_allocation_limit(generation_of(n)),
 37614|                     generation_allocation_pointer(generation_of(n)));
 37615| #endif //USE_REGIONS
 37616|             heap_segment* seg = generation_start_segment(generation_of(n));
 37617|             while (seg)
 37618|             {
 37619|                 STRESS_LOG4(LF_GC, LL_INFO10, "        Segment mem %p alloc = %p used %p committed %p\n",
 37620|                         heap_segment_mem(seg),
 37621|                         heap_segment_allocated(seg),
 37622|                         heap_segment_used(seg),
 37623|                         heap_segment_committed(seg));
 37624|                 seg = heap_segment_next(seg);
 37625|             }
 37626|         }
 37627|     }
 37628| #endif  // STRESS_LOG
 37629| #ifdef TRACE_GC
 37630|     dprintf (2, ("lowest_address: %zx highest_address: %zx",
 37631|              (size_t) lowest_address, (size_t) highest_address));
 37632| #ifdef BACKGROUND_GC
 37633|     dprintf (2, ("bgc lowest_address: %zx bgc highest_address: %zx",
 37634|              (size_t) background_saved_lowest_address, (size_t) background_saved_highest_address));
 37635| #endif //BACKGROUND_GC
 37636|     if (heap_number == 0)
 37637|     {
 37638| #ifdef USE_REGIONS
 37639|         size_t alloc_size = get_total_heap_size () / 1024 / 1024;
 37640|         size_t commit_size = get_total_committed_size () / 1024 / 1024;
 37641|         size_t frag_size = get_total_fragmentation () / 1024 / 1024;
 37642|         int total_new_gen0_regions_in_plns = get_total_new_gen0_regions_in_plns ();
 37643|         int total_new_regions_in_prr = get_total_new_regions_in_prr ();
 37644|         int total_new_regions_in_threading = get_total_new_regions_in_threading ();
 37645|         uint64_t elapsed_time_so_far = GetHighPrecisionTimeStamp () - process_start_time;
 37646|         size_t idx = VolatileLoadWithoutBarrier (&settings.gc_index);
 37647|         dprintf (REGIONS_LOG, ("[%s] GC#%5Id [%s] heap %Idmb (F: %Idmb %d%%) commit size: %Idmb, %0.3f min, %d,%d new in plan, %d in threading",
 37648|             msg, idx, (settings.promotion ? "PM" : "NPM"), alloc_size, frag_size,
 37649|             (int)((double)frag_size * 100.0 / (double)alloc_size),
 37650|             commit_size,
 37651|             (double)elapsed_time_so_far / (double)1000000 / (double)60,
 37652|             total_new_gen0_regions_in_plns, total_new_regions_in_prr, total_new_regions_in_threading));
 37653|         size_t total_gen_size_mb[loh_generation + 1] = { 0, 0, 0, 0 };
 37654|         size_t total_gen_fragmentation_mb[loh_generation + 1] = { 0, 0, 0, 0 };
 37655|         for (int i = 0; i < (loh_generation + 1); i++)
 37656|         {
 37657|             total_gen_size_mb[i] = get_total_generation_size (i) / 1024 / 1024;
 37658|             total_gen_fragmentation_mb[i] = get_total_gen_fragmentation (i) / 1024 / 1024;
 37659|         }
 37660|         int bgcs = VolatileLoadWithoutBarrier (&current_bgc_state);
 37661| #ifdef SIMPLE_DPRINTF
 37662|         dprintf (REGIONS_LOG, ("[%s] GC#%Id (bgcs: %d, %s) g0: %Idmb (f: %Idmb %d%%), g1: %Idmb (f: %Idmb %d%%), g2: %Idmb (f: %Idmb %d%%), g3: %Idmb (f: %Idmb %d%%)",
 37663|             msg, idx, bgcs, str_bgc_state[bgcs],
 37664|             total_gen_size_mb[0], total_gen_fragmentation_mb[0], (total_gen_size_mb[0] ? (int)((double)total_gen_fragmentation_mb[0] * 100.0 / (double)total_gen_size_mb[0]) : 0),
 37665|             total_gen_size_mb[1], total_gen_fragmentation_mb[1], (total_gen_size_mb[1] ? (int)((double)total_gen_fragmentation_mb[1] * 100.0 / (double)total_gen_size_mb[1]) : 0),
 37666|             total_gen_size_mb[2], total_gen_fragmentation_mb[2], (total_gen_size_mb[2] ? (int)((double)total_gen_fragmentation_mb[2] * 100.0 / (double)total_gen_size_mb[2]) : 0),
 37667|             total_gen_size_mb[3], total_gen_fragmentation_mb[3], (total_gen_size_mb[3] ? (int)((double)total_gen_fragmentation_mb[3] * 100.0 / (double)total_gen_size_mb[3]) : 0)));
 37668| #endif //SIMPLE_DPRINTF
 37669|         if ((idx % 20) == 0)
 37670|         {
 37671|             dprintf (1, ("[%5s] GC#%5Id total heap size: %Idmb (F: %Idmb %d%%) commit size: %Idmb, %0.3f min, %d,%d new in plan, %d in threading\n",
 37672|                 msg, idx, alloc_size, frag_size,
 37673|                 (int)((double)frag_size * 100.0 / (double)alloc_size),
 37674|                 commit_size,
 37675|                 (double)elapsed_time_so_far / (double)1000000 / (double)60,
 37676|                 total_new_gen0_regions_in_plns, total_new_regions_in_prr, total_new_regions_in_threading));
 37677|         }
 37678| #endif //USE_REGIONS
 37679|     }
 37680|     for (int curr_gen_number = total_generation_count - 1; curr_gen_number >= 0; curr_gen_number--)
 37681|     {
 37682|         size_t total_gen_size = generation_size (curr_gen_number);
 37683| #ifdef SIMPLE_DPRINTF
 37684|         dprintf (GTC_LOG, ("[%s][g%d]gen %d:, size: %zd, frag: %zd(L: %zd, O: %zd), f: %d%% %s %s %s",
 37685|                       msg,
 37686|                       settings.condemned_generation,
 37687|                       curr_gen_number,
 37688|                       total_gen_size,
 37689|                       dd_fragmentation (dynamic_data_of (curr_gen_number)),
 37690|                       generation_free_list_space (generation_of (curr_gen_number)),
 37691|                       generation_free_obj_space (generation_of (curr_gen_number)),
 37692|                       (total_gen_size ?
 37693|                         (int)(((double)dd_fragmentation (dynamic_data_of (curr_gen_number)) / (double)total_gen_size) * 100) :
 37694|                         0),
 37695|                       (settings.compaction ? "(compact)" : "(sweep)"),
 37696|                       (settings.heap_expansion ? "(EX)" : " "),
 37697|                       (settings.promotion ? "Promotion" : "NoPromotion")));
 37698| #else
 37699|         dprintf (2, ( "Generation %d: generation size: %zd, fragmentation: %zd",
 37700|                       curr_gen_number,
 37701|                       total_gen_size,
 37702|                       dd_fragmentation (dynamic_data_of (curr_gen_number))));
 37703| #endif //SIMPLE_DPRINTF
 37704|         generation* gen = generation_of (curr_gen_number);
 37705|         heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 37706| #ifdef USE_REGIONS
 37707|         dprintf (GTC_LOG, ("g%d: start seg: %p alloc seg: %p, tail region: %p",
 37708|             curr_gen_number,
 37709|             heap_segment_mem (seg),
 37710|             heap_segment_mem (generation_allocation_segment (gen)),
 37711|             heap_segment_mem (generation_tail_region (gen))));
 37712|         while (seg)
 37713|         {
 37714|             dprintf (GTC_LOG, ("g%d: (%d:p %d) [%zx %zx(sa: %zx, pa: %zx)[-%zx[ (%zd) (%zd)",
 37715|                                curr_gen_number,
 37716|                                heap_segment_gen_num (seg),
 37717|                                heap_segment_plan_gen_num (seg),
 37718|                                (size_t)heap_segment_mem (seg),
 37719|                                (size_t)heap_segment_allocated (seg),
 37720|                                (size_t)heap_segment_saved_allocated (seg),
 37721|                                (size_t)heap_segment_plan_allocated (seg),
 37722|                                (size_t)heap_segment_committed (seg),
 37723|                                (size_t)(heap_segment_allocated (seg) - heap_segment_mem (seg)),
 37724|                                (size_t)(heap_segment_committed (seg) - heap_segment_allocated (seg))));
 37725|             print_free_list (curr_gen_number, seg);
 37726|             seg = heap_segment_next (seg);
 37727|         }
 37728| #else
 37729|         while (seg && (seg != ephemeral_heap_segment))
 37730|         {
 37731|             dprintf (GTC_LOG, ("g%d: [%zx %zx[-%zx[ (%zd) (%zd)",
 37732|                         curr_gen_number,
 37733|                         (size_t)heap_segment_mem (seg),
 37734|                         (size_t)heap_segment_allocated (seg),
 37735|                         (size_t)heap_segment_committed (seg),
 37736|                         (size_t)(heap_segment_allocated (seg) - heap_segment_mem (seg)),
 37737|                         (size_t)(heap_segment_committed (seg) - heap_segment_allocated (seg))));
 37738|             print_free_list (curr_gen_number, seg);
 37739|             seg = heap_segment_next (seg);
 37740|         }
 37741|         if (seg && (seg != generation_start_segment (gen)))
 37742|         {
 37743|             dprintf (GTC_LOG, ("g%d: [%zx %zx[",
 37744|                          curr_gen_number,
 37745|                          (size_t)heap_segment_mem (seg),
 37746|                          (size_t)generation_allocation_start (generation_of (curr_gen_number-1))));
 37747|             print_free_list (curr_gen_number, seg);
 37748|         }
 37749|         else if (seg)
 37750|         {
 37751|             dprintf (GTC_LOG, ("g%d: [%zx %zx[",
 37752|                          curr_gen_number,
 37753|                          (size_t)generation_allocation_start (generation_of (curr_gen_number)),
 37754|                          (size_t)(((curr_gen_number == 0)) ?
 37755|                                   (heap_segment_allocated
 37756|                                    (generation_start_segment
 37757|                                     (generation_of (curr_gen_number)))) :
 37758|                                   (generation_allocation_start
 37759|                                    (generation_of (curr_gen_number - 1))))
 37760|                          ));
 37761|             print_free_list (curr_gen_number, seg);
 37762|         }
 37763| #endif //USE_REGIONS
 37764|     }
 37765| #endif //TRACE_GC
 37766| }
 37767| VOLATILE(BOOL)    GCHeap::GcInProgress            = FALSE;
 37768| GCEvent           *GCHeap::WaitForGCEvent         = NULL;
 37769| unsigned          GCHeap::GcCondemnedGeneration   = 0;
 37770| size_t            GCHeap::totalSurvivedSize       = 0;
 37771| #ifdef FEATURE_PREMORTEM_FINALIZATION
 37772| CFinalize*        GCHeap::m_Finalize              = 0;
 37773| BOOL              GCHeap::GcCollectClasses        = FALSE;
 37774| VOLATILE(int32_t) GCHeap::m_GCFLock               = 0;
 37775| #ifndef FEATURE_NATIVEAOT // NativeAOT forces relocation a different way
 37776| #ifdef STRESS_HEAP
 37777| #ifndef MULTIPLE_HEAPS
 37778| OBJECTHANDLE      GCHeap::m_StressObjs[NUM_HEAP_STRESS_OBJS];
 37779| int               GCHeap::m_CurStressObj          = 0;
 37780| #endif // !MULTIPLE_HEAPS
 37781| #endif // STRESS_HEAP
 37782| #endif // FEATURE_NATIVEAOT
 37783| #endif //FEATURE_PREMORTEM_FINALIZATION
 37784| class NoGCRegionLockHolder
 37785| {
 37786| public:
 37787|     NoGCRegionLockHolder()
 37788|     {
 37789|         enter_spin_lock_noinstru(&g_no_gc_lock);
 37790|     }
 37791|     ~NoGCRegionLockHolder()
 37792|     {
 37793|         leave_spin_lock_noinstru(&g_no_gc_lock);
 37794|     }
 37795| };
 37796| enable_no_gc_region_callback_status gc_heap::enable_no_gc_callback(NoGCRegionCallbackFinalizerWorkItem* callback, uint64_t callback_threshold)
 37797| {
 37798|     dprintf(1, ("[no_gc_callback] calling enable_no_gc_callback with callback_threshold = %llu\n", callback_threshold));
 37799|     enable_no_gc_region_callback_status status = enable_no_gc_region_callback_status::succeed;
 37800|     suspend_EE();
 37801|     {
 37802|         if (!current_no_gc_region_info.started)
 37803|         {
 37804|             status = enable_no_gc_region_callback_status::not_started;
 37805|         }
 37806|         else if (current_no_gc_region_info.callback != nullptr)
 37807|         {
 37808|             status = enable_no_gc_region_callback_status::already_registered;
 37809|         }
 37810|         else
 37811|         {
 37812|             uint64_t total_original_soh_budget = 0;
 37813|             uint64_t total_original_loh_budget = 0;
 37814| #ifdef MULTIPLE_HEAPS
 37815|             for (int i = 0; i < gc_heap::n_heaps; i++)
 37816|             {
 37817|                 gc_heap* hp = gc_heap::g_heaps [i];
 37818| #else
 37819|             {
 37820|                 gc_heap* hp = pGenGCHeap;
 37821| #endif
 37822|                 total_original_soh_budget += hp->soh_allocation_no_gc;
 37823|                 total_original_loh_budget += hp->loh_allocation_no_gc;
 37824|             }
 37825|             uint64_t total_original_budget = total_original_soh_budget + total_original_loh_budget;
 37826|             if (total_original_budget >= callback_threshold)
 37827|             {
 37828|                 uint64_t total_withheld = total_original_budget - callback_threshold;
 37829|                 float soh_ratio = ((float)total_original_soh_budget)/total_original_budget;
 37830|                 float loh_ratio = ((float)total_original_loh_budget)/total_original_budget;
 37831|                 size_t soh_withheld_budget = (size_t)(soh_ratio * total_withheld);
 37832|                 size_t loh_withheld_budget = (size_t)(loh_ratio * total_withheld);
 37833| #ifdef MULTIPLE_HEAPS
 37834|                 soh_withheld_budget = soh_withheld_budget / gc_heap::n_heaps;
 37835|                 loh_withheld_budget = loh_withheld_budget / gc_heap::n_heaps;
 37836| #endif
 37837|                 soh_withheld_budget = max(soh_withheld_budget, 1);
 37838|                 soh_withheld_budget = Align(soh_withheld_budget, get_alignment_constant (TRUE));
 37839|                 loh_withheld_budget = Align(loh_withheld_budget, get_alignment_constant (FALSE));
 37840| #ifdef MULTIPLE_HEAPS
 37841|                 for (int i = 0; i < gc_heap::n_heaps; i++)
 37842|                 {
 37843|                     gc_heap* hp = gc_heap::g_heaps [i];
 37844| #else
 37845|                 {
 37846|                     gc_heap* hp = pGenGCHeap;
 37847| #endif
 37848|                     if (dd_new_allocation (hp->dynamic_data_of (soh_gen0)) <= (ptrdiff_t)soh_withheld_budget)
 37849|                     {
 37850|                         dprintf(1, ("[no_gc_callback] failed because of running out of soh budget= %llu\n", soh_withheld_budget));
 37851|                         status = insufficient_budget;
 37852|                     }
 37853|                     if (dd_new_allocation (hp->dynamic_data_of (loh_generation)) <= (ptrdiff_t)loh_withheld_budget)
 37854|                     {
 37855|                         dprintf(1, ("[no_gc_callback] failed because of running out of loh budget= %llu\n", loh_withheld_budget));
 37856|                         status = insufficient_budget;
 37857|                     }
 37858|                 }
 37859|                 if (status == enable_no_gc_region_callback_status::succeed)
 37860|                 {
 37861|                     dprintf(1, ("[no_gc_callback] enabling succeed\n"));
 37862| #ifdef MULTIPLE_HEAPS
 37863|                     for (int i = 0; i < gc_heap::n_heaps; i++)
 37864|                     {
 37865|                         gc_heap* hp = gc_heap::g_heaps [i];
 37866| #else
 37867|                     {
 37868|                         gc_heap* hp = pGenGCHeap;
 37869| #endif
 37870|                         dd_new_allocation (hp->dynamic_data_of (soh_gen0)) -= soh_withheld_budget;
 37871|                         dd_new_allocation (hp->dynamic_data_of (loh_generation)) -= loh_withheld_budget;
 37872|                     }
 37873|                     current_no_gc_region_info.soh_withheld_budget = soh_withheld_budget;
 37874|                     current_no_gc_region_info.loh_withheld_budget = loh_withheld_budget;
 37875|                     current_no_gc_region_info.callback = callback;
 37876|                 }
 37877|             }
 37878|             else
 37879|             {
 37880|                 status = enable_no_gc_region_callback_status::insufficient_budget;
 37881|             }
 37882|         }
 37883|     }
 37884|     restart_EE();
 37885|     return status;
 37886| }
 37887| BOOL IsValidObject99(uint8_t *pObject)
 37888| {
 37889| #ifdef VERIFY_HEAP
 37890|     if (!((CObjectHeader*)pObject)->IsFree())
 37891|         ((CObjectHeader *) pObject)->Validate();
 37892| #endif //VERIFY_HEAP
 37893|     return(TRUE);
 37894| }
 37895| #ifdef BACKGROUND_GC
 37896| BOOL gc_heap::bgc_mark_array_range (heap_segment* seg,
 37897|                                     BOOL whole_seg_p,
 37898|                                     uint8_t** range_beg,
 37899|                                     uint8_t** range_end)
 37900| {
 37901|     uint8_t* seg_start = heap_segment_mem (seg);
 37902|     uint8_t* seg_end = (whole_seg_p ? heap_segment_reserved (seg) : align_on_mark_word (heap_segment_allocated (seg)));
 37903|     if ((seg_start < background_saved_highest_address) &&
 37904|         (seg_end > background_saved_lowest_address))
 37905|     {
 37906|         *range_beg = max (seg_start, background_saved_lowest_address);
 37907|         *range_end = min (seg_end, background_saved_highest_address);
 37908|         return TRUE;
 37909|     }
 37910|     else
 37911|     {
 37912|         return FALSE;
 37913|     }
 37914| }
 37915| void gc_heap::bgc_verify_mark_array_cleared (heap_segment* seg, bool always_verify_p)
 37916| {
 37917| #ifdef _DEBUG
 37918|     if (gc_heap::background_running_p() || always_verify_p)
 37919|     {
 37920|         uint8_t* range_beg = 0;
 37921|         uint8_t* range_end = 0;
 37922|         if (bgc_mark_array_range (seg, TRUE, &range_beg, &range_end) || always_verify_p)
 37923|         {
 37924|             if (always_verify_p)
 37925|             {
 37926|                 range_beg = heap_segment_mem (seg);
 37927|                 range_end = heap_segment_reserved (seg);
 37928|             }
 37929|             size_t  markw = mark_word_of (range_beg);
 37930|             size_t  markw_end = mark_word_of (range_end);
 37931|             while (markw < markw_end)
 37932|             {
 37933|                 if (mark_array [markw])
 37934|                 {
 37935|                     dprintf (1, ("The mark bits at 0x%zx:0x%u(addr: 0x%p) were not cleared",
 37936|                                     markw, mark_array [markw], mark_word_address (markw)));
 37937|                     FATAL_GC_ERROR();
 37938|                 }
 37939|                 markw++;
 37940|             }
 37941|             uint8_t* p = mark_word_address (markw_end);
 37942|             while (p < range_end)
 37943|             {
 37944|                 assert (!(mark_array_marked (p)));
 37945|                 p++;
 37946|             }
 37947|         }
 37948|     }
 37949| #endif //_DEBUG
 37950| }
 37951| void gc_heap::verify_mark_bits_cleared (uint8_t* obj, size_t s)
 37952| {
 37953| #ifdef VERIFY_HEAP
 37954|     size_t start_mark_bit = mark_bit_of (obj) + 1;
 37955|     size_t end_mark_bit = mark_bit_of (obj + s);
 37956|     unsigned int startbit = mark_bit_bit (start_mark_bit);
 37957|     unsigned int endbit = mark_bit_bit (end_mark_bit);
 37958|     size_t startwrd = mark_bit_word (start_mark_bit);
 37959|     size_t endwrd = mark_bit_word (end_mark_bit);
 37960|     unsigned int result = 0;
 37961|     unsigned int firstwrd = ~(lowbits (~0, startbit));
 37962|     unsigned int lastwrd = ~(highbits (~0, endbit));
 37963|     if (startwrd == endwrd)
 37964|     {
 37965|         unsigned int wrd = firstwrd & lastwrd;
 37966|         result = mark_array[startwrd] & wrd;
 37967|         if (result)
 37968|         {
 37969|             FATAL_GC_ERROR();
 37970|         }
 37971|         return;
 37972|     }
 37973|     if (startbit)
 37974|     {
 37975|         result = mark_array[startwrd] & firstwrd;
 37976|         if (result)
 37977|         {
 37978|             FATAL_GC_ERROR();
 37979|         }
 37980|         startwrd++;
 37981|     }
 37982|     for (size_t wrdtmp = startwrd; wrdtmp < endwrd; wrdtmp++)
 37983|     {
 37984|         result = mark_array[wrdtmp];
 37985|         if (result)
 37986|         {
 37987|             FATAL_GC_ERROR();
 37988|         }
 37989|     }
 37990|     if (endbit)
 37991|     {
 37992|         result = mark_array[endwrd] & lastwrd;
 37993|         if (result)
 37994|         {
 37995|             FATAL_GC_ERROR();
 37996|         }
 37997|     }
 37998| #endif //VERIFY_HEAP
 37999| }
 38000| void gc_heap::clear_all_mark_array()
 38001| {
 38002|     for (int i = get_start_generation_index(); i < total_generation_count; i++)
 38003|     {
 38004|         generation* gen = generation_of (i);
 38005|         heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 38006|         while (seg)
 38007|         {
 38008|             uint8_t* range_beg = 0;
 38009|             uint8_t* range_end = 0;
 38010|             if (bgc_mark_array_range (seg, (seg == ephemeral_heap_segment), &range_beg, &range_end))
 38011|             {
 38012|                 size_t markw = mark_word_of (range_beg);
 38013|                 size_t markw_end = mark_word_of (range_end);
 38014|                 size_t size_total = (markw_end - markw) * sizeof (uint32_t);
 38015|                 size_t size = 0;
 38016|                 size_t size_left = 0;
 38017|                 assert (((size_t)&mark_array[markw] & (sizeof(PTR_PTR)-1)) == 0);
 38018|                 if ((size_total & (sizeof(PTR_PTR) - 1)) != 0)
 38019|                 {
 38020|                     size = (size_total & ~(sizeof(PTR_PTR) - 1));
 38021|                     size_left = size_total - size;
 38022|                     assert ((size_left & (sizeof (uint32_t) - 1)) == 0);
 38023|                 }
 38024|                 else
 38025|                 {
 38026|                     size = size_total;
 38027|                 }
 38028|                 memclr ((uint8_t*)&mark_array[markw], size);
 38029|                 if (size_left != 0)
 38030|                 {
 38031|                     uint32_t* markw_to_clear = &mark_array[markw + size / sizeof (uint32_t)];
 38032|                     for (size_t i = 0; i < (size_left / sizeof (uint32_t)); i++)
 38033|                     {
 38034|                         *markw_to_clear = 0;
 38035|                         markw_to_clear++;
 38036|                     }
 38037|                 }
 38038|             }
 38039|             seg = heap_segment_next_rw (seg);
 38040|         }
 38041|     }
 38042| }
 38043| void gc_heap::verify_mark_array_cleared()
 38044| {
 38045| #ifdef VERIFY_HEAP
 38046|     if (gc_heap::background_running_p() &&
 38047|         (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC))
 38048|     {
 38049|         for (int i = get_start_generation_index(); i < total_generation_count; i++)
 38050|         {
 38051|             generation* gen = generation_of (i);
 38052|             heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 38053|             while (seg)
 38054|             {
 38055|                 bgc_verify_mark_array_cleared (seg);
 38056|                 seg = heap_segment_next_rw (seg);
 38057|             }
 38058|         }
 38059|     }
 38060| #endif //VERIFY_HEAP
 38061| }
 38062| #endif //BACKGROUND_GC
 38063| void gc_heap::verify_soh_segment_list()
 38064| {
 38065| #ifdef VERIFY_HEAP
 38066|     if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
 38067|     {
 38068|         for (int i = get_start_generation_index(); i <= max_generation; i++)
 38069|         {
 38070|             generation* gen = generation_of (i);
 38071|             heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 38072|             heap_segment* last_seg = 0;
 38073|             while (seg)
 38074|             {
 38075|                 last_seg = seg;
 38076|                 seg = heap_segment_next_rw (seg);
 38077|             }
 38078| #ifdef USE_REGIONS
 38079|             if (last_seg != generation_tail_region (gen))
 38080| #else
 38081|             if (last_seg != ephemeral_heap_segment)
 38082| #endif //USE_REGIONS
 38083|             {
 38084|                 FATAL_GC_ERROR();
 38085|             }
 38086|         }
 38087|     }
 38088| #endif //VERIFY_HEAP
 38089| }
 38090| #ifdef BACKGROUND_GC
 38091| void gc_heap::verify_partial()
 38092| {
 38093|     BOOL mark_missed_p = FALSE;
 38094|     BOOL bad_ref_p = FALSE;
 38095|     BOOL free_ref_p = FALSE;
 38096|     for (int i = get_start_generation_index(); i < total_generation_count; i++)
 38097|     {
 38098|         generation* gen = generation_of (i);
 38099|         int align_const = get_alignment_constant (i == max_generation);
 38100|         heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 38101|         while (seg)
 38102|         {
 38103|             uint8_t* o = heap_segment_mem (seg);
 38104|             uint8_t* end = heap_segment_allocated (seg);
 38105|             while (o < end)
 38106|             {
 38107|                 size_t s = size (o);
 38108|                 BOOL marked_p = background_object_marked (o, FALSE);
 38109|                 if (marked_p)
 38110|                 {
 38111|                     go_through_object_cl (method_table (o), o, s, oo,
 38112|                         {
 38113|                             if (*oo)
 38114|                             {
 38115|                                 MethodTable *pMT = method_table (*oo);
 38116|                                 if (pMT == g_gc_pFreeObjectMethodTable)
 38117|                                 {
 38118|                                     free_ref_p = TRUE;
 38119|                                     FATAL_GC_ERROR();
 38120|                                 }
 38121|                                 if (!pMT->SanityCheck())
 38122|                                 {
 38123|                                     bad_ref_p = TRUE;
 38124|                                     dprintf (1, ("Bad member of %zx %zx",
 38125|                                                 (size_t)oo, (size_t)*oo));
 38126|                                     FATAL_GC_ERROR();
 38127|                                 }
 38128|                                 if (current_bgc_state == bgc_final_marking)
 38129|                                 {
 38130|                                     if (marked_p && !background_object_marked (*oo, FALSE))
 38131|                                     {
 38132|                                         mark_missed_p = TRUE;
 38133|                                         FATAL_GC_ERROR();
 38134|                                     }
 38135|                                 }
 38136|                             }
 38137|                         }
 38138|                     );
 38139|                 }
 38140|                 o = o + Align(s, align_const);
 38141|             }
 38142|             seg = heap_segment_next_rw (seg);
 38143|         }
 38144|     }
 38145| }
 38146| #endif //BACKGROUND_GC
 38147| #ifdef VERIFY_HEAP
 38148| void
 38149| gc_heap::verify_free_lists ()
 38150| {
 38151|     for (int gen_num = 0; gen_num < total_generation_count; gen_num++)
 38152|     {
 38153|         dprintf (3, ("Verifying free list for gen:%d", gen_num));
 38154|         allocator* gen_alloc = generation_allocator (generation_of (gen_num));
 38155|         size_t sz = gen_alloc->first_bucket_size();
 38156|         bool verify_undo_slot = (gen_num != 0) && (gen_num <= max_generation) && !gen_alloc->discard_if_no_fit_p();
 38157|         for (unsigned int a_l_number = 0; a_l_number < gen_alloc->number_of_buckets(); a_l_number++)
 38158|         {
 38159|             uint8_t* free_list = gen_alloc->alloc_list_head_of (a_l_number);
 38160|             uint8_t* prev = 0;
 38161|             while (free_list)
 38162|             {
 38163|                 if (!((CObjectHeader*)free_list)->IsFree())
 38164|                 {
 38165|                     dprintf (1, ("Verifiying Heap: curr free list item %zx isn't a free object)",
 38166|                                  (size_t)free_list));
 38167|                     FATAL_GC_ERROR();
 38168|                 }
 38169|                 if (((a_l_number < (gen_alloc->number_of_buckets()-1))&& (unused_array_size (free_list) >= sz))
 38170|                     || ((a_l_number != 0) && (unused_array_size (free_list) < sz/2)))
 38171|                 {
 38172|                     dprintf (1, ("Verifiying Heap: curr free list item %zx isn't in the right bucket",
 38173|                                  (size_t)free_list));
 38174|                     FATAL_GC_ERROR();
 38175|                 }
 38176|                 if (verify_undo_slot && (free_list_undo (free_list) != UNDO_EMPTY))
 38177|                 {
 38178|                     dprintf (1, ("Verifiying Heap: curr free list item %zx has non empty undo slot",
 38179|                                  (size_t)free_list));
 38180|                     FATAL_GC_ERROR();
 38181|                 }
 38182|                 if ((gen_num <= max_generation) && (object_gennum (free_list)!= gen_num))
 38183|                 {
 38184|                     dprintf (1, ("Verifiying Heap: curr free list item %zx is in the wrong generation free list",
 38185|                                  (size_t)free_list));
 38186|                     FATAL_GC_ERROR();
 38187|                 }
 38188| #ifdef DOUBLY_LINKED_FL
 38189|                 uint8_t* prev_free_item = free_list_prev (free_list);
 38190|                 if (gen_num == max_generation)
 38191|                 {
 38192|                     if (prev_free_item != prev)
 38193|                     {
 38194|                         dprintf (1, ("%p prev should be: %p, actual: %p", free_list, prev_free_item, prev));
 38195|                         FATAL_GC_ERROR();
 38196|                     }
 38197|                 }
 38198| #endif //DOUBLY_LINKED_FL
 38199| #if defined(USE_REGIONS) && defined(MULTIPLE_HEAPS)
 38200|                 heap_segment* region = region_of (free_list);
 38201|                 if (region->heap != this)
 38202|                 {
 38203|                     dprintf (1, ("curr free item %p should be on heap %d, but actually is on heap %d: %d", free_list, this->heap_number, region->heap->heap_number));
 38204|                     FATAL_GC_ERROR();
 38205|                 }
 38206| #endif //USE_REGIONS && MULTIPLE_HEAPS
 38207|                 prev = free_list;
 38208|                 free_list = free_list_slot (free_list);
 38209|             }
 38210|             uint8_t* tail = gen_alloc->alloc_list_tail_of (a_l_number);
 38211|             if (!((tail == 0) || (tail == prev)))
 38212|             {
 38213|                 dprintf (1, ("Verifying Heap: tail of free list is not correct, tail %p, prev %p", tail, prev));
 38214|                 FATAL_GC_ERROR();
 38215|             }
 38216|             if (tail == 0)
 38217|             {
 38218|                 uint8_t* head = gen_alloc->alloc_list_head_of (a_l_number);
 38219|                 if ((head != 0) && (free_list_slot (head) != 0))
 38220|                 {
 38221|                     dprintf (1, ("Verifying Heap: head of free list is not correct, head %p -> %p",
 38222|                         head, free_list_slot (head)));
 38223|                     FATAL_GC_ERROR();
 38224|                 }
 38225|             }
 38226|             sz *=2;
 38227|         }
 38228|     }
 38229| }
 38230| void gc_heap::verify_regions (int gen_number, bool can_verify_gen_num, bool can_verify_tail, size_t* p_total_committed)
 38231| {
 38232| #ifdef USE_REGIONS
 38233|     generation* gen = generation_of (gen_number);
 38234|     int num_regions_in_gen = 0;
 38235|     heap_segment* seg_in_gen = heap_segment_rw (generation_start_segment (gen));
 38236|     heap_segment* prev_region_in_gen = 0;
 38237|     heap_segment* tail_region = generation_tail_region (gen);
 38238|     while (seg_in_gen)
 38239|     {
 38240|         if (p_total_committed)
 38241|         {
 38242|             if (!heap_segment_read_only_p (seg_in_gen))
 38243|             {
 38244|                 *p_total_committed += (heap_segment_committed (seg_in_gen) - get_region_start (seg_in_gen));
 38245|             }
 38246|         }
 38247|         if (can_verify_gen_num)
 38248|         {
 38249|             if (heap_segment_gen_num (seg_in_gen) != min (gen_number, max_generation))
 38250|             {
 38251|                 dprintf (REGIONS_LOG, ("h%d gen%d region %p(%p) gen is %d!",
 38252|                     heap_number, gen_number, seg_in_gen, heap_segment_mem (seg_in_gen),
 38253|                     heap_segment_gen_num (seg_in_gen)));
 38254|                 FATAL_GC_ERROR();
 38255|             }
 38256|             if (heap_segment_gen_num (seg_in_gen) != heap_segment_plan_gen_num (seg_in_gen))
 38257|             {
 38258|                 dprintf (REGIONS_LOG, ("h%d gen%d region %p(%p) gen is %d but plan gen is %d!!",
 38259|                     heap_number, gen_number, seg_in_gen, heap_segment_mem (seg_in_gen),
 38260|                     heap_segment_gen_num (seg_in_gen), heap_segment_plan_gen_num (seg_in_gen)));
 38261|                 FATAL_GC_ERROR();
 38262|             }
 38263|         }
 38264|         if (heap_segment_allocated (seg_in_gen) > heap_segment_reserved (seg_in_gen))
 38265|         {
 38266|             dprintf (REGIONS_LOG, ("h%d gen%d region %p alloc %p > reserved %p!!",
 38267|                 heap_number, gen_number, heap_segment_mem (seg_in_gen),
 38268|                 heap_segment_allocated (seg_in_gen), heap_segment_reserved (seg_in_gen)));
 38269|             FATAL_GC_ERROR();
 38270|         }
 38271|         prev_region_in_gen = seg_in_gen;
 38272|         num_regions_in_gen++;
 38273|         heap_segment* next_region = heap_segment_next (seg_in_gen);
 38274|         if (seg_in_gen == next_region)
 38275|         {
 38276|             dprintf (REGIONS_LOG, ("h%d gen%d region %p(%p) pointing to itself!!",
 38277|                 heap_number, gen_number, seg_in_gen, heap_segment_mem (seg_in_gen)));
 38278|             FATAL_GC_ERROR();
 38279|         }
 38280|         seg_in_gen = next_region;
 38281|     }
 38282|     if (num_regions_in_gen == 0)
 38283|     {
 38284|         dprintf (REGIONS_LOG, ("h%d gen%d has no regions!!", heap_number, gen_number));
 38285|         FATAL_GC_ERROR();
 38286|     }
 38287|     if (can_verify_tail && (tail_region != prev_region_in_gen))
 38288|     {
 38289|         dprintf (REGIONS_LOG, ("h%d gen%d tail region is %p(%p), diff from last region %p(%p)!!",
 38290|             heap_number, gen_number,
 38291|             tail_region, heap_segment_mem (tail_region),
 38292|             prev_region_in_gen, heap_segment_mem (prev_region_in_gen)));
 38293|         FATAL_GC_ERROR();
 38294|     }
 38295| #endif //USE_REGIONS
 38296| }
 38297| inline bool is_user_alloc_gen (int gen_number)
 38298| {
 38299|     return ((gen_number == soh_gen0) || (gen_number == loh_generation) || (gen_number == poh_generation));
 38300| }
 38301| void gc_heap::verify_regions (bool can_verify_gen_num, bool concurrent_p)
 38302| {
 38303| #ifdef USE_REGIONS
 38304|     size_t total_committed = 0;
 38305|     for (int i = 0; i < total_generation_count; i++)
 38306|     {
 38307|         bool can_verify_tail = (concurrent_p ? !is_user_alloc_gen (i) : true);
 38308|         verify_regions (i, can_verify_gen_num, can_verify_tail, &total_committed);
 38309|         if (can_verify_gen_num &&
 38310|             can_verify_tail &&
 38311|             (i >= max_generation) &&
 38312|             heap_hard_limit)
 38313|         {
 38314|             int oh = i - max_generation;
 38315| #ifdef BACKGROUND_GC
 38316|             if (oh == soh)
 38317|             {
 38318|                 heap_segment* freeable = freeable_soh_segment;
 38319|                 while (freeable)
 38320|                 {
 38321|                     total_committed += (heap_segment_committed (freeable) - get_region_start (freeable));
 38322|                     freeable = heap_segment_next (freeable);
 38323|                 }
 38324|             }
 38325|             else
 38326|             {
 38327|                 heap_segment* freeable = freeable_uoh_segment;
 38328|                 while (freeable)
 38329|                 {
 38330|                     if (heap_segment_oh (freeable) == oh)
 38331|                     {
 38332|                         total_committed += (heap_segment_committed (freeable) - get_region_start (freeable));
 38333|                     }
 38334|                     freeable = heap_segment_next (freeable);
 38335|                 }
 38336|             }
 38337| #endif //BACKGROUND_GC
 38338| #ifdef MULTIPLE_HEAPS
 38339| #ifdef _DEBUG
 38340|             size_t total_accounted = committed_by_oh_per_heap[i - max_generation];
 38341| #else // _DEBUG
 38342|             size_t total_accounted = total_committed;
 38343| #endif // _DEBUG
 38344| #else // MULTIPLE_HEAPS
 38345|             size_t total_accounted = committed_by_oh[i - max_generation];
 38346| #endif // MULTIPLE_HEAPS
 38347|             if (total_committed != total_accounted)
 38348|             {
 38349|                 FATAL_GC_ERROR();
 38350|             }
 38351|             dprintf(3, ("commit-accounting:  checkpoint for %d for heap %d", oh, heap_number));
 38352|             total_committed = 0;
 38353|         }
 38354|     }
 38355| #endif //USE_REGIONS
 38356| }
 38357| BOOL gc_heap::check_need_card (uint8_t* child_obj, int gen_num_for_cards,
 38358|                                uint8_t* low, uint8_t* high)
 38359| {
 38360| #ifdef USE_REGIONS
 38361|     return (is_in_heap_range (child_obj) && (get_region_gen_num (child_obj) < gen_num_for_cards));
 38362| #else
 38363|     return ((child_obj < high) && (child_obj >= low));
 38364| #endif //USE_REGIONS
 38365| }
 38366| void gc_heap::enter_gc_lock_for_verify_heap()
 38367| {
 38368| #ifdef VERIFY_HEAP
 38369|     if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
 38370|     {
 38371|         enter_spin_lock (&gc_heap::gc_lock);
 38372|         dprintf (SPINLOCK_LOG, ("enter gc_lock for verify_heap"));
 38373|     }
 38374| #endif // VERIFY_HEAP
 38375| }
 38376| void gc_heap::leave_gc_lock_for_verify_heap()
 38377| {
 38378| #ifdef VERIFY_HEAP
 38379|     if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
 38380|     {
 38381|         dprintf (SPINLOCK_LOG, ("leave gc_lock taken for verify_heap"));
 38382|         leave_spin_lock (&gc_heap::gc_lock);
 38383|     }
 38384| #endif // VERIFY_HEAP
 38385| }
 38386| void gc_heap::verify_heap (BOOL begin_gc_p)
 38387| {
 38388|     int heap_verify_level = static_cast<int>(GCConfig::GetHeapVerifyLevel());
 38389| #ifdef MULTIPLE_HEAPS
 38390|     t_join* current_join = &gc_t_join;
 38391| #ifdef BACKGROUND_GC
 38392|     if (settings.concurrent && (bgc_thread_id.IsCurrentThread()))
 38393|     {
 38394|         current_join = &bgc_t_join;
 38395|     }
 38396| #endif //BACKGROUND_GC
 38397| #endif //MULTIPLE_HEAPS
 38398| #ifndef TRACE_GC
 38399|     UNREFERENCED_PARAMETER(begin_gc_p);
 38400| #endif //!TRACE_GC
 38401| #ifdef BACKGROUND_GC
 38402|     dprintf (2,("[%s]GC#%zu(%s): Verifying heap - begin",
 38403|         (begin_gc_p ? "BEG" : "END"),
 38404|         VolatileLoad(&settings.gc_index),
 38405|         (settings.concurrent ? "BGC" : (gc_heap::background_running_p() ? "FGC" : "NGC"))));
 38406| #else
 38407|     dprintf (2,("[%s]GC#%zu: Verifying heap - begin",
 38408|                 (begin_gc_p ? "BEG" : "END"), VolatileLoad(&settings.gc_index)));
 38409| #endif //BACKGROUND_GC
 38410| #ifndef MULTIPLE_HEAPS
 38411| #ifndef USE_REGIONS
 38412|     if ((ephemeral_low != generation_allocation_start (generation_of (max_generation - 1))) ||
 38413|         (ephemeral_high != heap_segment_reserved (ephemeral_heap_segment)))
 38414|     {
 38415|         FATAL_GC_ERROR();
 38416|     }
 38417| #endif //!USE_REGIONS
 38418| #endif //MULTIPLE_HEAPS
 38419| #ifdef BACKGROUND_GC
 38420|     if (!settings.concurrent)
 38421| #endif //BACKGROUND_GC
 38422|     {
 38423|         if (!(heap_verify_level & GCConfig::HEAPVERIFY_NO_MEM_FILL))
 38424|         {
 38425|             for (int i = get_start_generation_index(); i < total_generation_count; i++)
 38426|             {
 38427|                 generation* gen1 = generation_of (i);
 38428|                 heap_segment* seg1 = heap_segment_rw (generation_start_segment (gen1));
 38429|                 while (seg1)
 38430|                 {
 38431|                     uint8_t* clear_start = heap_segment_allocated (seg1) - plug_skew;
 38432|                     if (heap_segment_used (seg1) > clear_start)
 38433|                     {
 38434|                         dprintf (3, ("setting end of seg %p: [%p-[%p to 0xaa",
 38435|                             heap_segment_mem (seg1),
 38436|                             clear_start ,
 38437|                             heap_segment_used (seg1)));
 38438|                         memset (heap_segment_allocated (seg1) - plug_skew, 0xaa,
 38439|                             (heap_segment_used (seg1) - clear_start));
 38440|                     }
 38441|                     seg1 = heap_segment_next_rw (seg1);
 38442|                 }
 38443|             }
 38444|         }
 38445|     }
 38446| #ifdef MULTIPLE_HEAPS
 38447|     current_join->join(this, gc_join_verify_copy_table);
 38448|     if (current_join->joined())
 38449|     {
 38450|         for (int i = 0; i < n_heaps; i++)
 38451|         {
 38452|             if (g_gc_card_table != g_heaps[i]->card_table)
 38453|             {
 38454|                 g_heaps[i]->copy_brick_card_table();
 38455|             }
 38456|         }
 38457|         current_join->restart();
 38458|     }
 38459| #else
 38460|         if (g_gc_card_table != card_table)
 38461|             copy_brick_card_table();
 38462| #endif //MULTIPLE_HEAPS
 38463|     {
 38464| #ifdef USE_REGIONS
 38465|         verify_regions (true, settings.concurrent);
 38466| #else //USE_REGIONS
 38467|         generation* gen = generation_of (max_generation);
 38468|         assert (generation_allocation_start (gen) ==
 38469|                 heap_segment_mem (heap_segment_rw (generation_start_segment (gen))));
 38470|         int gen_num = max_generation-1;
 38471|         generation* prev_gen = gen;
 38472|         while (gen_num >= 0)
 38473|         {
 38474|             gen = generation_of (gen_num);
 38475|             assert (generation_allocation_segment (gen) == ephemeral_heap_segment);
 38476|             assert (generation_allocation_start (gen) >= heap_segment_mem (ephemeral_heap_segment));
 38477|             assert (generation_allocation_start (gen) < heap_segment_allocated (ephemeral_heap_segment));
 38478|             if (generation_start_segment (prev_gen ) ==
 38479|                 generation_start_segment (gen))
 38480|             {
 38481|                 assert (generation_allocation_start (prev_gen) <
 38482|                         generation_allocation_start (gen));
 38483|             }
 38484|             prev_gen = gen;
 38485|             gen_num--;
 38486|         }
 38487| #endif //USE_REGIONS
 38488|     }
 38489|     size_t          total_objects_verified = 0;
 38490|     size_t          total_objects_verified_deep = 0;
 38491|     BOOL            bCurrentBrickInvalid = FALSE;
 38492|     size_t          last_valid_brick = 0;
 38493|     size_t          curr_brick = 0;
 38494|     size_t          prev_brick = (size_t)-1;
 38495|     int             gen_num_for_cards = 0;
 38496| #ifdef USE_REGIONS
 38497|     int             gen_num_to_stop = 0;
 38498|     uint8_t*        e_high = 0;
 38499|     uint8_t*        next_boundary = 0;
 38500| #else //USE_REGIONS
 38501|     int gen_num_to_stop = max_generation;
 38502|     uint8_t*        e_high = ephemeral_high;
 38503|     uint8_t*        next_boundary = generation_allocation_start (generation_of (max_generation - 1));
 38504|     uint8_t*        begin_youngest = generation_allocation_start(generation_of(0));
 38505| #endif //!USE_REGIONS
 38506|     for (int curr_gen_num = total_generation_count - 1; curr_gen_num >= gen_num_to_stop; curr_gen_num--)
 38507|     {
 38508|         int             align_const = get_alignment_constant (curr_gen_num == max_generation);
 38509|         BOOL            large_brick_p = (curr_gen_num > max_generation);
 38510| #ifdef USE_REGIONS
 38511|         gen_num_for_cards = ((curr_gen_num >= max_generation) ? max_generation : curr_gen_num);
 38512| #endif //USE_REGIONS
 38513|         heap_segment*   seg = heap_segment_in_range (generation_start_segment (generation_of (curr_gen_num) ));
 38514|         while (seg)
 38515|         {
 38516|             uint8_t*        curr_object = heap_segment_mem (seg);
 38517|             uint8_t*        prev_object = 0;
 38518|             bool verify_bricks_p = true;
 38519| #ifdef USE_REGIONS
 38520|             if (heap_segment_read_only_p(seg))
 38521|             {
 38522|                 dprintf(1, ("seg %zx is ro! Shouldn't happen with regions", (size_t)seg));
 38523|                 FATAL_GC_ERROR();
 38524|             }
 38525|             if (heap_segment_gen_num (seg) != heap_segment_plan_gen_num (seg))
 38526|             {
 38527|                 dprintf (1, ("Seg %p, gen num is %d, plan gen num is %d",
 38528|                     heap_segment_mem (seg), heap_segment_gen_num (seg), heap_segment_plan_gen_num (seg)));
 38529|                 FATAL_GC_ERROR();
 38530|             }
 38531| #else //USE_REGIONS
 38532|             if (heap_segment_read_only_p(seg))
 38533|             {
 38534|                 size_t current_brick = brick_of(max(heap_segment_mem(seg), lowest_address));
 38535|                 size_t end_brick = brick_of(min(heap_segment_reserved(seg), highest_address) - 1);
 38536|                 while (current_brick <= end_brick)
 38537|                 {
 38538|                     if (brick_table[current_brick] != 0)
 38539|                     {
 38540|                         dprintf(1, ("Verifying Heap: %zx brick of a frozen segment is not zeroed", current_brick));
 38541|                         FATAL_GC_ERROR();
 38542|                     }
 38543|                     current_brick++;
 38544|                 }
 38545|                 verify_bricks_p = false;
 38546|             }
 38547| #endif //USE_REGIONS
 38548| #ifdef BACKGROUND_GC
 38549|             BOOL consider_bgc_mark_p    = FALSE;
 38550|             BOOL check_current_sweep_p  = FALSE;
 38551|             BOOL check_saved_sweep_p    = FALSE;
 38552|             should_check_bgc_mark (seg, &consider_bgc_mark_p, &check_current_sweep_p, &check_saved_sweep_p);
 38553| #endif //BACKGROUND_GC
 38554|             while (curr_object < heap_segment_allocated (seg))
 38555|             {
 38556|                 if (is_mark_set (curr_object))
 38557|                 {
 38558|                     dprintf (1, ("curr_object: %zx is marked!",(size_t)curr_object));
 38559|                     FATAL_GC_ERROR();
 38560|                 }
 38561|                 size_t s = size (curr_object);
 38562|                 dprintf (3, ("o: %zx, s: %zu", (size_t)curr_object, s));
 38563|                 if (s == 0)
 38564|                 {
 38565|                     dprintf (1, ("Verifying Heap: size of current object %p == 0", curr_object));
 38566|                     FATAL_GC_ERROR();
 38567|                 }
 38568| #ifndef USE_REGIONS
 38569|                 if (seg == ephemeral_heap_segment)
 38570|                 {
 38571|                     if ((curr_gen_num > 0) && (curr_object >= next_boundary))
 38572|                     {
 38573|                         curr_gen_num--;
 38574|                         if (curr_gen_num > 0)
 38575|                         {
 38576|                             next_boundary = generation_allocation_start (generation_of (curr_gen_num - 1));
 38577|                         }
 38578|                     }
 38579|                 }
 38580| #endif //!USE_REGIONS
 38581| #ifdef USE_REGIONS
 38582|                 if (verify_bricks_p && curr_gen_num != 0)
 38583| #else
 38584|                 if (verify_bricks_p && ((seg != ephemeral_heap_segment) ||
 38585|                      (brick_of(curr_object) < brick_of(begin_youngest))))
 38586| #endif //USE_REGIONS
 38587|                 {
 38588|                     curr_brick = brick_of(curr_object);
 38589|                     if (curr_brick != prev_brick)
 38590|                     {
 38591|                         if (bCurrentBrickInvalid &&
 38592|                             (curr_brick != brick_of (heap_segment_mem (seg))) &&
 38593|                             !heap_segment_read_only_p (seg))
 38594|                         {
 38595|                             dprintf (1, ("curr brick %zx invalid", curr_brick));
 38596|                             FATAL_GC_ERROR();
 38597|                         }
 38598|                         if (large_brick_p)
 38599|                         {
 38600|                             if ((heap_segment_reserved (seg) <= highest_address) &&
 38601|                                 (heap_segment_mem (seg) >= lowest_address) &&
 38602|                                 brick_table [curr_brick] != 0)
 38603|                             {
 38604|                                 dprintf (1, ("curr_brick %zx for large object %zx is set to %zx",
 38605|                                     curr_brick, (size_t)curr_object, (size_t)brick_table[curr_brick]));
 38606|                                 FATAL_GC_ERROR();
 38607|                             }
 38608|                             else
 38609|                             {
 38610|                                 bCurrentBrickInvalid = FALSE;
 38611|                             }
 38612|                         }
 38613|                         else
 38614|                         {
 38615|                             if (brick_table [curr_brick] <= 0)
 38616|                             {
 38617|                                 if (brick_table [curr_brick] == 0)
 38618|                                 {
 38619|                                     dprintf(1, ("curr_brick %zx for object %zx set to 0",
 38620|                                             curr_brick, (size_t)curr_object));
 38621|                                     FATAL_GC_ERROR();
 38622|                                 }
 38623|                                 ptrdiff_t i = curr_brick;
 38624|                                 while ((i >= ((ptrdiff_t) brick_of (heap_segment_mem (seg)))) &&
 38625|                                        (brick_table[i] < 0))
 38626|                                 {
 38627|                                     i = i + brick_table[i];
 38628|                                 }
 38629|                                 if (i <  ((ptrdiff_t)(brick_of (heap_segment_mem (seg))) - 1))
 38630|                                 {
 38631|                                     dprintf (1, ("ptrdiff i: %zx < brick_of (heap_segment_mem (seg)):%zx - 1. curr_brick: %zx",
 38632|                                             i, brick_of (heap_segment_mem (seg)),
 38633|                                             curr_brick));
 38634|                                     FATAL_GC_ERROR();
 38635|                                 }
 38636|                                 bCurrentBrickInvalid = FALSE;
 38637|                             }
 38638|                             else if (!heap_segment_read_only_p (seg))
 38639|                             {
 38640|                                 bCurrentBrickInvalid = TRUE;
 38641|                             }
 38642|                         }
 38643|                     }
 38644|                     if (bCurrentBrickInvalid)
 38645|                     {
 38646|                         if (curr_object == (brick_address(curr_brick) + brick_table[curr_brick] - 1))
 38647|                         {
 38648|                             bCurrentBrickInvalid = FALSE;
 38649|                             last_valid_brick = curr_brick;
 38650|                         }
 38651|                     }
 38652|                 }
 38653|                 if (*((uint8_t**)curr_object) != (uint8_t *) g_gc_pFreeObjectMethodTable)
 38654|                 {
 38655| #ifdef FEATURE_LOH_COMPACTION
 38656|                     if ((curr_gen_num == loh_generation) && (prev_object != 0))
 38657|                     {
 38658|                         assert (method_table (prev_object) == g_gc_pFreeObjectMethodTable);
 38659|                     }
 38660| #endif //FEATURE_LOH_COMPACTION
 38661|                     total_objects_verified++;
 38662|                     BOOL can_verify_deep = TRUE;
 38663| #ifdef BACKGROUND_GC
 38664|                     can_verify_deep = fgc_should_consider_object (curr_object, seg, consider_bgc_mark_p, check_current_sweep_p, check_saved_sweep_p);
 38665| #endif //BACKGROUND_GC
 38666|                     BOOL deep_verify_obj = can_verify_deep;
 38667|                     if ((heap_verify_level & GCConfig::HEAPVERIFY_DEEP_ON_COMPACT) && !settings.compaction)
 38668|                         deep_verify_obj = FALSE;
 38669|                     ((CObjectHeader*)curr_object)->ValidateHeap(deep_verify_obj);
 38670|                     if (can_verify_deep)
 38671|                     {
 38672|                         if (curr_gen_num > 0)
 38673|                         {
 38674|                             BOOL need_card_p = FALSE;
 38675|                             if (contain_pointers_or_collectible (curr_object))
 38676|                             {
 38677|                                 dprintf (4, ("curr_object: %zx", (size_t)curr_object));
 38678|                                 size_t crd = card_of (curr_object);
 38679|                                 BOOL found_card_p = card_set_p (crd);
 38680| #ifdef COLLECTIBLE_CLASS
 38681|                                 if (is_collectible(curr_object))
 38682|                                 {
 38683|                                     uint8_t* class_obj = get_class_object (curr_object);
 38684|                                     if (check_need_card (class_obj, gen_num_for_cards, next_boundary, e_high))
 38685|                                     {
 38686|                                         if (!found_card_p)
 38687|                                         {
 38688|                                             dprintf (1, ("Card not set, curr_object = [%zx:%zx pointing to class object %p",
 38689|                                                         card_of (curr_object), (size_t)curr_object, class_obj));
 38690|                                             FATAL_GC_ERROR();
 38691|                                         }
 38692|                                     }
 38693|                                 }
 38694| #endif //COLLECTIBLE_CLASS
 38695|                                 if (contain_pointers(curr_object))
 38696|                                 {
 38697|                                     go_through_object_nostart
 38698|                                         (method_table(curr_object), curr_object, s, oo,
 38699|                                         {
 38700|                                             if (crd != card_of ((uint8_t*)oo))
 38701|                                             {
 38702|                                                 crd = card_of ((uint8_t*)oo);
 38703|                                                 found_card_p = card_set_p (crd);
 38704|                                                 need_card_p = FALSE;
 38705|                                             }
 38706|                                             if (*oo && check_need_card (*oo, gen_num_for_cards, next_boundary, e_high))
 38707|                                             {
 38708|                                                 need_card_p = TRUE;
 38709|                                             }
 38710|                                             if (need_card_p && !found_card_p)
 38711|                                             {
 38712|                                                 dprintf (1, ("Card not set, curr_object = [%zx:%zx, %zx:%zx[",
 38713|                                                             card_of (curr_object), (size_t)curr_object,
 38714|                                                             card_of (curr_object+Align(s, align_const)),
 38715|                                                             (size_t)(curr_object+Align(s, align_const))));
 38716|                                                 FATAL_GC_ERROR();
 38717|                                             }
 38718|                                         }
 38719|                                             );
 38720|                                 }
 38721|                                 if (need_card_p && !found_card_p)
 38722|                                 {
 38723|                                     dprintf (1, ("Card not set, curr_object = [%zx:%zx, %zx:%zx[",
 38724|                                         card_of (curr_object), (size_t)curr_object,
 38725|                                         card_of (curr_object + Align(s, align_const)),
 38726|                                         (size_t)(curr_object + Align(s, align_const))));
 38727|                                     FATAL_GC_ERROR();
 38728|                                 }
 38729|                             }
 38730|                         }
 38731|                         total_objects_verified_deep++;
 38732|                     }
 38733|                 }
 38734|                 prev_object = curr_object;
 38735|                 prev_brick = curr_brick;
 38736|                 curr_object = curr_object + Align(s, align_const);
 38737|                 if (curr_object < prev_object)
 38738|                 {
 38739|                     dprintf (1, ("overflow because of a bad object size: %p size %zx", prev_object, s));
 38740|                     FATAL_GC_ERROR();
 38741|                 }
 38742|             }
 38743|             if (curr_object > heap_segment_allocated(seg))
 38744|             {
 38745|                 dprintf (1, ("Verifiying Heap: curr_object: %zx > heap_segment_allocated (seg: %zx) %p",
 38746|                         (size_t)curr_object, (size_t)seg, heap_segment_allocated (seg)));
 38747|                 FATAL_GC_ERROR();
 38748|             }
 38749|             seg = heap_segment_next_in_range (seg);
 38750|         }
 38751|     }
 38752| #ifdef BACKGROUND_GC
 38753|     dprintf (2, ("(%s)(%s)(%s) total_objects_verified is %zd, total_objects_verified_deep is %zd",
 38754|                  (settings.concurrent ? "BGC" : (gc_heap::background_running_p () ? "FGC" : "NGC")),
 38755|                  (begin_gc_p ? "BEG" : "END"),
 38756|                  ((current_c_gc_state == c_gc_state_planning) ? "in plan" : "not in plan"),
 38757|                  total_objects_verified, total_objects_verified_deep));
 38758|     if (current_c_gc_state != c_gc_state_planning)
 38759|     {
 38760|         assert (total_objects_verified == total_objects_verified_deep);
 38761|     }
 38762| #endif //BACKGROUND_GC
 38763|     verify_free_lists();
 38764| #ifdef FEATURE_PREMORTEM_FINALIZATION
 38765|     finalize_queue->CheckFinalizerObjects();
 38766| #endif // FEATURE_PREMORTEM_FINALIZATION
 38767|     {
 38768|         ScanContext sc;
 38769|         sc.thread_number = heap_number;
 38770|         sc.thread_count = n_heaps;
 38771|         GCScan::VerifyHandleTable(max_generation, max_generation, &sc);
 38772|     }
 38773| #ifdef MULTIPLE_HEAPS
 38774|     current_join->join(this, gc_join_verify_objects_done);
 38775|     if (current_join->joined())
 38776| #endif //MULTIPLE_HEAPS
 38777|     {
 38778|         GCToEEInterface::VerifySyncTableEntry();
 38779| #ifdef MULTIPLE_HEAPS
 38780| #ifdef USE_REGIONS
 38781|         for (int hn = n_heaps; hn < n_max_heaps; hn++)
 38782|         {
 38783|             gc_heap* hp = g_heaps[hn];
 38784|             hp->check_decommissioned_heap();
 38785|         }
 38786| #endif //USE_REGIONS
 38787|         current_join->restart();
 38788| #endif //MULTIPLE_HEAPS
 38789|     }
 38790| #ifdef BACKGROUND_GC
 38791|     if (settings.concurrent)
 38792|     {
 38793|         verify_mark_array_cleared();
 38794|     }
 38795|     dprintf (2,("GC%zu(%s): Verifying heap - end",
 38796|         VolatileLoad(&settings.gc_index),
 38797|         (settings.concurrent ? "BGC" : (gc_heap::background_running_p() ? "FGC" : "NGC"))));
 38798| #else
 38799|     dprintf (2,("GC#d: Verifying heap - end", VolatileLoad(&settings.gc_index)));
 38800| #endif //BACKGROUND_GC
 38801| }
 38802| #endif  //VERIFY_HEAP
 38803| void GCHeap::ValidateObjectMember (Object* obj)
 38804| {
 38805| #ifdef VERIFY_HEAP
 38806|     size_t s = size (obj);
 38807|     uint8_t* o = (uint8_t*)obj;
 38808|     go_through_object_cl (method_table (obj), o, s, oo,
 38809|         {
 38810|             uint8_t* child_o = *oo;
 38811|             if (child_o)
 38812|             {
 38813|                 MethodTable *pMT = method_table (child_o);
 38814|                 assert(pMT);
 38815|                 if (!pMT->SanityCheck()) {
 38816|                     dprintf (1, ("Bad member of %zx %zx",
 38817|                                 (size_t)oo, (size_t)child_o));
 38818|                     FATAL_GC_ERROR();
 38819|                 }
 38820|             }
 38821|         } );
 38822| #endif // VERIFY_HEAP
 38823| }
 38824| HRESULT GCHeap::StaticShutdown()
 38825| {
 38826|     deleteGCShadow();
 38827|     GCScan::GcRuntimeStructuresValid (FALSE);
 38828|     uint32_t* ct = &g_gc_card_table[card_word (gcard_of (g_gc_lowest_address))];
 38829|     if (card_table_refcount (ct) == 0)
 38830|     {
 38831|         destroy_card_table (ct);
 38832|         g_gc_card_table = nullptr;
 38833| #ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
 38834|         g_gc_card_bundle_table = nullptr;
 38835| #endif
 38836| #ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 38837|         SoftwareWriteWatch::StaticClose();
 38838| #endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
 38839|     }
 38840| #ifndef USE_REGIONS
 38841|     while(gc_heap::segment_standby_list != 0)
 38842|     {
 38843|         heap_segment* next_seg = heap_segment_next (gc_heap::segment_standby_list);
 38844| #ifdef MULTIPLE_HEAPS
 38845|         (gc_heap::g_heaps[0])->delete_heap_segment (gc_heap::segment_standby_list, FALSE);
 38846| #else //MULTIPLE_HEAPS
 38847|         pGenGCHeap->delete_heap_segment (gc_heap::segment_standby_list, FALSE);
 38848| #endif //MULTIPLE_HEAPS
 38849|         gc_heap::segment_standby_list = next_seg;
 38850|     }
 38851| #endif // USE_REGIONS
 38852| #ifdef MULTIPLE_HEAPS
 38853|     for (int i = 0; i < gc_heap::n_heaps; i ++)
 38854|     {
 38855|         gc_heap::destroy_gc_heap (gc_heap::g_heaps[i]);
 38856|     }
 38857| #else
 38858|     gc_heap::destroy_gc_heap (pGenGCHeap);
 38859| #endif //MULTIPLE_HEAPS
 38860|     gc_heap::shutdown_gc();
 38861|     return S_OK;
 38862| }
 38863| #ifdef FEATURE_PREMORTEM_FINALIZATION
 38864| static
 38865| HRESULT AllocateCFinalize(CFinalize **pCFinalize)
 38866| {
 38867|     *pCFinalize = new (nothrow) CFinalize();
 38868|     if (*pCFinalize == NULL || !(*pCFinalize)->Initialize())
 38869|         return E_OUTOFMEMORY;
 38870|     return S_OK;
 38871| }
 38872| #endif // FEATURE_PREMORTEM_FINALIZATION
 38873| HRESULT GCHeap::Init(size_t hn)
 38874| {
 38875|     HRESULT hres = S_OK;
 38876| #ifdef MULTIPLE_HEAPS
 38877|     if ((pGenGCHeap = gc_heap::make_gc_heap(this, (int)hn)) == 0)
 38878|         hres = E_OUTOFMEMORY;
 38879| #else
 38880|     UNREFERENCED_PARAMETER(hn);
 38881|     if (!gc_heap::make_gc_heap())
 38882|         hres = E_OUTOFMEMORY;
 38883| #endif //MULTIPLE_HEAPS
 38884|     return hres;
 38885| }
 38886| HRESULT GCHeap::Initialize()
 38887| {
 38888| #ifndef TRACE_GC
 38889|     STRESS_LOG_VA (1, (ThreadStressLog::gcLoggingIsOffMsg()));
 38890| #endif
 38891|     HRESULT hr = S_OK;
 38892|     qpf = (uint64_t)GCToOSInterface::QueryPerformanceFrequency();
 38893|     qpf_ms = 1000.0 / (double)qpf;
 38894|     qpf_us = 1000.0 * 1000.0 / (double)qpf;
 38895|     g_gc_pFreeObjectMethodTable = GCToEEInterface::GetFreeObjectMethodTable();
 38896|     g_num_processors = GCToOSInterface::GetTotalProcessorCount();
 38897|     assert(g_num_processors != 0);
 38898|     gc_heap::total_physical_mem = (size_t)GCConfig::GetGCTotalPhysicalMemory();
 38899|     if (gc_heap::total_physical_mem != 0)
 38900|     {
 38901|         gc_heap::is_restricted_physical_mem = true;
 38902| #ifdef FEATURE_EVENT_TRACE
 38903|         gc_heap::physical_memory_from_config = (size_t)gc_heap::total_physical_mem;
 38904| #endif //FEATURE_EVENT_TRACE
 38905|     }
 38906|     else
 38907|     {
 38908|         gc_heap::total_physical_mem = GCToOSInterface::GetPhysicalMemoryLimit (&gc_heap::is_restricted_physical_mem);
 38909|     }
 38910|     memset (gc_heap::committed_by_oh, 0, sizeof (gc_heap::committed_by_oh));
 38911|     if (!gc_heap::compute_hard_limit())
 38912|     {
 38913|         return CLR_E_GC_BAD_HARD_LIMIT;
 38914|     }
 38915|     uint32_t nhp = 1;
 38916|     uint32_t nhp_from_config = 0;
 38917|     uint32_t max_nhp_from_config = (uint32_t)GCConfig::GetMaxHeapCount();
 38918| #ifndef MULTIPLE_HEAPS
 38919|     GCConfig::SetServerGC(false);
 38920| #else //!MULTIPLE_HEAPS
 38921|     GCConfig::SetServerGC(true);
 38922|     AffinitySet config_affinity_set;
 38923|     GCConfigStringHolder cpu_index_ranges_holder(GCConfig::GetGCHeapAffinitizeRanges());
 38924|     uintptr_t config_affinity_mask = static_cast<uintptr_t>(GCConfig::GetGCHeapAffinitizeMask());
 38925|     if (!ParseGCHeapAffinitizeRanges(cpu_index_ranges_holder.Get(), &config_affinity_set, config_affinity_mask))
 38926|     {
 38927|         return CLR_E_GC_BAD_AFFINITY_CONFIG_FORMAT;
 38928|     }
 38929|     const AffinitySet* process_affinity_set = GCToOSInterface::SetGCThreadsAffinitySet(config_affinity_mask, &config_affinity_set);
 38930|     GCConfig::SetGCHeapAffinitizeMask(static_cast<int64_t>(config_affinity_mask));
 38931|     if (process_affinity_set->IsEmpty())
 38932|     {
 38933|         return CLR_E_GC_BAD_AFFINITY_CONFIG;
 38934|     }
 38935|     if ((cpu_index_ranges_holder.Get() != nullptr)
 38936| #ifdef TARGET_WINDOWS
 38937|         || (config_affinity_mask != 0)
 38938| #endif
 38939|     )
 38940|     {
 38941|         affinity_config_specified_p = true;
 38942|     }
 38943|     nhp_from_config = static_cast<uint32_t>(GCConfig::GetHeapCount());
 38944|     g_num_active_processors = min (GCToEEInterface::GetCurrentProcessCpuCount(), g_num_processors);
 38945|     if (nhp_from_config)
 38946|     {
 38947|         nhp_from_config = min (nhp_from_config, g_num_active_processors);
 38948|     }
 38949|     nhp = ((nhp_from_config == 0) ? g_num_active_processors : nhp_from_config);
 38950|     nhp = min (nhp, MAX_SUPPORTED_CPUS);
 38951|     gc_heap::gc_thread_no_affinitize_p = (gc_heap::heap_hard_limit ?
 38952|         !affinity_config_specified_p : (GCConfig::GetNoAffinitize() != 0));
 38953|     if (!(gc_heap::gc_thread_no_affinitize_p))
 38954|     {
 38955|         uint32_t num_affinitized_processors = (uint32_t)process_affinity_set->Count();
 38956|         if (num_affinitized_processors != 0)
 38957|         {
 38958|             nhp = min(nhp, num_affinitized_processors);
 38959|         }
 38960|     }
 38961| #endif //!MULTIPLE_HEAPS
 38962|     if (gc_heap::heap_hard_limit)
 38963|     {
 38964|         gc_heap::hard_limit_config_p = true;
 38965|     }
 38966|     size_t seg_size_from_config = 0;
 38967|     bool compute_memory_settings_succeed = gc_heap::compute_memory_settings(true, nhp, nhp_from_config, seg_size_from_config, 0);
 38968|     assert (compute_memory_settings_succeed);
 38969|     if ((!gc_heap::heap_hard_limit) && gc_heap::use_large_pages_p)
 38970|     {
 38971|         return CLR_E_GC_LARGE_PAGE_MISSING_HARD_LIMIT;
 38972|     }
 38973|     GCConfig::SetGCLargePages(gc_heap::use_large_pages_p);
 38974| #ifdef USE_REGIONS
 38975|     gc_heap::regions_range = (size_t)GCConfig::GetGCRegionRange();
 38976|     if (gc_heap::regions_range == 0)
 38977|     {
 38978|         if (gc_heap::heap_hard_limit)
 38979|         {
 38980|             if (gc_heap::heap_hard_limit_oh[soh])
 38981|             {
 38982|                 gc_heap::regions_range = gc_heap::heap_hard_limit;
 38983|             }
 38984|             else
 38985|             {
 38986|                 gc_heap::regions_range = ((gc_heap::use_large_pages_p) ? (2 * gc_heap::heap_hard_limit)
 38987|                                                                        : (5 * gc_heap::heap_hard_limit));
 38988|             }
 38989|         }
 38990|         else
 38991|         {
 38992|             gc_heap::regions_range = min(GCToOSInterface::GetVirtualMemoryLimit()/2, max((size_t)256 * 1024 * 1024 * 1024, (size_t)(2 * gc_heap::total_physical_mem)));
 38993|         }
 38994|         gc_heap::regions_range = align_on_page(gc_heap::regions_range);
 38995|     }
 38996|     GCConfig::SetGCRegionRange(gc_heap::regions_range);
 38997| #endif //USE_REGIONS
 38998|     size_t seg_size = 0;
 38999|     size_t large_seg_size = 0;
 39000|     size_t pin_seg_size = 0;
 39001|     seg_size = gc_heap::soh_segment_size;
 39002| #ifndef USE_REGIONS
 39003|     if (gc_heap::heap_hard_limit)
 39004|     {
 39005|         if (gc_heap::heap_hard_limit_oh[soh])
 39006|         {
 39007|             large_seg_size = max (gc_heap::adjust_segment_size_hard_limit (gc_heap::heap_hard_limit_oh[loh], nhp), seg_size_from_config);
 39008|             pin_seg_size = max (gc_heap::adjust_segment_size_hard_limit (gc_heap::heap_hard_limit_oh[poh], nhp), seg_size_from_config);
 39009|         }
 39010|         else
 39011|         {
 39012|             large_seg_size = gc_heap::use_large_pages_p ? gc_heap::soh_segment_size : gc_heap::soh_segment_size * 2;
 39013|             pin_seg_size = large_seg_size;
 39014|         }
 39015|         if (gc_heap::use_large_pages_p)
 39016|             gc_heap::min_segment_size = min_segment_size_hard_limit;
 39017|     }
 39018|     else
 39019|     {
 39020|         large_seg_size = get_valid_segment_size (TRUE);
 39021|         pin_seg_size = large_seg_size;
 39022|     }
 39023|     assert (g_theGCHeap->IsValidSegmentSize (seg_size));
 39024|     assert (g_theGCHeap->IsValidSegmentSize (large_seg_size));
 39025|     assert (g_theGCHeap->IsValidSegmentSize (pin_seg_size));
 39026|     dprintf (1, ("%d heaps, soh seg size: %zd mb, loh: %zd mb\n",
 39027|         nhp,
 39028|         (seg_size / (size_t)1024 / 1024),
 39029|         (large_seg_size / 1024 / 1024)));
 39030|     gc_heap::min_uoh_segment_size = min (large_seg_size, pin_seg_size);
 39031|     if (gc_heap::min_segment_size == 0)
 39032|     {
 39033|         gc_heap::min_segment_size = min (seg_size, gc_heap::min_uoh_segment_size);
 39034|     }
 39035| #endif //!USE_REGIONS
 39036|     GCConfig::SetHeapCount(static_cast<int64_t>(nhp));
 39037| #ifdef USE_REGIONS
 39038|     gc_heap::enable_special_regions_p = (bool)GCConfig::GetGCEnableSpecialRegions();
 39039|     size_t gc_region_size = (size_t)GCConfig::GetGCRegionSize();
 39040|     if (gc_region_size >= MAX_REGION_SIZE)
 39041|     {
 39042|         return CLR_E_GC_BAD_REGION_SIZE;
 39043|     }
 39044|     if (gc_region_size == 0)
 39045|     {
 39046|         size_t max_region_size = gc_heap::regions_range / 2 / nhp / min_regions_per_heap;
 39047|         if (max_region_size >= (4 * 1024 * 1024))
 39048|         {
 39049|             gc_region_size = 4 * 1024 * 1024;
 39050|         }
 39051|         else if (max_region_size >= (2 * 1024 * 1024))
 39052|         {
 39053|             gc_region_size = 2 * 1024 * 1024;
 39054|         }
 39055|         else
 39056|         {
 39057|             gc_region_size = 1 * 1024 * 1024;
 39058|         }
 39059|     }
 39060|     if (!power_of_two_p(gc_region_size) || ((gc_region_size * nhp * min_regions_per_heap) > gc_heap::regions_range))
 39061|     {
 39062|         return E_OUTOFMEMORY;
 39063|     }
 39064|     gc_heap::min_segment_size_shr = index_of_highest_set_bit (gc_region_size);
 39065| #else
 39066|     gc_heap::min_segment_size_shr = index_of_highest_set_bit (gc_heap::min_segment_size);
 39067| #endif //USE_REGIONS
 39068| #ifdef MULTIPLE_HEAPS
 39069|     assert (nhp <= g_num_processors);
 39070|     if (max_nhp_from_config)
 39071|     {
 39072|         nhp = min (nhp, max_nhp_from_config);
 39073|     }
 39074|     gc_heap::n_max_heaps = nhp;
 39075|     gc_heap::n_heaps = nhp;
 39076|     hr = gc_heap::initialize_gc (seg_size, large_seg_size, pin_seg_size, nhp);
 39077| #else
 39078|     hr = gc_heap::initialize_gc (seg_size, large_seg_size, pin_seg_size);
 39079| #endif //MULTIPLE_HEAPS
 39080|     GCConfig::SetGCHeapHardLimit(static_cast<int64_t>(gc_heap::heap_hard_limit));
 39081|     GCConfig::SetGCHeapHardLimitSOH(static_cast<int64_t>(gc_heap::heap_hard_limit_oh[soh]));
 39082|     GCConfig::SetGCHeapHardLimitLOH(static_cast<int64_t>(gc_heap::heap_hard_limit_oh[loh]));
 39083|     GCConfig::SetGCHeapHardLimitPOH(static_cast<int64_t>(gc_heap::heap_hard_limit_oh[poh]));
 39084|     if (hr != S_OK)
 39085|         return hr;
 39086|     gc_heap::pm_stress_on = (GCConfig::GetGCProvModeStress() != 0);
 39087| #if defined(HOST_64BIT)
 39088|     gc_heap::youngest_gen_desired_th = gc_heap::mem_one_percent;
 39089| #endif // HOST_64BIT
 39090|     WaitForGCEvent = new (nothrow) GCEvent;
 39091|     if (!WaitForGCEvent)
 39092|     {
 39093|         return E_OUTOFMEMORY;
 39094|     }
 39095|     if (!WaitForGCEvent->CreateManualEventNoThrow(TRUE))
 39096|     {
 39097|         GCToEEInterface::LogErrorToHost("Creation of WaitForGCEvent failed");
 39098|         return E_FAIL;
 39099|     }
 39100| #ifndef FEATURE_NATIVEAOT // NativeAOT forces relocation a different way
 39101| #if defined (STRESS_HEAP) && !defined (MULTIPLE_HEAPS)
 39102|     if (GCStress<cfg_any>::IsEnabled())
 39103|     {
 39104|         for (int i = 0; i < GCHeap::NUM_HEAP_STRESS_OBJS; i++)
 39105|         {
 39106|             m_StressObjs[i] = CreateGlobalHandle(0);
 39107|         }
 39108|         m_CurStressObj = 0;
 39109|     }
 39110| #endif //STRESS_HEAP && !MULTIPLE_HEAPS
 39111| #endif // FEATURE_NATIVEAOT
 39112|     initGCShadow();         // If we are debugging write barriers, initialize heap shadow
 39113| #ifdef USE_REGIONS
 39114|     gc_heap::ephemeral_low = MAX_PTR;
 39115|     gc_heap::ephemeral_high = nullptr;
 39116| #endif //!USE_REGIONS
 39117| #ifdef MULTIPLE_HEAPS
 39118|     for (uint32_t i = 0; i < nhp; i++)
 39119|     {
 39120|         GCHeap* Hp = new (nothrow) GCHeap();
 39121|         if (!Hp)
 39122|             return E_OUTOFMEMORY;
 39123|         if ((hr = Hp->Init (i))!= S_OK)
 39124|         {
 39125|             return hr;
 39126|         }
 39127|     }
 39128|     heap_select::init_numa_node_to_heap_map (nhp);
 39129|     if (g_num_active_processors > nhp)
 39130|         heap_select::distribute_other_procs();
 39131|     gc_heap* hp = gc_heap::g_heaps[0];
 39132|     dynamic_data* gen0_dd = hp->dynamic_data_of (0);
 39133|     gc_heap::min_gen0_balance_delta = (dd_min_size (gen0_dd) >> 6);
 39134|     bool can_use_cpu_groups = GCToOSInterface::CanEnableGCCPUGroups();
 39135|     GCConfig::SetGCCpuGroup(can_use_cpu_groups);
 39136| #ifdef HEAP_BALANCE_INSTRUMENTATION
 39137|     cpu_group_enabled_p = can_use_cpu_groups;
 39138|     if (!GCToOSInterface::GetNumaInfo (&total_numa_nodes_on_machine, &procs_per_numa_node))
 39139|     {
 39140|         total_numa_nodes_on_machine = 1;
 39141|         if (GCToOSInterface::GetCPUGroupInfo (&total_cpu_groups_on_machine, &procs_per_cpu_group))
 39142|             procs_per_numa_node = procs_per_cpu_group + ((total_cpu_groups_on_machine - 1) << 6);
 39143|         else
 39144|             procs_per_numa_node = g_num_processors;
 39145|     }
 39146|     hb_info_numa_nodes = new (nothrow) heap_balance_info_numa[total_numa_nodes_on_machine];
 39147|     dprintf (HEAP_BALANCE_LOG, ("total: %d, numa: %d", g_num_processors, total_numa_nodes_on_machine));
 39148|     int hb_info_size_per_proc = sizeof (heap_balance_info_proc);
 39149|     for (int numa_node_index = 0; numa_node_index < total_numa_nodes_on_machine; numa_node_index++)
 39150|     {
 39151|         int hb_info_size_per_node = hb_info_size_per_proc * procs_per_numa_node;
 39152|         uint8_t* numa_mem = (uint8_t*)GCToOSInterface::VirtualReserve (hb_info_size_per_node, 0, 0, numa_node_index);
 39153|         if (!numa_mem)
 39154|         {
 39155|             GCToEEInterface::LogErrorToHost("Reservation of numa_mem failed");
 39156|             return E_FAIL;
 39157|         }
 39158|         if (!GCToOSInterface::VirtualCommit (numa_mem, hb_info_size_per_node, numa_node_index))
 39159|         {
 39160|             GCToEEInterface::LogErrorToHost("Commit of numa_mem failed");
 39161|             return E_FAIL;
 39162|         }
 39163|         heap_balance_info_proc* hb_info_procs = (heap_balance_info_proc*)numa_mem;
 39164|         hb_info_numa_nodes[numa_node_index].hb_info_procs = hb_info_procs;
 39165|         for (int proc_index = 0; proc_index < (int)procs_per_numa_node; proc_index++)
 39166|         {
 39167|             heap_balance_info_proc* hb_info_proc = &hb_info_procs[proc_index];
 39168|             hb_info_proc->count = default_max_hb_heap_balance_info;
 39169|             hb_info_proc->index = 0;
 39170|         }
 39171|     }
 39172| #endif //HEAP_BALANCE_INSTRUMENTATION
 39173| #else
 39174|     hr = Init (0);
 39175| #endif //MULTIPLE_HEAPS
 39176| #ifdef USE_REGIONS
 39177|     if (initial_regions)
 39178|     {
 39179|         delete[] initial_regions;
 39180|     }
 39181| #endif //USE_REGIONS
 39182|     if (hr == S_OK)
 39183|     {
 39184| #ifdef DYNAMIC_HEAP_COUNT
 39185|         if (gc_heap::dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes)
 39186|         {
 39187|             gc_heap::smoothed_desired_total[0] /= gc_heap::n_heaps;
 39188|             int initial_n_heaps = 1;
 39189|             dprintf (9999, ("gc_heap::n_heaps is %d, initial %d", gc_heap::n_heaps, initial_n_heaps));
 39190|             {
 39191|                 if (!gc_heap::prepare_to_change_heap_count (initial_n_heaps))
 39192|                 {
 39193|                     return E_FAIL;
 39194|                 }
 39195|                 gc_heap::dynamic_heap_count_data.new_n_heaps = initial_n_heaps;
 39196|                 gc_heap::dynamic_heap_count_data.idle_thread_count = 0;
 39197|                 gc_heap::dynamic_heap_count_data.init_only_p = true;
 39198|                 int max_threads_to_wake = max (gc_heap::n_heaps, initial_n_heaps);
 39199|                 gc_t_join.update_n_threads (max_threads_to_wake);
 39200|                 gc_heap::gc_start_event.Set ();
 39201|             }
 39202|             gc_heap::g_heaps[0]->change_heap_count (initial_n_heaps);
 39203|             gc_heap::gc_start_event.Reset ();
 39204|             gc_heap::dynamic_heap_count_data.last_n_heaps = 0;
 39205|         }
 39206| #endif //DYNAMIC_HEAP_COUNT
 39207|         GCScan::GcRuntimeStructuresValid (TRUE);
 39208|         GCToEEInterface::DiagUpdateGenerationBounds();
 39209| #if defined(STRESS_REGIONS) && defined(FEATURE_BASICFREEZE)
 39210| #ifdef MULTIPLE_HEAPS
 39211|         gc_heap* hp = gc_heap::g_heaps[0];
 39212| #else
 39213|         gc_heap* hp = pGenGCHeap;
 39214| #endif //MULTIPLE_HEAPS
 39215|         for (int i = 0; i < 2; i++)
 39216|         {
 39217|             size_t ro_seg_size = 1024 * 1024;
 39218|             uint8_t* seg_mem = new (nothrow) uint8_t [ro_seg_size];
 39219|             heap_segment* ro_seg = (heap_segment*) seg_mem;
 39220|             uint8_t* start = seg_mem + gc_heap::segment_info_size;
 39221|             heap_segment_mem (ro_seg) = start;
 39222|             heap_segment_used (ro_seg) = start;
 39223|             heap_segment_reserved (ro_seg) = seg_mem + ro_seg_size;
 39224|             heap_segment_committed (ro_seg) = heap_segment_reserved (ro_seg);
 39225|             gc_heap::init_heap_segment (ro_seg, hp, seg_mem, ro_seg_size, 2);
 39226|             ro_seg->flags = heap_segment_flags_readonly;
 39227|             hp->insert_ro_segment (ro_seg);
 39228|         }
 39229| #endif //STRESS_REGIONS && FEATURE_BASICFREEZE
 39230|     }
 39231|     return hr;
 39232| }
 39233| bool GCHeap::IsPromoted(Object* object)
 39234| {
 39235|     uint8_t* o = (uint8_t*)object;
 39236|     bool is_marked;
 39237|     if (gc_heap::settings.condemned_generation == max_generation)
 39238|     {
 39239| #ifdef MULTIPLE_HEAPS
 39240|         gc_heap* hp = gc_heap::g_heaps[0];
 39241| #else
 39242|         gc_heap* hp = pGenGCHeap;
 39243| #endif //MULTIPLE_HEAPS
 39244| #ifdef BACKGROUND_GC
 39245|         if (gc_heap::settings.concurrent)
 39246|         {
 39247|             is_marked = (!((o < hp->background_saved_highest_address) && (o >= hp->background_saved_lowest_address))||
 39248|                             hp->background_marked (o));
 39249|         }
 39250|         else
 39251| #endif //BACKGROUND_GC
 39252|         {
 39253|             is_marked = (!((o < hp->highest_address) && (o >= hp->lowest_address))
 39254|                         || hp->is_mark_set (o));
 39255|         }
 39256|     }
 39257|     else
 39258|     {
 39259| #ifdef USE_REGIONS
 39260|         is_marked = (gc_heap::is_in_gc_range (o) ? (gc_heap::is_in_condemned_gc (o) ? gc_heap::is_mark_set (o) : true) : true);
 39261| #else
 39262|         gc_heap* hp = gc_heap::heap_of (o);
 39263|         is_marked = (!((o < hp->gc_high) && (o >= hp->gc_low))
 39264|                    || hp->is_mark_set (o));
 39265| #endif //USE_REGIONS
 39266|     }
 39267| #ifdef _DEBUG
 39268|     if (o)
 39269|     {
 39270|         ((CObjectHeader*)o)->Validate(TRUE, TRUE, is_marked);
 39271|         assert(is_marked || !IsInFrozenSegment(object));
 39272|     }
 39273| #endif //_DEBUG
 39274|     return is_marked;
 39275| }
 39276| size_t GCHeap::GetPromotedBytes(int heap_index)
 39277| {
 39278| #ifdef BACKGROUND_GC
 39279|     if (gc_heap::settings.concurrent)
 39280|     {
 39281|         return gc_heap::bpromoted_bytes (heap_index);
 39282|     }
 39283|     else
 39284| #endif //BACKGROUND_GC
 39285|     {
 39286|         gc_heap* hp =
 39287| #ifdef MULTIPLE_HEAPS
 39288|             gc_heap::g_heaps[heap_index];
 39289| #else
 39290|             pGenGCHeap;
 39291| #endif //MULTIPLE_HEAPS
 39292|         return hp->get_promoted_bytes();
 39293|     }
 39294| }
 39295| void GCHeap::SetYieldProcessorScalingFactor (float scalingFactor)
 39296| {
 39297|     if (!gc_heap::spin_count_unit_config_p)
 39298|     {
 39299|         assert (yp_spin_count_unit != 0);
 39300|         uint32_t saved_yp_spin_count_unit = yp_spin_count_unit;
 39301|         yp_spin_count_unit = (uint32_t)((float)original_spin_count_unit * scalingFactor / (float)9);
 39302|         if ((yp_spin_count_unit == 0) || (yp_spin_count_unit > MAX_YP_SPIN_COUNT_UNIT))
 39303|         {
 39304|             yp_spin_count_unit = saved_yp_spin_count_unit;
 39305|         }
 39306|     }
 39307| }
 39308| unsigned int GCHeap::WhichGeneration (Object* object)
 39309| {
 39310|     uint8_t* o = (uint8_t*)object;
 39311| #ifdef FEATURE_BASICFREEZE
 39312|     if (!((o < g_gc_highest_address) && (o >= g_gc_lowest_address)))
 39313|     {
 39314|         return INT32_MAX;
 39315|     }
 39316| #ifndef USE_REGIONS
 39317|     if (GCHeap::IsInFrozenSegment (object))
 39318|     {
 39319|         return INT32_MAX;
 39320|     }
 39321| #endif
 39322| #endif //FEATURE_BASICFREEZE
 39323|     gc_heap* hp = gc_heap::heap_of (o);
 39324|     unsigned int g = hp->object_gennum (o);
 39325|     dprintf (3, ("%zx is in gen %d", (size_t)object, g));
 39326|     return g;
 39327| }
 39328| enable_no_gc_region_callback_status GCHeap::EnableNoGCRegionCallback(NoGCRegionCallbackFinalizerWorkItem* callback, uint64_t callback_threshold)
 39329| {
 39330|     return gc_heap::enable_no_gc_callback(callback, callback_threshold);
 39331| }
 39332| FinalizerWorkItem* GCHeap::GetExtraWorkForFinalization()
 39333| {
 39334|     return Interlocked::ExchangePointer(&gc_heap::finalizer_work, nullptr);
 39335| }
 39336| unsigned int GCHeap::GetGenerationWithRange (Object* object, uint8_t** ppStart, uint8_t** ppAllocated, uint8_t** ppReserved)
 39337| {
 39338|     int generation = -1;
 39339|     heap_segment * hs = gc_heap::find_segment ((uint8_t*)object, FALSE);
 39340| #ifdef USE_REGIONS
 39341|     generation = heap_segment_gen_num (hs);
 39342|     if (generation == max_generation)
 39343|     {
 39344|         if (heap_segment_loh_p (hs))
 39345|         {
 39346|             generation = loh_generation;
 39347|         }
 39348|         else if (heap_segment_poh_p (hs))
 39349|         {
 39350|             generation = poh_generation;
 39351|         }
 39352|     }
 39353|     *ppStart = heap_segment_mem (hs);
 39354|     *ppAllocated = heap_segment_allocated (hs);
 39355|     *ppReserved = heap_segment_reserved (hs);
 39356| #else
 39357| #ifdef MULTIPLE_HEAPS
 39358|     gc_heap* hp = heap_segment_heap (hs);
 39359| #else
 39360|     gc_heap* hp = __this;
 39361| #endif //MULTIPLE_HEAPS
 39362|     if (hs == hp->ephemeral_heap_segment)
 39363|     {
 39364|         uint8_t* reserved = heap_segment_reserved (hs);
 39365|         uint8_t* end = heap_segment_allocated(hs);
 39366|         for (int gen = 0; gen < max_generation; gen++)
 39367|         {
 39368|             uint8_t* start = generation_allocation_start (hp->generation_of (gen));
 39369|             if ((uint8_t*)object >= start)
 39370|             {
 39371|                 generation = gen;
 39372|                 *ppStart = start;
 39373|                 *ppAllocated = end;
 39374|                 *ppReserved = reserved;
 39375|                 break;
 39376|             }
 39377|             end = reserved = start;
 39378|         }
 39379|         if (generation == -1)
 39380|         {
 39381|             generation = max_generation;
 39382|             *ppStart = heap_segment_mem (hs);
 39383|             *ppAllocated = *ppReserved = generation_allocation_start (hp->generation_of (max_generation - 1));
 39384|         }
 39385|     }
 39386|     else
 39387|     {
 39388|         generation = max_generation;
 39389|         if (heap_segment_loh_p (hs))
 39390|         {
 39391|             generation = loh_generation;
 39392|         }
 39393|         else if (heap_segment_poh_p (hs))
 39394|         {
 39395|             generation = poh_generation;
 39396|         }
 39397|         *ppStart = heap_segment_mem (hs);
 39398|         *ppAllocated = heap_segment_allocated (hs);
 39399|         *ppReserved = heap_segment_reserved (hs);
 39400|     }
 39401| #endif //USE_REGIONS
 39402|     return (unsigned int)generation;
 39403| }
 39404| bool GCHeap::IsEphemeral (Object* object)
 39405| {
 39406|     uint8_t* o = (uint8_t*)object;
 39407| #if defined(FEATURE_BASICFREEZE) && defined(USE_REGIONS)
 39408|     if (!is_in_heap_range (o))
 39409|     {
 39410|         return FALSE;
 39411|     }
 39412| #endif
 39413|     gc_heap* hp = gc_heap::heap_of (o);
 39414|     return !!hp->ephemeral_pointer_p (o);
 39415| }
 39416| Object * GCHeap::NextObj (Object * object)
 39417| {
 39418| #ifdef VERIFY_HEAP
 39419|     uint8_t* o = (uint8_t*)object;
 39420| #ifndef FEATURE_BASICFREEZE
 39421|     if (!((o < g_gc_highest_address) && (o >= g_gc_lowest_address)))
 39422|     {
 39423|         return NULL;
 39424|     }
 39425| #endif //!FEATURE_BASICFREEZE
 39426|     heap_segment * hs = gc_heap::find_segment (o, FALSE);
 39427|     if (!hs)
 39428|     {
 39429|         return NULL;
 39430|     }
 39431|     BOOL large_object_p = heap_segment_uoh_p (hs);
 39432|     if (large_object_p)
 39433|         return NULL; //could be racing with another core allocating.
 39434| #ifdef MULTIPLE_HEAPS
 39435|     gc_heap* hp = heap_segment_heap (hs);
 39436| #else //MULTIPLE_HEAPS
 39437|     gc_heap* hp = 0;
 39438| #endif //MULTIPLE_HEAPS
 39439| #ifdef USE_REGIONS
 39440|     unsigned int g = heap_segment_gen_num (hs);
 39441| #else
 39442|     unsigned int g = hp->object_gennum ((uint8_t*)object);
 39443| #endif
 39444|     int align_const = get_alignment_constant (!large_object_p);
 39445|     uint8_t* nextobj = o + Align (size (o), align_const);
 39446|     if (nextobj <= o) // either overflow or 0 sized object.
 39447|     {
 39448|         return NULL;
 39449|     }
 39450|     if (nextobj < heap_segment_mem (hs))
 39451|     {
 39452|         return NULL;
 39453|     }
 39454|     uint8_t* saved_alloc_allocated = hp->alloc_allocated;
 39455|     heap_segment* saved_ephemeral_heap_segment = hp->ephemeral_heap_segment;
 39456|     if ((nextobj >= heap_segment_allocated (hs)) &&
 39457|         ((hs != saved_ephemeral_heap_segment) ||
 39458|          !in_range_for_segment(saved_alloc_allocated, saved_ephemeral_heap_segment) ||
 39459|          (nextobj >= saved_alloc_allocated)))
 39460|     {
 39461|         return NULL;
 39462|     }
 39463|     return (Object *)nextobj;
 39464| #else
 39465|     return nullptr;
 39466| #endif // VERIFY_HEAP
 39467| }
 39468| bool GCHeap::IsHeapPointer (void* vpObject, bool small_heap_only)
 39469| {
 39470|     uint8_t* object = (uint8_t*) vpObject;
 39471| #ifndef FEATURE_BASICFREEZE
 39472|     if (!((object < g_gc_highest_address) && (object >= g_gc_lowest_address)))
 39473|         return FALSE;
 39474| #endif //!FEATURE_BASICFREEZE
 39475|     heap_segment * hs = gc_heap::find_segment (object, small_heap_only);
 39476|     return !!hs;
 39477| }
 39478| void GCHeap::Promote(Object** ppObject, ScanContext* sc, uint32_t flags)
 39479| {
 39480|     THREAD_NUMBER_FROM_CONTEXT;
 39481| #ifndef MULTIPLE_HEAPS
 39482|     const int thread = 0;
 39483| #endif //!MULTIPLE_HEAPS
 39484|     uint8_t* o = (uint8_t*)*ppObject;
 39485|     if (!gc_heap::is_in_find_object_range (o))
 39486|     {
 39487|         return;
 39488|     }
 39489| #ifdef DEBUG_DestroyedHandleValue
 39490|     if (o == (uint8_t*)DEBUG_DestroyedHandleValue)
 39491|         return;
 39492| #endif //DEBUG_DestroyedHandleValue
 39493|     HEAP_FROM_THREAD;
 39494|     gc_heap* hp = gc_heap::heap_of (o);
 39495| #ifdef USE_REGIONS
 39496|     if (!gc_heap::is_in_condemned_gc (o))
 39497| #else //USE_REGIONS
 39498|     if ((o < hp->gc_low) || (o >= hp->gc_high))
 39499| #endif //USE_REGIONS
 39500|     {
 39501|         return;
 39502|     }
 39503|     dprintf (3, ("Promote %zx", (size_t)o));
 39504|     if (flags & GC_CALL_INTERIOR)
 39505|     {
 39506|         if ((o = hp->find_object (o)) == 0)
 39507|         {
 39508|             return;
 39509|         }
 39510|     }
 39511| #ifdef FEATURE_CONSERVATIVE_GC
 39512|     if (GCConfig::GetConservativeGC()
 39513|         && ((CObjectHeader*)o)->IsFree())
 39514|     {
 39515|         return;
 39516|     }
 39517| #endif
 39518| #ifdef _DEBUG
 39519|     ((CObjectHeader*)o)->Validate();
 39520| #else
 39521|     UNREFERENCED_PARAMETER(sc);
 39522| #endif //_DEBUG
 39523|     if (flags & GC_CALL_PINNED)
 39524|         hp->pin_object (o, (uint8_t**) ppObject);
 39525| #ifdef STRESS_PINNING
 39526|     if ((++n_promote % 20) == 1)
 39527|             hp->pin_object (o, (uint8_t**) ppObject);
 39528| #endif //STRESS_PINNING
 39529|     hpt->mark_object_simple (&o THREAD_NUMBER_ARG);
 39530|     STRESS_LOG_ROOT_PROMOTE(ppObject, o, o ? header(o)->GetMethodTable() : NULL);
 39531| }
 39532| void GCHeap::Relocate (Object** ppObject, ScanContext* sc,
 39533|                        uint32_t flags)
 39534| {
 39535|     UNREFERENCED_PARAMETER(sc);
 39536|     uint8_t* object = (uint8_t*)(Object*)(*ppObject);
 39537|     if (!gc_heap::is_in_find_object_range (object))
 39538|     {
 39539|         return;
 39540|     }
 39541|     THREAD_NUMBER_FROM_CONTEXT;
 39542|     dprintf (3, ("R: %zx", (size_t)ppObject));
 39543|     gc_heap* hp = gc_heap::heap_of (object);
 39544| #ifdef _DEBUG
 39545|     if (!(flags & GC_CALL_INTERIOR))
 39546|     {
 39547| #ifdef USE_REGIONS
 39548|         if (!gc_heap::is_in_condemned_gc (object))
 39549| #else //USE_REGIONS
 39550|         if (!((object >= hp->gc_low) && (object < hp->gc_high)))
 39551| #endif //USE_REGIONS
 39552|         {
 39553|             ((CObjectHeader*)object)->Validate(FALSE);
 39554|         }
 39555|     }
 39556| #endif //_DEBUG
 39557|     dprintf (3, ("Relocate %zx\n", (size_t)object));
 39558|     uint8_t* pheader;
 39559|     if ((flags & GC_CALL_INTERIOR) && gc_heap::settings.loh_compaction)
 39560|     {
 39561| #ifdef USE_REGIONS
 39562|         if (!gc_heap::is_in_condemned_gc (object))
 39563| #else //USE_REGIONS
 39564|         if (!((object >= hp->gc_low) && (object < hp->gc_high)))
 39565| #endif //USE_REGIONS
 39566|         {
 39567|             return;
 39568|         }
 39569|         if (gc_heap::loh_object_p (object))
 39570|         {
 39571|             pheader = hp->find_object (object);
 39572|             if (pheader == 0)
 39573|             {
 39574|                 return;
 39575|             }
 39576|             ptrdiff_t ref_offset = object - pheader;
 39577|             hp->relocate_address(&pheader THREAD_NUMBER_ARG);
 39578|             *ppObject = (Object*)(pheader + ref_offset);
 39579|             return;
 39580|         }
 39581|     }
 39582|     {
 39583|         pheader = object;
 39584|         hp->relocate_address(&pheader THREAD_NUMBER_ARG);
 39585|         *ppObject = (Object*)pheader;
 39586|     }
 39587|     STRESS_LOG_ROOT_RELOCATE(ppObject, object, pheader, ((!(flags & GC_CALL_INTERIOR)) ? ((Object*)object)->GetGCSafeMethodTable() : 0));
 39588| }
 39589| /*static*/ bool GCHeap::IsLargeObject(Object *pObj)
 39590| {
 39591|     return size( pObj ) >= loh_size_threshold;
 39592| }
 39593| #ifndef FEATURE_NATIVEAOT // NativeAOT forces relocation a different way
 39594| #ifdef STRESS_HEAP
 39595| void StressHeapDummy ();
 39596| int StressRNG(int iMaxValue)
 39597| {
 39598|     static BOOL bisRandInit = FALSE;
 39599|     static int lHoldrand = 1L;
 39600|     if (!bisRandInit)
 39601|     {
 39602|         lHoldrand = (int)time(NULL);
 39603|         bisRandInit = TRUE;
 39604|     }
 39605|     int randValue = (((lHoldrand = lHoldrand * 214013L + 2531011L) >> 16) & 0x7fff);
 39606|     return randValue % iMaxValue;
 39607| }
 39608| #endif // STRESS_HEAP
 39609| #endif // !FEATURE_NATIVEAOT
 39610| bool GCHeap::StressHeap(gc_alloc_context * context)
 39611| {
 39612| #if defined(STRESS_HEAP) && !defined(FEATURE_NATIVEAOT)
 39613|     alloc_context* acontext = static_cast<alloc_context*>(context);
 39614|     assert(context != nullptr);
 39615|     if (!GCStressPolicy::IsEnabled())
 39616|         return FALSE;
 39617| #ifdef _DEBUG
 39618|     if (g_pConfig->FastGCStressLevel() && !GCToEEInterface::GetThread()->StressHeapIsEnabled()) {
 39619|         return FALSE;
 39620|     }
 39621| #endif //_DEBUG
 39622|     if ((g_pConfig->GetGCStressLevel() & EEConfig::GCSTRESS_UNIQUE)
 39623| #ifdef _DEBUG
 39624|         || g_pConfig->FastGCStressLevel() > 1
 39625| #endif //_DEBUG
 39626|         ) {
 39627|         if (!Thread::UniqueStack(&acontext)) {
 39628|             return FALSE;
 39629|         }
 39630|     }
 39631| #ifdef BACKGROUND_GC
 39632|     if (GCToEEInterface::WasCurrentThreadCreatedByGC())
 39633|     {
 39634|         return FALSE;
 39635|     }
 39636| #endif //BACKGROUND_GC
 39637|     if (g_pStringClass == 0)
 39638|     {
 39639|         _ASSERTE(g_fEEInit);
 39640|         return FALSE;
 39641|     }
 39642| #ifndef MULTIPLE_HEAPS
 39643|     static int32_t OneAtATime = -1;
 39644|     if (Interlocked::Increment(&OneAtATime) == 0 &&
 39645|         !TrackAllocations()) // Messing with object sizes can confuse the profiler (see ICorProfilerInfo::GetObjectSize)
 39646|     {
 39647|         StringObject* str;
 39648|         if (HndFetchHandle(m_StressObjs[m_CurStressObj]) == 0)
 39649|         {
 39650|             int i = m_CurStressObj;
 39651|             while(HndFetchHandle(m_StressObjs[i]) == 0)
 39652|             {
 39653|                 _ASSERTE(m_StressObjs[i] != 0);
 39654|                 unsigned strLen = ((unsigned)loh_size_threshold - 32) / sizeof(WCHAR);
 39655|                 unsigned strSize = PtrAlign(StringObject::GetSize(strLen));
 39656|                 SetTypeHandleOnThreadForAlloc(TypeHandle(g_pStringClass));
 39657|                 str = (StringObject*) pGenGCHeap->allocate (strSize, acontext, /*flags*/ 0);
 39658|                 if (str)
 39659|                 {
 39660|                     str->SetMethodTable (g_pStringClass);
 39661|                     str->SetStringLength (strLen);
 39662|                     HndAssignHandle(m_StressObjs[i], ObjectToOBJECTREF(str));
 39663|                 }
 39664|                 i = (i + 1) % NUM_HEAP_STRESS_OBJS;
 39665|                 if (i == m_CurStressObj) break;
 39666|             }
 39667|             m_CurStressObj = (m_CurStressObj + 1) % NUM_HEAP_STRESS_OBJS;
 39668|         }
 39669|         str = (StringObject*) OBJECTREFToObject(HndFetchHandle(m_StressObjs[m_CurStressObj]));
 39670|         if (str)
 39671|         {
 39672|             unsigned sizeOfNewObj = (unsigned)Align(min_obj_size * 31);
 39673|             if (str->GetStringLength() > sizeOfNewObj / sizeof(WCHAR))
 39674|             {
 39675|                 unsigned sizeToNextObj = (unsigned)Align(size(str));
 39676|                 uint8_t* freeObj = ((uint8_t*) str) + sizeToNextObj - sizeOfNewObj;
 39677|                 pGenGCHeap->make_unused_array (freeObj, sizeOfNewObj);
 39678| #if !defined(TARGET_AMD64) && !defined(TARGET_X86)
 39679|                 MemoryBarrier();
 39680| #endif
 39681|                 str->SetStringLength(str->GetStringLength() - (sizeOfNewObj / sizeof(WCHAR)));
 39682|             }
 39683|             else
 39684|             {
 39685|                 HndAssignHandle(m_StressObjs[m_CurStressObj], 0);
 39686|             }
 39687|         }
 39688|     }
 39689|     Interlocked::Decrement(&OneAtATime);
 39690| #endif // !MULTIPLE_HEAPS
 39691|     if (IsConcurrentGCEnabled())
 39692|     {
 39693|         int rgen = StressRNG(10);
 39694|         if (rgen >= 8)
 39695|             rgen = 2;
 39696|         else if (rgen >= 4)
 39697|             rgen = 1;
 39698|     else
 39699|             rgen = 0;
 39700|         GarbageCollectTry (rgen, FALSE, collection_gcstress);
 39701|     }
 39702|     else
 39703|     {
 39704|         GarbageCollect(max_generation, FALSE, collection_gcstress);
 39705|     }
 39706|     return TRUE;
 39707| #else
 39708|     UNREFERENCED_PARAMETER(context);
 39709|     return FALSE;
 39710| #endif //STRESS_HEAP && !FEATURE_NATIVEAOT
 39711| }
 39712| #ifdef FEATURE_PREMORTEM_FINALIZATION
 39713| #define REGISTER_FOR_FINALIZATION(_object, _size) \
 39714|     hp->finalize_queue->RegisterForFinalization (0, (_object), (_size))
 39715| #else // FEATURE_PREMORTEM_FINALIZATION
 39716| #define REGISTER_FOR_FINALIZATION(_object, _size) true
 39717| #endif // FEATURE_PREMORTEM_FINALIZATION
 39718| #define CHECK_ALLOC_AND_POSSIBLY_REGISTER_FOR_FINALIZATION(_object, _size, _register) do {  \
 39719|     if ((_object) == NULL || ((_register) && !REGISTER_FOR_FINALIZATION(_object, _size)))   \
 39720|     {                                                                                       \
 39721|         STRESS_LOG_OOM_STACK(_size);                                                        \
 39722|         return NULL;                                                                        \
 39723|     }                                                                                       \
 39724| } while (false)
 39725| #ifdef FEATURE_64BIT_ALIGNMENT
 39726| Object* AllocAlign8(alloc_context* acontext, gc_heap* hp, size_t size, uint32_t flags)
 39727| {
 39728|     CONTRACTL {
 39729|         NOTHROW;
 39730|         GC_TRIGGERS;
 39731|     } CONTRACTL_END;
 39732|     Object* newAlloc = NULL;
 39733|     size_t desiredAlignment = (flags & GC_ALLOC_ALIGN8_BIAS) ? 4 : 0;
 39734|     uint8_t*  result = acontext->alloc_ptr;
 39735|     if ((((size_t)result & 7) == desiredAlignment) && ((result + size) <= acontext->alloc_limit))
 39736|     {
 39737|         newAlloc = (Object*) hp->allocate (size, acontext, flags);
 39738|         ASSERT(((size_t)newAlloc & 7) == desiredAlignment);
 39739|     }
 39740|     else
 39741|     {
 39742|         ASSERT((Align(min_obj_size) & 7) == 4);
 39743|         CObjectHeader *freeobj = (CObjectHeader*) hp->allocate (Align(size) + Align(min_obj_size), acontext, flags);
 39744|         if (freeobj)
 39745|         {
 39746|             if (((size_t)freeobj & 7) == desiredAlignment)
 39747|             {
 39748|                 newAlloc = (Object*)freeobj;
 39749|                 freeobj = (CObjectHeader*)((uint8_t*)freeobj + Align(size));
 39750|             }
 39751|             else
 39752|             {
 39753|                 newAlloc = (Object*)((uint8_t*)freeobj + Align(min_obj_size));
 39754|                 ASSERT(((size_t)newAlloc & 7) == desiredAlignment);
 39755|                 if (flags & GC_ALLOC_ZEROING_OPTIONAL)
 39756|                 {
 39757|                     *(((PTR_PTR)newAlloc)-1) = 0;
 39758|                 }
 39759|             }
 39760|             freeobj->SetFree(min_obj_size);
 39761|         }
 39762|     }
 39763|     return newAlloc;
 39764| }
 39765| #endif // FEATURE_64BIT_ALIGNMENT
 39766| Object*
 39767| GCHeap::Alloc(gc_alloc_context* context, size_t size, uint32_t flags REQD_ALIGN_DCL)
 39768| {
 39769|     CONTRACTL {
 39770|         NOTHROW;
 39771|         GC_TRIGGERS;
 39772|     } CONTRACTL_END;
 39773|     TRIGGERSGC();
 39774|     Object* newAlloc = NULL;
 39775|     alloc_context* acontext = static_cast<alloc_context*>(context);
 39776| #ifdef MULTIPLE_HEAPS
 39777|     if (acontext->get_alloc_heap() == 0)
 39778|     {
 39779|         AssignHeap (acontext);
 39780|         assert (acontext->get_alloc_heap());
 39781|     }
 39782|     gc_heap* hp = acontext->get_alloc_heap()->pGenGCHeap;
 39783| #else
 39784|     gc_heap* hp = pGenGCHeap;
 39785| #ifdef _PREFAST_
 39786|     PREFIX_ASSUME(hp != NULL);
 39787| #endif //_PREFAST_
 39788| #endif //MULTIPLE_HEAPS
 39789|     assert(size < loh_size_threshold || (flags & GC_ALLOC_LARGE_OBJECT_HEAP));
 39790|     if (flags & GC_ALLOC_USER_OLD_HEAP)
 39791|     {
 39792|         ASSERT((flags & GC_ALLOC_ALIGN8_BIAS) == 0);
 39793|         ASSERT(65536 < loh_size_threshold);
 39794|         int gen_num = (flags & GC_ALLOC_PINNED_OBJECT_HEAP) ? poh_generation : loh_generation;
 39795|         newAlloc = (Object*) hp->allocate_uoh_object (size + ComputeMaxStructAlignPadLarge(requiredAlignment), flags, gen_num, acontext->alloc_bytes_uoh);
 39796|         ASSERT(((size_t)newAlloc & 7) == 0);
 39797| #ifdef MULTIPLE_HEAPS
 39798|         if (flags & GC_ALLOC_FINALIZE)
 39799|         {
 39800|             hp = gc_heap::heap_of ((uint8_t*)newAlloc);
 39801|         }
 39802| #endif //MULTIPLE_HEAPS
 39803| #ifdef FEATURE_STRUCTALIGN
 39804|         newAlloc = (Object*) hp->pad_for_alignment_large ((uint8_t*) newAlloc, requiredAlignment, size);
 39805| #endif // FEATURE_STRUCTALIGN
 39806|     }
 39807|     else
 39808|     {
 39809| #ifdef FEATURE_64BIT_ALIGNMENT
 39810|         if (flags & GC_ALLOC_ALIGN8)
 39811|         {
 39812|             newAlloc = AllocAlign8 (acontext, hp, size, flags);
 39813|         }
 39814|         else
 39815| #else
 39816|         assert ((flags & GC_ALLOC_ALIGN8) == 0);
 39817| #endif
 39818|         {
 39819|             newAlloc = (Object*) hp->allocate (size + ComputeMaxStructAlignPad(requiredAlignment), acontext, flags);
 39820|         }
 39821| #ifdef MULTIPLE_HEAPS
 39822|         if (flags & GC_ALLOC_FINALIZE)
 39823|         {
 39824| #ifdef DYNAMIC_HEAP_COUNT
 39825|             hp = (newAlloc == nullptr) ? acontext->get_alloc_heap()->pGenGCHeap : gc_heap::heap_of ((uint8_t*)newAlloc);
 39826| #else //DYNAMIC_HEAP_COUNT
 39827|             hp = acontext->get_alloc_heap()->pGenGCHeap;
 39828|             assert ((newAlloc == nullptr) || (hp == gc_heap::heap_of ((uint8_t*)newAlloc)));
 39829| #endif //DYNAMIC_HEAP_COUNT
 39830|         }
 39831| #endif //MULTIPLE_HEAPS
 39832| #ifdef FEATURE_STRUCTALIGN
 39833|         newAlloc = (Object*) hp->pad_for_alignment ((uint8_t*) newAlloc, requiredAlignment, size, acontext);
 39834| #endif // FEATURE_STRUCTALIGN
 39835|     }
 39836|     CHECK_ALLOC_AND_POSSIBLY_REGISTER_FOR_FINALIZATION(newAlloc, size, flags & GC_ALLOC_FINALIZE);
 39837| #ifdef USE_REGIONS
 39838|     assert (IsHeapPointer (newAlloc));
 39839| #endif //USE_REGIONS
 39840|     return newAlloc;
 39841| }
 39842| void
 39843| GCHeap::FixAllocContext (gc_alloc_context* context, void* arg, void *heap)
 39844| {
 39845|     alloc_context* acontext = static_cast<alloc_context*>(context);
 39846| #ifdef MULTIPLE_HEAPS
 39847|     if (arg != 0)
 39848|         acontext->alloc_count = 0;
 39849|     uint8_t * alloc_ptr = acontext->alloc_ptr;
 39850|     if (!alloc_ptr)
 39851|         return;
 39852|     gc_heap* hp = gc_heap::heap_of (alloc_ptr);
 39853| #else
 39854|     gc_heap* hp = pGenGCHeap;
 39855| #endif //MULTIPLE_HEAPS
 39856|     if (heap == NULL || heap == hp)
 39857|     {
 39858|         hp->fix_allocation_context (acontext, ((arg != 0)? TRUE : FALSE), TRUE);
 39859|     }
 39860| }
 39861| Object*
 39862| GCHeap::GetContainingObject (void *pInteriorPtr, bool fCollectedGenOnly)
 39863| {
 39864|     uint8_t *o = (uint8_t*)pInteriorPtr;
 39865|     if (!gc_heap::is_in_find_object_range (o))
 39866|     {
 39867|         return NULL;
 39868|     }
 39869|     gc_heap* hp = gc_heap::heap_of (o);
 39870| #ifdef USE_REGIONS
 39871|     if (fCollectedGenOnly && !gc_heap::is_in_condemned_gc (o))
 39872|     {
 39873|         return NULL;
 39874|     }
 39875| #else //USE_REGIONS
 39876|     uint8_t* lowest = (fCollectedGenOnly ? hp->gc_low : hp->lowest_address);
 39877|     uint8_t* highest = (fCollectedGenOnly ? hp->gc_high : hp->highest_address);
 39878|     if (!((o >= lowest) && (o < highest)))
 39879|     {
 39880|         return NULL;
 39881|     }
 39882| #endif //USE_REGIONS
 39883|     return (Object*)(hp->find_object (o));
 39884| }
 39885| BOOL should_collect_optimized (dynamic_data* dd, BOOL low_memory_p)
 39886| {
 39887|     if (dd_new_allocation (dd) < 0)
 39888|     {
 39889|         return TRUE;
 39890|     }
 39891|     if (((float)(dd_new_allocation (dd)) / (float)dd_desired_allocation (dd)) < (low_memory_p ? 0.7 : 0.3))
 39892|     {
 39893|         return TRUE;
 39894|     }
 39895|     return FALSE;
 39896| }
 39897| HRESULT
 39898| GCHeap::GarbageCollect (int generation, bool low_memory_p, int mode)
 39899| {
 39900| #if defined(HOST_64BIT)
 39901|     if (low_memory_p)
 39902|     {
 39903|         size_t total_allocated = 0;
 39904|         size_t total_desired = 0;
 39905| #ifdef MULTIPLE_HEAPS
 39906|         int hn = 0;
 39907|         for (hn = 0; hn < gc_heap::n_heaps; hn++)
 39908|         {
 39909|             gc_heap* hp = gc_heap::g_heaps [hn];
 39910|             total_desired += dd_desired_allocation (hp->dynamic_data_of (0));
 39911|             total_allocated += dd_desired_allocation (hp->dynamic_data_of (0))-
 39912|                 dd_new_allocation (hp->dynamic_data_of (0));
 39913|         }
 39914| #else
 39915|         gc_heap* hp = pGenGCHeap;
 39916|         total_desired = dd_desired_allocation (hp->dynamic_data_of (0));
 39917|         total_allocated = dd_desired_allocation (hp->dynamic_data_of (0))-
 39918|             dd_new_allocation (hp->dynamic_data_of (0));
 39919| #endif //MULTIPLE_HEAPS
 39920|         if ((total_desired > gc_heap::mem_one_percent) && (total_allocated < gc_heap::mem_one_percent))
 39921|         {
 39922|             dprintf (2, ("Async low mem but we've only allocated %zu (< 10%% of physical mem) out of %zu, returning",
 39923|                          total_allocated, total_desired));
 39924|             return S_OK;
 39925|         }
 39926|     }
 39927| #endif // HOST_64BIT
 39928| #ifdef MULTIPLE_HEAPS
 39929|     gc_heap* hpt = gc_heap::g_heaps[0];
 39930| #else
 39931|     gc_heap* hpt = 0;
 39932| #endif //MULTIPLE_HEAPS
 39933|     generation = (generation < 0) ? max_generation : min (generation, max_generation);
 39934|     dynamic_data* dd = hpt->dynamic_data_of (generation);
 39935| #ifdef BACKGROUND_GC
 39936|     if (gc_heap::background_running_p())
 39937|     {
 39938|         if ((mode == collection_optimized) || (mode & collection_non_blocking))
 39939|         {
 39940|             return S_OK;
 39941|         }
 39942|         if (mode & collection_blocking)
 39943|         {
 39944|             pGenGCHeap->background_gc_wait();
 39945|             if (mode & collection_optimized)
 39946|             {
 39947|                 return S_OK;
 39948|             }
 39949|         }
 39950|     }
 39951| #endif //BACKGROUND_GC
 39952|     if (mode & collection_optimized)
 39953|     {
 39954|         if (pGenGCHeap->gc_started)
 39955|         {
 39956|             return S_OK;
 39957|         }
 39958|         else
 39959|         {
 39960|             BOOL should_collect = FALSE;
 39961|             BOOL should_check_uoh = (generation == max_generation);
 39962| #ifdef MULTIPLE_HEAPS
 39963|             for (int heap_number = 0; heap_number < gc_heap::n_heaps; heap_number++)
 39964|             {
 39965|                 dynamic_data* dd1 = gc_heap::g_heaps [heap_number]->dynamic_data_of (generation);
 39966|                 should_collect = should_collect_optimized (dd1, low_memory_p);
 39967|                 if (should_check_uoh)
 39968|                 {
 39969|                     for (int i = uoh_start_generation; i < total_generation_count && !should_collect; i++)
 39970|                     {
 39971|                         should_collect = should_collect_optimized (gc_heap::g_heaps [heap_number]->dynamic_data_of (i), low_memory_p);
 39972|                     }
 39973|                 }
 39974|                 if (should_collect)
 39975|                     break;
 39976|             }
 39977| #else
 39978|             should_collect = should_collect_optimized (dd, low_memory_p);
 39979|             if (should_check_uoh)
 39980|             {
 39981|                 for (int i = uoh_start_generation; i < total_generation_count && !should_collect; i++)
 39982|                 {
 39983|                     should_collect = should_collect_optimized (hpt->dynamic_data_of (i), low_memory_p);
 39984|                 }
 39985|             }
 39986| #endif //MULTIPLE_HEAPS
 39987|             if (!should_collect)
 39988|             {
 39989|                 return S_OK;
 39990|             }
 39991|         }
 39992|     }
 39993|     size_t CollectionCountAtEntry = dd_collection_count (dd);
 39994|     size_t BlockingCollectionCountAtEntry = gc_heap::full_gc_counts[gc_type_blocking];
 39995|     size_t CurrentCollectionCount = 0;
 39996| retry:
 39997|     CurrentCollectionCount = GarbageCollectTry(generation, low_memory_p, mode);
 39998|     if ((mode & collection_blocking) &&
 39999|         (generation == max_generation) &&
 40000|         (gc_heap::full_gc_counts[gc_type_blocking] == BlockingCollectionCountAtEntry))
 40001|     {
 40002| #ifdef BACKGROUND_GC
 40003|         if (gc_heap::background_running_p())
 40004|         {
 40005|             pGenGCHeap->background_gc_wait();
 40006|         }
 40007| #endif //BACKGROUND_GC
 40008|         goto retry;
 40009|     }
 40010|     if (CollectionCountAtEntry == CurrentCollectionCount)
 40011|     {
 40012|         goto retry;
 40013|     }
 40014|     return S_OK;
 40015| }
 40016| size_t
 40017| GCHeap::GarbageCollectTry (int generation, BOOL low_memory_p, int mode)
 40018| {
 40019|     int gen = (generation < 0) ?
 40020|                max_generation : min (generation, max_generation);
 40021|     gc_reason reason = reason_empty;
 40022|     if (low_memory_p)
 40023|     {
 40024|         if (mode & collection_blocking)
 40025|         {
 40026|             reason = reason_lowmemory_blocking;
 40027|         }
 40028|         else
 40029|         {
 40030|             reason = reason_lowmemory;
 40031|         }
 40032|     }
 40033|     else
 40034|     {
 40035|         reason = reason_induced;
 40036|     }
 40037|     if (reason == reason_induced)
 40038|     {
 40039|         if (mode & collection_aggressive)
 40040|         {
 40041|             reason = reason_induced_aggressive;
 40042|         }
 40043|         else if (mode & collection_compacting)
 40044|         {
 40045|             reason = reason_induced_compacting;
 40046|         }
 40047|         else if (mode & collection_non_blocking)
 40048|         {
 40049|             reason = reason_induced_noforce;
 40050|         }
 40051| #ifdef STRESS_HEAP
 40052|         else if (mode & collection_gcstress)
 40053|         {
 40054|             reason = reason_gcstress;
 40055|         }
 40056| #endif
 40057|     }
 40058|     return GarbageCollectGeneration (gen, reason);
 40059| }
 40060| #ifdef BACKGROUND_GC
 40061| void gc_heap::add_bgc_pause_duration_0()
 40062| {
 40063|     if (settings.concurrent)
 40064|     {
 40065|         uint64_t suspended_end_ts = GetHighPrecisionTimeStamp();
 40066|         size_t pause_duration = (size_t)(suspended_end_ts - suspended_start_time);
 40067|         last_recorded_gc_info* last_gc_info = &(last_bgc_info[last_bgc_info_index]);
 40068|         last_gc_info->pause_durations[0] = pause_duration;
 40069|         if (last_gc_info->index < last_ephemeral_gc_info.index)
 40070|         {
 40071|             last_gc_info->pause_durations[0] -= last_ephemeral_gc_info.pause_durations[0];
 40072|         }
 40073|         total_suspended_time += last_gc_info->pause_durations[0];
 40074|     }
 40075| }
 40076| last_recorded_gc_info* gc_heap::get_completed_bgc_info()
 40077| {
 40078|     int completed_bgc_index = gc_heap::background_running_p() ?
 40079|         (int)(!(gc_heap::last_bgc_info_index)) : (int)gc_heap::last_bgc_info_index;
 40080|     return &gc_heap::last_bgc_info[completed_bgc_index];
 40081| }
 40082| #endif //BACKGROUND_GC
 40083| void gc_heap::do_pre_gc()
 40084| {
 40085|     STRESS_LOG_GC_STACK;
 40086| #ifdef STRESS_LOG
 40087|     STRESS_LOG_GC_START(VolatileLoad(&settings.gc_index),
 40088|                         (uint32_t)settings.condemned_generation,
 40089|                         (uint32_t)settings.reason);
 40090| #endif // STRESS_LOG
 40091| #ifdef MULTIPLE_HEAPS
 40092|     gc_heap* hp = g_heaps[0];
 40093| #else
 40094|     gc_heap* hp = 0;
 40095| #endif //MULTIPLE_HEAPS
 40096| #ifdef BACKGROUND_GC
 40097|     settings.b_state = hp->current_bgc_state;
 40098|     if (settings.concurrent)
 40099|     {
 40100|         last_bgc_info_index = !last_bgc_info_index;
 40101|         last_bgc_info[last_bgc_info_index].index = settings.gc_index;
 40102|     }
 40103| #endif //BACKGROUND_GC
 40104| #ifdef TRACE_GC
 40105|     size_t total_allocated_since_last_gc = get_total_allocated_since_last_gc();
 40106| #ifdef BACKGROUND_GC
 40107|     dprintf (1, (ThreadStressLog::gcDetailedStartMsg(),
 40108|         VolatileLoad(&settings.gc_index),
 40109|         dd_collection_count (hp->dynamic_data_of (0)),
 40110|         settings.condemned_generation,
 40111|         total_allocated_since_last_gc,
 40112|         (settings.concurrent ? "BGC" : (gc_heap::background_running_p() ? "FGC" : "NGC")),
 40113|         settings.b_state));
 40114| #else
 40115|     dprintf (1, ("*GC* %d(gen0:%d)(%d)(alloc: %zd)",
 40116|         VolatileLoad(&settings.gc_index),
 40117|         dd_collection_count(hp->dynamic_data_of(0)),
 40118|         settings.condemned_generation,
 40119|         total_allocated_since_last_gc));
 40120| #endif //BACKGROUND_GC
 40121|     if (heap_hard_limit)
 40122|     {
 40123|         size_t total_heap_committed = get_total_committed_size();
 40124|         size_t total_heap_committed_recorded = current_total_committed - current_total_committed_bookkeeping;
 40125|         dprintf (1, ("(%d)GC commit BEG #%zd: %zd (recorded: %zd = %zd-%zd)",
 40126|             settings.condemned_generation,
 40127|             (size_t)settings.gc_index, total_heap_committed, total_heap_committed_recorded,
 40128|             current_total_committed, current_total_committed_bookkeeping));
 40129|     }
 40130| #endif //TRACE_GC
 40131|     GCHeap::UpdatePreGCCounters();
 40132|     fire_committed_usage_event();
 40133| #if defined(__linux__)
 40134|     GCToEEInterface::UpdateGCEventStatus(static_cast<int>(GCEventStatus::GetEnabledLevel(GCEventProvider_Default)),
 40135|                                          static_cast<int>(GCEventStatus::GetEnabledKeywords(GCEventProvider_Default)),
 40136|                                          static_cast<int>(GCEventStatus::GetEnabledLevel(GCEventProvider_Private)),
 40137|                                          static_cast<int>(GCEventStatus::GetEnabledKeywords(GCEventProvider_Private)));
 40138| #endif // __linux__
 40139|     if (settings.concurrent)
 40140|     {
 40141| #ifdef BACKGROUND_GC
 40142|         full_gc_counts[gc_type_background]++;
 40143| #endif // BACKGROUND_GC
 40144|     }
 40145|     else
 40146|     {
 40147|         if (settings.condemned_generation == max_generation)
 40148|         {
 40149|             full_gc_counts[gc_type_blocking]++;
 40150|         }
 40151|         else
 40152|         {
 40153| #ifdef BACKGROUND_GC
 40154|             if (settings.background_p)
 40155|             {
 40156|                 ephemeral_fgc_counts[settings.condemned_generation]++;
 40157|             }
 40158| #endif //BACKGROUND_GC
 40159|         }
 40160|     }
 40161| }
 40162| #ifdef GC_CONFIG_DRIVEN
 40163| void gc_heap::record_interesting_info_per_heap()
 40164| {
 40165|     if (!(settings.concurrent))
 40166|     {
 40167|         for (int i = 0; i < max_idp_count; i++)
 40168|         {
 40169|             interesting_data_per_heap[i] += interesting_data_per_gc[i];
 40170|         }
 40171|     }
 40172|     int compact_reason = get_gc_data_per_heap()->get_mechanism (gc_heap_compact);
 40173|     if (compact_reason >= 0)
 40174|         (compact_reasons_per_heap[compact_reason])++;
 40175|     int expand_mechanism = get_gc_data_per_heap()->get_mechanism (gc_heap_expand);
 40176|     if (expand_mechanism >= 0)
 40177|         (expand_mechanisms_per_heap[expand_mechanism])++;
 40178|     for (int i = 0; i < max_gc_mechanism_bits_count; i++)
 40179|     {
 40180|         if (get_gc_data_per_heap()->is_mechanism_bit_set ((gc_mechanism_bit_per_heap)i))
 40181|             (interesting_mechanism_bits_per_heap[i])++;
 40182|     }
 40183|     cprintf (("%2d | %6d | %1d | %1s | %2s | %2s | %2s | %2s | %2s || %5Id | %5Id | %5Id | %5Id | %5Id | %5Id | %5Id | %5Id | %5Id |",
 40184|             heap_number,
 40185|             (size_t)settings.gc_index,
 40186|             settings.condemned_generation,
 40187|             (settings.compaction ? (((compact_reason >= 0) && gc_heap_compact_reason_mandatory_p[compact_reason]) ? "M" : "W") : ""), // compaction
 40188|             ((expand_mechanism >= 0)? "X" : ""), // EX
 40189|             ((expand_mechanism == expand_reuse_normal) ? "X" : ""), // NF
 40190|             ((expand_mechanism == expand_reuse_bestfit) ? "X" : ""), // BF
 40191|             (get_gc_data_per_heap()->is_mechanism_bit_set (gc_mark_list_bit) ? "X" : ""), // ML
 40192|             (get_gc_data_per_heap()->is_mechanism_bit_set (gc_demotion_bit) ? "X" : ""), // DM
 40193|             interesting_data_per_gc[idp_pre_short],
 40194|             interesting_data_per_gc[idp_post_short],
 40195|             interesting_data_per_gc[idp_merged_pin],
 40196|             interesting_data_per_gc[idp_converted_pin],
 40197|             interesting_data_per_gc[idp_pre_pin],
 40198|             interesting_data_per_gc[idp_post_pin],
 40199|             interesting_data_per_gc[idp_pre_and_post_pin],
 40200|             interesting_data_per_gc[idp_pre_short_padded],
 40201|             interesting_data_per_gc[idp_post_short_padded]));
 40202| }
 40203| void gc_heap::record_global_mechanisms()
 40204| {
 40205|     for (int i = 0; i < max_global_mechanisms_count; i++)
 40206|     {
 40207|         if (gc_data_global.get_mechanism_p ((gc_global_mechanism_p)i))
 40208|         {
 40209|             ::record_global_mechanism (i);
 40210|         }
 40211|     }
 40212| }
 40213| BOOL gc_heap::should_do_sweeping_gc (BOOL compact_p)
 40214| {
 40215|     if (!compact_ratio)
 40216|         return (!compact_p);
 40217|     size_t compact_count = compact_or_sweep_gcs[0];
 40218|     size_t sweep_count = compact_or_sweep_gcs[1];
 40219|     size_t total_count = compact_count + sweep_count;
 40220|     BOOL should_compact = compact_p;
 40221|     if (total_count > 3)
 40222|     {
 40223|         if (compact_p)
 40224|         {
 40225|             int temp_ratio = (int)((compact_count + 1) * 100 / (total_count + 1));
 40226|             if (temp_ratio > compact_ratio)
 40227|             {
 40228|                 should_compact = FALSE;
 40229|             }
 40230|         }
 40231|         else
 40232|         {
 40233|             int temp_ratio = (int)((sweep_count + 1) * 100 / (total_count + 1));
 40234|             if (temp_ratio > (100 - compact_ratio))
 40235|             {
 40236|                 should_compact = TRUE;
 40237|             }
 40238|         }
 40239|     }
 40240|     return !should_compact;
 40241| }
 40242| #endif //GC_CONFIG_DRIVEN
 40243| #ifdef BGC_SERVO_TUNING
 40244| void gc_heap::check_and_adjust_bgc_tuning (int gen_number, size_t physical_size, ptrdiff_t virtual_fl_size)
 40245| {
 40246|     int min_gen_to_check = ((gen_number == max_generation) ? (max_generation - 1) : 0);
 40247|     if (settings.condemned_generation >= min_gen_to_check)
 40248|     {
 40249| #ifdef MULTIPLE_HEAPS
 40250|         gc_heap* hp = g_heaps[0];
 40251| #else
 40252|         gc_heap* hp = pGenGCHeap;
 40253| #endif //MULTIPLE_HEAPS
 40254|         size_t total_gen_size = physical_size;
 40255|         size_t total_generation_fl_size = get_total_generation_fl_size (gen_number);
 40256|         double gen_flr = (double)total_generation_fl_size * 100.0 / (double)total_gen_size;
 40257|         size_t gen1_index = dd_collection_count (hp->dynamic_data_of (max_generation - 1));
 40258|         size_t gen2_index = dd_collection_count (hp->dynamic_data_of (max_generation));
 40259|         bgc_tuning::tuning_calculation* current_gen_calc = &bgc_tuning::gen_calc[gen_number - max_generation];
 40260|         bgc_tuning::tuning_stats* current_gen_stats = &bgc_tuning::gen_stats[gen_number - max_generation];
 40261|         bool gen_size_inc_p = (total_gen_size > current_gen_calc->last_bgc_size);
 40262|         if ((settings.condemned_generation >= min_gen_to_check) &&
 40263|             (settings.condemned_generation != max_generation))
 40264|         {
 40265|             if (gen_size_inc_p)
 40266|             {
 40267|                 current_gen_stats->last_gen_increase_flr = gen_flr;
 40268|                 dprintf (BGC_TUNING_LOG, ("BTLp[g1: %zd, g2: %zd]: gen%d size inc %s %zd->%zd, flr: %.3f",
 40269|                         gen1_index, gen2_index, gen_number,
 40270|                         (gc_heap::background_running_p() ? "during bgc" : ""),
 40271|                         current_gen_stats->last_bgc_physical_size, total_gen_size, gen_flr));
 40272|             }
 40273|             if (!bgc_tuning::fl_tuning_triggered)
 40274|             {
 40275|                 if (bgc_tuning::enable_fl_tuning)
 40276|                 {
 40277|                     if (!((gc_heap::background_running_p() || (hp->current_bgc_state == bgc_initialized))))
 40278|                     {
 40279|                         assert (settings.entry_memory_load);
 40280|                         if ((settings.entry_memory_load >= (bgc_tuning::memory_load_goal * 2 / 3)) &&
 40281|                             (full_gc_counts[gc_type_background] >= 2))
 40282|                         {
 40283|                             bgc_tuning::next_bgc_p = true;
 40284|                             current_gen_calc->first_alloc_to_trigger = get_total_servo_alloc (gen_number);
 40285|                             dprintf (BGC_TUNING_LOG, ("BTL[g1: %zd] mem high enough: %d(goal: %d), gen%d fl alloc: %zd, trigger BGC!",
 40286|                                 gen1_index, settings.entry_memory_load, bgc_tuning::memory_load_goal,
 40287|                                 gen_number, current_gen_calc->first_alloc_to_trigger));
 40288|                         }
 40289|                     }
 40290|                 }
 40291|             }
 40292|         }
 40293|         if ((settings.condemned_generation == max_generation) && !(settings.concurrent))
 40294|         {
 40295|             size_t total_survived = get_total_surv_size (gen_number);
 40296|             size_t total_begin = get_total_begin_data_size (gen_number);
 40297|             double current_gc_surv_rate = (double)total_survived * 100.0 / (double)total_begin;
 40298|             double total_virtual_size = (double)physical_size + (double)virtual_fl_size;
 40299|             double total_fl_size = (double)total_generation_fl_size + (double)virtual_fl_size;
 40300|             double new_gen_flr = total_fl_size * 100.0 / total_virtual_size;
 40301|             dprintf (BGC_TUNING_LOG, ("BTL%d NGC2 size %zd->%zd, fl %zd(%.3f)->%zd(%.3f)",
 40302|                 gen_number, physical_size, (size_t)total_virtual_size,
 40303|                 total_generation_fl_size, gen_flr,
 40304|                 (size_t)total_fl_size, new_gen_flr));
 40305|             dprintf (BGC_TUNING_LOG, ("BTL%d* %zd, %.3f, %.3f, %.3f, %.3f, %.3f, %d, %d, %d, %zd",
 40306|                                     gen_number,
 40307|                                     (size_t)total_virtual_size,
 40308|                                     0.0,
 40309|                                     0.0,
 40310|                                     new_gen_flr,
 40311|                                     current_gen_stats->last_gen_increase_flr,
 40312|                                     current_gc_surv_rate,
 40313|                                     0,
 40314|                                     0,
 40315|                                     0,
 40316|                                     current_gen_calc->alloc_to_trigger));
 40317|             bgc_tuning::gen1_index_last_bgc_end = gen1_index;
 40318|             current_gen_calc->last_bgc_size = total_gen_size;
 40319|             current_gen_calc->last_bgc_flr = new_gen_flr;
 40320|             current_gen_calc->last_sweep_above_p = false;
 40321|             current_gen_calc->last_bgc_end_alloc = 0;
 40322|             current_gen_stats->last_alloc_end_to_start = 0;
 40323|             current_gen_stats->last_alloc_start_to_sweep = 0;
 40324|             current_gen_stats->last_alloc_sweep_to_end = 0;
 40325|             current_gen_stats->last_bgc_fl_size = total_generation_fl_size;
 40326|             current_gen_stats->last_bgc_surv_rate = current_gc_surv_rate;
 40327|             current_gen_stats->last_gen_increase_flr = 0;
 40328|         }
 40329|     }
 40330| }
 40331| void gc_heap::get_and_reset_loh_alloc_info()
 40332| {
 40333|     if (!bgc_tuning::enable_fl_tuning)
 40334|         return;
 40335|     total_loh_a_last_bgc = 0;
 40336|     uint64_t total_loh_a_no_bgc = 0;
 40337|     uint64_t total_loh_a_bgc_marking = 0;
 40338|     uint64_t total_loh_a_bgc_planning = 0;
 40339| #ifdef MULTIPLE_HEAPS
 40340|     for (int i = 0; i < gc_heap::n_heaps; i++)
 40341|     {
 40342|         gc_heap* hp = gc_heap::g_heaps[i];
 40343| #else //MULTIPLE_HEAPS
 40344|     {
 40345|         gc_heap* hp = pGenGCHeap;
 40346| #endif //MULTIPLE_HEAPS
 40347|         total_loh_a_no_bgc += hp->loh_a_no_bgc;
 40348|         hp->loh_a_no_bgc = 0;
 40349|         total_loh_a_bgc_marking += hp->loh_a_bgc_marking;
 40350|         hp->loh_a_bgc_marking = 0;
 40351|         total_loh_a_bgc_planning += hp->loh_a_bgc_planning;
 40352|         hp->loh_a_bgc_planning = 0;
 40353|     }
 40354|     dprintf (2, ("LOH alloc: outside bgc: %zd; bm: %zd; bp: %zd",
 40355|         total_loh_a_no_bgc,
 40356|         total_loh_a_bgc_marking,
 40357|         total_loh_a_bgc_planning));
 40358|     total_loh_a_last_bgc = total_loh_a_no_bgc + total_loh_a_bgc_marking + total_loh_a_bgc_planning;
 40359| }
 40360| #endif //BGC_SERVO_TUNING
 40361| bool gc_heap::is_pm_ratio_exceeded()
 40362| {
 40363|     size_t maxgen_frag = 0;
 40364|     size_t maxgen_size = 0;
 40365|     size_t total_heap_size = get_total_heap_size();
 40366| #ifdef MULTIPLE_HEAPS
 40367|     for (int i = 0; i < gc_heap::n_heaps; i++)
 40368|     {
 40369|         gc_heap* hp = gc_heap::g_heaps[i];
 40370| #else //MULTIPLE_HEAPS
 40371|     {
 40372|         gc_heap* hp = pGenGCHeap;
 40373| #endif //MULTIPLE_HEAPS
 40374|         maxgen_frag += dd_fragmentation (hp->dynamic_data_of (max_generation));
 40375|         maxgen_size += hp->generation_size (max_generation);
 40376|     }
 40377|     double maxgen_ratio = (double)maxgen_size / (double)total_heap_size;
 40378|     double maxgen_frag_ratio = (double)maxgen_frag / (double)maxgen_size;
 40379|     dprintf (GTC_LOG, ("maxgen %zd(%d%% total heap), frag: %zd (%d%% maxgen)",
 40380|         maxgen_size, (int)(maxgen_ratio * 100.0),
 40381|         maxgen_frag, (int)(maxgen_frag_ratio * 100.0)));
 40382|     bool maxgen_highfrag_p = ((maxgen_ratio > 0.5) && (maxgen_frag_ratio > 0.1));
 40383|     if (maxgen_highfrag_p)
 40384|     {
 40385|         settings.should_lock_elevation = FALSE;
 40386|         dprintf (GTC_LOG, ("high frag gen2, turn off elevation"));
 40387|     }
 40388|     return maxgen_highfrag_p;
 40389| }
 40390| void gc_heap::update_recorded_gen_data (last_recorded_gc_info* gc_info)
 40391| {
 40392|     memset (gc_info->gen_info, 0, sizeof (gc_info->gen_info));
 40393| #ifdef MULTIPLE_HEAPS
 40394|     for (int i = 0; i < gc_heap::n_heaps; i++)
 40395|     {
 40396|         gc_heap* hp = gc_heap::g_heaps[i];
 40397| #else //MULTIPLE_HEAPS
 40398|     {
 40399|         gc_heap* hp = pGenGCHeap;
 40400| #endif //MULTIPLE_HEAPS
 40401|         gc_history_per_heap* current_gc_data_per_heap = hp->get_gc_data_per_heap();
 40402|         for (int gen_number = 0; gen_number < total_generation_count; gen_number++)
 40403|         {
 40404|             recorded_generation_info* recorded_info = &(gc_info->gen_info[gen_number]);
 40405|             gc_generation_data* data = &(current_gc_data_per_heap->gen_data[gen_number]);
 40406|             recorded_info->size_before += data->size_before;
 40407|             recorded_info->fragmentation_before += data->free_list_space_before + data->free_obj_space_before;
 40408|             recorded_info->size_after += data->size_after;
 40409|             recorded_info->fragmentation_after += data->free_list_space_after + data->free_obj_space_after;
 40410|         }
 40411|     }
 40412| }
 40413| void gc_heap::do_post_gc()
 40414| {
 40415| #ifdef MULTIPLE_HEAPS
 40416|     gc_heap* hp = g_heaps[0];
 40417| #else
 40418|     gc_heap* hp = 0;
 40419| #endif //MULTIPLE_HEAPS
 40420|     GCToEEInterface::GcDone(settings.condemned_generation);
 40421|     GCToEEInterface::DiagGCEnd(VolatileLoad(&settings.gc_index),
 40422|                          (uint32_t)settings.condemned_generation,
 40423|                          (uint32_t)settings.reason,
 40424|                          !!settings.concurrent);
 40425|     add_to_history();
 40426|     uint32_t current_memory_load = 0;
 40427| #ifdef BGC_SERVO_TUNING
 40428|     if (bgc_tuning::enable_fl_tuning)
 40429|     {
 40430|         uint64_t current_available_physical = 0;
 40431|         size_t gen2_physical_size = 0;
 40432|         size_t gen3_physical_size = 0;
 40433|         ptrdiff_t gen2_virtual_fl_size = 0;
 40434|         ptrdiff_t gen3_virtual_fl_size = 0;
 40435|         ptrdiff_t vfl_from_kp = 0;
 40436|         ptrdiff_t vfl_from_ki = 0;
 40437|         gen2_physical_size = get_total_generation_size (max_generation);
 40438|         gen3_physical_size = get_total_generation_size (loh_generation);
 40439|         get_memory_info (&current_memory_load, &current_available_physical);
 40440|         if ((settings.condemned_generation == max_generation) && !settings.concurrent)
 40441|         {
 40442|             double gen2_size_ratio = (double)gen2_physical_size / ((double)gen2_physical_size + (double)gen3_physical_size);
 40443|             double total_virtual_fl_size = bgc_tuning::calculate_ml_tuning (current_available_physical, true, &vfl_from_kp, &vfl_from_ki);
 40444|             gen2_virtual_fl_size = (ptrdiff_t)(total_virtual_fl_size * gen2_size_ratio);
 40445|             gen3_virtual_fl_size = (ptrdiff_t)(total_virtual_fl_size * (1.0 - gen2_size_ratio));
 40446| #ifdef SIMPLE_DPRINTF
 40447|             dprintf (BGC_TUNING_LOG, ("BTL: ml: %d (g: %d)(%s), a: %zd (g: %zd, elg: %zd+%zd=%zd, %zd+%zd=%zd), vfl: %zd=%zd+%zd(NGC2)",
 40448|                 current_memory_load, bgc_tuning::memory_load_goal,
 40449|                 ((current_available_physical > bgc_tuning::available_memory_goal) ? "above" : "below"),
 40450|                 current_available_physical, bgc_tuning::available_memory_goal,
 40451|                 gen2_physical_size, gen2_virtual_fl_size, (gen2_physical_size + gen2_virtual_fl_size),
 40452|                 gen3_physical_size, gen3_virtual_fl_size, (gen3_physical_size + gen3_virtual_fl_size),
 40453|                 (ptrdiff_t)total_virtual_fl_size, vfl_from_kp, vfl_from_ki));
 40454| #endif //SIMPLE_DPRINTF
 40455|         }
 40456|         check_and_adjust_bgc_tuning (max_generation, gen2_physical_size, gen2_virtual_fl_size);
 40457|         check_and_adjust_bgc_tuning (loh_generation, gen3_physical_size, gen3_virtual_fl_size);
 40458|     }
 40459| #endif //BGC_SERVO_TUNING
 40460| #ifdef BACKGROUND_GC
 40461|     const char* str_gc_type = (settings.concurrent ? "BGC" : (gc_heap::background_running_p () ? "FGC" : "NGC"));
 40462| #else
 40463|     const char* str_gc_type = "NGC";
 40464| #endif //BACKGROUND_GC
 40465|     dprintf (1, (ThreadStressLog::gcDetailedEndMsg(),
 40466|         VolatileLoad (&settings.gc_index),
 40467|         dd_collection_count (hp->dynamic_data_of (0)),
 40468|         (size_t)(GetHighPrecisionTimeStamp () / 1000),
 40469|         settings.condemned_generation,
 40470|         (settings.concurrent ? "BGC" : (gc_heap::background_running_p() ? "FGC" : "NGC")),
 40471|         (settings.compaction ? "C" : "S"),
 40472|         (settings.promotion ? "P" : "S"),
 40473|         settings.entry_memory_load,
 40474|         current_memory_load));
 40475| #if defined(TRACE_GC) && defined(SIMPLE_DPRINTF)
 40476|     flush_gc_log (false);
 40477| #endif //TRACE_GC && SIMPLE_DPRINTF
 40478|     last_recorded_gc_info* last_gc_info = 0;
 40479| #ifdef BACKGROUND_GC
 40480|     if (settings.concurrent)
 40481|     {
 40482|         last_gc_info = &last_bgc_info[last_bgc_info_index];
 40483|         assert (last_gc_info->index == settings.gc_index);
 40484|     }
 40485|     else
 40486| #endif //BACKGROUND_GC
 40487|     {
 40488|         last_gc_info = ((settings.condemned_generation == max_generation) ?
 40489|                         &last_full_blocking_gc_info : &last_ephemeral_gc_info);
 40490|         last_gc_info->index = settings.gc_index;
 40491|     }
 40492|     size_t total_heap_committed = get_total_committed_size();
 40493|     last_gc_info->total_committed = total_heap_committed;
 40494|     last_gc_info->promoted = get_total_promoted();
 40495|     last_gc_info->pinned_objects = get_total_pinned_objects();
 40496|     last_gc_info->finalize_promoted_objects = GCHeap::GetFinalizablePromotedCount();
 40497|     if (!settings.concurrent)
 40498|     {
 40499|         dynamic_data* dd = hp->dynamic_data_of (settings.condemned_generation);
 40500|         uint64_t gc_start_ts = dd_time_clock (dd);
 40501|         size_t pause_duration = (size_t)(end_gc_time - dd_time_clock (dd));
 40502| #ifdef BACKGROUND_GC
 40503|         if ((hp->current_bgc_state != bgc_initialized) && (settings.reason != reason_pm_full_gc))
 40504|         {
 40505|             pause_duration += (size_t)(gc_start_ts - suspended_start_time);
 40506|         }
 40507| #endif //BACKGROUND_GC
 40508|         last_gc_info->pause_durations[0] = pause_duration;
 40509|         total_suspended_time += pause_duration;
 40510|         last_gc_info->pause_durations[1] = 0;
 40511|     }
 40512|     uint64_t total_process_time = end_gc_time - process_start_time;
 40513|     last_gc_info->pause_percentage = (float)(total_process_time ?
 40514|         ((double)total_suspended_time / (double)total_process_time * 100.0) : 0);
 40515|     update_recorded_gen_data (last_gc_info);
 40516|     last_gc_info->heap_size = get_total_heap_size();
 40517|     last_gc_info->fragmentation = get_total_fragmentation();
 40518|     if (settings.exit_memory_load != 0)
 40519|         last_gc_info->memory_load = settings.exit_memory_load;
 40520|     else if (settings.entry_memory_load != 0)
 40521|         last_gc_info->memory_load = settings.entry_memory_load;
 40522|     last_gc_info->condemned_generation = (uint8_t)settings.condemned_generation;
 40523|     last_gc_info->compaction = settings.compaction;
 40524|     last_gc_info->concurrent = settings.concurrent;
 40525| #ifdef BACKGROUND_GC
 40526|     is_last_recorded_bgc = settings.concurrent;
 40527| #endif //BACKGROUND_GC
 40528| #ifdef TRACE_GC
 40529|     if (heap_hard_limit)
 40530|     {
 40531|         size_t total_heap_committed_recorded = current_total_committed - current_total_committed_bookkeeping;
 40532|         dprintf (1, ("(%d)GC commit END #%zd: %zd (recorded: %zd=%zd-%zd), heap %zd, frag: %zd",
 40533|             settings.condemned_generation,
 40534|             (size_t)settings.gc_index, total_heap_committed, total_heap_committed_recorded,
 40535|             current_total_committed, current_total_committed_bookkeeping,
 40536|             last_gc_info->heap_size, last_gc_info->fragmentation));
 40537|     }
 40538| #endif //TRACE_GC
 40539|     if ((settings.condemned_generation == max_generation) && (!settings.concurrent))
 40540|     {
 40541|         if (pm_stress_on)
 40542|         {
 40543|             size_t full_compacting_gc_count = full_gc_counts[gc_type_compacting];
 40544|             if (provisional_mode_triggered)
 40545|             {
 40546|                 uint64_t r = gc_rand::get_rand(10);
 40547|                 if ((full_compacting_gc_count - provisional_triggered_gc_count) >= r)
 40548|                 {
 40549|                     provisional_mode_triggered = false;
 40550|                     provisional_off_gc_count = full_compacting_gc_count;
 40551|                     dprintf (GTC_LOG, ("%zd NGC2s when turned on, %zd NGCs since(%zd)",
 40552|                         provisional_triggered_gc_count, (full_compacting_gc_count - provisional_triggered_gc_count),
 40553|                         num_provisional_triggered));
 40554|                 }
 40555|             }
 40556|             else
 40557|             {
 40558|                 uint64_t r = gc_rand::get_rand(5);
 40559|                 if ((full_compacting_gc_count - provisional_off_gc_count) >= r)
 40560|                 {
 40561|                     provisional_mode_triggered = true;
 40562|                     provisional_triggered_gc_count = full_compacting_gc_count;
 40563|                     num_provisional_triggered++;
 40564|                     dprintf (GTC_LOG, ("%zd NGC2s when turned off, %zd NGCs since(%zd)",
 40565|                         provisional_off_gc_count, (full_compacting_gc_count - provisional_off_gc_count),
 40566|                         num_provisional_triggered));
 40567|                 }
 40568|             }
 40569|         }
 40570|         else
 40571|         {
 40572|             if (provisional_mode_triggered)
 40573|             {
 40574|                 if ((settings.entry_memory_load < high_memory_load_th) ||
 40575|                     !is_pm_ratio_exceeded())
 40576|                 {
 40577|                     dprintf (GTC_LOG, ("turning off PM"));
 40578|                     provisional_mode_triggered = false;
 40579|                 }
 40580|             }
 40581|             else if ((settings.entry_memory_load >= high_memory_load_th) && is_pm_ratio_exceeded())
 40582|             {
 40583|                 dprintf (GTC_LOG, ("highmem && highfrag - turning on PM"));
 40584|                 provisional_mode_triggered = true;
 40585|                 num_provisional_triggered++;
 40586|             }
 40587|         }
 40588|     }
 40589|     if (!settings.concurrent)
 40590|     {
 40591|         fire_committed_usage_event ();
 40592|     }
 40593|     GCHeap::UpdatePostGCCounters();
 40594|     reinit_pinned_objects();
 40595| #ifdef STRESS_LOG
 40596|     STRESS_LOG_GC_END(VolatileLoad(&settings.gc_index),
 40597|                       (uint32_t)settings.condemned_generation,
 40598|                       (uint32_t)settings.reason);
 40599| #endif // STRESS_LOG
 40600| #ifdef GC_CONFIG_DRIVEN
 40601|     if (!settings.concurrent)
 40602|     {
 40603|         if (settings.compaction)
 40604|             (compact_or_sweep_gcs[0])++;
 40605|         else
 40606|             (compact_or_sweep_gcs[1])++;
 40607|     }
 40608| #ifdef MULTIPLE_HEAPS
 40609|     for (int i = 0; i < n_heaps; i++)
 40610|         g_heaps[i]->record_interesting_info_per_heap();
 40611| #else
 40612|     record_interesting_info_per_heap();
 40613| #endif //MULTIPLE_HEAPS
 40614|     record_global_mechanisms();
 40615| #endif //GC_CONFIG_DRIVEN
 40616|     if (mark_list_overflow)
 40617|     {
 40618|         grow_mark_list();
 40619|         mark_list_overflow = false;
 40620|     }
 40621| }
 40622| unsigned GCHeap::GetGcCount()
 40623| {
 40624|     return (unsigned int)VolatileLoad(&pGenGCHeap->settings.gc_index);
 40625| }
 40626| size_t
 40627| GCHeap::GarbageCollectGeneration (unsigned int gen, gc_reason reason)
 40628| {
 40629|     dprintf (2, ("triggered a GC!"));
 40630| #ifdef COMMITTED_BYTES_SHADOW
 40631|     GCHeap::RefreshMemoryLimit();
 40632| #endif //COMMITTED_BYTES_SHADOW
 40633| #ifdef MULTIPLE_HEAPS
 40634|     gc_heap* hpt = gc_heap::g_heaps[0];
 40635| #else
 40636|     gc_heap* hpt = 0;
 40637| #endif //MULTIPLE_HEAPS
 40638|     bool cooperative_mode = true;
 40639|     dynamic_data* dd = hpt->dynamic_data_of (gen);
 40640|     size_t localCount = dd_collection_count (dd);
 40641|     enter_spin_lock (&gc_heap::gc_lock);
 40642|     dprintf (SPINLOCK_LOG, ("GC Egc"));
 40643|     ASSERT_HOLDING_SPIN_LOCK(&gc_heap::gc_lock);
 40644|     {
 40645|         size_t col_count = dd_collection_count (dd);
 40646|         if (localCount != col_count)
 40647|         {
 40648| #ifdef SYNCHRONIZATION_STATS
 40649|             gc_lock_contended++;
 40650| #endif //SYNCHRONIZATION_STATS
 40651|             dprintf (SPINLOCK_LOG, ("no need GC Lgc"));
 40652|             leave_spin_lock (&gc_heap::gc_lock);
 40653|             return col_count;
 40654|          }
 40655|     }
 40656|     gc_heap::g_low_memory_status = (reason == reason_lowmemory) ||
 40657|                                     (reason == reason_lowmemory_blocking) ||
 40658|                                     (gc_heap::latency_level == latency_level_memory_footprint);
 40659|     gc_trigger_reason = reason;
 40660| #ifdef MULTIPLE_HEAPS
 40661|     for (int i = 0; i < gc_heap::n_heaps; i++)
 40662|     {
 40663|         gc_heap::g_heaps[i]->reset_gc_done();
 40664|     }
 40665| #else
 40666|     gc_heap::reset_gc_done();
 40667| #endif //MULTIPLE_HEAPS
 40668|     gc_heap::gc_started = TRUE;
 40669|     {
 40670|         init_sync_log_stats();
 40671| #ifndef MULTIPLE_HEAPS
 40672|         cooperative_mode = gc_heap::enable_preemptive ();
 40673|         dprintf (2, ("Suspending EE"));
 40674|         gc_heap::suspended_start_time = GetHighPrecisionTimeStamp();
 40675|         BEGIN_TIMING(suspend_ee_during_log);
 40676|         GCToEEInterface::SuspendEE(SUSPEND_FOR_GC);
 40677|         END_TIMING(suspend_ee_during_log);
 40678|         gc_heap::proceed_with_gc_p = gc_heap::should_proceed_with_gc();
 40679|         gc_heap::disable_preemptive (cooperative_mode);
 40680|         if (gc_heap::proceed_with_gc_p)
 40681|             pGenGCHeap->settings.init_mechanisms();
 40682|         else
 40683|             gc_heap::update_collection_counts_for_no_gc();
 40684| #endif //!MULTIPLE_HEAPS
 40685|     }
 40686|     unsigned int condemned_generation_number = gen;
 40687|     FIRE_EVENT(GCTriggered, static_cast<uint32_t>(reason));
 40688| #ifdef MULTIPLE_HEAPS
 40689|     GcCondemnedGeneration = condemned_generation_number;
 40690|     cooperative_mode = gc_heap::enable_preemptive ();
 40691|     BEGIN_TIMING(gc_during_log);
 40692|     gc_heap::ee_suspend_event.Set();
 40693|     gc_heap::wait_for_gc_done();
 40694|     END_TIMING(gc_during_log);
 40695|     gc_heap::disable_preemptive (cooperative_mode);
 40696|     condemned_generation_number = GcCondemnedGeneration;
 40697| #else
 40698|     if (gc_heap::proceed_with_gc_p)
 40699|     {
 40700|         BEGIN_TIMING(gc_during_log);
 40701|         pGenGCHeap->garbage_collect (condemned_generation_number);
 40702|         if (gc_heap::pm_trigger_full_gc)
 40703|         {
 40704|             pGenGCHeap->garbage_collect_pm_full_gc();
 40705|         }
 40706|         END_TIMING(gc_during_log);
 40707|     }
 40708| #endif //MULTIPLE_HEAPS
 40709| #ifndef MULTIPLE_HEAPS
 40710| #ifdef BACKGROUND_GC
 40711|     if (!gc_heap::dont_restart_ee_p)
 40712| #endif //BACKGROUND_GC
 40713|     {
 40714| #ifdef BACKGROUND_GC
 40715|         gc_heap::add_bgc_pause_duration_0();
 40716| #endif //BACKGROUND_GC
 40717|         BEGIN_TIMING(restart_ee_during_log);
 40718|         GCToEEInterface::RestartEE(TRUE);
 40719|         END_TIMING(restart_ee_during_log);
 40720|     }
 40721| #endif //!MULTIPLE_HEAPS
 40722| #ifndef MULTIPLE_HEAPS
 40723|     process_sync_log_stats();
 40724|     gc_heap::gc_started = FALSE;
 40725|     gc_heap::set_gc_done();
 40726|     dprintf (SPINLOCK_LOG, ("GC Lgc"));
 40727|     leave_spin_lock (&gc_heap::gc_lock);
 40728| #endif //!MULTIPLE_HEAPS
 40729| #ifdef FEATURE_PREMORTEM_FINALIZATION
 40730|     GCToEEInterface::EnableFinalization(!pGenGCHeap->settings.concurrent && pGenGCHeap->settings.found_finalizers);
 40731| #endif // FEATURE_PREMORTEM_FINALIZATION
 40732|     return dd_collection_count (dd);
 40733| }
 40734| size_t      GCHeap::GetTotalBytesInUse ()
 40735| {
 40736|     enter_spin_lock (&pGenGCHeap->gc_lock);
 40737| #ifdef MULTIPLE_HEAPS
 40738|     size_t tot_size = 0;
 40739|     for (int i = 0; i < gc_heap::n_heaps; i++)
 40740|     {
 40741|         GCHeap* Hp = gc_heap::g_heaps [i]->vm_heap;
 40742|         tot_size += Hp->ApproxTotalBytesInUse();
 40743|     }
 40744| #else
 40745|     size_t tot_size = ApproxTotalBytesInUse();
 40746| #endif //MULTIPLE_HEAPS
 40747|     leave_spin_lock (&pGenGCHeap->gc_lock);
 40748|     return tot_size;
 40749| }
 40750| uint64_t GCHeap::GetTotalAllocatedBytes()
 40751| {
 40752| #ifdef MULTIPLE_HEAPS
 40753|     uint64_t total_alloc_bytes = 0;
 40754|     for (int i = 0; i < gc_heap::n_heaps; i++)
 40755|     {
 40756|         gc_heap* hp = gc_heap::g_heaps[i];
 40757|         total_alloc_bytes += hp->total_alloc_bytes_soh;
 40758|         total_alloc_bytes += hp->total_alloc_bytes_uoh;
 40759|     }
 40760|     return total_alloc_bytes;
 40761| #else
 40762|     return (pGenGCHeap->total_alloc_bytes_soh +  pGenGCHeap->total_alloc_bytes_uoh);
 40763| #endif //MULTIPLE_HEAPS
 40764| }
 40765| int GCHeap::CollectionCount (int generation, int get_bgc_fgc_count)
 40766| {
 40767|     if (get_bgc_fgc_count != 0)
 40768|     {
 40769| #ifdef BACKGROUND_GC
 40770|         if (generation == max_generation)
 40771|         {
 40772|             return (int)(gc_heap::full_gc_counts[gc_type_background]);
 40773|         }
 40774|         else
 40775|         {
 40776|             return (int)(gc_heap::ephemeral_fgc_counts[generation]);
 40777|         }
 40778| #else
 40779|         return 0;
 40780| #endif //BACKGROUND_GC
 40781|     }
 40782| #ifdef MULTIPLE_HEAPS
 40783|     gc_heap* hp = gc_heap::g_heaps [0];
 40784| #else  //MULTIPLE_HEAPS
 40785|     gc_heap* hp = pGenGCHeap;
 40786| #endif //MULTIPLE_HEAPS
 40787|     if (generation > max_generation)
 40788|         return 0;
 40789|     else
 40790|         return (int)dd_collection_count (hp->dynamic_data_of (generation));
 40791| }
 40792| size_t GCHeap::ApproxTotalBytesInUse(BOOL small_heap_only)
 40793| {
 40794|     size_t totsize = 0;
 40795|     generation* gen = pGenGCHeap->generation_of (0);
 40796|     size_t gen0_frag = generation_free_list_space (gen) + generation_free_obj_space (gen);
 40797|     uint8_t* current_alloc_allocated = pGenGCHeap->alloc_allocated;
 40798|     heap_segment* current_eph_seg = pGenGCHeap->ephemeral_heap_segment;
 40799|     size_t gen0_size = 0;
 40800| #ifdef USE_REGIONS
 40801|     heap_segment* gen0_seg = generation_start_segment (gen);
 40802|     while (gen0_seg)
 40803|     {
 40804|         uint8_t* end = in_range_for_segment (current_alloc_allocated, gen0_seg) ?
 40805|                        current_alloc_allocated : heap_segment_allocated (gen0_seg);
 40806|         gen0_size += end - heap_segment_mem (gen0_seg);
 40807|         if (gen0_seg == current_eph_seg)
 40808|         {
 40809|             break;
 40810|         }
 40811|         gen0_seg = heap_segment_next (gen0_seg);
 40812|     }
 40813| #else //USE_REGIONS
 40814|     gen0_size = current_alloc_allocated - heap_segment_mem (current_eph_seg);
 40815| #endif //USE_REGIONS
 40816|     totsize = gen0_size - gen0_frag;
 40817|     int stop_gen_index = max_generation;
 40818| #ifdef BACKGROUND_GC
 40819|     if (gc_heap::current_c_gc_state == c_gc_state_planning)
 40820|     {
 40821|         generation* oldest_gen = pGenGCHeap->generation_of (max_generation);
 40822|         totsize = pGenGCHeap->background_soh_size_end_mark - generation_free_list_space (oldest_gen) - generation_free_obj_space (oldest_gen);
 40823|         stop_gen_index--;
 40824|     }
 40825| #endif //BACKGROUND_GC
 40826|     for (int i = (max_generation - 1); i <= stop_gen_index; i++)
 40827|     {
 40828|         generation* gen = pGenGCHeap->generation_of (i);
 40829|         totsize += pGenGCHeap->generation_size (i) - generation_free_list_space (gen) - generation_free_obj_space (gen);
 40830|     }
 40831|     if (!small_heap_only)
 40832|     {
 40833|         for (int i = uoh_start_generation; i < total_generation_count; i++)
 40834|         {
 40835|             generation* gen = pGenGCHeap->generation_of (i);
 40836|             totsize += pGenGCHeap->generation_size (i) - generation_free_list_space (gen) - generation_free_obj_space (gen);
 40837|         }
 40838|     }
 40839|     return totsize;
 40840| }
 40841| #ifdef MULTIPLE_HEAPS
 40842| void GCHeap::AssignHeap (alloc_context* acontext)
 40843| {
 40844|     acontext->set_alloc_heap(GetHeap(heap_select::select_heap(acontext)));
 40845|     acontext->set_home_heap(acontext->get_alloc_heap());
 40846| }
 40847| GCHeap* GCHeap::GetHeap (int n)
 40848| {
 40849|     assert (n < gc_heap::n_heaps);
 40850|     return gc_heap::g_heaps[n]->vm_heap;
 40851| }
 40852| #endif //MULTIPLE_HEAPS
 40853| bool GCHeap::IsThreadUsingAllocationContextHeap(gc_alloc_context* context, int thread_number)
 40854| {
 40855|     alloc_context* acontext = static_cast<alloc_context*>(context);
 40856| #ifdef MULTIPLE_HEAPS
 40857|     assert (thread_number < gc_heap::n_heaps);
 40858|     assert ((acontext->get_home_heap() == 0) ||
 40859|             (acontext->get_home_heap()->pGenGCHeap->heap_number < gc_heap::n_heaps));
 40860|     return ((acontext->get_home_heap() == GetHeap(thread_number)) ||
 40861|             ((acontext->get_home_heap() == 0) && (thread_number == 0)));
 40862| #else
 40863|     UNREFERENCED_PARAMETER(acontext);
 40864|     UNREFERENCED_PARAMETER(thread_number);
 40865|     return true;
 40866| #endif //MULTIPLE_HEAPS
 40867| }
 40868| int GCHeap::GetNumberOfHeaps ()
 40869| {
 40870| #ifdef MULTIPLE_HEAPS
 40871|     return gc_heap::n_heaps;
 40872| #else
 40873|     return 1;
 40874| #endif //MULTIPLE_HEAPS
 40875| }
 40876| /*
 40877|   in this way we spend extra time cycling through all the heaps while create the handle
 40878|   it ought to be changed by keeping alloc_context.home_heap as number (equals heap_number)
 40879| */
 40880| int GCHeap::GetHomeHeapNumber ()
 40881| {
 40882| #ifdef MULTIPLE_HEAPS
 40883|     gc_alloc_context* ctx = GCToEEInterface::GetAllocContext();
 40884|     if (!ctx)
 40885|     {
 40886|         return 0;
 40887|     }
 40888|     GCHeap *hp = static_cast<alloc_context*>(ctx)->get_home_heap();
 40889|     return (hp ? hp->pGenGCHeap->heap_number : 0);
 40890| #else
 40891|     return 0;
 40892| #endif //MULTIPLE_HEAPS
 40893| }
 40894| unsigned int GCHeap::GetCondemnedGeneration()
 40895| {
 40896|     return gc_heap::settings.condemned_generation;
 40897| }
 40898| void GCHeap::GetMemoryInfo(uint64_t* highMemLoadThresholdBytes,
 40899|                            uint64_t* totalAvailableMemoryBytes,
 40900|                            uint64_t* lastRecordedMemLoadBytes,
 40901|                            uint64_t* lastRecordedHeapSizeBytes,
 40902|                            uint64_t* lastRecordedFragmentationBytes,
 40903|                            uint64_t* totalCommittedBytes,
 40904|                            uint64_t* promotedBytes,
 40905|                            uint64_t* pinnedObjectCount,
 40906|                            uint64_t* finalizationPendingCount,
 40907|                            uint64_t* index,
 40908|                            uint32_t* generation,
 40909|                            uint32_t* pauseTimePct,
 40910|                            bool* isCompaction,
 40911|                            bool* isConcurrent,
 40912|                            uint64_t* genInfoRaw,
 40913|                            uint64_t* pauseInfoRaw,
 40914|                            int kind)
 40915| {
 40916|     last_recorded_gc_info* last_gc_info = 0;
 40917|     if ((gc_kind)kind == gc_kind_ephemeral)
 40918|     {
 40919|         last_gc_info = &gc_heap::last_ephemeral_gc_info;
 40920|     }
 40921|     else if ((gc_kind)kind == gc_kind_full_blocking)
 40922|     {
 40923|         last_gc_info = &gc_heap::last_full_blocking_gc_info;
 40924|     }
 40925| #ifdef BACKGROUND_GC
 40926|     else if ((gc_kind)kind == gc_kind_background)
 40927|     {
 40928|         last_gc_info = gc_heap::get_completed_bgc_info();
 40929|     }
 40930| #endif //BACKGROUND_GC
 40931|     else
 40932|     {
 40933|         assert ((gc_kind)kind == gc_kind_any);
 40934| #ifdef BACKGROUND_GC
 40935|         if (gc_heap::is_last_recorded_bgc)
 40936|         {
 40937|             last_gc_info = gc_heap::get_completed_bgc_info();
 40938|         }
 40939|         else
 40940| #endif //BACKGROUND_GC
 40941|         {
 40942|             last_gc_info = ((gc_heap::last_ephemeral_gc_info.index > gc_heap::last_full_blocking_gc_info.index) ?
 40943|                 &gc_heap::last_ephemeral_gc_info : &gc_heap::last_full_blocking_gc_info);
 40944|         }
 40945|     }
 40946|     *highMemLoadThresholdBytes = (uint64_t) (((double)(gc_heap::high_memory_load_th)) / 100 * gc_heap::total_physical_mem);
 40947|     *totalAvailableMemoryBytes = gc_heap::heap_hard_limit != 0 ? gc_heap::heap_hard_limit : gc_heap::total_physical_mem;
 40948|     *lastRecordedMemLoadBytes = (uint64_t) (((double)(last_gc_info->memory_load)) / 100 * gc_heap::total_physical_mem);
 40949|     *lastRecordedHeapSizeBytes = last_gc_info->heap_size;
 40950|     *lastRecordedFragmentationBytes = last_gc_info->fragmentation;
 40951|     *totalCommittedBytes = last_gc_info->total_committed;
 40952|     *promotedBytes = last_gc_info->promoted;
 40953|     *pinnedObjectCount = last_gc_info->pinned_objects;
 40954|     *finalizationPendingCount = last_gc_info->finalize_promoted_objects;
 40955|     *index = last_gc_info->index;
 40956|     *generation = last_gc_info->condemned_generation;
 40957|     *pauseTimePct = (int)(last_gc_info->pause_percentage * 100);
 40958|     *isCompaction = last_gc_info->compaction;
 40959|     *isConcurrent = last_gc_info->concurrent;
 40960|     int genInfoIndex = 0;
 40961|     for (int i = 0; i < total_generation_count; i++)
 40962|     {
 40963|         genInfoRaw[genInfoIndex++] = last_gc_info->gen_info[i].size_before;
 40964|         genInfoRaw[genInfoIndex++] = last_gc_info->gen_info[i].fragmentation_before;
 40965|         genInfoRaw[genInfoIndex++] = last_gc_info->gen_info[i].size_after;
 40966|         genInfoRaw[genInfoIndex++] = last_gc_info->gen_info[i].fragmentation_after;
 40967|     }
 40968|     for (int i = 0; i < 2; i++)
 40969|     {
 40970|         pauseInfoRaw[i] = (uint64_t)(last_gc_info->pause_durations[i]) * 10;
 40971|     }
 40972| #ifdef _DEBUG
 40973|     if (VolatileLoadWithoutBarrier (&last_gc_info->index) != 0)
 40974|     {
 40975|         if ((gc_kind)kind == gc_kind_ephemeral)
 40976|         {
 40977|             assert (last_gc_info->condemned_generation < max_generation);
 40978|         }
 40979|         else if ((gc_kind)kind == gc_kind_full_blocking)
 40980|         {
 40981|             assert (last_gc_info->condemned_generation == max_generation);
 40982|             assert (last_gc_info->concurrent == false);
 40983|         }
 40984| #ifdef BACKGROUND_GC
 40985|         else if ((gc_kind)kind == gc_kind_background)
 40986|         {
 40987|             assert (last_gc_info->condemned_generation == max_generation);
 40988|             assert (last_gc_info->concurrent == true);
 40989|         }
 40990| #endif //BACKGROUND_GC
 40991|     }
 40992| #endif //_DEBUG
 40993| }
 40994| int64_t GCHeap::GetTotalPauseDuration()
 40995| {
 40996|     return (int64_t)(gc_heap::total_suspended_time * 10);
 40997| }
 40998| void GCHeap::EnumerateConfigurationValues(void* context, ConfigurationValueFunc configurationValueFunc)
 40999| {
 41000|     GCConfig::EnumerateConfigurationValues(context, configurationValueFunc);
 41001| }
 41002| uint32_t GCHeap::GetMemoryLoad()
 41003| {
 41004|     uint32_t memory_load = 0;
 41005|     if (gc_heap::settings.exit_memory_load != 0)
 41006|         memory_load = gc_heap::settings.exit_memory_load;
 41007|     else if (gc_heap::settings.entry_memory_load != 0)
 41008|         memory_load = gc_heap::settings.entry_memory_load;
 41009|     return memory_load;
 41010| }
 41011| int GCHeap::GetGcLatencyMode()
 41012| {
 41013|     return (int)(pGenGCHeap->settings.pause_mode);
 41014| }
 41015| int GCHeap::SetGcLatencyMode (int newLatencyMode)
 41016| {
 41017|     if (gc_heap::settings.pause_mode == pause_no_gc)
 41018|         return (int)set_pause_mode_no_gc;
 41019|     gc_pause_mode new_mode = (gc_pause_mode)newLatencyMode;
 41020|     if (new_mode == pause_low_latency)
 41021|     {
 41022| #ifndef MULTIPLE_HEAPS
 41023|         pGenGCHeap->settings.pause_mode = new_mode;
 41024| #endif //!MULTIPLE_HEAPS
 41025|     }
 41026|     else if (new_mode == pause_sustained_low_latency)
 41027|     {
 41028| #ifdef BACKGROUND_GC
 41029|         if (gc_heap::gc_can_use_concurrent)
 41030|         {
 41031|             pGenGCHeap->settings.pause_mode = new_mode;
 41032|         }
 41033| #endif //BACKGROUND_GC
 41034|     }
 41035|     else
 41036|     {
 41037|         pGenGCHeap->settings.pause_mode = new_mode;
 41038|     }
 41039| #ifdef BACKGROUND_GC
 41040|     if (gc_heap::background_running_p())
 41041|     {
 41042|         if (gc_heap::saved_bgc_settings.pause_mode != new_mode)
 41043|         {
 41044|             gc_heap::saved_bgc_settings.pause_mode = new_mode;
 41045|         }
 41046|     }
 41047| #endif //BACKGROUND_GC
 41048|     return (int)set_pause_mode_success;
 41049| }
 41050| int GCHeap::GetLOHCompactionMode()
 41051| {
 41052| #ifdef FEATURE_LOH_COMPACTION
 41053|     return pGenGCHeap->loh_compaction_mode;
 41054| #else
 41055|     return loh_compaction_default;
 41056| #endif //FEATURE_LOH_COMPACTION
 41057| }
 41058| void GCHeap::SetLOHCompactionMode (int newLOHCompactionMode)
 41059| {
 41060| #ifdef FEATURE_LOH_COMPACTION
 41061|     pGenGCHeap->loh_compaction_mode = (gc_loh_compaction_mode)newLOHCompactionMode;
 41062| #endif //FEATURE_LOH_COMPACTION
 41063| }
 41064| bool GCHeap::RegisterForFullGCNotification(uint32_t gen2Percentage,
 41065|                                            uint32_t lohPercentage)
 41066| {
 41067| #ifdef MULTIPLE_HEAPS
 41068|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 41069|     {
 41070|         gc_heap* hp = gc_heap::g_heaps [hn];
 41071|         hp->fgn_last_alloc = dd_new_allocation (hp->dynamic_data_of (0));
 41072|         hp->fgn_maxgen_percent = gen2Percentage;
 41073|     }
 41074| #else //MULTIPLE_HEAPS
 41075|     pGenGCHeap->fgn_last_alloc = dd_new_allocation (pGenGCHeap->dynamic_data_of (0));
 41076|     pGenGCHeap->fgn_maxgen_percent = gen2Percentage;
 41077| #endif //MULTIPLE_HEAPS
 41078|     pGenGCHeap->full_gc_approach_event.Reset();
 41079|     pGenGCHeap->full_gc_end_event.Reset();
 41080|     pGenGCHeap->full_gc_approach_event_set = false;
 41081|     pGenGCHeap->fgn_loh_percent = lohPercentage;
 41082|     return TRUE;
 41083| }
 41084| bool GCHeap::CancelFullGCNotification()
 41085| {
 41086| #ifdef MULTIPLE_HEAPS
 41087|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 41088|     {
 41089|         gc_heap* hp = gc_heap::g_heaps [hn];
 41090|         hp->fgn_maxgen_percent = 0;
 41091|     }
 41092| #else //MULTIPLE_HEAPS
 41093|     pGenGCHeap->fgn_maxgen_percent = 0;
 41094| #endif //MULTIPLE_HEAPS
 41095|     pGenGCHeap->fgn_loh_percent = 0;
 41096|     pGenGCHeap->full_gc_approach_event.Set();
 41097|     pGenGCHeap->full_gc_end_event.Set();
 41098|     return TRUE;
 41099| }
 41100| int GCHeap::WaitForFullGCApproach(int millisecondsTimeout)
 41101| {
 41102|     dprintf (2, ("WFGA: Begin wait"));
 41103|     int result = gc_heap::full_gc_wait (&(pGenGCHeap->full_gc_approach_event), millisecondsTimeout);
 41104|     dprintf (2, ("WFGA: End wait"));
 41105|     return result;
 41106| }
 41107| int GCHeap::WaitForFullGCComplete(int millisecondsTimeout)
 41108| {
 41109|     dprintf (2, ("WFGE: Begin wait"));
 41110|     int result = gc_heap::full_gc_wait (&(pGenGCHeap->full_gc_end_event), millisecondsTimeout);
 41111|     dprintf (2, ("WFGE: End wait"));
 41112|     return result;
 41113| }
 41114| int GCHeap::StartNoGCRegion(uint64_t totalSize, bool lohSizeKnown, uint64_t lohSize, bool disallowFullBlockingGC)
 41115| {
 41116|     NoGCRegionLockHolder lh;
 41117|     dprintf (1, ("begin no gc called"));
 41118|     start_no_gc_region_status status = gc_heap::prepare_for_no_gc_region (totalSize, lohSizeKnown, lohSize, disallowFullBlockingGC);
 41119|     if (status == start_no_gc_success)
 41120|     {
 41121|         GarbageCollect (max_generation);
 41122|         status = gc_heap::get_start_no_gc_region_status();
 41123|     }
 41124|     if (status != start_no_gc_success)
 41125|         gc_heap::handle_failure_for_no_gc();
 41126|     return (int)status;
 41127| }
 41128| int GCHeap::EndNoGCRegion()
 41129| {
 41130|     NoGCRegionLockHolder lh;
 41131|     return (int)gc_heap::end_no_gc_region();
 41132| }
 41133| void GCHeap::PublishObject (uint8_t* Obj)
 41134| {
 41135| #ifdef BACKGROUND_GC
 41136|     gc_heap* hp = gc_heap::heap_of (Obj);
 41137|     hp->bgc_alloc_lock->uoh_alloc_done (Obj);
 41138|     hp->bgc_untrack_uoh_alloc();
 41139| #endif //BACKGROUND_GC
 41140| }
 41141| size_t GCHeap::ApproxFreeBytes()
 41142| {
 41143|     enter_spin_lock (&pGenGCHeap->gc_lock);
 41144|     generation* gen = pGenGCHeap->generation_of (0);
 41145|     size_t res = generation_allocation_limit (gen) - generation_allocation_pointer (gen);
 41146|     leave_spin_lock (&pGenGCHeap->gc_lock);
 41147|     return res;
 41148| }
 41149| HRESULT GCHeap::GetGcCounters(int gen, gc_counters* counters)
 41150| {
 41151|     if ((gen < 0) || (gen > max_generation))
 41152|         return E_FAIL;
 41153| #ifdef MULTIPLE_HEAPS
 41154|     counters->current_size = 0;
 41155|     counters->promoted_size = 0;
 41156|     counters->collection_count = 0;
 41157|     for (int i = 0; i < gc_heap::n_heaps; i++)
 41158|     {
 41159|         dynamic_data* dd = gc_heap::g_heaps [i]->dynamic_data_of (gen);
 41160|         counters->current_size += dd_current_size (dd);
 41161|         counters->promoted_size += dd_promoted_size (dd);
 41162|         if (i == 0)
 41163|         counters->collection_count += dd_collection_count (dd);
 41164|     }
 41165| #else
 41166|     dynamic_data* dd = pGenGCHeap->dynamic_data_of (gen);
 41167|     counters->current_size = dd_current_size (dd);
 41168|     counters->promoted_size = dd_promoted_size (dd);
 41169|     counters->collection_count = dd_collection_count (dd);
 41170| #endif //MULTIPLE_HEAPS
 41171|     return S_OK;
 41172| }
 41173| size_t GCHeap::GetValidSegmentSize(bool large_seg)
 41174| {
 41175| #ifdef USE_REGIONS
 41176|     return (large_seg ? global_region_allocator.get_large_region_alignment() :
 41177|                         global_region_allocator.get_region_alignment());
 41178| #else
 41179|     return (large_seg ? gc_heap::min_uoh_segment_size : gc_heap::soh_segment_size);
 41180| #endif //USE_REGIONS
 41181| }
 41182| size_t gc_heap::get_gen0_min_size()
 41183| {
 41184|     size_t gen0size = static_cast<size_t>(GCConfig::GetGen0Size());
 41185|     bool is_config_invalid = ((gen0size == 0) || !g_theGCHeap->IsValidGen0MaxSize(gen0size));
 41186|     if (is_config_invalid)
 41187|     {
 41188| #ifdef SERVER_GC
 41189|         gen0size = max(GCToOSInterface::GetCacheSizePerLogicalCpu(FALSE),(256*1024));
 41190|         size_t trueSize = max(GCToOSInterface::GetCacheSizePerLogicalCpu(TRUE),(256*1024));
 41191|         dprintf (1, ("cache: %zd-%zd",
 41192|             GCToOSInterface::GetCacheSizePerLogicalCpu(FALSE),
 41193|             GCToOSInterface::GetCacheSizePerLogicalCpu(TRUE)));
 41194|         int n_heaps = gc_heap::n_heaps;
 41195| #else //SERVER_GC
 41196|         size_t trueSize = GCToOSInterface::GetCacheSizePerLogicalCpu(TRUE);
 41197|         gen0size = max((4*trueSize/5),(256*1024));
 41198|         trueSize = max(trueSize, (256*1024));
 41199|         int n_heaps = 1;
 41200| #endif //SERVER_GC
 41201| #ifdef DYNAMIC_HEAP_COUNT
 41202|         if (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes)
 41203|         {
 41204|             gen0size = min (gen0size, (4*1024*1024));
 41205|         }
 41206| #endif //DYNAMIC_HEAP_COUNT
 41207|         dprintf (1, ("gen0size: %zd * %d = %zd, physical mem: %zd / 6 = %zd",
 41208|                 gen0size, n_heaps, (gen0size * n_heaps),
 41209|                 gc_heap::total_physical_mem,
 41210|                 gc_heap::total_physical_mem / 6));
 41211|         while ((gen0size * n_heaps) > (gc_heap::total_physical_mem / 6))
 41212|         {
 41213|             gen0size = gen0size / 2;
 41214|             if (gen0size <= trueSize)
 41215|             {
 41216|                 gen0size = trueSize;
 41217|                 break;
 41218|             }
 41219|         }
 41220|     }
 41221| #ifdef FEATURE_EVENT_TRACE
 41222|     else
 41223|     {
 41224|         gen0_min_budget_from_config = gen0size;
 41225|     }
 41226| #endif //FEATURE_EVENT_TRACE
 41227|     size_t seg_size = gc_heap::soh_segment_size;
 41228|     assert (seg_size);
 41229|     if (gen0size >= (seg_size / 2))
 41230|         gen0size = seg_size / 2;
 41231|     if (is_config_invalid)
 41232|     {
 41233|         if (heap_hard_limit)
 41234|         {
 41235|             size_t gen0size_seg = seg_size / 8;
 41236|             if (gen0size >= gen0size_seg)
 41237|             {
 41238|                 dprintf (1, ("gen0 limited by seg size %zd->%zd", gen0size, gen0size_seg));
 41239|                 gen0size = gen0size_seg;
 41240|             }
 41241|         }
 41242|         gen0size = gen0size / 8 * 5;
 41243|     }
 41244| #ifdef USE_REGIONS
 41245| #ifdef STRESS_REGIONS
 41246|     gen0size = ((size_t)1 << min_segment_size_shr) * 3;
 41247| #endif //STRESS_REGIONS
 41248| #endif //USE_REGIONS
 41249|     gen0size = Align (gen0size);
 41250|     return gen0size;
 41251| }
 41252| void GCHeap::SetReservedVMLimit (size_t vmlimit)
 41253| {
 41254|     gc_heap::reserved_memory_limit = vmlimit;
 41255| }
 41256| #ifdef FEATURE_PREMORTEM_FINALIZATION
 41257| Object* GCHeap::GetNextFinalizableObject()
 41258| {
 41259| #ifdef MULTIPLE_HEAPS
 41260|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 41261|     {
 41262|         gc_heap* hp = gc_heap::g_heaps [hn];
 41263|         Object* O = hp->finalize_queue->GetNextFinalizableObject(TRUE);
 41264|         if (O)
 41265|             return O;
 41266|     }
 41267|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 41268|     {
 41269|         gc_heap* hp = gc_heap::g_heaps [hn];
 41270|         Object* O = hp->finalize_queue->GetNextFinalizableObject(FALSE);
 41271|         if (O)
 41272|             return O;
 41273|     }
 41274|     return 0;
 41275| #else //MULTIPLE_HEAPS
 41276|     return pGenGCHeap->finalize_queue->GetNextFinalizableObject();
 41277| #endif //MULTIPLE_HEAPS
 41278| }
 41279| size_t GCHeap::GetNumberFinalizableObjects()
 41280| {
 41281| #ifdef MULTIPLE_HEAPS
 41282|     size_t cnt = 0;
 41283|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 41284|     {
 41285|         gc_heap* hp = gc_heap::g_heaps [hn];
 41286|         cnt += hp->finalize_queue->GetNumberFinalizableObjects();
 41287|     }
 41288|     return cnt;
 41289| #else //MULTIPLE_HEAPS
 41290|     return pGenGCHeap->finalize_queue->GetNumberFinalizableObjects();
 41291| #endif //MULTIPLE_HEAPS
 41292| }
 41293| size_t GCHeap::GetFinalizablePromotedCount()
 41294| {
 41295| #ifdef MULTIPLE_HEAPS
 41296|     size_t cnt = 0;
 41297|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 41298|     {
 41299|         gc_heap* hp = gc_heap::g_heaps [hn];
 41300|         cnt += hp->finalize_queue->GetPromotedCount();
 41301|     }
 41302|     return cnt;
 41303| #else //MULTIPLE_HEAPS
 41304|     return pGenGCHeap->finalize_queue->GetPromotedCount();
 41305| #endif //MULTIPLE_HEAPS
 41306| }
 41307| bool GCHeap::RegisterForFinalization (int gen, Object* obj)
 41308| {
 41309|     if (gen == -1)
 41310|         gen = 0;
 41311|     if (((((CObjectHeader*)obj)->GetHeader()->GetBits()) & BIT_SBLK_FINALIZER_RUN))
 41312|     {
 41313|         ((CObjectHeader*)obj)->GetHeader()->ClrBit(BIT_SBLK_FINALIZER_RUN);
 41314|         return true;
 41315|     }
 41316|     else
 41317|     {
 41318|         gc_heap* hp = gc_heap::heap_of ((uint8_t*)obj);
 41319|         return hp->finalize_queue->RegisterForFinalization (gen, obj);
 41320|     }
 41321| }
 41322| void GCHeap::SetFinalizationRun (Object* obj)
 41323| {
 41324|     ((CObjectHeader*)obj)->GetHeader()->SetBit(BIT_SBLK_FINALIZER_RUN);
 41325| }
 41326| inline
 41327| unsigned int gen_segment (int gen)
 41328| {
 41329|     assert (((signed)total_generation_count - gen - 1)>=0);
 41330|     return (total_generation_count - gen - 1);
 41331| }
 41332| bool CFinalize::Initialize()
 41333| {
 41334|     CONTRACTL {
 41335|         NOTHROW;
 41336|         GC_NOTRIGGER;
 41337|     } CONTRACTL_END;
 41338|     m_Array = new (nothrow)(Object*[100]);
 41339|     if (!m_Array)
 41340|     {
 41341|         ASSERT (m_Array);
 41342|         STRESS_LOG_OOM_STACK(sizeof(Object*[100]));
 41343|         if (GCConfig::GetBreakOnOOM())
 41344|         {
 41345|             GCToOSInterface::DebugBreak();
 41346|         }
 41347|         return false;
 41348|     }
 41349|     m_EndArray = &m_Array[100];
 41350|     for (int i =0; i < FreeList; i++)
 41351|     {
 41352|         SegQueueLimit (i) = m_Array;
 41353|     }
 41354|     m_PromotedCount = 0;
 41355|     lock = -1;
 41356| #ifdef _DEBUG
 41357|     lockowner_threadid.Clear();
 41358| #endif // _DEBUG
 41359|     return true;
 41360| }
 41361| CFinalize::~CFinalize()
 41362| {
 41363|     delete m_Array;
 41364| }
 41365| size_t CFinalize::GetPromotedCount ()
 41366| {
 41367|     return m_PromotedCount;
 41368| }
 41369| inline
 41370| void CFinalize::EnterFinalizeLock()
 41371| {
 41372|     _ASSERTE(dbgOnly_IsSpecialEEThread() ||
 41373|              GCToEEInterface::GetThread() == 0 ||
 41374|              GCToEEInterface::IsPreemptiveGCDisabled());
 41375| retry:
 41376|     if (Interlocked::CompareExchange(&lock, 0, -1) >= 0)
 41377|     {
 41378|         unsigned int i = 0;
 41379|         while (lock >= 0)
 41380|         {
 41381|             if (g_num_processors > 1)
 41382|             {
 41383|                 int spin_count = 128 * yp_spin_count_unit;
 41384|                 for (int j = 0; j < spin_count; j++)
 41385|                 {
 41386|                     if (lock < 0)
 41387|                         break;
 41388|                     YieldProcessor ();           // indicate to the processor that we are spinning
 41389|                 }
 41390|             }
 41391|             if (lock < 0)
 41392|                 break;
 41393|             if (++i & 7)
 41394|                 GCToOSInterface::YieldThread (0);
 41395|             else
 41396|                 GCToOSInterface::Sleep (5);
 41397|         }
 41398|         goto retry;
 41399|     }
 41400| #ifdef _DEBUG
 41401|     lockowner_threadid.SetToCurrentThread();
 41402| #endif // _DEBUG
 41403| }
 41404| inline
 41405| void CFinalize::LeaveFinalizeLock()
 41406| {
 41407|     _ASSERTE(dbgOnly_IsSpecialEEThread() ||
 41408|              GCToEEInterface::GetThread() == 0 ||
 41409|              GCToEEInterface::IsPreemptiveGCDisabled());
 41410| #ifdef _DEBUG
 41411|     lockowner_threadid.Clear();
 41412| #endif // _DEBUG
 41413|     lock = -1;
 41414| }
 41415| bool
 41416| CFinalize::RegisterForFinalization (int gen, Object* obj, size_t size)
 41417| {
 41418|     CONTRACTL {
 41419|         NOTHROW;
 41420|         GC_NOTRIGGER;
 41421|     } CONTRACTL_END;
 41422|     EnterFinalizeLock();
 41423|     unsigned int dest = gen_segment (gen);
 41424|     Object*** s_i = &SegQueue (FreeList);
 41425|     if ((*s_i) == m_EndArray)
 41426|     {
 41427|         if (!GrowArray())
 41428|         {
 41429|             LeaveFinalizeLock();
 41430|             if (method_table(obj) == NULL)
 41431|             {
 41432|                 assert (size >= Align (min_obj_size));
 41433|                 dprintf (3, (ThreadStressLog::gcMakeUnusedArrayMsg(), (size_t)obj, (size_t)(obj+size)));
 41434|                 ((CObjectHeader*)obj)->SetFree(size);
 41435|             }
 41436|             STRESS_LOG_OOM_STACK(0);
 41437|             if (GCConfig::GetBreakOnOOM())
 41438|             {
 41439|                 GCToOSInterface::DebugBreak();
 41440|             }
 41441|             return false;
 41442|         }
 41443|     }
 41444|     Object*** end_si = &SegQueueLimit (dest);
 41445|     do
 41446|     {
 41447|         if (!(*s_i == *(s_i-1)))
 41448|         {
 41449|             *(*s_i) = *(*(s_i-1));
 41450|         }
 41451|         (*s_i)++;
 41452|         s_i--;
 41453|     } while (s_i > end_si);
 41454|     **s_i = obj;
 41455|     (*s_i)++;
 41456|     LeaveFinalizeLock();
 41457|     return true;
 41458| }
 41459| Object*
 41460| CFinalize::GetNextFinalizableObject (BOOL only_non_critical)
 41461| {
 41462|     Object* obj = 0;
 41463|     EnterFinalizeLock();
 41464|     if (!IsSegEmpty(FinalizerListSeg))
 41465|     {
 41466|         obj =  *(--SegQueueLimit (FinalizerListSeg));
 41467|     }
 41468|     else if (!only_non_critical && !IsSegEmpty(CriticalFinalizerListSeg))
 41469|     {
 41470|         obj =  *(--SegQueueLimit (CriticalFinalizerListSeg));
 41471|         --SegQueueLimit (FinalizerListSeg);
 41472|     }
 41473|     if (obj)
 41474|     {
 41475|         dprintf (3, ("running finalizer for %p (mt: %p)", obj, method_table (obj)));
 41476|     }
 41477|     LeaveFinalizeLock();
 41478|     return obj;
 41479| }
 41480| size_t
 41481| CFinalize::GetNumberFinalizableObjects()
 41482| {
 41483|     return SegQueueLimit(FinalizerListSeg) - SegQueue(FinalizerListSeg);
 41484| }
 41485| void
 41486| CFinalize::MoveItem (Object** fromIndex,
 41487|                      unsigned int fromSeg,
 41488|                      unsigned int toSeg)
 41489| {
 41490|     int step;
 41491|     ASSERT (fromSeg != toSeg);
 41492|     if (fromSeg > toSeg)
 41493|         step = -1;
 41494|     else
 41495|         step = +1;
 41496|     Object** srcIndex = fromIndex;
 41497|     for (unsigned int i = fromSeg; i != toSeg; i+= step)
 41498|     {
 41499|         Object**& destFill = m_FillPointers[i+(step - 1 )/2];
 41500|         Object** destIndex = destFill - (step + 1)/2;
 41501|         if (srcIndex != destIndex)
 41502|         {
 41503|             Object* tmp = *srcIndex;
 41504|             *srcIndex = *destIndex;
 41505|             *destIndex = tmp;
 41506|         }
 41507|         destFill -= step;
 41508|         srcIndex = destIndex;
 41509|     }
 41510| }
 41511| void
 41512| CFinalize::GcScanRoots (promote_func* fn, int hn, ScanContext *pSC)
 41513| {
 41514|     ScanContext sc;
 41515|     if (pSC == 0)
 41516|         pSC = &sc;
 41517|     pSC->thread_number = hn;
 41518|     Object** startIndex  = SegQueue (CriticalFinalizerListSeg);
 41519|     Object** stopIndex  = SegQueueLimit (FinalizerListSeg);
 41520|     for (Object** po = startIndex; po < stopIndex; po++)
 41521|     {
 41522|         Object* o = *po;
 41523|         dprintf (3, ("scan f %zx", (size_t)o));
 41524|         (*fn)(po, pSC, 0);
 41525|     }
 41526| }
 41527| void CFinalize::WalkFReachableObjects (fq_walk_fn fn)
 41528| {
 41529|     Object** startIndex = SegQueue (CriticalFinalizerListSeg);
 41530|     Object** stopCriticalIndex = SegQueueLimit (CriticalFinalizerListSeg);
 41531|     Object** stopIndex  = SegQueueLimit (FinalizerListSeg);
 41532|     for (Object** po = startIndex; po < stopIndex; po++)
 41533|     {
 41534|         fn(po < stopCriticalIndex, *po);
 41535|     }
 41536| }
 41537| BOOL
 41538| CFinalize::ScanForFinalization (promote_func* pfn, int gen, BOOL mark_only_p,
 41539|                                 gc_heap* hp)
 41540| {
 41541|     ScanContext sc;
 41542|     sc.promotion = TRUE;
 41543| #ifdef MULTIPLE_HEAPS
 41544|     sc.thread_number = hp->heap_number;
 41545|     sc.thread_count = gc_heap::n_heaps;
 41546| #else
 41547|     UNREFERENCED_PARAMETER(hp);
 41548|     sc.thread_count = 1;
 41549| #endif //MULTIPLE_HEAPS
 41550|     BOOL finalizedFound = FALSE;
 41551|     unsigned int startSeg = gen_segment (gen);
 41552|     {
 41553|         m_PromotedCount = 0;
 41554|         for (unsigned int Seg = startSeg; Seg <= gen_segment(0); Seg++)
 41555|         {
 41556|             Object** endIndex = SegQueue (Seg);
 41557|             for (Object** i = SegQueueLimit (Seg)-1; i >= endIndex ;i--)
 41558|             {
 41559|                 CObjectHeader* obj = (CObjectHeader*)*i;
 41560|                 dprintf (3, ("scanning: %zx", (size_t)obj));
 41561|                 if (!g_theGCHeap->IsPromoted (obj))
 41562|                 {
 41563|                     dprintf (3, ("freacheable: %zx", (size_t)obj));
 41564|                     assert (method_table(obj)->HasFinalizer());
 41565|                     if (GCToEEInterface::EagerFinalized(obj))
 41566|                     {
 41567|                         MoveItem (i, Seg, FreeList);
 41568|                     }
 41569|                     else if ((obj->GetHeader()->GetBits()) & BIT_SBLK_FINALIZER_RUN)
 41570|                     {
 41571|                         MoveItem (i, Seg, FreeList);
 41572|                         obj->GetHeader()->ClrBit (BIT_SBLK_FINALIZER_RUN);
 41573|                     }
 41574|                     else
 41575|                     {
 41576|                         m_PromotedCount++;
 41577|                         if (method_table(obj)->HasCriticalFinalizer())
 41578|                         {
 41579|                             MoveItem (i, Seg, CriticalFinalizerListSeg);
 41580|                         }
 41581|                         else
 41582|                         {
 41583|                             MoveItem (i, Seg, FinalizerListSeg);
 41584|                         }
 41585|                     }
 41586|                 }
 41587| #ifdef BACKGROUND_GC
 41588|                 else
 41589|                 {
 41590|                     if ((gen == max_generation) && (gc_heap::background_running_p()))
 41591|                     {
 41592|                         dprintf (3, ("%zx is marked", (size_t)obj));
 41593|                     }
 41594|                 }
 41595| #endif //BACKGROUND_GC
 41596|             }
 41597|         }
 41598|     }
 41599|     finalizedFound = !IsSegEmpty(FinalizerListSeg) ||
 41600|                      !IsSegEmpty(CriticalFinalizerListSeg);
 41601|     if (finalizedFound)
 41602|     {
 41603|         GcScanRoots (pfn,
 41604| #ifdef MULTIPLE_HEAPS
 41605|                      hp->heap_number
 41606| #else
 41607|                      0
 41608| #endif //MULTIPLE_HEAPS
 41609|                      , 0);
 41610|         hp->settings.found_finalizers = TRUE;
 41611| #ifdef BACKGROUND_GC
 41612|         if (hp->settings.concurrent)
 41613|         {
 41614|             hp->settings.found_finalizers = !(IsSegEmpty(FinalizerListSeg) && IsSegEmpty(CriticalFinalizerListSeg));
 41615|         }
 41616| #endif //BACKGROUND_GC
 41617|         if (hp->settings.concurrent && hp->settings.found_finalizers)
 41618|         {
 41619|             if (!mark_only_p)
 41620|                 GCToEEInterface::EnableFinalization(true);
 41621|         }
 41622|     }
 41623|     return finalizedFound;
 41624| }
 41625| void
 41626| CFinalize::RelocateFinalizationData (int gen, gc_heap* hp)
 41627| {
 41628|     ScanContext sc;
 41629|     sc.promotion = FALSE;
 41630| #ifdef MULTIPLE_HEAPS
 41631|     sc.thread_number = hp->heap_number;
 41632|     sc.thread_count = gc_heap::n_heaps;
 41633| #else
 41634|     UNREFERENCED_PARAMETER(hp);
 41635|     sc.thread_count = 1;
 41636| #endif //MULTIPLE_HEAPS
 41637|     unsigned int Seg = gen_segment (gen);
 41638|     Object** startIndex = SegQueue (Seg);
 41639|     dprintf (3, ("RelocateFinalizationData gen=%d, [%p,%p[", gen, startIndex, SegQueue (FreeList)));
 41640|     for (Object** po = startIndex; po < SegQueue (FreeList);po++)
 41641|     {
 41642|         GCHeap::Relocate (po, &sc);
 41643|     }
 41644| }
 41645| void
 41646| CFinalize::UpdatePromotedGenerations (int gen, BOOL gen_0_empty_p)
 41647| {
 41648|     dprintf(3, ("UpdatePromotedGenerations gen=%d, gen_0_empty_p=%d", gen, gen_0_empty_p));
 41649|     if (gen_0_empty_p)
 41650|     {
 41651|         for (int i = min (gen+1, max_generation); i > 0; i--)
 41652|         {
 41653|             m_FillPointers [gen_segment(i)] = m_FillPointers [gen_segment(i-1)];
 41654|         }
 41655|     }
 41656|     else
 41657|     {
 41658|         for (int i = gen; i >= 0; i--)
 41659|         {
 41660|             unsigned int Seg = gen_segment (i);
 41661|             Object** startIndex = SegQueue (Seg);
 41662|             for (Object** po = startIndex;
 41663|                  po < SegQueueLimit (gen_segment(i)); po++)
 41664|             {
 41665|                 int new_gen = g_theGCHeap->WhichGeneration (*po);
 41666|                 if (new_gen != i)
 41667|                 {
 41668|                     assert (new_gen <= max_generation);
 41669|                     dprintf (3, ("Moving object %p->%p from gen %d to gen %d", po, *po, i, new_gen));
 41670|                     if (new_gen > i)
 41671|                     {
 41672|                         MoveItem (po, gen_segment (i), gen_segment (new_gen));
 41673|                     }
 41674|                     else
 41675|                     {
 41676|                         MoveItem (po, gen_segment (i), gen_segment (new_gen));
 41677|                         po--;
 41678|                     }
 41679|                 }
 41680|             }
 41681|         }
 41682|     }
 41683| }
 41684| BOOL
 41685| CFinalize::GrowArray()
 41686| {
 41687|     size_t oldArraySize = (m_EndArray - m_Array);
 41688|     size_t newArraySize =  (size_t)(((float)oldArraySize / 10) * 12);
 41689|     Object** newArray = new (nothrow) Object*[newArraySize];
 41690|     if (!newArray)
 41691|     {
 41692|         return FALSE;
 41693|     }
 41694|     memcpy (newArray, m_Array, oldArraySize*sizeof(Object*));
 41695|     dprintf (3, ("Grow finalizer array [%p,%p[ -> [%p,%p[", m_Array, m_EndArray, newArray, &m_Array[newArraySize]));
 41696|     for (int i = 0; i < FreeList; i++)
 41697|     {
 41698|         m_FillPointers [i] += (newArray - m_Array);
 41699|     }
 41700|     delete[] m_Array;
 41701|     m_Array = newArray;
 41702|     m_EndArray = &m_Array [newArraySize];
 41703|     return TRUE;
 41704| }
 41705| bool CFinalize::MergeFinalizationData (CFinalize* other_fq)
 41706| {
 41707|     size_t otherNeededArraySize = other_fq->SegQueue (FreeList) - other_fq->m_Array;
 41708|     if (otherNeededArraySize == 0)
 41709|     {
 41710|         return true;
 41711|     }
 41712|     size_t thisArraySize = (m_EndArray - m_Array);
 41713|     size_t thisNeededArraySize = SegQueue (FreeList) - m_Array;
 41714|     size_t neededArraySize = thisNeededArraySize + otherNeededArraySize;
 41715|     Object ** newArray = m_Array;
 41716|     if (thisArraySize < neededArraySize)
 41717|     {
 41718|         newArray = new (nothrow) Object*[neededArraySize];
 41719|         if (!newArray)
 41720|         {
 41721|             dprintf (3, ("ran out of space merging finalization data"));
 41722|             return false;
 41723|         }
 41724|     }
 41725|     for (int i = FreeList - 1; i >= 0; i--)
 41726|     {
 41727|         size_t thisIndex = SegQueue (i) - m_Array;
 41728|         size_t otherIndex = other_fq->SegQueue (i) - other_fq->m_Array;
 41729|         size_t thisLimit = SegQueueLimit (i) - m_Array;
 41730|         size_t otherLimit = other_fq->SegQueueLimit (i) - other_fq->m_Array;
 41731|         size_t thisSize = thisLimit - thisIndex;
 41732|         size_t otherSize = otherLimit - otherIndex;
 41733|         memmove (&newArray[thisIndex + otherIndex],           &m_Array[thisIndex ], sizeof(newArray[0])*thisSize );
 41734|         memmove (&newArray[thisLimit + otherIndex], &other_fq->m_Array[otherIndex], sizeof(newArray[0])*otherSize);
 41735|     }
 41736|     for (int i = FreeList - 1; i >= 0; i--)
 41737|     {
 41738|         size_t thisLimit = SegQueueLimit (i) - m_Array;
 41739|         size_t otherLimit = other_fq->SegQueueLimit (i) - other_fq->m_Array;
 41740|         SegQueueLimit (i) = &newArray[thisLimit + otherLimit];
 41741|         other_fq->SegQueueLimit (i) = other_fq->m_Array;
 41742|     }
 41743|     if (m_Array != newArray)
 41744|     {
 41745|         delete[] m_Array;
 41746|         m_Array = newArray;
 41747|         m_EndArray = &m_Array [neededArraySize];
 41748|     }
 41749|     return true;
 41750| }
 41751| bool CFinalize::SplitFinalizationData (CFinalize* other_fq)
 41752| {
 41753|     size_t otherCurrentArraySize = other_fq->SegQueue (FreeList) - other_fq->m_Array;
 41754|     assert (otherCurrentArraySize == 0);
 41755|     size_t thisCurrentArraySize = SegQueue (FreeList) - m_Array;
 41756|     if (thisCurrentArraySize == 0)
 41757|     {
 41758|         return true;
 41759|     }
 41760|     size_t otherNeededArraySize = thisCurrentArraySize / 2;
 41761|     size_t otherArraySize = other_fq->m_EndArray - other_fq->m_Array;
 41762|     if (otherArraySize < otherNeededArraySize)
 41763|     {
 41764|         Object ** newArray = new (nothrow) Object*[otherNeededArraySize];
 41765|         if (!newArray)
 41766|         {
 41767|             return false;
 41768|         }
 41769|         delete[] other_fq->m_Array;
 41770|         other_fq->m_Array = newArray;
 41771|         other_fq->m_EndArray = &other_fq->m_Array[otherNeededArraySize];
 41772|     }
 41773|     PTR_PTR_Object newFillPointers[FreeList];
 41774|     PTR_PTR_Object segQueue = m_Array;
 41775|     for (int i = 0; i < FreeList; i++)
 41776|     {
 41777|         size_t thisIndex = SegQueue (i) - m_Array;
 41778|         size_t thisLimit = SegQueueLimit (i) - m_Array;
 41779|         size_t thisSize = thisLimit - thisIndex;
 41780|         size_t otherSize = thisSize / 2;
 41781|         size_t otherIndex = other_fq->SegQueue (i) - other_fq->m_Array;
 41782|         size_t thisNewSize = thisSize - otherSize;
 41783|         memmove (&other_fq->m_Array[otherIndex], &m_Array[thisIndex + thisNewSize], sizeof(other_fq->m_Array[0])*otherSize);
 41784|         other_fq->SegQueueLimit (i) = &other_fq->m_Array[otherIndex + otherSize];
 41785|         memmove (segQueue, &m_Array[thisIndex], sizeof(m_Array[0])*thisNewSize);
 41786|         segQueue += thisNewSize;
 41787|         newFillPointers[i] = segQueue;
 41788|     }
 41789|     for (int i = 0; i < FreeList; i++)
 41790|     {
 41791|         m_FillPointers[i] = newFillPointers[i];
 41792|     }
 41793|     return true;
 41794| }
 41795| #ifdef VERIFY_HEAP
 41796| void CFinalize::CheckFinalizerObjects()
 41797| {
 41798|     for (int i = 0; i <= max_generation; i++)
 41799|     {
 41800|         Object **startIndex = SegQueue (gen_segment (i));
 41801|         Object **stopIndex  = SegQueueLimit (gen_segment (i));
 41802|         for (Object **po = startIndex; po < stopIndex; po++)
 41803|         {
 41804|             if ((int)g_theGCHeap->WhichGeneration (*po) < i)
 41805|                 FATAL_GC_ERROR ();
 41806|             ((CObjectHeader*)*po)->Validate();
 41807|         }
 41808|     }
 41809| }
 41810| #endif //VERIFY_HEAP
 41811| #endif // FEATURE_PREMORTEM_FINALIZATION
 41812| void gc_heap::walk_heap_per_heap (walk_fn fn, void* context, int gen_number, BOOL walk_large_object_heap_p)
 41813| {
 41814|     generation* gen = gc_heap::generation_of (gen_number);
 41815|     heap_segment*    seg = generation_start_segment (gen);
 41816|     uint8_t* x = ((gen_number == max_generation) ? heap_segment_mem (seg) : get_soh_start_object (seg, gen));
 41817|     uint8_t*       end = heap_segment_allocated (seg);
 41818|     int align_const = get_alignment_constant (TRUE);
 41819|     BOOL walk_pinned_object_heap = walk_large_object_heap_p;
 41820|     while (1)
 41821|     {
 41822|         if (x >= end)
 41823|         {
 41824|             if ((seg = heap_segment_next (seg)) != 0)
 41825|             {
 41826|                 x = heap_segment_mem (seg);
 41827|                 end = heap_segment_allocated (seg);
 41828|                 continue;
 41829|             }
 41830| #ifdef USE_REGIONS
 41831|             else if (gen_number > 0)
 41832|             {
 41833|                 gen_number--;
 41834|                 gen = gc_heap::generation_of (gen_number);
 41835|                 seg = generation_start_segment (gen);
 41836|                 x = heap_segment_mem (seg);
 41837|                 end = heap_segment_allocated (seg);
 41838|                 continue;
 41839|             }
 41840| #endif // USE_REGIONS
 41841|             else
 41842|             {
 41843|                 if (walk_large_object_heap_p)
 41844|                 {
 41845|                     walk_large_object_heap_p = FALSE;
 41846|                     seg = generation_start_segment (large_object_generation);
 41847|                 }
 41848|                 else if (walk_pinned_object_heap)
 41849|                 {
 41850|                     walk_pinned_object_heap = FALSE;
 41851|                     seg = generation_start_segment (pinned_object_generation);
 41852|                 }
 41853|                 else
 41854|                 {
 41855|                     break;
 41856|                 }
 41857|                 align_const = get_alignment_constant (FALSE);
 41858|                 x = heap_segment_mem (seg);
 41859|                 end = heap_segment_allocated (seg);
 41860|                 continue;
 41861|             }
 41862|         }
 41863|         size_t s = size (x);
 41864|         CObjectHeader* o = (CObjectHeader*)x;
 41865|         if (!o->IsFree())
 41866|         {
 41867|             _ASSERTE(((size_t)o & 0x3) == 0); // Last two bits should never be set at this point
 41868|             if (!fn (o->GetObjectBase(), context))
 41869|                 return;
 41870|         }
 41871|         x = x + Align (s, align_const);
 41872|     }
 41873| }
 41874| void gc_heap::walk_finalize_queue (fq_walk_fn fn)
 41875| {
 41876| #ifdef FEATURE_PREMORTEM_FINALIZATION
 41877|     finalize_queue->WalkFReachableObjects (fn);
 41878| #endif //FEATURE_PREMORTEM_FINALIZATION
 41879| }
 41880| void gc_heap::walk_heap (walk_fn fn, void* context, int gen_number, BOOL walk_large_object_heap_p)
 41881| {
 41882| #ifdef MULTIPLE_HEAPS
 41883|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 41884|     {
 41885|         gc_heap* hp = gc_heap::g_heaps [hn];
 41886|         hp->walk_heap_per_heap (fn, context, gen_number, walk_large_object_heap_p);
 41887|     }
 41888| #else
 41889|     walk_heap_per_heap(fn, context, gen_number, walk_large_object_heap_p);
 41890| #endif //MULTIPLE_HEAPS
 41891| }
 41892| void GCHeap::DiagWalkObject (Object* obj, walk_fn fn, void* context)
 41893| {
 41894|     uint8_t* o = (uint8_t*)obj;
 41895|     if (o)
 41896|     {
 41897|         go_through_object_cl (method_table (o), o, size(o), oo,
 41898|                                     {
 41899|                                         if (*oo)
 41900|                                         {
 41901|                                             Object *oh = (Object*)*oo;
 41902|                                             if (!fn (oh, context))
 41903|                                                 return;
 41904|                                         }
 41905|                                     }
 41906|             );
 41907|     }
 41908| }
 41909| void GCHeap::DiagWalkObject2 (Object* obj, walk_fn2 fn, void* context)
 41910| {
 41911|     uint8_t* o = (uint8_t*)obj;
 41912|     if (o)
 41913|     {
 41914|         go_through_object_cl (method_table (o), o, size(o), oo,
 41915|                                     {
 41916|                                         if (*oo)
 41917|                                         {
 41918|                                             if (!fn (obj, oo, context))
 41919|                                                 return;
 41920|                                         }
 41921|                                     }
 41922|             );
 41923|     }
 41924| }
 41925| void GCHeap::DiagWalkSurvivorsWithType (void* gc_context, record_surv_fn fn, void* diag_context, walk_surv_type type, int gen_number)
 41926| {
 41927|     gc_heap* hp = (gc_heap*)gc_context;
 41928|     if (type == walk_for_uoh)
 41929|     {
 41930|         hp->walk_survivors_for_uoh (diag_context, fn, gen_number);
 41931|     }
 41932|     else
 41933|     {
 41934|         hp->walk_survivors (fn, diag_context, type);
 41935|     }
 41936| }
 41937| void GCHeap::DiagWalkHeap (walk_fn fn, void* context, int gen_number, bool walk_large_object_heap_p)
 41938| {
 41939|     gc_heap::walk_heap (fn, context, gen_number, walk_large_object_heap_p);
 41940| }
 41941| void GCHeap::DiagWalkFinalizeQueue (void* gc_context, fq_walk_fn fn)
 41942| {
 41943|     gc_heap* hp = (gc_heap*)gc_context;
 41944|     hp->walk_finalize_queue (fn);
 41945| }
 41946| void GCHeap::DiagScanFinalizeQueue (fq_scan_fn fn, ScanContext* sc)
 41947| {
 41948| #ifdef MULTIPLE_HEAPS
 41949|     for (int hn = 0; hn < gc_heap::n_heaps; hn++)
 41950|     {
 41951|         gc_heap* hp = gc_heap::g_heaps [hn];
 41952|         hp->finalize_queue->GcScanRoots(fn, hn, sc);
 41953|     }
 41954| #else
 41955|         pGenGCHeap->finalize_queue->GcScanRoots(fn, 0, sc);
 41956| #endif //MULTIPLE_HEAPS
 41957| }
 41958| void GCHeap::DiagScanHandles (handle_scan_fn fn, int gen_number, ScanContext* context)
 41959| {
 41960|     GCScan::GcScanHandlesForProfilerAndETW (gen_number, context, fn);
 41961| }
 41962| void GCHeap::DiagScanDependentHandles (handle_scan_fn fn, int gen_number, ScanContext* context)
 41963| {
 41964|     GCScan::GcScanDependentHandlesForProfilerAndETW (gen_number, context, fn);
 41965| }
 41966| void GCHeap::DiagGetGCSettings(EtwGCSettingsInfo* etw_settings)
 41967| {
 41968| #ifdef FEATURE_EVENT_TRACE
 41969|     etw_settings->heap_hard_limit = gc_heap::heap_hard_limit;
 41970|     etw_settings->loh_threshold = loh_size_threshold;
 41971|     etw_settings->physical_memory_from_config = gc_heap::physical_memory_from_config;
 41972|     etw_settings->gen0_min_budget_from_config = gc_heap::gen0_min_budget_from_config;
 41973|     etw_settings->gen0_max_budget_from_config = gc_heap::gen0_max_budget_from_config;
 41974|     etw_settings->high_mem_percent_from_config = gc_heap::high_mem_percent_from_config;
 41975| #ifdef BACKGROUND_GC
 41976|     etw_settings->concurrent_gc_p = gc_heap::gc_can_use_concurrent;
 41977| #else
 41978|     etw_settings->concurrent_gc_p = false;
 41979| #endif //BACKGROUND_GC
 41980|     etw_settings->use_large_pages_p = gc_heap::use_large_pages_p;
 41981|     etw_settings->use_frozen_segments_p = gc_heap::use_frozen_segments_p;
 41982|     etw_settings->hard_limit_config_p = gc_heap::hard_limit_config_p;
 41983|     etw_settings->no_affinitize_p =
 41984| #ifdef MULTIPLE_HEAPS
 41985|         gc_heap::gc_thread_no_affinitize_p;
 41986| #else
 41987|         true;
 41988| #endif //MULTIPLE_HEAPS
 41989| #endif //FEATURE_EVENT_TRACE
 41990| }
 41991| #if defined(WRITE_BARRIER_CHECK) && !defined (SERVER_GC)
 41992| void deleteGCShadow()
 41993| {
 41994|     if (g_GCShadow != 0)
 41995|         GCToOSInterface::VirtualRelease (g_GCShadow, g_GCShadowEnd - g_GCShadow);
 41996|     g_GCShadow = 0;
 41997|     g_GCShadowEnd = 0;
 41998| }
 41999| void initGCShadow()
 42000| {
 42001|     if (!(GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_BARRIERCHECK))
 42002|         return;
 42003|     uint8_t* highest = nullptr;
 42004| #ifdef USE_REGIONS
 42005|     highest = global_region_allocator.get_left_used_unsafe();
 42006| #else
 42007|     highest = g_gc_highest_address;
 42008| #endif
 42009|     size_t len = g_gc_highest_address - g_gc_lowest_address;
 42010|     size_t commit_len = highest - g_gc_lowest_address;
 42011|     if (len > (size_t)(g_GCShadowEnd - g_GCShadow))
 42012|     {
 42013|         deleteGCShadow();
 42014|         g_GCShadowEnd = g_GCShadow = (uint8_t *)GCToOSInterface::VirtualReserve (len, 0, VirtualReserveFlags::None);
 42015|         if (g_GCShadow == NULL || !GCToOSInterface::VirtualCommit (g_GCShadow, commit_len))
 42016|         {
 42017|             _ASSERTE(!"Not enough memory to run HeapVerify level 2");
 42018|             deleteGCShadow();
 42019|             return;
 42020|         }
 42021|         g_GCShadowEnd += commit_len;
 42022|     }
 42023|     g_shadow_lowest_address = g_gc_lowest_address;
 42024|     for (int i = get_start_generation_index(); i < total_generation_count; i++)
 42025|     {
 42026|         generation* gen = gc_heap::generation_of (i);
 42027|         heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 42028|         ptrdiff_t delta = g_GCShadow - g_gc_lowest_address;
 42029|         while (seg)
 42030|         {
 42031|             uint8_t* start = heap_segment_mem (seg);
 42032|             uint8_t* end = heap_segment_allocated (seg);
 42033|             memcpy (start + delta, start, end - start);
 42034|             seg = heap_segment_next_rw (seg);
 42035|         }
 42036|     }
 42037| }
 42038| #define INVALIDGCVALUE (void*)((size_t)0xcccccccd)
 42039| inline void testGCShadow(Object** ptr)
 42040| {
 42041|     Object** shadow = (Object**) &g_GCShadow[((uint8_t*) ptr - g_gc_lowest_address)];
 42042|     if (*ptr != 0 && (uint8_t*) shadow < g_GCShadowEnd && *ptr != *shadow)
 42043|     {
 42044|         if(*shadow!=INVALIDGCVALUE)
 42045|         {
 42046| #ifdef FEATURE_BASICFREEZE
 42047|             if (!g_theGCHeap->IsInFrozenSegment (*ptr))
 42048| #endif // FEATURE_BASICFREEZE
 42049|             {
 42050|                 _ASSERTE(!"Pointer updated without using write barrier");
 42051|             }
 42052|         }
 42053|         /*
 42054|         else
 42055|         {
 42056|              printf("saw a INVALIDGCVALUE. (just to let you know)\n");
 42057|         }
 42058|         */
 42059|     }
 42060| }
 42061| void testGCShadowHelper (uint8_t* x)
 42062| {
 42063|     size_t s = size (x);
 42064|     if (contain_pointers (x))
 42065|     {
 42066|         go_through_object_nostart (method_table(x), x, s, oo,
 42067|                            { testGCShadow((Object**) oo); });
 42068|     }
 42069| }
 42070| void checkGCWriteBarrier()
 42071| {
 42072|     if (g_GCShadowEnd <= g_GCShadow || g_shadow_lowest_address != g_gc_lowest_address)
 42073|     {
 42074|         return;
 42075|     }
 42076|     for (int i = get_start_generation_index(); i < total_generation_count; i++)
 42077|     {
 42078|         int alignment = get_alignment_constant(i <= max_generation);
 42079|         {
 42080|             generation* gen = gc_heap::generation_of (i);
 42081|             heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
 42082|             PREFIX_ASSUME(seg != NULL);
 42083|             while(seg)
 42084|             {
 42085|                 uint8_t* x = heap_segment_mem (seg);
 42086|                 while (x < heap_segment_allocated (seg))
 42087|                 {
 42088|                     size_t s = size (x);
 42089|                     testGCShadowHelper (x);
 42090|                     x = x + Align (s, alignment);
 42091|                 }
 42092|                 seg = heap_segment_next_rw (seg);
 42093|             }
 42094|         }
 42095|     }
 42096| }
 42097| #endif //WRITE_BARRIER_CHECK && !SERVER_GC
 42098| #ifdef FEATURE_BASICFREEZE
 42099| void gc_heap::walk_read_only_segment(heap_segment *seg, void *pvContext, object_callback_func pfnMethodTable, object_callback_func pfnObjRef)
 42100| {
 42101|     uint8_t *o = heap_segment_mem(seg);
 42102|     int alignment = get_alignment_constant(TRUE);
 42103|     while (o < heap_segment_allocated(seg))
 42104|     {
 42105|         pfnMethodTable(pvContext, o);
 42106|         if (contain_pointers (o))
 42107|         {
 42108|             go_through_object_nostart (method_table (o), o, size(o), oo,
 42109|                    {
 42110|                        if (*oo)
 42111|                            pfnObjRef(pvContext, oo);
 42112|                    }
 42113|             );
 42114|         }
 42115|         o += Align(size(o), alignment);
 42116|     }
 42117| }
 42118| #endif // FEATURE_BASICFREEZE
 42119| HRESULT GCHeap::WaitUntilConcurrentGCCompleteAsync(int millisecondsTimeout)
 42120| {
 42121| #ifdef BACKGROUND_GC
 42122|     if (gc_heap::background_running_p())
 42123|     {
 42124|         uint32_t dwRet = pGenGCHeap->background_gc_wait(awr_ignored, millisecondsTimeout);
 42125|         if (dwRet == WAIT_OBJECT_0)
 42126|             return S_OK;
 42127|         else if (dwRet == WAIT_TIMEOUT)
 42128|             return HRESULT_FROM_WIN32(ERROR_TIMEOUT);
 42129|         else
 42130|             return E_FAIL;      // It is not clear if what the last error would be if the wait failed,
 42131|     }
 42132| #endif
 42133|     return S_OK;
 42134| }
 42135| void GCHeap::TemporaryEnableConcurrentGC()
 42136| {
 42137| #ifdef BACKGROUND_GC
 42138|     gc_heap::temp_disable_concurrent_p = false;
 42139| #endif //BACKGROUND_GC
 42140| }
 42141| void GCHeap::TemporaryDisableConcurrentGC()
 42142| {
 42143| #ifdef BACKGROUND_GC
 42144|     gc_heap::temp_disable_concurrent_p = true;
 42145| #endif //BACKGROUND_GC
 42146| }
 42147| bool GCHeap::IsConcurrentGCEnabled()
 42148| {
 42149| #ifdef BACKGROUND_GC
 42150|     return (gc_heap::gc_can_use_concurrent && !(gc_heap::temp_disable_concurrent_p));
 42151| #else
 42152|     return FALSE;
 42153| #endif //BACKGROUND_GC
 42154| }
 42155| void PopulateDacVars(GcDacVars *gcDacVars)
 42156| {
 42157|     bool v2 = gcDacVars->minor_version_number >= 2;
 42158| #define DEFINE_FIELD(field_name, field_type) offsetof(CLASS_NAME, field_name),
 42159| #define DEFINE_DPTR_FIELD(field_name, field_type) offsetof(CLASS_NAME, field_name),
 42160| #define DEFINE_ARRAY_FIELD(field_name, field_type, array_length) offsetof(CLASS_NAME, field_name),
 42161| #define DEFINE_MISSING_FIELD(field_name) -1,
 42162| #ifdef MULTIPLE_HEAPS
 42163|     static int gc_heap_field_offsets[] = {
 42164| #define CLASS_NAME gc_heap
 42165| #include "dac_gcheap_fields.h"
 42166| #undef CLASS_NAME
 42167|     };
 42168| #endif //MULTIPLE_HEAPS
 42169|     static int generation_field_offsets[] = {
 42170| #define CLASS_NAME generation
 42171| #include "dac_generation_fields.h"
 42172| #undef CLASS_NAME
 42173| #undef DEFINE_MISSING_FIELD
 42174| #undef DEFINE_ARRAY_FIELD
 42175| #undef DEFINE_DPTR_FIELD
 42176| #undef DEFINE_FIELD
 42177|     };
 42178|     assert(gcDacVars != nullptr);
 42179|     gcDacVars->major_version_number = 2;
 42180|     gcDacVars->minor_version_number = 0;
 42181|     if (v2)
 42182|     {
 42183|         gcDacVars->total_bookkeeping_elements = total_bookkeeping_elements;
 42184|         gcDacVars->card_table_info_size = sizeof(card_table_info);
 42185|     }
 42186| #ifdef USE_REGIONS
 42187|     gcDacVars->minor_version_number |= 1;
 42188|     if (v2)
 42189|     {
 42190|         gcDacVars->count_free_region_kinds = count_free_region_kinds;
 42191|         gcDacVars->global_regions_to_decommit = reinterpret_cast<dac_region_free_list**>(&gc_heap::global_regions_to_decommit);
 42192|         gcDacVars->global_free_huge_regions = reinterpret_cast<dac_region_free_list**>(&gc_heap::global_free_huge_regions);
 42193|     }
 42194| #endif //USE_REGIONS
 42195| #ifndef BACKGROUND_GC
 42196|     gcDacVars->minor_version_number |= 2;
 42197| #endif //!BACKGROUND_GC
 42198|     gcDacVars->built_with_svr = &g_built_with_svr_gc;
 42199|     gcDacVars->build_variant = &g_build_variant;
 42200|     gcDacVars->gc_structures_invalid_cnt = const_cast<int32_t*>(&GCScan::m_GcStructuresInvalidCnt);
 42201|     gcDacVars->generation_size = sizeof(generation);
 42202|     gcDacVars->total_generation_count = total_generation_count;
 42203|     gcDacVars->max_gen = &g_max_generation;
 42204| #ifdef BACKGROUND_GC
 42205|     gcDacVars->current_c_gc_state = const_cast<c_gc_state*>(&gc_heap::current_c_gc_state);
 42206| #else //BACKGROUND_GC
 42207|     gcDacVars->current_c_gc_state = 0;
 42208| #endif //BACKGROUND_GC
 42209| #ifndef MULTIPLE_HEAPS
 42210|     gcDacVars->ephemeral_heap_segment = reinterpret_cast<dac_heap_segment**>(&gc_heap::ephemeral_heap_segment);
 42211| #ifdef USE_REGIONS
 42212|     if (v2)
 42213|     {
 42214|         gcDacVars->free_regions = reinterpret_cast<dac_region_free_list**>(&gc_heap::free_regions);
 42215|     }
 42216| #endif
 42217| #ifdef BACKGROUND_GC
 42218|     gcDacVars->mark_array = &gc_heap::mark_array;
 42219|     gcDacVars->background_saved_lowest_address = &gc_heap::background_saved_lowest_address;
 42220|     gcDacVars->background_saved_highest_address = &gc_heap::background_saved_highest_address;
 42221|     if (v2)
 42222|     {
 42223|         gcDacVars->freeable_soh_segment = reinterpret_cast<dac_heap_segment**>(&gc_heap::freeable_soh_segment);
 42224|         gcDacVars->freeable_uoh_segment = reinterpret_cast<dac_heap_segment**>(&gc_heap::freeable_uoh_segment);
 42225|     }
 42226|     gcDacVars->next_sweep_obj = &gc_heap::next_sweep_obj;
 42227| #ifdef USE_REGIONS
 42228|     gcDacVars->saved_sweep_ephemeral_seg = 0;
 42229|     gcDacVars->saved_sweep_ephemeral_start = 0;
 42230| #else
 42231|     gcDacVars->saved_sweep_ephemeral_seg = reinterpret_cast<dac_heap_segment**>(&gc_heap::saved_sweep_ephemeral_seg);
 42232|     gcDacVars->saved_sweep_ephemeral_start = &gc_heap::saved_sweep_ephemeral_start;
 42233| #endif //USE_REGIONS
 42234| #else //BACKGROUND_GC
 42235|     gcDacVars->mark_array = 0;
 42236|     gcDacVars->background_saved_lowest_address = 0;
 42237|     gcDacVars->background_saved_highest_address = 0;
 42238|     if (v2)
 42239|     {
 42240|         gcDacVars->freeable_soh_segment = 0;
 42241|         gcDacVars->freeable_uoh_segment = 0;
 42242|     }
 42243|     gcDacVars->next_sweep_obj = 0;
 42244|     gcDacVars->saved_sweep_ephemeral_seg = 0;
 42245|     gcDacVars->saved_sweep_ephemeral_start = 0;
 42246| #endif //BACKGROUND_GC
 42247|     gcDacVars->alloc_allocated = &gc_heap::alloc_allocated;
 42248|     gcDacVars->oom_info = &gc_heap::oom_info;
 42249|     gcDacVars->finalize_queue = reinterpret_cast<dac_finalize_queue**>(&gc_heap::finalize_queue);
 42250|     gcDacVars->generation_table = reinterpret_cast<unused_generation**>(&gc_heap::generation_table);
 42251| #ifdef GC_CONFIG_DRIVEN
 42252|     gcDacVars->gc_global_mechanisms = reinterpret_cast<size_t**>(&gc_global_mechanisms);
 42253|     gcDacVars->interesting_data_per_heap = reinterpret_cast<size_t**>(&gc_heap::interesting_data_per_heap);
 42254|     gcDacVars->compact_reasons_per_heap = reinterpret_cast<size_t**>(&gc_heap::compact_reasons_per_heap);
 42255|     gcDacVars->expand_mechanisms_per_heap = reinterpret_cast<size_t**>(&gc_heap::expand_mechanisms_per_heap);
 42256|     gcDacVars->interesting_mechanism_bits_per_heap = reinterpret_cast<size_t**>(&gc_heap::interesting_mechanism_bits_per_heap);
 42257| #endif // GC_CONFIG_DRIVEN
 42258| #ifdef HEAP_ANALYZE
 42259|     gcDacVars->internal_root_array = &gc_heap::internal_root_array;
 42260|     gcDacVars->internal_root_array_index = &gc_heap::internal_root_array_index;
 42261|     gcDacVars->heap_analyze_success = &gc_heap::heap_analyze_success;
 42262| #endif // HEAP_ANALYZE
 42263| #else
 42264|     gcDacVars->n_heaps = &gc_heap::n_heaps;
 42265|     gcDacVars->g_heaps = reinterpret_cast<unused_gc_heap***>(&gc_heap::g_heaps);
 42266|     gcDacVars->gc_heap_field_offsets = reinterpret_cast<int**>(&gc_heap_field_offsets);
 42267| #endif // MULTIPLE_HEAPS
 42268|     gcDacVars->generation_field_offsets = reinterpret_cast<int**>(&generation_field_offsets);
 42269|     if (v2)
 42270|     {
 42271|         gcDacVars->bookkeeping_start = &gc_heap::bookkeeping_start;
 42272|     }
 42273| }
 42274| int GCHeap::RefreshMemoryLimit()
 42275| {
 42276|     return gc_heap::refresh_memory_limit();
 42277| }
 42278| bool gc_heap::compute_hard_limit()
 42279| {
 42280|     heap_hard_limit_oh[soh] = 0;
 42281| #ifdef HOST_64BIT
 42282|     heap_hard_limit = (size_t)GCConfig::GetGCHeapHardLimit();
 42283|     heap_hard_limit_oh[soh] = (size_t)GCConfig::GetGCHeapHardLimitSOH();
 42284|     heap_hard_limit_oh[loh] = (size_t)GCConfig::GetGCHeapHardLimitLOH();
 42285|     heap_hard_limit_oh[poh] = (size_t)GCConfig::GetGCHeapHardLimitPOH();
 42286|     use_large_pages_p = GCConfig::GetGCLargePages();
 42287|     if (heap_hard_limit_oh[soh] || heap_hard_limit_oh[loh] || heap_hard_limit_oh[poh])
 42288|     {
 42289|         if (!heap_hard_limit_oh[soh])
 42290|         {
 42291|             return false;
 42292|         }
 42293|         if (!heap_hard_limit_oh[loh])
 42294|         {
 42295|             return false;
 42296|         }
 42297|         heap_hard_limit = heap_hard_limit_oh[soh] +
 42298|             heap_hard_limit_oh[loh] + heap_hard_limit_oh[poh];
 42299|     }
 42300|     else
 42301|     {
 42302|         uint32_t percent_of_mem_soh = (uint32_t)GCConfig::GetGCHeapHardLimitSOHPercent();
 42303|         uint32_t percent_of_mem_loh = (uint32_t)GCConfig::GetGCHeapHardLimitLOHPercent();
 42304|         uint32_t percent_of_mem_poh = (uint32_t)GCConfig::GetGCHeapHardLimitPOHPercent();
 42305|         if (percent_of_mem_soh || percent_of_mem_loh || percent_of_mem_poh)
 42306|         {
 42307|             if ((percent_of_mem_soh <= 0) || (percent_of_mem_soh >= 100))
 42308|             {
 42309|                 return false;
 42310|             }
 42311|             if ((percent_of_mem_loh <= 0) || (percent_of_mem_loh >= 100))
 42312|             {
 42313|                 return false;
 42314|             }
 42315|             else if ((percent_of_mem_poh < 0) || (percent_of_mem_poh >= 100))
 42316|             {
 42317|                 return false;
 42318|             }
 42319|             if ((percent_of_mem_soh + percent_of_mem_loh + percent_of_mem_poh) >= 100)
 42320|             {
 42321|                 return false;
 42322|             }
 42323|             heap_hard_limit_oh[soh] = (size_t)(total_physical_mem * (uint64_t)percent_of_mem_soh / (uint64_t)100);
 42324|             heap_hard_limit_oh[loh] = (size_t)(total_physical_mem * (uint64_t)percent_of_mem_loh / (uint64_t)100);
 42325|             heap_hard_limit_oh[poh] = (size_t)(total_physical_mem * (uint64_t)percent_of_mem_poh / (uint64_t)100);
 42326|             heap_hard_limit = heap_hard_limit_oh[soh] +
 42327|                 heap_hard_limit_oh[loh] + heap_hard_limit_oh[poh];
 42328|         }
 42329|     }
 42330|     if (heap_hard_limit_oh[soh] && (!heap_hard_limit_oh[poh]) && (!use_large_pages_p))
 42331|     {
 42332|         return false;
 42333|     }
 42334|     if (!(heap_hard_limit))
 42335|     {
 42336|         uint32_t percent_of_mem = (uint32_t)GCConfig::GetGCHeapHardLimitPercent();
 42337|         if ((percent_of_mem > 0) && (percent_of_mem < 100))
 42338|         {
 42339|             heap_hard_limit = (size_t)(total_physical_mem * (uint64_t)percent_of_mem / (uint64_t)100);
 42340|         }
 42341|     }
 42342| #endif //HOST_64BIT
 42343|     return true;
 42344| }
 42345| bool gc_heap::compute_memory_settings(bool is_initialization, uint32_t& nhp, uint32_t nhp_from_config, size_t& seg_size_from_config, size_t new_current_total_committed)
 42346| {
 42347| #ifdef HOST_64BIT
 42348|     if (!hard_limit_config_p)
 42349|     {
 42350|         if (is_restricted_physical_mem)
 42351|         {
 42352|             uint64_t physical_mem_for_gc = total_physical_mem * (uint64_t)75 / (uint64_t)100;
 42353| #ifndef USE_REGIONS
 42354|             if (is_initialization)
 42355| #endif //USE_REGIONS
 42356|             {
 42357|                 heap_hard_limit = (size_t)max ((20 * 1024 * 1024), physical_mem_for_gc);
 42358|             }
 42359|         }
 42360|     }
 42361|     if (heap_hard_limit && (heap_hard_limit < new_current_total_committed))
 42362|     {
 42363|         return false;
 42364|     }
 42365| #endif //HOST_64BIT
 42366| #ifdef USE_REGIONS
 42367|     {
 42368| #else
 42369|     if (is_initialization)
 42370|     {
 42371| #endif //USE_REGIONS
 42372|         if (heap_hard_limit)
 42373|         {
 42374|             if (is_initialization && (!nhp_from_config))
 42375|             {
 42376|                 nhp = adjust_heaps_hard_limit (nhp);
 42377|             }
 42378|             seg_size_from_config = (size_t)GCConfig::GetSegmentSize();
 42379|             if (seg_size_from_config)
 42380|             {
 42381|                 seg_size_from_config = adjust_segment_size_hard_limit_va (seg_size_from_config);
 42382|             }
 42383|             size_t limit_to_check = (heap_hard_limit_oh[soh] ? heap_hard_limit_oh[soh] : heap_hard_limit);
 42384|             soh_segment_size = max (adjust_segment_size_hard_limit (limit_to_check, nhp), seg_size_from_config);
 42385|         }
 42386|         else
 42387|         {
 42388|             soh_segment_size = get_valid_segment_size();
 42389|         }
 42390|     }
 42391|     mem_one_percent = total_physical_mem / 100;
 42392| #ifndef MULTIPLE_HEAPS
 42393|     mem_one_percent /= g_num_processors;
 42394| #endif //!MULTIPLE_HEAPS
 42395|     uint32_t highmem_th_from_config = (uint32_t)GCConfig::GetGCHighMemPercent();
 42396|     if (highmem_th_from_config)
 42397|     {
 42398|         high_memory_load_th = min (99, highmem_th_from_config);
 42399|         v_high_memory_load_th = min (99, (highmem_th_from_config + 7));
 42400| #ifdef FEATURE_EVENT_TRACE
 42401|         high_mem_percent_from_config = highmem_th_from_config;
 42402| #endif //FEATURE_EVENT_TRACE
 42403|     }
 42404|     else
 42405|     {
 42406|         int available_mem_th = 10;
 42407|         if (total_physical_mem >= ((uint64_t)80 * 1024 * 1024 * 1024))
 42408|         {
 42409|             int adjusted_available_mem_th = 3 + (int)((float)47 / (float)g_num_processors);
 42410|             available_mem_th = min (available_mem_th, adjusted_available_mem_th);
 42411|         }
 42412|         high_memory_load_th = 100 - available_mem_th;
 42413|         v_high_memory_load_th = 97;
 42414|     }
 42415|     m_high_memory_load_th = min ((high_memory_load_th + 5), v_high_memory_load_th);
 42416|     return true;
 42417| }
 42418| #ifdef USE_REGIONS
 42419| void gc_heap::compute_committed_bytes(size_t& total_committed, size_t& committed_decommit, size_t& committed_free,
 42420|                                       size_t& committed_bookkeeping, size_t& new_current_total_committed, size_t& new_current_total_committed_bookkeeping,
 42421|                                       size_t* new_committed_by_oh)
 42422| {
 42423|     for (int oh = soh; oh < total_oh_count; oh++)
 42424|     {
 42425|         int start_generation = (oh == 0) ? 0 : oh + max_generation;
 42426|         int end_generation = oh + max_generation;
 42427|         size_t total_committed_per_oh = 0;
 42428| #ifdef MULTIPLE_HEAPS
 42429|         for (int h = 0; h < n_heaps; h++)
 42430|         {
 42431|             gc_heap* heap = g_heaps[h];
 42432| #else
 42433|         {
 42434|             gc_heap* heap = pGenGCHeap;
 42435| #endif //MULTIPLE_HEAPS
 42436|             size_t total_committed_per_heap = 0;
 42437|             for (int gen = start_generation; gen <= end_generation; gen++)
 42438|             {
 42439|                 heap->accumulate_committed_bytes ( generation_start_segment (heap->generation_of (gen)), total_committed_per_heap, committed_bookkeeping);
 42440|             }
 42441|             if (oh == soh)
 42442|             {
 42443|                 heap->accumulate_committed_bytes (heap->freeable_soh_segment, total_committed_per_heap, committed_bookkeeping);
 42444|             }
 42445|             else
 42446|             {
 42447|                 heap->accumulate_committed_bytes (heap->freeable_uoh_segment, total_committed_per_heap, committed_bookkeeping, (gc_oh_num)oh);
 42448|             }
 42449| #if defined(MULTIPLE_HEAPS) && defined(_DEBUG)
 42450|             heap->committed_by_oh_per_heap_refresh[oh] = total_committed_per_heap;
 42451| #endif //MULTIPLE_HEAPS && _DEBUG
 42452|             total_committed_per_oh += total_committed_per_heap;
 42453|         }
 42454|         new_committed_by_oh[oh] = total_committed_per_oh;
 42455|         total_committed += total_committed_per_oh;
 42456|     }
 42457|     size_t committed_old_free = 0;
 42458|     committed_free = 0;
 42459| #ifdef MULTIPLE_HEAPS
 42460|     for (int h = 0; h < n_heaps; h++)
 42461|     {
 42462|         gc_heap* heap = g_heaps[h];
 42463| #else
 42464|     {
 42465|         gc_heap* heap = pGenGCHeap;
 42466| #endif //MULTIPLE_HEAPS
 42467|         for (int i = 0; i < count_free_region_kinds; i++)
 42468|         {
 42469|             heap_segment* seg = heap->free_regions[i].get_first_free_region();
 42470|             heap->accumulate_committed_bytes (seg, committed_free, committed_bookkeeping);
 42471|         }
 42472|     }
 42473|     committed_old_free += committed_free;
 42474|     committed_decommit = 0;
 42475|     for (int i = 0; i < count_free_region_kinds; i++)
 42476|     {
 42477|         heap_segment* seg = global_regions_to_decommit[i].get_first_free_region();
 42478| #ifdef MULTIPLE_HEAPS
 42479|         gc_heap* heap = g_heaps[0];
 42480| #else
 42481|         gc_heap* heap = nullptr;
 42482| #endif //MULTIPLE_HEAPS
 42483|         heap->accumulate_committed_bytes (seg, committed_decommit, committed_bookkeeping);
 42484|     }
 42485|     committed_old_free += committed_decommit;
 42486|     {
 42487|         heap_segment* seg = global_free_huge_regions.get_first_free_region();
 42488| #ifdef MULTIPLE_HEAPS
 42489|         gc_heap* heap = g_heaps[0];
 42490| #else
 42491|         gc_heap* heap = pGenGCHeap;
 42492| #endif //MULTIPLE_HEAPS
 42493|         heap->accumulate_committed_bytes (seg, committed_old_free, committed_bookkeeping);
 42494|     }
 42495|     new_committed_by_oh[recorded_committed_free_bucket] = committed_old_free;
 42496|     total_committed += committed_old_free;
 42497|     uint8_t* commit_begins[total_bookkeeping_elements];
 42498|     size_t commit_sizes[total_bookkeeping_elements];
 42499|     size_t new_sizes[total_bookkeeping_elements];
 42500|     bool get_card_table_commit_layout_result = get_card_table_commit_layout(g_gc_lowest_address, bookkeeping_covered_committed, commit_begins, commit_sizes, new_sizes);
 42501|     assert (get_card_table_commit_layout_result);
 42502|     for (int i = card_table_element; i <= seg_mapping_table_element; i++)
 42503|     {
 42504|         assert (commit_sizes[i] >= 0);
 42505|         committed_bookkeeping += commit_sizes[i];
 42506|     }
 42507|     new_current_total_committed_bookkeeping = committed_bookkeeping;
 42508|     new_committed_by_oh[recorded_committed_bookkeeping_bucket] = committed_bookkeeping;
 42509|     total_committed += committed_bookkeeping;
 42510|     new_current_total_committed = total_committed;
 42511| }
 42512| #endif //USE_REGIONS
 42513| int gc_heap::refresh_memory_limit()
 42514| {
 42515|     refresh_memory_limit_status status = refresh_success;
 42516|     if (GCConfig::GetGCTotalPhysicalMemory() != 0)
 42517|     {
 42518|         return (int)status;
 42519|     }
 42520|     GCToEEInterface::SuspendEE(SUSPEND_FOR_GC);
 42521| #ifdef USE_REGIONS
 42522|     decommit_lock.Enter();
 42523|     size_t total_committed = 0;
 42524|     size_t committed_decommit; // unused
 42525|     size_t committed_free; // unused
 42526|     size_t committed_bookkeeping = 0;
 42527|     size_t new_current_total_committed;
 42528|     size_t new_current_total_committed_bookkeeping;
 42529|     size_t new_committed_by_oh[recorded_committed_bucket_counts];
 42530|     compute_committed_bytes(total_committed, committed_decommit, committed_free,
 42531|                             committed_bookkeeping, new_current_total_committed, new_current_total_committed_bookkeeping,
 42532|                             new_committed_by_oh);
 42533| #endif //USE_REGIONS
 42534|     uint32_t nhp_from_config = static_cast<uint32_t>(GCConfig::GetHeapCount());
 42535| #ifdef MULTIPLE_HEAPS
 42536|     uint32_t nhp = n_heaps;
 42537| #else
 42538|     uint32_t nhp = 1;
 42539| #endif //MULTIPLE_HEAPS
 42540|     size_t seg_size_from_config;
 42541|     bool old_is_restricted_physical_mem = is_restricted_physical_mem;
 42542|     uint64_t old_total_physical_mem = total_physical_mem;
 42543|     size_t old_heap_hard_limit = heap_hard_limit;
 42544|     size_t old_heap_hard_limit_soh = heap_hard_limit_oh[soh];
 42545|     size_t old_heap_hard_limit_loh = heap_hard_limit_oh[loh];
 42546|     size_t old_heap_hard_limit_poh = heap_hard_limit_oh[poh];
 42547|     bool old_hard_limit_config_p = hard_limit_config_p;
 42548|     total_physical_mem = GCToOSInterface::GetPhysicalMemoryLimit (&is_restricted_physical_mem, true);
 42549|     bool succeed = true;
 42550| #ifdef USE_REGIONS
 42551|     GCConfig::RefreshHeapHardLimitSettings();
 42552|     if (!compute_hard_limit())
 42553|     {
 42554|         succeed = false;
 42555|         status = refresh_hard_limit_invalid;
 42556|     }
 42557|     hard_limit_config_p = heap_hard_limit != 0;
 42558| #else
 42559|     size_t new_current_total_committed = 0;
 42560| #endif //USE_REGIONS
 42561|     if (succeed && !compute_memory_settings(false, nhp, nhp_from_config, seg_size_from_config, new_current_total_committed))
 42562|     {
 42563|         succeed = false;
 42564|         status = refresh_hard_limit_too_low;
 42565|     }
 42566|     if (!succeed)
 42567|     {
 42568|         is_restricted_physical_mem = old_is_restricted_physical_mem;
 42569|         total_physical_mem = old_total_physical_mem;
 42570|         heap_hard_limit = old_heap_hard_limit;
 42571|         heap_hard_limit_oh[soh] = old_heap_hard_limit_soh;
 42572|         heap_hard_limit_oh[loh] = old_heap_hard_limit_loh;
 42573|         heap_hard_limit_oh[poh] = old_heap_hard_limit_poh;
 42574|         hard_limit_config_p = old_hard_limit_config_p;
 42575|     }
 42576|     else
 42577| #ifndef COMMITTED_BYTES_SHADOW
 42578|     if (!old_heap_hard_limit && heap_hard_limit)
 42579| #endif //COMMITTED_BYTES_SHADOW
 42580|     {
 42581| #ifdef USE_REGIONS
 42582|         check_commit_cs.Initialize();
 42583| #ifdef COMMITTED_BYTES_SHADOW
 42584|         assert (new_current_total_committed == current_total_committed);
 42585|         assert (new_current_total_committed_bookkeeping == current_total_committed_bookkeeping);
 42586| #else
 42587|         current_total_committed = new_current_total_committed;
 42588|         current_total_committed_bookkeeping = new_current_total_committed_bookkeeping;
 42589| #endif
 42590|         for (int i = 0; i < recorded_committed_bucket_counts; i++)
 42591|         {
 42592| #ifdef COMMITTED_BYTES_SHADOW
 42593|             assert (new_committed_by_oh[i] == committed_by_oh[i]);
 42594| #else
 42595|             committed_by_oh[i] = new_committed_by_oh[i];
 42596| #endif
 42597|         }
 42598| #ifdef MULTIPLE_HEAPS
 42599| #ifdef _DEBUG
 42600|         for (int h = 0; h < n_heaps; h++)
 42601|         {
 42602|             for (int oh = soh; oh < total_oh_count; oh++)
 42603|             {
 42604| #ifdef COMMITTED_BYTES_SHADOW
 42605|                 assert (g_heaps[h]->committed_by_oh_per_heap[oh] == g_heaps[h]->committed_by_oh_per_heap_refresh[oh]);
 42606| #else
 42607|                 g_heaps[h]->committed_by_oh_per_heap[oh] = g_heaps[h]->committed_by_oh_per_heap_refresh[oh];
 42608| #endif
 42609|             }
 42610|         }
 42611| #endif //_DEBUG
 42612| #endif //MULTIPLE_HEAPS
 42613| #else
 42614|         assert (!"NYI - Segments");
 42615| #endif //USE_REGIONS
 42616|     }
 42617| #ifdef USE_REGIONS
 42618|     decommit_lock.Leave();
 42619| #endif
 42620|     GCToEEInterface::RestartEE(TRUE);
 42621|     return (int)status;
 42622| }
 42623| #ifdef USE_REGIONS
 42624| void gc_heap::accumulate_committed_bytes(heap_segment* seg, size_t& committed_bytes, size_t& mark_array_committed_bytes, gc_oh_num oh)
 42625| {
 42626|     seg = heap_segment_rw (seg);
 42627|     while (seg)
 42628|     {
 42629|         if ((oh == unknown) || (heap_segment_oh (seg) == oh))
 42630|         {
 42631|             mark_array_committed_bytes += get_mark_array_size (seg);
 42632|             committed_bytes += (heap_segment_committed (seg) - get_region_start (seg));
 42633|         }
 42634|         seg = heap_segment_next_rw (seg);
 42635|     }
 42636| }
 42637| size_t gc_heap::get_mark_array_size (heap_segment* seg)
 42638| {
 42639|     if (seg->flags & heap_segment_flags_ma_committed)
 42640|     {
 42641|         uint32_t* mark_array_addr = mark_array;
 42642|         uint8_t* begin = get_start_address (seg);
 42643|         uint8_t* end = heap_segment_reserved (seg);
 42644|         size_t beg_word = mark_word_of (begin);
 42645|         size_t end_word = mark_word_of (align_on_mark_word (end));
 42646|         uint8_t* commit_start = align_lower_page ((uint8_t*)&mark_array_addr[beg_word]);
 42647|         uint8_t* commit_end = align_on_page ((uint8_t*)&mark_array_addr[end_word]);
 42648|         return (size_t)(commit_end - commit_start);
 42649|     }
 42650|     return 0;
 42651| }
 42652| #endif //USE_REGIONS


# ====================================================================
# FILE: src/coreclr/gc/gcconfig.h
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-147 ---
     1| #ifndef __GCCONFIG_H__
     2| #define __GCCONFIG_H__
     3| class GCConfigStringHolder
     4| {
     5| private:
     6|     const char* m_str;
     7| public:
     8|     explicit GCConfigStringHolder(const char* str)
     9|       : m_str(str) {}
    10|     GCConfigStringHolder(const GCConfigStringHolder&) = delete;
    11|     GCConfigStringHolder& operator=(const GCConfigStringHolder&) = delete;
    12|     GCConfigStringHolder(GCConfigStringHolder&&) = default;
    13|     ~GCConfigStringHolder()
    14|     {
    15|         if (m_str)
    16|         {
    17|             GCToEEInterface::FreeStringConfigValue(m_str);
    18|         }
    19|         m_str = nullptr;
    20|     }
    21|     const char* Get() const { return m_str; }
    22| };
    23| #define GC_CONFIGURATION_KEYS \
    24|     BOOL_CONFIG  (ServerGC,                  "gcServer",                  "System.GC.Server",                  false,              "Whether we should be using Server GC")                                                   \
    25|     BOOL_CONFIG  (ConcurrentGC,              "gcConcurrent",              "System.GC.Concurrent",              true,               "Whether we should be using Concurrent GC")                                                \
    26|     BOOL_CONFIG  (ConservativeGC,            "gcConservative",            NULL,                                false,              "Enables/Disables conservative GC")                                                       \
    27|     BOOL_CONFIG  (ForceCompact,              "gcForceCompact",            NULL,                                false,              "When set to true, always do compacting GC")                                              \
    28|     BOOL_CONFIG  (RetainVM,                  "GCRetainVM",                "System.GC.RetainVM",                false,              "When set we put the segments that should be deleted on a standby list (instead of "       \
    29|                                                                                                                                           "releasing them back to the OS) which will be considered to satisfy new segment requests" \
    30|                                                                                                                                           " (note that the same thing can be specified via API which is the supported way)")        \
    31|     BOOL_CONFIG  (BreakOnOOM,                "GCBreakOnOOM",              NULL,                                false,              "Does a DebugBreak at the soonest time we detect an OOM")                                 \
    32|     BOOL_CONFIG  (NoAffinitize,              "GCNoAffinitize",            "System.GC.NoAffinitize",            false,              "If set, do not affinitize server GC threads")                                             \
    33|     BOOL_CONFIG  (LogEnabled,                "GCLogEnabled",              NULL,                                false,              "Specifies if you want to turn on logging in GC")                                         \
    34|     BOOL_CONFIG  (ConfigLogEnabled,          "GCConfigLogEnabled",        NULL,                                false,              "Specifies the name of the GC config log file")                                           \
    35|     BOOL_CONFIG  (GCNumaAware,               "GCNumaAware",               NULL,                                true,               "Enables numa allocations in the GC")                                                     \
    36|     BOOL_CONFIG  (GCCpuGroup,                "GCCpuGroup",                "System.GC.CpuGroup",                false,              "Enables CPU groups in the GC")                                                            \
    37|     BOOL_CONFIG  (GCLargePages,              "GCLargePages",              "System.GC.LargePages",              false,              "Enables using Large Pages in the GC")                                                     \
    38|     INT_CONFIG   (HeapVerifyLevel,           "HeapVerify",                NULL,                                HEAPVERIFY_NONE,    "When set verifies the integrity of the managed heap on entry and exit of each GC")       \
    39|     INT_CONFIG   (LOHCompactionMode,         "GCLOHCompact",              NULL,                                0,                  "Specifies the LOH compaction mode")                                                      \
    40|     INT_CONFIG   (LOHThreshold,              "GCLOHThreshold",            NULL,                                LARGE_OBJECT_SIZE,  "Specifies the size that will make objects go on LOH")                                     \
    41|     INT_CONFIG   (BGCSpinCount,              "BGCSpinCount",              NULL,                                140,                "Specifies the bgc spin count")                                                           \
    42|     INT_CONFIG   (BGCSpin,                   "BGCSpin",                   NULL,                                2,                  "Specifies the bgc spin time")                                                            \
    43|     INT_CONFIG   (HeapCount,                 "GCHeapCount",               "System.GC.HeapCount",               0,                  "Specifies the number of server GC heaps")                                                 \
    44|     INT_CONFIG   (MaxHeapCount,              "GCMaxHeapCount",            "System.GC.MaxHeapCount",            0,                  "Specifies the max number of server GC heaps to adjust to")                                                 \
    45|     INT_CONFIG   (Gen0Size,                  "GCgen0size",                NULL,                                0,                  "Specifies the smallest gen0 budget")                                                     \
    46|     INT_CONFIG   (SegmentSize,               "GCSegmentSize",             NULL,                                0,                  "Specifies the managed heap segment size")                                                \
    47|     INT_CONFIG   (LatencyMode,               "GCLatencyMode",             NULL,                                -1,                 "Specifies the GC latency mode - batch, interactive or low latency (note that the same "   \
    48|                                                                                                                                            "thing can be specified via API which is the supported way")                             \
    49|     INT_CONFIG   (LatencyLevel,              "GCLatencyLevel",            NULL,                                1,                  "Specifies the GC latency level that you want to optimize for. Must be a number from 0 "  \
    50|                                                                                                                                           "to 3. See documentation for more details on each level.")                                \
    51|     INT_CONFIG   (LogFileSize,               "GCLogFileSize",             NULL,                                0,                  "Specifies the GC log file size")                                                         \
    52|     INT_CONFIG   (CompactRatio,              "GCCompactRatio",            NULL,                                0,                  "Specifies the ratio compacting GCs vs sweeping")                                         \
    53|     INT_CONFIG   (GCHeapAffinitizeMask,      "GCHeapAffinitizeMask",      "System.GC.HeapAffinitizeMask",      0,                  "Specifies processor mask for Server GC threads")                                          \
    54|     STRING_CONFIG(GCHeapAffinitizeRanges,    "GCHeapAffinitizeRanges",    "System.GC.HeapAffinitizeRanges",                        "Specifies list of processors for Server GC threads. The format is a comma separated "     \
    55|                                                                                                                                           "list of processor numbers or ranges of processor numbers. On Windows, each entry is "    \
    56|                                                                                                                                           "prefixed by the CPU group number. Example: Unix - 1,3,5,7-9,12, Windows - 0:1,1:7-9")    \
    57|     INT_CONFIG   (GCHighMemPercent,          "GCHighMemPercent",          "System.GC.HighMemoryPercent",       0,                  "The percent for GC to consider as high memory")                                           \
    58|     INT_CONFIG   (GCProvModeStress,          "GCProvModeStress",          NULL,                                0,                  "Stress the provisional modes")                                                           \
    59|     INT_CONFIG   (GCGen0MaxBudget,           "GCGen0MaxBudget",           NULL,                                0,                  "Specifies the largest gen0 allocation budget")                                           \
    60|     INT_CONFIG   (GCGen1MaxBudget,           "GCGen1MaxBudget",           NULL,                                0,                  "Specifies the largest gen1 allocation budget")                                           \
    61|     INT_CONFIG   (GCLowSkipRatio,            "GCLowSkipRatio",            NULL,                                30,                 "Specifies the low generation skip ratio")                                                \
    62|     INT_CONFIG   (GCHeapHardLimit,           "GCHeapHardLimit",           "System.GC.HeapHardLimit",           0,                  "Specifies a hard limit for the GC heap")                                                 \
    63|     INT_CONFIG   (GCHeapHardLimitPercent,    "GCHeapHardLimitPercent",    "System.GC.HeapHardLimitPercent",    0,                  "Specifies the GC heap usage as a percentage of the total memory")                        \
    64|     INT_CONFIG   (GCTotalPhysicalMemory,     "GCTotalPhysicalMemory",     NULL,                                0,                  "Specifies what the GC should consider to be total physical memory")                      \
    65|     INT_CONFIG   (GCRegionRange,             "GCRegionRange",             NULL,                                0,                  "Specifies the range for the GC heap")                                                    \
    66|     INT_CONFIG   (GCRegionSize,              "GCRegionSize",              NULL,                                0,                  "Specifies the size for a basic GC region")                                               \
    67|     INT_CONFIG   (GCEnableSpecialRegions,    "GCEnableSpecialRegions",    NULL,                                0,                  "Specifies to enable special handling some regions like SIP")                             \
    68|     STRING_CONFIG(LogFile,                   "GCLogFile",                 NULL,                                                    "Specifies the name of the GC log file")                                                  \
    69|     STRING_CONFIG(ConfigLogFile,             "GCConfigLogFile",           NULL,                                                    "Specifies the name of the GC config log file")                                           \
    70|     INT_CONFIG   (BGCFLTuningEnabled,        "BGCFLTuningEnabled",        NULL,                                0,                  "Enables FL tuning")                                                                      \
    71|     INT_CONFIG   (BGCMemGoal,                "BGCMemGoal",                NULL,                                75,                 "Specifies the physical memory load goal")                                                \
    72|     INT_CONFIG   (BGCMemGoalSlack,           "BGCMemGoalSlack",           NULL,                                10,                 "Specifies comfort zone of going above goal")                                             \
    73|     INT_CONFIG   (BGCFLSweepGoal,            "BGCFLSweepGoal",            NULL,                                0,                  "Specifies the gen2 sweep FL ratio goal")                                                 \
    74|     INT_CONFIG   (BGCFLSweepGoalLOH,         "BGCFLSweepGoalLOH",         NULL,                                0,                  "Specifies the LOH sweep FL ratio goal")                                                  \
    75|     INT_CONFIG   (BGCFLkp,                   "BGCFLkp",                   NULL,                                6000,               "Specifies kp for above goal tuning")                                                     \
    76|     INT_CONFIG   (BGCFLki,                   "BGCFLki",                   NULL,                                1000,               "Specifies ki for above goal tuning")                                                     \
    77|     INT_CONFIG   (BGCFLkd,                   "BGCFLkd",                   NULL,                                11,                 "Specifies kd for above goal tuning")                                                     \
    78|     INT_CONFIG   (BGCFLff,                   "BGCFLff",                   NULL,                                100,                "Specifies ff ratio")                                                                     \
    79|     INT_CONFIG   (BGCFLSmoothFactor,         "BGCFLSmoothFactor",         NULL,                                150,                "Smoothing over these")                                                                   \
    80|     INT_CONFIG   (BGCFLGradualD,             "BGCFLGradualD",             NULL,                                0,                  "Enable gradual D instead of cutting off at the value")                                   \
    81|     INT_CONFIG   (BGCMLkp,                   "BGCMLkp",                   NULL,                                1000,               "Specifies kp for ML tuning")                                                             \
    82|     INT_CONFIG   (BGCMLki,                   "BGCMLki",                   NULL,                                16,                 "Specifies ki for ML tuning")                                                             \
    83|     INT_CONFIG   (BGCFLEnableKi,             "BGCFLEnableKi",             NULL,                                1,                  "Enables ki for above goal tuning")                                                       \
    84|     INT_CONFIG   (BGCFLEnableKd,             "BGCFLEnableKd",             NULL,                                0,                  "Enables kd for above goal tuning")                                                       \
    85|     INT_CONFIG   (BGCFLEnableSmooth,         "BGCFLEnableSmooth",         NULL,                                0,                  "Enables smoothing")                                                                      \
    86|     INT_CONFIG   (BGCFLEnableTBH,            "BGCFLEnableTBH",            NULL,                                0,                  "Enables TBH")                                                                            \
    87|     INT_CONFIG   (BGCFLEnableFF,             "BGCFLEnableFF",             NULL,                                0,                  "Enables FF")                                                                             \
    88|     INT_CONFIG   (BGCG2RatioStep,            "BGCG2RatioStep",            NULL,                                5,                  "Ratio correction factor for ML loop")                                                    \
    89|     INT_CONFIG   (GCHeapHardLimitSOH,        "GCHeapHardLimitSOH",        "System.GC.HeapHardLimitSOH",        0,                  "Specifies a hard limit for the GC heap SOH")                                             \
    90|     INT_CONFIG   (GCHeapHardLimitLOH,        "GCHeapHardLimitLOH",        "System.GC.HeapHardLimitLOH",        0,                  "Specifies a hard limit for the GC heap LOH")                                             \
    91|     INT_CONFIG   (GCHeapHardLimitPOH,        "GCHeapHardLimitPOH",        "System.GC.HeapHardLimitPOH",        0,                  "Specifies a hard limit for the GC heap POH")                                             \
    92|     INT_CONFIG   (GCHeapHardLimitSOHPercent, "GCHeapHardLimitSOHPercent", "System.GC.HeapHardLimitSOHPercent", 0,                  "Specifies the GC heap SOH usage as a percentage of the total memory")                    \
    93|     INT_CONFIG   (GCHeapHardLimitLOHPercent, "GCHeapHardLimitLOHPercent", "System.GC.HeapHardLimitLOHPercent", 0,                  "Specifies the GC heap LOH usage as a percentage of the total memory")                    \
    94|     INT_CONFIG   (GCHeapHardLimitPOHPercent, "GCHeapHardLimitPOHPercent", "System.GC.HeapHardLimitPOHPercent", 0,                  "Specifies the GC heap POH usage as a percentage of the total memory")                    \
    95|     INT_CONFIG   (GCEnabledInstructionSets,  "GCEnabledInstructionSets",  NULL,                                -1,                 "Specifies whether GC can use AVX2 or AVX512F - 0 for neither, 1 for AVX2, 3 for AVX512F")\
    96|     INT_CONFIG   (GCConserveMem,             "GCConserveMemory",          "System.GC.ConserveMemory",          0,                  "Specifies how hard GC should try to conserve memory - values 0-9")                       \
    97|     INT_CONFIG   (GCWriteBarrier,            "GCWriteBarrier",            NULL,                                0,                  "Specifies whether GC should use more precise but slower write barrier")                  \
    98|     STRING_CONFIG(GCName,                    "GCName",                    "System.GC.Name",                                        "Specifies the path of the standalone GC implementation.")                                \
    99|     INT_CONFIG   (GCSpinCountUnit,           "GCSpinCountUnit",           0,                                   0,                  "Specifies the spin count unit used by the GC.")                                          \
   100|     INT_CONFIG   (GCDynamicAdaptationMode,   "GCDynamicAdaptationMode",   "System.GC.DynamicAdaptationMode",   0,                  "Enable the GC to dynamically adapt to application sizes.")                               \
   101|     BOOL_CONFIG  (GCCacheSizeFromSysConf,    "GCCacheSizeFromSysConf",    NULL,                                false,              "Specifies using sysconf to retrieve the last level cache size for Unix.")
   102| class GCConfig
   103| {
   104| #define BOOL_CONFIG(name, unused_private_key, unused_public_key, unused_default, unused_doc) \
   105|   public: static bool Get##name();                                \
   106|   public: static bool Get##name(bool defaultValue);               \
   107|   public: static void Set##name(bool value);                      \
   108|   private: static bool s_##name;                                  \
   109|   private: static bool s_##name##Provided;                        \
   110|   private: static bool s_Updated##name;
   111| #define INT_CONFIG(name, unused_private_key, unused_public_key, unused_default, unused_doc) \
   112|   public: static int64_t Get##name();                            \
   113|   public: static int64_t Get##name(int64_t defaultValue);        \
   114|   public: static void Set##name(int64_t value);                  \
   115|   private: static int64_t s_##name;                              \
   116|   private: static bool s_##name##Provided;                       \
   117|   private: static int64_t s_Updated##name;                       \
   118| #define STRING_CONFIG(name, unused_private_key, unused_public_key, unused_doc) \
   119|   public: static GCConfigStringHolder Get##name();
   120| GC_CONFIGURATION_KEYS
   121| #undef BOOL_CONFIG
   122| #undef INT_CONFIG
   123| #undef STRING_CONFIG
   124| public:
   125|   static void RefreshHeapHardLimitSettings();
   126|   static void EnumerateConfigurationValues(void* context, ConfigurationValueFunc configurationValueFunc);
   127| enum HeapVerifyFlags {
   128|     HEAPVERIFY_NONE             = 0,
   129|     HEAPVERIFY_GC               = 1,   // Verify the heap at beginning and end of GC
   130|     HEAPVERIFY_BARRIERCHECK     = 2,   // Verify the brick table
   131|     HEAPVERIFY_SYNCBLK          = 4,   // Verify sync block scanning
   132|     HEAPVERIFY_NO_RANGE_CHECKS  = 0x10,   // Excludes checking if an OBJECTREF is within the bounds of the managed heap
   133|     HEAPVERIFY_NO_MEM_FILL      = 0x20,   // Excludes filling unused segment portions with fill pattern
   134|     HEAPVERIFY_POST_GC_ONLY     = 0x40,   // Performs heap verification post-GCs only (instead of before and after each GC)
   135|     HEAPVERIFY_DEEP_ON_COMPACT  = 0x80    // Performs deep object verfication only on compacting GCs.
   136| };
   137| enum WriteBarrierFlavor
   138| {
   139|     WRITE_BARRIER_DEFAULT = 0,
   140|     WRITE_BARRIER_REGION_BIT = 1,
   141|     WRITE_BARRIER_REGION_BYTE = 2,
   142|     WRITE_BARRIER_SERVER = 3,
   143| };
   144| static void Initialize();
   145| };
   146| bool ParseGCHeapAffinitizeRanges(const char* cpu_index_ranges, AffinitySet* config_affinity_set, uintptr_t& config_affinity_mask);
   147| #endif // __GCCONFIG_H__


# ====================================================================
# FILE: src/coreclr/gc/unix/gcenv.unix.cpp
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-1051 ---
     1| #define _WITH_GETLINE
     2| #include <cstdint>
     3| #include <cstddef>
     4| #include <cstdio>
     5| #include <cassert>
     6| #define __STDC_FORMAT_MACROS
     7| #include <cinttypes>
     8| #include <memory>
     9| #include <pthread.h>
    10| #include <signal.h>
    11| #include "config.gc.h"
    12| #include "common.h"
    13| #include "gcenv.structs.h"
    14| #include "gcenv.base.h"
    15| #include "gcenv.os.h"
    16| #include "gcenv.ee.h"
    17| #include "gcenv.unix.inl"
    18| #include "volatile.h"
    19| #include "gcconfig.h"
    20| #include "numasupport.h"
    21| #if HAVE_SWAPCTL
    22| #include <sys/swap.h>
    23| #endif
    24| #undef min
    25| #undef max
    26| #ifndef __has_cpp_attribute
    27| #define __has_cpp_attribute(x) (0)
    28| #endif
    29| #if __has_cpp_attribute(fallthrough)
    30| #define FALLTHROUGH [[fallthrough]]
    31| #else
    32| #define FALLTHROUGH
    33| #endif
    34| #include <algorithm>
    35| #if HAVE_SYS_TIME_H
    36|  #include <sys/time.h>
    37| #else
    38|  #error "sys/time.h required by GC PAL for the time being"
    39| #endif
    40| #if HAVE_SYS_MMAN_H
    41|  #include <sys/mman.h>
    42| #else
    43|  #error "sys/mman.h required by GC PAL"
    44| #endif
    45| #if HAVE_SYSCTLBYNAME
    46| #include <sys/types.h>
    47| #include <sys/sysctl.h>
    48| #endif
    49| #if HAVE_SYSINFO
    50| #include <sys/sysinfo.h>
    51| #endif
    52| #if HAVE_XSWDEV
    53| #include <vm/vm_param.h>
    54| #endif // HAVE_XSWDEV
    55| #ifdef __APPLE__
    56| #include <mach/vm_types.h>
    57| #include <mach/vm_param.h>
    58| #include <mach/mach_port.h>
    59| #include <mach/mach_host.h>
    60| #include <mach/task.h>
    61| #include <mach/vm_map.h>
    62| extern "C"
    63| {
    64| #  include <mach/thread_state.h>
    65| }
    66| #define CHECK_MACH(_msg, machret) do {                                      \
    67|         if (machret != KERN_SUCCESS)                                        \
    68|         {                                                                   \
    69|             char _szError[1024];                                            \
    70|             snprintf(_szError, ARRAY_SIZE(_szError), "%s: %u: %s", __FUNCTION__, __LINE__, _msg);  \
    71|             mach_error(_szError, machret);                                  \
    72|             abort();                                                        \
    73|         }                                                                   \
    74|     } while (false)
    75| #endif // __APPLE__
    76| #ifdef __linux__
    77| #include <sys/syscall.h> // __NR_membarrier
    78| # if !defined(__NR_membarrier)
    79| #  if defined(__amd64__)
    80| #   define __NR_membarrier  324
    81| #  elif defined(__i386__)
    82| #   define __NR_membarrier  375
    83| #  elif defined(__arm__)
    84| #   define __NR_membarrier  389
    85| #  elif defined(__aarch64__)
    86| #   define __NR_membarrier  283
    87| #  elif defined(__loongarch64)
    88| #   define __NR_membarrier  283
    89| #  else
    90| #   error Unknown architecture
    91| #  endif
    92| # endif
    93| #endif
    94| #if HAVE_PTHREAD_NP_H
    95| #include <pthread_np.h>
    96| #endif
    97| #if HAVE_CPUSET_T
    98| typedef cpuset_t cpu_set_t;
    99| #endif
   100| #include <time.h> // nanosleep
   101| #include <sched.h> // sched_yield
   102| #include <errno.h>
   103| #include <unistd.h> // sysconf
   104| #include "globals.h"
   105| #include "cgroup.h"
   106| #ifndef __APPLE__
   107| #if HAVE_SYSCONF && HAVE__SC_AVPHYS_PAGES
   108| #define SYSCONF_PAGES _SC_AVPHYS_PAGES
   109| #elif HAVE_SYSCONF && HAVE__SC_PHYS_PAGES
   110| #define SYSCONF_PAGES _SC_PHYS_PAGES
   111| #else
   112| #error Dont know how to get page-size on this architecture!
   113| #endif
   114| #endif // __APPLE__
   115| #if defined(HOST_ARM) || defined(HOST_ARM64) || defined(HOST_LOONGARCH64) || defined(HOST_RISCV64)
   116| #define SYSCONF_GET_NUMPROCS _SC_NPROCESSORS_CONF
   117| #else
   118| #define SYSCONF_GET_NUMPROCS _SC_NPROCESSORS_ONLN
   119| #endif
   120| static uint32_t g_totalCpuCount = 0;
   121| #ifdef __NR_membarrier
   122| # define membarrier(...)  syscall(__NR_membarrier, __VA_ARGS__)
   123| #else
   124| # define membarrier(...)  -ENOSYS
   125| #endif
   126| enum membarrier_cmd
   127| {
   128|     MEMBARRIER_CMD_QUERY                                 = 0,
   129|     MEMBARRIER_CMD_GLOBAL                                = (1 << 0),
   130|     MEMBARRIER_CMD_GLOBAL_EXPEDITED                      = (1 << 1),
   131|     MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED             = (1 << 2),
   132|     MEMBARRIER_CMD_PRIVATE_EXPEDITED                     = (1 << 3),
   133|     MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED            = (1 << 4),
   134|     MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE           = (1 << 5),
   135|     MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE  = (1 << 6)
   136| };
   137| bool CanFlushUsingMembarrier()
   138| {
   139| #ifdef TARGET_ANDROID
   140|     int apiLevel = android_get_device_api_level();
   141|     if (apiLevel < __ANDROID_API_Q__)
   142|     {
   143|         return false;
   144|     }
   145| #endif
   146|     int mask = membarrier(MEMBARRIER_CMD_QUERY, 0);
   147|     if (mask >= 0 &&
   148|         mask & MEMBARRIER_CMD_PRIVATE_EXPEDITED &&
   149|         membarrier(MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED, 0) == 0)
   150|     {
   151|         return true;
   152|     }
   153|     return false;
   154| }
   155| static int s_flushUsingMemBarrier = 0;
   156| static uint8_t* g_helperPage = 0;
   157| static pthread_mutex_t g_flushProcessWriteBuffersMutex;
   158| size_t GetRestrictedPhysicalMemoryLimit();
   159| bool GetPhysicalMemoryUsed(size_t* val);
   160| static size_t g_RestrictedPhysicalMemoryLimit = 0;
   161| uint32_t g_pageSizeUnixInl = 0;
   162| AffinitySet g_processAffinitySet;
   163| extern "C" int g_highestNumaNode;
   164| extern "C" bool g_numaAvailable;
   165| bool GCToOSInterface::Initialize()
   166| {
   167|     int pageSize = sysconf( _SC_PAGE_SIZE );
   168|     g_pageSizeUnixInl = uint32_t((pageSize > 0) ? pageSize : 0x1000);
   169|     int cpuCount = sysconf(SYSCONF_GET_NUMPROCS);
   170|     if (cpuCount == -1)
   171|     {
   172|         return false;
   173|     }
   174|     g_totalCpuCount = cpuCount;
   175|     assert(s_flushUsingMemBarrier == 0);
   176|     if (CanFlushUsingMembarrier())
   177|     {
   178|         s_flushUsingMemBarrier = TRUE;
   179|     }
   180| #ifndef TARGET_APPLE
   181|     else
   182|     {
   183|         assert(g_helperPage == 0);
   184|         g_helperPage = static_cast<uint8_t*>(mmap(0, OS_PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_PRIVATE, -1, 0));
   185|         if (g_helperPage == MAP_FAILED)
   186|         {
   187|             return false;
   188|         }
   189|         assert((((size_t)g_helperPage) & (OS_PAGE_SIZE - 1)) == 0);
   190|         int status = mlock(g_helperPage, OS_PAGE_SIZE);
   191|         if (status != 0)
   192|         {
   193|             return false;
   194|         }
   195|         status = pthread_mutex_init(&g_flushProcessWriteBuffersMutex, NULL);
   196|         if (status != 0)
   197|         {
   198|             munlock(g_helperPage, OS_PAGE_SIZE);
   199|             return false;
   200|         }
   201|     }
   202| #endif // !TARGET_APPLE
   203|     InitializeCGroup();
   204| #if HAVE_SCHED_GETAFFINITY
   205|     cpu_set_t cpuSet;
   206|     int st = sched_getaffinity(getpid(), sizeof(cpu_set_t), &cpuSet);
   207|     if (st == 0)
   208|     {
   209|         for (size_t i = 0; i < CPU_SETSIZE; i++)
   210|         {
   211|             if (CPU_ISSET(i, &cpuSet))
   212|             {
   213|                 g_processAffinitySet.Add(i);
   214|             }
   215|         }
   216|     }
   217|     else
   218|     {
   219|         assert(false);
   220|     }
   221| #else // HAVE_SCHED_GETAFFINITY
   222|     for (size_t i = 0; i < g_totalCpuCount; i++)
   223|     {
   224|         g_processAffinitySet.Add(i);
   225|     }
   226| #endif // HAVE_SCHED_GETAFFINITY
   227|     NUMASupportInitialize();
   228|     return true;
   229| }
   230| void GCToOSInterface::Shutdown()
   231| {
   232|     int ret = munlock(g_helperPage, OS_PAGE_SIZE);
   233|     assert(ret == 0);
   234|     ret = pthread_mutex_destroy(&g_flushProcessWriteBuffersMutex);
   235|     assert(ret == 0);
   236|     munmap(g_helperPage, OS_PAGE_SIZE);
   237|     CleanupCGroup();
   238| }
   239| uint64_t GCToOSInterface::GetCurrentThreadIdForLogging()
   240| {
   241| #if defined(__linux__)
   242|     return (uint64_t)syscall(SYS_gettid);
   243| #elif HAVE_PTHREAD_GETTHREADID_NP
   244|     return (uint64_t)pthread_getthreadid_np();
   245| #elif HAVE_PTHREAD_THREADID_NP
   246|     unsigned long long tid;
   247|     pthread_threadid_np(pthread_self(), &tid);
   248|     return (uint64_t)tid;
   249| #else
   250|     return (uint64_t)pthread_self();
   251| #endif
   252| }
   253| uint32_t GCToOSInterface::GetCurrentProcessId()
   254| {
   255|     return getpid();
   256| }
   257| bool GCToOSInterface::SetCurrentThreadIdealAffinity(uint16_t srcProcNo, uint16_t dstProcNo)
   258| {
   259|     return true;
   260| }
   261| uint32_t GCToOSInterface::GetCurrentProcessorNumber()
   262| {
   263| #if HAVE_SCHED_GETCPU
   264|     int processorNumber = sched_getcpu();
   265|     assert(processorNumber != -1);
   266|     return processorNumber;
   267| #else
   268|     assert(false); // This method is expected to be called only if CanGetCurrentProcessorNumber is true
   269|     return 0;
   270| #endif
   271| }
   272| bool GCToOSInterface::CanGetCurrentProcessorNumber()
   273| {
   274|     return HAVE_SCHED_GETCPU;
   275| }
   276| void GCToOSInterface::FlushProcessWriteBuffers()
   277| {
   278|     if (s_flushUsingMemBarrier)
   279|     {
   280|         int status = membarrier(MEMBARRIER_CMD_PRIVATE_EXPEDITED, 0);
   281|         assert(status == 0 && "Failed to flush using membarrier");
   282|     }
   283|     else if (g_helperPage != 0)
   284|     {
   285|         int status = pthread_mutex_lock(&g_flushProcessWriteBuffersMutex);
   286|         assert(status == 0 && "Failed to lock the flushProcessWriteBuffersMutex lock");
   287|         status = mprotect(g_helperPage, OS_PAGE_SIZE, PROT_READ | PROT_WRITE);
   288|         assert(status == 0 && "Failed to change helper page protection to read / write");
   289|         __sync_add_and_fetch((size_t*)g_helperPage, 1);
   290|         status = mprotect(g_helperPage, OS_PAGE_SIZE, PROT_NONE);
   291|         assert(status == 0 && "Failed to change helper page protection to no access");
   292|         status = pthread_mutex_unlock(&g_flushProcessWriteBuffersMutex);
   293|         assert(status == 0 && "Failed to unlock the flushProcessWriteBuffersMutex lock");
   294|     }
   295| #ifdef TARGET_APPLE
   296|     else
   297|     {
   298|         mach_msg_type_number_t cThreads;
   299|         thread_act_t *pThreads;
   300|         kern_return_t machret = task_threads(mach_task_self(), &pThreads, &cThreads);
   301|         CHECK_MACH("task_threads()", machret);
   302|         uintptr_t sp;
   303|         uintptr_t registerValues[128];
   304|         for (mach_msg_type_number_t i = 0; i < cThreads; i++)
   305|         {
   306|             if (__builtin_available (macOS 10.14, iOS 12, tvOS 9, *))
   307|             {
   308|                 size_t registers = 128;
   309|                 machret = thread_get_register_pointer_values(pThreads[i], &sp, &registers, registerValues);
   310|             }
   311|             else
   312|             {
   313| #if defined(HOST_AMD64)
   314|                 x86_thread_state64_t threadState;
   315|                 mach_msg_type_number_t count = x86_THREAD_STATE64_COUNT;
   316|                 machret = thread_get_state(pThreads[i], x86_THREAD_STATE64, (thread_state_t)&threadState, &count);
   317| #elif defined(HOST_ARM64)
   318|                 arm_thread_state64_t threadState;
   319|                 mach_msg_type_number_t count = ARM_THREAD_STATE64_COUNT;
   320|                 machret = thread_get_state(pThreads[i], ARM_THREAD_STATE64, (thread_state_t)&threadState, &count);
   321| #else
   322|                 #error Unexpected architecture
   323| #endif
   324|             }
   325|             if (machret == KERN_INSUFFICIENT_BUFFER_SIZE)
   326|             {
   327|                 CHECK_MACH("thread_get_register_pointer_values()", machret);
   328|             }
   329|             machret = mach_port_deallocate(mach_task_self(), pThreads[i]);
   330|             CHECK_MACH("mach_port_deallocate()", machret);
   331|         }
   332|         machret = vm_deallocate(mach_task_self(), (vm_address_t)pThreads, cThreads * sizeof(thread_act_t));
   333|         CHECK_MACH("vm_deallocate()", machret);
   334|     }
   335| #endif // TARGET_APPLE
   336| }
   337| void GCToOSInterface::DebugBreak()
   338| {
   339| #if __has_builtin(__builtin_debugtrap)
   340|     __builtin_debugtrap();
   341| #else
   342|     raise(SIGTRAP);
   343| #endif
   344| }
   345| void GCToOSInterface::Sleep(uint32_t sleepMSec)
   346| {
   347|     if (sleepMSec == 0)
   348|     {
   349|         return;
   350|     }
   351|     timespec requested;
   352|     requested.tv_sec = sleepMSec / tccSecondsToMilliSeconds;
   353|     requested.tv_nsec = (sleepMSec - requested.tv_sec * tccSecondsToMilliSeconds) * tccMilliSecondsToNanoSeconds;
   354|     timespec remaining;
   355|     while (nanosleep(&requested, &remaining) == EINTR)
   356|     {
   357|         requested = remaining;
   358|     }
   359| }
   360| void GCToOSInterface::YieldThread(uint32_t switchCount)
   361| {
   362|     int ret = sched_yield();
   363|     assert(ret == 0);
   364| }
   365| static void* VirtualReserveInner(size_t size, size_t alignment, uint32_t flags, uint32_t hugePagesFlag = 0)
   366| {
   367|     assert(!(flags & VirtualReserveFlags::WriteWatch) && "WriteWatch not supported on Unix");
   368|     if (alignment < OS_PAGE_SIZE)
   369|     {
   370|         alignment = OS_PAGE_SIZE;
   371|     }
   372|     size_t alignedSize = size + (alignment - OS_PAGE_SIZE);
   373|     void * pRetVal = mmap(nullptr, alignedSize, PROT_NONE, MAP_ANON | MAP_PRIVATE | hugePagesFlag, -1, 0);
   374|     if (pRetVal != MAP_FAILED)
   375|     {
   376|         void * pAlignedRetVal = (void *)(((size_t)pRetVal + (alignment - 1)) & ~(alignment - 1));
   377|         size_t startPadding = (size_t)pAlignedRetVal - (size_t)pRetVal;
   378|         if (startPadding != 0)
   379|         {
   380|             int ret = munmap(pRetVal, startPadding);
   381|             assert(ret == 0);
   382|         }
   383|         size_t endPadding = alignedSize - (startPadding + size);
   384|         if (endPadding != 0)
   385|         {
   386|             int ret = munmap((void *)((size_t)pAlignedRetVal + size), endPadding);
   387|             assert(ret == 0);
   388|         }
   389|         pRetVal = pAlignedRetVal;
   390| #ifdef MADV_DONTDUMP
   391|         madvise(pRetVal, size, MADV_DONTDUMP);
   392| #endif
   393|         return pRetVal;
   394|     }
   395|     return NULL; // return NULL if mmap failed
   396| }
   397| void* GCToOSInterface::VirtualReserve(size_t size, size_t alignment, uint32_t flags, uint16_t node)
   398| {
   399|     return VirtualReserveInner(size, alignment, flags);
   400| }
   401| bool GCToOSInterface::VirtualRelease(void* address, size_t size)
   402| {
   403|     int ret = munmap(address, size);
   404|     return (ret == 0);
   405| }
   406| void* GCToOSInterface::VirtualReserveAndCommitLargePages(size_t size, uint16_t node)
   407| {
   408| #if HAVE_MAP_HUGETLB
   409|     uint32_t largePagesFlag = MAP_HUGETLB;
   410| #elif HAVE_VM_FLAGS_SUPERPAGE_SIZE_ANY
   411|     uint32_t largePagesFlag = VM_FLAGS_SUPERPAGE_SIZE_ANY;
   412| #else
   413|     uint32_t largePagesFlag = 0;
   414| #endif
   415|     void* pRetVal = VirtualReserveInner(size, OS_PAGE_SIZE, 0, largePagesFlag);
   416|     if (VirtualCommit(pRetVal, size, node))
   417|     {
   418|         return pRetVal;
   419|     }
   420|     return nullptr;
   421| }
   422| bool GCToOSInterface::VirtualCommit(void* address, size_t size, uint16_t node)
   423| {
   424|     bool success = mprotect(address, size, PROT_WRITE | PROT_READ) == 0;
   425| #ifdef MADV_DODUMP
   426|     if (success)
   427|     {
   428|         madvise(address, size, MADV_DODUMP);
   429|     }
   430| #endif
   431| #ifdef TARGET_LINUX
   432|     if (success && g_numaAvailable && (node != NUMA_NODE_UNDEFINED))
   433|     {
   434|         if ((int)node <= g_highestNumaNode)
   435|         {
   436|             int usedNodeMaskBits = g_highestNumaNode + 1;
   437|             int nodeMaskLength = (usedNodeMaskBits + sizeof(unsigned long) - 1) / sizeof(unsigned long);
   438|             unsigned long nodeMask[nodeMaskLength];
   439|             memset(nodeMask, 0, sizeof(nodeMask));
   440|             int index = node / sizeof(unsigned long);
   441|             nodeMask[index] = ((unsigned long)1) << (node & (sizeof(unsigned long) - 1));
   442|             int st = BindMemoryPolicy(address, size, nodeMask, usedNodeMaskBits);
   443|             assert(st == 0);
   444|         }
   445|     }
   446| #endif // TARGET_LINUX
   447|     return success;
   448| }
   449| bool GCToOSInterface::VirtualDecommit(void* address, size_t size)
   450| {
   451|     bool bRetVal = mmap(address, size, PROT_NONE, MAP_FIXED | MAP_ANON | MAP_PRIVATE, -1, 0) != MAP_FAILED;
   452| #ifdef MADV_DONTDUMP
   453|     if (bRetVal)
   454|     {
   455|         madvise(address, size, MADV_DONTDUMP);
   456|     }
   457| #endif
   458|     return  bRetVal;
   459| }
   460| bool GCToOSInterface::VirtualReset(void * address, size_t size, bool unlock)
   461| {
   462|     int st;
   463| #if HAVE_MADV_FREE
   464|     st = madvise(address, size, MADV_FREE);
   465|     if (st != 0)
   466| #endif
   467|     {
   468| #if HAVE_POSIX_MADVISE
   469|         st = posix_madvise(address, size, MADV_DONTNEED);
   470| #else
   471|         st = EINVAL;
   472| #endif
   473|     }
   474| #ifdef MADV_DONTDUMP
   475|     if (st == 0)
   476|     {
   477|         madvise(address, size, MADV_DONTDUMP);
   478|     }
   479| #endif
   480|     return (st == 0);
   481| }
   482| bool GCToOSInterface::SupportsWriteWatch()
   483| {
   484|     return false;
   485| }
   486| void GCToOSInterface::ResetWriteWatch(void* address, size_t size)
   487| {
   488|     assert(!"should never call ResetWriteWatch on Unix");
   489| }
   490| bool GCToOSInterface::GetWriteWatch(bool resetState, void* address, size_t size, void** pageAddresses, uintptr_t* pageAddressesCount)
   491| {
   492|     assert(!"should never call GetWriteWatch on Unix");
   493|     return false;
   494| }
   495| bool ReadMemoryValueFromFile(const char* filename, uint64_t* val)
   496| {
   497|     bool result = false;
   498|     char* line = nullptr;
   499|     size_t lineLen = 0;
   500|     char* endptr = nullptr;
   501|     uint64_t num = 0, l, multiplier;
   502|     FILE* file = nullptr;
   503|     if (val == nullptr)
   504|         goto done;
   505|     file = fopen(filename, "r");
   506|     if (file == nullptr)
   507|         goto done;
   508|     if (getline(&line, &lineLen, file) == -1)
   509|         goto done;
   510|     errno = 0;
   511|     num = strtoull(line, &endptr, 0);
   512|     if (line == endptr || errno != 0)
   513|         goto done;
   514|     multiplier = 1;
   515|     switch (*endptr)
   516|     {
   517|     case 'g':
   518|     case 'G': multiplier = 1024;
   519|               FALLTHROUGH;
   520|     case 'm':
   521|     case 'M': multiplier = multiplier * 1024;
   522|               FALLTHROUGH;
   523|     case 'k':
   524|     case 'K': multiplier = multiplier * 1024;
   525|     }
   526|     *val = num * multiplier;
   527|     result = true;
   528|     if (*val / multiplier != num)
   529|         result = false;
   530| done:
   531|     if (file)
   532|         fclose(file);
   533|     free(line);
   534|     return result;
   535| }
   536| static void GetLogicalProcessorCacheSizeFromSysConf(size_t* cacheLevel, size_t* cacheSize)
   537| {
   538|     assert (cacheLevel != nullptr);
   539|     assert (cacheSize != nullptr);
   540| #if defined(_SC_LEVEL1_DCACHE_SIZE) || defined(_SC_LEVEL2_CACHE_SIZE) || defined(_SC_LEVEL3_CACHE_SIZE) || defined(_SC_LEVEL4_CACHE_SIZE)
   541|     const int cacheLevelNames[] =
   542|     {
   543|         _SC_LEVEL1_DCACHE_SIZE,
   544|         _SC_LEVEL2_CACHE_SIZE,
   545|         _SC_LEVEL3_CACHE_SIZE,
   546|         _SC_LEVEL4_CACHE_SIZE,
   547|     };
   548|     for (int i = ARRAY_SIZE(cacheLevelNames) - 1; i >= 0; i--)
   549|     {
   550|         long size = sysconf(cacheLevelNames[i]);
   551|         if (size > 0)
   552|         {
   553|             *cacheSize = (size_t)size;
   554|             *cacheLevel = i + 1;
   555|             break;
   556|         }
   557|     }
   558| #endif
   559| }
   560| static void GetLogicalProcessorCacheSizeFromSysFs(size_t* cacheLevel, size_t* cacheSize)
   561| {
   562|     assert (cacheLevel != nullptr);
   563|     assert (cacheSize != nullptr);
   564| #if defined(TARGET_LINUX) && !defined(HOST_ARM) && !defined(HOST_X86)
   565|     size_t level;
   566|     char path_to_size_file[] =  "/sys/devices/system/cpu/cpu0/cache/index-/size";
   567|     char path_to_level_file[] =  "/sys/devices/system/cpu/cpu0/cache/index-/level";
   568|     int index = 40;
   569|     assert(path_to_size_file[index] == '-');
   570|     assert(path_to_level_file[index] == '-');
   571|     for (int i = 0; i < 5; i++)
   572|     {
   573|         path_to_size_file[index] = (char)(48 + i);
   574|         uint64_t cache_size_from_sys_file = 0;
   575|         if (ReadMemoryValueFromFile(path_to_size_file, &cache_size_from_sys_file))
   576|         {
   577|             *cacheSize = std::max(*cacheSize, (size_t)cache_size_from_sys_file);
   578|             path_to_level_file[index] = (char)(48 + i);
   579|             if (ReadMemoryValueFromFile(path_to_level_file, &level))
   580|             {
   581|                 *cacheLevel = level;
   582|             }
   583|         }
   584|     }
   585| #endif 
   586| }
   587| static void GetLogicalProcessorCacheSizeFromHeuristic(size_t* cacheLevel, size_t* cacheSize)
   588| {
   589|     assert (cacheLevel != nullptr);
   590|     assert (cacheSize != nullptr);
   591| #if (defined(TARGET_LINUX) && !defined(TARGET_APPLE))
   592|     {
   593|         DWORD logicalCPUs = g_processAffinitySet.Count();
   594|         if (logicalCPUs < 5)
   595|         {
   596|             *cacheSize = 4;
   597|         }
   598|         else if (logicalCPUs < 17)
   599|         {
   600|             *cacheSize = 8;
   601|         }
   602|         else if (logicalCPUs < 65)
   603|         {
   604|             *cacheSize = 16;
   605|         }
   606|         else
   607|         {
   608|             *cacheSize = 32;
   609|         }
   610|         *cacheSize *= (1024 * 1024);
   611|     }
   612| #endif
   613| }
   614| static size_t GetLogicalProcessorCacheSizeFromOS()
   615| {
   616|     size_t cacheLevel = 0;
   617|     size_t cacheSize = 0;
   618|     if (GCConfig::GetGCCacheSizeFromSysConf())
   619|     {
   620|         GetLogicalProcessorCacheSizeFromSysConf(&cacheLevel, &cacheSize);
   621|     }
   622|     if (cacheSize == 0) 
   623|     {
   624|         GetLogicalProcessorCacheSizeFromSysFs(&cacheLevel, &cacheSize);
   625|         if (cacheSize == 0)
   626|         {
   627|             GetLogicalProcessorCacheSizeFromHeuristic(&cacheLevel, &cacheSize);
   628|         }
   629|     }
   630| #if HAVE_SYSCTLBYNAME
   631|     if (cacheSize == 0)
   632|     {
   633|         int64_t cacheSizeFromSysctl = 0;
   634|         size_t sz = sizeof(cacheSizeFromSysctl);
   635|         const bool success = false
   636|             || sysctlbyname("hw.perflevel0.l3cachesize", &cacheSizeFromSysctl, &sz, nullptr, 0) == 0
   637|             || sysctlbyname("hw.perflevel0.l2cachesize", &cacheSizeFromSysctl, &sz, nullptr, 0) == 0
   638|             || sysctlbyname("hw.l3cachesize", &cacheSizeFromSysctl, &sz, nullptr, 0) == 0
   639|             || sysctlbyname("hw.l2cachesize", &cacheSizeFromSysctl, &sz, nullptr, 0) == 0
   640|             || sysctlbyname("hw.l1dcachesize", &cacheSizeFromSysctl, &sz, nullptr, 0) == 0;
   641|         if (success)
   642|         {
   643|             assert(cacheSizeFromSysctl > 0);
   644|             cacheSize = (size_t) cacheSizeFromSysctl;
   645|         }
   646|     }
   647| #endif
   648| #if (defined(HOST_ARM64) || defined(HOST_LOONGARCH64)) && !defined(TARGET_APPLE)
   649|     if (cacheLevel != 3)
   650|     {
   651|         GetLogicalProcessorCacheSizeFromHeuristic(&cacheLevel, &cacheSize);
   652|     }
   653| #endif
   654|     return cacheSize;
   655| }
   656| static uint64_t GetMemorySizeMultiplier(char units)
   657| {
   658|     switch(units)
   659|     {
   660|         case 'g':
   661|         case 'G': return 1024 * 1024 * 1024;
   662|         case 'm':
   663|         case 'M': return 1024 * 1024;
   664|         case 'k':
   665|         case 'K': return 1024;
   666|     }
   667|     return 1;
   668| }
   669| #ifndef __APPLE__
   670| static bool ReadMemAvailable(uint64_t* memAvailable)
   671| {
   672|     bool foundMemAvailable = false;
   673|     FILE* memInfoFile = fopen("/proc/meminfo", "r");
   674|     if (memInfoFile != NULL)
   675|     {
   676|         char *line = nullptr;
   677|         size_t lineLen = 0;
   678|         while (getline(&line, &lineLen, memInfoFile) != -1)
   679|         {
   680|             char units = '\0';
   681|             uint64_t available;
   682|             int fieldsParsed = sscanf(line, "MemAvailable: %" SCNu64 " %cB", &available, &units);
   683|             if (fieldsParsed >= 1)
   684|             {
   685|                 uint64_t multiplier = GetMemorySizeMultiplier(units);
   686|                 *memAvailable = available * multiplier;
   687|                 foundMemAvailable = true;
   688|                 break;
   689|             }
   690|         }
   691|         free(line);
   692|         fclose(memInfoFile);
   693|     }
   694|     return foundMemAvailable;
   695| }
   696| #endif // __APPLE__
   697| size_t GCToOSInterface::GetCacheSizePerLogicalCpu(bool trueSize)
   698| {
   699|     static volatile size_t s_maxSize;
   700|     static volatile size_t s_maxTrueSize;
   701|     size_t size = trueSize ? s_maxTrueSize : s_maxSize;
   702|     if (size != 0)
   703|         return size;
   704|     size_t maxSize, maxTrueSize;
   705|     maxSize = maxTrueSize = GetLogicalProcessorCacheSizeFromOS(); // Returns the size of the highest level processor cache
   706|     s_maxSize = maxSize;
   707|     s_maxTrueSize = maxTrueSize;
   708|     return trueSize ? maxTrueSize : maxSize;
   709| }
   710| bool GCToOSInterface::SetThreadAffinity(uint16_t procNo)
   711| {
   712| #if HAVE_SCHED_SETAFFINITY || HAVE_PTHREAD_SETAFFINITY_NP
   713|     cpu_set_t cpuSet;
   714|     CPU_ZERO(&cpuSet);
   715|     CPU_SET((int)procNo, &cpuSet);
   716| #if HAVE_SCHED_SETAFFINITY
   717|     int st = sched_setaffinity(0, sizeof(cpu_set_t), &cpuSet);
   718| #else
   719|     int st = pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuSet);
   720| #endif
   721|     return (st == 0);
   722| #else  // !(HAVE_SCHED_SETAFFINITY || HAVE_PTHREAD_SETAFFINITY_NP)
   723|     return false;
   724| #endif // HAVE_SCHED_SETAFFINITY || HAVE_PTHREAD_SETAFFINITY_NP
   725| }
   726| bool GCToOSInterface::BoostThreadPriority()
   727| {
   728|     return false;
   729| }
   730| const AffinitySet* GCToOSInterface::SetGCThreadsAffinitySet(uintptr_t configAffinityMask, const AffinitySet* configAffinitySet)
   731| {
   732|     if (!configAffinitySet->IsEmpty())
   733|     {
   734|         for (size_t i = 0; i < MAX_SUPPORTED_CPUS; i++)
   735|         {
   736|             if (g_processAffinitySet.Contains(i) && !configAffinitySet->Contains(i))
   737|             {
   738|                 g_processAffinitySet.Remove(i);
   739|             }
   740|         }
   741|     }
   742|     return &g_processAffinitySet;
   743| }
   744| size_t GCToOSInterface::GetVirtualMemoryLimit()
   745| {
   746| #ifdef HOST_64BIT
   747| #ifndef TARGET_RISCV64
   748|     static const uint64_t _128TB = (1ull << 47);
   749|     return _128TB;
   750| #else // TARGET_RISCV64
   751|     static const uint64_t _256GB = (1ull << 38);
   752|     return _256GB;
   753| #endif // TARGET_RISCV64
   754| #else
   755|     return (size_t)-1;
   756| #endif
   757| }
   758| uint64_t GCToOSInterface::GetPhysicalMemoryLimit(bool* is_restricted, bool refresh)
   759| {
   760|     size_t restricted_limit;
   761|     if (is_restricted)
   762|         *is_restricted = false;
   763|     if (g_RestrictedPhysicalMemoryLimit == 0 || refresh)
   764|     {
   765|         restricted_limit = GetRestrictedPhysicalMemoryLimit();
   766|         VolatileStore(&g_RestrictedPhysicalMemoryLimit, restricted_limit);
   767|     }
   768|     restricted_limit = g_RestrictedPhysicalMemoryLimit;
   769|     if (restricted_limit != 0 && restricted_limit != SIZE_T_MAX)
   770|     {
   771|         if (is_restricted)
   772|             *is_restricted = true;
   773|         return restricted_limit;
   774|     }
   775| #if HAVE_SYSCONF && HAVE__SC_PHYS_PAGES
   776|     long pages = sysconf(_SC_PHYS_PAGES);
   777|     if (pages == -1)
   778|     {
   779|         return 0;
   780|     }
   781|     long pageSize = sysconf(_SC_PAGE_SIZE);
   782|     if (pageSize == -1)
   783|     {
   784|         return 0;
   785|     }
   786|     return (uint64_t)pages * (uint64_t)pageSize;
   787| #elif HAVE_SYSCTL
   788|     int mib[3];
   789|     mib[0] = CTL_HW;
   790|     mib[1] = HW_MEMSIZE;
   791|     int64_t physical_memory = 0;
   792|     size_t length = sizeof(INT64);
   793|     int rc = sysctl(mib, 2, &physical_memory, &length, NULL, 0);
   794|     assert(rc == 0);
   795|     return physical_memory;
   796| #else // HAVE_SYSCTL
   797| #error "Don't know how to get total physical memory on this platform"
   798| #endif // HAVE_SYSCTL
   799| }
   800| uint64_t GetAvailablePhysicalMemory()
   801| {
   802|     uint64_t available = 0;
   803| #ifndef __APPLE__
   804|     static volatile bool tryReadMemInfo = true;
   805|     if (tryReadMemInfo)
   806|     {
   807|         tryReadMemInfo = ReadMemAvailable(&available);
   808|     }
   809|     if (!tryReadMemInfo)
   810|     {
   811|         available = sysconf(SYSCONF_PAGES) * sysconf(_SC_PAGE_SIZE);
   812|     }
   813| #else // __APPLE__
   814|     vm_size_t page_size;
   815|     mach_port_t mach_port;
   816|     mach_msg_type_number_t count;
   817|     vm_statistics_data_t vm_stats;
   818|     mach_port = mach_host_self();
   819|     count = sizeof(vm_stats) / sizeof(natural_t);
   820|     if (KERN_SUCCESS == host_page_size(mach_port, &page_size))
   821|     {
   822|         if (KERN_SUCCESS == host_statistics(mach_port, HOST_VM_INFO, (host_info_t)&vm_stats, &count))
   823|         {
   824|             available = (int64_t)vm_stats.free_count * (int64_t)page_size;
   825|         }
   826|     }
   827|     mach_port_deallocate(mach_task_self(), mach_port);
   828| #endif // __APPLE__
   829|     return available;
   830| }
   831| uint64_t GetAvailablePageFile()
   832| {
   833|     uint64_t available = 0;
   834|     int mib[3];
   835|     int rc;
   836| #if HAVE_XSW_USAGE
   837|     struct xsw_usage xsu;
   838|     mib[0] = CTL_VM;
   839|     mib[1] = VM_SWAPUSAGE;
   840|     size_t length = sizeof(xsu);
   841|     rc = sysctl(mib, 2, &xsu, &length, NULL, 0);
   842|     if (rc == 0)
   843|     {
   844|         available = xsu.xsu_avail;
   845|     }
   846| #elif HAVE_XSWDEV
   847|     struct xswdev xsw;
   848|     size_t length = 2;
   849|     rc = sysctlnametomib("vm.swap_info", mib, &length);
   850|     if (rc == 0)
   851|     {
   852|         int pagesize = getpagesize();
   853|         for (mib[2] = 0; ; mib[2]++)
   854|         {
   855|             length = sizeof(xsw);
   856|             rc = sysctl(mib, 3, &xsw, &length, NULL, 0);
   857|             if ((rc < 0) || (xsw.xsw_version != XSWDEV_VERSION))
   858|             {
   859|                 break;
   860|             }
   861|             uint64_t avail = xsw.xsw_nblks - xsw.xsw_used;
   862|             available += avail * pagesize;
   863|         }
   864|     }
   865| #elif HAVE_SWAPCTL
   866|     struct anoninfo ai;
   867|     if (swapctl(SC_AINFO, &ai) != -1)
   868|     {
   869|         int pagesize = getpagesize();
   870|         available = ai.ani_free * pagesize;
   871|     }
   872| #elif HAVE_SYSINFO
   873|     struct sysinfo info;
   874|     rc = sysinfo(&info);
   875|     if (rc == 0)
   876|     {
   877|         available = info.freeswap;
   878| #if HAVE_SYSINFO_WITH_MEM_UNIT
   879|         available *= info.mem_unit;
   880| #endif // HAVE_SYSINFO_WITH_MEM_UNIT
   881|     }
   882| #endif // HAVE_SYSINFO
   883|     return available;
   884| }
   885| void GCToOSInterface::GetMemoryStatus(uint64_t restricted_limit, uint32_t* memory_load, uint64_t* available_physical, uint64_t* available_page_file)
   886| {
   887|     uint64_t available = 0;
   888|     uint32_t load = 0;
   889|     if (memory_load != nullptr || available_physical != nullptr)
   890|     {
   891|         size_t used;
   892|         if (restricted_limit != 0)
   893|         {
   894|             if (GetPhysicalMemoryUsed(&used))
   895|             {
   896|                 available = restricted_limit > used ? restricted_limit - used : 0;
   897|                 load = (uint32_t)(((float)used * 100) / (float)restricted_limit);
   898|             }
   899|         }
   900|         else
   901|         {
   902|             available = GetAvailablePhysicalMemory();
   903|             if (memory_load != NULL)
   904|             {
   905|                 bool isRestricted;
   906|                 uint64_t total = GetPhysicalMemoryLimit(&isRestricted);
   907|                 if (total > available)
   908|                 {
   909|                     used = total - available;
   910|                     load = (uint32_t)(((float)used * 100) / (float)total);
   911|                 }
   912|             }
   913|         }
   914|     }
   915|     if (available_physical != NULL)
   916|         *available_physical = available;
   917|     if (memory_load != nullptr)
   918|         *memory_load = load;
   919|     if (available_page_file != nullptr)
   920|         *available_page_file = GetAvailablePageFile();
   921| }
   922| int64_t GCToOSInterface::QueryPerformanceCounter()
   923| {
   924| #if HAVE_CLOCK_GETTIME_NSEC_NP
   925|     return (int64_t)clock_gettime_nsec_np(CLOCK_UPTIME_RAW);
   926| #elif HAVE_CLOCK_MONOTONIC
   927|     struct timespec ts;
   928|     int result = clock_gettime(CLOCK_MONOTONIC, &ts);
   929|     if (result != 0)
   930|     {
   931|         assert(!"clock_gettime(CLOCK_MONOTONIC) failed");
   932|         __UNREACHABLE();
   933|     }
   934|     return ((int64_t)(ts.tv_sec) * (int64_t)(tccSecondsToNanoSeconds)) + (int64_t)(ts.tv_nsec);
   935| #else
   936| #error " clock_gettime(CLOCK_MONOTONIC) or clock_gettime_nsec_np() must be supported."
   937| #endif
   938| }
   939| int64_t GCToOSInterface::QueryPerformanceFrequency()
   940| {
   941|     return tccSecondsToNanoSeconds;
   942| }
   943| uint64_t GCToOSInterface::GetLowPrecisionTimeStamp()
   944| {
   945|     uint64_t retval = 0;
   946| #if HAVE_CLOCK_GETTIME_NSEC_NP
   947|     retval = clock_gettime_nsec_np(CLOCK_UPTIME_RAW) / tccMilliSecondsToNanoSeconds;
   948| #elif HAVE_CLOCK_MONOTONIC
   949|     struct timespec ts;
   950| #if HAVE_CLOCK_MONOTONIC_COARSE
   951|     clockid_t clockType = CLOCK_MONOTONIC_COARSE; // good enough resolution, fastest speed
   952| #else
   953|     clockid_t clockType = CLOCK_MONOTONIC;
   954| #endif
   955|     if (clock_gettime(clockType, &ts) != 0)
   956|     {
   957| #if HAVE_CLOCK_MONOTONIC_COARSE
   958|         assert(!"clock_gettime(HAVE_CLOCK_MONOTONIC_COARSE) failed\n");
   959| #else
   960|         assert(!"clock_gettime(CLOCK_MONOTONIC) failed\n");
   961| #endif
   962|     }
   963|     retval = (ts.tv_sec * tccSecondsToMilliSeconds) + (ts.tv_nsec / tccMilliSecondsToNanoSeconds);
   964| #else
   965|     struct timeval tv;
   966|     if (gettimeofday(&tv, NULL) == 0)
   967|     {
   968|         retval = (tv.tv_sec * tccSecondsToMilliSeconds) + (tv.tv_usec / tccMilliSecondsToMicroSeconds);
   969|     }
   970|     else
   971|     {
   972|         assert(!"gettimeofday() failed\n");
   973|     }
   974| #endif
   975|     return retval;
   976| }
   977| uint32_t GCToOSInterface::GetTotalProcessorCount()
   978| {
   979|     return g_totalCpuCount;
   980| }
   981| bool GCToOSInterface::CanEnableGCNumaAware()
   982| {
   983|     return g_numaAvailable;
   984| }
   985| bool GCToOSInterface::CanEnableGCCPUGroups()
   986| {
   987|     return false;
   988| }
   989| bool GCToOSInterface::GetProcessorForHeap(uint16_t heap_number, uint16_t* proc_no, uint16_t* node_no)
   990| {
   991|     bool success = false;
   992|     uint16_t availableProcNumber = 0;
   993|     for (size_t procNumber = 0; procNumber < MAX_SUPPORTED_CPUS; procNumber++)
   994|     {
   995|         if (g_processAffinitySet.Contains(procNumber))
   996|         {
   997|             if (availableProcNumber == heap_number)
   998|             {
   999|                 *proc_no = procNumber;
  1000| #ifdef TARGET_LINUX
  1001|                 if (GCToOSInterface::CanEnableGCNumaAware())
  1002|                 {
  1003|                     int result = GetNumaNodeNumByCpu(procNumber);
  1004|                     *node_no = (result >= 0) ? (uint16_t)result : NUMA_NODE_UNDEFINED;
  1005|                 }
  1006|                 else
  1007| #endif // TARGET_LINUX
  1008|                 {
  1009|                     *node_no = NUMA_NODE_UNDEFINED;
  1010|                 }
  1011|                 success = true;
  1012|                 break;
  1013|             }
  1014|             availableProcNumber++;
  1015|         }
  1016|     }
  1017|     return success;
  1018| }
  1019| bool GCToOSInterface::ParseGCHeapAffinitizeRangesEntry(const char** config_string, size_t* start_index, size_t* end_index)
  1020| {
  1021|     return ParseIndexOrRange(config_string, start_index, end_index);
  1022| }
  1023| bool CLRCriticalSection::Initialize()
  1024| {
  1025|     pthread_mutexattr_t mutexAttributes;
  1026|     int st = pthread_mutexattr_init(&mutexAttributes);
  1027|     if (st != 0)
  1028|     {
  1029|         return false;
  1030|     }
  1031|     st = pthread_mutexattr_settype(&mutexAttributes, PTHREAD_MUTEX_RECURSIVE);
  1032|     if (st == 0)
  1033|     {
  1034|         st = pthread_mutex_init(&m_cs.mutex, &mutexAttributes);
  1035|     }
  1036|     pthread_mutexattr_destroy(&mutexAttributes);
  1037|     return (st == 0);
  1038| }
  1039| void CLRCriticalSection::Destroy()
  1040| {
  1041|     int st = pthread_mutex_destroy(&m_cs.mutex);
  1042|     assert(st == 0);
  1043| }
  1044| void CLRCriticalSection::Enter()
  1045| {
  1046|     pthread_mutex_lock(&m_cs.mutex);
  1047| }
  1048| void CLRCriticalSection::Leave()
  1049| {
  1050|     pthread_mutex_unlock(&m_cs.mutex);
  1051| }


# ====================================================================
# FILE: src/coreclr/utilcode/collections.cpp
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-623 ---
     1| #include "stdafx.h"
     2| #include "utilcode.h"
     3| #include "ex.h"
     4| #ifndef DACCESS_COMPILE
     5| HRESULT CHashTable::NewInit(            // Return status.
     6|     BYTE        *pcEntries,             // Array of structs we are managing.
     7|     ULONG      iEntrySize)             // Size of the entries.
     8| {
     9|     CONTRACTL
    10|     {
    11|         NOTHROW;
    12|     }
    13|     CONTRACTL_END;
    14|     _ASSERTE(iEntrySize >= sizeof(FREEHASHENTRY));
    15|     if ((m_piBuckets = new (nothrow) ULONG [m_iBuckets]) == NULL)
    16|         return (OutOfMemory());
    17|     memset(m_piBuckets, 0xff, m_iBuckets * sizeof(ULONG));
    18|     m_pcEntries = (TADDR)pcEntries;
    19|     m_iEntrySize = iEntrySize;
    20|     return (S_OK);
    21| }
    22| BYTE *CHashTable::Add(                  // New entry.
    23|     ULONG      iHash,                  // Hash value of entry to add.
    24|     ULONG      iIndex)                 // Index of struct in m_pcEntries.
    25| {
    26|     CONTRACTL
    27|     {
    28|         NOTHROW;
    29|     }
    30|     CONTRACTL_END;
    31|     HASHENTRY   *psEntry;               // The struct we are adding.
    32|     psEntry = EntryPtr(iIndex);
    33|     iHash %= m_iBuckets;
    34|     _ASSERTE(m_piBuckets[iHash] != iIndex &&
    35|         (m_piBuckets[iHash] == UINT32_MAX || EntryPtr(m_piBuckets[iHash])->iPrev != iIndex));
    36|     psEntry->iPrev = UINT32_MAX;
    37|     psEntry->iNext = m_piBuckets[iHash];
    38|     if (m_piBuckets[iHash] != UINT32_MAX)
    39|         EntryPtr(m_piBuckets[iHash])->iPrev = iIndex;
    40|     m_piBuckets[iHash] = iIndex;
    41|     return ((BYTE *) psEntry);
    42| }
    43| void CHashTable::Delete(
    44|     ULONG      iHash,                  // Hash value of entry to delete.
    45|     ULONG      iIndex)                 // Index of struct in m_pcEntries.
    46| {
    47|     WRAPPER_NO_CONTRACT;
    48|     HASHENTRY   *psEntry;               // Struct to delete.
    49|     psEntry = EntryPtr(iIndex);
    50|     Delete(iHash, psEntry);
    51| }
    52| void CHashTable::Delete(
    53|     ULONG      iHash,                  // Hash value of entry to delete.
    54|     HASHENTRY   *psEntry)               // The struct to delete.
    55| {
    56|     CONTRACTL
    57|     {
    58|         NOTHROW;
    59|     }
    60|     CONTRACTL_END;
    61|     iHash %= m_iBuckets;
    62|     _ASSERTE(psEntry->iPrev != psEntry->iNext || psEntry->iPrev == UINT32_MAX);
    63|     if (psEntry->iPrev == UINT32_MAX)
    64|         m_piBuckets[iHash] = psEntry->iNext;
    65|     else
    66|         EntryPtr(psEntry->iPrev)->iNext = psEntry->iNext;
    67|     if (psEntry->iNext != UINT32_MAX)
    68|         EntryPtr(psEntry->iNext)->iPrev = psEntry->iPrev;
    69| }
    70| void CHashTable::Move(
    71|     ULONG      iHash,                  // Hash value for the item.
    72|     ULONG      iNew)                   // New location.
    73| {
    74|     CONTRACTL
    75|     {
    76|         NOTHROW;
    77|     }
    78|     CONTRACTL_END;
    79|     HASHENTRY   *psEntry;               // The struct we are deleting.
    80|     psEntry = EntryPtr(iNew);
    81|     _ASSERTE(psEntry->iPrev != iNew && psEntry->iNext != iNew);
    82|     if (psEntry->iPrev != UINT32_MAX)
    83|         EntryPtr(psEntry->iPrev)->iNext = iNew;
    84|     else
    85|         m_piBuckets[iHash % m_iBuckets] = iNew;
    86|     if (psEntry->iNext != UINT32_MAX)
    87|         EntryPtr(psEntry->iNext)->iPrev = iNew;
    88| }
    89| #endif // !DACCESS_COMPILE
    90| BYTE *CHashTable::Find(                 // Index of struct in m_pcEntries.
    91|     ULONG      iHash,                  // Hash value of the item.
    92|     SIZE_T     key)                    // The key to match.
    93| {
    94|     CONTRACTL
    95|     {
    96|         NOTHROW;
    97|         GC_NOTRIGGER;
    98|         SUPPORTS_DAC;
    99|     }
   100|     CONTRACTL_END;
   101|     ULONG      iNext;                  // Used to traverse the chains.
   102|     HASHENTRY   *psEntry;               // Used to traverse the chains.
   103|     iNext = m_piBuckets[iHash % m_iBuckets];
   104| #ifdef _DEBUG
   105|     unsigned count = 0;
   106| #endif
   107|     while (iNext != UINT32_MAX)
   108|     {
   109|         psEntry = EntryPtr(iNext);
   110| #ifdef _DEBUG
   111|         count++;
   112| #endif
   113|         if (!Cmp(key, psEntry))
   114|         {
   115| #ifdef _DEBUG
   116|             if (count > m_maxSearch)
   117|                 m_maxSearch = count;
   118| #endif
   119|             return ((BYTE *) psEntry);
   120|         }
   121|         iNext = psEntry->iNext;
   122|     }
   123|     return (0);
   124| }
   125| ULONG CHashTable::FindNext(            // Index of struct in m_pcEntries.
   126|     SIZE_T     key,                    // The key to match.
   127|     ULONG      iIndex)                 // Index of previous match.
   128| {
   129|     CONTRACTL
   130|     {
   131|         NOTHROW;
   132|     }
   133|     CONTRACTL_END;
   134|     ULONG      iNext;                  // Used to traverse the chains.
   135|     HASHENTRY   *psEntry;               // Used to traverse the chains.
   136|     iNext = EntryPtr(iIndex)->iNext;
   137|     while (iNext != UINT32_MAX)
   138|     {
   139|         psEntry = EntryPtr(iNext);
   140|         if (!Cmp(key, psEntry))
   141|             return (iNext);
   142|         iNext = psEntry->iNext;
   143|     }
   144|     return (UINT32_MAX);
   145| }
   146| BYTE *CHashTable::FindNextEntry(        // The next entry, or0 for end of list.
   147|     HASHFIND    *psSrch)                // Search object.
   148| {
   149|     CONTRACTL
   150|     {
   151|         NOTHROW;
   152|         GC_NOTRIGGER;
   153|         SUPPORTS_DAC;
   154|     }
   155|     CONTRACTL_END;
   156|     HASHENTRY   *psEntry;               // Used to traverse the chains.
   157|     for (;;)
   158|     {
   159|         if (psSrch->iNext != UINT32_MAX)
   160|         {
   161|             psEntry = EntryPtr(psSrch->iNext);
   162| #if DACCESS_COMPILE
   163|             if (psEntry->iNext == psSrch->iNext)
   164|                 return NULL;
   165| #endif
   166|             psSrch->iNext = psEntry->iNext;
   167|             return ((BYTE *) psEntry);
   168|         }
   169|         if (psSrch->iBucket < m_iBuckets)
   170|             psSrch->iNext = m_piBuckets[psSrch->iBucket++];
   171|         else
   172|             break;
   173|     }
   174|     return (0);
   175| }
   176| #ifdef DACCESS_COMPILE
   177| void
   178| CHashTable::EnumMemoryRegions(CLRDataEnumMemoryFlags flags,
   179|                               ULONG numEntries)
   180| {
   181|     SUPPORTS_DAC;
   182|     DacEnumMemoryRegion(m_pcEntries,
   183|                         (ULONG)numEntries * m_iEntrySize);
   184|     DacEnumMemoryRegion(dac_cast<TADDR>(m_piBuckets),
   185|                         (ULONG)m_iBuckets * sizeof(ULONG));
   186| }
   187| #endif // #ifdef DACCESS_COMPILE
   188| void CClosedHashBase::Delete(
   189|     void        *pData)                 // Key value to delete.
   190| {
   191|     CONTRACTL
   192|     {
   193|         NOTHROW;
   194|     }
   195|     CONTRACTL_END;
   196|     BYTE        *ptr;
   197|     if ((ptr = Find(pData)) == 0)
   198|     {
   199|         _ASSERTE(0);
   200|         return;
   201|     }
   202|     if (m_bPerfect)
   203|     {
   204|         SetStatus(ptr, FREE);
   205|         --m_iCount;
   206|         return;
   207|     }
   208|     SetStatus(ptr, DELETED);
   209|     BYTE        *pnext;
   210|     if ((pnext = ptr + m_iEntrySize) > EntryPtr(m_iSize - 1))
   211|         pnext = &m_rgData[0];
   212|     if (Status(pnext) != FREE)
   213|         return;
   214|     while (Status(ptr) == DELETED)
   215|     {
   216|         SetStatus(ptr, FREE);
   217|         --m_iCount;
   218|         if ((ptr -= m_iEntrySize) < &m_rgData[0])
   219|             ptr = EntryPtr(m_iSize - 1);
   220|     }
   221| }
   222| void CClosedHashBase::DeleteLoop(
   223|     DELETELOOPFUNC pDeleteLoopFunc,     // Decides whether to delete item
   224|     void *pCustomizer)                  // Extra value passed to deletefunc.
   225| {
   226|     CONTRACTL
   227|     {
   228|         NOTHROW;
   229|     }
   230|     CONTRACTL_END;
   231|     int i;
   232|     if (m_rgData == 0)
   233|     {
   234|         return;
   235|     }
   236|     for (i = 0; i < m_iSize; i++)
   237|     {
   238|         BYTE *pEntry = EntryPtr(i);
   239|         if (Status(pEntry) == USED)
   240|         {
   241|             if (pDeleteLoopFunc(pEntry, pCustomizer))
   242|             {
   243|                 if (m_bPerfect)
   244|                 {
   245|                     SetStatus(pEntry, FREE);
   246|                     --m_iCount;
   247|                 }
   248|                 else
   249|                 {
   250|                     SetStatus(pEntry, DELETED);
   251|                 }
   252|             }
   253|         }
   254|     }
   255|     if (!m_bPerfect)
   256|     {
   257|         for (i = 0; i < m_iSize; i++)
   258|         {
   259|             if (Status(EntryPtr(i)) == FREE)
   260|             {
   261|                 break;
   262|             }
   263|         }
   264|         if (i != m_iSize)
   265|         {
   266|             int iFirstFree = i;
   267|             do
   268|             {
   269|                 if (i-- == 0)
   270|                 {
   271|                     i = m_iSize - 1;
   272|                 }
   273|                 while (Status(EntryPtr(i)) == DELETED)
   274|                 {
   275|                     SetStatus(EntryPtr(i), FREE);
   276|                     --m_iCount;
   277|                     if (i-- == 0)
   278|                     {
   279|                         i = m_iSize - 1;
   280|                     }
   281|                 }
   282|                 while (Status(EntryPtr(i)) != FREE)
   283|                 {
   284|                     if (i-- == 0)
   285|                     {
   286|                         i = m_iSize - 1;
   287|                     }
   288|                 }
   289|             }
   290|             while (i != iFirstFree);
   291|         }
   292|     }
   293| }
   294| BYTE *CClosedHashBase::Find(            // The item if found, 0 if not.
   295|     void        *pData)                 // The key to lookup.
   296| {
   297|     CONTRACTL
   298|     {
   299|         NOTHROW;
   300|     }
   301|     CONTRACTL_END;
   302|     unsigned int iHash;                // Hash value for this data.
   303|     int         iBucket;                // Which bucke to start at.
   304|     int         i;                      // Loop control.
   305|     if (!m_rgData || m_iCount == 0)
   306|         return (0);
   307|     iHash = Hash(pData);
   308|     iBucket = iHash % m_iBuckets;
   309|     if (m_bPerfect)
   310|     {
   311|         if (Status(EntryPtr(iBucket)) != FREE)
   312|             return (EntryPtr(iBucket));
   313|         return (0);
   314|     }
   315|     for (i=iBucket;  Status(EntryPtr(i)) != FREE;  )
   316|     {
   317|         if (Status(EntryPtr(i)) == DELETED)
   318|         {
   319|             if (++i >= m_iSize)
   320|                 i = 0;
   321|             continue;
   322|         }
   323|         if (Compare(pData, EntryPtr(i)) == 0)
   324|             return (EntryPtr(i));
   325|         if (!m_iCollisions)
   326|             return (0);
   327|         if (++i >= m_iSize)
   328|             i = 0;
   329|     }
   330|     return (0);
   331| }
   332| BYTE *CClosedHashBase::FindOrAdd(       // The item if found, 0 if not.
   333|     void        *pData,                 // The key to lookup.
   334|     bool        &bNew)                  // true if created.
   335| {
   336|     CONTRACTL
   337|     {
   338|         NOTHROW;
   339|     }
   340|     CONTRACTL_END;
   341|     unsigned int iHash;                // Hash value for this data.
   342|     int         iBucket;                // Which bucke to start at.
   343|     int         i;                      // Loop control.
   344|     if (!m_rgData || ((m_iCount + 1) > (m_iSize * 3 / 4) && !m_bPerfect))
   345|     {
   346|         if (!ReHash())
   347|             return (0);
   348|     }
   349|     bNew = false;
   350|     iHash = Hash(pData);
   351|     iBucket = iHash % m_iBuckets;
   352|     if (m_bPerfect)
   353|     {
   354|         if (Status(EntryPtr(iBucket)) != FREE)
   355|             return (EntryPtr(iBucket));
   356|         i = iBucket;
   357|     }
   358|     else
   359|     {
   360|         for (i=iBucket;  Status(EntryPtr(i)) != FREE;  )
   361|         {
   362|             if (Status(EntryPtr(i)) == DELETED)
   363|             {
   364|                 if (++i >= m_iSize)
   365|                     i = 0;
   366|                 continue;
   367|             }
   368|             if (Compare(pData, EntryPtr(i)) == 0)
   369|                 return (EntryPtr(i));
   370|             ++m_iCollisions;
   371|             if (++i >= m_iSize)
   372|                 i = 0;
   373|         }
   374|     }
   375|     _ASSERTE(Status(EntryPtr(i)) == FREE);
   376|     bNew = true;
   377|     ++m_iCount;
   378|     return (EntryPtr(i));
   379| }
   380| BYTE *CClosedHashBase::DoAdd(void *pData, BYTE *rgData, int &iBuckets, int iSize,
   381|             int &iCollisions, int &iCount)
   382| {
   383|     CONTRACTL
   384|     {
   385|         NOTHROW;
   386|     }
   387|     CONTRACTL_END;
   388|     unsigned int iHash;                // Hash value for this data.
   389|     int         iBucket;                // Which bucke to start at.
   390|     int         i;                      // Loop control.
   391|     iHash = Hash(pData);
   392|     iBucket = iHash % iBuckets;
   393|     if (m_bPerfect)
   394|     {
   395|         i = iBucket;
   396|         _ASSERTE(Status(EntryPtr(i, rgData)) == FREE);
   397|     }
   398|     else
   399|     {
   400|         for (i=iBucket;  Status(EntryPtr(i, rgData)) != FREE;  )
   401|         {
   402|             if (++i >= iSize)
   403|                 i = 0;
   404|             ++iCollisions;
   405|         }
   406|     }
   407|     ++iCount;
   408|     return (EntryPtr(i, rgData));
   409| }
   410| bool CClosedHashBase::ReHash()          // true if successful.
   411| {
   412|     CONTRACTL
   413|     {
   414|         NOTHROW;
   415|     }
   416|     CONTRACTL_END;
   417|     if (!m_rgData)
   418|     {
   419|         if ((m_rgData = new (nothrow) BYTE [m_iSize * m_iEntrySize]) == 0)
   420|             return (false);
   421|         InitFree(&m_rgData[0], m_iSize);
   422|         return (true);
   423|     }
   424|     BYTE        *rgTemp, *p;
   425|     int         iBuckets = m_iBuckets * 2 - 1;
   426|     int         iSize = iBuckets + 7;
   427|     int         iCollisions = 0;
   428|     int         iCount = 0;
   429|     if ((rgTemp = new (nothrow) BYTE [iSize * m_iEntrySize]) == 0)
   430|         return (false);
   431|     InitFree(&rgTemp[0], iSize);
   432|     m_bPerfect = false;
   433|     for (int i=0;  i<m_iSize;  i++)
   434|     {
   435|         if (Status(EntryPtr(i)) != USED)
   436|             continue;
   437|         VERIFY((p = DoAdd(GetKey(EntryPtr(i)), rgTemp, iBuckets,
   438|                 iSize, iCollisions, iCount)));
   439|         memmove(p, EntryPtr(i), m_iEntrySize);
   440|     }
   441|     delete [] m_rgData;
   442|     m_rgData = rgTemp;
   443|     m_iBuckets = iBuckets;
   444|     m_iSize = iSize;
   445|     m_iCollisions = iCollisions;
   446|     m_iCount = iCount;
   447|     return (true);
   448| }
   449| void *CStructArray::InsertThrowing(
   450|     int         iIndex)
   451| {
   452|     CONTRACTL
   453|     {
   454|         THROWS;
   455|     }
   456|     CONTRACTL_END;
   457|     _ASSERTE(iIndex >= 0);
   458|     if (iIndex > m_iCount)
   459|         return (NULL);
   460|     Grow(1);
   461|     BYTE *pcList = m_pList + iIndex * m_iElemSize;
   462|     if (iIndex < m_iCount)
   463|         memmove(pcList + m_iElemSize, pcList, (m_iCount - iIndex) * m_iElemSize);
   464|     ++m_iCount;
   465|     return(pcList);
   466| }
   467| void *CStructArray::Insert(int iIndex)
   468| {
   469|     CONTRACTL
   470|     {
   471|         NOTHROW;
   472|     }
   473|     CONTRACTL_END;
   474|     void *result = NULL;
   475|     EX_TRY
   476|     {
   477|         result = InsertThrowing(iIndex);
   478|     }
   479|     EX_CATCH
   480|     {
   481|     }
   482|     EX_END_CATCH(SwallowAllExceptions);
   483|     return result;
   484| }
   485| void *CStructArray::AppendThrowing()
   486| {
   487|     CONTRACTL
   488|     {
   489|         THROWS;
   490|     }
   491|     CONTRACTL_END;
   492|     Grow(1);
   493|     return (m_pList + m_iCount++ * m_iElemSize);
   494| }
   495| void *CStructArray::Append()
   496| {
   497|     CONTRACTL
   498|     {
   499|         NOTHROW;
   500|     }
   501|     CONTRACTL_END;
   502|     void *result = NULL;
   503|     EX_TRY
   504|     {
   505|         result = AppendThrowing();
   506|     }
   507|     EX_CATCH
   508|     {
   509|     }
   510|     EX_END_CATCH(SwallowAllExceptions);
   511|     return result;
   512| }
   513| void CStructArray::AllocateBlockThrowing(int iCount)
   514| {
   515|     CONTRACTL
   516|     {
   517|         THROWS;
   518|     }
   519|     CONTRACTL_END;
   520|     if (m_iSize < m_iCount+iCount)
   521|         Grow(iCount);
   522|     m_iCount += iCount;
   523| }
   524| int CStructArray::AllocateBlock(int iCount)
   525| {
   526|     CONTRACTL
   527|     {
   528|         NOTHROW;
   529|     }
   530|     CONTRACTL_END;
   531|     int result = FALSE;
   532|     EX_TRY
   533|     {
   534|         AllocateBlockThrowing(iCount);
   535|         result = TRUE;
   536|     }
   537|     EX_CATCH
   538|     {
   539|     }
   540|     EX_END_CATCH(SwallowAllExceptions);
   541|     return result;
   542| }
   543| void CStructArray::Delete(
   544|     int         iIndex)
   545| {
   546|     CONTRACTL
   547|     {
   548|         NOTHROW;
   549|     }
   550|     CONTRACTL_END;
   551|     _ASSERTE(iIndex >= 0);
   552|     if (iIndex < --m_iCount)
   553|     {
   554|         BYTE *pcList = m_pList + iIndex * m_iElemSize;
   555|         memmove(pcList, pcList + m_iElemSize, (m_iCount - iIndex) * m_iElemSize);
   556|     }
   557| }
   558| void CStructArray::Grow(
   559|     int         iCount)
   560| {
   561|     CONTRACTL {
   562|         THROWS;
   563|     } CONTRACTL_END;
   564|     BYTE        *pTemp;                 // temporary pointer used in realloc.
   565|     int         iGrow;
   566|     if (m_iSize < m_iCount+iCount)
   567|     {
   568|         if (m_pList == NULL)
   569|         {
   570|             iGrow = max(m_iGrowInc, iCount);
   571|             S_SIZE_T newSize = S_SIZE_T(iGrow) * S_SIZE_T(m_iElemSize);
   572|             if(newSize.IsOverflow())
   573|                 ThrowOutOfMemory();
   574|             else
   575|             {
   576|                 m_pList = new BYTE[newSize.Value()];
   577|                 m_iSize = iGrow;
   578|                 m_bFree = true;
   579|             }
   580|         }
   581|         else
   582|         {
   583|             if (m_iSize / m_iGrowInc >= 3)
   584|             {   // Don't overflow and go negative.
   585|                 int newinc = m_iGrowInc * 2;
   586|                 if (newinc > m_iGrowInc)
   587|                     m_iGrowInc = newinc;
   588|             }
   589|             iGrow = max(m_iGrowInc, iCount);
   590|             S_SIZE_T allocSize = (S_SIZE_T(m_iSize) + S_SIZE_T(iGrow)) * S_SIZE_T(m_iElemSize);
   591|             S_SIZE_T copyBytes = S_SIZE_T(m_iSize) * S_SIZE_T(m_iElemSize);
   592|             if(allocSize.IsOverflow() || copyBytes.IsOverflow())
   593|                 ThrowOutOfMemory();
   594|             if (m_bFree)
   595|             {   // We already own memory.
   596|                 pTemp = new BYTE[allocSize.Value()];
   597|                 memcpy (pTemp, m_pList, copyBytes.Value());
   598|                 delete [] m_pList;
   599|             }
   600|             else
   601|             {   // We don't own memory; get our own.
   602|                 pTemp = new BYTE[allocSize.Value()];
   603|                 memcpy(pTemp, m_pList, copyBytes.Value());
   604|                 m_bFree = true;
   605|             }
   606|             m_pList = pTemp;
   607|             m_iSize += iGrow;
   608|         }
   609|     }
   610| }
   611| void CStructArray::Clear()
   612| {
   613|     CONTRACTL
   614|     {
   615|         NOTHROW;
   616|     }
   617|     CONTRACTL_END;
   618|     if (m_bFree && m_pList != NULL)
   619|         delete [] m_pList;
   620|     m_pList = NULL;
   621|     m_iSize = 0;
   622|     m_iCount = 0;
   623| }


# ====================================================================
# FILE: src/libraries/Common/src/Microsoft/Win32/SafeHandles/SafeCertContextHandleWithKeyContainerDeletion.cs
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-65 ---
     1| using System;
     2| using System.Diagnostics;
     3| using System.Runtime.InteropServices;
     4| using System.Security.Cryptography;
     5| namespace Microsoft.Win32.SafeHandles
     6| {
     7|     internal sealed class SafeCertContextHandleWithKeyContainerDeletion : SafeCertContextHandle
     8|     {
     9|         protected sealed override bool ReleaseHandle()
    10|         {
    11|             using (SafeCertContextHandle certContext = Interop.Crypt32.CertDuplicateCertificateContext(handle))
    12|             {
    13|                 DeleteKeyContainer(certContext);
    14|             }
    15|             base.ReleaseHandle();
    16|             return true;
    17|         }
    18|         internal static void DeleteKeyContainer(SafeCertContextHandle pCertContext)
    19|         {
    20|             if (pCertContext.IsInvalid)
    21|                 return;
    22|             int cb = 0;
    23|             bool containsPrivateKey = Interop.Crypt32.CertGetCertificateContextProperty(pCertContext, Interop.Crypt32.CertContextPropId.CERT_KEY_PROV_INFO_PROP_ID, null, ref cb);
    24|             if (!containsPrivateKey)
    25|                 return;
    26|             byte[] provInfoAsBytes = new byte[cb];
    27|             if (!Interop.Crypt32.CertGetCertificateContextProperty(pCertContext, Interop.Crypt32.CertContextPropId.CERT_KEY_PROV_INFO_PROP_ID, provInfoAsBytes, ref cb))
    28|                 return;
    29|             unsafe
    30|             {
    31|                 fixed (byte* pProvInfoAsBytes = provInfoAsBytes)
    32|                 {
    33|                     Interop.Crypt32.CRYPT_KEY_PROV_INFO* pProvInfo = (Interop.Crypt32.CRYPT_KEY_PROV_INFO*)pProvInfoAsBytes;
    34|                     if (pProvInfo->dwProvType == 0)
    35|                     {
    36|                         string providerName = Marshal.PtrToStringUni((IntPtr)(pProvInfo->pwszProvName))!;
    37|                         string keyContainerName = Marshal.PtrToStringUni((IntPtr)(pProvInfo->pwszContainerName))!;
    38|                         CngKeyOpenOptions openOpts = CngKeyOpenOptions.None;
    39|                         if ((pProvInfo->dwFlags & Interop.Crypt32.CryptAcquireContextFlags.CRYPT_MACHINE_KEYSET) != 0)
    40|                         {
    41|                             openOpts = CngKeyOpenOptions.MachineKey;
    42|                         }
    43|                         try
    44|                         {
    45|                             using (CngKey cngKey = CngKey.Open(keyContainerName, new CngProvider(providerName), openOpts))
    46|                             {
    47|                                 cngKey.Delete();
    48|                             }
    49|                         }
    50|                         catch (CryptographicException)
    51|                         {
    52|                         }
    53|                     }
    54|                     else
    55|                     {
    56|                         Interop.Crypt32.CryptAcquireContextFlags flags = (pProvInfo->dwFlags & Interop.Crypt32.CryptAcquireContextFlags.CRYPT_MACHINE_KEYSET) | Interop.Crypt32.CryptAcquireContextFlags.CRYPT_DELETEKEYSET;
    57|                         IntPtr hProv;
    58|                         _ = Interop.Advapi32.CryptAcquireContext(out hProv, pProvInfo->pwszContainerName, pProvInfo->pwszProvName, pProvInfo->dwProvType, flags);
    59|                         Debug.Assert(hProv == IntPtr.Zero);
    60|                     }
    61|                 }
    62|             }
    63|         }
    64|     }
    65| }


# ====================================================================
# FILE: src/libraries/System.Net.Mail/src/System/Net/Mail/SmtpNegotiateAuthenticationModule.cs
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-104 ---
     1| using System.Buffers;
     2| using System.Collections.Generic;
     3| using System.ComponentModel;
     4| using System.Net.Security;
     5| using System.Security.Authentication.ExtendedProtection;
     6| namespace System.Net.Mail
     7| {
     8|     internal sealed class SmtpNegotiateAuthenticationModule : ISmtpAuthenticationModule
     9|     {
    10|         private static readonly byte[] s_saslNoSecurtyLayerToken = new byte[] { 1, 0, 0, 0 };
    11|         private readonly Dictionary<object, NegotiateAuthentication> _sessions = new Dictionary<object, NegotiateAuthentication>();
    12|         internal SmtpNegotiateAuthenticationModule()
    13|         {
    14|         }
    15|         public Authorization? Authenticate(string? challenge, NetworkCredential? credential, object sessionCookie, string? spn, ChannelBinding? channelBindingToken)
    16|         {
    17|             lock (_sessions)
    18|             {
    19|                 NegotiateAuthentication? clientContext;
    20|                 if (!_sessions.TryGetValue(sessionCookie, out clientContext))
    21|                 {
    22|                     if (credential == null)
    23|                     {
    24|                         return null;
    25|                     }
    26|                     ProtectionLevel protectionLevel = ProtectionLevel.Sign;
    27|                     if (OperatingSystem.IsLinux())
    28|                     {
    29|                         protectionLevel = ProtectionLevel.EncryptAndSign;
    30|                     }
    31|                     _sessions[sessionCookie] = clientContext =
    32|                         new NegotiateAuthentication(
    33|                             new NegotiateAuthenticationClientOptions
    34|                             {
    35|                                 Credential = credential,
    36|                                 TargetName = spn,
    37|                                 RequiredProtectionLevel = protectionLevel,
    38|                                 Binding = channelBindingToken
    39|                             });
    40|                 }
    41|                 string? resp = null;
    42|                 NegotiateAuthenticationStatusCode statusCode;
    43|                 if (!clientContext.IsAuthenticated)
    44|                 {
    45|                     resp = clientContext.GetOutgoingBlob(challenge, out statusCode);
    46|                     if (statusCode != NegotiateAuthenticationStatusCode.Completed &&
    47|                         statusCode != NegotiateAuthenticationStatusCode.ContinueNeeded)
    48|                     {
    49|                         return null;
    50|                     }
    51|                 }
    52|                 else
    53|                 {
    54|                     resp = GetSecurityLayerOutgoingBlob(challenge, clientContext);
    55|                 }
    56|                 return new Authorization(resp, clientContext.IsAuthenticated);
    57|             }
    58|         }
    59|         public string AuthenticationType
    60|         {
    61|             get
    62|             {
    63|                 return "gssapi";
    64|             }
    65|         }
    66|         public void CloseContext(object sessionCookie)
    67|         {
    68|             NegotiateAuthentication? clientContext = null;
    69|             lock (_sessions)
    70|             {
    71|                 if (_sessions.TryGetValue(sessionCookie, out clientContext))
    72|                 {
    73|                     _sessions.Remove(sessionCookie);
    74|                 }
    75|             }
    76|             clientContext?.Dispose();
    77|         }
    78|         private static string? GetSecurityLayerOutgoingBlob(string? challenge, NegotiateAuthentication clientContext)
    79|         {
    80|             if (challenge == null)
    81|                 return null;
    82|             byte[] input = Convert.FromBase64String(challenge);
    83|             Span<byte> unwrappedChallenge;
    84|             NegotiateAuthenticationStatusCode statusCode;
    85|             statusCode = clientContext.UnwrapInPlace(input, out int newOffset, out int newLength, out _);
    86|             if (statusCode != NegotiateAuthenticationStatusCode.Completed)
    87|             {
    88|                 return null;
    89|             }
    90|             unwrappedChallenge = input.AsSpan(newOffset, newLength);
    91|             if (unwrappedChallenge.Length != 4 || (unwrappedChallenge[0] & 1) != 1)
    92|             {
    93|                 return null;
    94|             }
    95|             ArrayBufferWriter<byte> outputWriter = new ArrayBufferWriter<byte>();
    96|             statusCode = clientContext.Wrap(s_saslNoSecurtyLayerToken, outputWriter, false, out _);
    97|             if (statusCode != NegotiateAuthenticationStatusCode.Completed)
    98|             {
    99|                 return null;
   100|             }
   101|             return Convert.ToBase64String(outputWriter.WrittenSpan);
   102|         }
   103|     }
   104| }


# ====================================================================
# FILE: src/libraries/System.Net.Quic/src/System/Net/Quic/Internal/MsQuicApi.cs
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-213 ---
     1| using System.Diagnostics;
     2| using System.Diagnostics.CodeAnalysis;
     3| using System.Net.Sockets;
     4| using System.Runtime.InteropServices;
     5| using Microsoft.Quic;
     6| using static Microsoft.Quic.MsQuic;
     7| #if TARGET_WINDOWS
     8| using Microsoft.Win32;
     9| #endif
    10| namespace System.Net.Quic;
    11| internal sealed unsafe partial class MsQuicApi
    12| {
    13|     private static readonly Version s_minWindowsVersion = new Version(10, 0, 20145, 1000);
    14|     private static readonly Version s_minMsQuicVersion = new Version(2, 2, 2);
    15|     private static readonly delegate* unmanaged[Cdecl]<uint, QUIC_API_TABLE**, int> MsQuicOpenVersion;
    16|     private static readonly delegate* unmanaged[Cdecl]<QUIC_API_TABLE*, void> MsQuicClose;
    17|     public MsQuicSafeHandle Registration { get; }
    18|     public QUIC_API_TABLE* ApiTable { get; }
    19|     [DynamicDependency(DynamicallyAccessedMemberTypes.PublicConstructors, typeof(MsQuicSafeHandle))]
    20|     [DynamicDependency(DynamicallyAccessedMemberTypes.PublicConstructors, typeof(MsQuicContextSafeHandle))]
    21|     private MsQuicApi(QUIC_API_TABLE* apiTable)
    22|     {
    23|         ApiTable = apiTable;
    24|         fixed (byte* pAppName = "System.Net.Quic"u8)
    25|         {
    26|             var cfg = new QUIC_REGISTRATION_CONFIG
    27|             {
    28|                 AppName = (sbyte*)pAppName,
    29|                 ExecutionProfile = QUIC_EXECUTION_PROFILE.LOW_LATENCY
    30|             };
    31|             QUIC_HANDLE* handle;
    32|             ThrowHelper.ThrowIfMsQuicError(ApiTable->RegistrationOpen(&cfg, &handle), "RegistrationOpen failed");
    33|             Registration = new MsQuicSafeHandle(handle, apiTable->RegistrationClose, SafeHandleType.Registration);
    34|         }
    35|     }
    36|     private static readonly Lazy<MsQuicApi> _api = new Lazy<MsQuicApi>(AllocateMsQuicApi);
    37|     internal static MsQuicApi Api => _api.Value;
    38|     internal static bool IsQuicSupported { get; }
    39|     internal static string MsQuicLibraryVersion { get; } = "unknown";
    40|     internal static string? NotSupportedReason { get; }
    41|     internal static bool UsesSChannelBackend { get; }
    42|     internal static bool Tls13ServerMayBeDisabled { get; }
    43|     internal static bool Tls13ClientMayBeDisabled { get; }
    44| #pragma warning disable CA1810 // Initialize all static fields in 'MsQuicApi' when those fields are declared and remove the explicit static constructor
    45|     [UnconditionalSuppressMessage("SingleFile", "IL3000: Avoid accessing Assembly file path when publishing as a single file",
    46|         Justification = "The code handles the Assembly.Location being null/empty by falling back to AppContext.BaseDirectory")]
    47|     static MsQuicApi()
    48|     {
    49|         bool loaded = false;
    50|         IntPtr msQuicHandle;
    51|         if (!Socket.OSSupportsIPv6)
    52|         {
    53|             NotSupportedReason = "OS does not support dual mode sockets.";
    54|             if (NetEventSource.Log.IsEnabled())
    55|             {
    56|                 NetEventSource.Info(null, NotSupportedReason);
    57|             }
    58|             return;
    59|         }
    60|         if (OperatingSystem.IsWindows())
    61|         {
    62| #pragma warning disable IL3000 // Avoid accessing Assembly file path when publishing as a single file
    63|             string path = typeof(MsQuicApi).Assembly.Location is string assemblyLocation && !string.IsNullOrEmpty(assemblyLocation)
    64|                 ? System.IO.Path.GetDirectoryName(assemblyLocation)!
    65|                 : AppContext.BaseDirectory;
    66| #pragma warning restore IL3000
    67|             path = System.IO.Path.Combine(path, Interop.Libraries.MsQuic);
    68|             if (NetEventSource.Log.IsEnabled())
    69|             {
    70|                 NetEventSource.Info(null, $"Attempting to load MsQuic from {path}");
    71|             }
    72|             loaded = NativeLibrary.TryLoad(path, typeof(MsQuicApi).Assembly, DllImportSearchPath.LegacyBehavior, out msQuicHandle);
    73|         }
    74|         else
    75|         {
    76|             loaded = NativeLibrary.TryLoad($"{Interop.Libraries.MsQuic}.{s_minMsQuicVersion.Major}", typeof(MsQuicApi).Assembly, null, out msQuicHandle) ||
    77|                      NativeLibrary.TryLoad(Interop.Libraries.MsQuic, typeof(MsQuicApi).Assembly, null, out msQuicHandle);
    78|         }
    79|         if (!loaded)
    80|         {
    81|             NotSupportedReason = $"Unable to load MsQuic library version '{s_minMsQuicVersion.Major}'.";
    82|             if (NetEventSource.Log.IsEnabled())
    83|             {
    84|                 NetEventSource.Info(null, NotSupportedReason);
    85|             }
    86|             return;
    87|         }
    88|         MsQuicOpenVersion = (delegate* unmanaged[Cdecl]<uint, QUIC_API_TABLE**, int>)NativeLibrary.GetExport(msQuicHandle, nameof(MsQuicOpenVersion));
    89|         MsQuicClose = (delegate* unmanaged[Cdecl]<QUIC_API_TABLE*, void>)NativeLibrary.GetExport(msQuicHandle, nameof(MsQuicClose));
    90|         if (!TryOpenMsQuic(out QUIC_API_TABLE* apiTable, out int openStatus))
    91|         {
    92|             NotSupportedReason = $"MsQuicOpenVersion for version {s_minMsQuicVersion.Major} returned {openStatus} status code.";
    93|             if (NetEventSource.Log.IsEnabled())
    94|             {
    95|                 NetEventSource.Info(null, NotSupportedReason);
    96|             }
    97|             return;
    98|         }
    99|         try
   100|         {
   101|             uint paramSize;
   102|             int status;
   103|             paramSize = 4 * sizeof(uint);
   104|             uint* libVersion = stackalloc uint[4];
   105|             status = apiTable->GetParam(null, QUIC_PARAM_GLOBAL_LIBRARY_VERSION, &paramSize, libVersion);
   106|             if (StatusFailed(status))
   107|             {
   108|                 if (NetEventSource.Log.IsEnabled())
   109|                 {
   110|                     NetEventSource.Error(null, $"Cannot retrieve {nameof(QUIC_PARAM_GLOBAL_LIBRARY_VERSION)} from MsQuic library: '{status}'.");
   111|                 }
   112|                 return;
   113|             }
   114|             Version version = new Version((int)libVersion[0], (int)libVersion[1], (int)libVersion[2], (int)libVersion[3]);
   115|             paramSize = 64 * sizeof(sbyte);
   116|             sbyte* libGitHash = stackalloc sbyte[64];
   117|             status = apiTable->GetParam(null, QUIC_PARAM_GLOBAL_LIBRARY_GIT_HASH, &paramSize, libGitHash);
   118|             if (StatusFailed(status))
   119|             {
   120|                 if (NetEventSource.Log.IsEnabled())
   121|                 {
   122|                     NetEventSource.Error(null, $"Cannot retrieve {nameof(QUIC_PARAM_GLOBAL_LIBRARY_GIT_HASH)} from MsQuic library: '{status}'.");
   123|                 }
   124|                 return;
   125|             }
   126|             string? gitHash = Marshal.PtrToStringUTF8((IntPtr)libGitHash);
   127|             MsQuicLibraryVersion = $"{Interop.Libraries.MsQuic} {version} ({gitHash})";
   128|             if (version < s_minMsQuicVersion)
   129|             {
   130|                 NotSupportedReason = $"Incompatible MsQuic library version '{version}', expecting higher than '{s_minMsQuicVersion}'.";
   131|                 if (NetEventSource.Log.IsEnabled())
   132|                 {
   133|                     NetEventSource.Info(null, NotSupportedReason);
   134|                 }
   135|                 return;
   136|             }
   137|             if (NetEventSource.Log.IsEnabled())
   138|             {
   139|                 NetEventSource.Info(null, $"Loaded MsQuic library '{MsQuicLibraryVersion}'.");
   140|             }
   141|             QUIC_TLS_PROVIDER provider = OperatingSystem.IsWindows() ? QUIC_TLS_PROVIDER.SCHANNEL : QUIC_TLS_PROVIDER.OPENSSL;
   142|             paramSize = sizeof(QUIC_TLS_PROVIDER);
   143|             apiTable->GetParam(null, QUIC_PARAM_GLOBAL_TLS_PROVIDER, &paramSize, &provider);
   144|             UsesSChannelBackend = provider == QUIC_TLS_PROVIDER.SCHANNEL;
   145|             if (UsesSChannelBackend)
   146|             {
   147|                 if (!IsWindowsVersionSupported())
   148|                 {
   149|                     NotSupportedReason = $"Current Windows version ({Environment.OSVersion}) is not supported by QUIC. Minimal supported version is {s_minWindowsVersion}.";
   150|                     if (NetEventSource.Log.IsEnabled())
   151|                     {
   152|                         NetEventSource.Info(null, NotSupportedReason);
   153|                     }
   154|                     return;
   155|                 }
   156|                 Tls13ServerMayBeDisabled = IsTls13Disabled(isServer: true);
   157|                 Tls13ClientMayBeDisabled = IsTls13Disabled(isServer: false);
   158|             }
   159|             IsQuicSupported = true;
   160|         }
   161|         finally
   162|         {
   163|             MsQuicClose(apiTable);
   164|         }
   165|     }
   166| #pragma warning restore CA1810
   167|     private static MsQuicApi AllocateMsQuicApi()
   168|     {
   169|         Debug.Assert(IsQuicSupported);
   170|         if (!TryOpenMsQuic(out QUIC_API_TABLE* apiTable, out int openStatus))
   171|         {
   172|             throw ThrowHelper.GetExceptionForMsQuicStatus(openStatus);
   173|         }
   174|         return new MsQuicApi(apiTable);
   175|     }
   176|     private static bool TryOpenMsQuic(out QUIC_API_TABLE* apiTable, out int openStatus)
   177|     {
   178|         Debug.Assert(MsQuicOpenVersion != null);
   179|         QUIC_API_TABLE* table = null;
   180|         openStatus = MsQuicOpenVersion((uint)s_minMsQuicVersion.Major, &table);
   181|         if (StatusFailed(openStatus))
   182|         {
   183|             apiTable = null;
   184|             return false;
   185|         }
   186|         apiTable = table;
   187|         return true;
   188|     }
   189|     private static bool IsWindowsVersionSupported() => OperatingSystem.IsWindowsVersionAtLeast(s_minWindowsVersion.Major,
   190|         s_minWindowsVersion.Minor, s_minWindowsVersion.Build, s_minWindowsVersion.Revision);
   191|     private static bool IsTls13Disabled(bool isServer)
   192|     {
   193| #if TARGET_WINDOWS
   194|         string SChannelTls13RegistryKey = isServer
   195|             ? @"SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.3\Server"
   196|             : @"SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.3\Client";
   197|         using var regKey = Registry.LocalMachine.OpenSubKey(SChannelTls13RegistryKey);
   198|         if (regKey is null)
   199|         {
   200|             return false;
   201|         }
   202|         if (regKey.GetValue("Enabled") is int enabled && enabled == 0)
   203|         {
   204|             return true;
   205|         }
   206|         if (regKey.GetValue("DisabledByDefault") is int disabled && disabled == 1)
   207|         {
   208|             return true;
   209|         }
   210| #endif
   211|         return false;
   212|     }
   213| }


# ====================================================================
# FILE: src/libraries/System.Net.Security/src/System/Net/Security/SslStreamPal.Windows.cs
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-570 ---
     1| using System.Collections.Generic;
     2| using System.ComponentModel;
     3| using System.Diagnostics;
     4| using System.Runtime.InteropServices;
     5| using System.Security.Authentication;
     6| using System.Security.Authentication.ExtendedProtection;
     7| using System.Security.Cryptography.X509Certificates;
     8| using System.Security.Principal;
     9| using Microsoft.Win32.SafeHandles;
    10| namespace System.Net.Security
    11| {
    12|     internal static class SslStreamPal
    13|     {
    14|         private static readonly byte[] s_http1 = Interop.Sec_Application_Protocols.ToByteArray(new List<SslApplicationProtocol> { SslApplicationProtocol.Http11 });
    15|         private static readonly byte[] s_http2 = Interop.Sec_Application_Protocols.ToByteArray(new List<SslApplicationProtocol> { SslApplicationProtocol.Http2 });
    16|         private static readonly byte[] s_http12 = Interop.Sec_Application_Protocols.ToByteArray(new List<SslApplicationProtocol> { SslApplicationProtocol.Http11, SslApplicationProtocol.Http2 });
    17|         private static readonly byte[] s_http21 = Interop.Sec_Application_Protocols.ToByteArray(new List<SslApplicationProtocol> { SslApplicationProtocol.Http2, SslApplicationProtocol.Http11 });
    18|         private static readonly bool UseNewCryptoApi =
    19|             Environment.OSVersion.Version.Major >= 10 && Environment.OSVersion.Version.Build >= 18836;
    20|         private const string SecurityPackage = "Microsoft Unified Security Protocol Provider";
    21|         private const Interop.SspiCli.ContextFlags RequiredFlags =
    22|             Interop.SspiCli.ContextFlags.ReplayDetect |
    23|             Interop.SspiCli.ContextFlags.SequenceDetect |
    24|             Interop.SspiCli.ContextFlags.Confidentiality |
    25|             Interop.SspiCli.ContextFlags.AllocateMemory;
    26|         private const Interop.SspiCli.ContextFlags ServerRequiredFlags =
    27|             RequiredFlags | Interop.SspiCli.ContextFlags.AcceptStream | Interop.SspiCli.ContextFlags.AcceptExtendedError;
    28|         public static Exception GetException(SecurityStatusPal status)
    29|         {
    30|             int win32Code = (int)SecurityStatusAdapterPal.GetInteropFromSecurityStatusPal(status);
    31|             return new Win32Exception(win32Code);
    32|         }
    33|         internal const bool StartMutualAuthAsAnonymous = true;
    34|         internal const bool CanEncryptEmptyMessage = true;
    35|         private static readonly byte[] s_sessionTokenBuffer = InitSessionTokenBuffer();
    36|         private static byte[] InitSessionTokenBuffer()
    37|         {
    38|             var schannelSessionToken = new Interop.SChannel.SCHANNEL_SESSION_TOKEN() {
    39|                 dwTokenType = Interop.SChannel.SCHANNEL_SESSION,
    40|                 dwFlags = Interop.SChannel.SSL_SESSION_DISABLE_RECONNECTS,
    41|             };
    42|             return MemoryMarshal.AsBytes(new ReadOnlySpan<Interop.SChannel.SCHANNEL_SESSION_TOKEN>(in schannelSessionToken)).ToArray();
    43|         }
    44|         public static void VerifyPackageInfo()
    45|         {
    46|             SSPIWrapper.GetVerifyPackageInfo(GlobalSSPI.SSPISecureChannel, SecurityPackage, true);
    47|         }
    48|         private static unsafe void SetAlpn(ref InputSecurityBuffers inputBuffers, List<SslApplicationProtocol> alpn, Span<byte> localBuffer)
    49|         {
    50|             if (alpn.Count == 1 && alpn[0] == SslApplicationProtocol.Http11)
    51|             {
    52|                 inputBuffers.SetNextBuffer(new InputSecurityBuffer(s_http1, SecurityBufferType.SECBUFFER_APPLICATION_PROTOCOLS));
    53|             }
    54|             else if (alpn.Count == 1 && alpn[0] == SslApplicationProtocol.Http2)
    55|             {
    56|                 inputBuffers.SetNextBuffer(new InputSecurityBuffer(s_http2, SecurityBufferType.SECBUFFER_APPLICATION_PROTOCOLS));
    57|             }
    58|             else if (alpn.Count == 2 && alpn[0] == SslApplicationProtocol.Http11 && alpn[1] == SslApplicationProtocol.Http2)
    59|             {
    60|                 inputBuffers.SetNextBuffer(new InputSecurityBuffer(s_http12, SecurityBufferType.SECBUFFER_APPLICATION_PROTOCOLS));
    61|             }
    62|             else if (alpn.Count == 2 && alpn[0] == SslApplicationProtocol.Http2 && alpn[1] == SslApplicationProtocol.Http11)
    63|             {
    64|                 inputBuffers.SetNextBuffer(new InputSecurityBuffer(s_http21, SecurityBufferType.SECBUFFER_APPLICATION_PROTOCOLS));
    65|             }
    66|             else
    67|             {
    68|                 int protocolLength = Interop.Sec_Application_Protocols.GetProtocolLength(alpn);
    69|                 int bufferLength = sizeof(Interop.Sec_Application_Protocols) + protocolLength;
    70|                 Span<byte> alpnBuffer = bufferLength <= localBuffer.Length ? localBuffer : new byte[bufferLength];
    71|                 Interop.Sec_Application_Protocols.SetProtocols(alpnBuffer, alpn, protocolLength);
    72|                 inputBuffers.SetNextBuffer(new InputSecurityBuffer(alpnBuffer, SecurityBufferType.SECBUFFER_APPLICATION_PROTOCOLS));
    73|             }
    74|         }
    75|         public static SecurityStatusPal SelectApplicationProtocol(
    76|             SafeFreeCredentials? credentialsHandle,
    77|             SafeDeleteSslContext? context,
    78|             SslAuthenticationOptions sslAuthenticationOptions,
    79|             ReadOnlySpan<byte> clientProtocols)
    80|         {
    81|             throw new PlatformNotSupportedException(nameof(SelectApplicationProtocol));
    82|         }
    83|         public static unsafe SecurityStatusPal AcceptSecurityContext(
    84|             ref SafeFreeCredentials? credentialsHandle,
    85|             ref SafeDeleteSslContext? context,
    86|             ReadOnlySpan<byte> inputBuffer,
    87|             ref byte[]? outputBuffer,
    88|             SslAuthenticationOptions sslAuthenticationOptions)
    89|         {
    90|             Interop.SspiCli.ContextFlags unusedAttributes = default;
    91|             scoped InputSecurityBuffers inputBuffers = default;
    92|             inputBuffers.SetNextBuffer(new InputSecurityBuffer(inputBuffer, SecurityBufferType.SECBUFFER_TOKEN));
    93|             inputBuffers.SetNextBuffer(new InputSecurityBuffer(default, SecurityBufferType.SECBUFFER_EMPTY));
    94|             if (context == null && sslAuthenticationOptions.ApplicationProtocols != null && sslAuthenticationOptions.ApplicationProtocols.Count != 0)
    95|             {
    96|                 Span<byte> localBuffer = stackalloc byte[64];
    97|                 SetAlpn(ref inputBuffers, sslAuthenticationOptions.ApplicationProtocols, localBuffer);
    98|             }
    99|             var resultBuffer = new SecurityBuffer(outputBuffer, SecurityBufferType.SECBUFFER_TOKEN);
   100|             int errorCode = SSPIWrapper.AcceptSecurityContext(
   101|                 GlobalSSPI.SSPISecureChannel,
   102|                 credentialsHandle,
   103|                 ref context,
   104|                 ServerRequiredFlags | (sslAuthenticationOptions.RemoteCertRequired ? Interop.SspiCli.ContextFlags.MutualAuth : Interop.SspiCli.ContextFlags.Zero),
   105|                 Interop.SspiCli.Endianness.SECURITY_NATIVE_DREP,
   106|                 inputBuffers,
   107|                 ref resultBuffer,
   108|                 ref unusedAttributes);
   109|             outputBuffer = resultBuffer.token;
   110|             return SecurityStatusAdapterPal.GetSecurityStatusPalFromNativeInt(errorCode);
   111|         }
   112|         public static bool TryUpdateClintCertificate(
   113|             SafeFreeCredentials? _1,
   114|             SafeDeleteSslContext? _2,
   115|             SslAuthenticationOptions _3)
   116|         {
   117|             return false;
   118|         }
   119|         public static unsafe SecurityStatusPal InitializeSecurityContext(
   120|             ref SafeFreeCredentials? credentialsHandle,
   121|             ref SafeDeleteSslContext? context,
   122|             string? targetName,
   123|             ReadOnlySpan<byte> inputBuffer,
   124|             ref byte[]? outputBuffer,
   125|             SslAuthenticationOptions sslAuthenticationOptions)
   126|         {
   127|             bool newContext = context == null;
   128|             Interop.SspiCli.ContextFlags unusedAttributes = default;
   129|             scoped InputSecurityBuffers inputBuffers = default;
   130|             inputBuffers.SetNextBuffer(new InputSecurityBuffer(inputBuffer, SecurityBufferType.SECBUFFER_TOKEN));
   131|             inputBuffers.SetNextBuffer(new InputSecurityBuffer(default, SecurityBufferType.SECBUFFER_EMPTY));
   132|             if (context == null && sslAuthenticationOptions.ApplicationProtocols != null && sslAuthenticationOptions.ApplicationProtocols.Count != 0)
   133|             {
   134|                 Span<byte> localBuffer = stackalloc byte[64];
   135|                 SetAlpn(ref inputBuffers, sslAuthenticationOptions.ApplicationProtocols, localBuffer);
   136|             }
   137|             var resultBuffer = new SecurityBuffer(outputBuffer, SecurityBufferType.SECBUFFER_TOKEN);
   138|             int errorCode = SSPIWrapper.InitializeSecurityContext(
   139|                             GlobalSSPI.SSPISecureChannel,
   140|                             ref credentialsHandle,
   141|                             ref context,
   142|                             targetName,
   143|                             RequiredFlags | Interop.SspiCli.ContextFlags.InitManualCredValidation,
   144|                             Interop.SspiCli.Endianness.SECURITY_NATIVE_DREP,
   145|                             inputBuffers,
   146|                             ref resultBuffer,
   147|                             ref unusedAttributes);
   148|             if (!sslAuthenticationOptions.AllowTlsResume && newContext && context != null)
   149|             {
   150|                 var securityBuffer = new SecurityBuffer(s_sessionTokenBuffer, SecurityBufferType.SECBUFFER_TOKEN);
   151|                 SecurityStatusPal result = SecurityStatusAdapterPal.GetSecurityStatusPalFromNativeInt(SSPIWrapper.ApplyControlToken(
   152|                     GlobalSSPI.SSPISecureChannel,
   153|                     ref context,
   154|                     in securityBuffer));
   155|                 if (result.ErrorCode != SecurityStatusPalErrorCode.OK)
   156|                 {
   157|                     return result;
   158|                 }
   159|             }
   160|             outputBuffer = resultBuffer.token;
   161|             return SecurityStatusAdapterPal.GetSecurityStatusPalFromNativeInt(errorCode);
   162|         }
   163|         public static SecurityStatusPal Renegotiate(
   164|             ref SafeFreeCredentials? credentialsHandle,
   165|             ref SafeDeleteSslContext? context,
   166|             SslAuthenticationOptions sslAuthenticationOptions,
   167|             out byte[]? outputBuffer)
   168|         {
   169|             byte[]? output = Array.Empty<byte>();
   170|             SecurityStatusPal status = AcceptSecurityContext(ref credentialsHandle, ref context, Span<byte>.Empty, ref output, sslAuthenticationOptions);
   171|             outputBuffer = output;
   172|             return status;
   173|         }
   174|         public static SafeFreeCredentials AcquireCredentialsHandle(SslAuthenticationOptions sslAuthenticationOptions, bool newCredentialsRequested)
   175|         {
   176|             SslStreamCertificateContext? certificateContext = sslAuthenticationOptions.CertificateContext;
   177|             try
   178|             {
   179|                 EncryptionPolicy policy = sslAuthenticationOptions.EncryptionPolicy;
   180| #pragma warning disable SYSLIB0040 // NoEncryption and AllowNoEncryption are obsolete
   181|                 SafeFreeCredentials cred = !UseNewCryptoApi || policy == EncryptionPolicy.NoEncryption ?
   182|                     AcquireCredentialsHandleSchannelCred(sslAuthenticationOptions) :
   183|                     AcquireCredentialsHandleSchCredentials(sslAuthenticationOptions);
   184| #pragma warning restore SYSLIB0040
   185|                 if (certificateContext != null && certificateContext.Trust != null && certificateContext.Trust._sendTrustInHandshake)
   186|                 {
   187|                     AttachCertificateStore(cred, certificateContext.Trust._store!);
   188|                 }
   189|                 if (newCredentialsRequested && sslAuthenticationOptions.CertificateContext != null)
   190|                 {
   191|                     SafeFreeCredential_SECURITY handle = (SafeFreeCredential_SECURITY)cred;
   192|                     handle.HasLocalCertificate = true;
   193|                 }
   194|                 return cred;
   195|             }
   196|             catch (Win32Exception e) when (e.NativeErrorCode == (int)Interop.SECURITY_STATUS.NoCredentials && certificateContext != null)
   197|             {
   198|                 Debug.Assert(certificateContext.TargetCertificate.HasPrivateKey);
   199|                 using SafeCertContextHandle safeCertContextHandle = Interop.Crypt32.CertDuplicateCertificateContext(certificateContext.TargetCertificate.Handle);
   200|                 throw new AuthenticationException(safeCertContextHandle.HasEphemeralPrivateKey ? SR.net_auth_ephemeral : SR.net_auth_SSPI, e);
   201|             }
   202|             catch (Win32Exception e)
   203|             {
   204|                 throw new AuthenticationException(SR.net_auth_SSPI, e);
   205|             }
   206|         }
   207|         private static unsafe void AttachCertificateStore(SafeFreeCredentials cred, X509Store store)
   208|         {
   209|             Interop.SspiCli.SecPkgCred_ClientCertPolicy clientCertPolicy = default;
   210|             fixed (char* ptr = store.Name)
   211|             {
   212|                 clientCertPolicy.pwszSslCtlStoreName = ptr;
   213|                 Interop.SECURITY_STATUS errorCode = Interop.SspiCli.SetCredentialsAttributesW(
   214|                             cred._handle,
   215|                             (long)Interop.SspiCli.ContextAttribute.SECPKG_ATTR_CLIENT_CERT_POLICY,
   216|                             clientCertPolicy,
   217|                             sizeof(Interop.SspiCli.SecPkgCred_ClientCertPolicy));
   218|                 if (errorCode != Interop.SECURITY_STATUS.OK)
   219|                 {
   220|                     throw new Win32Exception((int)errorCode);
   221|                 }
   222|             }
   223|             return;
   224|         }
   225|         public static unsafe SafeFreeCredentials AcquireCredentialsHandleSchannelCred(SslAuthenticationOptions authOptions)
   226|         {
   227|             X509Certificate2? certificate = authOptions.CertificateContext?.TargetCertificate;
   228|             bool isServer = authOptions.IsServer;
   229|             int protocolFlags = GetProtocolFlagsFromSslProtocols(authOptions.EnabledSslProtocols, isServer);
   230|             Interop.SspiCli.SCHANNEL_CRED.Flags flags;
   231|             Interop.SspiCli.CredentialUse direction;
   232|             if (!isServer)
   233|             {
   234|                 direction = Interop.SspiCli.CredentialUse.SECPKG_CRED_OUTBOUND;
   235|                 flags =
   236|                     Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_MANUAL_CRED_VALIDATION |
   237|                     Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_NO_DEFAULT_CREDS |
   238|                     Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_SEND_AUX_RECORD;
   239|                 if (authOptions.CertificateRevocationCheckMode != X509RevocationMode.NoCheck)
   240|                 {
   241|                     flags |=
   242|                         Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_REVOCATION_CHECK_END_CERT |
   243|                         Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_IGNORE_NO_REVOCATION_CHECK |
   244|                         Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_IGNORE_REVOCATION_OFFLINE;
   245|                 }
   246|             }
   247|             else
   248|             {
   249|                 direction = Interop.SspiCli.CredentialUse.SECPKG_CRED_INBOUND;
   250|                 flags =
   251|                     Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_SEND_AUX_RECORD |
   252|                     Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_NO_SYSTEM_MAPPER;
   253|                 if (!authOptions.AllowTlsResume)
   254|                 {
   255|                     flags |= Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_DISABLE_RECONNECTS;
   256|                 }
   257|             }
   258|             EncryptionPolicy policy = authOptions.EncryptionPolicy;
   259| #pragma warning disable SYSLIB0040 // NoEncryption and AllowNoEncryption are obsolete
   260|             if (((protocolFlags == 0) ||
   261|                     (protocolFlags & ~(Interop.SChannel.SP_PROT_SSL2 | Interop.SChannel.SP_PROT_SSL3)) != 0)
   262|                     && (policy != EncryptionPolicy.AllowNoEncryption) && (policy != EncryptionPolicy.NoEncryption))
   263|             {
   264|                 flags |= Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_USE_STRONG_CRYPTO;
   265|             }
   266| #pragma warning restore SYSLIB0040
   267|             if (NetEventSource.Log.IsEnabled()) NetEventSource.Info($"flags=({flags}), ProtocolFlags=({protocolFlags}), EncryptionPolicy={policy}");
   268|             Interop.SspiCli.SCHANNEL_CRED secureCredential = CreateSecureCredential(
   269|                 flags,
   270|                 protocolFlags,
   271|                 policy);
   272|             if (!isServer && !authOptions.AllowTlsResume)
   273|             {
   274|                 secureCredential.dwSessionLifespan = -1;
   275|             }
   276|             if (certificate != null)
   277|             {
   278|                 secureCredential.cCreds = 1;
   279|                 Interop.Crypt32.CERT_CONTEXT* certificateHandle = (Interop.Crypt32.CERT_CONTEXT*)certificate.Handle;
   280|                 secureCredential.paCred = &certificateHandle;
   281|             }
   282|             return AcquireCredentialsHandle(direction, &secureCredential);
   283|         }
   284|         public static unsafe SafeFreeCredentials AcquireCredentialsHandleSchCredentials(SslAuthenticationOptions authOptions)
   285|         {
   286|             X509Certificate2? certificate = authOptions.CertificateContext?.TargetCertificate;
   287|             bool isServer = authOptions.IsServer;
   288|             int protocolFlags = GetProtocolFlagsFromSslProtocols(authOptions.EnabledSslProtocols, isServer);
   289|             Interop.SspiCli.SCH_CREDENTIALS.Flags flags;
   290|             Interop.SspiCli.CredentialUse direction;
   291|             if (isServer)
   292|             {
   293|                 direction = Interop.SspiCli.CredentialUse.SECPKG_CRED_INBOUND;
   294|                 flags =
   295|                     Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_SEND_AUX_RECORD |
   296|                     Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_NO_SYSTEM_MAPPER;
   297|                 if (!authOptions.AllowTlsResume)
   298|                 {
   299|                     flags |= Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_DISABLE_RECONNECTS;
   300|                 }
   301|             }
   302|             else
   303|             {
   304|                 direction = Interop.SspiCli.CredentialUse.SECPKG_CRED_OUTBOUND;
   305|                 flags =
   306|                     Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_MANUAL_CRED_VALIDATION |
   307|                     Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_NO_DEFAULT_CREDS |
   308|                     Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_SEND_AUX_RECORD;
   309|                 if (authOptions.CertificateRevocationCheckMode != X509RevocationMode.NoCheck)
   310|                 {
   311|                     flags |=
   312|                         Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_REVOCATION_CHECK_END_CERT |
   313|                         Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_IGNORE_NO_REVOCATION_CHECK |
   314|                         Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_IGNORE_REVOCATION_OFFLINE;
   315|                 }
   316|             }
   317|             EncryptionPolicy policy = authOptions.EncryptionPolicy;
   318|             if (policy == EncryptionPolicy.RequireEncryption)
   319|             {
   320|                 if ((protocolFlags & Interop.SChannel.SP_PROT_SSL3) == 0)
   321|                 {
   322|                     flags |= Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_USE_STRONG_CRYPTO;
   323|                 }
   324|             }
   325| #pragma warning disable SYSLIB0040 // NoEncryption and AllowNoEncryption are obsolete
   326|             else if (policy == EncryptionPolicy.AllowNoEncryption)
   327|             {
   328|                 flags |= Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_ALLOW_NULL_ENCRYPTION;
   329|             }
   330| #pragma warning restore SYSLIB0040
   331|             else
   332|             {
   333|                 throw new ArgumentException(SR.Format(SR.net_invalid_enum, "EncryptionPolicy"), nameof(policy));
   334|             }
   335|             Interop.SspiCli.SCH_CREDENTIALS credential = default;
   336|             credential.dwVersion = Interop.SspiCli.SCH_CREDENTIALS.CurrentVersion;
   337|             credential.dwFlags = flags;
   338|             if (!isServer && !authOptions.AllowTlsResume)
   339|             {
   340|                 credential.dwSessionLifespan = -1;
   341|             }
   342|             if (certificate != null)
   343|             {
   344|                 credential.cCreds = 1;
   345|                 Interop.Crypt32.CERT_CONTEXT* certificateHandle = (Interop.Crypt32.CERT_CONTEXT*)certificate.Handle;
   346|                 credential.paCred = &certificateHandle;
   347|             }
   348|             if (NetEventSource.Log.IsEnabled()) NetEventSource.Info($"flags=({flags}), ProtocolFlags=({protocolFlags}), EncryptionPolicy={policy}");
   349|             if (protocolFlags != 0)
   350|             {
   351|                 Interop.SspiCli.TLS_PARAMETERS tlsParameters = default;
   352|                 tlsParameters.grbitDisabledProtocols = (uint)protocolFlags ^ uint.MaxValue;
   353|                 credential.cTlsParameters = 1;
   354|                 credential.pTlsParameters = &tlsParameters;
   355|             }
   356|             return AcquireCredentialsHandle(direction, &credential);
   357|         }
   358|         public static unsafe SecurityStatusPal EncryptMessage(SafeDeleteSslContext securityContext, ReadOnlyMemory<byte> input, int headerSize, int trailerSize, ref byte[] output, out int resultSize)
   359|         {
   360|             int bufferSizeNeeded = checked(input.Length + headerSize + trailerSize);
   361|             if (output == null || output.Length < bufferSizeNeeded)
   362|             {
   363|                 output = new byte[bufferSizeNeeded];
   364|             }
   365|             input.Span.CopyTo(new Span<byte>(output, headerSize, input.Length));
   366|             const int NumSecBuffers = 4; // header + data + trailer + empty
   367|             Interop.SspiCli.SecBuffer* unmanagedBuffer = stackalloc Interop.SspiCli.SecBuffer[NumSecBuffers];
   368|             Interop.SspiCli.SecBufferDesc sdcInOut = new Interop.SspiCli.SecBufferDesc(NumSecBuffers)
   369|             {
   370|                 pBuffers = unmanagedBuffer
   371|             };
   372|             fixed (byte* outputPtr = output)
   373|             {
   374|                 Interop.SspiCli.SecBuffer* headerSecBuffer = &unmanagedBuffer[0];
   375|                 headerSecBuffer->BufferType = SecurityBufferType.SECBUFFER_STREAM_HEADER;
   376|                 headerSecBuffer->pvBuffer = (IntPtr)outputPtr;
   377|                 headerSecBuffer->cbBuffer = headerSize;
   378|                 Interop.SspiCli.SecBuffer* dataSecBuffer = &unmanagedBuffer[1];
   379|                 dataSecBuffer->BufferType = SecurityBufferType.SECBUFFER_DATA;
   380|                 dataSecBuffer->pvBuffer = (IntPtr)(outputPtr + headerSize);
   381|                 dataSecBuffer->cbBuffer = input.Length;
   382|                 Interop.SspiCli.SecBuffer* trailerSecBuffer = &unmanagedBuffer[2];
   383|                 trailerSecBuffer->BufferType = SecurityBufferType.SECBUFFER_STREAM_TRAILER;
   384|                 trailerSecBuffer->pvBuffer = (IntPtr)(outputPtr + headerSize + input.Length);
   385|                 trailerSecBuffer->cbBuffer = trailerSize;
   386|                 Interop.SspiCli.SecBuffer* emptySecBuffer = &unmanagedBuffer[3];
   387|                 emptySecBuffer->BufferType = SecurityBufferType.SECBUFFER_EMPTY;
   388|                 emptySecBuffer->cbBuffer = 0;
   389|                 emptySecBuffer->pvBuffer = IntPtr.Zero;
   390|                 int errorCode = GlobalSSPI.SSPISecureChannel.EncryptMessage(securityContext, ref sdcInOut, 0);
   391|                 if (errorCode != 0)
   392|                 {
   393|                     if (NetEventSource.Log.IsEnabled())
   394|                         NetEventSource.Info(securityContext, $"Encrypt ERROR {errorCode:X}");
   395|                     resultSize = 0;
   396|                     return SecurityStatusAdapterPal.GetSecurityStatusPalFromNativeInt(errorCode);
   397|                 }
   398|                 Debug.Assert(headerSecBuffer->cbBuffer >= 0 && dataSecBuffer->cbBuffer >= 0 && trailerSecBuffer->cbBuffer >= 0);
   399|                 Debug.Assert(checked(headerSecBuffer->cbBuffer + dataSecBuffer->cbBuffer + trailerSecBuffer->cbBuffer) <= output.Length);
   400|                 resultSize = checked(headerSecBuffer->cbBuffer + dataSecBuffer->cbBuffer + trailerSecBuffer->cbBuffer);
   401|                 return new SecurityStatusPal(SecurityStatusPalErrorCode.OK);
   402|             }
   403|         }
   404|         public static unsafe SecurityStatusPal DecryptMessage(SafeDeleteSslContext? securityContext, Span<byte> buffer, out int offset, out int count)
   405|         {
   406|             const int NumSecBuffers = 4; // data + empty + empty + empty
   407|             fixed (byte* bufferPtr = buffer)
   408|             {
   409|                 Interop.SspiCli.SecBuffer* unmanagedBuffer = stackalloc Interop.SspiCli.SecBuffer[NumSecBuffers];
   410|                 Interop.SspiCli.SecBuffer* dataBuffer = &unmanagedBuffer[0];
   411|                 dataBuffer->BufferType = SecurityBufferType.SECBUFFER_DATA;
   412|                 dataBuffer->pvBuffer = (IntPtr)bufferPtr;
   413|                 dataBuffer->cbBuffer = buffer.Length;
   414|                 for (int i = 1; i < NumSecBuffers; i++)
   415|                 {
   416|                     Interop.SspiCli.SecBuffer* emptyBuffer = &unmanagedBuffer[i];
   417|                     emptyBuffer->BufferType = SecurityBufferType.SECBUFFER_EMPTY;
   418|                     emptyBuffer->pvBuffer = IntPtr.Zero;
   419|                     emptyBuffer->cbBuffer = 0;
   420|                 }
   421|                 Interop.SspiCli.SecBufferDesc sdcInOut = new Interop.SspiCli.SecBufferDesc(NumSecBuffers)
   422|                 {
   423|                     pBuffers = unmanagedBuffer
   424|                 };
   425|                 Interop.SECURITY_STATUS errorCode = (Interop.SECURITY_STATUS)GlobalSSPI.SSPISecureChannel.DecryptMessage(securityContext!, ref sdcInOut, out _);
   426|                 count = 0;
   427|                 offset = 0;
   428|                 for (int i = 0; i < NumSecBuffers; i++)
   429|                 {
   430|                     if ((errorCode == Interop.SECURITY_STATUS.OK && unmanagedBuffer[i].BufferType == SecurityBufferType.SECBUFFER_DATA)
   431|                         || (errorCode != Interop.SECURITY_STATUS.OK && unmanagedBuffer[i].BufferType == SecurityBufferType.SECBUFFER_EXTRA))
   432|                     {
   433|                         offset = (int)((byte*)unmanagedBuffer[i].pvBuffer - bufferPtr);
   434|                         count = unmanagedBuffer[i].cbBuffer;
   435|                         Debug.Assert(offset >= 0 && count >= 0, $"Expected offset and count greater than 0, got {offset} and {count}");
   436|                         Debug.Assert(checked(offset + count) <= buffer.Length, $"Expected offset+count <= buffer.Length, got {offset}+{count}>={buffer.Length}");
   437|                         break;
   438|                     }
   439|                 }
   440|                 return SecurityStatusAdapterPal.GetSecurityStatusPalFromInterop(errorCode);
   441|             }
   442|         }
   443|         public static SecurityStatusPal ApplyAlertToken(SafeDeleteSslContext? securityContext, TlsAlertType alertType, TlsAlertMessage alertMessage)
   444|         {
   445|             var alertToken = new Interop.SChannel.SCHANNEL_ALERT_TOKEN
   446|             {
   447|                 dwTokenType = Interop.SChannel.SCHANNEL_ALERT,
   448|                 dwAlertType = (uint)alertType,
   449|                 dwAlertNumber = (uint)alertMessage
   450|             };
   451|             byte[] buffer = MemoryMarshal.AsBytes(new ReadOnlySpan<Interop.SChannel.SCHANNEL_ALERT_TOKEN>(in alertToken)).ToArray();
   452|             var securityBuffer = new SecurityBuffer(buffer, SecurityBufferType.SECBUFFER_TOKEN);
   453|             var errorCode = (Interop.SECURITY_STATUS)SSPIWrapper.ApplyControlToken(
   454|                 GlobalSSPI.SSPISecureChannel,
   455|                 ref securityContext,
   456|                 in securityBuffer);
   457|             return SecurityStatusAdapterPal.GetSecurityStatusPalFromInterop(errorCode, attachException: true);
   458|         }
   459|         private static readonly byte[] s_schannelShutdownBytes = BitConverter.GetBytes(Interop.SChannel.SCHANNEL_SHUTDOWN);
   460|         public static SecurityStatusPal ApplyShutdownToken(SafeDeleteSslContext? securityContext)
   461|         {
   462|             var securityBuffer = new SecurityBuffer(s_schannelShutdownBytes, SecurityBufferType.SECBUFFER_TOKEN);
   463|             var errorCode = (Interop.SECURITY_STATUS)SSPIWrapper.ApplyControlToken(
   464|                 GlobalSSPI.SSPISecureChannel,
   465|                 ref securityContext,
   466|                 in securityBuffer);
   467|             return SecurityStatusAdapterPal.GetSecurityStatusPalFromInterop(errorCode, attachException: true);
   468|         }
   469|         public static SafeFreeContextBufferChannelBinding? QueryContextChannelBinding(SafeDeleteContext securityContext, ChannelBindingKind attribute)
   470|         {
   471|             return SSPIWrapper.QueryContextChannelBinding(GlobalSSPI.SSPISecureChannel, securityContext, (Interop.SspiCli.ContextAttribute)attribute);
   472|         }
   473|         public static void QueryContextStreamSizes(SafeDeleteContext securityContext, out StreamSizes streamSizes)
   474|         {
   475|             SecPkgContext_StreamSizes interopStreamSizes = default;
   476|             bool success = SSPIWrapper.QueryBlittableContextAttributes(GlobalSSPI.SSPISecureChannel, securityContext, Interop.SspiCli.ContextAttribute.SECPKG_ATTR_STREAM_SIZES, ref interopStreamSizes);
   477|             Debug.Assert(success);
   478|             streamSizes = new StreamSizes(interopStreamSizes);
   479|         }
   480|         public static void QueryContextConnectionInfo(SafeDeleteContext securityContext, ref SslConnectionInfo connectionInfo)
   481|         {
   482|             connectionInfo.UpdateSslConnectionInfo(securityContext);
   483|         }
   484|         private static int GetProtocolFlagsFromSslProtocols(SslProtocols protocols, bool isServer)
   485|         {
   486|             int protocolFlags = (int)protocols;
   487|             if (isServer)
   488|             {
   489|                 protocolFlags &= Interop.SChannel.ServerProtocolMask;
   490|             }
   491|             else
   492|             {
   493|                 protocolFlags &= Interop.SChannel.ClientProtocolMask;
   494|             }
   495|             return protocolFlags;
   496|         }
   497|         private static Interop.SspiCli.SCHANNEL_CRED CreateSecureCredential(
   498|             Interop.SspiCli.SCHANNEL_CRED.Flags flags,
   499|             int protocols, EncryptionPolicy policy)
   500|         {
   501|             var credential = new Interop.SspiCli.SCHANNEL_CRED()
   502|             {
   503|                 hRootStore = IntPtr.Zero,
   504|                 aphMappers = IntPtr.Zero,
   505|                 palgSupportedAlgs = IntPtr.Zero,
   506|                 paCred = null,
   507|                 cCreds = 0,
   508|                 cMappers = 0,
   509|                 cSupportedAlgs = 0,
   510|                 dwSessionLifespan = 0,
   511|                 reserved = 0,
   512|                 dwVersion = Interop.SspiCli.SCHANNEL_CRED.CurrentVersion
   513|             };
   514|             if (policy == EncryptionPolicy.RequireEncryption)
   515|             {
   516|                 credential.dwMinimumCipherStrength = 0;
   517|                 credential.dwMaximumCipherStrength = 0;
   518|             }
   519| #pragma warning disable SYSLIB0040 // NoEncryption and AllowNoEncryption are obsolete
   520|             else if (policy == EncryptionPolicy.AllowNoEncryption)
   521|             {
   522|                 credential.dwMinimumCipherStrength = -1;
   523|                 credential.dwMaximumCipherStrength = 0;
   524|             }
   525|             else if (policy == EncryptionPolicy.NoEncryption)
   526|             {
   527|                 credential.dwMinimumCipherStrength = -1;
   528|                 credential.dwMaximumCipherStrength = -1;
   529|             }
   530| #pragma warning restore SYSLIB0040
   531|             else
   532|             {
   533|                 throw new ArgumentException(SR.Format(SR.net_invalid_enum, "EncryptionPolicy"), nameof(policy));
   534|             }
   535|             credential.dwFlags = flags;
   536|             credential.grbitEnabledProtocols = protocols;
   537|             return credential;
   538|         }
   539|         private static unsafe SafeFreeCredentials AcquireCredentialsHandle(Interop.SspiCli.CredentialUse credUsage, Interop.SspiCli.SCHANNEL_CRED* secureCredential)
   540|         {
   541|             try
   542|             {
   543|                 using SafeAccessTokenHandle invalidHandle = SafeAccessTokenHandle.InvalidHandle;
   544|                 return WindowsIdentity.RunImpersonated<SafeFreeCredentials>(invalidHandle, () =>
   545|                 {
   546|                     return SSPIWrapper.AcquireCredentialsHandle(GlobalSSPI.SSPISecureChannel, SecurityPackage, credUsage, secureCredential);
   547|                 });
   548|             }
   549|             catch
   550|             {
   551|                 return SSPIWrapper.AcquireCredentialsHandle(GlobalSSPI.SSPISecureChannel, SecurityPackage, credUsage, secureCredential);
   552|             }
   553|         }
   554|         private static unsafe SafeFreeCredentials AcquireCredentialsHandle(Interop.SspiCli.CredentialUse credUsage, Interop.SspiCli.SCH_CREDENTIALS* secureCredential)
   555|         {
   556|             try
   557|             {
   558|                 using SafeAccessTokenHandle invalidHandle = SafeAccessTokenHandle.InvalidHandle;
   559|                 return WindowsIdentity.RunImpersonated<SafeFreeCredentials>(invalidHandle, () =>
   560|                 {
   561|                     return SSPIWrapper.AcquireCredentialsHandle(GlobalSSPI.SSPISecureChannel, SecurityPackage, credUsage, secureCredential);
   562|                 });
   563|             }
   564|             catch
   565|             {
   566|                 return SSPIWrapper.AcquireCredentialsHandle(GlobalSSPI.SSPISecureChannel, SecurityPackage, credUsage, secureCredential);
   567|             }
   568|         }
   569|     }
   570| }


# ====================================================================
# FILE: src/libraries/System.Private.CoreLib/src/System/Threading/Tasks/Task.cs
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-3414 ---
     1| using System.Collections.Generic;
     2| using System.Diagnostics;
     3| using System.Diagnostics.CodeAnalysis;
     4| using System.Diagnostics.Tracing;
     5| using System.Runtime.CompilerServices;
     6| using System.Runtime.ExceptionServices;
     7| using System.Runtime.InteropServices;
     8| using System.Runtime.Versioning;
     9| namespace System.Threading.Tasks
    10| {
    11|     public enum TaskStatus
    12|     {
    13|         Created,
    14|         WaitingForActivation,
    15|         WaitingToRun,
    16|         Running,
    17|         WaitingForChildrenToComplete,
    18|         RanToCompletion,
    19|         Canceled,
    20|         Faulted
    21|     }
    22|     [DebuggerTypeProxy(typeof(SystemThreadingTasks_TaskDebugView))]
    23|     [DebuggerDisplay("Id = {Id}, Status = {Status}, Method = {DebuggerDisplayMethodDescription}")]
    24|     public class Task : IAsyncResult, IDisposable
    25|     {
    26|         [ThreadStatic]
    27|         internal static Task? t_currentTask;  // The currently executing task.
    28|         private static int s_taskIdCounter; // static counter used to generate unique task IDs
    29|         private int m_taskId; // this task's unique ID. initialized only if it is ever requested
    30|         internal Delegate? m_action;
    31|         private protected object? m_stateObject; // A state object that can be optionally supplied, passed to action.
    32|         internal TaskScheduler? m_taskScheduler; // The task scheduler this task runs under.
    33|         internal volatile int m_stateFlags; // SOS DumpAsync command depends on this name
    34|         private Task? ParentForDebugger => m_contingentProperties?.m_parent; // Private property used by a debugger to access this Task's parent
    35|         private int StateFlagsForDebugger => m_stateFlags; // Private property used by a debugger to access this Task's state flags
    36|         private TaskStateFlags StateFlags => (TaskStateFlags)(m_stateFlags & ~(int)TaskStateFlags.OptionsMask); // Private property used to help with debugging
    37|         [Flags]
    38|         internal enum TaskStateFlags
    39|         {
    40|             Started = 0x10000,                       // bin: 0000 0000 0000 0001 0000 0000 0000 0000
    41|             DelegateInvoked = 0x20000,               // bin: 0000 0000 0000 0010 0000 0000 0000 0000
    42|             Disposed = 0x40000,                      // bin: 0000 0000 0000 0100 0000 0000 0000 0000
    43|             ExceptionObservedByParent = 0x80000,     // bin: 0000 0000 0000 1000 0000 0000 0000 0000
    44|             CancellationAcknowledged = 0x100000,     // bin: 0000 0000 0001 0000 0000 0000 0000 0000
    45|             Faulted = 0x200000,                      // bin: 0000 0000 0010 0000 0000 0000 0000 0000
    46|             Canceled = 0x400000,                     // bin: 0000 0000 0100 0000 0000 0000 0000 0000
    47|             WaitingOnChildren = 0x800000,            // bin: 0000 0000 1000 0000 0000 0000 0000 0000
    48|             RanToCompletion = 0x1000000,             // bin: 0000 0001 0000 0000 0000 0000 0000 0000
    49|             WaitingForActivation = 0x2000000,        // bin: 0000 0010 0000 0000 0000 0000 0000 0000
    50|             CompletionReserved = 0x4000000,          // bin: 0000 0100 0000 0000 0000 0000 0000 0000
    51|             WaitCompletionNotification = 0x10000000, // bin: 0001 0000 0000 0000 0000 0000 0000 0000
    52|             ExecutionContextIsNull = 0x20000000,     // bin: 0010 0000 0000 0000 0000 0000 0000 0000
    53|             TaskScheduledWasFired = 0x40000000,      // bin: 0100 0000 0000 0000 0000 0000 0000 0000
    54|             CompletedMask = Canceled | Faulted | RanToCompletion, // A mask for all of the final states a task may be in. SOS DumpAsync command depends on these values.
    55|             OptionsMask = 0xFFFF,                    // signifies the Options portion of m_stateFlags bin: 0000 0000 0000 0000 1111 1111 1111 1111
    56|         }
    57|         private const int CANCELLATION_REQUESTED = 0x1;
    58|         private volatile object? m_continuationObject; // SOS DumpAsync command depends on this name
    59|         private static readonly object s_taskCompletionSentinel = new object();
    60|         internal static bool s_asyncDebuggingEnabled; // false by default
    61|         private static Dictionary<int, Task>? s_currentActiveTasks;
    62|         internal static bool AddToActiveTasks(Task task)
    63|         {
    64|             Debug.Assert(task != null, "Null Task objects can't be added to the ActiveTasks collection");
    65|             Dictionary<int, Task> activeTasks =
    66|                 Volatile.Read(ref s_currentActiveTasks) ??
    67|                 Interlocked.CompareExchange(ref s_currentActiveTasks, new Dictionary<int, Task>(), null) ??
    68|                 s_currentActiveTasks;
    69|             int taskId = task.Id;
    70|             lock (activeTasks)
    71|             {
    72|                 activeTasks[taskId] = task;
    73|             }
    74|             return true;
    75|         }
    76|         internal static void RemoveFromActiveTasks(Task task)
    77|         {
    78|             Dictionary<int, Task>? activeTasks = s_currentActiveTasks;
    79|             if (activeTasks is null)
    80|                 return;
    81|             int taskId = task.Id;
    82|             lock (activeTasks)
    83|             {
    84|                 activeTasks.Remove(taskId);
    85|             }
    86|         }
    87|         internal sealed class ContingentProperties
    88|         {
    89|             internal ExecutionContext? m_capturedContext; // The execution context to run the task within, if any. Only set from non-concurrent contexts.
    90|             internal volatile ManualResetEventSlim? m_completionEvent; // Lazily created if waiting is required.
    91|             internal volatile TaskExceptionHolder? m_exceptionsHolder; // Tracks exceptions, if any have occurred
    92|             internal CancellationToken m_cancellationToken; // Task's cancellation token, if it has one
    93|             internal StrongBox<CancellationTokenRegistration>? m_cancellationRegistration; // Task's registration with the cancellation token
    94|             internal volatile int m_internalCancellationRequested; // Its own field because multiple threads legally try to set it.
    95|             internal volatile int m_completionCountdown = 1;
    96|             internal volatile List<Task>? m_exceptionalChildren;
    97|             internal Task? m_parent;
    98|             internal void SetCompleted()
    99|             {
   100|                 ManualResetEventSlim? mres = m_completionEvent;
   101|                 if (mres != null) SetEvent(mres);
   102|             }
   103|             internal static void SetEvent(ManualResetEventSlim mres)
   104|             {
   105|                 try
   106|                 {
   107|                     mres.Set();
   108|                 }
   109|                 catch (ObjectDisposedException)
   110|                 {
   111|                 }
   112|             }
   113|             internal void UnregisterCancellationCallback()
   114|             {
   115|                 if (m_cancellationRegistration != null)
   116|                 {
   117|                     try { m_cancellationRegistration.Value.Dispose(); }
   118|                     catch (ObjectDisposedException) { }
   119|                     m_cancellationRegistration = null;
   120|                 }
   121|             }
   122|         }
   123|         internal ContingentProperties? m_contingentProperties;
   124|         internal Task(bool canceled, TaskCreationOptions creationOptions, CancellationToken ct)
   125|         {
   126|             int optionFlags = (int)creationOptions;
   127|             if (canceled)
   128|             {
   129|                 m_stateFlags = (int)TaskStateFlags.Canceled | (int)TaskStateFlags.CancellationAcknowledged | optionFlags;
   130|                 m_contingentProperties = new ContingentProperties() // can't have children, so just instantiate directly
   131|                 {
   132|                     m_cancellationToken = ct,
   133|                     m_internalCancellationRequested = CANCELLATION_REQUESTED,
   134|                 };
   135|             }
   136|             else
   137|             {
   138|                 m_stateFlags = (int)TaskStateFlags.RanToCompletion | optionFlags;
   139|             }
   140|         }
   141|         internal Task()
   142|         {
   143|             m_stateFlags = (int)TaskStateFlags.WaitingForActivation | (int)InternalTaskOptions.PromiseTask;
   144|         }
   145|         internal Task(object? state, TaskCreationOptions creationOptions, bool promiseStyle)
   146|         {
   147|             Debug.Assert(promiseStyle, "Promise CTOR: promiseStyle was false");
   148|             if ((creationOptions & ~(TaskCreationOptions.AttachedToParent | TaskCreationOptions.RunContinuationsAsynchronously)) != 0)
   149|             {
   150|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.creationOptions);
   151|             }
   152|             if ((creationOptions & TaskCreationOptions.AttachedToParent) != 0)
   153|             {
   154|                 Task? parent = InternalCurrent;
   155|                 if (parent != null)
   156|                 {
   157|                     EnsureContingentPropertiesInitializedUnsafe().m_parent = parent;
   158|                 }
   159|             }
   160|             TaskConstructorCore(null, state, default, creationOptions, InternalTaskOptions.PromiseTask, null);
   161|         }
   162|         public Task(Action action)
   163|             : this(action, null, null, default, TaskCreationOptions.None, InternalTaskOptions.None, null)
   164|         {
   165|         }
   166|         public Task(Action action, CancellationToken cancellationToken)
   167|             : this(action, null, null, cancellationToken, TaskCreationOptions.None, InternalTaskOptions.None, null)
   168|         {
   169|         }
   170|         public Task(Action action, TaskCreationOptions creationOptions)
   171|             : this(action, null, InternalCurrentIfAttached(creationOptions), default, creationOptions, InternalTaskOptions.None, null)
   172|         {
   173|         }
   174|         public Task(Action action, CancellationToken cancellationToken, TaskCreationOptions creationOptions)
   175|             : this(action, null, InternalCurrentIfAttached(creationOptions), cancellationToken, creationOptions, InternalTaskOptions.None, null)
   176|         {
   177|         }
   178|         public Task(Action<object?> action, object? state)
   179|             : this(action, state, null, default, TaskCreationOptions.None, InternalTaskOptions.None, null)
   180|         {
   181|         }
   182|         public Task(Action<object?> action, object? state, CancellationToken cancellationToken)
   183|             : this(action, state, null, cancellationToken, TaskCreationOptions.None, InternalTaskOptions.None, null)
   184|         {
   185|         }
   186|         public Task(Action<object?> action, object? state, TaskCreationOptions creationOptions)
   187|             : this(action, state, InternalCurrentIfAttached(creationOptions), default, creationOptions, InternalTaskOptions.None, null)
   188|         {
   189|         }
   190|         public Task(Action<object?> action, object? state, CancellationToken cancellationToken, TaskCreationOptions creationOptions)
   191|             : this(action, state, InternalCurrentIfAttached(creationOptions), cancellationToken, creationOptions, InternalTaskOptions.None, null)
   192|         {
   193|         }
   194|         internal Task(Delegate action, object? state, Task? parent, CancellationToken cancellationToken,
   195|             TaskCreationOptions creationOptions, InternalTaskOptions internalOptions, TaskScheduler? scheduler)
   196|         {
   197|             if (action == null)
   198|             {
   199|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.action);
   200|             }
   201|             if (parent != null && (creationOptions & TaskCreationOptions.AttachedToParent) != 0)
   202|             {
   203|                 EnsureContingentPropertiesInitializedUnsafe().m_parent = parent;
   204|             }
   205|             TaskConstructorCore(action, state, cancellationToken, creationOptions, internalOptions, scheduler);
   206|             Debug.Assert(m_contingentProperties == null || m_contingentProperties.m_capturedContext == null,
   207|                 "Captured an ExecutionContext when one was already captured.");
   208|             CapturedContext = ExecutionContext.Capture();
   209|         }
   210|         internal void TaskConstructorCore(Delegate? action, object? state, CancellationToken cancellationToken,
   211|             TaskCreationOptions creationOptions, InternalTaskOptions internalOptions, TaskScheduler? scheduler)
   212|         {
   213|             m_action = action;
   214|             m_stateObject = state;
   215|             m_taskScheduler = scheduler;
   216|             if ((creationOptions &
   217|                     ~(TaskCreationOptions.AttachedToParent |
   218|                       TaskCreationOptions.LongRunning |
   219|                       TaskCreationOptions.DenyChildAttach |
   220|                       TaskCreationOptions.HideScheduler |
   221|                       TaskCreationOptions.PreferFairness |
   222|                       TaskCreationOptions.RunContinuationsAsynchronously)) != 0)
   223|             {
   224|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.creationOptions);
   225|             }
   226| #if DEBUG
   227|             int illegalInternalOptions =
   228|                     (int)(internalOptions &
   229|                             ~(InternalTaskOptions.PromiseTask |
   230|                               InternalTaskOptions.HiddenState |
   231|                               InternalTaskOptions.ContinuationTask |
   232|                               InternalTaskOptions.LazyCancellation |
   233|                               InternalTaskOptions.QueuedByRuntime));
   234|             Debug.Assert(illegalInternalOptions == 0, "TaskConstructorCore: Illegal internal options");
   235| #endif
   236|             Debug.Assert(m_stateFlags == 0, "TaskConstructorCore: non-zero m_stateFlags");
   237|             Debug.Assert((((int)creationOptions) | (int)TaskStateFlags.OptionsMask) == (int)TaskStateFlags.OptionsMask, "TaskConstructorCore: options take too many bits");
   238|             int tmpFlags = (int)creationOptions | (int)internalOptions; // one write to the volatile m_stateFlags instead of two when setting the above options
   239|             m_stateFlags = m_action == null || (internalOptions & InternalTaskOptions.ContinuationTask) != 0 ?
   240|                 tmpFlags | (int)TaskStateFlags.WaitingForActivation :
   241|                 tmpFlags;
   242|             ContingentProperties? props = m_contingentProperties;
   243|             if (props != null)
   244|             {
   245|                 Task? parent = props.m_parent;
   246|                 if (parent != null
   247|                     && ((creationOptions & TaskCreationOptions.AttachedToParent) != 0)
   248|                     && ((parent.CreationOptions & TaskCreationOptions.DenyChildAttach) == 0))
   249|                 {
   250|                     parent.AddNewChild();
   251|                 }
   252|             }
   253|             if (cancellationToken.CanBeCanceled)
   254|             {
   255|                 Debug.Assert((internalOptions & InternalTaskOptions.ContinuationTask) == 0, "TaskConstructorCore: Did not expect to see cancelable token for continuation task.");
   256|                 AssignCancellationToken(cancellationToken, null, null);
   257|             }
   258|         }
   259|         private void AssignCancellationToken(CancellationToken cancellationToken, Task? antecedent, TaskContinuation? continuation)
   260|         {
   261|             ContingentProperties props = EnsureContingentPropertiesInitializedUnsafe();
   262|             props.m_cancellationToken = cancellationToken;
   263|             try
   264|             {
   265|                 if (((InternalTaskOptions)Options &
   266|                     (InternalTaskOptions.QueuedByRuntime | InternalTaskOptions.PromiseTask | InternalTaskOptions.LazyCancellation)) == 0)
   267|                 {
   268|                     if (cancellationToken.IsCancellationRequested)
   269|                     {
   270|                         InternalCancel();
   271|                     }
   272|                     else
   273|                     {
   274|                         CancellationTokenRegistration ctr;
   275|                         if (antecedent == null)
   276|                         {
   277|                             ctr = cancellationToken.UnsafeRegister(static t => ((Task)t!).InternalCancel(), this);
   278|                         }
   279|                         else
   280|                         {
   281|                             Debug.Assert(continuation != null);
   282|                             ctr = cancellationToken.UnsafeRegister(static t =>
   283|                             {
   284|                                 var tuple = (TupleSlim<Task, Task, TaskContinuation>)t!;
   285|                                 Task targetTask = tuple.Item1;
   286|                                 Task antecedentTask = tuple.Item2;
   287|                                 antecedentTask.RemoveContinuation(tuple.Item3);
   288|                                 targetTask.InternalCancel();
   289|                             }, new TupleSlim<Task, Task, TaskContinuation>(this, antecedent, continuation));
   290|                         }
   291|                         props.m_cancellationRegistration = new StrongBox<CancellationTokenRegistration>(ctr);
   292|                     }
   293|                 }
   294|             }
   295|             catch
   296|             {
   297|                 Task? parent = m_contingentProperties?.m_parent;
   298|                 if ((parent != null) &&
   299|                     ((Options & TaskCreationOptions.AttachedToParent) != 0)
   300|                      && ((parent.Options & TaskCreationOptions.DenyChildAttach) == 0))
   301|                 {
   302|                     parent.DisregardChild();
   303|                 }
   304|                 throw;
   305|             }
   306|         }
   307|         private string DebuggerDisplayMethodDescription => m_action?.Method.ToString() ?? "{null}";
   308|         internal TaskCreationOptions Options => OptionsMethod(m_stateFlags);
   309|         internal static TaskCreationOptions OptionsMethod(int flags)
   310|         {
   311|             Debug.Assert(((int)TaskStateFlags.OptionsMask & 1) == 1, "OptionsMask needs a shift in Options.get");
   312|             return (TaskCreationOptions)(flags & (int)TaskStateFlags.OptionsMask);
   313|         }
   314|         internal bool AtomicStateUpdate(int newBits, int illegalBits)
   315|         {
   316|             int oldFlags = m_stateFlags;
   317|             return
   318|                 (oldFlags & illegalBits) == 0 &&
   319|                 (Interlocked.CompareExchange(ref m_stateFlags, oldFlags | newBits, oldFlags) == oldFlags ||
   320|                  AtomicStateUpdateSlow(newBits, illegalBits));
   321|         }
   322|         private bool AtomicStateUpdateSlow(int newBits, int illegalBits)
   323|         {
   324|             int flags = m_stateFlags;
   325|             while (true)
   326|             {
   327|                 if ((flags & illegalBits) != 0) return false;
   328|                 int oldFlags = Interlocked.CompareExchange(ref m_stateFlags, flags | newBits, flags);
   329|                 if (oldFlags == flags)
   330|                 {
   331|                     return true;
   332|                 }
   333|                 flags = oldFlags;
   334|             }
   335|         }
   336|         internal bool AtomicStateUpdate(int newBits, int illegalBits, ref int oldFlags)
   337|         {
   338|             int flags = oldFlags = m_stateFlags;
   339|             while (true)
   340|             {
   341|                 if ((flags & illegalBits) != 0) return false;
   342|                 oldFlags = Interlocked.CompareExchange(ref m_stateFlags, flags | newBits, flags);
   343|                 if (oldFlags == flags)
   344|                 {
   345|                     return true;
   346|                 }
   347|                 flags = oldFlags;
   348|             }
   349|         }
   350|         internal void SetNotificationForWaitCompletion(bool enabled)
   351|         {
   352|             Debug.Assert((Options & (TaskCreationOptions)InternalTaskOptions.PromiseTask) != 0,
   353|                 "Should only be used for promise-style tasks"); // hasn't been vetted on other kinds as there hasn't been a need
   354|             if (enabled)
   355|             {
   356|                 bool success = AtomicStateUpdate((int)TaskStateFlags.WaitCompletionNotification,
   357|                                   (int)TaskStateFlags.CompletedMask | (int)TaskStateFlags.CompletionReserved);
   358|                 Debug.Assert(success, "Tried to set enabled on completed Task");
   359|             }
   360|             else
   361|             {
   362|                 Interlocked.And(ref m_stateFlags, ~(int)TaskStateFlags.WaitCompletionNotification);
   363|             }
   364|         }
   365|         internal bool NotifyDebuggerOfWaitCompletionIfNecessary()
   366|         {
   367|             if (IsWaitNotificationEnabled && ShouldNotifyDebuggerOfWaitCompletion)
   368|             {
   369|                 NotifyDebuggerOfWaitCompletion();
   370|                 return true;
   371|             }
   372|             return false;
   373|         }
   374|         internal static bool AnyTaskRequiresNotifyDebuggerOfWaitCompletion(Task?[] tasks)
   375|         {
   376|             Debug.Assert(tasks != null, "Expected non-null array of tasks");
   377|             foreach (Task? task in tasks)
   378|             {
   379|                 if (task != null &&
   380|                     task.IsWaitNotificationEnabled &&
   381|                     task.ShouldNotifyDebuggerOfWaitCompletion) // potential recursion
   382|                 {
   383|                     return true;
   384|                 }
   385|             }
   386|             return false;
   387|         }
   388|         internal bool IsWaitNotificationEnabledOrNotRanToCompletion
   389|         {
   390|             [MethodImpl(MethodImplOptions.AggressiveInlining)]
   391|             get => (m_stateFlags & ((int)TaskStateFlags.WaitCompletionNotification | (int)TaskStateFlags.RanToCompletion))
   392|                         != (int)TaskStateFlags.RanToCompletion;
   393|         }
   394|         private protected virtual bool ShouldNotifyDebuggerOfWaitCompletion
   395|         {
   396|             get
   397|             {
   398|                 bool isWaitNotificationEnabled = IsWaitNotificationEnabled;
   399|                 Debug.Assert(isWaitNotificationEnabled, "Should only be called if the wait completion bit is set.");
   400|                 return isWaitNotificationEnabled;
   401|             }
   402|         }
   403|         internal bool IsWaitNotificationEnabled => // internal only to enable unit tests; would otherwise be private
   404|             (m_stateFlags & (int)TaskStateFlags.WaitCompletionNotification) != 0;
   405|         [MethodImpl(MethodImplOptions.NoOptimization | MethodImplOptions.NoInlining)]
   406|         private void NotifyDebuggerOfWaitCompletion()
   407|         {
   408|             Debug.Assert(IsWaitNotificationEnabled, "Should only be called if the wait completion bit is set.");
   409|             SetNotificationForWaitCompletion(enabled: false);
   410|         }
   411|         internal bool MarkStarted()
   412|         {
   413|             return AtomicStateUpdate((int)TaskStateFlags.Started, (int)TaskStateFlags.Canceled | (int)TaskStateFlags.Started);
   414|         }
   415|         internal void FireTaskScheduledIfNeeded(TaskScheduler ts)
   416|         {
   417|             if ((m_stateFlags & (int)TaskStateFlags.TaskScheduledWasFired) == 0)
   418|             {
   419|                 m_stateFlags |= (int)TaskStateFlags.TaskScheduledWasFired;
   420|                 if (TplEventSource.Log.IsEnabled())
   421|                 {
   422|                     Task? currentTask = InternalCurrent;
   423|                     Task? parentTask = m_contingentProperties?.m_parent;
   424|                     TplEventSource.Log.TaskScheduled(ts.Id, currentTask == null ? 0 : currentTask.Id,
   425|                                         this.Id, parentTask == null ? 0 : parentTask.Id, (int)this.Options);
   426|                 }
   427|             }
   428|         }
   429|         internal void AddNewChild()
   430|         {
   431|             Debug.Assert(InternalCurrent == this, "Task.AddNewChild(): Called from an external context");
   432|             ContingentProperties props = EnsureContingentPropertiesInitialized();
   433|             if (props.m_completionCountdown == 1)
   434|             {
   435|                 props.m_completionCountdown++;
   436|             }
   437|             else
   438|             {
   439|                 Interlocked.Increment(ref props.m_completionCountdown);
   440|             }
   441|         }
   442|         internal void DisregardChild()
   443|         {
   444|             Debug.Assert(InternalCurrent == this, "Task.DisregardChild(): Called from an external context");
   445|             ContingentProperties props = EnsureContingentPropertiesInitialized();
   446|             Debug.Assert(props.m_completionCountdown >= 2, "Task.DisregardChild(): Expected parent count to be >= 2");
   447|             Interlocked.Decrement(ref props.m_completionCountdown);
   448|         }
   449|         public void Start()
   450|         {
   451|             Start(TaskScheduler.Current);
   452|         }
   453|         public void Start(TaskScheduler scheduler)
   454|         {
   455|             int flags = m_stateFlags;
   456|             if (IsCompletedMethod(flags))
   457|             {
   458|                 ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_Start_TaskCompleted);
   459|             }
   460|             if (scheduler == null)
   461|             {
   462|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
   463|             }
   464|             TaskCreationOptions options = OptionsMethod(flags);
   465|             if ((options & (TaskCreationOptions)InternalTaskOptions.PromiseTask) != 0)
   466|             {
   467|                 ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_Start_Promise);
   468|             }
   469|             if ((options & (TaskCreationOptions)InternalTaskOptions.ContinuationTask) != 0)
   470|             {
   471|                 ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_Start_ContinuationTask);
   472|             }
   473|             if (Interlocked.CompareExchange(ref m_taskScheduler, scheduler, null) != null)
   474|             {
   475|                 ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_Start_AlreadyStarted);
   476|             }
   477|             ScheduleAndStart(true);
   478|         }
   479|         public void RunSynchronously()
   480|         {
   481|             InternalRunSynchronously(TaskScheduler.Current, waitForCompletion: true);
   482|         }
   483|         public void RunSynchronously(TaskScheduler scheduler)
   484|         {
   485|             if (scheduler == null)
   486|             {
   487|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
   488|             }
   489|             InternalRunSynchronously(scheduler, waitForCompletion: true);
   490|         }
   491|         internal void InternalRunSynchronously(TaskScheduler scheduler, bool waitForCompletion)
   492|         {
   493|             Debug.Assert(scheduler != null, "Task.InternalRunSynchronously(): null TaskScheduler");
   494|             int flags = m_stateFlags;
   495|             TaskCreationOptions options = OptionsMethod(flags);
   496|             if ((options & (TaskCreationOptions)InternalTaskOptions.ContinuationTask) != 0)
   497|             {
   498|                 ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_RunSynchronously_Continuation);
   499|             }
   500|             if ((options & (TaskCreationOptions)InternalTaskOptions.PromiseTask) != 0)
   501|             {
   502|                 ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_RunSynchronously_Promise);
   503|             }
   504|             if (IsCompletedMethod(flags))
   505|             {
   506|                 ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_RunSynchronously_TaskCompleted);
   507|             }
   508|             if (Interlocked.CompareExchange(ref m_taskScheduler, scheduler, null) != null)
   509|             {
   510|                 ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_RunSynchronously_AlreadyStarted);
   511|             }
   512|             if (MarkStarted())
   513|             {
   514|                 bool taskQueued = false;
   515|                 try
   516|                 {
   517|                     if (!scheduler.TryRunInline(this, false))
   518|                     {
   519|                         scheduler.InternalQueueTask(this);
   520|                         taskQueued = true; // only mark this after successfully queuing the task.
   521|                     }
   522|                     if (waitForCompletion && !IsCompleted)
   523|                     {
   524|                         SpinThenBlockingWait(Timeout.Infinite, default);
   525|                     }
   526|                 }
   527|                 catch (Exception e)
   528|                 {
   529|                     if (!taskQueued)
   530|                     {
   531|                         TaskSchedulerException tse = new TaskSchedulerException(e);
   532|                         AddException(tse);
   533|                         Finish(false);
   534|                         Debug.Assert(
   535|                             (m_contingentProperties != null) &&
   536|                             (m_contingentProperties.m_exceptionsHolder != null) &&
   537|                             (m_contingentProperties.m_exceptionsHolder.ContainsFaultList),
   538|                             "Task.InternalRunSynchronously(): Expected m_contingentProperties.m_exceptionsHolder to exist " +
   539|                             "and to have faults recorded.");
   540|                         m_contingentProperties.m_exceptionsHolder.MarkAsHandled(false);
   541|                         throw tse;
   542|                     }
   543|                     else throw;
   544|                 }
   545|             }
   546|             else
   547|             {
   548|                 Debug.Assert((m_stateFlags & (int)TaskStateFlags.Canceled) != 0, "Task.RunSynchronously: expected TaskStateFlags.Canceled to be set");
   549|                 ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_RunSynchronously_TaskCompleted);
   550|             }
   551|         }
   552|         internal static Task InternalStartNew(
   553|             Task? creatingTask, Delegate action, object? state, CancellationToken cancellationToken, TaskScheduler scheduler,
   554|             TaskCreationOptions options, InternalTaskOptions internalOptions)
   555|         {
   556|             if (scheduler == null)
   557|             {
   558|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
   559|             }
   560|             Task t = new Task(action, state, creatingTask, cancellationToken, options, internalOptions | InternalTaskOptions.QueuedByRuntime, scheduler);
   561|             t.ScheduleAndStart(false);
   562|             return t;
   563|         }
   564|         internal static int NewId()
   565|         {
   566|             int newId;
   567|             do
   568|             {
   569|                 newId = Interlocked.Increment(ref s_taskIdCounter);
   570|             }
   571|             while (newId == 0);
   572|             if (TplEventSource.Log.IsEnabled())
   573|                 TplEventSource.Log.NewID(newId);
   574|             return newId;
   575|         }
   576|         public int Id
   577|         {
   578|             get
   579|             {
   580|                 if (Volatile.Read(ref m_taskId) == 0)
   581|                 {
   582|                     int newId = NewId();
   583|                     Interlocked.CompareExchange(ref m_taskId, newId, 0);
   584|                 }
   585|                 return m_taskId;
   586|             }
   587|         }
   588|         public static int? CurrentId
   589|         {
   590|             get
   591|             {
   592|                 Task? currentTask = InternalCurrent;
   593|                 if (currentTask != null)
   594|                     return currentTask.Id;
   595|                 else
   596|                     return null;
   597|             }
   598|         }
   599|         internal static Task? InternalCurrent => t_currentTask;
   600|         internal static Task? InternalCurrentIfAttached(TaskCreationOptions creationOptions)
   601|         {
   602|             return (creationOptions & TaskCreationOptions.AttachedToParent) != 0 ? InternalCurrent : null;
   603|         }
   604|         public AggregateException? Exception
   605|         {
   606|             get
   607|             {
   608|                 AggregateException? e = null;
   609|                 if (IsFaulted) e = GetExceptions(false);
   610|                 Debug.Assert((e == null) || IsFaulted, "Task.Exception_get(): returning non-null value when not Faulted");
   611|                 return e;
   612|             }
   613|         }
   614|         public TaskStatus Status
   615|         {
   616|             get
   617|             {
   618|                 TaskStatus rval;
   619|                 int sf = m_stateFlags;
   620|                 if ((sf & (int)TaskStateFlags.Faulted) != 0) rval = TaskStatus.Faulted;
   621|                 else if ((sf & (int)TaskStateFlags.Canceled) != 0) rval = TaskStatus.Canceled;
   622|                 else if ((sf & (int)TaskStateFlags.RanToCompletion) != 0) rval = TaskStatus.RanToCompletion;
   623|                 else if ((sf & (int)TaskStateFlags.WaitingOnChildren) != 0) rval = TaskStatus.WaitingForChildrenToComplete;
   624|                 else if ((sf & (int)TaskStateFlags.DelegateInvoked) != 0) rval = TaskStatus.Running;
   625|                 else if ((sf & (int)TaskStateFlags.Started) != 0) rval = TaskStatus.WaitingToRun;
   626|                 else if ((sf & (int)TaskStateFlags.WaitingForActivation) != 0) rval = TaskStatus.WaitingForActivation;
   627|                 else rval = TaskStatus.Created;
   628|                 return rval;
   629|             }
   630|         }
   631|         public bool IsCanceled =>
   632|                 (m_stateFlags & ((int)TaskStateFlags.Canceled | (int)TaskStateFlags.Faulted)) == (int)TaskStateFlags.Canceled;
   633|         internal bool IsCancellationRequested
   634|         {
   635|             get
   636|             {
   637|                 ContingentProperties? props = Volatile.Read(ref m_contingentProperties);
   638|                 return props != null &&
   639|                     (props.m_internalCancellationRequested == CANCELLATION_REQUESTED ||
   640|                      props.m_cancellationToken.IsCancellationRequested);
   641|             }
   642|         }
   643|         internal ContingentProperties EnsureContingentPropertiesInitialized()
   644|         {
   645|             return Volatile.Read(ref m_contingentProperties) ?? InitializeContingentProperties();
   646|             ContingentProperties InitializeContingentProperties()
   647|             {
   648|                 Interlocked.CompareExchange(ref m_contingentProperties, new ContingentProperties(), null);
   649|                 return m_contingentProperties;
   650|             }
   651|         }
   652|         internal ContingentProperties EnsureContingentPropertiesInitializedUnsafe() =>
   653|             m_contingentProperties ??= new ContingentProperties();
   654|         internal CancellationToken CancellationToken
   655|         {
   656|             get
   657|             {
   658|                 ContingentProperties? props = Volatile.Read(ref m_contingentProperties);
   659|                 return (props == null) ? default : props.m_cancellationToken;
   660|             }
   661|         }
   662|         internal bool IsCancellationAcknowledged => (m_stateFlags & (int)TaskStateFlags.CancellationAcknowledged) != 0;
   663|         public bool IsCompleted
   664|         {
   665|             get
   666|             {
   667|                 int stateFlags = m_stateFlags; // enable inlining of IsCompletedMethod by "cast"ing away the volatility
   668|                 return IsCompletedMethod(stateFlags);
   669|             }
   670|         }
   671|         private static bool IsCompletedMethod(int flags)
   672|         {
   673|             return (flags & (int)TaskStateFlags.CompletedMask) != 0;
   674|         }
   675|         public bool IsCompletedSuccessfully => (m_stateFlags & (int)TaskStateFlags.CompletedMask) == (int)TaskStateFlags.RanToCompletion;
   676|         public TaskCreationOptions CreationOptions => Options & (TaskCreationOptions)(~InternalTaskOptions.InternalOptionsMask);
   677|         internal void SpinUntilCompleted()
   678|         {
   679|             SpinWait sw = default;
   680|             while (!IsCompleted)
   681|             {
   682|                 sw.SpinOnce();
   683|             }
   684|         }
   685|         WaitHandle IAsyncResult.AsyncWaitHandle
   686|         {
   687|             get
   688|             {
   689|                 bool isDisposed = (m_stateFlags & (int)TaskStateFlags.Disposed) != 0;
   690|                 if (isDisposed)
   691|                 {
   692|                     ThrowHelper.ThrowObjectDisposedException(ExceptionResource.Task_ThrowIfDisposed);
   693|                 }
   694|                 return CompletedEvent.WaitHandle;
   695|             }
   696|         }
   697|         public object? AsyncState => (m_stateFlags & (int)InternalTaskOptions.HiddenState) == 0 ? m_stateObject : null;
   698|         bool IAsyncResult.CompletedSynchronously => false;
   699|         internal TaskScheduler? ExecutingTaskScheduler => m_taskScheduler;
   700|         public static TaskFactory Factory { get; } = new TaskFactory();
   701|         internal static readonly Task<VoidTaskResult> s_cachedCompleted = new Task<VoidTaskResult>(false, default, (TaskCreationOptions)InternalTaskOptions.DoNotDispose, default);
   702|         public static Task CompletedTask => s_cachedCompleted;
   703|         internal ManualResetEventSlim CompletedEvent
   704|         {
   705|             get
   706|             {
   707|                 ContingentProperties contingentProps = EnsureContingentPropertiesInitialized();
   708|                 if (contingentProps.m_completionEvent == null)
   709|                 {
   710|                     bool wasCompleted = IsCompleted;
   711|                     ManualResetEventSlim newMre = new ManualResetEventSlim(wasCompleted);
   712|                     if (Interlocked.CompareExchange(ref contingentProps.m_completionEvent, newMre, null) != null)
   713|                     {
   714|                         newMre.Dispose();
   715|                     }
   716|                     else if (!wasCompleted && IsCompleted)
   717|                     {
   718|                         ContingentProperties.SetEvent(newMre);
   719|                     }
   720|                 }
   721|                 return contingentProps.m_completionEvent;
   722|             }
   723|         }
   724|         internal bool ExceptionRecorded
   725|         {
   726|             get
   727|             {
   728|                 ContingentProperties? props = Volatile.Read(ref m_contingentProperties);
   729|                 return (props != null) && (props.m_exceptionsHolder != null) && (props.m_exceptionsHolder.ContainsFaultList);
   730|             }
   731|         }
   732|         [MemberNotNullWhen(true, nameof(Exception))]
   733|         public bool IsFaulted =>
   734|             (m_stateFlags & (int)TaskStateFlags.Faulted) != 0;
   735|         internal ExecutionContext? CapturedContext
   736|         {
   737|             get
   738|             {
   739|                 if ((m_stateFlags & (int)TaskStateFlags.ExecutionContextIsNull) == (int)TaskStateFlags.ExecutionContextIsNull)
   740|                 {
   741|                     return null;
   742|                 }
   743|                 else
   744|                 {
   745|                     return m_contingentProperties?.m_capturedContext ?? ExecutionContext.Default;
   746|                 }
   747|             }
   748|             set
   749|             {
   750|                 if (value == null)
   751|                 {
   752|                     m_stateFlags |= (int)TaskStateFlags.ExecutionContextIsNull;
   753|                 }
   754|                 else if (value != ExecutionContext.Default) // not the default context, then inflate the contingent properties and set it
   755|                 {
   756|                     EnsureContingentPropertiesInitializedUnsafe().m_capturedContext = value;
   757|                 }
   758|             }
   759|         }
   760|         public void Dispose()
   761|         {
   762|             Dispose(true);
   763|             GC.SuppressFinalize(this);
   764|         }
   765|         protected virtual void Dispose(bool disposing)
   766|         {
   767|             if (disposing)
   768|             {
   769|                 if ((Options & (TaskCreationOptions)InternalTaskOptions.DoNotDispose) != 0)
   770|                 {
   771|                     return;
   772|                 }
   773|                 if (!IsCompleted)
   774|                 {
   775|                     ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_Dispose_NotCompleted);
   776|                 }
   777|                 ContingentProperties? cp = Volatile.Read(ref m_contingentProperties);
   778|                 if (cp != null)
   779|                 {
   780|                     ManualResetEventSlim? ev = cp.m_completionEvent;
   781|                     if (ev != null)
   782|                     {
   783|                         cp.m_completionEvent = null;
   784|                         ContingentProperties.SetEvent(ev);
   785|                         ev.Dispose();
   786|                     }
   787|                 }
   788|             }
   789|             m_stateFlags |= (int)TaskStateFlags.Disposed;
   790|         }
   791|         internal void ScheduleAndStart(bool needsProtection)
   792|         {
   793|             Debug.Assert(m_taskScheduler != null, "expected a task scheduler to have been selected");
   794|             Debug.Assert((m_stateFlags & (int)TaskStateFlags.Started) == 0, "task has already started");
   795|             if (needsProtection)
   796|             {
   797|                 if (!MarkStarted())
   798|                 {
   799|                     return;
   800|                 }
   801|             }
   802|             else
   803|             {
   804|                 m_stateFlags |= (int)TaskStateFlags.Started;
   805|             }
   806|             if (s_asyncDebuggingEnabled)
   807|                 AddToActiveTasks(this);
   808|             if (TplEventSource.Log.IsEnabled() && (Options & (TaskCreationOptions)InternalTaskOptions.ContinuationTask) == 0)
   809|             {
   810|                 Debug.Assert(m_action != null, "Must have a delegate to be in ScheduleAndStart");
   811|                 TplEventSource.Log.TraceOperationBegin(this.Id, "Task: " + m_action.Method.Name, 0);
   812|             }
   813|             try
   814|             {
   815|                 m_taskScheduler.InternalQueueTask(this);
   816|             }
   817|             catch (Exception e)
   818|             {
   819|                 TaskSchedulerException tse = new TaskSchedulerException(e);
   820|                 AddException(tse);
   821|                 Finish(false);
   822|                 if ((Options & (TaskCreationOptions)InternalTaskOptions.ContinuationTask) == 0)
   823|                 {
   824|                     Debug.Assert(
   825|                         (m_contingentProperties != null) &&
   826|                         (m_contingentProperties.m_exceptionsHolder != null) &&
   827|                         (m_contingentProperties.m_exceptionsHolder.ContainsFaultList),
   828|                             "Task.ScheduleAndStart(): Expected m_contingentProperties.m_exceptionsHolder to exist " +
   829|                             "and to have faults recorded.");
   830|                     m_contingentProperties.m_exceptionsHolder.MarkAsHandled(false);
   831|                 }
   832|                 throw tse;
   833|             }
   834|         }
   835|         internal void AddException(object exceptionObject)
   836|         {
   837|             Debug.Assert(exceptionObject != null, "Task.AddException: Expected a non-null exception object");
   838|             AddException(exceptionObject, representsCancellation: false);
   839|         }
   840|         internal void AddException(object exceptionObject, bool representsCancellation)
   841|         {
   842|             Debug.Assert(exceptionObject != null, "Task.AddException: Expected a non-null exception object");
   843| #if DEBUG
   844|             var eoAsException = exceptionObject as Exception;
   845|             var eoAsEnumerableException = exceptionObject as IEnumerable<Exception>;
   846|             var eoAsEdi = exceptionObject as ExceptionDispatchInfo;
   847|             var eoAsEnumerableEdi = exceptionObject as IEnumerable<ExceptionDispatchInfo>;
   848|             Debug.Assert(
   849|                 eoAsException != null || eoAsEnumerableException != null || eoAsEdi != null || eoAsEnumerableEdi != null,
   850|                 "Task.AddException: Expected an Exception, ExceptionDispatchInfo, or an IEnumerable<> of one of those");
   851|             var eoAsOce = exceptionObject as OperationCanceledException;
   852|             Debug.Assert(
   853|                 !representsCancellation ||
   854|                 eoAsOce != null ||
   855|                 (eoAsEdi != null && eoAsEdi.SourceException is OperationCanceledException),
   856|                 "representsCancellation should be true only if an OCE was provided.");
   857| #endif
   858|             ContingentProperties props = EnsureContingentPropertiesInitialized();
   859|             if (props.m_exceptionsHolder == null)
   860|             {
   861|                 TaskExceptionHolder holder = new TaskExceptionHolder(this);
   862|                 if (Interlocked.CompareExchange(ref props.m_exceptionsHolder, holder, null) != null)
   863|                 {
   864|                     holder.MarkAsHandled(false);
   865|                 }
   866|             }
   867|             lock (props)
   868|             {
   869|                 props.m_exceptionsHolder.Add(exceptionObject, representsCancellation);
   870|             }
   871|         }
   872|         private AggregateException? GetExceptions(bool includeTaskCanceledExceptions)
   873|         {
   874|             Exception? canceledException = null;
   875|             if (includeTaskCanceledExceptions && IsCanceled)
   876|             {
   877|                 canceledException = new TaskCanceledException(this);
   878|                 canceledException.SetCurrentStackTrace();
   879|             }
   880|             if (ExceptionRecorded)
   881|             {
   882|                 Debug.Assert(m_contingentProperties != null && m_contingentProperties.m_exceptionsHolder != null, "ExceptionRecorded should imply this");
   883|                 return m_contingentProperties.m_exceptionsHolder.CreateExceptionObject(false, canceledException);
   884|             }
   885|             else if (canceledException != null)
   886|             {
   887|                 return new AggregateException(canceledException);
   888|             }
   889|             return null;
   890|         }
   891|         internal List<ExceptionDispatchInfo> GetExceptionDispatchInfos()
   892|         {
   893|             Debug.Assert(IsFaulted && ExceptionRecorded, "Must only be used when the task has faulted with exceptions.");
   894|             return m_contingentProperties!.m_exceptionsHolder!.GetExceptionDispatchInfos();
   895|         }
   896|         internal ExceptionDispatchInfo? GetCancellationExceptionDispatchInfo()
   897|         {
   898|             Debug.Assert(IsCanceled, "Must only be used when the task has canceled.");
   899|             return Volatile.Read(ref m_contingentProperties)?.m_exceptionsHolder?.GetCancellationExceptionDispatchInfo(); // may be null
   900|         }
   901|         internal void MarkExceptionsAsHandled()
   902|         {
   903|             Volatile.Read(ref m_contingentProperties)?.m_exceptionsHolder?.MarkAsHandled(calledFromFinalizer: false);
   904|         }
   905|         internal void ThrowIfExceptional(bool includeTaskCanceledExceptions)
   906|         {
   907|             Debug.Assert(IsCompleted, "ThrowIfExceptional(): Expected IsCompleted == true");
   908|             Exception? exception = GetExceptions(includeTaskCanceledExceptions);
   909|             if (exception != null)
   910|             {
   911|                 UpdateExceptionObservedStatus();
   912|                 throw exception;
   913|             }
   914|         }
   915|         internal static void ThrowAsync(Exception exception, SynchronizationContext? targetContext)
   916|         {
   917|             var edi = ExceptionDispatchInfo.Capture(exception);
   918|             if (targetContext != null)
   919|             {
   920|                 try
   921|                 {
   922|                     targetContext.Post(static state => ((ExceptionDispatchInfo)state!).Throw(), edi);
   923|                     return;
   924|                 }
   925|                 catch (Exception postException)
   926|                 {
   927|                     edi = ExceptionDispatchInfo.Capture(new AggregateException(exception, postException));
   928|                 }
   929|             }
   930| #if NATIVEAOT
   931|             RuntimeExceptionHelpers.ReportUnhandledException(edi.SourceException);
   932| #else
   933|             ThreadPool.QueueUserWorkItem(static state => ((ExceptionDispatchInfo)state!).Throw(), edi);
   934| #endif // NATIVEAOT
   935|         }
   936|         internal void UpdateExceptionObservedStatus()
   937|         {
   938|             Task? parent = m_contingentProperties?.m_parent;
   939|             if ((parent != null)
   940|                 && ((Options & TaskCreationOptions.AttachedToParent) != 0)
   941|                 && ((parent.CreationOptions & TaskCreationOptions.DenyChildAttach) == 0)
   942|                 && InternalCurrent == parent)
   943|             {
   944|                 m_stateFlags |= (int)TaskStateFlags.ExceptionObservedByParent;
   945|             }
   946|         }
   947|         internal bool IsExceptionObservedByParent => (m_stateFlags & (int)TaskStateFlags.ExceptionObservedByParent) != 0;
   948|         internal bool IsDelegateInvoked => (m_stateFlags & (int)TaskStateFlags.DelegateInvoked) != 0;
   949|         internal void Finish(bool userDelegateExecute)
   950|         {
   951|             if (m_contingentProperties == null)
   952|             {
   953|                 FinishStageTwo();
   954|             }
   955|             else
   956|             {
   957|                 FinishSlow(userDelegateExecute);
   958|             }
   959|         }
   960|         private void FinishSlow(bool userDelegateExecute)
   961|         {
   962|             Debug.Assert(userDelegateExecute || m_contingentProperties != null);
   963|             if (!userDelegateExecute)
   964|             {
   965|                 FinishStageTwo();
   966|             }
   967|             else
   968|             {
   969|                 ContingentProperties props = m_contingentProperties!;
   970|                 if ((props.m_completionCountdown == 1) ||
   971|                     Interlocked.Decrement(ref props.m_completionCountdown) == 0) // Reaching this sub clause means there may be remaining active children,
   972|                 {
   973|                     FinishStageTwo();
   974|                 }
   975|                 else
   976|                 {
   977|                     AtomicStateUpdate((int)TaskStateFlags.WaitingOnChildren, (int)TaskStateFlags.Faulted | (int)TaskStateFlags.Canceled | (int)TaskStateFlags.RanToCompletion);
   978|                 }
   979|                 List<Task>? exceptionalChildren = props.m_exceptionalChildren;
   980|                 if (exceptionalChildren != null)
   981|                 {
   982|                     lock (exceptionalChildren)
   983|                     {
   984|                         exceptionalChildren.RemoveAll(t => t.IsExceptionObservedByParent); // RemoveAll has better performance than doing it ourselves
   985|                     }
   986|                 }
   987|             }
   988|         }
   989|         private void FinishStageTwo()
   990|         {
   991|             ContingentProperties? cp = Volatile.Read(ref m_contingentProperties);
   992|             if (cp != null)
   993|             {
   994|                 AddExceptionsFromChildren(cp);
   995|             }
   996|             int completionState;
   997|             if (ExceptionRecorded)
   998|             {
   999|                 completionState = (int)TaskStateFlags.Faulted;
  1000|                 if (TplEventSource.Log.IsEnabled())
  1001|                     TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Error);
  1002|                 if (s_asyncDebuggingEnabled)
  1003|                     RemoveFromActiveTasks(this);
  1004|             }
  1005|             else if (IsCancellationRequested && IsCancellationAcknowledged)
  1006|             {
  1007|                 completionState = (int)TaskStateFlags.Canceled;
  1008|                 if (TplEventSource.Log.IsEnabled())
  1009|                     TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Canceled);
  1010|                 if (s_asyncDebuggingEnabled)
  1011|                     RemoveFromActiveTasks(this);
  1012|             }
  1013|             else
  1014|             {
  1015|                 completionState = (int)TaskStateFlags.RanToCompletion;
  1016|                 if (TplEventSource.Log.IsEnabled())
  1017|                     TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Completed);
  1018|                 if (s_asyncDebuggingEnabled)
  1019|                     RemoveFromActiveTasks(this);
  1020|             }
  1021|             Interlocked.Exchange(ref m_stateFlags, m_stateFlags | completionState);
  1022|             cp = Volatile.Read(ref m_contingentProperties); // need to re-read after updating state
  1023|             if (cp != null)
  1024|             {
  1025|                 cp.SetCompleted();
  1026|                 cp.UnregisterCancellationCallback();
  1027|             }
  1028|             FinishStageThree();
  1029|         }
  1030|         internal void FinishStageThree()
  1031|         {
  1032|             m_action = null;
  1033|             ContingentProperties? cp = m_contingentProperties;
  1034|             if (cp != null)
  1035|             {
  1036|                 cp.m_capturedContext = null;
  1037|                 NotifyParentIfPotentiallyAttachedTask();
  1038|             }
  1039|             FinishContinuations();
  1040|         }
  1041|         internal void NotifyParentIfPotentiallyAttachedTask()
  1042|         {
  1043|             Task? parent = m_contingentProperties?.m_parent;
  1044|             if (parent != null
  1045|                  && ((parent.CreationOptions & TaskCreationOptions.DenyChildAttach) == 0)
  1046|                  && (((TaskCreationOptions)(m_stateFlags & (int)TaskStateFlags.OptionsMask)) & TaskCreationOptions.AttachedToParent) != 0)
  1047|             {
  1048|                 parent.ProcessChildCompletion(this);
  1049|             }
  1050|         }
  1051|         internal void ProcessChildCompletion(Task childTask)
  1052|         {
  1053|             Debug.Assert(childTask != null);
  1054|             Debug.Assert(childTask.IsCompleted, "ProcessChildCompletion was called for an uncompleted task");
  1055|             Debug.Assert(childTask.m_contingentProperties?.m_parent == this, "ProcessChildCompletion should only be called for a child of this task");
  1056|             ContingentProperties? props = Volatile.Read(ref m_contingentProperties);
  1057|             if (childTask.IsFaulted && !childTask.IsExceptionObservedByParent)
  1058|             {
  1059|                 if (props!.m_exceptionalChildren == null)
  1060|                 {
  1061|                     Interlocked.CompareExchange(ref props.m_exceptionalChildren, new List<Task>(), null);
  1062|                 }
  1063|                 List<Task>? tmp = props.m_exceptionalChildren;
  1064|                 if (tmp != null)
  1065|                 {
  1066|                     lock (tmp)
  1067|                     {
  1068|                         tmp.Add(childTask);
  1069|                     }
  1070|                 }
  1071|             }
  1072|             if (Interlocked.Decrement(ref props!.m_completionCountdown) == 0)
  1073|             {
  1074|                 FinishStageTwo();
  1075|             }
  1076|         }
  1077|         internal void AddExceptionsFromChildren(ContingentProperties props)
  1078|         {
  1079|             Debug.Assert(props != null);
  1080|             List<Task>? exceptionalChildren = props.m_exceptionalChildren;
  1081|             if (exceptionalChildren != null)
  1082|             {
  1083|                 lock (exceptionalChildren)
  1084|                 {
  1085|                     foreach (Task task in exceptionalChildren)
  1086|                     {
  1087|                         Debug.Assert(task.IsCompleted, "Expected all tasks in list to be completed");
  1088|                         if (task.IsFaulted && !task.IsExceptionObservedByParent)
  1089|                         {
  1090|                             TaskExceptionHolder? exceptionHolder = Volatile.Read(ref task.m_contingentProperties)!.m_exceptionsHolder;
  1091|                             Debug.Assert(exceptionHolder != null);
  1092|                             AddException(exceptionHolder.CreateExceptionObject(false, null));
  1093|                         }
  1094|                     }
  1095|                 }
  1096|                 props.m_exceptionalChildren = null;
  1097|             }
  1098|         }
  1099|         internal bool ExecuteEntry()
  1100|         {
  1101|             int previousState = 0;
  1102|             if (!AtomicStateUpdate((int)TaskStateFlags.DelegateInvoked,
  1103|                                     (int)TaskStateFlags.DelegateInvoked | (int)TaskStateFlags.CompletedMask,
  1104|                                     ref previousState) && (previousState & (int)TaskStateFlags.Canceled) == 0)
  1105|             {
  1106|                 return false;
  1107|             }
  1108|             if (!IsCancellationRequested & !IsCanceled)
  1109|             {
  1110|                 ExecuteWithThreadLocal(ref t_currentTask);
  1111|             }
  1112|             else
  1113|             {
  1114|                 ExecuteEntryCancellationRequestedOrCanceled();
  1115|             }
  1116|             return true;
  1117|         }
  1118|         internal virtual void ExecuteFromThreadPool(Thread threadPoolThread) => ExecuteEntryUnsafe(threadPoolThread);
  1119|         internal void ExecuteEntryUnsafe(Thread? threadPoolThread) // used instead of ExecuteEntry() when we don't have to worry about double-execution prevent
  1120|         {
  1121|             m_stateFlags |= (int)TaskStateFlags.DelegateInvoked;
  1122|             if (!IsCancellationRequested & !IsCanceled)
  1123|             {
  1124|                 ExecuteWithThreadLocal(ref t_currentTask, threadPoolThread);
  1125|             }
  1126|             else
  1127|             {
  1128|                 ExecuteEntryCancellationRequestedOrCanceled();
  1129|             }
  1130|         }
  1131|         internal void ExecuteEntryCancellationRequestedOrCanceled()
  1132|         {
  1133|             if (!IsCanceled)
  1134|             {
  1135|                 int prevState = Interlocked.Exchange(ref m_stateFlags, m_stateFlags | (int)TaskStateFlags.Canceled);
  1136|                 if ((prevState & (int)TaskStateFlags.Canceled) == 0)
  1137|                 {
  1138|                     CancellationCleanupLogic();
  1139|                 }
  1140|             }
  1141|         }
  1142|         private void ExecuteWithThreadLocal(ref Task? currentTaskSlot, Thread? threadPoolThread = null)
  1143|         {
  1144|             Task? previousTask = currentTaskSlot;
  1145|             TplEventSource log = TplEventSource.Log;
  1146|             Guid savedActivityID = default;
  1147|             bool etwIsEnabled = log.IsEnabled();
  1148|             if (etwIsEnabled)
  1149|             {
  1150|                 if (log.TasksSetActivityIds)
  1151|                     EventSource.SetCurrentThreadActivityId(TplEventSource.CreateGuidForTaskID(this.Id), out savedActivityID);
  1152|                 if (previousTask != null)
  1153|                     log.TaskStarted(previousTask.m_taskScheduler!.Id, previousTask.Id, this.Id);
  1154|                 else
  1155|                     log.TaskStarted(TaskScheduler.Current.Id, 0, this.Id);
  1156|                 log.TraceSynchronousWorkBegin(this.Id, CausalitySynchronousWork.Execution);
  1157|             }
  1158|             try
  1159|             {
  1160|                 currentTaskSlot = this;
  1161|                 try
  1162|                 {
  1163|                     ExecutionContext? ec = CapturedContext;
  1164|                     if (ec == null)
  1165|                     {
  1166|                         InnerInvoke();
  1167|                     }
  1168|                     else
  1169|                     {
  1170|                         if (threadPoolThread is null)
  1171|                         {
  1172|                             ExecutionContext.RunInternal(ec, s_ecCallback, this);
  1173|                         }
  1174|                         else
  1175|                         {
  1176|                             ExecutionContext.RunFromThreadPoolDispatchLoop(threadPoolThread, ec, s_ecCallback, this);
  1177|                         }
  1178|                     }
  1179|                 }
  1180|                 catch (Exception exn)
  1181|                 {
  1182|                     HandleException(exn);
  1183|                 }
  1184|                 if (etwIsEnabled)
  1185|                     log.TraceSynchronousWorkEnd(CausalitySynchronousWork.Execution);
  1186|                 Finish(true);
  1187|             }
  1188|             finally
  1189|             {
  1190|                 currentTaskSlot = previousTask;
  1191|                 if (etwIsEnabled)
  1192|                 {
  1193|                     if (previousTask != null)
  1194|                         log.TaskCompleted(previousTask.m_taskScheduler!.Id, previousTask.Id, this.Id, IsFaulted);
  1195|                     else
  1196|                         log.TaskCompleted(TaskScheduler.Current.Id, 0, this.Id, IsFaulted);
  1197|                     if (log.TasksSetActivityIds)
  1198|                         EventSource.SetCurrentThreadActivityId(savedActivityID);
  1199|                 }
  1200|             }
  1201|         }
  1202|         private static readonly ContextCallback s_ecCallback = obj =>
  1203|         {
  1204|             Debug.Assert(obj is Task);
  1205|             Unsafe.As<Task>(obj).InnerInvoke();
  1206|         };
  1207|         internal virtual void InnerInvoke()
  1208|         {
  1209|             Debug.Assert(m_action != null, "Null action in InnerInvoke()");
  1210|             if (m_action is Action action)
  1211|             {
  1212|                 action();
  1213|                 return;
  1214|             }
  1215|             if (m_action is Action<object?> actionWithState)
  1216|             {
  1217|                 actionWithState(m_stateObject);
  1218|                 return;
  1219|             }
  1220|             Debug.Fail("Invalid m_action in Task");
  1221|         }
  1222|         private void HandleException(Exception unhandledException)
  1223|         {
  1224|             Debug.Assert(unhandledException != null);
  1225|             if (unhandledException is OperationCanceledException exceptionAsOce && IsCancellationRequested &&
  1226|                 m_contingentProperties!.m_cancellationToken == exceptionAsOce.CancellationToken)
  1227|             {
  1228|                 SetCancellationAcknowledged();
  1229|                 AddException(exceptionAsOce, representsCancellation: true);
  1230|             }
  1231|             else
  1232|             {
  1233|                 AddException(unhandledException);
  1234|             }
  1235|         }
  1236|         #region Await Support
  1237|         public TaskAwaiter GetAwaiter()
  1238|         {
  1239|             return new TaskAwaiter(this);
  1240|         }
  1241|         public ConfiguredTaskAwaitable ConfigureAwait(bool continueOnCapturedContext)
  1242|         {
  1243|             return new ConfiguredTaskAwaitable(this, continueOnCapturedContext ? ConfigureAwaitOptions.ContinueOnCapturedContext : ConfigureAwaitOptions.None);
  1244|         }
  1245|         public ConfiguredTaskAwaitable ConfigureAwait(ConfigureAwaitOptions options)
  1246|         {
  1247|             if ((options & ~(ConfigureAwaitOptions.ContinueOnCapturedContext |
  1248|                              ConfigureAwaitOptions.SuppressThrowing |
  1249|                              ConfigureAwaitOptions.ForceYielding)) != 0)
  1250|             {
  1251|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.options);
  1252|             }
  1253|             return new ConfiguredTaskAwaitable(this, options);
  1254|         }
  1255|         internal void SetContinuationForAwait(
  1256|             Action continuationAction, bool continueOnCapturedContext, bool flowExecutionContext)
  1257|         {
  1258|             Debug.Assert(continuationAction != null);
  1259|             TaskContinuation? tc;
  1260|             if (continueOnCapturedContext)
  1261|             {
  1262|                 if (SynchronizationContext.Current is SynchronizationContext syncCtx && syncCtx.GetType() != typeof(SynchronizationContext))
  1263|                 {
  1264|                     tc = new SynchronizationContextAwaitTaskContinuation(syncCtx, continuationAction, flowExecutionContext);
  1265|                     goto HaveTaskContinuation;
  1266|                 }
  1267|                 if (TaskScheduler.InternalCurrent is TaskScheduler scheduler && scheduler != TaskScheduler.Default)
  1268|                 {
  1269|                     tc = new TaskSchedulerAwaitTaskContinuation(scheduler, continuationAction, flowExecutionContext);
  1270|                     goto HaveTaskContinuation;
  1271|                 }
  1272|             }
  1273|             if (flowExecutionContext)
  1274|             {
  1275|                 tc = new AwaitTaskContinuation(continuationAction, flowExecutionContext: true);
  1276|                 goto HaveTaskContinuation;
  1277|             }
  1278|             Debug.Assert(!flowExecutionContext, "We already determined we're not required to flow context.");
  1279|             if (!AddTaskContinuation(continuationAction, addBeforeOthers: false))
  1280|             {
  1281|                 AwaitTaskContinuation.UnsafeScheduleAction(continuationAction, this);
  1282|             }
  1283|             return;
  1284|             HaveTaskContinuation:
  1285|             if (!AddTaskContinuation(tc, addBeforeOthers: false))
  1286|             {
  1287|                 tc.Run(this, canInlineContinuationTask: false);
  1288|             }
  1289|         }
  1290|         internal void UnsafeSetContinuationForAwait(IAsyncStateMachineBox stateMachineBox, bool continueOnCapturedContext)
  1291|         {
  1292|             Debug.Assert(stateMachineBox != null);
  1293|             TaskContinuation? tc;
  1294|             if (continueOnCapturedContext)
  1295|             {
  1296|                 if (SynchronizationContext.Current is SynchronizationContext syncCtx && syncCtx.GetType() != typeof(SynchronizationContext))
  1297|                 {
  1298|                     tc = new SynchronizationContextAwaitTaskContinuation(syncCtx, stateMachineBox.MoveNextAction, flowExecutionContext: false);
  1299|                     goto HaveTaskContinuation;
  1300|                 }
  1301|                 if (TaskScheduler.InternalCurrent is TaskScheduler scheduler && scheduler != TaskScheduler.Default)
  1302|                 {
  1303|                     tc = new TaskSchedulerAwaitTaskContinuation(scheduler, stateMachineBox.MoveNextAction, flowExecutionContext: false);
  1304|                     goto HaveTaskContinuation;
  1305|                 }
  1306|             }
  1307|             if (!AddTaskContinuation(stateMachineBox, addBeforeOthers: false))
  1308|             {
  1309|                 ThreadPool.UnsafeQueueUserWorkItemInternal(stateMachineBox, preferLocal: true);
  1310|             }
  1311|             return;
  1312|             HaveTaskContinuation:
  1313|             if (!AddTaskContinuation(tc, addBeforeOthers: false))
  1314|             {
  1315|                 tc.Run(this, canInlineContinuationTask: false);
  1316|             }
  1317|         }
  1318|         public static YieldAwaitable Yield()
  1319|         {
  1320|             return default;
  1321|         }
  1322|         #endregion
  1323|         public void Wait()
  1324|         {
  1325| #if DEBUG
  1326|             bool waitResult =
  1327| #endif
  1328|             Wait(Timeout.Infinite, default);
  1329| #if DEBUG
  1330|             Debug.Assert(waitResult, "expected wait to succeed");
  1331| #endif
  1332|         }
  1333|         public bool Wait(TimeSpan timeout) => Wait(timeout, default);
  1334|         public bool Wait(TimeSpan timeout, CancellationToken cancellationToken)
  1335|         {
  1336|             long totalMilliseconds = (long)timeout.TotalMilliseconds;
  1337|             if (totalMilliseconds < -1 || totalMilliseconds > int.MaxValue)
  1338|             {
  1339|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.timeout);
  1340|             }
  1341|             cancellationToken.ThrowIfCancellationRequested();
  1342|             return Wait((int)totalMilliseconds, cancellationToken);
  1343|         }
  1344|         public void Wait(CancellationToken cancellationToken)
  1345|         {
  1346|             Wait(Timeout.Infinite, cancellationToken);
  1347|         }
  1348|         public bool Wait(int millisecondsTimeout)
  1349|         {
  1350|             return Wait(millisecondsTimeout, default);
  1351|         }
  1352|         public bool Wait(int millisecondsTimeout, CancellationToken cancellationToken)
  1353|         {
  1354|             if (millisecondsTimeout < -1)
  1355|             {
  1356|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.millisecondsTimeout);
  1357|             }
  1358|             if (!IsWaitNotificationEnabledOrNotRanToCompletion) // (!DebuggerBitSet && RanToCompletion)
  1359|                 return true;
  1360|             if (!InternalWait(millisecondsTimeout, cancellationToken))
  1361|                 return false;
  1362|             if (IsWaitNotificationEnabledOrNotRanToCompletion) // avoid a few unnecessary volatile reads if we completed successfully
  1363|             {
  1364|                 NotifyDebuggerOfWaitCompletionIfNecessary();
  1365|                 if (IsCanceled) cancellationToken.ThrowIfCancellationRequested();
  1366|                 ThrowIfExceptional(true);
  1367|             }
  1368|             Debug.Assert((m_stateFlags & (int)TaskStateFlags.Faulted) == 0, "Task.Wait() completing when in Faulted state.");
  1369|             return true;
  1370|         }
  1371|         public Task WaitAsync(CancellationToken cancellationToken) => WaitAsync(Timeout.UnsignedInfinite, TimeProvider.System, cancellationToken);
  1372|         public Task WaitAsync(TimeSpan timeout) => WaitAsync(ValidateTimeout(timeout, ExceptionArgument.timeout), TimeProvider.System, default);
  1373|         public Task WaitAsync(TimeSpan timeout, TimeProvider timeProvider)
  1374|         {
  1375|             ArgumentNullException.ThrowIfNull(timeProvider);
  1376|             return WaitAsync(ValidateTimeout(timeout, ExceptionArgument.timeout), timeProvider, default);
  1377|         }
  1378|         public Task WaitAsync(TimeSpan timeout, CancellationToken cancellationToken) =>
  1379|             WaitAsync(ValidateTimeout(timeout, ExceptionArgument.timeout), TimeProvider.System, cancellationToken);
  1380|         public Task WaitAsync(TimeSpan timeout, TimeProvider timeProvider, CancellationToken cancellationToken)
  1381|         {
  1382|             ArgumentNullException.ThrowIfNull(timeProvider);
  1383|             return WaitAsync(ValidateTimeout(timeout, ExceptionArgument.timeout), timeProvider, cancellationToken);
  1384|         }
  1385|         private Task WaitAsync(uint millisecondsTimeout, TimeProvider timeProvider, CancellationToken cancellationToken)
  1386|         {
  1387|             if (IsCompleted || (!cancellationToken.CanBeCanceled && millisecondsTimeout == Timeout.UnsignedInfinite))
  1388|             {
  1389|                 return this;
  1390|             }
  1391|             if (cancellationToken.IsCancellationRequested)
  1392|             {
  1393|                 return FromCanceled(cancellationToken);
  1394|             }
  1395|             if (millisecondsTimeout == 0)
  1396|             {
  1397|                 return FromException(new TimeoutException());
  1398|             }
  1399|             return new CancellationPromise<VoidTaskResult>(this, millisecondsTimeout, timeProvider, cancellationToken);
  1400|         }
  1401|         private protected sealed class CancellationPromise<TResult> : Task<TResult>, ITaskCompletionAction
  1402|         {
  1403|             private readonly Task _task;
  1404|             private readonly CancellationTokenRegistration _registration;
  1405|             private readonly ITimer? _timer;
  1406|             internal CancellationPromise(Task source, uint millisecondsDelay, TimeProvider timeProvider, CancellationToken token)
  1407|             {
  1408|                 Debug.Assert(source != null);
  1409|                 Debug.Assert(millisecondsDelay != 0);
  1410|                 _task = source;
  1411|                 source.AddCompletionAction(this);
  1412|                 if (millisecondsDelay != Timeout.UnsignedInfinite)
  1413|                 {
  1414|                     TimerCallback callback = static state =>
  1415|                     {
  1416|                         var thisRef = (CancellationPromise<TResult>)state!;
  1417|                         if (thisRef.TrySetException(new TimeoutException()))
  1418|                         {
  1419|                             thisRef.Cleanup();
  1420|                         }
  1421|                     };
  1422|                     if (timeProvider == TimeProvider.System)
  1423|                     {
  1424|                         _timer = new TimerQueueTimer(callback, this, millisecondsDelay, Timeout.UnsignedInfinite, flowExecutionContext: false);
  1425|                     }
  1426|                     else
  1427|                     {
  1428|                         using (ExecutionContext.SuppressFlow())
  1429|                         {
  1430|                             _timer = timeProvider.CreateTimer(callback, this, TimeSpan.FromMilliseconds(millisecondsDelay), Timeout.InfiniteTimeSpan);
  1431|                         }
  1432|                     }
  1433|                 }
  1434|                 _registration = token.UnsafeRegister(static (state, cancellationToken) =>
  1435|                 {
  1436|                     var thisRef = (CancellationPromise<TResult>)state!;
  1437|                     if (thisRef.TrySetCanceled(cancellationToken))
  1438|                     {
  1439|                         thisRef.Cleanup();
  1440|                     }
  1441|                 }, this);
  1442|                 if (IsCompleted)
  1443|                 {
  1444|                     Cleanup();
  1445|                 }
  1446|             }
  1447|             bool ITaskCompletionAction.InvokeMayRunArbitraryCode => true;
  1448|             void ITaskCompletionAction.Invoke(Task completingTask)
  1449|             {
  1450|                 Debug.Assert(completingTask.IsCompleted);
  1451|                 bool set = completingTask.Status switch
  1452|                 {
  1453|                     TaskStatus.Canceled => TrySetCanceled(completingTask.CancellationToken, completingTask.GetCancellationExceptionDispatchInfo()),
  1454|                     TaskStatus.Faulted => TrySetException(completingTask.GetExceptionDispatchInfos()),
  1455|                     _ => completingTask is Task<TResult> taskTResult ? TrySetResult(taskTResult.Result) : TrySetResult(),
  1456|                 };
  1457|                 if (set)
  1458|                 {
  1459|                     Cleanup();
  1460|                 }
  1461|             }
  1462|             private void Cleanup()
  1463|             {
  1464|                 _registration.Dispose();
  1465|                 _timer?.Dispose();
  1466|                 _task.RemoveContinuation(this);
  1467|             }
  1468|         }
  1469|         private bool WrappedTryRunInline()
  1470|         {
  1471|             if (m_taskScheduler == null)
  1472|                 return false;
  1473|             try
  1474|             {
  1475|                 return m_taskScheduler.TryRunInline(this, true);
  1476|             }
  1477|             catch (Exception e)
  1478|             {
  1479|                 throw new TaskSchedulerException(e);
  1480|             }
  1481|         }
  1482|         [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
  1483|         internal bool InternalWait(int millisecondsTimeout, CancellationToken cancellationToken) =>
  1484|             InternalWaitCore(millisecondsTimeout, cancellationToken);
  1485|         private bool InternalWaitCore(int millisecondsTimeout, CancellationToken cancellationToken)
  1486|         {
  1487|             bool returnValue = IsCompleted;
  1488|             if (returnValue)
  1489|             {
  1490|                 return true;
  1491|             }
  1492|             TplEventSource log = TplEventSource.Log;
  1493|             bool etwIsEnabled = log.IsEnabled();
  1494|             if (etwIsEnabled)
  1495|             {
  1496|                 Task? currentTask = InternalCurrent;
  1497|                 log.TaskWaitBegin(
  1498|                     currentTask != null ? currentTask.m_taskScheduler!.Id : TaskScheduler.Default.Id, currentTask != null ? currentTask.Id : 0,
  1499|                     this.Id, TplEventSource.TaskWaitBehavior.Synchronous, 0);
  1500|             }
  1501|             Debugger.NotifyOfCrossThreadDependency();
  1502|             if (millisecondsTimeout == Timeout.Infinite && !cancellationToken.CanBeCanceled &&
  1503|                 WrappedTryRunInline() && IsCompleted) // TryRunInline doesn't guarantee completion, as there may be unfinished children.
  1504|             {
  1505|                 returnValue = true;
  1506|             }
  1507|             else
  1508|             {
  1509|                 returnValue = SpinThenBlockingWait(millisecondsTimeout, cancellationToken);
  1510|             }
  1511|             Debug.Assert(IsCompleted || millisecondsTimeout != Timeout.Infinite);
  1512|             if (etwIsEnabled)
  1513|             {
  1514|                 Task? currentTask = InternalCurrent;
  1515|                 if (currentTask != null)
  1516|                 {
  1517|                     log.TaskWaitEnd(currentTask.m_taskScheduler!.Id, currentTask.Id, this.Id);
  1518|                 }
  1519|                 else
  1520|                 {
  1521|                     log.TaskWaitEnd(TaskScheduler.Default.Id, 0, this.Id);
  1522|                 }
  1523|                 log.TaskWaitContinuationComplete(this.Id);
  1524|             }
  1525|             return returnValue;
  1526|         }
  1527|         private sealed class SetOnInvokeMres : ManualResetEventSlim, ITaskCompletionAction
  1528|         {
  1529|             internal SetOnInvokeMres() : base(false, 0) { }
  1530|             public void Invoke(Task completingTask) { Set(); }
  1531|             public bool InvokeMayRunArbitraryCode => false;
  1532|         }
  1533|         private bool SpinThenBlockingWait(int millisecondsTimeout, CancellationToken cancellationToken)
  1534|         {
  1535|             bool infiniteWait = millisecondsTimeout == Timeout.Infinite;
  1536|             uint startTimeTicks = infiniteWait ? 0 : (uint)Environment.TickCount;
  1537|             bool returnValue = SpinWait(millisecondsTimeout);
  1538|             if (!returnValue)
  1539|             {
  1540| #if CORECLR
  1541|                 if (ThreadPoolWorkQueue.s_prioritizationExperiment)
  1542|                 {
  1543|                     ThreadPoolWorkQueue.TransferAllLocalWorkItemsToHighPriorityGlobalQueue();
  1544|                 }
  1545| #endif
  1546|                 var mres = new SetOnInvokeMres();
  1547|                 try
  1548|                 {
  1549|                     AddCompletionAction(mres, addBeforeOthers: true);
  1550| #pragma warning disable CA1416 // Validate platform compatibility, issue: https://github.com/dotnet/runtime/issues/44622
  1551|                     if (infiniteWait)
  1552|                     {
  1553|                         bool notifyWhenUnblocked = ThreadPool.NotifyThreadBlocked();
  1554|                         try
  1555|                         {
  1556|                             returnValue = mres.Wait(Timeout.Infinite, cancellationToken);
  1557|                         }
  1558|                         finally
  1559|                         {
  1560|                             if (notifyWhenUnblocked)
  1561|                             {
  1562|                                 ThreadPool.NotifyThreadUnblocked();
  1563|                             }
  1564|                         }
  1565|                     }
  1566|                     else
  1567|                     {
  1568|                         uint elapsedTimeTicks = ((uint)Environment.TickCount) - startTimeTicks;
  1569|                         if (elapsedTimeTicks < millisecondsTimeout)
  1570|                         {
  1571|                             bool notifyWhenUnblocked = ThreadPool.NotifyThreadBlocked();
  1572|                             try
  1573|                             {
  1574|                                 returnValue = mres.Wait((int)(millisecondsTimeout - elapsedTimeTicks), cancellationToken);
  1575|                             }
  1576|                             finally
  1577|                             {
  1578|                                 if (notifyWhenUnblocked)
  1579|                                 {
  1580|                                     ThreadPool.NotifyThreadUnblocked();
  1581|                                 }
  1582|                             }
  1583|                         }
  1584|                     }
  1585| #pragma warning restore CA1416
  1586|                 }
  1587|                 finally
  1588|                 {
  1589|                     if (!IsCompleted) RemoveContinuation(mres);
  1590|                 }
  1591|             }
  1592|             return returnValue;
  1593|         }
  1594|         private bool SpinWait(int millisecondsTimeout)
  1595|         {
  1596|             if (IsCompleted) return true;
  1597|             if (millisecondsTimeout == 0)
  1598|             {
  1599|                 return false;
  1600|             }
  1601|             int spinCount = Threading.SpinWait.SpinCountforSpinBeforeWait;
  1602|             SpinWait spinner = default;
  1603|             while (spinner.Count < spinCount)
  1604|             {
  1605|                 spinner.SpinOnce(sleep1Threshold: -1);
  1606|                 if (IsCompleted)
  1607|                 {
  1608|                     return true;
  1609|                 }
  1610|             }
  1611|             return false;
  1612|         }
  1613|         internal void InternalCancel()
  1614|         {
  1615|             Debug.Assert((Options & (TaskCreationOptions)InternalTaskOptions.PromiseTask) == 0, "Task.InternalCancel() did not expect promise-style task");
  1616|             TaskSchedulerException? tse = null;
  1617|             bool popped = false;
  1618|             if ((m_stateFlags & (int)TaskStateFlags.Started) != 0)
  1619|             {
  1620|                 TaskScheduler? ts = m_taskScheduler;
  1621|                 try
  1622|                 {
  1623|                     popped = (ts != null) && ts.TryDequeue(this);
  1624|                 }
  1625|                 catch (Exception e)
  1626|                 {
  1627|                     tse = new TaskSchedulerException(e);
  1628|                 }
  1629|             }
  1630|             RecordInternalCancellationRequest();
  1631|             bool mustCleanup = false;
  1632|             if (popped)
  1633|             {
  1634|                 mustCleanup = AtomicStateUpdate((int)TaskStateFlags.Canceled, (int)TaskStateFlags.Canceled | (int)TaskStateFlags.DelegateInvoked);
  1635|             }
  1636|             else if ((m_stateFlags & (int)TaskStateFlags.Started) == 0)
  1637|             {
  1638|                 mustCleanup = AtomicStateUpdate((int)TaskStateFlags.Canceled,
  1639|                     (int)TaskStateFlags.Canceled | (int)TaskStateFlags.Started | (int)TaskStateFlags.RanToCompletion |
  1640|                     (int)TaskStateFlags.Faulted | (int)TaskStateFlags.DelegateInvoked);
  1641|             }
  1642|             if (mustCleanup)
  1643|             {
  1644|                 CancellationCleanupLogic();
  1645|             }
  1646|             if (tse != null)
  1647|             {
  1648|                 throw tse;
  1649|             }
  1650|         }
  1651|         internal void InternalCancelContinueWithInitialState()
  1652|         {
  1653|             const int IllegalFlags = (int)TaskStateFlags.Started | (int)TaskStateFlags.CompletedMask | (int)TaskStateFlags.DelegateInvoked;
  1654|             Debug.Assert((m_stateFlags & IllegalFlags) == 0, "The continuation was in an invalid state.");
  1655|             Debug.Assert((m_stateFlags & (int)TaskStateFlags.WaitingForActivation) != 0, "Expected continuation to be waiting for activation");
  1656|             Debug.Assert(m_contingentProperties is null || m_contingentProperties.m_cancellationToken == default);
  1657|             m_stateFlags |= (int)TaskStateFlags.Canceled; // no synchronization necessary, per above comment
  1658|             CancellationCleanupLogic();
  1659|         }
  1660|         internal void RecordInternalCancellationRequest()
  1661|         {
  1662|             EnsureContingentPropertiesInitialized().m_internalCancellationRequested = CANCELLATION_REQUESTED;
  1663|         }
  1664|         internal void RecordInternalCancellationRequest(CancellationToken tokenToRecord, object? cancellationException)
  1665|         {
  1666|             Debug.Assert((Options & (TaskCreationOptions)InternalTaskOptions.PromiseTask) != 0, "Task.RecordInternalCancellationRequest(CancellationToken) only valid for promise-style task");
  1667|             RecordInternalCancellationRequest();
  1668|             Debug.Assert(m_contingentProperties!.m_cancellationToken == default);
  1669|             if (tokenToRecord != default)
  1670|             {
  1671|                 m_contingentProperties.m_cancellationToken = tokenToRecord;
  1672|             }
  1673|             if (cancellationException != null)
  1674|             {
  1675| #if DEBUG
  1676|                 var oce = cancellationException as OperationCanceledException;
  1677|                 if (oce == null)
  1678|                 {
  1679|                     var edi = cancellationException as ExceptionDispatchInfo;
  1680|                     Debug.Assert(edi != null, "Expected either an OCE or an EDI");
  1681|                     oce = edi.SourceException as OperationCanceledException;
  1682|                     Debug.Assert(oce != null, "Expected EDI to contain an OCE");
  1683|                 }
  1684|                 Debug.Assert(oce.CancellationToken == tokenToRecord,
  1685|                                 "Expected OCE's token to match the provided token.");
  1686| #endif
  1687|                 AddException(cancellationException, representsCancellation: true);
  1688|             }
  1689|         }
  1690|         internal void CancellationCleanupLogic()
  1691|         {
  1692|             Debug.Assert((m_stateFlags & ((int)TaskStateFlags.Canceled | (int)TaskStateFlags.CompletionReserved)) != 0, "Task.CancellationCleanupLogic(): Task not canceled or reserved.");
  1693|             Interlocked.Exchange(ref m_stateFlags, m_stateFlags | (int)TaskStateFlags.Canceled);
  1694|             ContingentProperties? cp = Volatile.Read(ref m_contingentProperties);
  1695|             if (cp != null)
  1696|             {
  1697|                 cp.SetCompleted();
  1698|                 cp.UnregisterCancellationCallback();
  1699|             }
  1700|             if (TplEventSource.Log.IsEnabled())
  1701|                 TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Canceled);
  1702|             if (s_asyncDebuggingEnabled)
  1703|                 RemoveFromActiveTasks(this);
  1704|             FinishStageThree();
  1705|         }
  1706|         private void SetCancellationAcknowledged()
  1707|         {
  1708|             Debug.Assert(this == InternalCurrent, "SetCancellationAcknowledged() should only be called while this is still the current task");
  1709|             Debug.Assert(IsCancellationRequested, "SetCancellationAcknowledged() should not be called if the task's CT wasn't signaled");
  1710|             m_stateFlags |= (int)TaskStateFlags.CancellationAcknowledged;
  1711|         }
  1712|         internal bool TrySetResult()
  1713|         {
  1714|             if (AtomicStateUpdate(
  1715|                 (int)TaskStateFlags.CompletionReserved | (int)TaskStateFlags.RanToCompletion,
  1716|                 (int)TaskStateFlags.CompletionReserved | (int)TaskStateFlags.RanToCompletion | (int)TaskStateFlags.Faulted | (int)TaskStateFlags.Canceled))
  1717|             {
  1718|                 ContingentProperties? props = m_contingentProperties;
  1719|                 if (props != null)
  1720|                 {
  1721|                     NotifyParentIfPotentiallyAttachedTask();
  1722|                     props.SetCompleted();
  1723|                 }
  1724|                 FinishContinuations();
  1725|                 return true;
  1726|             }
  1727|             return false;
  1728|         }
  1729|         internal bool TrySetException(object exceptionObject)
  1730|         {
  1731|             Debug.Assert(exceptionObject != null, "Expected non-null exceptionObject argument");
  1732|             Debug.Assert(
  1733|                 (exceptionObject is Exception) || (exceptionObject is IEnumerable<Exception>) ||
  1734|                 (exceptionObject is ExceptionDispatchInfo) || (exceptionObject is IEnumerable<ExceptionDispatchInfo>),
  1735|                 "Expected exceptionObject to be either Exception, ExceptionDispatchInfo, or IEnumerable<> of one of those");
  1736|             bool returnValue = false;
  1737|             EnsureContingentPropertiesInitialized();
  1738|             if (AtomicStateUpdate(
  1739|                 (int)TaskStateFlags.CompletionReserved,
  1740|                 (int)TaskStateFlags.CompletionReserved | (int)TaskStateFlags.RanToCompletion | (int)TaskStateFlags.Faulted | (int)TaskStateFlags.Canceled))
  1741|             {
  1742|                 AddException(exceptionObject); // handles singleton exception or exception collection
  1743|                 Finish(false);
  1744|                 returnValue = true;
  1745|             }
  1746|             return returnValue;
  1747|         }
  1748|         internal bool TrySetCanceled(CancellationToken tokenToRecord)
  1749|         {
  1750|             return TrySetCanceled(tokenToRecord, null);
  1751|         }
  1752|         internal bool TrySetCanceled(CancellationToken tokenToRecord, object? cancellationException)
  1753|         {
  1754|             Debug.Assert(
  1755|                 cancellationException == null ||
  1756|                 cancellationException is OperationCanceledException ||
  1757|                 (cancellationException as ExceptionDispatchInfo)?.SourceException is OperationCanceledException,
  1758|                 "Expected null or an OperationCanceledException");
  1759|             bool returnValue = false;
  1760|             if (AtomicStateUpdate(
  1761|                 (int)TaskStateFlags.CompletionReserved,
  1762|                 (int)TaskStateFlags.CompletionReserved | (int)TaskStateFlags.Canceled | (int)TaskStateFlags.Faulted | (int)TaskStateFlags.RanToCompletion))
  1763|             {
  1764|                 RecordInternalCancellationRequest(tokenToRecord, cancellationException);
  1765|                 CancellationCleanupLogic(); // perform cancellation cleanup actions
  1766|                 returnValue = true;
  1767|             }
  1768|             return returnValue;
  1769|         }
  1770|         internal void FinishContinuations()
  1771|         {
  1772|             object? continuationObject = Interlocked.Exchange(ref m_continuationObject, s_taskCompletionSentinel);
  1773|             if (continuationObject != null)
  1774|             {
  1775|                 RunContinuations(continuationObject);
  1776|             }
  1777|         }
  1778|         private void RunContinuations(object continuationObject) // separated out of FinishContinuations to enable it to be inlined
  1779|         {
  1780|             Debug.Assert(continuationObject != null);
  1781|             TplEventSource log = TplEventSource.Log;
  1782|             bool etwIsEnabled = log.IsEnabled();
  1783|             if (etwIsEnabled)
  1784|                 log.TraceSynchronousWorkBegin(this.Id, CausalitySynchronousWork.CompletionNotification);
  1785|             bool canInlineContinuations =
  1786|                 (m_stateFlags & (int)TaskCreationOptions.RunContinuationsAsynchronously) == 0 &&
  1787|                 RuntimeHelpers.TryEnsureSufficientExecutionStack();
  1788|             switch (continuationObject)
  1789|             {
  1790|                 case IAsyncStateMachineBox stateMachineBox:
  1791|                     AwaitTaskContinuation.RunOrScheduleAction(stateMachineBox, canInlineContinuations);
  1792|                     LogFinishCompletionNotification();
  1793|                     return;
  1794|                 case Action action:
  1795|                     AwaitTaskContinuation.RunOrScheduleAction(action, canInlineContinuations);
  1796|                     LogFinishCompletionNotification();
  1797|                     return;
  1798|                 case TaskContinuation tc:
  1799|                     tc.Run(this, canInlineContinuations);
  1800|                     LogFinishCompletionNotification();
  1801|                     return;
  1802|                 case ITaskCompletionAction completionAction:
  1803|                     RunOrQueueCompletionAction(completionAction, canInlineContinuations);
  1804|                     LogFinishCompletionNotification();
  1805|                     return;
  1806|             }
  1807|             List<object?> continuations = (List<object?>)continuationObject;
  1808|             lock (continuations) { }
  1809|             int continuationCount = continuations.Count;
  1810|             if (canInlineContinuations)
  1811|             {
  1812|                 bool forceContinuationsAsync = false;
  1813|                 for (int i = 0; i < continuationCount; i++)
  1814|                 {
  1815|                     object? currentContinuation = continuations[i];
  1816|                     if (currentContinuation == null)
  1817|                     {
  1818|                         continue;
  1819|                     }
  1820|                     else if (currentContinuation is ContinueWithTaskContinuation stc)
  1821|                     {
  1822|                         if ((stc.m_options & TaskContinuationOptions.ExecuteSynchronously) == 0)
  1823|                         {
  1824|                             continuations[i] = null; // so that we can skip this later
  1825|                             if (etwIsEnabled)
  1826|                                 log.RunningContinuationList(Id, i, stc);
  1827|                             stc.Run(this, canInlineContinuationTask: false);
  1828|                         }
  1829|                     }
  1830|                     else if (!(currentContinuation is ITaskCompletionAction))
  1831|                     {
  1832|                         if (forceContinuationsAsync)
  1833|                         {
  1834|                             continuations[i] = null;
  1835|                             if (etwIsEnabled)
  1836|                                 log.RunningContinuationList(Id, i, currentContinuation);
  1837|                             switch (currentContinuation)
  1838|                             {
  1839|                                 case IAsyncStateMachineBox stateMachineBox:
  1840|                                     AwaitTaskContinuation.RunOrScheduleAction(stateMachineBox, allowInlining: false);
  1841|                                     break;
  1842|                                 case Action action:
  1843|                                     AwaitTaskContinuation.RunOrScheduleAction(action, allowInlining: false);
  1844|                                     break;
  1845|                                 default:
  1846|                                     Debug.Assert(currentContinuation is TaskContinuation);
  1847|                                     ((TaskContinuation)currentContinuation).Run(this, canInlineContinuationTask: false);
  1848|                                     break;
  1849|                             }
  1850|                         }
  1851|                         forceContinuationsAsync = true;
  1852|                     }
  1853|                 }
  1854|             }
  1855|             for (int i = 0; i < continuationCount; i++)
  1856|             {
  1857|                 object? currentContinuation = continuations[i];
  1858|                 if (currentContinuation == null)
  1859|                 {
  1860|                     continue;
  1861|                 }
  1862|                 continuations[i] = null; // to enable free'ing up memory earlier
  1863|                 if (etwIsEnabled)
  1864|                    log.RunningContinuationList(Id, i, currentContinuation);
  1865|                 switch (currentContinuation)
  1866|                 {
  1867|                     case IAsyncStateMachineBox stateMachineBox:
  1868|                         AwaitTaskContinuation.RunOrScheduleAction(stateMachineBox, canInlineContinuations);
  1869|                         break;
  1870|                     case Action action:
  1871|                         AwaitTaskContinuation.RunOrScheduleAction(action, canInlineContinuations);
  1872|                         break;
  1873|                     case TaskContinuation tc:
  1874|                         tc.Run(this, canInlineContinuations);
  1875|                         break;
  1876|                     default:
  1877|                         Debug.Assert(currentContinuation is ITaskCompletionAction);
  1878|                         RunOrQueueCompletionAction((ITaskCompletionAction)currentContinuation, canInlineContinuations);
  1879|                         break;
  1880|                 }
  1881|             }
  1882|             LogFinishCompletionNotification();
  1883|         }
  1884|         private void RunOrQueueCompletionAction(ITaskCompletionAction completionAction, bool allowInlining)
  1885|         {
  1886|             if (allowInlining || !completionAction.InvokeMayRunArbitraryCode)
  1887|             {
  1888|                 completionAction.Invoke(this);
  1889|             }
  1890|             else
  1891|             {
  1892|                 ThreadPool.UnsafeQueueUserWorkItemInternal(new CompletionActionInvoker(completionAction, this), preferLocal: true);
  1893|             }
  1894|         }
  1895|         private static void LogFinishCompletionNotification()
  1896|         {
  1897|             if (TplEventSource.Log.IsEnabled())
  1898|                 TplEventSource.Log.TraceSynchronousWorkEnd(CausalitySynchronousWork.CompletionNotification);
  1899|         }
  1900|         #region Continuation methods
  1901|         #region Action<Task> continuation
  1902|         public Task ContinueWith(Action<Task> continuationAction)
  1903|         {
  1904|             return ContinueWith(continuationAction, TaskScheduler.Current, default, TaskContinuationOptions.None);
  1905|         }
  1906|         public Task ContinueWith(Action<Task> continuationAction, CancellationToken cancellationToken)
  1907|         {
  1908|             return ContinueWith(continuationAction, TaskScheduler.Current, cancellationToken, TaskContinuationOptions.None);
  1909|         }
  1910|         public Task ContinueWith(Action<Task> continuationAction, TaskScheduler scheduler)
  1911|         {
  1912|             return ContinueWith(continuationAction, scheduler, default, TaskContinuationOptions.None);
  1913|         }
  1914|         public Task ContinueWith(Action<Task> continuationAction, TaskContinuationOptions continuationOptions)
  1915|         {
  1916|             return ContinueWith(continuationAction, TaskScheduler.Current, default, continuationOptions);
  1917|         }
  1918|         public Task ContinueWith(Action<Task> continuationAction, CancellationToken cancellationToken,
  1919|                                  TaskContinuationOptions continuationOptions, TaskScheduler scheduler)
  1920|         {
  1921|             return ContinueWith(continuationAction, scheduler, cancellationToken, continuationOptions);
  1922|         }
  1923|         private Task ContinueWith(Action<Task> continuationAction, TaskScheduler scheduler,
  1924|             CancellationToken cancellationToken, TaskContinuationOptions continuationOptions)
  1925|         {
  1926|             if (continuationAction == null)
  1927|             {
  1928|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.continuationAction);
  1929|             }
  1930|             if (scheduler == null)
  1931|             {
  1932|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
  1933|             }
  1934|             CreationOptionsFromContinuationOptions(continuationOptions, out TaskCreationOptions creationOptions, out InternalTaskOptions internalOptions);
  1935|             Task continuationTask = new ContinuationTaskFromTask(
  1936|                 this, continuationAction, null,
  1937|                 creationOptions, internalOptions
  1938|             );
  1939|             ContinueWithCore(continuationTask, scheduler, cancellationToken, continuationOptions);
  1940|             return continuationTask;
  1941|         }
  1942|         #endregion
  1943|         #region Action<Task, Object> continuation
  1944|         public Task ContinueWith(Action<Task, object?> continuationAction, object? state)
  1945|         {
  1946|             return ContinueWith(continuationAction, state, TaskScheduler.Current, default, TaskContinuationOptions.None);
  1947|         }
  1948|         public Task ContinueWith(Action<Task, object?> continuationAction, object? state, CancellationToken cancellationToken)
  1949|         {
  1950|             return ContinueWith(continuationAction, state, TaskScheduler.Current, cancellationToken, TaskContinuationOptions.None);
  1951|         }
  1952|         public Task ContinueWith(Action<Task, object?> continuationAction, object? state, TaskScheduler scheduler)
  1953|         {
  1954|             return ContinueWith(continuationAction, state, scheduler, default, TaskContinuationOptions.None);
  1955|         }
  1956|         public Task ContinueWith(Action<Task, object?> continuationAction, object? state, TaskContinuationOptions continuationOptions)
  1957|         {
  1958|             return ContinueWith(continuationAction, state, TaskScheduler.Current, default, continuationOptions);
  1959|         }
  1960|         public Task ContinueWith(Action<Task, object?> continuationAction, object? state, CancellationToken cancellationToken,
  1961|                                  TaskContinuationOptions continuationOptions, TaskScheduler scheduler)
  1962|         {
  1963|             return ContinueWith(continuationAction, state, scheduler, cancellationToken, continuationOptions);
  1964|         }
  1965|         private Task ContinueWith(Action<Task, object?> continuationAction, object? state, TaskScheduler scheduler,
  1966|             CancellationToken cancellationToken, TaskContinuationOptions continuationOptions)
  1967|         {
  1968|             if (continuationAction == null)
  1969|             {
  1970|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.continuationAction);
  1971|             }
  1972|             if (scheduler == null)
  1973|             {
  1974|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
  1975|             }
  1976|             CreationOptionsFromContinuationOptions(continuationOptions, out TaskCreationOptions creationOptions, out InternalTaskOptions internalOptions);
  1977|             Task continuationTask = new ContinuationTaskFromTask(
  1978|                 this, continuationAction, state,
  1979|                 creationOptions, internalOptions
  1980|             );
  1981|             ContinueWithCore(continuationTask, scheduler, cancellationToken, continuationOptions);
  1982|             return continuationTask;
  1983|         }
  1984|         #endregion
  1985|         #region Func<Task, TResult> continuation
  1986|         public Task<TResult> ContinueWith<TResult>(Func<Task, TResult> continuationFunction)
  1987|         {
  1988|             return ContinueWith(continuationFunction, TaskScheduler.Current, default,
  1989|                 TaskContinuationOptions.None);
  1990|         }
  1991|         public Task<TResult> ContinueWith<TResult>(Func<Task, TResult> continuationFunction, CancellationToken cancellationToken)
  1992|         {
  1993|             return ContinueWith(continuationFunction, TaskScheduler.Current, cancellationToken, TaskContinuationOptions.None);
  1994|         }
  1995|         public Task<TResult> ContinueWith<TResult>(Func<Task, TResult> continuationFunction, TaskScheduler scheduler)
  1996|         {
  1997|             return ContinueWith(continuationFunction, scheduler, default, TaskContinuationOptions.None);
  1998|         }
  1999|         public Task<TResult> ContinueWith<TResult>(Func<Task, TResult> continuationFunction, TaskContinuationOptions continuationOptions)
  2000|         {
  2001|             return ContinueWith(continuationFunction, TaskScheduler.Current, default, continuationOptions);
  2002|         }
  2003|         public Task<TResult> ContinueWith<TResult>(Func<Task, TResult> continuationFunction, CancellationToken cancellationToken,
  2004|                                                    TaskContinuationOptions continuationOptions, TaskScheduler scheduler)
  2005|         {
  2006|             return ContinueWith(continuationFunction, scheduler, cancellationToken, continuationOptions);
  2007|         }
  2008|         private Task<TResult> ContinueWith<TResult>(Func<Task, TResult> continuationFunction, TaskScheduler scheduler,
  2009|             CancellationToken cancellationToken, TaskContinuationOptions continuationOptions)
  2010|         {
  2011|             if (continuationFunction == null)
  2012|             {
  2013|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.continuationFunction);
  2014|             }
  2015|             if (scheduler == null)
  2016|             {
  2017|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
  2018|             }
  2019|             CreationOptionsFromContinuationOptions(continuationOptions, out TaskCreationOptions creationOptions, out InternalTaskOptions internalOptions);
  2020|             Task<TResult> continuationTask = new ContinuationResultTaskFromTask<TResult>(
  2021|                 this, continuationFunction, null,
  2022|                 creationOptions, internalOptions
  2023|             );
  2024|             ContinueWithCore(continuationTask, scheduler, cancellationToken, continuationOptions);
  2025|             return continuationTask;
  2026|         }
  2027|         #endregion
  2028|         #region Func<Task, Object, TResult> continuation
  2029|         public Task<TResult> ContinueWith<TResult>(Func<Task, object?, TResult> continuationFunction, object? state)
  2030|         {
  2031|             return ContinueWith(continuationFunction, state, TaskScheduler.Current, default,
  2032|                 TaskContinuationOptions.None);
  2033|         }
  2034|         public Task<TResult> ContinueWith<TResult>(Func<Task, object?, TResult> continuationFunction, object? state, CancellationToken cancellationToken)
  2035|         {
  2036|             return ContinueWith(continuationFunction, state, TaskScheduler.Current, cancellationToken, TaskContinuationOptions.None);
  2037|         }
  2038|         public Task<TResult> ContinueWith<TResult>(Func<Task, object?, TResult> continuationFunction, object? state, TaskScheduler scheduler)
  2039|         {
  2040|             return ContinueWith(continuationFunction, state, scheduler, default, TaskContinuationOptions.None);
  2041|         }
  2042|         public Task<TResult> ContinueWith<TResult>(Func<Task, object?, TResult> continuationFunction, object? state, TaskContinuationOptions continuationOptions)
  2043|         {
  2044|             return ContinueWith(continuationFunction, state, TaskScheduler.Current, default, continuationOptions);
  2045|         }
  2046|         public Task<TResult> ContinueWith<TResult>(Func<Task, object?, TResult> continuationFunction, object? state, CancellationToken cancellationToken,
  2047|                                                    TaskContinuationOptions continuationOptions, TaskScheduler scheduler)
  2048|         {
  2049|             return ContinueWith(continuationFunction, state, scheduler, cancellationToken, continuationOptions);
  2050|         }
  2051|         private Task<TResult> ContinueWith<TResult>(Func<Task, object?, TResult> continuationFunction, object? state, TaskScheduler scheduler,
  2052|             CancellationToken cancellationToken, TaskContinuationOptions continuationOptions)
  2053|         {
  2054|             if (continuationFunction == null)
  2055|             {
  2056|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.continuationFunction);
  2057|             }
  2058|             if (scheduler == null)
  2059|             {
  2060|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
  2061|             }
  2062|             CreationOptionsFromContinuationOptions(continuationOptions, out TaskCreationOptions creationOptions, out InternalTaskOptions internalOptions);
  2063|             Task<TResult> continuationTask = new ContinuationResultTaskFromTask<TResult>(
  2064|                 this, continuationFunction, state,
  2065|                 creationOptions, internalOptions
  2066|             );
  2067|             ContinueWithCore(continuationTask, scheduler, cancellationToken, continuationOptions);
  2068|             return continuationTask;
  2069|         }
  2070|         #endregion
  2071|         internal static void CreationOptionsFromContinuationOptions(
  2072|             TaskContinuationOptions continuationOptions,
  2073|             out TaskCreationOptions creationOptions,
  2074|             out InternalTaskOptions internalOptions)
  2075|         {
  2076|             const TaskContinuationOptions NotOnAnything =
  2077|                 TaskContinuationOptions.NotOnCanceled |
  2078|                 TaskContinuationOptions.NotOnFaulted |
  2079|                 TaskContinuationOptions.NotOnRanToCompletion;
  2080|             const TaskContinuationOptions CreationOptionsMask =
  2081|                 TaskContinuationOptions.PreferFairness |
  2082|                 TaskContinuationOptions.LongRunning |
  2083|                 TaskContinuationOptions.DenyChildAttach |
  2084|                 TaskContinuationOptions.HideScheduler |
  2085|                 TaskContinuationOptions.AttachedToParent |
  2086|                 TaskContinuationOptions.RunContinuationsAsynchronously;
  2087|             const TaskContinuationOptions IllegalMask = TaskContinuationOptions.ExecuteSynchronously | TaskContinuationOptions.LongRunning;
  2088|             if ((continuationOptions & IllegalMask) == IllegalMask)
  2089|             {
  2090|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.continuationOptions, ExceptionResource.Task_ContinueWith_ESandLR);
  2091|             }
  2092|             if ((continuationOptions &
  2093|                 ~(CreationOptionsMask | NotOnAnything |
  2094|                     TaskContinuationOptions.LazyCancellation | TaskContinuationOptions.ExecuteSynchronously)) != 0)
  2095|             {
  2096|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.continuationOptions);
  2097|             }
  2098|             if ((continuationOptions & NotOnAnything) == NotOnAnything)
  2099|             {
  2100|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.continuationOptions, ExceptionResource.Task_ContinueWith_NotOnAnything);
  2101|             }
  2102|             creationOptions = (TaskCreationOptions)(continuationOptions & CreationOptionsMask);
  2103|             internalOptions = (continuationOptions & TaskContinuationOptions.LazyCancellation) != 0 ?
  2104|                 InternalTaskOptions.ContinuationTask | InternalTaskOptions.LazyCancellation :
  2105|                 InternalTaskOptions.ContinuationTask;
  2106|         }
  2107|         internal void ContinueWithCore(Task continuationTask,
  2108|                                        TaskScheduler scheduler,
  2109|                                        CancellationToken cancellationToken,
  2110|                                        TaskContinuationOptions options)
  2111|         {
  2112|             Debug.Assert(continuationTask != null, "Task.ContinueWithCore(): null continuationTask");
  2113|             Debug.Assert(scheduler != null, "Task.ContinueWithCore(): null scheduler");
  2114|             Debug.Assert(!continuationTask.IsCompleted, "Did not expect continuationTask to be completed");
  2115|             var continuation = new ContinueWithTaskContinuation(continuationTask, options, scheduler);
  2116|             if (cancellationToken.CanBeCanceled)
  2117|             {
  2118|                 if (IsCompleted || cancellationToken.IsCancellationRequested)
  2119|                 {
  2120|                     continuationTask.AssignCancellationToken(cancellationToken, null, null);
  2121|                 }
  2122|                 else
  2123|                 {
  2124|                     continuationTask.AssignCancellationToken(cancellationToken, this, continuation);
  2125|                 }
  2126|             }
  2127|             if (!continuationTask.IsCompleted)
  2128|             {
  2129|                 if ((this.Options & (TaskCreationOptions)InternalTaskOptions.PromiseTask) != 0 &&
  2130|                     !(this is ITaskCompletionAction))
  2131|                 {
  2132|                     TplEventSource log = TplEventSource.Log;
  2133|                     if (log.IsEnabled())
  2134|                     {
  2135|                         log.AwaitTaskContinuationScheduled(TaskScheduler.Current.Id, CurrentId ?? 0, continuationTask.Id);
  2136|                     }
  2137|                 }
  2138|                 bool continuationQueued = AddTaskContinuation(continuation, addBeforeOthers: false);
  2139|                 if (!continuationQueued) continuation.Run(this, canInlineContinuationTask: true);
  2140|             }
  2141|         }
  2142|         #endregion
  2143|         internal void AddCompletionAction(ITaskCompletionAction action, bool addBeforeOthers = false)
  2144|         {
  2145|             if (!AddTaskContinuation(action, addBeforeOthers))
  2146|                 action.Invoke(this); // run the action directly if we failed to queue the continuation (i.e., the task completed)
  2147|         }
  2148|         private bool AddTaskContinuationComplex(object tc, bool addBeforeOthers)
  2149|         {
  2150|             Debug.Assert(tc != null, "Expected non-null tc object in AddTaskContinuationComplex");
  2151|             object? oldValue = m_continuationObject;
  2152|             if ((oldValue != s_taskCompletionSentinel) && (!(oldValue is List<object?>)))
  2153|             {
  2154|                 Interlocked.CompareExchange(ref m_continuationObject, new List<object?> { oldValue }, oldValue);
  2155|             }
  2156|             List<object?>? list = m_continuationObject as List<object?>;
  2157|             Debug.Assert((list != null) || (m_continuationObject == s_taskCompletionSentinel),
  2158|                 "Expected m_continuationObject to be list or sentinel");
  2159|             if (list != null)
  2160|             {
  2161|                 lock (list)
  2162|                 {
  2163|                     if (m_continuationObject != s_taskCompletionSentinel)
  2164|                     {
  2165|                         if (list.Count == list.Capacity)
  2166|                         {
  2167|                             list.RemoveAll(l => l == null);
  2168|                         }
  2169|                         if (addBeforeOthers)
  2170|                             list.Insert(0, tc);
  2171|                         else
  2172|                             list.Add(tc);
  2173|                         return true; // continuation successfully queued, so return true.
  2174|                     }
  2175|                 }
  2176|             }
  2177|             return false;
  2178|         }
  2179|         private bool AddTaskContinuation(object tc, bool addBeforeOthers)
  2180|         {
  2181|             Debug.Assert(tc != null);
  2182|             if (IsCompleted) return false;
  2183|             if ((m_continuationObject != null) || (Interlocked.CompareExchange(ref m_continuationObject, tc, null) != null))
  2184|             {
  2185|                 return AddTaskContinuationComplex(tc, addBeforeOthers);
  2186|             }
  2187|             else return true;
  2188|         }
  2189|         internal void RemoveContinuation(object continuationObject) // could be TaskContinuation or Action<Task>
  2190|         {
  2191|             object? continuationsLocalRef = m_continuationObject;
  2192|             if (continuationsLocalRef == s_taskCompletionSentinel) return;
  2193|             List<object?>? continuationsLocalListRef = continuationsLocalRef as List<object?>;
  2194|             if (continuationsLocalListRef is null)
  2195|             {
  2196|                 if (Interlocked.CompareExchange(ref m_continuationObject, new List<object?>(), continuationObject) != continuationObject)
  2197|                 {
  2198|                     continuationsLocalListRef = m_continuationObject as List<object?>;
  2199|                 }
  2200|                 else
  2201|                 {
  2202|                     return;
  2203|                 }
  2204|             }
  2205|             if (continuationsLocalListRef != null)
  2206|             {
  2207|                 lock (continuationsLocalListRef)
  2208|                 {
  2209|                     if (m_continuationObject == s_taskCompletionSentinel) return;
  2210|                     int index = continuationsLocalListRef.IndexOf(continuationObject);
  2211|                     if (index >= 0)
  2212|                     {
  2213|                         continuationsLocalListRef[index] = null;
  2214|                     }
  2215|                 }
  2216|             }
  2217|         }
  2218|         [UnsupportedOSPlatform("browser")]
  2219|         [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
  2220|         public static void WaitAll(params Task[] tasks)
  2221|         {
  2222| #if DEBUG
  2223|             bool waitResult =
  2224| #endif
  2225|             WaitAllCore(tasks, Timeout.Infinite, default);
  2226| #if DEBUG
  2227|             Debug.Assert(waitResult, "expected wait to succeed");
  2228| #endif
  2229|         }
  2230|         [UnsupportedOSPlatform("browser")]
  2231|         [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
  2232|         public static bool WaitAll(Task[] tasks, TimeSpan timeout)
  2233|         {
  2234|             long totalMilliseconds = (long)timeout.TotalMilliseconds;
  2235|             if (totalMilliseconds < -1 || totalMilliseconds > int.MaxValue)
  2236|             {
  2237|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.timeout);
  2238|             }
  2239|             return WaitAllCore(tasks, (int)totalMilliseconds, default);
  2240|         }
  2241|         [UnsupportedOSPlatform("browser")]
  2242|         [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
  2243|         public static bool WaitAll(Task[] tasks, int millisecondsTimeout)
  2244|         {
  2245|             return WaitAllCore(tasks, millisecondsTimeout, default);
  2246|         }
  2247|         [UnsupportedOSPlatform("browser")]
  2248|         [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
  2249|         public static void WaitAll(Task[] tasks, CancellationToken cancellationToken)
  2250|         {
  2251|             WaitAllCore(tasks, Timeout.Infinite, cancellationToken);
  2252|         }
  2253|         [UnsupportedOSPlatform("browser")]
  2254|         [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
  2255|         public static bool WaitAll(Task[] tasks, int millisecondsTimeout, CancellationToken cancellationToken) =>
  2256|             WaitAllCore(tasks, millisecondsTimeout, cancellationToken);
  2257|         [UnsupportedOSPlatform("browser")]
  2258|         private static bool WaitAllCore(Task[] tasks, int millisecondsTimeout, CancellationToken cancellationToken)
  2259|         {
  2260|             if (tasks == null)
  2261|             {
  2262|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
  2263|             }
  2264|             if (millisecondsTimeout < -1)
  2265|             {
  2266|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.millisecondsTimeout);
  2267|             }
  2268|             cancellationToken.ThrowIfCancellationRequested(); // early check before we make any allocations
  2269|             List<Exception>? exceptions = null;
  2270|             List<Task>? waitedOnTaskList = null;
  2271|             List<Task>? notificationTasks = null;
  2272|             bool exceptionSeen = false, cancellationSeen = false;
  2273|             bool returnValue = true;
  2274|             for (int i = tasks.Length - 1; i >= 0; i--)
  2275|             {
  2276|                 Task task = tasks[i];
  2277|                 if (task == null)
  2278|                 {
  2279|                     ThrowHelper.ThrowArgumentException(ExceptionResource.Task_WaitMulti_NullTask, ExceptionArgument.tasks);
  2280|                 }
  2281|                 bool taskIsCompleted = task.IsCompleted;
  2282|                 if (!taskIsCompleted)
  2283|                 {
  2284|                     if (millisecondsTimeout != Timeout.Infinite || cancellationToken.CanBeCanceled)
  2285|                     {
  2286|                         AddToList(task, ref waitedOnTaskList, initSize: tasks.Length);
  2287|                     }
  2288|                     else
  2289|                     {
  2290|                         taskIsCompleted = task.WrappedTryRunInline() && task.IsCompleted; // A successful TryRunInline doesn't guarantee completion
  2291|                         if (!taskIsCompleted) AddToList(task, ref waitedOnTaskList, initSize: tasks.Length);
  2292|                     }
  2293|                 }
  2294|                 if (taskIsCompleted)
  2295|                 {
  2296|                     if (task.IsFaulted) exceptionSeen = true;
  2297|                     else if (task.IsCanceled) cancellationSeen = true;
  2298|                     if (task.IsWaitNotificationEnabled) AddToList(task, ref notificationTasks, initSize: 1);
  2299|                 }
  2300|             }
  2301|             if (waitedOnTaskList != null)
  2302|             {
  2303|                 returnValue = WaitAllBlockingCore(waitedOnTaskList, millisecondsTimeout, cancellationToken);
  2304|                 if (returnValue)
  2305|                 {
  2306|                     foreach (Task task in waitedOnTaskList)
  2307|                     {
  2308|                         if (task.IsFaulted) exceptionSeen = true;
  2309|                         else if (task.IsCanceled) cancellationSeen = true;
  2310|                         if (task.IsWaitNotificationEnabled) AddToList(task, ref notificationTasks, initSize: 1);
  2311|                     }
  2312|                 }
  2313|                 GC.KeepAlive(tasks);
  2314|             }
  2315|             if (returnValue && notificationTasks != null)
  2316|             {
  2317|                 foreach (Task task in notificationTasks)
  2318|                 {
  2319|                     if (task.NotifyDebuggerOfWaitCompletionIfNecessary()) break;
  2320|                 }
  2321|             }
  2322|             if (returnValue && (exceptionSeen || cancellationSeen))
  2323|             {
  2324|                 if (!exceptionSeen) cancellationToken.ThrowIfCancellationRequested();
  2325|                 foreach (Task task in tasks) AddExceptionsForCompletedTask(ref exceptions, task);
  2326|                 Debug.Assert(exceptions != null, "Should have seen at least one exception");
  2327|                 ThrowHelper.ThrowAggregateException(exceptions);
  2328|             }
  2329|             return returnValue;
  2330|         }
  2331|         private static void AddToList<T>(T item, ref List<T>? list, int initSize)
  2332|         {
  2333|             list ??= new List<T>(initSize);
  2334|             list.Add(item);
  2335|         }
  2336|         [UnsupportedOSPlatform("browser")]
  2337|         private static bool WaitAllBlockingCore(List<Task> tasks, int millisecondsTimeout, CancellationToken cancellationToken)
  2338|         {
  2339|             Debug.Assert(tasks != null, "Expected a non-null list of tasks");
  2340|             Debug.Assert(tasks.Count > 0, "Expected at least one task");
  2341|             bool waitCompleted = false;
  2342|             var mres = new SetOnCountdownMres(tasks.Count);
  2343|             try
  2344|             {
  2345|                 foreach (Task task in tasks)
  2346|                 {
  2347|                     task.AddCompletionAction(mres, addBeforeOthers: true);
  2348|                 }
  2349|                 waitCompleted = mres.Wait(millisecondsTimeout, cancellationToken);
  2350|             }
  2351|             finally
  2352|             {
  2353|                 if (!waitCompleted)
  2354|                 {
  2355|                     foreach (Task task in tasks)
  2356|                     {
  2357|                         if (!task.IsCompleted) task.RemoveContinuation(mres);
  2358|                     }
  2359|                 }
  2360|             }
  2361|             return waitCompleted;
  2362|         }
  2363|         private sealed class SetOnCountdownMres : ManualResetEventSlim, ITaskCompletionAction
  2364|         {
  2365|             private int _count;
  2366|             internal SetOnCountdownMres(int count)
  2367|             {
  2368|                 Debug.Assert(count > 0, "Expected count > 0");
  2369|                 _count = count;
  2370|             }
  2371|             public void Invoke(Task completingTask)
  2372|             {
  2373|                 if (Interlocked.Decrement(ref _count) == 0) Set();
  2374|                 Debug.Assert(_count >= 0, "Count should never go below 0");
  2375|             }
  2376|             public bool InvokeMayRunArbitraryCode => false;
  2377|         }
  2378|         internal static void AddExceptionsForCompletedTask(ref List<Exception>? exceptions, Task t)
  2379|         {
  2380|             AggregateException? ex = t.GetExceptions(true);
  2381|             if (ex != null)
  2382|             {
  2383|                 t.UpdateExceptionObservedStatus();
  2384|                 exceptions ??= new List<Exception>(ex.InnerExceptionCount);
  2385|                 exceptions.AddRange(ex.InternalInnerExceptions);
  2386|             }
  2387|         }
  2388|         [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
  2389|         public static int WaitAny(params Task[] tasks)
  2390|         {
  2391|             int waitResult = WaitAnyCore(tasks, Timeout.Infinite, default);
  2392|             Debug.Assert(tasks.Length == 0 || waitResult != -1, "expected wait to succeed");
  2393|             return waitResult;
  2394|         }
  2395|         [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
  2396|         public static int WaitAny(Task[] tasks, TimeSpan timeout)
  2397|         {
  2398|             long totalMilliseconds = (long)timeout.TotalMilliseconds;
  2399|             if (totalMilliseconds < -1 || totalMilliseconds > int.MaxValue)
  2400|             {
  2401|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.timeout);
  2402|             }
  2403|             return WaitAnyCore(tasks, (int)totalMilliseconds, default);
  2404|         }
  2405|         [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
  2406|         public static int WaitAny(Task[] tasks, CancellationToken cancellationToken)
  2407|         {
  2408|             return WaitAnyCore(tasks, Timeout.Infinite, cancellationToken);
  2409|         }
  2410|         [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
  2411|         public static int WaitAny(Task[] tasks, int millisecondsTimeout)
  2412|         {
  2413|             return WaitAnyCore(tasks, millisecondsTimeout, default);
  2414|         }
  2415|         [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
  2416|         public static int WaitAny(Task[] tasks, int millisecondsTimeout, CancellationToken cancellationToken) =>
  2417|             WaitAnyCore(tasks, millisecondsTimeout, cancellationToken);
  2418|         private static int WaitAnyCore(Task[] tasks, int millisecondsTimeout, CancellationToken cancellationToken)
  2419|         {
  2420|             if (tasks == null)
  2421|             {
  2422|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
  2423|             }
  2424|             if (millisecondsTimeout < -1)
  2425|             {
  2426|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.millisecondsTimeout);
  2427|             }
  2428|             cancellationToken.ThrowIfCancellationRequested(); // early check before we make any allocations
  2429|             int signaledTaskIndex = -1;
  2430|             for (int taskIndex = 0; taskIndex < tasks.Length; taskIndex++)
  2431|             {
  2432|                 Task task = tasks[taskIndex];
  2433|                 if (task == null)
  2434|                 {
  2435|                     ThrowHelper.ThrowArgumentException(ExceptionResource.Task_WaitMulti_NullTask, ExceptionArgument.tasks);
  2436|                 }
  2437|                 if (signaledTaskIndex == -1 && task.IsCompleted)
  2438|                 {
  2439|                     signaledTaskIndex = taskIndex;
  2440|                 }
  2441|             }
  2442|             if (signaledTaskIndex == -1 && tasks.Length != 0)
  2443|             {
  2444|                 Task<Task> firstCompleted = TaskFactory.CommonCWAnyLogic(tasks, isSyncBlocking: true);
  2445|                 bool waitCompleted = firstCompleted.Wait(millisecondsTimeout, cancellationToken);
  2446|                 if (waitCompleted)
  2447|                 {
  2448|                     Debug.Assert(firstCompleted.Status == TaskStatus.RanToCompletion);
  2449|                     signaledTaskIndex = Array.IndexOf(tasks, firstCompleted.Result);
  2450|                     Debug.Assert(signaledTaskIndex >= 0);
  2451|                 }
  2452|                 else
  2453|                 {
  2454|                     TaskFactory.CommonCWAnyLogicCleanup(firstCompleted);
  2455|                 }
  2456|             }
  2457|             GC.KeepAlive(tasks);
  2458|             return signaledTaskIndex;
  2459|         }
  2460|         #region FromResult / FromException / FromCanceled
  2461|         [MethodImpl(MethodImplOptions.AggressiveInlining)] // method looks long, but for a given TResult it results in a relatively small amount of asm
  2462|         public static unsafe Task<TResult> FromResult<TResult>(TResult result)
  2463|         {
  2464| #pragma warning disable 8500 // address of / sizeof of managed types
  2465|             if (result is null)
  2466|             {
  2467|                 return Task<TResult>.s_defaultResultTask;
  2468|             }
  2469|             if (typeof(TResult) == typeof(bool)) // only the relevant branches are kept for each value-type generic instantiation
  2470|             {
  2471|                 Task<bool> task = *(bool*)&result ? TaskCache.s_trueTask : TaskCache.s_falseTask;
  2472|                 return *(Task<TResult>*)&task;
  2473|             }
  2474|             if (typeof(TResult) == typeof(int))
  2475|             {
  2476|                 int value = *(int*)&result;
  2477|                 if ((uint)(value - TaskCache.InclusiveInt32Min) < (TaskCache.ExclusiveInt32Max - TaskCache.InclusiveInt32Min))
  2478|                 {
  2479|                     Task<int> task = TaskCache.s_int32Tasks[value - TaskCache.InclusiveInt32Min];
  2480|                     return *(Task<TResult>*)&task;
  2481|                 }
  2482|             }
  2483|             else if (!RuntimeHelpers.IsReferenceOrContainsReferences<TResult>())
  2484|             {
  2485|                 if ((sizeof(TResult) == sizeof(byte) && *(byte*)&result == default(byte)) ||
  2486|                     (sizeof(TResult) == sizeof(ushort) && *(ushort*)&result == default(ushort)) ||
  2487|                     (sizeof(TResult) == sizeof(uint) && *(uint*)&result == default) ||
  2488|                     (sizeof(TResult) == sizeof(ulong) && *(ulong*)&result == default) ||
  2489|                     (sizeof(TResult) == sizeof(UInt128) && *(UInt128*)&result == default))
  2490|                 {
  2491|                     return Task<TResult>.s_defaultResultTask;
  2492|                 }
  2493|             }
  2494|             return new Task<TResult>(result);
  2495| #pragma warning restore 8500
  2496|         }
  2497|         public static Task FromException(Exception exception)
  2498|         {
  2499|             if (exception == null) ThrowHelper.ThrowArgumentNullException(ExceptionArgument.exception);
  2500|             var task = new Task();
  2501|             bool succeeded = task.TrySetException(exception);
  2502|             Debug.Assert(succeeded, "This should always succeed on a new task.");
  2503|             return task;
  2504|         }
  2505|         public static Task<TResult> FromException<TResult>(Exception exception)
  2506|         {
  2507|             if (exception == null) ThrowHelper.ThrowArgumentNullException(ExceptionArgument.exception);
  2508|             var task = new Task<TResult>();
  2509|             bool succeeded = task.TrySetException(exception);
  2510|             Debug.Assert(succeeded, "This should always succeed on a new task.");
  2511|             return task;
  2512|         }
  2513|         public static Task FromCanceled(CancellationToken cancellationToken)
  2514|         {
  2515|             if (!cancellationToken.IsCancellationRequested)
  2516|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.cancellationToken);
  2517|             return new Task(true, TaskCreationOptions.None, cancellationToken);
  2518|         }
  2519|         public static Task<TResult> FromCanceled<TResult>(CancellationToken cancellationToken)
  2520|         {
  2521|             if (!cancellationToken.IsCancellationRequested)
  2522|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.cancellationToken);
  2523|             return new Task<TResult>(true, default, TaskCreationOptions.None, cancellationToken);
  2524|         }
  2525|         internal static Task FromCanceled(OperationCanceledException exception)
  2526|         {
  2527|             Debug.Assert(exception != null);
  2528|             var task = new Task();
  2529|             bool succeeded = task.TrySetCanceled(exception.CancellationToken, exception);
  2530|             Debug.Assert(succeeded, "This should always succeed on a new task.");
  2531|             return task;
  2532|         }
  2533|         internal static Task<TResult> FromCanceled<TResult>(OperationCanceledException exception)
  2534|         {
  2535|             Debug.Assert(exception != null);
  2536|             var task = new Task<TResult>();
  2537|             bool succeeded = task.TrySetCanceled(exception.CancellationToken, exception);
  2538|             Debug.Assert(succeeded, "This should always succeed on a new task.");
  2539|             return task;
  2540|         }
  2541|         #endregion
  2542|         #region Run methods
  2543|         public static Task Run(Action action)
  2544|         {
  2545|             return InternalStartNew(null, action, null, default, TaskScheduler.Default,
  2546|                 TaskCreationOptions.DenyChildAttach, InternalTaskOptions.None);
  2547|         }
  2548|         public static Task Run(Action action, CancellationToken cancellationToken)
  2549|         {
  2550|             return InternalStartNew(null, action, null, cancellationToken, TaskScheduler.Default,
  2551|                 TaskCreationOptions.DenyChildAttach, InternalTaskOptions.None);
  2552|         }
  2553|         public static Task<TResult> Run<TResult>(Func<TResult> function)
  2554|         {
  2555|             return Task<TResult>.StartNew(null, function, default,
  2556|                 TaskCreationOptions.DenyChildAttach, InternalTaskOptions.None, TaskScheduler.Default);
  2557|         }
  2558|         public static Task<TResult> Run<TResult>(Func<TResult> function, CancellationToken cancellationToken)
  2559|         {
  2560|             return Task<TResult>.StartNew(null, function, cancellationToken,
  2561|                 TaskCreationOptions.DenyChildAttach, InternalTaskOptions.None, TaskScheduler.Default);
  2562|         }
  2563|         public static Task Run(Func<Task?> function)
  2564|         {
  2565|             return Run(function, default);
  2566|         }
  2567|         public static Task Run(Func<Task?> function, CancellationToken cancellationToken)
  2568|         {
  2569|             if (function == null) ThrowHelper.ThrowArgumentNullException(ExceptionArgument.function);
  2570|             if (cancellationToken.IsCancellationRequested)
  2571|                 return FromCanceled(cancellationToken);
  2572|             Task<Task?> task1 = Task<Task?>.Factory.StartNew(function, cancellationToken, TaskCreationOptions.DenyChildAttach, TaskScheduler.Default);
  2573|             UnwrapPromise<VoidTaskResult> promise = new UnwrapPromise<VoidTaskResult>(task1, lookForOce: true);
  2574|             return promise;
  2575|         }
  2576|         public static Task<TResult> Run<TResult>(Func<Task<TResult>?> function)
  2577|         {
  2578|             return Run(function, default);
  2579|         }
  2580|         public static Task<TResult> Run<TResult>(Func<Task<TResult>?> function, CancellationToken cancellationToken)
  2581|         {
  2582|             if (function == null) ThrowHelper.ThrowArgumentNullException(ExceptionArgument.function);
  2583|             if (cancellationToken.IsCancellationRequested)
  2584|                 return FromCanceled<TResult>(cancellationToken);
  2585|             Task<Task<TResult>?> task1 = Task<Task<TResult>?>.Factory.StartNew(function, cancellationToken, TaskCreationOptions.DenyChildAttach, TaskScheduler.Default);
  2586|             UnwrapPromise<TResult> promise = new UnwrapPromise<TResult>(task1, lookForOce: true);
  2587|             return promise;
  2588|         }
  2589|         #endregion
  2590|         #region Delay methods
  2591|         public static Task Delay(TimeSpan delay) => Delay(delay, TimeProvider.System, default);
  2592|         public static Task Delay(TimeSpan delay, TimeProvider timeProvider) => Delay(delay, timeProvider, default);
  2593|         public static Task Delay(TimeSpan delay, CancellationToken cancellationToken) =>
  2594|             Delay(delay, TimeProvider.System, cancellationToken);
  2595|         public static Task Delay(TimeSpan delay, TimeProvider timeProvider, CancellationToken cancellationToken)
  2596|         {
  2597|             ArgumentNullException.ThrowIfNull(timeProvider);
  2598|             return Delay(ValidateTimeout(delay, ExceptionArgument.delay), timeProvider, cancellationToken);
  2599|         }
  2600|         public static Task Delay(int millisecondsDelay) => Delay(millisecondsDelay, default);
  2601|         public static Task Delay(int millisecondsDelay, CancellationToken cancellationToken)
  2602|         {
  2603|             if (millisecondsDelay < -1)
  2604|             {
  2605|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.millisecondsDelay, ExceptionResource.Task_Delay_InvalidMillisecondsDelay);
  2606|             }
  2607|             return Delay((uint)millisecondsDelay, TimeProvider.System, cancellationToken);
  2608|         }
  2609|         private static Task Delay(uint millisecondsDelay, TimeProvider timeProvider, CancellationToken cancellationToken) =>
  2610|             cancellationToken.IsCancellationRequested ? FromCanceled(cancellationToken) :
  2611|             millisecondsDelay == 0 ? CompletedTask :
  2612|             cancellationToken.CanBeCanceled ? new DelayPromiseWithCancellation(millisecondsDelay, timeProvider, cancellationToken) :
  2613|             new DelayPromise(millisecondsDelay, timeProvider);
  2614|         internal static uint ValidateTimeout(TimeSpan timeout, ExceptionArgument argument)
  2615|         {
  2616|             long totalMilliseconds = (long)timeout.TotalMilliseconds;
  2617|             if (totalMilliseconds < -1 || totalMilliseconds > Timer.MaxSupportedTimeout)
  2618|             {
  2619|                 ThrowHelper.ThrowArgumentOutOfRangeException(argument, ExceptionResource.Task_InvalidTimerTimeSpan);
  2620|             }
  2621|             return (uint)totalMilliseconds;
  2622|         }
  2623|         private class DelayPromise : Task
  2624|         {
  2625|             private static readonly TimerCallback s_timerCallback = TimerCallback;
  2626|             private readonly ITimer? _timer;
  2627|             internal DelayPromise(uint millisecondsDelay, TimeProvider timeProvider)
  2628|             {
  2629|                 Debug.Assert(millisecondsDelay != 0);
  2630|                 if (TplEventSource.Log.IsEnabled())
  2631|                     TplEventSource.Log.TraceOperationBegin(this.Id, "Task.Delay", 0);
  2632|                 if (s_asyncDebuggingEnabled)
  2633|                     AddToActiveTasks(this);
  2634|                 if (millisecondsDelay != Timeout.UnsignedInfinite) // no need to create the timer if it's an infinite timeout
  2635|                 {
  2636|                     if (timeProvider == TimeProvider.System)
  2637|                     {
  2638|                         _timer = new TimerQueueTimer(s_timerCallback, this, millisecondsDelay, Timeout.UnsignedInfinite, flowExecutionContext: false);
  2639|                     }
  2640|                     else
  2641|                     {
  2642|                         using (ExecutionContext.SuppressFlow())
  2643|                         {
  2644|                             _timer = timeProvider.CreateTimer(s_timerCallback, this, TimeSpan.FromMilliseconds(millisecondsDelay), Timeout.InfiniteTimeSpan);
  2645|                         }
  2646|                     }
  2647|                     if (IsCompleted)
  2648|                     {
  2649|                         _timer.Dispose();
  2650|                     }
  2651|                 }
  2652|             }
  2653|             private static void TimerCallback(object? state) => ((DelayPromise)state!).CompleteTimedOut();
  2654|             private void CompleteTimedOut()
  2655|             {
  2656|                 if (TrySetResult())
  2657|                 {
  2658|                     Cleanup();
  2659|                     if (s_asyncDebuggingEnabled)
  2660|                         RemoveFromActiveTasks(this);
  2661|                     if (TplEventSource.Log.IsEnabled())
  2662|                         TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Completed);
  2663|                 }
  2664|             }
  2665|             protected virtual void Cleanup() => _timer?.Dispose();
  2666|         }
  2667|         private sealed class DelayPromiseWithCancellation : DelayPromise
  2668|         {
  2669|             private readonly CancellationTokenRegistration _registration;
  2670|             internal DelayPromiseWithCancellation(uint millisecondsDelay, TimeProvider timeProvider, CancellationToken token) : base(millisecondsDelay, timeProvider)
  2671|             {
  2672|                 Debug.Assert(token.CanBeCanceled);
  2673|                 _registration = token.UnsafeRegister(static (state, cancellationToken) =>
  2674|                 {
  2675|                     var thisRef = (DelayPromiseWithCancellation)state!;
  2676|                     thisRef.AtomicStateUpdate((int)TaskCreationOptions.RunContinuationsAsynchronously, 0);
  2677|                     if (thisRef.TrySetCanceled(cancellationToken))
  2678|                     {
  2679|                         thisRef.Cleanup();
  2680|                     }
  2681|                 }, this);
  2682|                 if (IsCompleted)
  2683|                 {
  2684|                     _registration.Dispose();
  2685|                 }
  2686|             }
  2687|             protected override void Cleanup()
  2688|             {
  2689|                 _registration.Dispose();
  2690|                 base.Cleanup();
  2691|             }
  2692|         }
  2693|         #endregion
  2694|         #region WhenAll
  2695|         public static Task WhenAll(IEnumerable<Task> tasks)
  2696|         {
  2697|             if (tasks is null)
  2698|             {
  2699|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
  2700|             }
  2701|             if (tasks is ICollection<Task> taskCollection)
  2702|             {
  2703|                 if (tasks is Task[] taskArray)
  2704|                 {
  2705|                     return WhenAll((ReadOnlySpan<Task>)taskArray);
  2706|                 }
  2707|                 if (tasks is List<Task> taskList)
  2708|                 {
  2709|                     return WhenAll(CollectionsMarshal.AsSpan(taskList));
  2710|                 }
  2711|                 taskArray = new Task[taskCollection.Count];
  2712|                 taskCollection.CopyTo(taskArray, 0);
  2713|                 return WhenAll((ReadOnlySpan<Task>)taskArray);
  2714|             }
  2715|             else
  2716|             {
  2717|                 var taskList = new List<Task>();
  2718|                 foreach (Task task in tasks)
  2719|                 {
  2720|                     taskList.Add(task);
  2721|                 }
  2722|                 return WhenAll(CollectionsMarshal.AsSpan(taskList));
  2723|             }
  2724|         }
  2725|         public static Task WhenAll(params Task[] tasks)
  2726|         {
  2727|             if (tasks is null)
  2728|             {
  2729|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
  2730|             }
  2731|             return WhenAll((ReadOnlySpan<Task>)tasks);
  2732|         }
  2733|         internal static Task WhenAll(ReadOnlySpan<Task> tasks) => // TODO https://github.com/dotnet/runtime/issues/77873: Make this public.
  2734|             tasks.Length != 0 ? new WhenAllPromise(tasks) : CompletedTask;
  2735|         private sealed class WhenAllPromise : Task, ITaskCompletionAction
  2736|         {
  2737|             private object? _failedOrCanceled;
  2738|             private int _remainingToComplete;
  2739|             internal WhenAllPromise(ReadOnlySpan<Task> tasks)
  2740|             {
  2741|                 Debug.Assert(tasks.Length != 0, "Expected a non-zero length task array");
  2742|                 foreach (Task task in tasks)
  2743|                 {
  2744|                     if (task is null)
  2745|                     {
  2746|                         ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
  2747|                     }
  2748|                 }
  2749|                 if (TplEventSource.Log.IsEnabled())
  2750|                 {
  2751|                     TplEventSource.Log.TraceOperationBegin(Id, "Task.WhenAll", 0);
  2752|                 }
  2753|                 if (s_asyncDebuggingEnabled)
  2754|                 {
  2755|                     AddToActiveTasks(this);
  2756|                 }
  2757|                 _remainingToComplete = tasks.Length;
  2758|                 foreach (Task task in tasks)
  2759|                 {
  2760|                     if (task is null || task.IsCompleted)
  2761|                     {
  2762|                         Invoke(task); // short-circuit the completion action, if possible
  2763|                     }
  2764|                     else
  2765|                     {
  2766|                         task.AddCompletionAction(this); // simple completion action
  2767|                     }
  2768|                 }
  2769|             }
  2770|             public void Invoke(Task? completedTask)
  2771|             {
  2772|                 if (TplEventSource.Log.IsEnabled())
  2773|                 {
  2774|                     TplEventSource.Log.TraceOperationRelation(Id, CausalityRelation.Join);
  2775|                 }
  2776|                 if (completedTask is not null)
  2777|                 {
  2778|                     if (completedTask.IsWaitNotificationEnabled)
  2779|                     {
  2780|                         SetNotificationForWaitCompletion(enabled: true);
  2781|                     }
  2782|                     if (!completedTask.IsCompletedSuccessfully)
  2783|                     {
  2784|                         if (Interlocked.CompareExchange(ref _failedOrCanceled, completedTask, null) != null)
  2785|                         {
  2786|                             while (true)
  2787|                             {
  2788|                                 object? failedOrCanceled = _failedOrCanceled;
  2789|                                 Debug.Assert(failedOrCanceled is not null);
  2790|                                 if (_failedOrCanceled is List<Task> list)
  2791|                                 {
  2792|                                     lock (list)
  2793|                                     {
  2794|                                         list.Add(completedTask);
  2795|                                     }
  2796|                                     break;
  2797|                                 }
  2798|                                 Debug.Assert(failedOrCanceled is Task, $"Expected Task, got {failedOrCanceled}");
  2799|                                 if (Interlocked.CompareExchange(ref _failedOrCanceled, new List<Task> { (Task)failedOrCanceled, completedTask }, failedOrCanceled) == failedOrCanceled)
  2800|                                 {
  2801|                                     break;
  2802|                                 }
  2803|                                 Debug.Assert(_failedOrCanceled is List<Task>);
  2804|                             }
  2805|                         }
  2806|                     }
  2807|                 }
  2808|                 if (Interlocked.Decrement(ref _remainingToComplete) == 0)
  2809|                 {
  2810|                     object? failedOrCanceled = _failedOrCanceled;
  2811|                     if (failedOrCanceled is null)
  2812|                     {
  2813|                         if (TplEventSource.Log.IsEnabled())
  2814|                         {
  2815|                             TplEventSource.Log.TraceOperationEnd(Id, AsyncCausalityStatus.Completed);
  2816|                         }
  2817|                         if (s_asyncDebuggingEnabled)
  2818|                         {
  2819|                             RemoveFromActiveTasks(this);
  2820|                         }
  2821|                         bool completed = TrySetResult();
  2822|                         Debug.Assert(completed);
  2823|                     }
  2824|                     else
  2825|                     {
  2826|                         List<ExceptionDispatchInfo>? observedExceptions = null;
  2827|                         Task? canceledTask = null;
  2828|                         void HandleTask(Task task)
  2829|                         {
  2830|                             if (task.IsFaulted)
  2831|                             {
  2832|                                 (observedExceptions ??= new()).AddRange(task.GetExceptionDispatchInfos());
  2833|                             }
  2834|                             else if (task.IsCanceled)
  2835|                             {
  2836|                                 canceledTask ??= task; // use the first task that's canceled
  2837|                             }
  2838|                         }
  2839|                         if (failedOrCanceled is List<Task> list)
  2840|                         {
  2841|                             foreach (Task task in list)
  2842|                             {
  2843|                                 HandleTask(task);
  2844|                             }
  2845|                         }
  2846|                         else
  2847|                         {
  2848|                             Debug.Assert(failedOrCanceled is Task);
  2849|                             HandleTask((Task)failedOrCanceled);
  2850|                         }
  2851|                         if (observedExceptions != null)
  2852|                         {
  2853|                             Debug.Assert(observedExceptions.Count > 0, "Expected at least one exception");
  2854|                             TrySetException(observedExceptions);
  2855|                         }
  2856|                         else if (canceledTask != null)
  2857|                         {
  2858|                             TrySetCanceled(canceledTask.CancellationToken, canceledTask.GetCancellationExceptionDispatchInfo());
  2859|                         }
  2860|                     }
  2861|                     Debug.Assert(IsCompleted);
  2862|                 }
  2863|                 Debug.Assert(_remainingToComplete >= 0, "Count should never go below 0");
  2864|             }
  2865|             public bool InvokeMayRunArbitraryCode => true;
  2866|         }
  2867|         public static Task<TResult[]> WhenAll<TResult>(IEnumerable<Task<TResult>> tasks)
  2868|         {
  2869|             if (tasks is Task<TResult>[] taskArray)
  2870|             {
  2871|                 return WhenAll(taskArray);
  2872|             }
  2873|             if (tasks is ICollection<Task<TResult>> taskCollection)
  2874|             {
  2875|                 taskArray = new Task<TResult>[taskCollection.Count];
  2876|                 taskCollection.CopyTo(taskArray, 0);
  2877|                 foreach (Task<TResult> task in taskArray)
  2878|                 {
  2879|                     if (task is null)
  2880|                     {
  2881|                         ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
  2882|                     }
  2883|                 }
  2884|                 return InternalWhenAll(taskArray);
  2885|             }
  2886|             if (tasks == null) ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
  2887|             List<Task<TResult>> taskList = new List<Task<TResult>>();
  2888|             foreach (Task<TResult> task in tasks)
  2889|             {
  2890|                 if (task == null) ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
  2891|                 taskList.Add(task);
  2892|             }
  2893|             return InternalWhenAll(taskList.ToArray());
  2894|         }
  2895|         public static Task<TResult[]> WhenAll<TResult>(params Task<TResult>[] tasks)
  2896|         {
  2897|             if (tasks == null) ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
  2898|             int taskCount = tasks.Length;
  2899|             if (taskCount == 0) return InternalWhenAll(tasks); // small optimization in the case of an empty task array
  2900|             Task<TResult>[] tasksCopy = (Task<TResult>[])tasks.Clone();
  2901|             foreach (Task<TResult> task in tasksCopy)
  2902|             {
  2903|                 if (task is null)
  2904|                 {
  2905|                     ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
  2906|                 }
  2907|             }
  2908|             return InternalWhenAll(tasksCopy);
  2909|         }
  2910|         private static Task<TResult[]> InternalWhenAll<TResult>(Task<TResult>[] tasks)
  2911|         {
  2912|             Debug.Assert(tasks != null, "Expected a non-null tasks array");
  2913|             return (tasks.Length == 0) ? // take shortcut if there are no tasks upon which to wait
  2914|                 new Task<TResult[]>(false, Array.Empty<TResult>(), TaskCreationOptions.None, default) :
  2915|                 new WhenAllPromise<TResult>(tasks);
  2916|         }
  2917|         private sealed class WhenAllPromise<T> : Task<T[]>, ITaskCompletionAction
  2918|         {
  2919|             private readonly Task<T>?[] m_tasks;
  2920|             private int m_count;
  2921|             internal WhenAllPromise(Task<T>[] tasks)
  2922|             {
  2923|                 Debug.Assert(tasks != null, "Expected a non-null task array");
  2924|                 Debug.Assert(tasks.Length > 0, "Expected a non-zero length task array");
  2925|                 m_tasks = tasks;
  2926|                 m_count = tasks.Length;
  2927|                 if (TplEventSource.Log.IsEnabled())
  2928|                     TplEventSource.Log.TraceOperationBegin(this.Id, "Task.WhenAll", 0);
  2929|                 if (s_asyncDebuggingEnabled)
  2930|                     AddToActiveTasks(this);
  2931|                 foreach (Task<T> task in tasks)
  2932|                 {
  2933|                     if (task.IsCompleted) this.Invoke(task); // short-circuit the completion action, if possible
  2934|                     else task.AddCompletionAction(this); // simple completion action
  2935|                 }
  2936|             }
  2937|             public void Invoke(Task ignored)
  2938|             {
  2939|                 if (TplEventSource.Log.IsEnabled())
  2940|                     TplEventSource.Log.TraceOperationRelation(this.Id, CausalityRelation.Join);
  2941|                 if (Interlocked.Decrement(ref m_count) == 0)
  2942|                 {
  2943|                     T[] results = new T[m_tasks.Length];
  2944|                     List<ExceptionDispatchInfo>? observedExceptions = null;
  2945|                     Task? canceledTask = null;
  2946|                     for (int i = 0; i < m_tasks.Length; i++)
  2947|                     {
  2948|                         Task<T>? task = m_tasks[i];
  2949|                         Debug.Assert(task != null, "Constituent task in WhenAll should never be null");
  2950|                         if (task.IsFaulted)
  2951|                         {
  2952|                             observedExceptions ??= new List<ExceptionDispatchInfo>();
  2953|                             observedExceptions.AddRange(task.GetExceptionDispatchInfos());
  2954|                         }
  2955|                         else if (task.IsCanceled)
  2956|                         {
  2957|                             canceledTask ??= task; // use the first task that's canceled
  2958|                         }
  2959|                         else
  2960|                         {
  2961|                             Debug.Assert(task.Status == TaskStatus.RanToCompletion);
  2962|                             results[i] = task.GetResultCore(waitCompletionNotification: false); // avoid Result, which would triggering debug notification
  2963|                         }
  2964|                         if (task.IsWaitNotificationEnabled) this.SetNotificationForWaitCompletion(enabled: true);
  2965|                         else m_tasks[i] = null; // avoid holding onto tasks unnecessarily
  2966|                     }
  2967|                     if (observedExceptions != null)
  2968|                     {
  2969|                         Debug.Assert(observedExceptions.Count > 0, "Expected at least one exception");
  2970|                         TrySetException(observedExceptions);
  2971|                     }
  2972|                     else if (canceledTask != null)
  2973|                     {
  2974|                         TrySetCanceled(canceledTask.CancellationToken, canceledTask.GetCancellationExceptionDispatchInfo());
  2975|                     }
  2976|                     else
  2977|                     {
  2978|                         if (TplEventSource.Log.IsEnabled())
  2979|                             TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Completed);
  2980|                         if (s_asyncDebuggingEnabled)
  2981|                             RemoveFromActiveTasks(this);
  2982|                         TrySetResult(results);
  2983|                     }
  2984|                 }
  2985|                 Debug.Assert(m_count >= 0, "Count should never go below 0");
  2986|             }
  2987|             public bool InvokeMayRunArbitraryCode => true;
  2988|             private protected override bool ShouldNotifyDebuggerOfWaitCompletion =>
  2989|                 base.ShouldNotifyDebuggerOfWaitCompletion &&
  2990|                 AnyTaskRequiresNotifyDebuggerOfWaitCompletion(m_tasks);
  2991|         }
  2992|         #endregion
  2993|         #region WhenAny
  2994|         public static Task<Task> WhenAny(params Task[] tasks)
  2995|         {
  2996|             ArgumentNullException.ThrowIfNull(tasks);
  2997|             return WhenAny((ReadOnlySpan<Task>)tasks);
  2998|         }
  2999|         private static Task<TTask> WhenAny<TTask>(ReadOnlySpan<TTask> tasks) where TTask : Task
  3000|         {
  3001|             if (tasks.Length == 2)
  3002|             {
  3003|                 return WhenAny(tasks[0], tasks[1]);
  3004|             }
  3005|             if (tasks.IsEmpty)
  3006|             {
  3007|                 ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_EmptyTaskList, ExceptionArgument.tasks);
  3008|             }
  3009|             TTask[] tasksCopy = tasks.ToArray();
  3010|             foreach (TTask task in tasksCopy)
  3011|             {
  3012|                 if (task is null)
  3013|                 {
  3014|                     ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
  3015|                 }
  3016|             }
  3017|             return TaskFactory.CommonCWAnyLogic(tasksCopy);
  3018|         }
  3019|         public static Task<Task> WhenAny(Task task1, Task task2) =>
  3020|             WhenAny<Task>(task1, task2);
  3021|         private static Task<TTask> WhenAny<TTask>(TTask task1, TTask task2) where TTask : Task
  3022|         {
  3023|             ArgumentNullException.ThrowIfNull(task1);
  3024|             ArgumentNullException.ThrowIfNull(task2);
  3025|             return
  3026|                 task1.IsCompleted ? FromResult(task1) :
  3027|                 task2.IsCompleted ? FromResult(task2) :
  3028|                 new TwoTaskWhenAnyPromise<TTask>(task1, task2);
  3029|         }
  3030|         private sealed class TwoTaskWhenAnyPromise<TTask> : Task<TTask>, ITaskCompletionAction where TTask : Task
  3031|         {
  3032|             private TTask? _task1, _task2;
  3033|             public TwoTaskWhenAnyPromise(TTask task1, TTask task2)
  3034|             {
  3035|                 Debug.Assert(task1 != null && task2 != null);
  3036|                 _task1 = task1;
  3037|                 _task2 = task2;
  3038|                 if (TplEventSource.Log.IsEnabled())
  3039|                 {
  3040|                     TplEventSource.Log.TraceOperationBegin(this.Id, "Task.WhenAny", 0);
  3041|                 }
  3042|                 if (s_asyncDebuggingEnabled)
  3043|                 {
  3044|                     AddToActiveTasks(this);
  3045|                 }
  3046|                 task1.AddCompletionAction(this);
  3047|                 task2.AddCompletionAction(this);
  3048|                 if (task1.IsCompleted)
  3049|                 {
  3050|                     task2.RemoveContinuation(this);
  3051|                 }
  3052|             }
  3053|             public void Invoke(Task completingTask)
  3054|             {
  3055|                 Task? task1;
  3056|                 if ((task1 = Interlocked.Exchange(ref _task1, null)) != null)
  3057|                 {
  3058|                     Task? task2 = _task2;
  3059|                     _task2 = null;
  3060|                     Debug.Assert(task1 != null && task2 != null);
  3061|                     Debug.Assert(task1.IsCompleted || task2.IsCompleted);
  3062|                     if (TplEventSource.Log.IsEnabled())
  3063|                     {
  3064|                         TplEventSource.Log.TraceOperationRelation(this.Id, CausalityRelation.Choice);
  3065|                         TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Completed);
  3066|                     }
  3067|                     if (s_asyncDebuggingEnabled)
  3068|                     {
  3069|                         RemoveFromActiveTasks(this);
  3070|                     }
  3071|                     if (!task1.IsCompleted)
  3072|                     {
  3073|                         task1.RemoveContinuation(this);
  3074|                     }
  3075|                     else
  3076|                     {
  3077|                         task2.RemoveContinuation(this);
  3078|                     }
  3079|                     bool success = TrySetResult((TTask)completingTask);
  3080|                     Debug.Assert(success, "Only one task should have gotten to this point, and thus this must be successful.");
  3081|                 }
  3082|             }
  3083|             public bool InvokeMayRunArbitraryCode => true;
  3084|         }
  3085|         public static Task<Task> WhenAny(IEnumerable<Task> tasks) =>
  3086|             WhenAny<Task>(tasks);
  3087|         private static Task<TTask> WhenAny<TTask>(IEnumerable<TTask> tasks) where TTask : Task
  3088|         {
  3089|             if (tasks is ICollection<TTask> tasksAsCollection)
  3090|             {
  3091|                 if (tasks is List<TTask> tasksAsList)
  3092|                 {
  3093|                     return WhenAny((ReadOnlySpan<TTask>)CollectionsMarshal.AsSpan(tasksAsList));
  3094|                 }
  3095|                 if (tasks is TTask[] tasksAsArray)
  3096|                 {
  3097|                     return WhenAny((ReadOnlySpan<TTask>)tasksAsArray);
  3098|                 }
  3099|                 int count = tasksAsCollection.Count;
  3100|                 if (count <= 0)
  3101|                 {
  3102|                     ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_EmptyTaskList, ExceptionArgument.tasks);
  3103|                 }
  3104|                 var taskArray = new TTask[count];
  3105|                 tasksAsCollection.CopyTo(taskArray, 0);
  3106|                 foreach (TTask task in taskArray)
  3107|                 {
  3108|                     if (task is null)
  3109|                     {
  3110|                         ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
  3111|                     }
  3112|                 }
  3113|                 return TaskFactory.CommonCWAnyLogic(taskArray);
  3114|             }
  3115|             if (tasks is null)
  3116|             {
  3117|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
  3118|             }
  3119|             var taskList = new List<TTask>();
  3120|             foreach (TTask task in tasks)
  3121|             {
  3122|                 if (task is null)
  3123|                 {
  3124|                     ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
  3125|                 }
  3126|                 taskList.Add(task);
  3127|             }
  3128|             if (taskList.Count == 0)
  3129|             {
  3130|                 ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_EmptyTaskList, ExceptionArgument.tasks);
  3131|             }
  3132|             return TaskFactory.CommonCWAnyLogic(taskList);
  3133|         }
  3134|         public static Task<Task<TResult>> WhenAny<TResult>(params Task<TResult>[] tasks)
  3135|         {
  3136|             ArgumentNullException.ThrowIfNull(tasks);
  3137|             return WhenAny((ReadOnlySpan<Task<TResult>>)tasks);
  3138|         }
  3139|         public static Task<Task<TResult>> WhenAny<TResult>(Task<TResult> task1, Task<TResult> task2) =>
  3140|             WhenAny<Task<TResult>>(task1, task2);
  3141|         public static Task<Task<TResult>> WhenAny<TResult>(IEnumerable<Task<TResult>> tasks) =>
  3142|             WhenAny<Task<TResult>>(tasks);
  3143|         #endregion
  3144|         internal static Task<TResult> CreateUnwrapPromise<TResult>(Task outerTask, bool lookForOce)
  3145|         {
  3146|             Debug.Assert(outerTask != null);
  3147|             return new UnwrapPromise<TResult>(outerTask, lookForOce);
  3148|         }
  3149|         internal virtual Delegate[]? GetDelegateContinuationsForDebugger()
  3150|         {
  3151|             if (m_continuationObject != this)
  3152|                 return GetDelegatesFromContinuationObject(m_continuationObject);
  3153|             else
  3154|                 return null;
  3155|         }
  3156|         private static Delegate[]? GetDelegatesFromContinuationObject(object? continuationObject)
  3157|         {
  3158|             if (continuationObject != null)
  3159|             {
  3160|                 if (continuationObject is Action singleAction)
  3161|                 {
  3162|                     return new Delegate[] { AsyncMethodBuilderCore.TryGetStateMachineForDebugger(singleAction) };
  3163|                 }
  3164|                 if (continuationObject is TaskContinuation taskContinuation)
  3165|                 {
  3166|                     return taskContinuation.GetDelegateContinuationsForDebugger();
  3167|                 }
  3168|                 if (continuationObject is Task continuationTask)
  3169|                 {
  3170|                     Delegate[]? delegates = continuationTask.GetDelegateContinuationsForDebugger();
  3171|                     if (delegates != null)
  3172|                         return delegates;
  3173|                 }
  3174|                 if (continuationObject is ITaskCompletionAction singleCompletionAction)
  3175|                 {
  3176|                     return new Delegate[] { new Action<Task>(singleCompletionAction.Invoke) };
  3177|                 }
  3178|                 if (continuationObject is List<object?> continuationList)
  3179|                 {
  3180|                     List<Delegate> result = new List<Delegate>();
  3181|                     foreach (object? obj in continuationList)
  3182|                     {
  3183|                         Delegate[]? innerDelegates = GetDelegatesFromContinuationObject(obj);
  3184|                         if (innerDelegates != null)
  3185|                         {
  3186|                             foreach (Delegate del in innerDelegates)
  3187|                             {
  3188|                                 if (del != null)
  3189|                                     result.Add(del);
  3190|                             }
  3191|                         }
  3192|                     }
  3193|                     return result.ToArray();
  3194|                 }
  3195|             }
  3196|             return null;
  3197|         }
  3198|         private static Task? GetActiveTaskFromId(int taskId)
  3199|         {
  3200|             Task? task = null;
  3201|             s_currentActiveTasks?.TryGetValue(taskId, out task);
  3202|             return task;
  3203|         }
  3204|     }
  3205|     internal sealed class CompletionActionInvoker : IThreadPoolWorkItem
  3206|     {
  3207|         private readonly ITaskCompletionAction m_action;
  3208|         private readonly Task m_completingTask;
  3209|         internal CompletionActionInvoker(ITaskCompletionAction action, Task completingTask)
  3210|         {
  3211|             m_action = action;
  3212|             m_completingTask = completingTask;
  3213|         }
  3214|         void IThreadPoolWorkItem.Execute()
  3215|         {
  3216|             m_action.Invoke(m_completingTask);
  3217|         }
  3218|     }
  3219|     internal sealed class SystemThreadingTasks_TaskDebugView
  3220|     {
  3221|         private readonly Task m_task;
  3222|         public SystemThreadingTasks_TaskDebugView(Task task)
  3223|         {
  3224|             m_task = task;
  3225|         }
  3226|         public object? AsyncState => m_task.AsyncState;
  3227|         public TaskCreationOptions CreationOptions => m_task.CreationOptions;
  3228|         public Exception? Exception => m_task.Exception;
  3229|         public int Id => m_task.Id;
  3230|         public bool CancellationPending => (m_task.Status == TaskStatus.WaitingToRun) && m_task.CancellationToken.IsCancellationRequested;
  3231|         public TaskStatus Status => m_task.Status;
  3232|     }
  3233|     [Flags]
  3234|     public enum TaskCreationOptions
  3235|     {
  3236|         None = 0x0,
  3237|         PreferFairness = 0x01,
  3238|         LongRunning = 0x02,
  3239|         AttachedToParent = 0x04,
  3240|         DenyChildAttach = 0x08,
  3241|         HideScheduler = 0x10,
  3242|         RunContinuationsAsynchronously = 0x40
  3243|     }
  3244|     [Flags]
  3245|     internal enum InternalTaskOptions
  3246|     {
  3247|         None,
  3248|         InternalOptionsMask = 0x0000FF00,
  3249|         ContinuationTask = 0x0200,
  3250|         PromiseTask = 0x0400,
  3251|         HiddenState = 0x0800,
  3252|         LazyCancellation = 0x1000,
  3253|         QueuedByRuntime = 0x2000,
  3254|         DoNotDispose = 0x4000
  3255|     }
  3256|     [Flags]
  3257|     public enum TaskContinuationOptions
  3258|     {
  3259|         None = 0,
  3260|         PreferFairness = 0x01,
  3261|         LongRunning = 0x02,
  3262|         AttachedToParent = 0x04,
  3263|         DenyChildAttach = 0x08,
  3264|         HideScheduler = 0x10,
  3265|         LazyCancellation = 0x20,
  3266|         RunContinuationsAsynchronously = 0x40,
  3267|         NotOnRanToCompletion = 0x10000,
  3268|         NotOnFaulted = 0x20000,
  3269|         NotOnCanceled = 0x40000,
  3270|         OnlyOnRanToCompletion = NotOnFaulted | NotOnCanceled,
  3271|         OnlyOnFaulted = NotOnRanToCompletion | NotOnCanceled,
  3272|         OnlyOnCanceled = NotOnRanToCompletion | NotOnFaulted,
  3273|         ExecuteSynchronously = 0x80000
  3274|     }
  3275|     internal readonly struct VoidTaskResult { }
  3276|     internal interface ITaskCompletionAction
  3277|     {
  3278|         void Invoke(Task completingTask);
  3279|         bool InvokeMayRunArbitraryCode { get; }
  3280|     }
  3281|     internal sealed class UnwrapPromise<TResult> : Task<TResult>, ITaskCompletionAction
  3282|     {
  3283|         private const byte STATE_WAITING_ON_OUTER_TASK = 0; // Invoke() means "process completed outer task"
  3284|         private const byte STATE_WAITING_ON_INNER_TASK = 1; // Invoke() means "process completed inner task"
  3285|         private const byte STATE_DONE = 2;                  // Invoke() means "something went wrong and we are hosed!"
  3286|         private byte _state;
  3287|         private readonly bool _lookForOce;
  3288|         public UnwrapPromise(Task outerTask, bool lookForOce)
  3289|             : base((object?)null, outerTask.CreationOptions & TaskCreationOptions.AttachedToParent)
  3290|         {
  3291|             Debug.Assert(outerTask != null, "Expected non-null outerTask");
  3292|             _lookForOce = lookForOce;
  3293|             if (TplEventSource.Log.IsEnabled())
  3294|                 TplEventSource.Log.TraceOperationBegin(this.Id, "Task.Unwrap", 0);
  3295|             if (s_asyncDebuggingEnabled)
  3296|                 AddToActiveTasks(this);
  3297|             if (outerTask.IsCompleted)
  3298|             {
  3299|                 ProcessCompletedOuterTask(outerTask);
  3300|             }
  3301|             else // Otherwise, process its completion asynchronously.
  3302|             {
  3303|                 outerTask.AddCompletionAction(this);
  3304|             }
  3305|         }
  3306|         public void Invoke(Task completingTask)
  3307|         {
  3308|             if (RuntimeHelpers.TryEnsureSufficientExecutionStack())
  3309|             {
  3310|                 InvokeCore(completingTask);
  3311|             }
  3312|             else
  3313|             {
  3314|                 InvokeCoreAsync(completingTask);
  3315|             }
  3316|         }
  3317|         private void InvokeCore(Task completingTask)
  3318|         {
  3319|             switch (_state)
  3320|             {
  3321|                 case STATE_WAITING_ON_OUTER_TASK:
  3322|                     ProcessCompletedOuterTask(completingTask);
  3323|                     break;
  3324|                 case STATE_WAITING_ON_INNER_TASK:
  3325|                     bool result = TrySetFromTask(completingTask, lookForOce: false);
  3326|                     _state = STATE_DONE; // bump the state
  3327|                     Debug.Assert(result, "Expected TrySetFromTask from inner task to succeed");
  3328|                     break;
  3329|                 default:
  3330|                     Debug.Fail("UnwrapPromise in illegal state");
  3331|                     break;
  3332|             }
  3333|         }
  3334|         private void InvokeCoreAsync(Task completingTask)
  3335|         {
  3336|             ThreadPool.UnsafeQueueUserWorkItem(static state =>
  3337|             {
  3338|                 var tuple = (TupleSlim<UnwrapPromise<TResult>, Task>)state!;
  3339|                 tuple.Item1.InvokeCore(tuple.Item2);
  3340|             }, new TupleSlim<UnwrapPromise<TResult>, Task>(this, completingTask));
  3341|         }
  3342|         private void ProcessCompletedOuterTask(Task task)
  3343|         {
  3344|             Debug.Assert(task != null && task.IsCompleted, "Expected non-null, completed outer task");
  3345|             Debug.Assert(_state == STATE_WAITING_ON_OUTER_TASK, "We're in the wrong state!");
  3346|             _state = STATE_WAITING_ON_INNER_TASK;
  3347|             switch (task.Status)
  3348|             {
  3349|                 case TaskStatus.Canceled:
  3350|                 case TaskStatus.Faulted:
  3351|                     bool result = TrySetFromTask(task, _lookForOce);
  3352|                     Debug.Assert(result, "Expected TrySetFromTask from outer task to succeed");
  3353|                     break;
  3354|                 case TaskStatus.RanToCompletion:
  3355|                     ProcessInnerTask(task is Task<Task<TResult>> taskOfTaskOfTResult ? // it's either a Task<Task> or Task<Task<TResult>>
  3356|                         taskOfTaskOfTResult.Result : ((Task<Task>)task).Result);
  3357|                     break;
  3358|             }
  3359|         }
  3360|         private bool TrySetFromTask(Task task, bool lookForOce)
  3361|         {
  3362|             Debug.Assert(task != null && task.IsCompleted, "TrySetFromTask: Expected task to have completed.");
  3363|             if (TplEventSource.Log.IsEnabled())
  3364|                 TplEventSource.Log.TraceOperationRelation(this.Id, CausalityRelation.Join);
  3365|             bool result = false;
  3366|             switch (task.Status)
  3367|             {
  3368|                 case TaskStatus.Canceled:
  3369|                     result = TrySetCanceled(task.CancellationToken, task.GetCancellationExceptionDispatchInfo());
  3370|                     break;
  3371|                 case TaskStatus.Faulted:
  3372|                     List<ExceptionDispatchInfo> edis = task.GetExceptionDispatchInfos();
  3373|                     ExceptionDispatchInfo oceEdi;
  3374|                     if (lookForOce && edis.Count > 0 &&
  3375|                         (oceEdi = edis[0]) != null &&
  3376|                         oceEdi.SourceException is OperationCanceledException oce)
  3377|                     {
  3378|                         result = TrySetCanceled(oce.CancellationToken, oceEdi);
  3379|                     }
  3380|                     else
  3381|                     {
  3382|                         result = TrySetException(edis);
  3383|                     }
  3384|                     break;
  3385|                 case TaskStatus.RanToCompletion:
  3386|                     if (TplEventSource.Log.IsEnabled())
  3387|                         TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Completed);
  3388|                     if (s_asyncDebuggingEnabled)
  3389|                         RemoveFromActiveTasks(this);
  3390|                     result = TrySetResult(task is Task<TResult> taskTResult ? taskTResult.Result : default);
  3391|                     break;
  3392|             }
  3393|             return result;
  3394|         }
  3395|         private void ProcessInnerTask(Task? task)
  3396|         {
  3397|             if (task == null)
  3398|             {
  3399|                 TrySetCanceled(default);
  3400|                 _state = STATE_DONE; // ... and record that we are done
  3401|             }
  3402|             else if (task.IsCompleted)
  3403|             {
  3404|                 TrySetFromTask(task, lookForOce: false);
  3405|                 _state = STATE_DONE; // ... and record that we are done
  3406|             }
  3407|             else
  3408|             {
  3409|                 task.AddCompletionAction(this);
  3410|             }
  3411|         }
  3412|         public bool InvokeMayRunArbitraryCode => true;
  3413|     }
  3414| }


# ====================================================================
# FILE: src/libraries/System.Private.CoreLib/src/System/Threading/ThreadPoolWorkQueue.cs
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-1320 ---
     1| using System.Collections.Concurrent;
     2| using System.Collections.Generic;
     3| using System.Diagnostics;
     4| using System.Diagnostics.CodeAnalysis;
     5| using System.Diagnostics.Tracing;
     6| using System.Runtime.CompilerServices;
     7| using System.Runtime.InteropServices;
     8| using System.Runtime.Versioning;
     9| using System.Threading.Tasks;
    10| namespace System.Threading
    11| {
    12|     internal sealed partial class ThreadPoolWorkQueue
    13|     {
    14|         internal static class WorkStealingQueueList
    15|         {
    16| #pragma warning disable CA1825 // avoid the extra generic instantiation for Array.Empty<T>(); this is the only place we'll ever create this array
    17|             private static volatile WorkStealingQueue[] _queues = new WorkStealingQueue[0];
    18| #pragma warning restore CA1825
    19|             public static WorkStealingQueue[] Queues => _queues;
    20|             public static void Add(WorkStealingQueue queue)
    21|             {
    22|                 Debug.Assert(queue != null);
    23|                 while (true)
    24|                 {
    25|                     WorkStealingQueue[] oldQueues = _queues;
    26|                     Debug.Assert(Array.IndexOf(oldQueues, queue) < 0);
    27|                     var newQueues = new WorkStealingQueue[oldQueues.Length + 1];
    28|                     Array.Copy(oldQueues, newQueues, oldQueues.Length);
    29|                     newQueues[^1] = queue;
    30|                     if (Interlocked.CompareExchange(ref _queues, newQueues, oldQueues) == oldQueues)
    31|                     {
    32|                         break;
    33|                     }
    34|                 }
    35|             }
    36|             public static void Remove(WorkStealingQueue queue)
    37|             {
    38|                 Debug.Assert(queue != null);
    39|                 while (true)
    40|                 {
    41|                     WorkStealingQueue[] oldQueues = _queues;
    42|                     if (oldQueues.Length == 0)
    43|                     {
    44|                         return;
    45|                     }
    46|                     int pos = Array.IndexOf(oldQueues, queue);
    47|                     if (pos < 0)
    48|                     {
    49|                         Debug.Fail("Should have found the queue");
    50|                         return;
    51|                     }
    52|                     var newQueues = new WorkStealingQueue[oldQueues.Length - 1];
    53|                     if (pos == 0)
    54|                     {
    55|                         Array.Copy(oldQueues, 1, newQueues, 0, newQueues.Length);
    56|                     }
    57|                     else if (pos == oldQueues.Length - 1)
    58|                     {
    59|                         Array.Copy(oldQueues, newQueues, newQueues.Length);
    60|                     }
    61|                     else
    62|                     {
    63|                         Array.Copy(oldQueues, newQueues, pos);
    64|                         Array.Copy(oldQueues, pos + 1, newQueues, pos, newQueues.Length - pos);
    65|                     }
    66|                     if (Interlocked.CompareExchange(ref _queues, newQueues, oldQueues) == oldQueues)
    67|                     {
    68|                         break;
    69|                     }
    70|                 }
    71|             }
    72|         }
    73|         internal sealed class WorkStealingQueue
    74|         {
    75|             private const int INITIAL_SIZE = 32;
    76|             internal volatile object?[] m_array = new object[INITIAL_SIZE]; // SOS's ThreadPool command depends on this name
    77|             private volatile int m_mask = INITIAL_SIZE - 1;
    78| #if DEBUG
    79|             private const int START_INDEX = int.MaxValue;
    80| #else
    81|             private const int START_INDEX = 0;
    82| #endif
    83|             private volatile int m_headIndex = START_INDEX;
    84|             private volatile int m_tailIndex = START_INDEX;
    85|             private SpinLock m_foreignLock = new SpinLock(enableThreadOwnerTracking: false);
    86|             public void LocalPush(object obj)
    87|             {
    88|                 int tail = m_tailIndex;
    89|                 if (tail == int.MaxValue)
    90|                 {
    91|                     tail = LocalPush_HandleTailOverflow();
    92|                 }
    93|                 if (tail < m_headIndex + m_mask)
    94|                 {
    95|                     Volatile.Write(ref m_array[tail & m_mask], obj);
    96|                     m_tailIndex = tail + 1;
    97|                 }
    98|                 else
    99|                 {
   100|                     bool lockTaken = false;
   101|                     try
   102|                     {
   103|                         m_foreignLock.Enter(ref lockTaken);
   104|                         int head = m_headIndex;
   105|                         int count = m_tailIndex - m_headIndex;
   106|                         if (count >= m_mask)
   107|                         {
   108|                             var newArray = new object?[m_array.Length << 1];
   109|                             for (int i = 0; i < m_array.Length; i++)
   110|                                 newArray[i] = m_array[(i + head) & m_mask];
   111|                             m_array = newArray;
   112|                             m_headIndex = 0;
   113|                             m_tailIndex = tail = count;
   114|                             m_mask = (m_mask << 1) | 1;
   115|                         }
   116|                         Volatile.Write(ref m_array[tail & m_mask], obj);
   117|                         m_tailIndex = tail + 1;
   118|                     }
   119|                     finally
   120|                     {
   121|                         if (lockTaken)
   122|                             m_foreignLock.Exit(useMemoryBarrier: false);
   123|                     }
   124|                 }
   125|             }
   126|             [MethodImpl(MethodImplOptions.NoInlining)]
   127|             private int LocalPush_HandleTailOverflow()
   128|             {
   129|                 bool lockTaken = false;
   130|                 try
   131|                 {
   132|                     m_foreignLock.Enter(ref lockTaken);
   133|                     int tail = m_tailIndex;
   134|                     if (tail == int.MaxValue)
   135|                     {
   136|                         m_headIndex &= m_mask;
   137|                         m_tailIndex = tail = m_tailIndex & m_mask;
   138|                         Debug.Assert(m_headIndex <= m_tailIndex);
   139|                     }
   140|                     return tail;
   141|                 }
   142|                 finally
   143|                 {
   144|                     if (lockTaken)
   145|                         m_foreignLock.Exit(useMemoryBarrier: true);
   146|                 }
   147|             }
   148|             public bool LocalFindAndPop(object obj)
   149|             {
   150|                 if (m_array[(m_tailIndex - 1) & m_mask] == obj)
   151|                 {
   152|                     object? unused = LocalPop();
   153|                     Debug.Assert(unused == null || unused == obj);
   154|                     return unused != null;
   155|                 }
   156|                 for (int i = m_tailIndex - 2; i >= m_headIndex; i--)
   157|                 {
   158|                     if (m_array[i & m_mask] == obj)
   159|                     {
   160|                         bool lockTaken = false;
   161|                         try
   162|                         {
   163|                             m_foreignLock.Enter(ref lockTaken);
   164|                             if (m_array[i & m_mask] == null)
   165|                                 return false;
   166|                             Volatile.Write(ref m_array[i & m_mask], null);
   167|                             if (i == m_tailIndex)
   168|                                 m_tailIndex--;
   169|                             else if (i == m_headIndex)
   170|                                 m_headIndex++;
   171|                             return true;
   172|                         }
   173|                         finally
   174|                         {
   175|                             if (lockTaken)
   176|                                 m_foreignLock.Exit(useMemoryBarrier: false);
   177|                         }
   178|                     }
   179|                 }
   180|                 return false;
   181|             }
   182|             public object? LocalPop() => m_headIndex < m_tailIndex ? LocalPopCore() : null;
   183|             private object? LocalPopCore()
   184|             {
   185|                 while (true)
   186|                 {
   187|                     int tail = m_tailIndex;
   188|                     if (m_headIndex >= tail)
   189|                     {
   190|                         return null;
   191|                     }
   192|                     tail--;
   193|                     Interlocked.Exchange(ref m_tailIndex, tail);
   194|                     if (m_headIndex <= tail)
   195|                     {
   196|                         int idx = tail & m_mask;
   197|                         object? obj = Volatile.Read(ref m_array[idx]);
   198|                         if (obj == null) continue;
   199|                         m_array[idx] = null;
   200|                         return obj;
   201|                     }
   202|                     else
   203|                     {
   204|                         bool lockTaken = false;
   205|                         try
   206|                         {
   207|                             m_foreignLock.Enter(ref lockTaken);
   208|                             if (m_headIndex <= tail)
   209|                             {
   210|                                 int idx = tail & m_mask;
   211|                                 object? obj = Volatile.Read(ref m_array[idx]);
   212|                                 if (obj == null) continue;
   213|                                 m_array[idx] = null;
   214|                                 return obj;
   215|                             }
   216|                             else
   217|                             {
   218|                                 m_tailIndex = tail + 1;
   219|                                 return null;
   220|                             }
   221|                         }
   222|                         finally
   223|                         {
   224|                             if (lockTaken)
   225|                                 m_foreignLock.Exit(useMemoryBarrier: false);
   226|                         }
   227|                     }
   228|                 }
   229|             }
   230|             public bool CanSteal => m_headIndex < m_tailIndex;
   231|             public object? TrySteal(ref bool missedSteal)
   232|             {
   233|                 while (true)
   234|                 {
   235|                     if (CanSteal)
   236|                     {
   237|                         bool taken = false;
   238|                         try
   239|                         {
   240|                             m_foreignLock.TryEnter(ref taken);
   241|                             if (taken)
   242|                             {
   243|                                 int head = m_headIndex;
   244|                                 Interlocked.Exchange(ref m_headIndex, head + 1);
   245|                                 if (head < m_tailIndex)
   246|                                 {
   247|                                     int idx = head & m_mask;
   248|                                     object? obj = Volatile.Read(ref m_array[idx]);
   249|                                     if (obj == null) continue;
   250|                                     m_array[idx] = null;
   251|                                     return obj;
   252|                                 }
   253|                                 else
   254|                                 {
   255|                                     m_headIndex = head;
   256|                                 }
   257|                             }
   258|                         }
   259|                         finally
   260|                         {
   261|                             if (taken)
   262|                                 m_foreignLock.Exit(useMemoryBarrier: false);
   263|                         }
   264|                         missedSteal = true;
   265|                     }
   266|                     return null;
   267|                 }
   268|             }
   269|             public int Count
   270|             {
   271|                 get
   272|                 {
   273|                     bool lockTaken = false;
   274|                     try
   275|                     {
   276|                         m_foreignLock.Enter(ref lockTaken);
   277|                         return Math.Max(0, m_tailIndex - m_headIndex);
   278|                     }
   279|                     finally
   280|                     {
   281|                         if (lockTaken)
   282|                         {
   283|                             m_foreignLock.Exit(useMemoryBarrier: false);
   284|                         }
   285|                     }
   286|                 }
   287|             }
   288|         }
   289| #if CORECLR
   290|         internal static readonly bool s_prioritizationExperiment =
   291|             AppContextConfigHelper.GetBooleanConfig(
   292|                 "System.Threading.ThreadPool.PrioritizationExperiment",
   293|                 "DOTNET_ThreadPool_PrioritizationExperiment",
   294|                 defaultValue: false);
   295| #endif
   296|         private const int ProcessorsPerAssignableWorkItemQueue = 16;
   297|         private static readonly int s_assignableWorkItemQueueCount =
   298|             Environment.ProcessorCount <= 32 ? 0 :
   299|                 (Environment.ProcessorCount + (ProcessorsPerAssignableWorkItemQueue - 1)) / ProcessorsPerAssignableWorkItemQueue;
   300|         private bool _loggingEnabled;
   301|         private bool _dispatchNormalPriorityWorkFirst;
   302|         private int _mayHaveHighPriorityWorkItems;
   303|         internal readonly ConcurrentQueue<object> workItems = new ConcurrentQueue<object>();
   304|         internal readonly ConcurrentQueue<object> highPriorityWorkItems = new ConcurrentQueue<object>();
   305| #if CORECLR
   306|         internal readonly ConcurrentQueue<object> lowPriorityWorkItems =
   307|             s_prioritizationExperiment ? new ConcurrentQueue<object>() : null!;
   308| #endif
   309|         internal readonly ConcurrentQueue<object>[] _assignableWorkItemQueues =
   310|             new ConcurrentQueue<object>[s_assignableWorkItemQueueCount];
   311|         private readonly LowLevelLock _queueAssignmentLock = new();
   312|         private readonly int[] _assignedWorkItemQueueThreadCounts =
   313|             s_assignableWorkItemQueueCount > 0 ? new int[s_assignableWorkItemQueueCount] : Array.Empty<int>();
   314|         [StructLayout(LayoutKind.Sequential)]
   315|         private struct CacheLineSeparated
   316|         {
   317|             private readonly Internal.PaddingFor32 pad1;
   318|             public int hasOutstandingThreadRequest;
   319|             private readonly Internal.PaddingFor32 pad2;
   320|         }
   321|         private CacheLineSeparated _separated;
   322|         public ThreadPoolWorkQueue()
   323|         {
   324|             for (int i = 0; i < s_assignableWorkItemQueueCount; i++)
   325|             {
   326|                 _assignableWorkItemQueues[i] = new ConcurrentQueue<object>();
   327|             }
   328|             RefreshLoggingEnabled();
   329|         }
   330|         private void AssignWorkItemQueue(ThreadPoolWorkQueueThreadLocals tl)
   331|         {
   332|             Debug.Assert(s_assignableWorkItemQueueCount > 0);
   333|             _queueAssignmentLock.Acquire();
   334|             int queueIndex = -1;
   335|             int minCount = int.MaxValue;
   336|             int minCountQueueIndex = 0;
   337|             for (int i = 0; i < s_assignableWorkItemQueueCount; i++)
   338|             {
   339|                 int count = _assignedWorkItemQueueThreadCounts[i];
   340|                 Debug.Assert(count >= 0);
   341|                 if (count < ProcessorsPerAssignableWorkItemQueue)
   342|                 {
   343|                     queueIndex = i;
   344|                     _assignedWorkItemQueueThreadCounts[queueIndex] = count + 1;
   345|                     break;
   346|                 }
   347|                 if (count < minCount)
   348|                 {
   349|                     minCount = count;
   350|                     minCountQueueIndex = i;
   351|                 }
   352|             }
   353|             if (queueIndex < 0)
   354|             {
   355|                 queueIndex = minCountQueueIndex;
   356|                 _assignedWorkItemQueueThreadCounts[queueIndex]++;
   357|             }
   358|             _queueAssignmentLock.Release();
   359|             tl.queueIndex = queueIndex;
   360|             tl.assignedGlobalWorkItemQueue = _assignableWorkItemQueues[queueIndex];
   361|         }
   362|         private void TryReassignWorkItemQueue(ThreadPoolWorkQueueThreadLocals tl)
   363|         {
   364|             Debug.Assert(s_assignableWorkItemQueueCount > 0);
   365|             int queueIndex = tl.queueIndex;
   366|             if (queueIndex == 0)
   367|             {
   368|                 return;
   369|             }
   370|             if (!_queueAssignmentLock.TryAcquire())
   371|             {
   372|                 return;
   373|             }
   374|             Debug.Assert(_assignedWorkItemQueueThreadCounts[queueIndex] >= 0);
   375|             if (_assignedWorkItemQueueThreadCounts[queueIndex] > 1)
   376|             {
   377|                 for (int i = 0; i < queueIndex; i++)
   378|                 {
   379|                     if (_assignedWorkItemQueueThreadCounts[i] < ProcessorsPerAssignableWorkItemQueue)
   380|                     {
   381|                         _assignedWorkItemQueueThreadCounts[queueIndex]--;
   382|                         queueIndex = i;
   383|                         _assignedWorkItemQueueThreadCounts[queueIndex]++;
   384|                         break;
   385|                     }
   386|                 }
   387|             }
   388|             _queueAssignmentLock.Release();
   389|             tl.queueIndex = queueIndex;
   390|             tl.assignedGlobalWorkItemQueue = _assignableWorkItemQueues[queueIndex];
   391|         }
   392|         private void UnassignWorkItemQueue(ThreadPoolWorkQueueThreadLocals tl)
   393|         {
   394|             Debug.Assert(s_assignableWorkItemQueueCount > 0);
   395|             int queueIndex = tl.queueIndex;
   396|             _queueAssignmentLock.Acquire();
   397|             int newCount = --_assignedWorkItemQueueThreadCounts[queueIndex];
   398|             _queueAssignmentLock.Release();
   399|             Debug.Assert(newCount >= 0);
   400|             if (newCount > 0)
   401|             {
   402|                 return;
   403|             }
   404|             bool movedWorkItem = false;
   405|             ConcurrentQueue<object> queue = tl.assignedGlobalWorkItemQueue;
   406|             while (_assignedWorkItemQueueThreadCounts[queueIndex] <= 0 && queue.TryDequeue(out object? workItem))
   407|             {
   408|                 workItems.Enqueue(workItem);
   409|                 movedWorkItem = true;
   410|             }
   411|             if (movedWorkItem)
   412|             {
   413|                 EnsureThreadRequested();
   414|             }
   415|         }
   416|         public ThreadPoolWorkQueueThreadLocals GetOrCreateThreadLocals() =>
   417|             ThreadPoolWorkQueueThreadLocals.threadLocals ?? CreateThreadLocals();
   418|         [MethodImpl(MethodImplOptions.NoInlining)]
   419|         private ThreadPoolWorkQueueThreadLocals CreateThreadLocals()
   420|         {
   421|             Debug.Assert(ThreadPoolWorkQueueThreadLocals.threadLocals == null);
   422|             return ThreadPoolWorkQueueThreadLocals.threadLocals = new ThreadPoolWorkQueueThreadLocals(this);
   423|         }
   424|         [MethodImpl(MethodImplOptions.AggressiveInlining)]
   425|         public void RefreshLoggingEnabled()
   426|         {
   427|             if (!FrameworkEventSource.Log.IsEnabled())
   428|             {
   429|                 if (_loggingEnabled)
   430|                 {
   431|                     _loggingEnabled = false;
   432|                 }
   433|                 return;
   434|             }
   435|             RefreshLoggingEnabledFull();
   436|         }
   437|         [MethodImpl(MethodImplOptions.NoInlining)]
   438|         public void RefreshLoggingEnabledFull()
   439|         {
   440|             _loggingEnabled = FrameworkEventSource.Log.IsEnabled(EventLevel.Verbose, FrameworkEventSource.Keywords.ThreadPool | FrameworkEventSource.Keywords.ThreadTransfer);
   441|         }
   442|         [MethodImpl(MethodImplOptions.AggressiveInlining)]
   443|         internal void EnsureThreadRequested()
   444|         {
   445|             if (Interlocked.CompareExchange(ref _separated.hasOutstandingThreadRequest, 1, 0) == 0)
   446|             {
   447|                 ThreadPool.RequestWorkerThread();
   448|             }
   449|         }
   450|         [MethodImpl(MethodImplOptions.AggressiveInlining)]
   451|         internal void MarkThreadRequestSatisfied()
   452|         {
   453|             _separated.hasOutstandingThreadRequest = 0;
   454|             Interlocked.MemoryBarrier();
   455|         }
   456|         public void Enqueue(object callback, bool forceGlobal)
   457|         {
   458|             Debug.Assert((callback is IThreadPoolWorkItem) ^ (callback is Task));
   459|             if (_loggingEnabled && FrameworkEventSource.Log.IsEnabled())
   460|                 FrameworkEventSource.Log.ThreadPoolEnqueueWorkObject(callback);
   461| #if CORECLR
   462|             if (s_prioritizationExperiment)
   463|             {
   464|                 EnqueueForPrioritizationExperiment(callback, forceGlobal);
   465|             }
   466|             else
   467| #endif
   468|             {
   469|                 ThreadPoolWorkQueueThreadLocals? tl;
   470|                 if (!forceGlobal && (tl = ThreadPoolWorkQueueThreadLocals.threadLocals) != null)
   471|                 {
   472|                     tl.workStealingQueue.LocalPush(callback);
   473|                 }
   474|                 else
   475|                 {
   476|                     ConcurrentQueue<object> queue =
   477|                         s_assignableWorkItemQueueCount > 0 && (tl = ThreadPoolWorkQueueThreadLocals.threadLocals) != null
   478|                             ? tl.assignedGlobalWorkItemQueue
   479|                             : workItems;
   480|                     queue.Enqueue(callback);
   481|                 }
   482|             }
   483|             EnsureThreadRequested();
   484|         }
   485| #if CORECLR
   486|         [MethodImpl(MethodImplOptions.NoInlining)]
   487|         private void EnqueueForPrioritizationExperiment(object callback, bool forceGlobal)
   488|         {
   489|             ThreadPoolWorkQueueThreadLocals? tl = ThreadPoolWorkQueueThreadLocals.threadLocals;
   490|             if (!forceGlobal && tl != null)
   491|             {
   492|                 tl.workStealingQueue.LocalPush(callback);
   493|                 return;
   494|             }
   495|             ConcurrentQueue<object> queue;
   496|             if (tl == null && callback is QueueUserWorkItemCallbackBase)
   497|             {
   498|                 queue = lowPriorityWorkItems;
   499|             }
   500|             else if (s_assignableWorkItemQueueCount > 0 && tl != null)
   501|             {
   502|                 queue = tl.assignedGlobalWorkItemQueue;
   503|             }
   504|             else
   505|             {
   506|                 queue = workItems;
   507|             }
   508|             queue.Enqueue(callback);
   509|         }
   510| #endif
   511|         public void EnqueueAtHighPriority(object workItem)
   512|         {
   513|             Debug.Assert((workItem is IThreadPoolWorkItem) ^ (workItem is Task));
   514|             if (_loggingEnabled && FrameworkEventSource.Log.IsEnabled())
   515|                 FrameworkEventSource.Log.ThreadPoolEnqueueWorkObject(workItem);
   516|             highPriorityWorkItems.Enqueue(workItem);
   517|             Volatile.Write(ref _mayHaveHighPriorityWorkItems, 1);
   518|             EnsureThreadRequested();
   519|         }
   520|         internal static void TransferAllLocalWorkItemsToHighPriorityGlobalQueue()
   521|         {
   522|             if (ThreadPoolWorkQueueThreadLocals.threadLocals is not ThreadPoolWorkQueueThreadLocals tl)
   523|             {
   524|                 return;
   525|             }
   526|             ThreadPoolWorkQueue queue = ThreadPool.s_workQueue;
   527|             while (tl.workStealingQueue.LocalPop() is object workItem)
   528|             {
   529|                 queue.highPriorityWorkItems.Enqueue(workItem);
   530|             }
   531|             Volatile.Write(ref queue._mayHaveHighPriorityWorkItems, 1);
   532|             queue.EnsureThreadRequested();
   533|         }
   534|         internal static bool LocalFindAndPop(object callback)
   535|         {
   536|             ThreadPoolWorkQueueThreadLocals? tl = ThreadPoolWorkQueueThreadLocals.threadLocals;
   537|             return tl != null && tl.workStealingQueue.LocalFindAndPop(callback);
   538|         }
   539|         public object? Dequeue(ThreadPoolWorkQueueThreadLocals tl, ref bool missedSteal)
   540|         {
   541|             object? workItem = tl.workStealingQueue.LocalPop();
   542|             if (workItem != null)
   543|             {
   544|                 return workItem;
   545|             }
   546|             if (tl.isProcessingHighPriorityWorkItems)
   547|             {
   548|                 if (highPriorityWorkItems.TryDequeue(out workItem))
   549|                 {
   550|                     return workItem;
   551|                 }
   552|                 tl.isProcessingHighPriorityWorkItems = false;
   553|             }
   554|             else if (
   555|                 _mayHaveHighPriorityWorkItems != 0 &&
   556|                 Interlocked.CompareExchange(ref _mayHaveHighPriorityWorkItems, 0, 1) != 0 &&
   557|                 TryStartProcessingHighPriorityWorkItemsAndDequeue(tl, out workItem))
   558|             {
   559|                 return workItem;
   560|             }
   561|             if (s_assignableWorkItemQueueCount > 0 && tl.assignedGlobalWorkItemQueue.TryDequeue(out workItem))
   562|             {
   563|                 return workItem;
   564|             }
   565|             if (workItems.TryDequeue(out workItem))
   566|             {
   567|                 return workItem;
   568|             }
   569|             uint randomValue = tl.random.NextUInt32();
   570|             if (s_assignableWorkItemQueueCount > 0)
   571|             {
   572|                 int queueIndex = tl.queueIndex;
   573|                 int c = s_assignableWorkItemQueueCount;
   574|                 int maxIndex = c - 1;
   575|                 for (int i = (int)(randomValue % (uint)c); c > 0; i = i < maxIndex ? i + 1 : 0, c--)
   576|                 {
   577|                     if (i != queueIndex && _assignableWorkItemQueues[i].TryDequeue(out workItem))
   578|                     {
   579|                         return workItem;
   580|                     }
   581|                 }
   582|             }
   583| #if CORECLR
   584|             if (s_prioritizationExperiment && lowPriorityWorkItems.TryDequeue(out workItem))
   585|             {
   586|                 return workItem;
   587|             }
   588| #endif
   589|             {
   590|                 WorkStealingQueue localWsq = tl.workStealingQueue;
   591|                 WorkStealingQueue[] queues = WorkStealingQueueList.Queues;
   592|                 int c = queues.Length;
   593|                 Debug.Assert(c > 0, "There must at least be a queue for this thread.");
   594|                 int maxIndex = c - 1;
   595|                 for (int i = (int)(randomValue % (uint)c); c > 0; i = i < maxIndex ? i + 1 : 0, c--)
   596|                 {
   597|                     WorkStealingQueue otherQueue = queues[i];
   598|                     if (otherQueue != localWsq && otherQueue.CanSteal)
   599|                     {
   600|                         workItem = otherQueue.TrySteal(ref missedSteal);
   601|                         if (workItem != null)
   602|                         {
   603|                             return workItem;
   604|                         }
   605|                     }
   606|                 }
   607|             }
   608|             return null;
   609|         }
   610|         [MethodImpl(MethodImplOptions.NoInlining)]
   611|         private bool TryStartProcessingHighPriorityWorkItemsAndDequeue(
   612|             ThreadPoolWorkQueueThreadLocals tl,
   613|             [MaybeNullWhen(false)] out object workItem)
   614|         {
   615|             Debug.Assert(!tl.isProcessingHighPriorityWorkItems);
   616|             if (!highPriorityWorkItems.TryDequeue(out workItem))
   617|             {
   618|                 return false;
   619|             }
   620|             tl.isProcessingHighPriorityWorkItems = true;
   621|             _mayHaveHighPriorityWorkItems = 1;
   622|             return true;
   623|         }
   624|         public static long LocalCount
   625|         {
   626|             get
   627|             {
   628|                 long count = 0;
   629|                 foreach (WorkStealingQueue workStealingQueue in WorkStealingQueueList.Queues)
   630|                 {
   631|                     count += workStealingQueue.Count;
   632|                 }
   633|                 return count;
   634|             }
   635|         }
   636|         public long GlobalCount
   637|         {
   638|             get
   639|             {
   640|                 long count = (long)highPriorityWorkItems.Count + workItems.Count;
   641| #if CORECLR
   642|                 if (s_prioritizationExperiment)
   643|                 {
   644|                     count += lowPriorityWorkItems.Count;
   645|                 }
   646| #endif
   647|                 for (int i = 0; i < s_assignableWorkItemQueueCount; i++)
   648|                 {
   649|                     count += _assignableWorkItemQueues[i].Count;
   650|                 }
   651|                 return count;
   652|             }
   653|         }
   654|         public const uint DispatchQuantumMs = 30;
   655|         internal static bool Dispatch()
   656|         {
   657|             ThreadPoolWorkQueue workQueue = ThreadPool.s_workQueue;
   658|             ThreadPoolWorkQueueThreadLocals tl = workQueue.GetOrCreateThreadLocals();
   659|             if (s_assignableWorkItemQueueCount > 0)
   660|             {
   661|                 workQueue.AssignWorkItemQueue(tl);
   662|             }
   663|             workQueue.MarkThreadRequestSatisfied();
   664|             object? workItem = null;
   665|             {
   666|                 bool dispatchNormalPriorityWorkFirst = workQueue._dispatchNormalPriorityWorkFirst;
   667|                 if (dispatchNormalPriorityWorkFirst && !tl.workStealingQueue.CanSteal)
   668|                 {
   669|                     workQueue._dispatchNormalPriorityWorkFirst = !dispatchNormalPriorityWorkFirst;
   670|                     ConcurrentQueue<object> queue =
   671|                         s_assignableWorkItemQueueCount > 0 ? tl.assignedGlobalWorkItemQueue : workQueue.workItems;
   672|                     if (!queue.TryDequeue(out workItem) && s_assignableWorkItemQueueCount > 0)
   673|                     {
   674|                         workQueue.workItems.TryDequeue(out workItem);
   675|                     }
   676|                 }
   677|                 if (workItem == null)
   678|                 {
   679|                     bool missedSteal = false;
   680|                     workItem = workQueue.Dequeue(tl, ref missedSteal);
   681|                     if (workItem == null)
   682|                     {
   683|                         if (s_assignableWorkItemQueueCount > 0)
   684|                         {
   685|                             workQueue.UnassignWorkItemQueue(tl);
   686|                         }
   687|                         if (missedSteal)
   688|                         {
   689|                             workQueue.EnsureThreadRequested();
   690|                         }
   691|                         return true;
   692|                     }
   693|                 }
   694|                 workQueue.EnsureThreadRequested();
   695|             }
   696|             workQueue.RefreshLoggingEnabled();
   697|             object? threadLocalCompletionCountObject = tl.threadLocalCompletionCountObject;
   698|             Thread currentThread = tl.currentThread;
   699|             currentThread._executionContext = null;
   700|             currentThread._synchronizationContext = null;
   701|             int startTickCount = Environment.TickCount;
   702|             while (true)
   703|             {
   704|                 if (workItem == null)
   705|                 {
   706|                     bool missedSteal = false;
   707|                     workItem = workQueue.Dequeue(tl, ref missedSteal);
   708|                     if (workItem == null)
   709|                     {
   710|                         if (s_assignableWorkItemQueueCount > 0)
   711|                         {
   712|                             workQueue.UnassignWorkItemQueue(tl);
   713|                         }
   714|                         if (missedSteal)
   715|                         {
   716|                             workQueue.EnsureThreadRequested();
   717|                         }
   718|                         return true;
   719|                     }
   720|                 }
   721|                 if (workQueue._loggingEnabled && FrameworkEventSource.Log.IsEnabled())
   722|                 {
   723|                     FrameworkEventSource.Log.ThreadPoolDequeueWorkObject(workItem);
   724|                 }
   725| #if FEATURE_OBJCMARSHAL
   726|                 if (AutoreleasePool.EnableAutoreleasePool)
   727|                 {
   728|                     DispatchItemWithAutoreleasePool(workItem, currentThread);
   729|                 }
   730|                 else
   731| #endif
   732| #pragma warning disable CS0162 // Unreachable code detected. EnableWorkerTracking may be a constant in some runtimes.
   733|                 if (ThreadPool.EnableWorkerTracking)
   734|                 {
   735|                     DispatchWorkItemWithWorkerTracking(workItem, currentThread);
   736|                 }
   737|                 else
   738|                 {
   739|                     DispatchWorkItem(workItem, currentThread);
   740|                 }
   741| #pragma warning restore CS0162
   742|                 workItem = null;
   743|                 ExecutionContext.ResetThreadPoolThread(currentThread);
   744|                 currentThread.ResetThreadPoolThread();
   745|                 int currentTickCount = Environment.TickCount;
   746|                 if (!ThreadPool.NotifyWorkItemComplete(threadLocalCompletionCountObject!, currentTickCount))
   747|                 {
   748|                     tl.TransferLocalWork();
   749|                     tl.isProcessingHighPriorityWorkItems = false;
   750|                     if (s_assignableWorkItemQueueCount > 0)
   751|                     {
   752|                         workQueue.UnassignWorkItemQueue(tl);
   753|                     }
   754|                     return false;
   755|                 }
   756|                 if ((uint)(currentTickCount - startTickCount) < DispatchQuantumMs)
   757|                 {
   758|                     continue;
   759|                 }
   760|                 if (ThreadPool.YieldFromDispatchLoop)
   761|                 {
   762|                     tl.isProcessingHighPriorityWorkItems = false;
   763|                     if (s_assignableWorkItemQueueCount > 0)
   764|                     {
   765|                         workQueue.UnassignWorkItemQueue(tl);
   766|                     }
   767|                     return true;
   768|                 }
   769|                 if (s_assignableWorkItemQueueCount > 0)
   770|                 {
   771|                     workQueue.TryReassignWorkItemQueue(tl);
   772|                 }
   773|                 startTickCount = currentTickCount;
   774|                 workQueue.RefreshLoggingEnabled();
   775|             }
   776|         }
   777|         [MethodImpl(MethodImplOptions.NoInlining)]
   778|         private static void DispatchWorkItemWithWorkerTracking(object workItem, Thread currentThread)
   779|         {
   780|             Debug.Assert(ThreadPool.EnableWorkerTracking);
   781|             Debug.Assert(currentThread == Thread.CurrentThread);
   782|             bool reportedStatus = false;
   783|             try
   784|             {
   785|                 ThreadPool.ReportThreadStatus(isWorking: true);
   786|                 reportedStatus = true;
   787|                 DispatchWorkItem(workItem, currentThread);
   788|             }
   789|             finally
   790|             {
   791|                 if (reportedStatus)
   792|                     ThreadPool.ReportThreadStatus(isWorking: false);
   793|             }
   794|         }
   795|         [MethodImpl(MethodImplOptions.AggressiveInlining)]
   796|         private static void DispatchWorkItem(object workItem, Thread currentThread)
   797|         {
   798|             if (workItem is Task task)
   799|             {
   800|                 task.ExecuteFromThreadPool(currentThread);
   801|             }
   802|             else
   803|             {
   804|                 Debug.Assert(workItem is IThreadPoolWorkItem);
   805|                 Unsafe.As<IThreadPoolWorkItem>(workItem).Execute();
   806|             }
   807|         }
   808|     }
   809|     internal sealed class ThreadPoolWorkQueueThreadLocals
   810|     {
   811|         [ThreadStatic]
   812|         public static ThreadPoolWorkQueueThreadLocals? threadLocals;
   813|         public bool isProcessingHighPriorityWorkItems;
   814|         public int queueIndex;
   815|         public ConcurrentQueue<object> assignedGlobalWorkItemQueue;
   816|         public readonly ThreadPoolWorkQueue workQueue;
   817|         public readonly ThreadPoolWorkQueue.WorkStealingQueue workStealingQueue;
   818|         public readonly Thread currentThread;
   819|         public readonly object? threadLocalCompletionCountObject;
   820|         public readonly Random.XoshiroImpl random = new Random.XoshiroImpl();
   821|         public ThreadPoolWorkQueueThreadLocals(ThreadPoolWorkQueue tpq)
   822|         {
   823|             assignedGlobalWorkItemQueue = tpq.workItems;
   824|             workQueue = tpq;
   825|             workStealingQueue = new ThreadPoolWorkQueue.WorkStealingQueue();
   826|             ThreadPoolWorkQueue.WorkStealingQueueList.Add(workStealingQueue);
   827|             currentThread = Thread.CurrentThread;
   828|             threadLocalCompletionCountObject = ThreadPool.GetOrCreateThreadLocalCompletionCountObject();
   829|         }
   830|         public void TransferLocalWork()
   831|         {
   832|             while (workStealingQueue.LocalPop() is object cb)
   833|             {
   834|                 workQueue.Enqueue(cb, forceGlobal: true);
   835|             }
   836|         }
   837|         ~ThreadPoolWorkQueueThreadLocals()
   838|         {
   839|             if (null != workStealingQueue)
   840|             {
   841|                 TransferLocalWork();
   842|                 ThreadPoolWorkQueue.WorkStealingQueueList.Remove(workStealingQueue);
   843|             }
   844|         }
   845|     }
   846|     internal interface IThreadPoolTypedWorkItemQueueCallback<T>
   847|     {
   848|         static abstract void Invoke(T item);
   849|     }
   850|     internal sealed class ThreadPoolTypedWorkItemQueue<T, TCallback> : IThreadPoolWorkItem
   851|         where T : struct
   852|         where TCallback : struct, IThreadPoolTypedWorkItemQueueCallback<T>
   853|     {
   854|         private int _isScheduledForProcessing;
   855|         private readonly ConcurrentQueue<T> _workItems = new ConcurrentQueue<T>();
   856|         public int Count => _workItems.Count;
   857|         public void Enqueue(T workItem)
   858|         {
   859|             BatchEnqueue(workItem);
   860|             CompleteBatchEnqueue();
   861|         }
   862|         public void BatchEnqueue(T workItem) => _workItems.Enqueue(workItem);
   863|         public void CompleteBatchEnqueue() => ScheduleForProcessing();
   864|         private void ScheduleForProcessing()
   865|         {
   866|             if (Interlocked.CompareExchange(ref _isScheduledForProcessing, 1, 0) == 0)
   867|             {
   868|                 ThreadPool.UnsafeQueueHighPriorityWorkItemInternal(this);
   869|             }
   870|         }
   871|         void IThreadPoolWorkItem.Execute()
   872|         {
   873|             Debug.Assert(_isScheduledForProcessing != 0);
   874|             _isScheduledForProcessing = 0;
   875|             Interlocked.MemoryBarrier();
   876|             if (!_workItems.TryDequeue(out T workItem))
   877|             {
   878|                 return;
   879|             }
   880|             ScheduleForProcessing();
   881|             ThreadPoolWorkQueueThreadLocals tl = ThreadPoolWorkQueueThreadLocals.threadLocals!;
   882|             Debug.Assert(tl != null);
   883|             Thread currentThread = tl.currentThread;
   884|             Debug.Assert(currentThread == Thread.CurrentThread);
   885|             uint completedCount = 0;
   886|             int startTimeMs = Environment.TickCount;
   887|             while (true)
   888|             {
   889|                 TCallback.Invoke(workItem);
   890|                 if (++completedCount == uint.MaxValue ||
   891|                     tl.workStealingQueue.CanSteal ||
   892|                     (uint)(Environment.TickCount - startTimeMs) >= ThreadPoolWorkQueue.DispatchQuantumMs / 2 ||
   893|                     !_workItems.TryDequeue(out workItem))
   894|                 {
   895|                     break;
   896|                 }
   897|                 ExecutionContext.ResetThreadPoolThread(currentThread);
   898|                 currentThread.ResetThreadPoolThread();
   899|             }
   900|             ThreadInt64PersistentCounter.Add(tl.threadLocalCompletionCountObject!, completedCount);
   901|         }
   902|     }
   903|     public delegate void WaitCallback(object? state);
   904|     public delegate void WaitOrTimerCallback(object? state, bool timedOut);  // signaled or timed out
   905|     internal abstract class QueueUserWorkItemCallbackBase : IThreadPoolWorkItem
   906|     {
   907| #if DEBUG
   908|         private int executed;
   909|         ~QueueUserWorkItemCallbackBase()
   910|         {
   911|             Interlocked.MemoryBarrier(); // ensure that an old cached value is not read below
   912|             Debug.Assert(
   913|                 executed != 0, "A QueueUserWorkItemCallback was never called!");
   914|         }
   915| #endif
   916|         public virtual void Execute()
   917|         {
   918| #if DEBUG
   919|             GC.SuppressFinalize(this);
   920|             Debug.Assert(
   921|                 0 == Interlocked.Exchange(ref executed, 1),
   922|                 "A QueueUserWorkItemCallback was called twice!");
   923| #endif
   924|         }
   925|     }
   926|     internal sealed class QueueUserWorkItemCallback : QueueUserWorkItemCallbackBase
   927|     {
   928|         private WaitCallback? _callback; // SOS's ThreadPool command depends on this name
   929|         private readonly object? _state;
   930|         private readonly ExecutionContext _context;
   931|         private static readonly Action<QueueUserWorkItemCallback> s_executionContextShim = quwi =>
   932|         {
   933|             Debug.Assert(quwi._callback != null);
   934|             WaitCallback callback = quwi._callback;
   935|             quwi._callback = null;
   936|             callback(quwi._state);
   937|         };
   938|         internal QueueUserWorkItemCallback(WaitCallback callback, object? state, ExecutionContext context)
   939|         {
   940|             Debug.Assert(context != null);
   941|             _callback = callback;
   942|             _state = state;
   943|             _context = context;
   944|         }
   945|         public override void Execute()
   946|         {
   947|             base.Execute();
   948|             ExecutionContext.RunForThreadPoolUnsafe(_context, s_executionContextShim, this);
   949|         }
   950|     }
   951|     internal sealed class QueueUserWorkItemCallback<TState> : QueueUserWorkItemCallbackBase
   952|     {
   953|         private Action<TState>? _callback; // SOS's ThreadPool command depends on this name
   954|         private readonly TState _state;
   955|         private readonly ExecutionContext _context;
   956|         internal QueueUserWorkItemCallback(Action<TState> callback, TState state, ExecutionContext context)
   957|         {
   958|             Debug.Assert(callback != null);
   959|             _callback = callback;
   960|             _state = state;
   961|             _context = context;
   962|         }
   963|         public override void Execute()
   964|         {
   965|             base.Execute();
   966|             Debug.Assert(_callback != null);
   967|             Action<TState> callback = _callback;
   968|             _callback = null;
   969|             ExecutionContext.RunForThreadPoolUnsafe(_context, callback, in _state);
   970|         }
   971|     }
   972|     internal sealed class QueueUserWorkItemCallbackDefaultContext : QueueUserWorkItemCallbackBase
   973|     {
   974|         private WaitCallback? _callback; // SOS's ThreadPool command depends on this name
   975|         private readonly object? _state;
   976|         internal QueueUserWorkItemCallbackDefaultContext(WaitCallback callback, object? state)
   977|         {
   978|             Debug.Assert(callback != null);
   979|             _callback = callback;
   980|             _state = state;
   981|         }
   982|         public override void Execute()
   983|         {
   984|             ExecutionContext.CheckThreadPoolAndContextsAreDefault();
   985|             base.Execute();
   986|             Debug.Assert(_callback != null);
   987|             WaitCallback callback = _callback;
   988|             _callback = null;
   989|             callback(_state);
   990|         }
   991|     }
   992|     internal sealed class QueueUserWorkItemCallbackDefaultContext<TState> : QueueUserWorkItemCallbackBase
   993|     {
   994|         private Action<TState>? _callback; // SOS's ThreadPool command depends on this name
   995|         private readonly TState _state;
   996|         internal QueueUserWorkItemCallbackDefaultContext(Action<TState> callback, TState state)
   997|         {
   998|             Debug.Assert(callback != null);
   999|             _callback = callback;
  1000|             _state = state;
  1001|         }
  1002|         public override void Execute()
  1003|         {
  1004|             ExecutionContext.CheckThreadPoolAndContextsAreDefault();
  1005|             base.Execute();
  1006|             Debug.Assert(_callback != null);
  1007|             Action<TState> callback = _callback;
  1008|             _callback = null;
  1009|             callback(_state);
  1010|         }
  1011|     }
  1012|     internal sealed class _ThreadPoolWaitOrTimerCallback
  1013|     {
  1014|         private readonly WaitOrTimerCallback _waitOrTimerCallback;
  1015|         private readonly ExecutionContext? _executionContext;
  1016|         private readonly object? _state;
  1017|         private static readonly ContextCallback _ccbt = new ContextCallback(WaitOrTimerCallback_Context_t);
  1018|         private static readonly ContextCallback _ccbf = new ContextCallback(WaitOrTimerCallback_Context_f);
  1019|         internal _ThreadPoolWaitOrTimerCallback(WaitOrTimerCallback waitOrTimerCallback, object? state, bool flowExecutionContext)
  1020|         {
  1021|             _waitOrTimerCallback = waitOrTimerCallback;
  1022|             _state = state;
  1023|             if (flowExecutionContext)
  1024|             {
  1025|                 _executionContext = ExecutionContext.Capture();
  1026|             }
  1027|         }
  1028|         private static void WaitOrTimerCallback_Context_t(object? state) =>
  1029|             WaitOrTimerCallback_Context(state, timedOut: true);
  1030|         private static void WaitOrTimerCallback_Context_f(object? state) =>
  1031|             WaitOrTimerCallback_Context(state, timedOut: false);
  1032|         private static void WaitOrTimerCallback_Context(object? state, bool timedOut)
  1033|         {
  1034|             _ThreadPoolWaitOrTimerCallback helper = (_ThreadPoolWaitOrTimerCallback)state!;
  1035|             helper._waitOrTimerCallback(helper._state, timedOut);
  1036|         }
  1037|         internal static void PerformWaitOrTimerCallback(_ThreadPoolWaitOrTimerCallback helper, bool timedOut)
  1038|         {
  1039|             Debug.Assert(helper != null, "Null state passed to PerformWaitOrTimerCallback!");
  1040|             ExecutionContext? context = helper._executionContext;
  1041|             if (context == null)
  1042|             {
  1043|                 WaitOrTimerCallback callback = helper._waitOrTimerCallback;
  1044|                 callback(helper._state, timedOut);
  1045|             }
  1046|             else
  1047|             {
  1048|                 ExecutionContext.Run(context, timedOut ? _ccbt : _ccbf, helper);
  1049|             }
  1050|         }
  1051|     }
  1052|     public static partial class ThreadPool
  1053|     {
  1054|         internal const string WorkerThreadName = ".NET TP Worker";
  1055|         internal static readonly ThreadPoolWorkQueue s_workQueue = new ThreadPoolWorkQueue();
  1056|         internal static readonly Action<object?> s_invokeAsyncStateMachineBox = static state =>
  1057|         {
  1058|             if (state is IAsyncStateMachineBox box)
  1059|             {
  1060|                 box.MoveNext();
  1061|             }
  1062|             else
  1063|             {
  1064|                 ThrowHelper.ThrowUnexpectedStateForKnownCallback(state);
  1065|             }
  1066|         };
  1067|         internal static bool EnableWorkerTracking => IsWorkerTrackingEnabledInConfig && EventSource.IsSupported;
  1068| #if !FEATURE_WASM_THREADS
  1069|         [UnsupportedOSPlatform("browser")]
  1070| #endif
  1071|         [CLSCompliant(false)]
  1072|         public static RegisteredWaitHandle RegisterWaitForSingleObject(
  1073|              WaitHandle waitObject,
  1074|              WaitOrTimerCallback callBack,
  1075|              object? state,
  1076|              uint millisecondsTimeOutInterval,
  1077|              bool executeOnlyOnce    // NOTE: we do not allow other options that allow the callback to be queued as an APC
  1078|              )
  1079|         {
  1080|             if (millisecondsTimeOutInterval > (uint)int.MaxValue && millisecondsTimeOutInterval != uint.MaxValue)
  1081|                 throw new ArgumentOutOfRangeException(nameof(millisecondsTimeOutInterval), SR.ArgumentOutOfRange_LessEqualToIntegerMaxVal);
  1082|             return RegisterWaitForSingleObject(waitObject, callBack, state, millisecondsTimeOutInterval, executeOnlyOnce, true);
  1083|         }
  1084| #if !FEATURE_WASM_THREADS
  1085|         [UnsupportedOSPlatform("browser")]
  1086| #endif
  1087|         [CLSCompliant(false)]
  1088|         public static RegisteredWaitHandle UnsafeRegisterWaitForSingleObject(
  1089|              WaitHandle waitObject,
  1090|              WaitOrTimerCallback callBack,
  1091|              object? state,
  1092|              uint millisecondsTimeOutInterval,
  1093|              bool executeOnlyOnce    // NOTE: we do not allow other options that allow the callback to be queued as an APC
  1094|              )
  1095|         {
  1096|             if (millisecondsTimeOutInterval > (uint)int.MaxValue && millisecondsTimeOutInterval != uint.MaxValue)
  1097|                 throw new ArgumentOutOfRangeException(nameof(millisecondsTimeOutInterval), SR.ArgumentOutOfRange_NeedNonNegOrNegative1);
  1098|             return RegisterWaitForSingleObject(waitObject, callBack, state, millisecondsTimeOutInterval, executeOnlyOnce, false);
  1099|         }
  1100| #if !FEATURE_WASM_THREADS
  1101|         [UnsupportedOSPlatform("browser")]
  1102| #endif
  1103|         public static RegisteredWaitHandle RegisterWaitForSingleObject(
  1104|              WaitHandle waitObject,
  1105|              WaitOrTimerCallback callBack,
  1106|              object? state,
  1107|              int millisecondsTimeOutInterval,
  1108|              bool executeOnlyOnce    // NOTE: we do not allow other options that allow the callback to be queued as an APC
  1109|              )
  1110|         {
  1111|             ArgumentOutOfRangeException.ThrowIfLessThan(millisecondsTimeOutInterval, -1);
  1112|             return RegisterWaitForSingleObject(waitObject, callBack, state, (uint)millisecondsTimeOutInterval, executeOnlyOnce, true);
  1113|         }
  1114| #if !FEATURE_WASM_THREADS
  1115|         [UnsupportedOSPlatform("browser")]
  1116| #endif
  1117|         public static RegisteredWaitHandle UnsafeRegisterWaitForSingleObject(
  1118|              WaitHandle waitObject,
  1119|              WaitOrTimerCallback callBack,
  1120|              object? state,
  1121|              int millisecondsTimeOutInterval,
  1122|              bool executeOnlyOnce    // NOTE: we do not allow other options that allow the callback to be queued as an APC
  1123|              )
  1124|         {
  1125|             ArgumentOutOfRangeException.ThrowIfLessThan(millisecondsTimeOutInterval, -1);
  1126|             return RegisterWaitForSingleObject(waitObject, callBack, state, (uint)millisecondsTimeOutInterval, executeOnlyOnce, false);
  1127|         }
  1128| #if !FEATURE_WASM_THREADS
  1129|         [UnsupportedOSPlatform("browser")]
  1130| #endif
  1131|         public static RegisteredWaitHandle RegisterWaitForSingleObject(
  1132|             WaitHandle waitObject,
  1133|             WaitOrTimerCallback callBack,
  1134|             object? state,
  1135|             long millisecondsTimeOutInterval,
  1136|             bool executeOnlyOnce    // NOTE: we do not allow other options that allow the callback to be queued as an APC
  1137|         )
  1138|         {
  1139|             ArgumentOutOfRangeException.ThrowIfLessThan(millisecondsTimeOutInterval, -1);
  1140|             ArgumentOutOfRangeException.ThrowIfGreaterThan(millisecondsTimeOutInterval, int.MaxValue);
  1141|             return RegisterWaitForSingleObject(waitObject, callBack, state, (uint)millisecondsTimeOutInterval, executeOnlyOnce, true);
  1142|         }
  1143| #if !FEATURE_WASM_THREADS
  1144|         [UnsupportedOSPlatform("browser")]
  1145| #endif
  1146|         public static RegisteredWaitHandle UnsafeRegisterWaitForSingleObject(
  1147|             WaitHandle waitObject,
  1148|             WaitOrTimerCallback callBack,
  1149|             object? state,
  1150|             long millisecondsTimeOutInterval,
  1151|             bool executeOnlyOnce    // NOTE: we do not allow other options that allow the callback to be queued as an APC
  1152|         )
  1153|         {
  1154|             ArgumentOutOfRangeException.ThrowIfLessThan(millisecondsTimeOutInterval, -1);
  1155|             ArgumentOutOfRangeException.ThrowIfGreaterThan(millisecondsTimeOutInterval, int.MaxValue);
  1156|             return RegisterWaitForSingleObject(waitObject, callBack, state, (uint)millisecondsTimeOutInterval, executeOnlyOnce, false);
  1157|         }
  1158| #if !FEATURE_WASM_THREADS
  1159|         [UnsupportedOSPlatform("browser")]
  1160| #endif
  1161|         public static RegisteredWaitHandle RegisterWaitForSingleObject(
  1162|                           WaitHandle waitObject,
  1163|                           WaitOrTimerCallback callBack,
  1164|                           object? state,
  1165|                           TimeSpan timeout,
  1166|                           bool executeOnlyOnce
  1167|                           )
  1168|         {
  1169|             long tm = (long)timeout.TotalMilliseconds;
  1170|             ArgumentOutOfRangeException.ThrowIfLessThan(tm, -1, nameof(timeout));
  1171|             ArgumentOutOfRangeException.ThrowIfGreaterThan(tm, int.MaxValue, nameof(timeout));
  1172|             return RegisterWaitForSingleObject(waitObject, callBack, state, (uint)tm, executeOnlyOnce, true);
  1173|         }
  1174| #if !FEATURE_WASM_THREADS
  1175|         [UnsupportedOSPlatform("browser")]
  1176| #endif
  1177|         public static RegisteredWaitHandle UnsafeRegisterWaitForSingleObject(
  1178|                           WaitHandle waitObject,
  1179|                           WaitOrTimerCallback callBack,
  1180|                           object? state,
  1181|                           TimeSpan timeout,
  1182|                           bool executeOnlyOnce
  1183|                           )
  1184|         {
  1185|             long tm = (long)timeout.TotalMilliseconds;
  1186|             ArgumentOutOfRangeException.ThrowIfLessThan(tm, -1, nameof(timeout));
  1187|             ArgumentOutOfRangeException.ThrowIfGreaterThan(tm, int.MaxValue, nameof(timeout));
  1188|             return RegisterWaitForSingleObject(waitObject, callBack, state, (uint)tm, executeOnlyOnce, false);
  1189|         }
  1190|         public static bool QueueUserWorkItem(WaitCallback callBack) =>
  1191|             QueueUserWorkItem(callBack, null);
  1192|         public static bool QueueUserWorkItem(WaitCallback callBack, object? state)
  1193|         {
  1194|             if (callBack == null)
  1195|             {
  1196|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.callBack);
  1197|             }
  1198|             ExecutionContext? context = ExecutionContext.Capture();
  1199|             object tpcallBack = (context == null || context.IsDefault) ?
  1200|                 new QueueUserWorkItemCallbackDefaultContext(callBack!, state) :
  1201|                 (object)new QueueUserWorkItemCallback(callBack!, state, context);
  1202|             s_workQueue.Enqueue(tpcallBack, forceGlobal: true);
  1203|             return true;
  1204|         }
  1205|         public static bool QueueUserWorkItem<TState>(Action<TState> callBack, TState state, bool preferLocal)
  1206|         {
  1207|             if (callBack == null)
  1208|             {
  1209|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.callBack);
  1210|             }
  1211|             ExecutionContext? context = ExecutionContext.Capture();
  1212|             object tpcallBack = (context == null || context.IsDefault) ?
  1213|                 new QueueUserWorkItemCallbackDefaultContext<TState>(callBack!, state) :
  1214|                 (object)new QueueUserWorkItemCallback<TState>(callBack!, state, context);
  1215|             s_workQueue.Enqueue(tpcallBack, forceGlobal: !preferLocal);
  1216|             return true;
  1217|         }
  1218|         public static bool UnsafeQueueUserWorkItem<TState>(Action<TState> callBack, TState state, bool preferLocal)
  1219|         {
  1220|             if (callBack == null)
  1221|             {
  1222|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.callBack);
  1223|             }
  1224|             if (ReferenceEquals(callBack, s_invokeAsyncStateMachineBox))
  1225|             {
  1226|                 if (!(state is IAsyncStateMachineBox))
  1227|                 {
  1228|                     ThrowHelper.ThrowUnexpectedStateForKnownCallback(state);
  1229|                 }
  1230|                 UnsafeQueueUserWorkItemInternal((object)state!, preferLocal);
  1231|                 return true;
  1232|             }
  1233|             s_workQueue.Enqueue(
  1234|                 new QueueUserWorkItemCallbackDefaultContext<TState>(callBack!, state), forceGlobal: !preferLocal);
  1235|             return true;
  1236|         }
  1237|         public static bool UnsafeQueueUserWorkItem(WaitCallback callBack, object? state)
  1238|         {
  1239|             if (callBack == null)
  1240|             {
  1241|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.callBack);
  1242|             }
  1243|             object tpcallBack = new QueueUserWorkItemCallbackDefaultContext(callBack!, state);
  1244|             s_workQueue.Enqueue(tpcallBack, forceGlobal: true);
  1245|             return true;
  1246|         }
  1247|         public static bool UnsafeQueueUserWorkItem(IThreadPoolWorkItem callBack, bool preferLocal)
  1248|         {
  1249|             if (callBack == null)
  1250|             {
  1251|                 ThrowHelper.ThrowArgumentNullException(ExceptionArgument.callBack);
  1252|             }
  1253|             if (callBack is Task)
  1254|             {
  1255|                 ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.callBack);
  1256|             }
  1257|             UnsafeQueueUserWorkItemInternal(callBack!, preferLocal);
  1258|             return true;
  1259|         }
  1260|         internal static void UnsafeQueueUserWorkItemInternal(object callBack, bool preferLocal) =>
  1261|             s_workQueue.Enqueue(callBack, forceGlobal: !preferLocal);
  1262|         internal static void UnsafeQueueHighPriorityWorkItemInternal(IThreadPoolWorkItem callBack) =>
  1263|             s_workQueue.EnqueueAtHighPriority(callBack);
  1264|         internal static bool TryPopCustomWorkItem(object workItem)
  1265|         {
  1266|             Debug.Assert(null != workItem);
  1267|             return ThreadPoolWorkQueue.LocalFindAndPop(workItem);
  1268|         }
  1269|         internal static IEnumerable<object> GetQueuedWorkItems()
  1270|         {
  1271|             foreach (object workItem in s_workQueue.highPriorityWorkItems)
  1272|             {
  1273|                 yield return workItem;
  1274|             }
  1275|             foreach (ConcurrentQueue<object> queue in s_workQueue._assignableWorkItemQueues)
  1276|             {
  1277|                 foreach (object workItem in queue)
  1278|                 {
  1279|                     yield return workItem;
  1280|                 }
  1281|             }
  1282|             foreach (object workItem in s_workQueue.workItems)
  1283|             {
  1284|                 yield return workItem;
  1285|             }
  1286| #if CORECLR
  1287|             if (ThreadPoolWorkQueue.s_prioritizationExperiment)
  1288|             {
  1289|                 foreach (object workItem in s_workQueue.lowPriorityWorkItems)
  1290|                 {
  1291|                     yield return workItem;
  1292|                 }
  1293|             }
  1294| #endif
  1295|             foreach (ThreadPoolWorkQueue.WorkStealingQueue wsq in ThreadPoolWorkQueue.WorkStealingQueueList.Queues)
  1296|             {
  1297|                 if (wsq != null && wsq.m_array != null)
  1298|                 {
  1299|                     object?[] items = wsq.m_array;
  1300|                     for (int i = 0; i < items.Length; i++)
  1301|                     {
  1302|                         object? item = items[i];
  1303|                         if (item != null)
  1304|                         {
  1305|                             yield return item;
  1306|                         }
  1307|                     }
  1308|                 }
  1309|             }
  1310|         }
  1311|         public static long PendingWorkItemCount
  1312|         {
  1313|             get
  1314|             {
  1315|                 ThreadPoolWorkQueue workQueue = s_workQueue;
  1316|                 return ThreadPoolWorkQueue.LocalCount + workQueue.GlobalCount;
  1317|             }
  1318|         }
  1319|     }
  1320| }


# ====================================================================
# FILE: src/mono/mono/component/debugger-engine.c
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-1442 ---
     1| /**
     2|  * \file
     3|  * Debugger Engine shared code.
     4|  *
     5|  * Author:
     6|  *   Zoltan Varga (vargaz@gmail.com)
     7|  *   Rodrigo Kumpera (kumpera@gmail.com)
     8|  *
     9|  * Licensed under the MIT license. See LICENSE file in the project root for full license information.
    10|  */
    11| #include <config.h>
    12| #include <mono/mini/mini-runtime.h>
    13| #if !defined (DISABLE_SDB) || defined(TARGET_WASM)
    14| #include <glib.h>
    15| #include <mono/mini/seq-points.h>
    16| #include <mono/mini/aot-runtime.h>
    17| #include "debugger-engine.h"
    18| #include "debugger-state-machine.h"
    19| #include <mono/metadata/debug-internals.h>
    20| static void mono_de_ss_start (SingleStepReq *ss_req, SingleStepArgs *ss_args);
    21| static gboolean mono_de_ss_update (SingleStepReq *req, MonoJitInfo *ji, SeqPoint *sp, void *tls, MonoContext *ctx, MonoMethod* method);
    22| static gpointer get_this_addr(DbgEngineStackFrame* the_frame);
    23| static MonoMethod* get_set_notification_method(MonoClass* async_builder_class);
    24| static DebuggerEngineCallbacks rt_callbacks;
    25| /*
    26|  * Logging support
    27|  */
    28| static int log_level;
    29| static FILE *log_file;
    30| static bool using_icordbg = FALSE;
    31| /*
    32|  * Locking
    33|  */
    34| #define dbg_lock() mono_coop_mutex_lock (&debug_mutex)
    35| #define dbg_unlock() mono_coop_mutex_unlock (&debug_mutex)
    36| static MonoCoopMutex debug_mutex;
    37| void
    38| mono_de_lock (void)
    39| {
    40| 	dbg_lock ();
    41| }
    42| void
    43| mono_de_unlock (void)
    44| {
    45| 	dbg_unlock ();
    46| }
    47| /*
    48|  * Domain support
    49|  */
    50| /* A hash table containing all active domains */
    51| /* Protected by the loader lock */
    52| static GHashTable *domains;
    53| static void
    54| domains_init (void)
    55| {
    56| 	domains = g_hash_table_new (mono_aligned_addr_hash, NULL);
    57| }
    58| static void
    59| domains_cleanup (void)
    60| {
    61| }
    62| /*
    63|  * mono_de_foreach_domain:
    64|  *
    65|  * Iterate over all domains under debugging. Caller must take the loader lock.
    66|  *
    67|  * FIXME can we move the locking to here? Callers in sdb must be properly audited.
    68|  */
    69| void
    70| mono_de_foreach_domain (GHFunc func, gpointer user_data)
    71| {
    72| 	g_hash_table_foreach (domains, func, user_data);
    73| }
    74| /*
    75|  * LOCKING: Takes the loader lock
    76|  */
    77| void
    78| mono_de_domain_add (MonoDomain *domain)
    79| {
    80| 	mono_loader_lock ();
    81| 	g_hash_table_insert (domains, domain, domain);
    82| 	mono_loader_unlock ();
    83| }
    84| /*
    85|  * BREAKPOINTS
    86|  */
    87| /* List of breakpoints */
    88| /* Protected by the loader lock */
    89| static GPtrArray *breakpoints;
    90| /* Maps breakpoint locations to the number of breakpoints at that location */
    91| static GHashTable *bp_locs;
    92| static void
    93| breakpoints_init (void)
    94| {
    95| 	breakpoints = g_ptr_array_new ();
    96| 	bp_locs = g_hash_table_new (NULL, NULL);
    97| }
    98| /*
    99|  * insert_breakpoint:
   100|  *
   101|  *   Insert the breakpoint described by BP into the method described by
   102|  * JI.
   103|  */
   104| static void
   105| insert_breakpoint (MonoSeqPointInfo *seq_points, MonoDomain *domain, MonoJitInfo *ji, MonoBreakpoint *bp, MonoError *error)
   106| {
   107| 	int count;
   108| 	BreakpointInstance *inst;
   109| 	SeqPointIterator it;
   110| 	gboolean it_has_sp = FALSE;
   111| 	if (error)
   112| 		error_init (error);
   113| 	mono_seq_point_iterator_init (&it, seq_points);
   114| 	while (mono_seq_point_iterator_next (&it)) {
   115| 		if (it.seq_point.il_offset == bp->il_offset) {
   116| 			it_has_sp = TRUE;
   117| 			break;
   118| 		}
   119| 	}
   120| 	if (!it_has_sp) {
   121| 		/*
   122| 		 * The set of IL offsets with seq points doesn't completely match the
   123| 		 * info returned by CMD_METHOD_GET_DEBUG_INFO (#407).
   124| 		 */
   125| 		mono_seq_point_iterator_init (&it, seq_points);
   126| 		while (mono_seq_point_iterator_next (&it)) {
   127| 			if (it.seq_point.il_offset != METHOD_ENTRY_IL_OFFSET &&
   128| 				it.seq_point.il_offset != METHOD_EXIT_IL_OFFSET &&
   129| 				it.seq_point.il_offset + 1 == bp->il_offset) {
   130| 				it_has_sp = TRUE;
   131| 				break;
   132| 			}
   133| 		}
   134| 	}
   135| 	if (!it_has_sp && using_icordbg)
   136| 	{
   137| 		mono_seq_point_iterator_init (&it, seq_points);
   138| 		while (mono_seq_point_iterator_next (&it)) {
   139| 			if (it.seq_point.il_offset != METHOD_ENTRY_IL_OFFSET &&
   140| 				it.seq_point.il_offset != METHOD_EXIT_IL_OFFSET &&
   141| 				it.seq_point.il_offset > bp->il_offset) {
   142| 				it_has_sp = TRUE;
   143| 				break;
   144| 			}
   145| 		}
   146| 	}
   147| 	if (!it_has_sp) {
   148| 		char *s = g_strdup_printf ("Unable to insert breakpoint at %s:%ld", mono_method_full_name (jinfo_get_method (ji), TRUE), bp->il_offset);
   149| 		mono_seq_point_iterator_init (&it, seq_points);
   150| 		while (mono_seq_point_iterator_next (&it))
   151| 			PRINT_DEBUG_MSG (1, "%d\n", it.seq_point.il_offset);
   152| 		if (error) {
   153| 			mono_error_set_error (error, MONO_ERROR_GENERIC, "%s", s);
   154| 			g_warning ("%s", s);
   155| 			g_free (s);
   156| 			return;
   157| 		} else {
   158| 			g_warning ("%s", s);
   159| 			g_free (s);
   160| 			return;
   161| 		}
   162| 	}
   163| 	inst = g_new0 (BreakpointInstance, 1);
   164| 	inst->il_offset = it.seq_point.il_offset;
   165| 	inst->native_offset = it.seq_point.native_offset;
   166| 	inst->ip = (guint8*)ji->code_start + it.seq_point.native_offset;
   167| 	inst->ji = ji;
   168| 	inst->domain = domain;
   169| 	mono_loader_lock ();
   170| 	g_ptr_array_add (bp->children, inst);
   171| 	mono_loader_unlock ();
   172| 	dbg_lock ();
   173| 	count = GPOINTER_TO_INT (g_hash_table_lookup (bp_locs, inst->ip));
   174| 	g_hash_table_insert (bp_locs, inst->ip, GINT_TO_POINTER (count + 1));
   175| 	dbg_unlock ();
   176| 	if (it.seq_point.native_offset == SEQ_POINT_NATIVE_OFFSET_DEAD_CODE) {
   177| 		PRINT_DEBUG_MSG (1, "[dbg] Attempting to insert seq point at dead IL offset %d, ignoring.\n", (int)bp->il_offset);
   178| 	} else if (count == 0) {
   179| 		if (ji->is_interp) {
   180| 			mini_get_interp_callbacks_api ()->set_breakpoint (ji, inst->ip);
   181| 		} else {
   182| #ifdef MONO_ARCH_SOFT_DEBUG_SUPPORTED
   183| 			mono_arch_set_breakpoint (ji, inst->ip);
   184| #else
   185| 			NOT_IMPLEMENTED;
   186| #endif
   187| 		}
   188| 	}
   189| 	PRINT_DEBUG_MSG (1, "[dbg] Inserted breakpoint at %s:[il=0x%x,native=0x%x] [%p](%d).\n", mono_method_full_name (jinfo_get_method (ji), TRUE), (int)it.seq_point.il_offset, (int)it.seq_point.native_offset, inst->ip, count);
   190| }
   191| static void
   192| remove_breakpoint (BreakpointInstance *inst)
   193| {
   194| 	int count;
   195| 	MonoJitInfo *ji = inst->ji;
   196| 	guint8 *ip = inst->ip;
   197| 	dbg_lock ();
   198| 	count = GPOINTER_TO_INT (g_hash_table_lookup (bp_locs, ip));
   199| 	g_hash_table_insert (bp_locs, ip, GINT_TO_POINTER (count - 1));
   200| 	dbg_unlock ();
   201| 	g_assert (count > 0);
   202| 	if (count == 1 && inst->native_offset != SEQ_POINT_NATIVE_OFFSET_DEAD_CODE) {
   203| 		if (ji->is_interp) {
   204| 			mini_get_interp_callbacks_api ()->clear_breakpoint (ji, ip);
   205| 		} else {
   206| #ifdef MONO_ARCH_SOFT_DEBUG_SUPPORTED
   207| 			mono_arch_clear_breakpoint (ji, ip);
   208| #else
   209| 			NOT_IMPLEMENTED;
   210| #endif
   211| 		}
   212| 		PRINT_DEBUG_MSG (1, "[dbg] Clear breakpoint at %s [%p].\n", mono_method_full_name (jinfo_get_method (ji), TRUE), ip);
   213| 	}
   214| }
   215| /*
   216|  * This doesn't take any locks.
   217|  */
   218| static gboolean
   219| bp_matches_method (MonoBreakpoint *bp, MonoMethod *method)
   220| {
   221| 	if (!bp->method)
   222| 		return TRUE;
   223| 	if (method == bp->method)
   224| 		return TRUE;
   225| 	if (method->is_inflated && ((MonoMethodInflated*)method)->declaring == bp->method)
   226| 		return TRUE;
   227| 	if (bp->method->is_inflated && method->is_inflated) {
   228| 		MonoMethodInflated *bpimethod = (MonoMethodInflated*)bp->method;
   229| 		MonoMethodInflated *imethod = (MonoMethodInflated*)method;
   230| 		/* Open generic methods should match closed generic methods of the same class */
   231| 		if (bpimethod->declaring == imethod->declaring && bpimethod->context.class_inst == imethod->context.class_inst && bpimethod->context.method_inst && bpimethod->context.method_inst->is_open) {
   232| 			for (guint i = 0; i < bpimethod->context.method_inst->type_argc; ++i) {
   233| 				MonoType *t1 = bpimethod->context.method_inst->type_argv [i];
   234| 				/* FIXME: Handle !mvar */
   235| 				if (t1->type != MONO_TYPE_MVAR)
   236| 					return FALSE;
   237| 			}
   238| 			return TRUE;
   239| 		}
   240| 	}
   241| 	return FALSE;
   242| }
   243| /*
   244|  * mono_de_add_pending_breakpoints:
   245|  *
   246|  *   Insert pending breakpoints into the newly JITted method METHOD.
   247|  */
   248| void
   249| mono_de_add_pending_breakpoints (MonoMethod *method, MonoJitInfo *ji)
   250| {
   251| 	MonoSeqPointInfo *seq_points;
   252| 	MonoDomain *domain;
   253| 	if (!breakpoints)
   254| 		return;
   255| 	domain = mono_domain_get ();
   256| 	mono_loader_lock ();
   257| 	for (guint i = 0; i < breakpoints->len; ++i) {
   258| 		MonoBreakpoint *bp = (MonoBreakpoint *)g_ptr_array_index (breakpoints, i);
   259| 		gboolean found = FALSE;
   260| 		if (!bp_matches_method (bp, method))
   261| 			continue;
   262| 		for (guint j = 0; j < bp->children->len; ++j) {
   263| 			BreakpointInstance *inst = (BreakpointInstance *)g_ptr_array_index (bp->children, j);
   264| 			if (inst->ji == ji)
   265| 				found = TRUE;
   266| 		}
   267| 		if (!found) {
   268| 			seq_points = (MonoSeqPointInfo *) ji->seq_points;
   269| 			if (!seq_points) {
   270| 				MonoMethod *jmethod = jinfo_get_method (ji);
   271| 				if (jmethod->is_inflated) {
   272| 					MonoJitInfo *seq_ji;
   273| 					MonoMethod *declaring = mono_method_get_declaring_generic_method (jmethod);
   274| 					mono_jit_search_all_backends_for_jit_info (declaring, &seq_ji);
   275| 					seq_points = (MonoSeqPointInfo *) seq_ji->seq_points;
   276| 				}
   277| 			}
   278| 			if (!seq_points)
   279| 				/* Could be AOT code, or above "search_all_backends" call could have failed */
   280| 				continue;
   281| 			insert_breakpoint (seq_points, domain, ji, bp, NULL);
   282| 		}
   283| 	}
   284| 	mono_loader_unlock ();
   285| }
   286| static void
   287| set_bp_in_method (MonoDomain *domain, MonoMethod *method, MonoSeqPointInfo *seq_points, MonoBreakpoint *bp, MonoError *error)
   288| {
   289| 	MonoJitInfo *ji;
   290| 	if (error)
   291| 		error_init (error);
   292| 	(void)mono_jit_search_all_backends_for_jit_info (method, &ji);
   293| 	g_assert (ji);
   294| 	insert_breakpoint (seq_points, domain, ji, bp, error);
   295| }
   296| typedef struct {
   297| 	MonoBreakpoint *bp;
   298| 	GPtrArray *methods;
   299| 	GPtrArray *method_domains;
   300| 	GPtrArray *method_seq_points;
   301| } CollectDomainData;
   302| static void
   303| collect_domain_bp (gpointer key, gpointer value, gpointer user_data)
   304| {
   305| 	GHashTableIter iter;
   306| 	MonoSeqPointInfo *seq_points;
   307| 	MonoDomain *domain = (MonoDomain*)key;
   308| 	CollectDomainData *ud = (CollectDomainData*)user_data;
   309| 	MonoMethod *m;
   310| 	MonoJitMemoryManager *jit_mm = get_default_jit_mm ();
   311| 	jit_mm_lock (jit_mm);
   312| 	g_hash_table_iter_init (&iter, jit_mm->seq_points);
   313| 	while (g_hash_table_iter_next (&iter, (void**)&m, (void**)&seq_points)) {
   314| 		if (bp_matches_method (ud->bp, m)) {
   315| 			/* Save the info locally to simplify the code inside the domain lock */
   316| 			g_ptr_array_add (ud->methods, m);
   317| 			g_ptr_array_add (ud->method_domains, domain);
   318| 			g_ptr_array_add (ud->method_seq_points, seq_points);
   319| 		}
   320| 	}
   321| 	jit_mm_unlock (jit_mm);
   322| }
   323| /*
   324|  * mono_de_set_breakpoint:
   325|  *
   326|  *   Set a breakpoint at IL_OFFSET in METHOD.
   327|  * METHOD can be NULL, in which case a breakpoint is placed in all methods.
   328|  * METHOD can also be a generic method definition, in which case a breakpoint
   329|  * is placed in all instances of the method.
   330|  * If ERROR is non-NULL, then it is set and NULL is returnd if some breakpoints couldn't be
   331|  * inserted.
   332|  */
   333| MonoBreakpoint*
   334| mono_de_set_breakpoint (MonoMethod *method, long il_offset, EventRequest *req, MonoError *error)
   335| {
   336| 	MonoBreakpoint *bp;
   337| 	MonoDomain *domain;
   338| 	MonoMethod *m;
   339| 	MonoSeqPointInfo *seq_points;
   340| 	GPtrArray *methods;
   341| 	GPtrArray *method_domains;
   342| 	GPtrArray *method_seq_points;
   343| 	if (error)
   344| 		error_init (error);
   345| 	bp = g_new0 (MonoBreakpoint, 1);
   346| 	bp->method = method;
   347| 	bp->il_offset = il_offset;
   348| 	bp->req = req;
   349| 	bp->children = g_ptr_array_new ();
   350| 	PRINT_DEBUG_MSG  (1, "[dbg] Setting %sbreakpoint at %s:0x%x.\n", (req->event_kind == EVENT_KIND_STEP) ? "single step " : "", method ? mono_method_full_name (method, TRUE) : "<all>", (int)il_offset);
   351| 	methods = g_ptr_array_new ();
   352| 	method_domains = g_ptr_array_new ();
   353| 	method_seq_points = g_ptr_array_new ();
   354| 	mono_loader_lock ();
   355| 	CollectDomainData user_data;
   356| 	memset (&user_data, 0, sizeof (user_data));
   357| 	user_data.bp = bp;
   358| 	user_data.methods = methods;
   359| 	user_data.method_domains = method_domains;
   360| 	user_data.method_seq_points = method_seq_points;
   361| 	mono_de_foreach_domain (collect_domain_bp, &user_data);
   362| 	for (guint i = 0; i < methods->len; ++i) {
   363| 		m = (MonoMethod *)g_ptr_array_index (methods, i);
   364| 		domain = (MonoDomain *)g_ptr_array_index (method_domains, i);
   365| 		seq_points = (MonoSeqPointInfo *)g_ptr_array_index (method_seq_points, i);
   366| 		set_bp_in_method (domain, m, seq_points, bp, error);
   367| 	}
   368| 	if (methods->len == 0) 
   369| 	{
   370| 		MonoJitInfo *ji;
   371| 		(void)mono_jit_search_all_backends_for_jit_info (method, &ji);
   372| 		if (ji && ji->seq_points)
   373| 			set_bp_in_method (mono_get_root_domain (), method, ji->seq_points, bp, error);
   374| 	}
   375| 	g_ptr_array_add (breakpoints, bp);
   376| 	mono_debugger_log_add_bp (bp, bp->method, bp->il_offset);
   377| 	mono_loader_unlock ();
   378| 	g_ptr_array_free (methods, TRUE);
   379| 	g_ptr_array_free (method_domains, TRUE);
   380| 	g_ptr_array_free (method_seq_points, TRUE);
   381| 	if (error && !is_ok (error)) {
   382| 		mono_de_clear_breakpoint (bp);
   383| 		return NULL;
   384| 	}
   385| 	return bp;
   386| }
   387| void
   388| mono_de_clear_breakpoint (MonoBreakpoint *bp)
   389| {
   390| 	for (guint i = 0; i < bp->children->len; ++i) {
   391| 		BreakpointInstance *inst = (BreakpointInstance *)g_ptr_array_index (bp->children, i);
   392| 		remove_breakpoint (inst);
   393| 		g_free (inst);
   394| 	}
   395| 	mono_loader_lock ();
   396| 	mono_debugger_log_remove_bp (bp, bp->method, bp->il_offset);
   397| 	g_ptr_array_remove (breakpoints, bp);
   398| 	mono_loader_unlock ();
   399| 	g_ptr_array_free (bp->children, TRUE);
   400| 	g_free (bp);
   401| }
   402| void
   403| mono_de_collect_breakpoints_by_sp (SeqPoint *sp, MonoJitInfo *ji, GPtrArray *ss_reqs, GPtrArray *bp_reqs)
   404| {
   405| 	for (guint i = 0; i < breakpoints->len; ++i) {
   406| 		MonoBreakpoint *bp = (MonoBreakpoint *)g_ptr_array_index (breakpoints, i);
   407| 		if (!bp->method)
   408| 			continue;
   409| 		for (guint j = 0; j < bp->children->len; ++j) {
   410| 			BreakpointInstance *inst = (BreakpointInstance *)g_ptr_array_index (bp->children, j);
   411| 			if (inst->ji == ji && inst->il_offset == sp->il_offset && inst->native_offset == sp->native_offset) {
   412| 				if (bp->req->event_kind == EVENT_KIND_STEP) {
   413| 					if (ss_reqs)
   414| 						g_ptr_array_add (ss_reqs, bp->req);
   415| 				} else {
   416| 					if (bp_reqs)
   417| 						g_ptr_array_add (bp_reqs, bp->req);
   418| 				}
   419| 			}
   420| 		}
   421| 		}
   422| }
   423| static void
   424| breakpoints_cleanup (void)
   425| {
   426| 	mono_loader_lock ();
   427| 	for (guint i = 0; i < breakpoints->len; ++i)
   428| 		g_free (g_ptr_array_index (breakpoints, i));
   429| 	g_ptr_array_free (breakpoints, TRUE);
   430| 	g_hash_table_destroy (bp_locs);
   431| 	breakpoints = NULL;
   432| 	bp_locs = NULL;
   433| 	mono_loader_unlock ();
   434| }
   435| /*
   436|  * mono_de_clear_breakpoints_for_domain:
   437|  *
   438|  *   Clear breakpoint instances which reference DOMAIN.
   439|  */
   440| void
   441| mono_de_clear_breakpoints_for_domain (MonoDomain *domain)
   442| {
   443| 	/* This could be called after shutdown */
   444| 	if (!breakpoints)
   445| 		return;
   446| 	mono_loader_lock ();
   447| 	for (guint i = 0; i < breakpoints->len; ++i) {
   448| 		MonoBreakpoint *bp = (MonoBreakpoint *)g_ptr_array_index (breakpoints, i);
   449| 		guint j = 0;
   450| 		while (j < bp->children->len) {
   451| 			BreakpointInstance *inst = (BreakpointInstance *)g_ptr_array_index (bp->children, j);
   452| 			if (inst->domain == domain) {
   453| 				remove_breakpoint (inst);
   454| 				g_free (inst);
   455| 				g_ptr_array_remove_index_fast (bp->children, j);
   456| 			} else {
   457| 				j ++;
   458| 			}
   459| 		}
   460| 	}
   461| 	mono_loader_unlock ();
   462| }
   463| /* Single stepping engine */
   464| /* Number of single stepping operations in progress */
   465| static int ss_count;
   466| /* The single step request instances */
   467| static GPtrArray *the_ss_reqs;
   468| static void
   469| ss_req_init (void)
   470| {
   471| 	the_ss_reqs = g_ptr_array_new ();
   472| }
   473| static void
   474| ss_req_cleanup (void)
   475| {
   476| 	dbg_lock ();
   477| 	g_ptr_array_free (the_ss_reqs, TRUE);
   478| 	the_ss_reqs = NULL;
   479| 	dbg_unlock ();
   480| }
   481| /*
   482|  * mono_de_start_single_stepping:
   483|  *
   484|  *   Turn on single stepping. Can be called multiple times, for example,
   485|  * by a single step event request + a suspend.
   486|  */
   487| void
   488| mono_de_start_single_stepping (void)
   489| {
   490| 		int val = mono_atomic_inc_i32 (&ss_count);
   491| 	if (val == 1) {
   492| 		#ifdef MONO_ARCH_SOFT_DEBUG_SUPPORTED
   493| 		mono_arch_start_single_stepping ();
   494| #endif
   495| 		mini_get_interp_callbacks_api ()->start_single_stepping ();
   496| 	}
   497| }
   498| void
   499| mono_de_stop_single_stepping (void)
   500| {
   501| 		int val = mono_atomic_dec_i32 (&ss_count);
   502| 	if (val == 0) {
   503| 		#ifdef MONO_ARCH_SOFT_DEBUG_SUPPORTED
   504| 		mono_arch_stop_single_stepping ();
   505| #endif
   506| 		mini_get_interp_callbacks_api ()->stop_single_stepping ();
   507| 	}
   508| }
   509| static MonoJitInfo*
   510| get_top_method_ji (gpointer ip, MonoDomain **domain, gpointer *out_ip)
   511| {
   512| 	MonoJitInfo *ji;
   513| 	if (out_ip)
   514| 		*out_ip = ip;
   515| 	if (domain)
   516| 		*domain = mono_get_root_domain ();
   517| 	ji = mini_jit_info_table_find (ip);
   518| 	if (!ji) {
   519| 		/* Could be an interpreter method */
   520| 		MonoLMF *lmf = mono_get_lmf ();
   521| 		MonoInterpFrameHandle *frame;
   522| 		g_assert (((gsize)lmf->previous_lmf) & 2);
   523| 		MonoLMFExt *ext = (MonoLMFExt*)lmf;
   524| 		g_assert (ext->kind == MONO_LMFEXT_INTERP_EXIT || ext->kind == MONO_LMFEXT_INTERP_EXIT_WITH_CTX);
   525| 		frame = (MonoInterpFrameHandle*)ext->interp_exit_data;
   526| 		ji = mini_get_interp_callbacks_api ()->frame_get_jit_info (frame);
   527| 		if (domain)
   528| 			*domain = mono_domain_get ();
   529| 		if (out_ip)
   530| 			*out_ip = mini_get_interp_callbacks_api ()->frame_get_ip (frame);
   531| 	}
   532| 	return ji;
   533| }
   534| static void
   535| no_seq_points_found (MonoMethod *method, int offset)
   536| {
   537| 	/*
   538| 	 * This can happen in full-aot mode with assemblies AOTed without the 'soft-debug' option to save space.
   539| 	 */
   540| 	PRINT_MSG ("Unable to find seq points for method '%s', offset 0x%x.\n", mono_method_full_name (method, TRUE), offset);
   541| }
   542| static const char*
   543| ss_depth_to_string (StepDepth depth)
   544| {
   545| 	switch (depth) {
   546| 	case STEP_DEPTH_OVER:
   547| 		return "over";
   548| 	case STEP_DEPTH_OUT:
   549| 		return "out";
   550| 	case STEP_DEPTH_INTO:
   551| 		return "into";
   552| 	default:
   553| 		g_assert_not_reached ();
   554| 		return NULL;
   555| 	}
   556| }
   557| /*
   558|  * ss_stop:
   559|  *
   560|  *   Stop the single stepping operation given by SS_REQ.
   561|  */
   562| static void
   563| ss_stop (SingleStepReq *ss_req)
   564| {
   565| 	if (ss_req->bps) {
   566| 		GSList *l;
   567| 		for (l = ss_req->bps; l; l = l->next) {
   568| 			mono_de_clear_breakpoint ((MonoBreakpoint *)l->data);
   569| 		}
   570| 		g_slist_free (ss_req->bps);
   571| 		ss_req->bps = NULL;
   572| 	}
   573| 	ss_req->async_id = 0;
   574| 	ss_req->async_stepout_method = NULL;
   575| 	if (ss_req->global) {
   576| 		mono_de_stop_single_stepping ();
   577| 		ss_req->global = FALSE;
   578| 	}
   579| }
   580| static void
   581| ss_destroy (SingleStepReq *req)
   582| {
   583| 	PRINT_DEBUG_MSG (1, "[dbg] ss_destroy.\n");
   584| 	ss_stop (req);
   585| 	g_free (req);
   586| }
   587| static SingleStepReq*
   588| ss_req_acquire (MonoInternalThread *thread)
   589| {
   590| 	SingleStepReq *req = NULL;
   591| 	dbg_lock ();
   592| 	for (guint i = 0; i < the_ss_reqs->len; ++i) {
   593| 		SingleStepReq *current_req = (SingleStepReq *)g_ptr_array_index (the_ss_reqs, i);
   594| 		if (current_req->thread == thread) {
   595| 			current_req->refcount ++;
   596| 			req = current_req;
   597| 		}
   598| 	}
   599| 	dbg_unlock ();
   600| 	return req;
   601| }
   602| static int
   603| ss_req_count (void)
   604| {
   605| 	return the_ss_reqs->len;
   606| }
   607| static void
   608| mono_de_ss_req_release (SingleStepReq *req)
   609| {
   610| 	gboolean free = FALSE;
   611| 	dbg_lock ();
   612| 	g_assert (req->refcount);
   613| 	req->refcount --;
   614| 	if (req->refcount == 0)
   615| 		free = TRUE;
   616| 	if (free) {
   617| 		g_ptr_array_remove (the_ss_reqs, req);
   618| 		ss_destroy (req);
   619| 	}
   620| 	dbg_unlock ();
   621| }
   622| void
   623| mono_de_cancel_ss (SingleStepReq *req)
   624| {
   625| 	if (the_ss_reqs) {
   626| 		mono_de_ss_req_release (req);
   627| 	}
   628| }
   629| void
   630| mono_de_cancel_all_ss (void)
   631| {
   632| 	for (guint i = 0; i < the_ss_reqs->len; ++i) {
   633| 		SingleStepReq *current_req = (SingleStepReq *)g_ptr_array_index (the_ss_reqs, i);
   634| 		mono_de_ss_req_release (current_req);
   635| 	}
   636| }
   637| void
   638| mono_de_process_single_step (void *tls, gboolean from_signal)
   639| {
   640| 	MonoJitInfo *ji;
   641| 	guint8 *ip;
   642| 	GPtrArray *reqs;
   643| 	int il_offset;
   644| 	MonoDomain *domain;
   645| 	MonoContext *ctx = rt_callbacks.tls_get_restore_state (tls);
   646| 	MonoMethod *method;
   647| 	SeqPoint sp;
   648| 	MonoSeqPointInfo *info;
   649| 	SingleStepReq *ss_req;
   650| 	/* Skip the instruction causing the single step */
   651| 	rt_callbacks.begin_single_step_processing (ctx, from_signal);
   652| 	if (rt_callbacks.try_process_suspend (tls, ctx, FALSE))
   653| 		return;
   654| 	/*
   655| 	 * This can run concurrently with a clear_event_request () call, so needs locking/reference counts.
   656| 	 */
   657| 	ss_req = ss_req_acquire (mono_thread_internal_current ());
   658| 	if (!ss_req)
   659| 		return;
   660| 	ip = (guint8 *)MONO_CONTEXT_GET_IP (ctx);
   661| 	ji = get_top_method_ji (ip, &domain, (gpointer*)&ip);
   662| 	g_assert (ji && !ji->is_trampoline);
   663| 	if (log_level > 0) {
   664| 		PRINT_DEBUG_MSG (1, "[%p] Single step event (depth=%s) at %s (%p)[0x%x], sp %p, last sp %p\n", (gpointer) (gsize) mono_native_thread_id_get (), ss_depth_to_string (ss_req->depth), mono_method_full_name (jinfo_get_method (ji), TRUE), MONO_CONTEXT_GET_IP (ctx), (int)((guint8*)MONO_CONTEXT_GET_IP (ctx) - (guint8*)ji->code_start), MONO_CONTEXT_GET_SP (ctx), ss_req->last_sp);
   665| 	}
   666| 	method = jinfo_get_method (ji);
   667| 	g_assert (method);
   668| 	if (method->wrapper_type && method->wrapper_type != MONO_WRAPPER_DYNAMIC_METHOD)
   669| 		goto exit;
   670| 	/*
   671| 	 * FIXME:
   672| 	 * Stopping in memset makes half-initialized vtypes visible.
   673| 	 * Stopping in memcpy makes half-copied vtypes visible.
   674| 	 */
   675| 	if (method->klass == mono_get_string_class () && (!strcmp (method->name, "memset") || strstr (method->name, "memcpy")))
   676| 		goto exit;
   677| 	/*
   678| 	 * This could be in mono_de_ss_update method, but mono_find_next_seq_point_for_native_offset is pretty expensive method,
   679| 	 * hence we prefer this check here.
   680| 	 */
   681| 	if (ss_req->user_assemblies) {
   682| 		gboolean found = FALSE;
   683| 		for (int k = 0; ss_req->user_assemblies[k]; k++)
   684| 			if (ss_req->user_assemblies[k] == m_class_get_image (method->klass)->assembly) {
   685| 				found = TRUE;
   686| 				break;
   687| 			}
   688| 		if (!found)
   689| 			goto exit;
   690| 	}
   691| 	/*
   692| 	 * The ip points to the instruction causing the single step event, which is before
   693| 	 * the offset recorded in the seq point map, so find the next seq point after ip.
   694| 	 */
   695| 	if (!mono_find_next_seq_point_for_native_offset (method, GPTRDIFF_TO_INT32 ((guint8*)ip - (guint8*)ji->code_start), &info, &sp)) {
   696| 		g_assert_not_reached ();
   697| 		goto exit;
   698| 	}
   699| 	il_offset = sp.il_offset;
   700| 	if (!mono_de_ss_update (ss_req, ji, &sp, tls, ctx, method))
   701| 		goto exit;
   702| 	/* Start single stepping again from the current sequence point */
   703| 	SingleStepArgs args;
   704| 	memset (&args, 0, sizeof (args));
   705| 	args.method = method;
   706| 	args.ctx = ctx;
   707| 	args.tls = tls;
   708| 	args.step_to_catch = FALSE;
   709| 	args.sp = sp;
   710| 	args.info = info;
   711| 	args.frames = NULL;
   712| 	args.nframes = 0;
   713| 	mono_de_ss_start (ss_req, &args);
   714| 	if ((ss_req->filter & STEP_FILTER_STATIC_CTOR) &&
   715| 		(method->flags & METHOD_ATTRIBUTE_SPECIAL_NAME) &&
   716| 		!strcmp (method->name, ".cctor"))
   717| 		goto exit;
   718| 	reqs = g_ptr_array_new ();
   719| 	mono_loader_lock ();
   720| 	g_ptr_array_add (reqs, ss_req->req);
   721| 	void *bp_events;
   722| 	bp_events = mono_dbg_create_breakpoint_events (reqs, NULL, ji, EVENT_KIND_BREAKPOINT);
   723| 	g_ptr_array_free (reqs, TRUE);
   724| 	mono_loader_unlock ();
   725| 	mono_dbg_process_breakpoint_events (bp_events, method, ctx, il_offset);
   726|  exit:
   727| 	mono_de_ss_req_release (ss_req);
   728| }
   729| /*
   730|  * mono_de_ss_update:
   731|  *
   732|  * Return FALSE if single stepping needs to continue.
   733|  */
   734| static gboolean
   735| mono_de_ss_update (SingleStepReq *req, MonoJitInfo *ji, SeqPoint *sp, void *tls, MonoContext *ctx, MonoMethod* method)
   736| {
   737| 	MonoDebugMethodInfo *minfo;
   738| 	MonoDebugSourceLocation *loc = NULL;
   739| 	gboolean hit = TRUE;
   740| 	if ((req->filter & STEP_FILTER_STATIC_CTOR)) {
   741| 		DbgEngineStackFrame **frames;
   742| 		int nframes;
   743| 		rt_callbacks.ss_calculate_framecount (tls, ctx, TRUE, &frames, &nframes);
   744| 		gboolean ret = FALSE;
   745| 		gboolean method_in_stack = FALSE;
   746| 		for (int i = 0; i < nframes; i++) {
   747| 			MonoMethod *external_method = frames [i]->method;
   748| 			if (method == external_method)
   749| 				method_in_stack = TRUE;
   750| 			if (!ret) {
   751| 				ret = (external_method->flags & METHOD_ATTRIBUTE_SPECIAL_NAME);
   752| 				ret = ret && !strcmp (external_method->name, ".cctor");
   753| 				ret = ret && (external_method != req->start_method);
   754| 			}
   755| 		}
   756| 		if (!method_in_stack) {
   757| 			PRINT_ERROR_MSG ("[%p] The instruction pointer of the currently executing method(%s) is not on the recorded stack. This is likely due to a runtime bug. The %d frames are as follow: \n", (gpointer)(gsize)mono_native_thread_id_get (), mono_method_full_name (method, TRUE), nframes);
   758| 			/*PRINT_DEBUG_MSG (1, "[%p] The instruction pointer of the currently executing method(%s) is not on the recorded stack. This is likely due to a runtime bug. The %d frames are as follow: \n", (gpointer)(gsize)mono_native_thread_id_get (), mono_method_full_name (method, TRUE), tls->frame_count);*/
   759| 			for (int i=0; i < nframes; i++)
   760| 				PRINT_ERROR_MSG ("\t [%p] Frame (%d / %d): %s\n", (gpointer)(gsize)mono_native_thread_id_get (), i, nframes, mono_method_full_name (frames [i]->method, TRUE));
   761| 		}
   762| 		rt_callbacks.ss_discard_frame_context (tls);
   763| 		if (ret)
   764| 			return FALSE;
   765| 	}
   766| 	if (req->async_stepout_method == method) {
   767| 		PRINT_DEBUG_MSG (1, "[%p] Breakpoint hit during async step-out at %s hit, continuing stepping out.\n", (gpointer)(gsize)mono_native_thread_id_get (), method->name);
   768| 		return FALSE;
   769| 	}
   770| 	if (req->depth == STEP_DEPTH_OVER && (sp->flags & MONO_SEQ_POINT_FLAG_NONEMPTY_STACK) && !(sp->flags & MONO_SEQ_POINT_FLAG_NESTED_CALL) && req->start_method == method) {
   771| 		/*
   772| 		 * These seq points are inserted by the JIT after calls, step over needs to skip them.
   773| 		 */
   774| 		PRINT_DEBUG_MSG (1, "[%p] Seq point at nonempty stack %x while stepping over, continuing single stepping.\n", (gpointer) (gsize) mono_native_thread_id_get (), sp->il_offset);
   775| 		return FALSE;
   776| 	}
   777| 	if ((req->depth == STEP_DEPTH_OVER || req->depth == STEP_DEPTH_OUT) && hit && !req->async_stepout_method) {
   778| 		gboolean is_step_out = req->depth == STEP_DEPTH_OUT;
   779| 		int nframes;
   780| 		rt_callbacks.ss_calculate_framecount (tls, ctx, FALSE, NULL, &nframes);
   781| 		int target_frames = req->nframes + (is_step_out ? -1 : 0);
   782| 		if (req->nframes > 0 && nframes > 0 && nframes > target_frames) {
   783| 			/* Hit the breakpoint in a recursive call, don't halt */
   784| 			PRINT_DEBUG_MSG (1, "[%p] Breakpoint at lower frame while stepping %s, continuing single stepping.\n", (gpointer) (gsize) mono_native_thread_id_get (), is_step_out ? "out" : "over");
   785| 			return FALSE;
   786| 		}
   787| 	}
   788| 	if (req->depth == STEP_DEPTH_INTO && req->size == STEP_SIZE_MIN && (sp->flags & MONO_SEQ_POINT_FLAG_NONEMPTY_STACK) && req->start_method) {
   789| 		int nframes;
   790| 		rt_callbacks.ss_calculate_framecount (tls, ctx, FALSE, NULL, &nframes);
   791| 		if (req->start_method == method && req->nframes && nframes == req->nframes) { //Check also frame count(could be recursion)
   792| 			PRINT_DEBUG_MSG (1, "[%p] Seq point at nonempty stack %x while stepping in, continuing single stepping.\n", (gpointer) (gsize) mono_native_thread_id_get (), sp->il_offset);
   793| 			return FALSE;
   794| 		}
   795| 	}
   796| 	MonoDebugMethodAsyncInfo* async_method = mono_debug_lookup_method_async_debug_info (method);
   797| 	if (async_method) {
   798| 		for (int i = 0; i < async_method->num_awaits; i++) {
   799| 			if (async_method->yield_offsets[i] == sp->il_offset || async_method->resume_offsets[i] == sp->il_offset) {
   800| 				mono_debug_free_method_async_debug_info (async_method);
   801| 				return FALSE;
   802| 			}
   803| 		}
   804| 		mono_debug_free_method_async_debug_info (async_method);
   805| 	}
   806| 	if (req->size != STEP_SIZE_LINE_COLUMN)
   807| 		return TRUE;
   808| 	/* Have to check whenever a different source line was reached */
   809| 	minfo = mono_debug_lookup_method (method);
   810| 	if (minfo)
   811| 		loc = mono_debug_method_lookup_location (minfo, sp->il_offset);
   812| 	if (!loc) { //we should not continue single stepping because the client side can have symbols loaded dynamically
   813| 		PRINT_DEBUG_MSG (1, "[%p] No line number info for il offset %x, don't know if it's in the same line single stepping.\n", (gpointer) (gsize) mono_native_thread_id_get (), sp->il_offset);
   814| 		req->last_method = method;
   815| 		req->last_line = -1;
   816| 		req->last_column = -1;
   817| 		return hit;
   818| 	} else if (loc && method == req->last_method && loc->row == req->last_line && loc->column == req->last_column) {
   819| 		int nframes;
   820| 		rt_callbacks.ss_calculate_framecount (tls, ctx, FALSE, NULL, &nframes);
   821| 		if (nframes == req->nframes) { // If the frame has changed we're clearly not on the same source line.
   822| 			PRINT_DEBUG_MSG (1, "[%p] Same source line (%d), continuing single stepping.\n", (gpointer) (gsize) mono_native_thread_id_get (), loc->row);
   823| 			hit = FALSE;
   824| 		}
   825| 	}
   826| 	if (loc) {
   827| 		req->last_method = method;
   828| 		req->last_line = loc->row;
   829| 		req->last_column = loc->column;
   830| 		mono_debug_free_source_location (loc);
   831| 	}
   832| 	return hit;
   833| }
   834| void
   835| mono_de_process_breakpoint (void *void_tls, gboolean from_signal)
   836| {
   837| 	DebuggerTlsData *tls = (DebuggerTlsData*)void_tls;
   838| 	MonoJitInfo *ji;
   839| 	guint8 *ip;
   840| 	guint32 native_offset;
   841| 	GPtrArray *bp_reqs, *ss_reqs_orig, *ss_reqs;
   842| 	EventKind kind = EVENT_KIND_BREAKPOINT;
   843| 	MonoContext *ctx = rt_callbacks.tls_get_restore_state (tls);
   844| 	MonoMethod *method;
   845| 	MonoSeqPointInfo *info;
   846| 	SeqPoint sp;
   847| 	gboolean found_sp;
   848| 	if (rt_callbacks.try_process_suspend (tls, ctx, TRUE))
   849| 		return;
   850| 	ip = (guint8 *)MONO_CONTEXT_GET_IP (ctx);
   851| 	ji = get_top_method_ji (ip, NULL, (gpointer*)&ip);
   852| 	g_assert (ji && !ji->is_trampoline);
   853| 	method = jinfo_get_method (ji);
   854| 	/* Compute the native offset of the breakpoint from the ip */
   855| 	native_offset = GPTRDIFF_TO_UINT32 (ip - (guint8*)ji->code_start);
   856| 	if (!rt_callbacks.begin_breakpoint_processing (tls, ctx, ji, from_signal))
   857| 		return;
   858| 	if (method->wrapper_type)
   859| 		return;
   860| 	bp_reqs = g_ptr_array_new ();
   861| 	ss_reqs = g_ptr_array_new ();
   862| 	ss_reqs_orig = g_ptr_array_new ();
   863| 	mono_loader_lock ();
   864| 	/*
   865| 	 * The ip points to the instruction causing the breakpoint event, which is after
   866| 	 * the offset recorded in the seq point map, so find the prev seq point before ip.
   867| 	 */
   868| 	found_sp = mono_find_prev_seq_point_for_native_offset (method, native_offset, &info, &sp);
   869| 	if (!found_sp)
   870| 		no_seq_points_found (method, native_offset);
   871| 	g_assert (found_sp);
   872| 	PRINT_DEBUG_MSG (1, "[%p] Breakpoint hit, method=%s, ip=%p, [il=0x%x,native=0x%x].\n", (gpointer) (gsize) mono_native_thread_id_get (), method->name, ip, sp.il_offset, native_offset);
   873| 	mono_debugger_log_bp_hit (tls, method, sp.il_offset);
   874| 	mono_de_collect_breakpoints_by_sp (&sp, ji, ss_reqs_orig, bp_reqs);
   875| 	if (bp_reqs->len == 0 && ss_reqs_orig->len == 0) {
   876| 		/* Maybe a method entry/exit event */
   877| 		if (sp.il_offset == METHOD_ENTRY_IL_OFFSET)
   878| 			kind = EVENT_KIND_METHOD_ENTRY;
   879| 		else if (sp.il_offset == METHOD_EXIT_IL_OFFSET)
   880| 			kind = EVENT_KIND_METHOD_EXIT;
   881| 	}
   882| 	/* Process single step requests */
   883| 	for (guint i = 0; i < ss_reqs_orig->len; ++i) {
   884| 		EventRequest *req = (EventRequest *)g_ptr_array_index (ss_reqs_orig, i);
   885| 		SingleStepReq *ss_req = (SingleStepReq *)req->info;
   886| 		gboolean hit;
   887| 		if ((ss_req->async_stepout_method != method) && (ss_req->async_id || mono_thread_internal_current () != ss_req->thread)) {
   888| 			DbgEngineStackFrame **frames;
   889| 			int nframes;
   890| 			if (ss_req->async_id == 0)
   891| 				continue;
   892| 			rt_callbacks.ss_discard_frame_context (tls);
   893| 			rt_callbacks.ss_calculate_framecount (tls, ctx, FALSE, &frames, &nframes);
   894| 			if (nframes == 0 || !rt_callbacks.ensure_jit (frames [0]))
   895| 				continue;
   896| 			MonoDebugMethodAsyncInfo* asyncMethod = mono_debug_lookup_method_async_debug_info (method);
   897| 			if (!asyncMethod)
   898| 				continue;
   899| 			else
   900| 				mono_debug_free_method_async_debug_info (asyncMethod);
   901| 			if (ss_req->async_id != mono_de_frame_async_id (frames [0]))
   902| 				continue;
   903| 		}
   904| 		if (ss_req->async_stepout_method || ss_req->async_id) {
   905| 			int nframes;
   906| 			rt_callbacks.ss_discard_frame_context (tls);
   907| 			rt_callbacks.ss_calculate_framecount (tls, ctx, FALSE, NULL, &nframes);
   908| 			ss_req->thread = mono_thread_internal_current ();
   909| 			ss_req->nframes = nframes;
   910| 		}
   911| 		hit = mono_de_ss_update (ss_req, ji, &sp, tls, ctx, method);
   912| 		if (hit)
   913| 			g_ptr_array_add (ss_reqs, req);
   914| 		SingleStepArgs args;
   915| 		memset (&args, 0, sizeof (args));
   916| 		args.method = method;
   917| 		args.ctx = ctx;
   918| 		args.tls = tls;
   919| 		args.step_to_catch = FALSE;
   920| 		args.sp = sp;
   921| 		args.info = info;
   922| 		args.frames = NULL;
   923| 		args.nframes = 0;
   924| 		mono_de_ss_start (ss_req, &args);
   925| 	}
   926| 	void *bp_events = mono_dbg_create_breakpoint_events (ss_reqs, bp_reqs, ji, kind);
   927| 	mono_loader_unlock ();
   928| 	g_ptr_array_free (bp_reqs, TRUE);
   929| 	g_ptr_array_free (ss_reqs, TRUE);
   930| 	mono_dbg_process_breakpoint_events (bp_events, method, ctx, sp.il_offset);
   931| }
   932| /*
   933|  * ss_bp_is_unique:
   934|  *
   935|  * Reject breakpoint if it is a duplicate of one already in list or hash table.
   936|  */
   937| static gboolean
   938| ss_bp_is_unique (GSList *bps, GHashTable *ss_req_bp_cache, MonoMethod *method, guint32 il_offset)
   939| {
   940| 	if (ss_req_bp_cache) {
   941| 		MonoBreakpoint dummy = {method, (long)il_offset, NULL, NULL};
   942| 		return !g_hash_table_lookup (ss_req_bp_cache, &dummy);
   943| 	}
   944| 	for (GSList *l = bps; l; l = l->next) {
   945| 		MonoBreakpoint *bp = (MonoBreakpoint *)l->data;
   946| 		if (bp->method == method && bp->il_offset == il_offset)
   947| 			return FALSE;
   948| 	}
   949| 	return TRUE;
   950| }
   951| /*
   952|  * ss_bp_eq:
   953|  *
   954|  * GHashTable equality for a MonoBreakpoint (only care about method and il_offset fields)
   955|  */
   956| static gint
   957| ss_bp_eq (gconstpointer ka, gconstpointer kb)
   958| {
   959| 	const MonoBreakpoint *s1 = (const MonoBreakpoint *)ka;
   960| 	const MonoBreakpoint *s2 = (const MonoBreakpoint *)kb;
   961| 	return (s1->method == s2->method && s1->il_offset == s2->il_offset) ? 1 : 0;
   962| }
   963| /*
   964|  * ss_bp_eq:
   965|  *
   966|  * GHashTable hash for a MonoBreakpoint (only care about method and il_offset fields)
   967|  */
   968| static guint
   969| ss_bp_hash (gconstpointer data)
   970| {
   971| 	const MonoBreakpoint *s = (const MonoBreakpoint *)data;
   972| 	guint hash = (guint) (uintptr_t) s->method;
   973| 	hash ^= ((guint)s->il_offset) << 16; // Assume low bits are more interesting
   974| 	hash ^= ((guint)s->il_offset) >> 16;
   975| 	return hash;
   976| }
   977| #define MAX_LINEAR_SCAN_BPS 7
   978| /*
   979|  * ss_bp_add_one:
   980|  *
   981|  * Create a new breakpoint and add it to a step request.
   982|  * Will adjust the bp count and cache used by mono_de_ss_start.
   983|  */
   984| static void
   985| ss_bp_add_one (SingleStepReq *ss_req, int *ss_req_bp_count, GHashTable **ss_req_bp_cache,
   986| 	          MonoMethod *method, guint32 il_offset)
   987| {
   988| 	if (!*ss_req_bp_cache && *ss_req_bp_count > MAX_LINEAR_SCAN_BPS) {
   989| 		*ss_req_bp_cache = g_hash_table_new (ss_bp_hash, ss_bp_eq);
   990| 		for (GSList *l = ss_req->bps; l; l = l->next)
   991| 			g_hash_table_insert (*ss_req_bp_cache, l->data, l->data);
   992| 	}
   993| 	if (ss_bp_is_unique (ss_req->bps, *ss_req_bp_cache, method, il_offset)) {
   994| 		MonoBreakpoint *bp = mono_de_set_breakpoint (method, il_offset, ss_req->req, NULL);
   995| 		ss_req->bps = g_slist_append (ss_req->bps, bp);
   996| 		if (*ss_req_bp_cache)
   997| 			g_hash_table_insert (*ss_req_bp_cache, bp, bp);
   998| 		(*ss_req_bp_count)++;
   999| 	} else {
  1000| 		PRINT_DEBUG_MSG (1, "[dbg] Candidate breakpoint at %s:[il=0x%x] is a duplicate for this step request, will not add.\n", mono_method_full_name (method, TRUE), (int)il_offset);
  1001| 	}
  1002| }
  1003| static gboolean
  1004| is_last_non_empty (SeqPoint* sp, MonoSeqPointInfo *info)
  1005| {
  1006| 	if (!sp->next_len)
  1007| 		return TRUE;
  1008| 	SeqPoint* next = g_new (SeqPoint, sp->next_len);
  1009| 	mono_seq_point_init_next (info, *sp, next);
  1010| 	for (int i = 0; i < sp->next_len; i++) {
  1011| 		if (next [i].flags & MONO_SEQ_POINT_FLAG_NONEMPTY_STACK && !(next [i].flags & MONO_SEQ_POINT_FLAG_NESTED_CALL)) {
  1012| 			if (!is_last_non_empty (&next [i], info)) {
  1013| 				g_free (next);
  1014| 				return FALSE;
  1015| 			}
  1016| 		} else {
  1017| 			g_free (next);
  1018| 			return FALSE;
  1019| 		}
  1020| 	}
  1021| 	g_free (next);
  1022| 	return TRUE;
  1023| }
  1024| /*
  1025|  * mono_de_ss_start:
  1026|  *
  1027|  *   Start the single stepping operation given by SS_REQ from the sequence point SP.
  1028|  * If CTX is not set, then this can target any thread. If CTX is set, then TLS should
  1029|  * belong to the same thread as CTX.
  1030|  * If FRAMES is not-null, use that instead of tls->frames for placing breakpoints etc.
  1031|  */
  1032| static void
  1033| mono_de_ss_start (SingleStepReq *ss_req, SingleStepArgs *ss_args)
  1034| {
  1035| 	int frame_index;
  1036| 	SeqPoint *next_sp, *parent_sp = NULL;
  1037| 	SeqPoint local_sp, local_parent_sp;
  1038| 	gboolean found_sp;
  1039| 	MonoSeqPointInfo *parent_info = NULL;
  1040| 	MonoMethod *parent_sp_method = NULL;
  1041| 	gboolean enable_global = FALSE;
  1042| 	int ss_req_bp_count = g_slist_length (ss_req->bps);
  1043| 	GHashTable *ss_req_bp_cache = NULL;
  1044| 	/* Stop the previous operation */
  1045| 	ss_stop (ss_req);
  1046| 	gboolean locked = FALSE;
  1047| 	void *tls = ss_args->tls;
  1048| 	MonoMethod *method = ss_args->method;
  1049| 	DbgEngineStackFrame **frames = ss_args->frames;
  1050| 	int nframes = ss_args->nframes;
  1051| 	SeqPoint *sp = &ss_args->sp;
  1052| 	/* this can happen on a single step in a exception on android (Mono_UnhandledException_internal) and on IOS */
  1053| 	if (!method)
  1054| 		return;
  1055| 	/*
  1056| 	 * Implement single stepping using breakpoints if possible.
  1057| 	 */
  1058| 	if (ss_args->step_to_catch) {
  1059| 		ss_bp_add_one (ss_req, &ss_req_bp_count, &ss_req_bp_cache, method, sp->il_offset);
  1060| 	} else {
  1061| 		frame_index = 1;
  1062| #ifndef TARGET_WASM
  1063| 		if (ss_args->ctx && !frames) {
  1064| #else
  1065| 		if (!frames) {
  1066| #endif
  1067| 			mono_loader_lock ();
  1068| 			locked = TRUE;
  1069| 			/* Need parent frames */
  1070| 			rt_callbacks.ss_calculate_framecount (tls, ss_args->ctx, FALSE, &frames, &nframes);
  1071| 		}
  1072| 		MonoDebugMethodAsyncInfo* asyncMethod = mono_debug_lookup_method_async_debug_info (method);
  1073| 		/* Need to stop in catch clauses as well */
  1074| 		for (int i = ss_req->depth == STEP_DEPTH_OUT ? 1 : 0; i < nframes; ++i) {
  1075| 			DbgEngineStackFrame *frame = frames [i];
  1076| 			if (frame->ji) {
  1077| 				MonoJitInfo *jinfo = frame->ji;
  1078| 				for (guint32 j = 0; j < jinfo->num_clauses; ++j) {
  1079| 					if (asyncMethod && asyncMethod->num_awaits && i == 0 && j + 1 == jinfo->num_clauses)
  1080| 						break;
  1081| 					MonoJitExceptionInfo *ei = &jinfo->clauses [j];
  1082| 					if (mono_find_next_seq_point_for_native_offset (frame->method, GPTRDIFF_TO_INT32 ((char*)ei->handler_start - (char*)jinfo->code_start), NULL, &local_sp))
  1083| 						ss_bp_add_one (ss_req, &ss_req_bp_count, &ss_req_bp_cache, frame->method, local_sp.il_offset);
  1084| 				}
  1085| 			}
  1086| 		}
  1087| 		if (asyncMethod && asyncMethod->num_awaits && nframes && rt_callbacks.ensure_jit (frames [0])) {
  1088| 			for (int i = 0; i < asyncMethod->num_awaits; i++) {
  1089| 				if (sp->il_offset == asyncMethod->yield_offsets [i]) {
  1090| 					ss_req->async_id = mono_de_frame_async_id (frames [0]);
  1091| 					ss_bp_add_one (ss_req, &ss_req_bp_count, &ss_req_bp_cache, method, asyncMethod->resume_offsets [i]);
  1092| 					g_hash_table_destroy (ss_req_bp_cache);
  1093| 					mono_debug_free_method_async_debug_info (asyncMethod);
  1094| 					if (locked)
  1095| 						mono_loader_unlock ();
  1096| 					goto cleanup;
  1097| 				}
  1098| 			}
  1099| 			if (is_last_non_empty (sp, ss_args->info)) {
  1100| 				ss_req->depth = STEP_DEPTH_OUT;//setting depth to step-out is important, don't inline IF, because code later depends on this
  1101| 			}
  1102| 			if (ss_req->depth == STEP_DEPTH_OUT) {
  1103| 				if (set_set_notification_for_wait_completion_flag (frames [0])) {
  1104| 					ss_req->async_id = mono_de_frame_async_id (frames [0]);
  1105| 					ss_req->async_stepout_method = get_notify_debugger_of_wait_completion_method ();
  1106| 					ss_bp_add_one (ss_req, &ss_req_bp_count, &ss_req_bp_cache, ss_req->async_stepout_method, 0);
  1107| 					g_hash_table_destroy (ss_req_bp_cache);
  1108| 					mono_debug_free_method_async_debug_info (asyncMethod);
  1109| 					if (locked)
  1110| 						mono_loader_unlock ();
  1111| 					goto cleanup;
  1112| 				}
  1113| 			}
  1114| 		}
  1115| 		if (asyncMethod)
  1116| 			mono_debug_free_method_async_debug_info (asyncMethod);
  1117| 		/*
  1118| 		* Find the first sequence point in the current or in a previous frame which
  1119| 		* is not the last in its method.
  1120| 		*/
  1121| 		if (ss_req->depth == STEP_DEPTH_OUT) {
  1122| 			/* Ignore seq points in current method */
  1123| 			while (frame_index < nframes) {
  1124| 				DbgEngineStackFrame *frame = frames [frame_index];
  1125| 				method = frame->method;
  1126| 				found_sp = mono_find_prev_seq_point_for_native_offset (frame->method, frame->native_offset, &ss_args->info, &local_sp);
  1127| 				sp = (found_sp)? &local_sp : NULL;
  1128| 				frame_index ++;
  1129| 				if (sp && sp->next_len != 0)
  1130| 					break;
  1131| 			}
  1132| 		} else {
  1133| 			if (sp && sp->next_len == 0) {
  1134| 				sp = NULL;
  1135| 				while (frame_index < nframes) {
  1136| 					DbgEngineStackFrame *frame = frames [frame_index];
  1137| 					method = frame->method;
  1138| 					found_sp = mono_find_prev_seq_point_for_native_offset (frame->method, frame->native_offset, &ss_args->info, &local_sp);
  1139| 					sp = (found_sp)? &local_sp : NULL;
  1140| 					if (sp && sp->next_len != 0)
  1141| 						break;
  1142| 					sp = NULL;
  1143| 					frame_index ++;
  1144| 				}
  1145| 			} else {
  1146| 				/* Have to put a breakpoint into a parent frame since the seq points might not cover all control flow out of the method */
  1147| 				while (frame_index < nframes) {
  1148| 					DbgEngineStackFrame *frame = frames [frame_index];
  1149| 					parent_sp_method = frame->method;
  1150| 					found_sp = mono_find_prev_seq_point_for_native_offset (frame->method, frame->native_offset, &parent_info, &local_parent_sp);
  1151| 					parent_sp = found_sp ? &local_parent_sp : NULL;
  1152| 					if (found_sp && parent_sp->next_len != 0)
  1153| 						break;
  1154| 					parent_sp = NULL;
  1155| 					frame_index ++;
  1156| 				}
  1157| 			}
  1158| 		}
  1159| 		if (sp && sp->next_len > 0) {
  1160| 			SeqPoint* next = g_new(SeqPoint, sp->next_len);
  1161| 			mono_seq_point_init_next (ss_args->info, *sp, next);
  1162| 			for (int i = 0; i < sp->next_len; i++) {
  1163| 				next_sp = &next[i];
  1164| 				ss_bp_add_one (ss_req, &ss_req_bp_count, &ss_req_bp_cache, method, next_sp->il_offset);
  1165| 			}
  1166| 			g_free (next);
  1167| 		}
  1168| 		if (parent_sp) {
  1169| 			SeqPoint* next = g_new(SeqPoint, parent_sp->next_len);
  1170| 			mono_seq_point_init_next (parent_info, *parent_sp, next);
  1171| 			for (int i = 0; i < parent_sp->next_len; i++) {
  1172| 				next_sp = &next[i];
  1173| 				ss_bp_add_one (ss_req, &ss_req_bp_count, &ss_req_bp_cache, parent_sp_method, next_sp->il_offset);
  1174| 			}
  1175| 			g_free (next);
  1176| 		}
  1177| 		if (ss_req->nframes == 0)
  1178| 			ss_req->nframes = nframes;
  1179| 		if ((ss_req->depth == STEP_DEPTH_OVER) && (!sp && !parent_sp)) {
  1180| 			PRINT_DEBUG_MSG (1, "[dbg] No parent frame for step over, transition to step into.\n");
  1181| 			/*
  1182| 			 * This is needed since if we leave managed code, and later return to it, step over
  1183| 			 * is not going to stop.
  1184| 			 * This approach is a bit ugly, since we change the step depth, but it only affects
  1185| 			 * clients who reuse the same step request, and only in this special case.
  1186| 			 */
  1187| 			ss_req->depth = STEP_DEPTH_INTO;
  1188| 		}
  1189| 		if (ss_req->depth == STEP_DEPTH_INTO) {
  1190| 			/* Enable global stepping so we stop at method entry too */
  1191| 			enable_global = TRUE;
  1192| 		}
  1193| 		/*
  1194| 		 * The ctx/frame info computed above will become invalid when we continue.
  1195| 		 */
  1196| 		rt_callbacks.ss_discard_frame_context (tls);
  1197| 	}
  1198| 	if (enable_global) {
  1199| 		PRINT_DEBUG_MSG (1, "[dbg] Turning on global single stepping.\n");
  1200| 		ss_req->global = TRUE;
  1201| 		mono_de_start_single_stepping ();
  1202| 	} else if (!ss_req->bps) {
  1203| 		PRINT_DEBUG_MSG (1, "[dbg] Turning on global single stepping.\n");
  1204| 		ss_req->global = TRUE;
  1205| 		mono_de_start_single_stepping ();
  1206| 	} else {
  1207| 		ss_req->global = FALSE;
  1208| 	}
  1209| 	g_hash_table_destroy (ss_req_bp_cache);
  1210| 	if (locked)
  1211| 		mono_loader_unlock ();
  1212| cleanup:
  1213| 	mono_ss_args_destroy (ss_args);
  1214| }
  1215| /*
  1216|  * Start single stepping of thread THREAD
  1217|  */
  1218| DbgEngineErrorCode
  1219| mono_de_ss_create (MonoInternalThread *thread, StepSize size, StepDepth depth, StepFilter filter, EventRequest *req)
  1220| {
  1221| 	int err = rt_callbacks.ensure_runtime_is_suspended ();
  1222| 	if (err)
  1223| 		return err;
  1224| 	if (ss_req_count () > 1) {
  1225| 		err = rt_callbacks.handle_multiple_ss_requests ();
  1226| 		if (err == DE_ERR_NOT_IMPLEMENTED) {
  1227| 			PRINT_DEBUG_MSG (0, "Received a single step request while the previous one was still active.\n");
  1228| 			return DE_ERR_NOT_IMPLEMENTED;
  1229| 		}
  1230| 	}
  1231| 	PRINT_DEBUG_MSG (1, "[dbg] Starting single step of thread %p (depth=%s).\n", thread, ss_depth_to_string (depth));
  1232| 	SingleStepReq *ss_req = g_new0 (SingleStepReq, 1);
  1233| 	ss_req->req = req;
  1234| 	ss_req->thread = thread;
  1235| 	ss_req->size = size;
  1236| 	ss_req->depth = depth;
  1237| 	ss_req->filter = filter;
  1238| 	ss_req->refcount = 1;
  1239| 	req->info = ss_req;
  1240| 	for (int i = 0; i < req->nmodifiers; i++) {
  1241| 		if (req->modifiers[i].kind == MOD_KIND_ASSEMBLY_ONLY) {
  1242| 			ss_req->user_assemblies = req->modifiers[i].data.assemblies;
  1243| 			break;
  1244| 		}
  1245| 	}
  1246| 	SingleStepArgs args;
  1247| 	err = mono_ss_create_init_args (ss_req, &args);
  1248| 	if (err)
  1249| 		return err;
  1250| 	g_ptr_array_add (the_ss_reqs, ss_req);
  1251| 	mono_de_ss_start (ss_req, &args);
  1252| 	return DE_ERR_NONE;
  1253| }
  1254| /*
  1255|  * mono_de_set_log_level:
  1256|  *
  1257|  * Configures logging level and output file. Must be called together with mono_de_init.
  1258|  */
  1259| void
  1260| mono_de_set_log_level (int level, FILE *file)
  1261| {
  1262| 	log_level = level;
  1263| 	log_file = file;
  1264| }
  1265| void
  1266| mono_de_set_using_icordbg (void)
  1267| {
  1268| 	using_icordbg = TRUE;
  1269| }
  1270| /*
  1271|  * mono_de_init:
  1272|  *
  1273|  * Inits the shared debugger engine. Not reentrant.
  1274|  */
  1275| void
  1276| mono_de_init (DebuggerEngineCallbacks *cbs)
  1277| {
  1278| 	rt_callbacks = *cbs;
  1279| 	mono_coop_mutex_init_recursive (&debug_mutex);
  1280| 	domains_init ();
  1281| 	breakpoints_init ();
  1282| 	ss_req_init ();
  1283| 	mono_debugger_log_init ();
  1284| }
  1285| void
  1286| mono_de_cleanup (void)
  1287| {
  1288| 	breakpoints_cleanup ();
  1289| 	domains_cleanup ();
  1290| 	ss_req_cleanup ();
  1291| }
  1292| void
  1293| mono_debugger_free_objref (gpointer value)
  1294| {
  1295| 	ObjRef *o = (ObjRef *)value;
  1296| 	mono_gchandle_free_internal (o->handle);
  1297| 	g_free (o);
  1298| }
  1299| MonoClass *
  1300| get_class_to_get_builder_field (DbgEngineStackFrame *frame)
  1301| {
  1302| 	ERROR_DECL (error);
  1303| 	StackFrame *the_frame = (StackFrame *)frame;
  1304| 	gpointer this_addr = get_this_addr (frame);
  1305| 	MonoClass *original_class = frame->method->klass;
  1306| 	MonoClass *ret;
  1307| 	if (mono_class_is_open_constructed_type (m_class_get_byval_arg (original_class))) {
  1308| 		MonoObject *this_obj = *(MonoObject**)this_addr;
  1309| 		MonoGenericContext context;
  1310| 		MonoType *inflated_type;
  1311| 		if (!this_obj)
  1312| 			return NULL;
  1313| 		context = mono_get_generic_context_from_stack_frame (frame->ji, mono_get_generic_info_from_stack_frame (frame->ji, &the_frame->ctx));
  1314| 		inflated_type = mono_class_inflate_generic_type_checked (m_class_get_byval_arg (original_class), &context, error);
  1315| 		mono_error_assert_ok (error); /* FIXME don't swallow the error */
  1316| 		ret = mono_class_from_mono_type_internal (inflated_type);
  1317| 		mono_metadata_free_type (inflated_type);
  1318| 		return ret;
  1319| 	}
  1320| 	return original_class;
  1321| }
  1322| gboolean
  1323| set_set_notification_for_wait_completion_flag (DbgEngineStackFrame *frame)
  1324| {
  1325| 	MonoClassField *builder_field = mono_class_get_field_from_name_full (get_class_to_get_builder_field(frame), "<>t__builder", NULL);
  1326| 	if (!builder_field)
  1327| 		return FALSE;
  1328| 	gpointer builder = get_async_method_builder (frame);
  1329| 	if (!builder)
  1330| 		return FALSE;
  1331| 	MonoMethod* method = get_set_notification_method (mono_class_from_mono_type_internal (builder_field->type));
  1332| 	if (method == NULL)
  1333| 		return FALSE;
  1334| 	gboolean arg = TRUE;
  1335| 	ERROR_DECL (error);
  1336| 	void *args [ ] = { &arg };
  1337| 	mono_runtime_invoke_checked (method, builder, args, error);
  1338| 	mono_error_assert_ok (error);
  1339| 	return TRUE;
  1340| }
  1341| MonoMethod*
  1342| get_object_id_for_debugger_method (MonoClass* async_builder_class)
  1343| {
  1344| 	ERROR_DECL (error);
  1345| 	GPtrArray *array = mono_class_get_methods_by_name (async_builder_class, "get_ObjectIdForDebugger", 0x24, 1, FALSE, error);
  1346| 	mono_error_assert_ok (error);
  1347| 	if (array->len != 1) {
  1348| 		g_ptr_array_free (array, TRUE);
  1349| 		MonoProperty *prop = mono_class_get_property_from_name_internal (async_builder_class, "Task");
  1350| 		if (!prop) {
  1351| 			PRINT_DEBUG_MSG (1, "Impossible to debug async methods.\n");
  1352| 			return NULL;
  1353| 		}
  1354| 		return prop->get;
  1355| 	}
  1356| 	MonoMethod *method = (MonoMethod *)g_ptr_array_index (array, 0);
  1357| 	g_ptr_array_free (array, TRUE);
  1358| 	return method;
  1359| }
  1360| static gpointer
  1361| get_this_addr (DbgEngineStackFrame *the_frame)
  1362| {
  1363| 	StackFrame *frame = (StackFrame *)the_frame;
  1364| 	if (frame->de.ji->is_interp)
  1365| 		return mini_get_interp_callbacks_api ()->frame_get_this (frame->interp_frame);
  1366| 	MonoDebugVarInfo *var = frame->jit->this_var;
  1367| 	if ((var->index & MONO_DEBUG_VAR_ADDRESS_MODE_FLAGS) != MONO_DEBUG_VAR_ADDRESS_MODE_REGOFFSET)
  1368| 		return NULL;
  1369| 	guint8 *addr = (guint8 *)mono_arch_context_get_int_reg (&frame->ctx, var->index & ~MONO_DEBUG_VAR_ADDRESS_MODE_FLAGS);
  1370| 	addr += (gint32)var->offset;
  1371| 	return addr;
  1372| }
  1373| /* Return the address of the AsyncMethodBuilder struct belonging to the state machine method pointed to by FRAME */
  1374| gpointer
  1375| get_async_method_builder (DbgEngineStackFrame *frame)
  1376| {
  1377| 	MonoObject *this_obj;
  1378| 	MonoClassField *builder_field;
  1379| 	gpointer builder;
  1380| 	gpointer this_addr;
  1381| 	MonoClass* klass = frame->method->klass;
  1382| 	klass = get_class_to_get_builder_field(frame);
  1383| 	builder_field = mono_class_get_field_from_name_full (klass, "<>t__builder", NULL);
  1384| 	if (!builder_field)
  1385| 		return NULL;
  1386| 	this_addr = get_this_addr (frame);
  1387| 	if (!this_addr)
  1388| 		return NULL;
  1389| 	if (m_class_is_valuetype (klass)) {
  1390| 		builder = mono_vtype_get_field_addr (*(guint8**)this_addr, builder_field);
  1391| 	} else {
  1392| 		this_obj = *(MonoObject**)this_addr;
  1393| 		builder = (char*)this_obj + m_field_get_offset (builder_field);
  1394| 	}
  1395| 	return builder;
  1396| }
  1397| static MonoMethod*
  1398| get_set_notification_method (MonoClass* async_builder_class)
  1399| {
  1400| 	ERROR_DECL (error);
  1401| 	GPtrArray* array = mono_class_get_methods_by_name (async_builder_class, "SetNotificationForWaitCompletion", 0x24, 1, FALSE, error);
  1402| 	mono_error_assert_ok (error);
  1403| 	if (array->len == 0) {
  1404| 		g_ptr_array_free (array, TRUE);
  1405| 		return NULL;
  1406| 	}
  1407| 	MonoMethod* set_notification_method = (MonoMethod *)g_ptr_array_index (array, 0);
  1408| 	g_ptr_array_free (array, TRUE);
  1409| 	return set_notification_method;
  1410| }
  1411| static MonoMethod* notify_debugger_of_wait_completion_method_cache;
  1412| MonoMethod*
  1413| get_notify_debugger_of_wait_completion_method (void)
  1414| {
  1415| 	if (notify_debugger_of_wait_completion_method_cache != NULL)
  1416| 		return notify_debugger_of_wait_completion_method_cache;
  1417| 	ERROR_DECL (error);
  1418| 	MonoClass* task_class = mono_class_load_from_name (mono_get_corlib (), "System.Threading.Tasks", "Task");
  1419| 	GPtrArray* array = mono_class_get_methods_by_name (task_class, "NotifyDebuggerOfWaitCompletion", 0x24, 1, FALSE, error);
  1420| 	mono_error_assert_ok (error);
  1421| 	g_assert (array->len == 1);
  1422| 	notify_debugger_of_wait_completion_method_cache = (MonoMethod *)g_ptr_array_index (array, 0);
  1423| 	g_ptr_array_free (array, TRUE);
  1424| 	return notify_debugger_of_wait_completion_method_cache;
  1425| }
  1426| DbgEngineErrorCode
  1427| mono_de_set_interp_var (MonoType *t, gpointer addr, guint8 *val_buf)
  1428| {
  1429| 	int size;
  1430| 	if (m_type_is_byref (t)) {
  1431| 		addr = *(gpointer*)addr;
  1432| 		if (!addr)
  1433| 			return ERR_INVALID_OBJECT;
  1434| 	}
  1435| 	if (MONO_TYPE_IS_REFERENCE (t))
  1436| 		size = sizeof (gpointer);
  1437| 	else
  1438| 		size = mono_class_value_size (mono_class_from_mono_type_internal (t), NULL);
  1439| 	memcpy (addr, val_buf, size);
  1440| 	return ERR_NONE;
  1441| }
  1442| #endif


# ====================================================================
# FILE: src/mono/mono/metadata/class-internals.h
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-1279 ---
     1| /**
     2|  * \file
     3|  * Copyright 2012 Xamarin Inc
     4|  * Licensed under the MIT license. See LICENSE file in the project root for full license information.
     5|  */
     6| #ifndef __MONO_METADATA_CLASS_INTERNALS_H__
     7| #define __MONO_METADATA_CLASS_INTERNALS_H__
     8| #include <mono/metadata/class.h>
     9| #include <mono/metadata/object.h>
    10| #include <mono/metadata/mempool.h>
    11| #include <mono/metadata/metadata-internals.h>
    12| #include <mono/metadata/property-bag.h>
    13| #include "mono/utils/mono-compiler.h"
    14| #include "mono/utils/mono-error.h"
    15| #include "mono/sgen/gc-internal-agnostic.h"
    16| #include "mono/utils/mono-error-internals.h"
    17| #include "mono/utils/mono-memory-model.h"
    18| #include "mono/utils/mono-compiler.h"
    19| #define MONO_CLASS_IS_ARRAY(c) (m_class_get_rank (c))
    20| #define MONO_CLASS_HAS_STATIC_METADATA(klass) (m_class_get_type_token (klass) && !m_class_get_image (klass)->dynamic && !mono_class_is_ginst (klass))
    21| #define MONO_DEFAULT_SUPERTABLE_SIZE 6
    22| extern gboolean mono_print_vtable;
    23| extern gboolean mono_align_small_structs;
    24| extern gint32 mono_simd_register_size;
    25| typedef struct _MonoMethodWrapper MonoMethodWrapper;
    26| typedef struct _MonoMethodInflated MonoMethodInflated;
    27| typedef struct _MonoMethodPInvoke MonoMethodPInvoke;
    28| typedef struct _MonoDynamicMethod MonoDynamicMethod;
    29| /* Properties that applies to a group of structs should better use a higher number
    30|  * to avoid colision with type specific properties.
    31|  *
    32|  * This prop applies to class, method, property, event, assembly and image.
    33|  */
    34| #define MONO_PROP_DYNAMIC_CATTR 0x1000
    35| typedef enum {
    36| #define WRAPPER(e,n) MONO_WRAPPER_ ## e,
    37| #include "wrapper-types.h"
    38| #undef WRAPPER
    39| 	MONO_WRAPPER_NUM
    40| } MonoWrapperType;
    41| #define MONO_METHOD_PROP_GENERIC_CONTAINER 0
    42| /* verification success bit, protected by the image lock */
    43| #define MONO_METHOD_PROP_VERIFICATION_SUCCESS 1
    44| /* infrequent vtable layout bits protected by the loader lock */
    45| #define MONO_METHOD_PROP_INFREQUENT_BITS 2
    46| /* Infrequently accessed bits of method definitions stored in the image properties.
    47|  * The method must not be inflated.
    48|  *
    49|  * LOCKING: Reading the bits acquires the image lock.  Writing the bits assumes
    50|  * the loader lock is held.
    51|  */
    52| typedef struct _MonoMethodDefInfrequentBits {
    53| 	unsigned int is_reabstracted:1;  /* whenever this is a reabstraction of another interface */
    54| 	unsigned int is_covariant_override_impl:1; /* whether this is an override with a signature different from its declared method */
    55| } MonoMethodDefInfrequentBits;
    56| struct _MonoMethod {
    57| 	guint16 flags;  /* method flags */
    58| 	guint16 iflags; /* method implementation flags */
    59| 	guint32 token;
    60| 	MonoClass *klass; /* To what class does this method belong */
    61| 	MonoMethodSignature *signature;
    62| 	/* name is useful mostly for debugging */
    63| 	const char *name;
    64| 	/* this is used by the inlining algorithm */
    65| 	unsigned int inline_info:1;
    66| 	unsigned int inline_failure:1;
    67| 	unsigned int wrapper_type:5;
    68| 	unsigned int string_ctor:1;
    69| 	unsigned int save_lmf:1;
    70| 	unsigned int dynamic:1; /* created & destroyed during runtime */
    71| 	unsigned int sre_method:1; /* created at runtime using Reflection.Emit */
    72| 	unsigned int is_generic:1; /* whenever this is a generic method definition */
    73| 	unsigned int is_inflated:1; /* whether we're a MonoMethodInflated */
    74| 	unsigned int skip_visibility:1; /* whenever to skip JIT visibility checks */
    75| 	unsigned int _unused : 2; /* unused */
    76| 	signed int slot : 16;
    77| 	/*
    78| 	 * If is_generic is TRUE, the generic_container is stored in image->property_hash,
    79| 	 * using the key MONO_METHOD_PROP_GENERIC_CONTAINER.
    80| 	 */
    81| };
    82| struct _MonoMethodWrapper {
    83| 	MonoMethod method;
    84| 	MonoMethodHeader *header;
    85| 	MonoMemoryManager *mem_manager;
    86| 	void *method_data;
    87| };
    88| struct _MonoDynamicMethod {
    89| 	MonoMethodWrapper method;
    90| 	MonoAssembly *assembly;
    91| 	MonoMemPool *mp;
    92| };
    93| struct _MonoMethodPInvoke {
    94| 	MonoMethod method;
    95| 	gpointer addr;
    96| 	/* add marshal info */
    97| 	union {
    98| 		guint16 piflags;  /* pinvoke flags */
    99| 		guint16 icflags;  /* icall flags */
   100| 	};
   101| 	guint32 implmap_idx;  /* index into IMPLMAP */
   102| };
   103| /*
   104|  * Stores the default value / RVA of fields.
   105|  * This information is rarely needed, so it is stored separately from
   106|  * MonoClassField.
   107|  */
   108| typedef struct MonoFieldDefaultValue {
   109| 	/*
   110| 	 * If the field is constant, pointer to the metadata constant
   111| 	 * value.
   112| 	 * If the field has an RVA flag, pointer to the data.
   113| 	 * Else, invalid.
   114| 	 */
   115| 	const char      *data;
   116| 	/* If the field is constant, the type of the constant. */
   117| 	MonoTypeEnum     def_type;
   118| } MonoFieldDefaultValue;
   119| /*
   120|  * MonoClassField is just a runtime representation of the metadata for
   121|  * field, it doesn't contain the data directly.  Static fields are
   122|  * stored in MonoVTable->data.  Instance fields are allocated in the
   123|  * objects after the object header.
   124|  */
   125| struct _MonoClassField {
   126| 	/* Type of the field */
   127| 	MonoType        *type;
   128| 	const char      *name;
   129| 	/* Type where the field was defined */
   130| 	/* Do not access directly, use m_field_get_parent */
   131| 	/* We use the lowest bits of the pointer to store some flags, see m_field_get_meta_flags */
   132| 	uintptr_t	parent_and_flags;
   133| 	/*
   134| 	 * Offset where this field is stored; if it is an instance
   135| 	 * field, it's the offset from the start of the object, if
   136| 	 * it's static, it's from the start of the memory chunk
   137| 	 * allocated for statics for the class.
   138| 	 * -1 means its a special static field.
   139| 	 * -2 means its a collectible static field.
   140| 	 */
   141| 	int              offset;
   142| };
   143| /* a field is ignored if it's named "_Deleted" and it has the specialname and rtspecialname flags set */
   144| #define mono_field_is_deleted(field) (((field)->type->attrs & (FIELD_ATTRIBUTE_SPECIAL_NAME | FIELD_ATTRIBUTE_RT_SPECIAL_NAME)) \
   145| 				      && (strcmp (mono_field_get_name (field), "_Deleted") == 0))
   146| /* a field is ignored if it's named "_Deleted" and it has the specialname and rtspecialname flags set */
   147| /* Try to avoid loading the field's type */
   148| #define mono_field_is_deleted_with_flags(field, flags) (((flags) & (FIELD_ATTRIBUTE_SPECIAL_NAME | FIELD_ATTRIBUTE_RT_SPECIAL_NAME)) \
   149| 				      && (strcmp (mono_field_get_name (field), "_Deleted") == 0))
   150| typedef struct {
   151| 	MonoClassField *field;
   152| 	guint32 offset;
   153| 	MonoMarshalSpec *mspec;
   154| } MonoMarshalField;
   155| typedef struct {
   156| 	MonoPropertyBagItem head;
   157| 	guint32 native_size, min_align;
   158| 	guint32 num_fields;
   159| 	MonoMethod *ptr_to_str;
   160| 	MonoMethod *str_to_ptr;
   161| 	MonoMarshalField fields [MONO_ZERO_LEN_ARRAY];
   162| } MonoMarshalType;
   163| #define MONO_SIZEOF_MARSHAL_TYPE (offsetof (MonoMarshalType, fields))
   164| struct _MonoProperty {
   165| 	MonoClass *parent;
   166| 	const char *name;
   167| 	MonoMethod *get;
   168| 	MonoMethod *set;
   169| 	guint32 attrs; /* upper bits store non-ECMA flags */
   170| };
   171| /* non-ECMA flags for the MonoProperty attrs field */
   172| enum {
   173| 	/* added by metadata-update after class was created;
   174| 	 * not in MonoClassPropertyInfo array - don't do ptr arithmetic */
   175| 	MONO_PROPERTY_META_FLAG_FROM_UPDATE = 0x00010000,
   176| 	MONO_PROPERTY_META_FLAG_MASK = 0x00010000,
   177| };
   178| struct _MonoEvent {
   179| 	MonoClass *parent;
   180| 	const char *name;
   181| 	MonoMethod *add;
   182| 	MonoMethod *remove;
   183| 	MonoMethod *raise;
   184| #ifndef MONO_SMALL_CONFIG
   185| 	MonoMethod **other;
   186| #endif
   187| 	guint32 attrs;  /* upper bits store non-ECMA flags */
   188| };
   189| /* non-ECMA flags for the MonoEvent attrs field */
   190| enum {
   191| 	/* added by metadata-update after class was created;
   192| 	 * not in MonoClassEventInfo array - don't do ptr arithmetic */
   193| 	MONO_EVENT_META_FLAG_FROM_UPDATE = 0x00010000,
   194| 	MONO_EVENT_META_FLAG_MASK = 0x00010000,
   195| };
   196| /* type of exception being "on hold" for later processing (see exception_type) */
   197| typedef enum {
   198| 	MONO_EXCEPTION_NONE = 0,
   199| 	MONO_EXCEPTION_INVALID_PROGRAM = 3,
   200| 	MONO_EXCEPTION_UNVERIFIABLE_IL = 4,
   201| 	MONO_EXCEPTION_MISSING_METHOD = 5,
   202| 	MONO_EXCEPTION_MISSING_FIELD = 6,
   203| 	MONO_EXCEPTION_TYPE_LOAD = 7,
   204| 	MONO_EXCEPTION_FILE_NOT_FOUND = 8,
   205| 	MONO_EXCEPTION_METHOD_ACCESS = 9,
   206| 	MONO_EXCEPTION_FIELD_ACCESS = 10,
   207| 	MONO_EXCEPTION_GENERIC_SHARING_FAILED = 11,
   208| 	MONO_EXCEPTION_BAD_IMAGE = 12,
   209| 	MONO_EXCEPTION_OBJECT_SUPPLIED = 13, /*The exception object is already created.*/
   210| 	MONO_EXCEPTION_OUT_OF_MEMORY = 14,
   211| 	MONO_EXCEPTION_INLINE_FAILED = 15,
   212| 	MONO_EXCEPTION_MONO_ERROR = 16,
   213| 	/* add other exception type */
   214| } MonoExceptionType;
   215| typedef struct {
   216| 	MonoPropertyBagItem head;
   217| 	MonoProperty *properties;
   218| 	guint32 first, count;
   219| 	MonoFieldDefaultValue *def_values;
   220| } MonoClassPropertyInfo;
   221| typedef struct {
   222| 	MonoPropertyBagItem head;
   223| 	/* Initialized by a call to mono_class_setup_events () */
   224| 	MonoEvent *events;
   225| 	guint32 first, count;
   226| } MonoClassEventInfo;
   227| typedef enum {
   228| 	MONO_CLASS_DEF = 1, /* non-generic type */
   229| 	MONO_CLASS_GTD, /* generic type definition */
   230| 	MONO_CLASS_GINST, /* generic instantiation */
   231| 	MONO_CLASS_GPARAM, /* generic parameter */
   232| 	MONO_CLASS_ARRAY, /* vector or array, bounded or not */
   233| 	MONO_CLASS_POINTER, /* pointer or function pointer*/
   234| 	MONO_CLASS_GC_FILLER = 0xAC /* not a real class kind - used for sgen nursery filler arrays */
   235| } MonoTypeKind;
   236| typedef struct _MonoClassDef MonoClassDef;
   237| typedef struct _MonoClassGtd MonoClassGtd;
   238| typedef struct _MonoClassGenericInst MonoClassGenericInst;
   239| typedef struct _MonoClassGenericParam MonoClassGenericParam;
   240| typedef struct _MonoClassArray MonoClassArray;
   241| typedef struct _MonoClassPointer MonoClassPointer;
   242| union _MonoClassSizes {
   243| 		int class_size; /* size of area for static fields */
   244| 		int element_size; /* for array types */
   245| 		int generic_param_token; /* for generic param types, both var and mvar */
   246| };
   247| /* enabled only with small config for now: we might want to do it unconditionally */
   248| #ifdef MONO_SMALL_CONFIG
   249| #define COMPRESSED_INTERFACE_BITMAP 1
   250| #endif
   251| #ifdef ENABLE_CHECKED_BUILD_PRIVATE_TYPES
   252| #define MONO_CLASS_DEF_PRIVATE 1
   253| #endif
   254| /* Hide _MonoClass definition in checked build mode to ensure that
   255|  * it is only accessed via getter and setter methods.
   256|  */
   257| #ifndef MONO_CLASS_DEF_PRIVATE
   258| #include "class-private-definition.h"
   259| #endif
   260| /* If MonoClass definition is hidden, just declare the getters.
   261|  * Otherwise, define them as static inline functions.
   262|  *
   263|  * In-tree profilers are allowed to use the getters.  So if we're compiling
   264|  * with --enable-checked-build=private_types, mark the symbols with
   265|  * MONO_PROFILER_API
   266|  */
   267| #ifdef MONO_CLASS_DEF_PRIVATE
   268| #define MONO_CLASS_GETTER(funcname, rettype, optref, argtype, fieldname) MONO_PROFILER_API rettype funcname (argtype *klass);
   269| #else
   270| #define MONO_CLASS_GETTER(funcname, rettype, optref, argtype, fieldname) static inline rettype funcname (argtype *klass) { return optref klass-> fieldname ; }
   271| #endif
   272| #define MONO_CLASS_OFFSET(funcname, argtype, fieldname) /*nothing*/
   273| #include "class-getters.h"
   274| #undef MONO_CLASS_GETTER
   275| #undef MONO_CLASS_OFFSET
   276| #ifdef COMPRESSED_INTERFACE_BITMAP
   277| int mono_compress_bitmap (uint8_t *dest, const uint8_t *bitmap, int size);
   278| int mono_class_interface_match (const uint8_t *bitmap, int id);
   279| #else
   280| #define mono_class_interface_match(bmap,uiid) ((bmap) [(uiid) >> 3] & (1 << ((uiid)&7)))
   281| #endif
   282| #define MONO_CLASS_IMPLEMENTS_INTERFACE(k,uiid) (((uiid) <= m_class_get_max_interface_id (k)) && mono_class_interface_match (m_class_get_interface_bitmap (k), (uiid)))
   283| #define MONO_VTABLE_AVAILABLE_GC_BITS 4
   284| #ifdef DISABLE_COM
   285| #define mono_class_is_com_object(klass) (FALSE)
   286| #else
   287| #define mono_class_is_com_object(klass) (m_class_is_com_object (klass))
   288| #endif
   289| MONO_API int mono_class_interface_offset (MonoClass *klass, MonoClass *itf);
   290| MONO_COMPONENT_API int mono_class_interface_offset_with_variance (MonoClass *klass, MonoClass *itf, gboolean *non_exact_match);
   291| typedef gpointer MonoRuntimeGenericContext;
   292| typedef enum {
   293| 	/* array or string */
   294| 	MONO_VT_FLAG_ARRAY_OR_STRING = (1 << 0),
   295| 	MONO_VT_FLAG_HAS_REFERENCES = (1 << 1),
   296| 	MONO_VT_FLAG_ARRAY_IS_PRIMITIVE = (1 << 2),
   297| } MonoVTableFlags;
   298| /* the interface_offsets array is stored in memory before this struct */
   299| struct MonoVTable {
   300| 	MonoClass  *klass;
   301| 	 /*
   302| 	 * According to comments in gc_gcj.h, this should be the second word in
   303| 	 * the vtable.
   304| 	 */
   305| 	MonoGCDescriptor gc_descr;
   306| 	MonoDomain *domain;  /* each object/vtable belongs to exactly one domain */
   307| 	gpointer    type; /* System.Type type for klass */
   308| 	guint8     *interface_bitmap;
   309| 	MonoGCHandle loader_alloc; /* LoaderAllocator object for objects in collectible alcs */
   310| 	guint32     max_interface_id;
   311| 	guint8      rank;
   312| 	/* Keep this a guint8, the jit depends on it */
   313| 	guint8      initialized; /* cctor has been run */
   314| 	/* Keep this a guint8, the jit depends on it */
   315| 	guint8      flags; /* MonoVTableFlags */
   316| 	guint init_failed     : 1; /* cctor execution failed */
   317| 	guint has_static_fields : 1; /* pointer to the data stored at the end of the vtable array */
   318| 	guint gc_bits         : MONO_VTABLE_AVAILABLE_GC_BITS; /* Those bits are reserved for the usaged of the GC */
   319| 	guint32     imt_collisions_bitmap;
   320| 	MonoRuntimeGenericContext *runtime_generic_context;
   321| 	/* Maintained by the Execution Engine */
   322| 	gpointer ee_data;
   323| 	/* do not add any fields after vtable, the structure is dynamically extended */
   324| 	/* vtable contains function pointers to methods or their trampolines, at the
   325| 	 end there may be a slot containing the pointer to the static fields */
   326| 	gpointer    vtable [MONO_ZERO_LEN_ARRAY];
   327| };
   328| #define MONO_SIZEOF_VTABLE (sizeof (MonoVTable) - MONO_ZERO_LEN_ARRAY * SIZEOF_VOID_P)
   329| #define MONO_VTABLE_IMPLEMENTS_INTERFACE(vt,uiid) (((uiid) <= (vt)->max_interface_id) && mono_class_interface_match ((vt)->interface_bitmap, (uiid)))
   330| /*
   331|  * Generic instantiation data type encoding.
   332|  */
   333| /*
   334|  * A particular generic instantiation:
   335|  *
   336|  * All instantiations are cached and we don't distinguish between class and method
   337|  * instantiations here.
   338|  */
   339| struct _MonoGenericInst {
   340| #ifndef MONO_SMALL_CONFIG
   341| 	gint32 id;			/* unique ID for debugging */
   342| #endif
   343| 	guint type_argc    : 22;	/* number of type arguments */
   344| 	guint is_open      :  1;	/* if this is an open type */
   345| 	MonoType *type_argv [MONO_ZERO_LEN_ARRAY];
   346| };
   347| #define MONO_SIZEOF_GENERIC_INST (sizeof (MonoGenericInst) - MONO_ZERO_LEN_ARRAY * SIZEOF_VOID_P)
   348| /*
   349|  * The generic context: an instantiation of a set of class and method generic parameters.
   350|  *
   351|  * NOTE: Never allocate this directly on the heap.  It have to be either allocated on the stack,
   352|  *	 or embedded within other objects.  Don't store pointers to this, because it may be on the stack.
   353|  *	 If you really have to, ensure you store a pointer to the embedding object along with it.
   354|  */
   355| struct _MonoGenericContext {
   356| 	/* The instantiation corresponding to the class generic parameters */
   357| 	MonoGenericInst *class_inst;
   358| 	/* The instantiation corresponding to the method generic parameters */
   359| 	MonoGenericInst *method_inst;
   360| };
   361| /*
   362|  * Inflated generic method.
   363|  */
   364| struct _MonoMethodInflated {
   365| 	union {
   366| 		MonoMethod method;
   367| 		MonoMethodPInvoke pinvoke;
   368| 		MonoMethodWrapper wrapper;
   369| 	} method;
   370| 	MonoMethod *declaring;		/* the generic method definition. */
   371| 	MonoGenericContext context;	/* The current instantiation */
   372| 	MonoMemoryManager *owner; /* The mem manager that the inflated method belongs to. */
   373| };
   374| /*
   375|  * A particular instantiation of a generic type.
   376|  */
   377| struct _MonoGenericClass {
   378| 	MonoClass *container_class;	/* the generic type definition */
   379| 	MonoGenericContext context;	/* a context that contains the type instantiation doesn't contain any method instantiation */ /* FIXME: Only the class_inst member of "context" is ever used, so this field could be replaced with just a monogenericinst */
   380| 	guint is_dynamic  : 1;		/* Contains dynamic types */
   381| 	guint is_tb_open  : 1;		/* This is the fully open instantiation for a type_builder. Quite ugly, but it's temporary.*/
   382| 	MonoClass *cached_class;	/* if present, the MonoClass corresponding to the instantiation.  */
   383| 	/* The mem manager which owns this generic class. */
   384| 	MonoMemoryManager *owner;
   385| };
   386| /* Additional details about a MonoGenericParam */
   387| /* Keep in sync with managed Mono.RuntimeStructs.GenericParamInfo */
   388| typedef struct {
   389| 	MonoClass *pklass;		/* The corresponding `MonoClass'. */
   390| 	const char *name;
   391| 	guint16 flags;
   392| 	guint32 token;
   393| 	MonoClass** constraints; /* NULL means end of list */
   394| } MonoGenericParamInfo;
   395| /*
   396|  * A type parameter.
   397|  */
   398| struct _MonoGenericParam {
   399| 	/*
   400| 	 * Type or method this parameter was defined in.
   401| 	 */
   402| 	MonoGenericContainer *owner;
   403| 	guint16 num;
   404| 	/*
   405| 	 * If != NULL, this is a generated generic param used by the JIT to implement generic
   406| 	 * sharing.
   407| 	 */
   408| 	MonoType *gshared_constraint;
   409| 	MonoGenericParamInfo info;
   410| };
   411| typedef MonoGenericParam MonoGenericParamFull;
   412| /*
   413|  * The generic container.
   414|  *
   415|  * Stores the type parameters of a generic type definition or a generic method definition.
   416|  */
   417| struct _MonoGenericContainer {
   418| 	MonoGenericContext context;
   419| 	/* If we're a generic method definition in a generic type definition,
   420| 	   the generic container of the containing class. */
   421| 	MonoGenericContainer *parent;
   422| 	/* the generic type definition or the generic method definition corresponding to this container */
   423| 	/* Union rules: If is_anonymous, image field is valid; else if is_method, method field is valid; else klass is valid. */
   424| 	union {
   425| 		MonoClass *klass;
   426| 		MonoMethod *method;
   427| 		MonoImage *image;
   428| 	} owner;
   429| 	int type_argc    : 29; // Per the ECMA spec, this value is capped at 16 bits
   430| 	/* If true, we're a generic method, otherwise a generic type definition. */
   431| 	/* Invariant: parent != NULL => is_method */
   432| 	guint is_method     : 1;
   433| 	/* If true, this container has no associated class/method and only the image is known. This can happen:
   434| 	   1. For the special anonymous containers kept by MonoImage.
   435| 	   2. When user code creates a generic parameter via SRE, but has not yet set an owner. */
   436| 	guint is_anonymous : 1;
   437| 	/* Our type parameters. If this is a special anonymous container (case 1, above), this field is not valid, use mono_metadata_create_anon_gparam ()  */
   438| 	MonoGenericParamFull *type_params;
   439| };
   440| static inline MonoGenericParam *
   441| mono_generic_container_get_param (MonoGenericContainer *gc, int i)
   442| {
   443| 	return (MonoGenericParam *) &gc->type_params [i];
   444| }
   445| static inline MonoGenericParamInfo *
   446| mono_generic_container_get_param_info (MonoGenericContainer *gc, int i)
   447| {
   448| 	return &gc->type_params [i].info;
   449| }
   450| static inline MonoGenericContainer *
   451| mono_generic_param_owner (MonoGenericParam *p)
   452| {
   453| 	return p->owner;
   454| }
   455| static inline guint16
   456| mono_generic_param_num (MonoGenericParam *p)
   457| {
   458| 	return p->num;
   459| }
   460| static inline MonoGenericParamInfo *
   461| mono_generic_param_info (MonoGenericParam *p)
   462| {
   463| 	return &((MonoGenericParamFull *) p)->info;
   464| }
   465| static inline const char *
   466| mono_generic_param_name (MonoGenericParam *p)
   467| {
   468| 	return ((MonoGenericParamFull *) p)->info.name;
   469| }
   470| static inline MonoGenericContainer *
   471| mono_type_get_generic_param_owner (MonoType *t)
   472| {
   473| 	return mono_generic_param_owner (t->data.generic_param);
   474| }
   475| static inline guint16
   476| mono_type_get_generic_param_num (MonoType *t)
   477| {
   478| 	return mono_generic_param_num (t->data.generic_param);
   479| }
   480| /*
   481|  * Class information which might be cached by the runtime in the AOT file for
   482|  * example. Caching this allows us to avoid computing a generic vtable
   483|  * (class->vtable) in most cases, saving time and avoiding creation of lots of
   484|  * MonoMethod structures.
   485|  */
   486| typedef struct MonoCachedClassInfo {
   487| 	guint32 vtable_size;
   488| 	guint has_finalize : 1;
   489| 	guint ghcimpl : 1;
   490| 	guint has_cctor : 1;
   491| 	guint has_nested_classes : 1;
   492| 	guint blittable : 1;
   493| 	guint has_references : 1;
   494| 	guint has_static_refs : 1;
   495| 	guint no_special_static_fields : 1;
   496| 	guint is_generic_container : 1;
   497| 	guint has_weak_fields : 1;
   498| 	guint has_deferred_failure : 1;
   499| 	guint32 cctor_token;
   500| 	MonoImage *finalize_image;
   501| 	guint32 finalize_token;
   502| 	guint32 instance_size;
   503| 	guint32 class_size;
   504| 	guint32 packing_size;
   505| 	guint32 min_align;
   506| } MonoCachedClassInfo;
   507| typedef struct {
   508| 	const char *name;
   509| 	gconstpointer func;
   510| 	gconstpointer wrapper;
   511| 	gconstpointer trampoline;
   512| 	MonoMethodSignature *sig;
   513| 	const char *c_symbol;
   514| 	MonoMethod *wrapper_method;
   515| } MonoJitICallInfo;
   516| MONO_COMPONENT_API void
   517| mono_class_setup_supertypes (MonoClass *klass);
   518| /* WARNING
   519|  * Only call this function if you can ensure both @klass and @parent
   520|  * have supertype information initialized.
   521|  * This can be accomplished by mono_class_setup_supertypes or mono_class_init.
   522|  * If unsure, use mono_class_has_parent.
   523|  */
   524| static inline gboolean
   525| mono_class_has_parent_fast (MonoClass *klass, MonoClass *parent)
   526| {
   527| 	return (m_class_get_idepth (klass) >= m_class_get_idepth (parent)) && (m_class_get_supertypes (klass) [m_class_get_idepth (parent) - 1] == parent);
   528| }
   529| static inline gboolean
   530| mono_class_has_parent (MonoClass *klass, MonoClass *parent)
   531| {
   532| 	if (G_UNLIKELY (!m_class_get_supertypes (klass)))
   533| 		mono_class_setup_supertypes (klass);
   534| 	if (G_UNLIKELY (!m_class_get_supertypes (parent)))
   535| 		mono_class_setup_supertypes (parent);
   536| 	return mono_class_has_parent_fast (klass, parent);
   537| }
   538| typedef struct {
   539| 	MonoVTable *default_vtable;
   540| 	MonoVTable *xdomain_vtable;
   541| 	MonoClass *proxy_class;
   542| 	char* proxy_class_name;
   543| 	uint32_t interface_count;
   544| 	MonoClass *interfaces [MONO_ZERO_LEN_ARRAY];
   545| } MonoRemoteClass;
   546| #define MONO_SIZEOF_REMOTE_CLASS (sizeof (MonoRemoteClass) - MONO_ZERO_LEN_ARRAY * SIZEOF_VOID_P)
   547| typedef struct {
   548| 	gint32 initialized_class_count;
   549| 	gint32 generic_vtable_count;
   550| 	gint32 used_class_count;
   551| 	gint32 method_count;
   552| 	gint32 class_vtable_size;
   553| 	gint32 class_static_data_size;
   554| 	gint32 generic_class_count;
   555| 	gint32 inflated_method_count;
   556| 	gint32 inflated_type_count;
   557| 	gint32 delegate_creations;
   558| 	gint32 imt_tables_size;
   559| 	gint32 imt_number_of_tables;
   560| 	gint32 imt_number_of_methods;
   561| 	gint32 imt_used_slots;
   562| 	gint32 imt_slots_with_collisions;
   563| 	gint32 imt_max_collisions_in_slot;
   564| 	gint32 imt_method_count_when_max_collisions;
   565| 	gint32 imt_trampolines_size;
   566| 	gint32 jit_info_table_insert_count;
   567| 	gint32 jit_info_table_remove_count;
   568| 	gint32 jit_info_table_lookup_count;
   569| 	gint32 generics_sharable_methods;
   570| 	gint32 generics_unsharable_methods;
   571| 	gint32 generics_shared_methods;
   572| 	gint32 gsharedvt_methods;
   573| 	gboolean enabled;
   574| } MonoStats;
   575| /*
   576|  * The definition of the first field in SafeHandle,
   577|  * Keep in sync with SafeHandle.cs, this is only used
   578|  * to access the `handle' parameter.
   579|  */
   580| typedef struct {
   581| 	MonoObject  base;
   582| 	void       *handle;
   583| } MonoSafeHandle;
   584| /*
   585|  * Keep in sync with HandleRef.cs
   586|  */
   587| typedef struct {
   588| 	MonoObject *wrapper;
   589| 	void       *handle;
   590| } MonoHandleRef;
   591| extern MonoStats mono_stats;
   592| static inline gboolean
   593| method_is_dynamic (MonoMethod *method)
   594| {
   595| #ifdef DISABLE_REFLECTION_EMIT
   596| 	return FALSE;
   597| #else
   598| 	return method->dynamic;
   599| #endif
   600| }
   601| MonoMethod*
   602| mono_class_get_method_by_index (MonoClass *klass, int index);
   603| MonoMethod*
   604| mono_class_get_inflated_method (MonoClass *klass, MonoMethod *method, MonoError *error);
   605| MonoMethod*
   606| mono_class_get_vtable_entry (MonoClass *klass, int offset);
   607| GPtrArray*
   608| mono_class_get_implemented_interfaces (MonoClass *klass, MonoError *error);
   609| int
   610| mono_class_get_vtable_size (MonoClass *klass);
   611| MONO_COMPONENT_API gboolean
   612| mono_class_is_open_constructed_type (MonoType *t);
   613| void
   614| mono_class_get_overrides_full (MonoImage *image, guint32 type_token, MonoMethod ***overrides, gint32 *num_overrides, MonoGenericContext *generic_context, MonoError *error);
   615| MonoMethod*
   616| mono_class_get_cctor (MonoClass *klass);
   617| MonoMethod*
   618| mono_class_get_finalizer (MonoClass *klass);
   619| gboolean
   620| mono_class_needs_cctor_run (MonoClass *klass, MonoMethod *caller);
   621| MONO_COMPONENT_API gboolean
   622| mono_class_field_is_special_static (MonoClassField *field);
   623| MONO_COMPONENT_API guint32
   624| mono_class_field_get_special_static_type (MonoClassField *field);
   625| gboolean
   626| mono_class_has_special_static_fields (MonoClass *klass);
   627| const char*
   628| mono_class_get_field_default_value (MonoClassField *field, MonoTypeEnum *def_type);
   629| MONO_COMPONENT_API MonoProperty*
   630| mono_class_get_property_from_name_internal (MonoClass *klass, const char *name);
   631| const char*
   632| mono_class_get_property_default_value (MonoProperty *property, MonoTypeEnum *def_type);
   633| gpointer
   634| mono_lookup_dynamic_token (MonoImage *image, guint32 token, MonoGenericContext *context, MonoError *error);
   635| gpointer
   636| mono_lookup_dynamic_token_class (MonoImage *image, guint32 token, gboolean check_token, MonoClass **handle_class, MonoGenericContext *context, MonoError *error);
   637| MONO_PROFILER_API MonoGenericContext*
   638| mono_class_get_context (MonoClass *klass);
   639| MONO_PROFILER_API MonoMethodSignature*
   640| mono_method_signature_checked_slow (MonoMethod *m, MonoError *err);
   641| MONO_PROFILER_API MonoMethodSignature*
   642| mono_method_signature_internal_slow (MonoMethod *m);
   643| /**
   644|  * mono_method_signature_checked:
   645|  *
   646|  * Return the signature of the method M. On failure, returns NULL, and ERR is set.
   647|  */
   648| static inline MonoMethodSignature*
   649| mono_method_signature_checked (MonoMethod *m, MonoError *error)
   650| {
   651| 	error_init (error);
   652| 	MonoMethodSignature* sig = m->signature;
   653| 	return sig ? sig : mono_method_signature_checked_slow (m, error);
   654| }
   655| /**
   656|  * mono_method_signature_internal:
   657|  * \returns the signature of the method \p m. On failure, returns NULL.
   658|  */
   659| static inline MonoMethodSignature*
   660| mono_method_signature_internal (MonoMethod *m)
   661| {
   662| 	MonoMethodSignature* sig = m->signature;
   663| 	return sig ? sig : mono_method_signature_internal_slow (m);
   664| }
   665| MonoGenericContext*
   666| mono_method_get_context_general (MonoMethod *method, gboolean uninflated);
   667| MONO_PROFILER_API MonoGenericContext*
   668| mono_method_get_context (MonoMethod *method);
   669| /* Used by monodis, thus cannot be MONO_INTERNAL */
   670| MONO_API MonoGenericContainer*
   671| mono_method_get_generic_container (MonoMethod *method);
   672| MonoGenericContext*
   673| mono_generic_class_get_context (MonoGenericClass *gclass);
   674| void
   675| mono_method_set_generic_container (MonoMethod *method, MonoGenericContainer* container);
   676| void
   677| mono_method_set_verification_success (MonoMethod *method);
   678| gboolean
   679| mono_method_get_verification_success (MonoMethod *method);
   680| const MonoMethodDefInfrequentBits *
   681| mono_method_lookup_infrequent_bits (MonoMethod *methoddef);
   682| MonoMethodDefInfrequentBits *
   683| mono_method_get_infrequent_bits (MonoMethod *methoddef);
   684| gboolean
   685| mono_method_get_is_reabstracted (MonoMethod *method);
   686| void
   687| mono_method_set_is_reabstracted (MonoMethod *methoddef);
   688| gboolean
   689| mono_method_get_is_covariant_override_impl (MonoMethod *method);
   690| void
   691| mono_method_set_is_covariant_override_impl (MonoMethod *methoddef);
   692| MONO_COMPONENT_API MonoMethod*
   693| mono_class_inflate_generic_method_full_checked (MonoMethod *method, MonoClass *klass_hint, MonoGenericContext *context, MonoError *error);
   694| MONO_COMPONENT_API MonoMethod *
   695| mono_class_inflate_generic_method_checked (MonoMethod *method, MonoGenericContext *context, MonoError *error);
   696| MonoMemoryManager *
   697| mono_metadata_get_mem_manager_for_type (MonoType *type);
   698| MonoMemoryManager *
   699| mono_metadata_get_mem_manager_for_class (MonoClass *klass);
   700| MonoMemoryManager*
   701| mono_metadata_get_mem_manager_for_method (MonoMethodInflated *method);
   702| MONO_API MonoMethodSignature *
   703| mono_metadata_get_inflated_signature (MonoMethodSignature *sig, MonoGenericContext *context);
   704| MonoType*
   705| mono_class_inflate_generic_type_with_mempool (MonoImage *image, MonoType *type, MonoGenericContext *context, MonoError *error);
   706| MONO_COMPONENT_API MonoType*
   707| mono_class_inflate_generic_type_checked (MonoType *type, MonoGenericContext *context, MonoError *error);
   708| MONO_API void
   709| mono_metadata_free_inflated_signature (MonoMethodSignature *sig);
   710| MonoMethodSignature*
   711| mono_inflate_generic_signature (MonoMethodSignature *sig, MonoGenericContext *context, MonoError *error);
   712| MonoClass*
   713| mono_generic_param_get_base_type (MonoClass *klass);
   714| typedef struct {
   715| 	MonoImage *corlib;
   716| 	MonoClass *object_class;
   717| 	MonoClass *object_class_array; // used via token pasting in mono_array_class_get_cached
   718| 	MonoClass *byte_class;
   719| 	MonoClass *void_class;
   720| 	MonoClass *boolean_class;
   721| 	MonoClass *sbyte_class;
   722| 	MonoClass *int16_class;
   723| 	MonoClass *uint16_class;
   724| 	MonoClass *int32_class;
   725| 	MonoClass *uint32_class;
   726| 	MonoClass *int_class;
   727| 	MonoClass *uint_class;
   728| 	MonoClass *int64_class;
   729| 	MonoClass *uint64_class;
   730| 	MonoClass *single_class;
   731| 	MonoClass *double_class;
   732| 	MonoClass *char_class;
   733| 	MonoClass *string_class;
   734| 	MonoClass *enum_class;
   735| 	MonoClass *array_class;
   736| 	MonoClass *delegate_class;
   737| 	MonoClass *multicastdelegate_class;
   738| 	MonoClass *manualresetevent_class;
   739| 	MonoClass *typehandle_class;
   740| 	MonoClass *fieldhandle_class;
   741| 	MonoClass *methodhandle_class;
   742| 	MonoClass *systemtype_class;
   743| 	MonoClass *runtimetype_class;
   744| 	MonoClass *runtimetype_class_array; // used via token pasting in mono_array_class_get_cached
   745| 	MonoClass *exception_class;
   746| 	MonoClass *threadabortexception_class;
   747| 	MonoClass *thread_class;
   748| 	MonoClass *internal_thread_class;
   749| 	MonoClass *autoreleasepool_class;
   750| 	MonoClass *mono_method_message_class;
   751| 	MonoClass *field_info_class;
   752| 	MonoClass *method_info_class;
   753| 	MonoClass *stack_frame_class;
   754| 	MonoClass *marshal_class;
   755| 	MonoClass *typed_reference_class;
   756| 	MonoClass *argumenthandle_class;
   757| 	MonoClass *monitor_class;
   758| 	MonoClass *generic_ilist_class;
   759| 	MonoClass *generic_nullable_class;
   760| 	MonoClass *attribute_class;
   761| 	MonoClass *attribute_class_array; // used via token pasting in mono_array_class_get_cached
   762| 	MonoClass *critical_finalizer_object; /* MAYBE NULL */
   763| 	MonoClass *generic_ireadonlylist_class;
   764| 	MonoClass *generic_ienumerator_class;
   765| 	MonoClass *alc_class;
   766| 	MonoClass *appcontext_class;
   767| } MonoDefaults;
   768| /* If you need a MonoType, use one of the mono_get_*_type () functions in class-inlines.h */
   769| extern MonoDefaults mono_defaults;
   770| MONO_COMPONENT_API
   771| MonoDefaults *
   772| mono_get_defaults (void);
   773| #define GENERATE_GET_CLASS_WITH_CACHE_DECL(shortname) \
   774| MonoClass* mono_class_get_##shortname##_class (void);
   775| #define GENERATE_TRY_GET_CLASS_WITH_CACHE_DECL(shortname) \
   776| MonoClass* mono_class_try_get_##shortname##_class (void);
   777| static inline MonoImage *
   778| mono_class_generate_get_corlib_impl (void)
   779| {
   780| #ifdef COMPILING_COMPONENT_DYNAMIC
   781|   return mono_get_corlib ();
   782| #else
   783|   return mono_defaults.corlib;
   784| #endif
   785| }
   786| #define GENERATE_GET_CLASS_WITH_CACHE(shortname,name_space,name) \
   787| MonoClass*	\
   788| mono_class_get_##shortname##_class (void)	\
   789| {	\
   790| 	static MonoClass *tmp_class;	\
   791| 	MonoClass *klass = tmp_class;	\
   792| 	if (!klass) {	\
   793| 	  klass = mono_class_load_from_name (mono_class_generate_get_corlib_impl (), name_space, name); \
   794| 		mono_memory_barrier ();	/* FIXME excessive? */ \
   795| 		tmp_class = klass;	\
   796| 	}	\
   797| 	return klass;	\
   798| }
   799| #define GENERATE_TRY_GET_CLASS_WITH_CACHE(shortname,name_space,name) \
   800| MonoClass*	\
   801| mono_class_try_get_##shortname##_class (void)	\
   802| {	\
   803| 	static volatile MonoClass *tmp_class;	\
   804| 	static volatile gboolean inited;	\
   805| 	MonoClass *klass = (MonoClass *)tmp_class;	\
   806| 	mono_memory_barrier ();	\
   807| 	if (!inited) {	\
   808| 		klass = mono_class_try_load_from_name (mono_class_generate_get_corlib_impl (), name_space, name);	\
   809| 		tmp_class = klass;	\
   810| 		mono_memory_barrier ();	\
   811| 		inited = TRUE;	\
   812| 	}	\
   813| 	return klass;	\
   814| }
   815| GENERATE_TRY_GET_CLASS_WITH_CACHE_DECL (safehandle)
   816| #ifndef DISABLE_COM
   817| GENERATE_GET_CLASS_WITH_CACHE_DECL (interop_proxy)
   818| GENERATE_GET_CLASS_WITH_CACHE_DECL (idispatch)
   819| GENERATE_GET_CLASS_WITH_CACHE_DECL (iunknown)
   820| GENERATE_GET_CLASS_WITH_CACHE_DECL (com_object)
   821| GENERATE_GET_CLASS_WITH_CACHE_DECL (variant)
   822| #endif
   823| MonoClass* mono_class_get_appdomain_class (void);
   824| GENERATE_GET_CLASS_WITH_CACHE_DECL (appdomain_unloaded_exception)
   825| GENERATE_TRY_GET_CLASS_WITH_CACHE_DECL (appdomain_unloaded_exception)
   826| GENERATE_GET_CLASS_WITH_CACHE_DECL (valuetype)
   827| GENERATE_TRY_GET_CLASS_WITH_CACHE_DECL(handleref)
   828| GENERATE_GET_CLASS_WITH_CACHE_DECL (assembly_load_context)
   829| GENERATE_GET_CLASS_WITH_CACHE_DECL (native_library)
   830| void
   831| mono_loader_init           (void);
   832| void
   833| mono_loader_cleanup        (void);
   834| MONO_COMPONENT_API void
   835| mono_loader_lock           (void);
   836| MONO_COMPONENT_API void
   837| mono_loader_unlock         (void);
   838| MONO_COMPONENT_API void
   839| mono_loader_lock_track_ownership (gboolean track);
   840| MONO_COMPONENT_API gboolean
   841| mono_loader_lock_is_owned_by_self (void);
   842| void
   843| mono_loader_lock_if_inited (void);
   844| void
   845| mono_loader_unlock_if_inited (void);
   846| void
   847| mono_reflection_init       (void);
   848| void
   849| mono_icall_init            (void);
   850| MONO_COMPONENT_API gpointer
   851| mono_method_get_wrapper_data (MonoMethod *method, guint32 id);
   852| gboolean
   853| mono_metadata_has_generic_params (MonoImage *image, guint32 token);
   854| MONO_API MonoGenericContainer *
   855| mono_metadata_load_generic_params (MonoImage *image, guint32 token,
   856| 				   MonoGenericContainer *parent_container,
   857| 				   gpointer real_owner);
   858| MONO_API gboolean
   859| mono_metadata_load_generic_param_constraints_checked (MonoImage *image, guint32 token,
   860| 					      MonoGenericContainer *container, MonoError *error);
   861| void
   862| mono_register_jit_icall_info (MonoJitICallInfo *info, gconstpointer func, const char *name,
   863| 			      MonoMethodSignature *sig, gboolean no_wrapper, const char *c_symbol);
   864| #ifdef __cplusplus
   865| template <typename T>
   866| inline void
   867| mono_register_jit_icall_info (MonoJitICallInfo *info, T func, const char *name, MonoMethodSignature *sig, gboolean no_wrapper, const char *c_symbol)
   868| {
   869| 	mono_register_jit_icall_info (info, (gconstpointer)func, name, sig, no_wrapper, c_symbol);
   870| }
   871| #endif // __cplusplus
   872| #define mono_register_jit_icall(func, sig, no_wrapper) (mono_register_jit_icall_info (&mono_get_jit_icall_info ()->func, func, #func, (sig), (no_wrapper), NULL))
   873| MonoException*
   874| mono_class_get_exception_for_failure (MonoClass *klass);
   875| char*
   876| mono_identifier_escape_type_name_chars (const char* identifier);
   877| char*
   878| mono_type_get_full_name (MonoClass *klass);
   879| MONO_COMPONENT_API
   880| char *
   881| mono_method_get_name_full (MonoMethod *method, gboolean signature, gboolean ret, MonoTypeNameFormat format);
   882| MONO_PROFILER_API char *
   883| mono_method_get_full_name (MonoMethod *method);
   884| const char*
   885| mono_wrapper_type_to_str (guint32 wrapper_type);
   886| MonoArrayType *mono_dup_array_type (MonoImage *image, MonoArrayType *a);
   887| MonoMethodSignature *mono_metadata_signature_deep_dup (MonoImage *image, MonoMethodSignature *sig);
   888| MONO_API void
   889| mono_image_init_name_cache (MonoImage *image);
   890| MonoClass*
   891| mono_class_get_nullable_param_internal (MonoClass *klass);
   892| /* object debugging functions, for use inside gdb */
   893| MONO_API void mono_object_describe        (MonoObject *obj);
   894| MONO_API void mono_object_describe_fields (MonoObject *obj);
   895| MONO_API void mono_value_describe_fields  (MonoClass* klass, const char* addr);
   896| MONO_API void mono_class_describe_statics (MonoClass* klass);
   897| /* method debugging functions, for use inside gdb */
   898| MONO_API void mono_method_print_code (MonoMethod *method);
   899| MONO_PROFILER_API char *mono_signature_full_name (MonoMethodSignature *sig);
   900| /*Enum validation related functions*/
   901| MONO_API gboolean
   902| mono_type_is_valid_enum_basetype (MonoType * type);
   903| MONO_API gboolean
   904| mono_class_is_valid_enum (MonoClass *klass);
   905| MONO_PROFILER_API gboolean
   906| mono_type_is_primitive (MonoType *type);
   907| MonoType *
   908| mono_type_get_checked        (MonoImage *image, guint32 type_token, MonoGenericContext *context, MonoError *error);
   909| gboolean
   910| mono_generic_class_is_generic_type_definition (MonoGenericClass *gklass);
   911| MonoType*
   912| mono_type_get_basic_type_from_generic (MonoType *type);
   913| gboolean
   914| mono_method_can_access_method_full (MonoMethod *method, MonoMethod *called, MonoClass *context_klass);
   915| gboolean
   916| mono_method_can_access_field_full (MonoMethod *method, MonoClassField *field, MonoClass *context_klass);
   917| gboolean
   918| mono_class_can_access_class (MonoClass *access_class, MonoClass *target_class);
   919| MONO_COMPONENT_API MonoClass *
   920| mono_class_get_generic_type_definition (MonoClass *klass);
   921| gboolean
   922| mono_class_has_parent_and_ignore_generics (MonoClass *klass, MonoClass *parent);
   923| int
   924| mono_method_get_vtable_slot (MonoMethod *method);
   925| int
   926| mono_method_get_vtable_index (MonoMethod *method);
   927| MonoMethod*
   928| mono_method_get_base_method (MonoMethod *method, gboolean definition, MonoError *error);
   929| MonoMethod*
   930| mono_method_search_in_array_class (MonoClass *klass, const char *name, MonoMethodSignature *sig);
   931| void
   932| mono_class_setup_interface_id (MonoClass *klass);
   933| MONO_COMPONENT_API MonoGenericContainer*
   934| mono_class_get_generic_container (MonoClass *klass);
   935| gpointer
   936| mono_class_alloc (MonoClass *klass, int size);
   937| MONO_COMPONENT_API gpointer
   938| mono_class_alloc0 (MonoClass *klass, int size);
   939| #define mono_class_alloc0(klass, size) (g_cast (mono_class_alloc0 ((klass), (size))))
   940| MONO_COMPONENT_API void
   941| mono_class_setup_interfaces (MonoClass *klass, MonoError *error);
   942| MONO_COMPONENT_API MonoClassField*
   943| mono_class_get_field_from_name_full (MonoClass *klass, const char *name, MonoType *type);
   944| MONO_COMPONENT_API MonoVTable*
   945| mono_class_vtable_checked (MonoClass *klass, MonoError *error);
   946| void
   947| mono_class_is_assignable_from_checked (MonoClass *klass, MonoClass *oklass, gboolean *result, MonoError *error);
   948| void
   949| mono_class_signature_is_assignable_from (MonoClass *klass, MonoClass *oklass, gboolean *result, MonoError *error);
   950| gboolean
   951| mono_class_is_assignable_from_slow (MonoClass *target, MonoClass *candidate);
   952| gboolean
   953| mono_class_has_variant_generic_params (MonoClass *klass);
   954| gboolean
   955| mono_class_is_variant_compatible (MonoClass *klass, MonoClass *oklass, gboolean check_for_reference_conv);
   956| gboolean
   957| mono_class_is_subclass_of_internal (MonoClass *klass, MonoClass *klassc, gboolean check_interfaces);
   958| MONO_COMPONENT_API mono_bool
   959| mono_class_is_assignable_from_internal (MonoClass *klass, MonoClass *oklass);
   960| gboolean
   961| mono_byref_type_is_assignable_from (MonoType *type, MonoType *ctype, gboolean signature_assignment);
   962| gboolean mono_is_corlib_image (MonoImage *image);
   963| MonoType*
   964| mono_field_get_type_checked (MonoClassField *field, MonoError *error);
   965| MonoType*
   966| mono_field_get_type_internal (MonoClassField *field);
   967| MONO_COMPONENT_API MonoClassField*
   968| mono_class_get_fields_internal (MonoClass* klass, gpointer *iter);
   969| MonoClassField*
   970| mono_class_get_fields_lazy (MonoClass* klass, gpointer *iter);
   971| gboolean
   972| mono_class_check_vtable_constraints (MonoClass *klass, GList *in_setup);
   973| MONO_COMPONENT_API gboolean
   974| mono_class_has_finalizer (MonoClass *klass);
   975| void
   976| mono_unload_interface_id (MonoClass *klass);
   977| MONO_COMPONENT_API GPtrArray*
   978| mono_class_get_methods_by_name (MonoClass *klass, const char *name, guint32 bflags, guint32 mlisttype, gboolean allow_ctors, MonoError *error);
   979| char*
   980| mono_class_full_name (MonoClass *klass);
   981| MonoClass*
   982| mono_class_inflate_generic_class_checked (MonoClass *gklass, MonoGenericContext *context, MonoError *error);
   983| MONO_PROFILER_API MonoClass *
   984| mono_class_get_checked (MonoImage *image, guint32 type_token, MonoError *error);
   985| MonoClass *
   986| mono_class_get_and_inflate_typespec_checked (MonoImage *image, guint32 type_token, MonoGenericContext *context, MonoError *error);
   987| MONO_COMPONENT_API MonoClass *
   988| mono_class_from_name_checked (MonoImage *image, const char* name_space, const char *name, MonoError *error);
   989| MonoClass *
   990| mono_class_from_name_case_checked (MonoImage *image, const char* name_space, const char *name, MonoError *error);
   991| MONO_PROFILER_API MonoClass *
   992| mono_class_from_mono_type_internal (MonoType *type);
   993| MONO_COMPONENT_API MonoClassField*
   994| mono_field_from_token_checked (MonoImage *image, uint32_t token, MonoClass **retklass, MonoGenericContext *context, MonoError *error);
   995| MONO_COMPONENT_API gpointer
   996| mono_ldtoken_checked (MonoImage *image, guint32 token, MonoClass **handle_class, MonoGenericContext *context, MonoError *error);
   997| MonoImage *
   998| mono_get_image_for_generic_param (MonoGenericParam *param);
   999| char *
  1000| mono_make_generic_name_string (MonoImage *image, int num);
  1001| MONO_COMPONENT_API MonoClass *
  1002| mono_class_load_from_name (MonoImage *image, const char* name_space, const char *name);
  1003| MONO_COMPONENT_API MonoClass*
  1004| mono_class_try_load_from_name (MonoImage *image, const char* name_space, const char *name);
  1005| void
  1006| mono_error_set_for_class_failure (MonoError *orerror, const MonoClass *klass);
  1007| gboolean
  1008| mono_class_has_failure (const MonoClass *klass);
  1009| gboolean
  1010| mono_class_has_deferred_failure (const MonoClass *klass);
  1011| /* Kind specific accessors */
  1012| MONO_COMPONENT_API MonoGenericClass*
  1013| mono_class_get_generic_class (MonoClass *klass);
  1014| MonoGenericClass*
  1015| mono_class_try_get_generic_class (MonoClass *klass);
  1016| void
  1017| mono_class_set_flags (MonoClass *klass, guint32 flags);
  1018| MonoGenericContainer*
  1019| mono_class_try_get_generic_container (MonoClass *klass);
  1020| void
  1021| mono_class_set_generic_container (MonoClass *klass, MonoGenericContainer *container);
  1022| MonoType*
  1023| mono_class_gtd_get_canonical_inst (MonoClass *klass);
  1024| guint32
  1025| mono_class_get_first_method_idx (MonoClass *klass);
  1026| void
  1027| mono_class_set_first_method_idx (MonoClass *klass, guint32 idx);
  1028| guint32
  1029| mono_class_get_first_field_idx (MonoClass *klass);
  1030| void
  1031| mono_class_set_first_field_idx (MonoClass *klass, guint32 idx);
  1032| MONO_COMPONENT_API
  1033| guint32
  1034| mono_class_get_method_count (MonoClass *klass);
  1035| void
  1036| mono_class_set_method_count (MonoClass *klass, guint32 count);
  1037| MONO_COMPONENT_API
  1038| guint32
  1039| mono_class_get_field_count (MonoClass *klass);
  1040| void
  1041| mono_class_set_field_count (MonoClass *klass, guint32 count);
  1042| MonoMarshalType*
  1043| mono_class_get_marshal_info (MonoClass *klass);
  1044| void
  1045| mono_class_set_marshal_info (MonoClass *klass, MonoMarshalType *marshal_info);
  1046| MonoGCHandle
  1047| mono_class_get_ref_info_handle (MonoClass *klass);
  1048| MonoGCHandle
  1049| mono_class_set_ref_info_handle (MonoClass *klass, gpointer value);
  1050| MonoErrorBoxed*
  1051| mono_class_get_exception_data (MonoClass *klass);
  1052| void
  1053| mono_class_set_exception_data (MonoClass *klass, MonoErrorBoxed *value);
  1054| MONO_COMPONENT_API
  1055| GList*
  1056| mono_class_get_nested_classes_property (MonoClass *klass);
  1057| MONO_COMPONENT_API
  1058| void
  1059| mono_class_set_nested_classes_property (MonoClass *klass, GList *value);
  1060| MONO_COMPONENT_API MonoClassPropertyInfo*
  1061| mono_class_get_property_info (MonoClass *klass);
  1062| void
  1063| mono_class_set_property_info (MonoClass *klass, MonoClassPropertyInfo *info);
  1064| MONO_COMPONENT_API MonoClassEventInfo*
  1065| mono_class_get_event_info (MonoClass *klass);
  1066| void
  1067| mono_class_set_event_info (MonoClass *klass, MonoClassEventInfo *info);
  1068| MonoFieldDefaultValue*
  1069| mono_class_get_field_def_values (MonoClass *klass);
  1070| MonoFieldDefaultValue*
  1071| mono_class_get_field_def_values_with_swizzle (MonoClass *klass, int swizzle);
  1072| void
  1073| mono_class_set_field_def_values (MonoClass *klass, MonoFieldDefaultValue *values);
  1074| void
  1075| mono_class_set_field_def_values_with_swizzle (MonoClass *klass, MonoFieldDefaultValue *values, int swizzle);
  1076| guint32
  1077| mono_class_get_declsec_flags (MonoClass *klass);
  1078| void
  1079| mono_class_set_declsec_flags (MonoClass *klass, guint32 value);
  1080| void
  1081| mono_class_set_is_com_object (MonoClass *klass);
  1082| void
  1083| mono_class_set_weak_bitmap (MonoClass *klass, int nbits, gsize *bits);
  1084| gsize*
  1085| mono_class_get_weak_bitmap (MonoClass *klass, int *nbits);
  1086| gboolean
  1087| mono_class_has_dim_conflicts (MonoClass *klass);
  1088| gboolean
  1089| mono_class_is_method_ambiguous (MonoClass *klass, MonoMethod *method);
  1090| void
  1091| mono_class_set_dim_conflicts (MonoClass *klass, GSList *conflicts);
  1092| GSList*
  1093| mono_class_get_dim_conflicts (MonoClass *klass);
  1094| /* opaque struct of class specific hot reload info */
  1095| typedef struct _MonoClassMetadataUpdateInfo MonoClassMetadataUpdateInfo;
  1096| MONO_COMPONENT_API gboolean
  1097| mono_class_has_metadata_update_info (MonoClass *klass);
  1098| MONO_COMPONENT_API MonoClassMetadataUpdateInfo *
  1099| mono_class_get_metadata_update_info (MonoClass *klass);
  1100| MONO_COMPONENT_API void
  1101| mono_class_set_metadata_update_info (MonoClass *klass, MonoClassMetadataUpdateInfo *value);
  1102| MONO_COMPONENT_API MonoMethod *
  1103| mono_class_get_method_from_name_checked (MonoClass *klass, const char *name, int param_count, int flags, MonoError *error);
  1104| void
  1105| mono_class_set_is_simd_type (MonoClass *klass, gboolean is_simd);
  1106| MONO_COMPONENT_API gboolean
  1107| mono_method_has_no_body (MonoMethod *method);
  1108| MONO_COMPONENT_API MonoMethodHeader*
  1109| mono_method_get_header_internal (MonoMethod *method, MonoError *error);
  1110| gboolean
  1111| mono_method_metadata_has_header (MonoMethod *method);
  1112| MONO_COMPONENT_API void
  1113| mono_method_get_param_names_internal (MonoMethod *method, const char **names);
  1114| MonoType*
  1115| mono_class_find_enum_basetype (MonoClass *klass, MonoError *error);
  1116| gboolean
  1117| mono_class_set_failure (MonoClass *klass, MonoErrorBoxed *boxed_error);
  1118| void
  1119| mono_class_set_deferred_failure (MonoClass *klass);
  1120| gboolean
  1121| mono_class_set_type_load_failure_causedby_class (MonoClass *klass, const MonoClass *caused_by, const gchar* msg);
  1122| gboolean mono_class_get_cached_class_info (MonoClass *klass, MonoCachedClassInfo *res);
  1123| MonoMethod* mono_find_method_in_metadata (MonoClass *klass, const char *name, int param_count, int flags);
  1124| int
  1125| mono_class_get_object_finalize_slot (void);
  1126| MonoMethod *
  1127| mono_class_get_default_finalize_method (void);
  1128| MONO_COMPONENT_API const char *
  1129| mono_field_get_rva (MonoClassField *field, int swizzle);
  1130| MONO_COMPONENT_API void
  1131| mono_field_resolve_type (MonoClassField *field, MonoError *error);
  1132| gboolean
  1133| mono_type_has_exceptions (MonoType *type);
  1134| void
  1135| mono_class_set_nonblittable (MonoClass *klass);
  1136| gboolean
  1137| mono_class_publish_gc_descriptor (MonoClass *klass, MonoGCDescriptor gc_descr);
  1138| void
  1139| mono_class_compute_gc_descriptor (MonoClass *klass);
  1140| gboolean
  1141| mono_class_init_checked (MonoClass *klass, MonoError *error);
  1142| MonoType*
  1143| mono_class_enum_basetype_internal (MonoClass *klass);
  1144| gboolean
  1145| mono_method_is_constructor (MonoMethod *method);
  1146| gboolean
  1147| mono_class_has_default_constructor (MonoClass *klass, gboolean public_only);
  1148| gboolean
  1149| mono_method_has_unmanaged_callers_only_attribute (MonoMethod *method);
  1150| #define MONO_STATIC_POINTER_INIT(type, name)					\
  1151| 	static type *static_ ## name;						\
  1152| 	type *name; 								\
  1153| 	name = static_ ## name;							\
  1154| 	if (!name) {								\
  1155| 		/* Custom code here to initialize name */
  1156| #define MONO_STATIC_POINTER_INIT_END(type, name)				\
  1157| 		if (name) {							\
  1158| 			/* Success, commit to static. */			\
  1159| 			mono_atomic_store_seq (&static_ ## name, name);		\
  1160| 		}								\
  1161| 	}									\
  1162| /* Metadata flags for MonoClassField.  These are stored in the lowest bits of a pointer, so there
  1163|  * can't be too many. */
  1164| enum {
  1165| 	/* This MonoClassField was added by EnC metadata update, it's not part of the
  1166| 	 * MonoClass:fields array, and at runtime it is not stored like ordinary instance or static
  1167| 	 * fields. */
  1168| 	MONO_CLASS_FIELD_META_FLAG_FROM_UPDATE = 0x01u,
  1169| 	/* Lowest 2 bits of a pointer reserved for flags */
  1170| 	MONO_CLASS_FIELD_META_FLAG_MASK = 0x03u,
  1171| };
  1172| static inline MonoClass *
  1173| m_field_get_parent (MonoClassField *field)
  1174| {
  1175| 	return (MonoClass*)(field->parent_and_flags & ~MONO_CLASS_FIELD_META_FLAG_MASK);
  1176| }
  1177| static inline unsigned int
  1178| m_field_get_meta_flags (MonoClassField *field)
  1179| {
  1180| 	return (unsigned int)(field->parent_and_flags & MONO_CLASS_FIELD_META_FLAG_MASK);
  1181| }
  1182| static inline int
  1183| m_field_get_offset (MonoClassField *field)
  1184| {
  1185| 	g_assert (m_class_is_fields_inited (m_field_get_parent (field)));
  1186| 	return field->offset;
  1187| }
  1188| static inline gboolean
  1189| m_field_is_from_update (MonoClassField *field)
  1190| {
  1191| 	return (m_field_get_meta_flags (field) & MONO_CLASS_FIELD_META_FLAG_FROM_UPDATE) != 0;
  1192| }
  1193| static inline gboolean
  1194| m_property_is_from_update (MonoProperty *prop)
  1195| {
  1196| 	return (prop->attrs & MONO_PROPERTY_META_FLAG_FROM_UPDATE) != 0;
  1197| }
  1198| static inline gboolean
  1199| m_event_is_from_update (MonoEvent *evt)
  1200| {
  1201| 	return (evt->attrs & MONO_EVENT_META_FLAG_FROM_UPDATE) != 0;
  1202| }
  1203| /*
  1204|  * Memory allocation for images/classes/methods
  1205|  *
  1206|  *   These should be used to allocate memory whose lifetime is equal to
  1207|  * the lifetime of the image/class/method.
  1208|  */
  1209| static inline MonoMemoryManager*
  1210| mono_mem_manager_get_ambient (void)
  1211| {
  1212| 	return (MonoMemoryManager *)mono_alc_get_default ()->memory_manager;
  1213| }
  1214| static inline MonoMemoryManager*
  1215| m_image_get_mem_manager (MonoImage *image)
  1216| {
  1217| 	MonoAssemblyLoadContext *alc = mono_image_get_alc (image);
  1218| 	if (!alc)
  1219| 		alc = mono_alc_get_default ();
  1220| 	return alc->memory_manager;
  1221| }
  1222| static inline void *
  1223| m_image_alloc (MonoImage *image, guint size)
  1224| {
  1225| 	return mono_mem_manager_alloc (m_image_get_mem_manager (image), size);
  1226| }
  1227| static inline void *
  1228| m_image_alloc0 (MonoImage *image, guint size)
  1229| {
  1230| 	return mono_mem_manager_alloc0 (m_image_get_mem_manager (image), size);
  1231| }
  1232| static inline MonoMemoryManager*
  1233| m_class_get_mem_manager (MonoClass *klass)
  1234| {
  1235| 	if (m_class_get_class_kind (klass) == MONO_CLASS_GINST)
  1236| 		return mono_class_get_generic_class (klass)->owner;
  1237| 	if (m_class_get_rank (klass))
  1238| 		return m_class_get_mem_manager (m_class_get_element_class (klass));
  1239| 	MonoAssemblyLoadContext *alc = mono_image_get_alc (m_class_get_image (klass));
  1240| 	if (alc)
  1241| 		return alc->memory_manager;
  1242| 	else
  1243| 		/* Dynamic assemblies */
  1244| 		return mono_mem_manager_get_ambient ();
  1245| }
  1246| static inline void *
  1247| m_class_alloc (MonoClass *klass, guint size)
  1248| {
  1249| 	return mono_mem_manager_alloc (m_class_get_mem_manager (klass), size);
  1250| }
  1251| static inline void *
  1252| m_class_alloc0 (MonoClass *klass, guint size)
  1253| {
  1254| 	return mono_mem_manager_alloc0 (m_class_get_mem_manager (klass), size);
  1255| }
  1256| static inline MonoMemoryManager*
  1257| m_method_get_mem_manager (MonoMethod *method)
  1258| {
  1259| 	if (method->is_inflated)
  1260| 		return ((MonoMethodInflated*)method)->owner;
  1261| 	else if (method->wrapper_type && ((MonoMethodWrapper*)method)->mem_manager)
  1262| 		return ((MonoMethodWrapper*)method)->mem_manager;
  1263| 	else
  1264| 		return m_class_get_mem_manager (method->klass);
  1265| }
  1266| static inline void *
  1267| m_method_alloc (MonoMethod *method, guint size)
  1268| {
  1269| 	return mono_mem_manager_alloc (m_method_get_mem_manager (method), size);
  1270| }
  1271| static inline void *
  1272| m_method_alloc0 (MonoMethod *method, guint size)
  1273| {
  1274| 	return mono_mem_manager_alloc0 (m_method_get_mem_manager (method), size);
  1275| }
  1276| #include "jit-icall-reg.h"
  1277| /*Now that everything has been defined, let's include the inline functions */
  1278| #include <mono/metadata/class-inlines.h>
  1279| #endif /* __MONO_METADATA_CLASS_INTERNALS_H__ */


# ====================================================================
# FILE: src/mono/mono/mini/method-to-ir.c
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-11711 ---
     1| /**
     2|  * \file
     3|  * Convert CIL to the JIT internal representation
     4|  *
     5|  * Author:
     6|  *   Paolo Molaro (lupus@ximian.com)
     7|  *   Dietmar Maurer (dietmar@ximian.com)
     8|  *
     9|  * (C) 2002 Ximian, Inc.
    10|  * Copyright 2003-2010 Novell, Inc (http://www.novell.com)
    11|  * Copyright 2011 Xamarin, Inc (http://www.xamarin.com)
    12|  * Licensed under the MIT license. See LICENSE file in the project root for full license information.
    13|  */
    14| #include <config.h>
    15| #include <glib.h>
    16| #include <mono/utils/mono-compiler.h>
    17| #include "mini.h"
    18| #ifndef DISABLE_JIT
    19| #include <signal.h>
    20| #ifdef HAVE_UNISTD_H
    21| #include <unistd.h>
    22| #endif
    23| #include <math.h>
    24| #include <string.h>
    25| #include <ctype.h>
    26| #ifdef HAVE_SYS_TIME_H
    27| #include <sys/time.h>
    28| #endif
    29| #ifdef HAVE_ALLOCA_H
    30| #include <alloca.h>
    31| #endif
    32| #include <mono/utils/memcheck.h>
    33| #include <mono/metadata/abi-details.h>
    34| #include <mono/metadata/assembly.h>
    35| #include <mono/metadata/assembly-internals.h>
    36| #include <mono/metadata/attrdefs.h>
    37| #include <mono/metadata/loader.h>
    38| #include <mono/metadata/tabledefs.h>
    39| #include <mono/metadata/class.h>
    40| #include <mono/metadata/class-abi-details.h>
    41| #include <mono/metadata/object.h>
    42| #include <mono/metadata/exception.h>
    43| #include <mono/metadata/exception-internals.h>
    44| #include <mono/metadata/opcodes.h>
    45| #include <mono/metadata/mono-endian.h>
    46| #include <mono/metadata/tokentype.h>
    47| #include <mono/metadata/tabledefs.h>
    48| #include <mono/metadata/marshal.h>
    49| #include <mono/metadata/debug-helpers.h>
    50| #include <mono/metadata/debug-internals.h>
    51| #include <mono/metadata/gc-internals.h>
    52| #include <mono/metadata/threads-types.h>
    53| #include <mono/metadata/profiler-private.h>
    54| #include <mono/metadata/profiler.h>
    55| #include <mono/metadata/monitor.h>
    56| #include <mono/utils/mono-memory-model.h>
    57| #include <mono/utils/mono-error-internals.h>
    58| #include <mono/metadata/mono-basic-block.h>
    59| #include <mono/metadata/reflection-internals.h>
    60| #include <mono/utils/mono-threads-coop.h>
    61| #include <mono/utils/mono-utils-debug.h>
    62| #include <mono/utils/mono-logger-internals.h>
    63| #include <mono/metadata/verify-internals.h>
    64| #include <mono/metadata/icall-decl.h>
    65| #include "mono/metadata/icall-signatures.h"
    66| #include "trace.h"
    67| #include "ir-emit.h"
    68| #include "jit-icalls.h"
    69| #include <mono/jit/jit.h>
    70| #include "seq-points.h"
    71| #include "aot-compiler.h"
    72| #include "mini-llvm.h"
    73| #include "mini-runtime.h"
    74| #include "llvmonly-runtime.h"
    75| #include "mono/utils/mono-tls-inline.h"
    76| MONO_DISABLE_WARNING(4127) /* conditional expression is constant */
    77| #define BRANCH_COST 10
    78| #define CALL_COST 10
    79| /* Used for the JIT */
    80| #define INLINE_LENGTH_LIMIT 20
    81| /*
    82|  * The aot and jit inline limits should be different,
    83|  * since aot sees the whole program so we can let opt inline methods for us,
    84|  * while the jit only sees one method, so we have to inline things ourselves.
    85|  */
    86| /* Used by LLVM AOT */
    87| #define LLVM_AOT_INLINE_LENGTH_LIMIT 30
    88| /* Used to LLVM JIT */
    89| #define LLVM_JIT_INLINE_LENGTH_LIMIT 100
    90| static const gboolean debug_tailcall = FALSE;               // logging
    91| static const gboolean debug_tailcall_try_all = FALSE;       // consider any call followed by ret
    92| gboolean
    93| mono_tailcall_print_enabled (void)
    94| {
    95| 	return debug_tailcall || MONO_TRACE_IS_TRACED (G_LOG_LEVEL_DEBUG, MONO_TRACE_TAILCALL);
    96| }
    97| void
    98| mono_tailcall_print (const char *format, ...)
    99| {
   100| 	if (!mono_tailcall_print_enabled ())
   101| 		return;
   102| 	va_list args;
   103| 	va_start (args, format);
   104| 	g_printv (format, args);
   105| 	va_end (args);
   106| }
   107| /* These have 'cfg' as an implicit argument */
   108| #define INLINE_FAILURE(msg) do {									\
   109| 	if ((cfg->method != cfg->current_method) && (cfg->current_method->wrapper_type == MONO_WRAPPER_NONE)) { \
   110| 		inline_failure (cfg, msg);										\
   111| 		goto exception_exit;											\
   112| 	} \
   113| 	} while (0)
   114| #define CHECK_CFG_EXCEPTION do {\
   115| 		if (cfg->exception_type != MONO_EXCEPTION_NONE)	\
   116| 			goto exception_exit;						\
   117| 	} while (0)
   118| #define FIELD_ACCESS_FAILURE(method, field) do {					\
   119| 		field_access_failure ((cfg), (method), (field));			\
   120| 		goto exception_exit;	\
   121| 	} while (0)
   122| #define GENERIC_SHARING_FAILURE(opcode) do {		\
   123| 		if (cfg->gshared) {									\
   124| 			gshared_failure (cfg, opcode, __FILE__, __LINE__);	\
   125| 			goto exception_exit;	\
   126| 		}			\
   127| 	} while (0)
   128| #define GSHAREDVT_FAILURE(opcode) do {		\
   129| 	if (cfg->gsharedvt) {												\
   130| 		gsharedvt_failure (cfg, opcode, __FILE__, __LINE__);			\
   131| 		goto exception_exit;											\
   132| 	}																	\
   133| 	} while (0)
   134| #define OUT_OF_MEMORY_FAILURE do {	\
   135| 		mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);		\
   136| 		mono_error_set_out_of_memory (cfg->error, "");					\
   137| 		goto exception_exit;	\
   138| 	} while (0)
   139| #define DISABLE_AOT(cfg) do { \
   140| 		if ((cfg)->verbose_level >= 2)						  \
   141| 			printf ("AOT disabled: %s:%d\n", __FILE__, __LINE__);	\
   142| 		(cfg)->disable_aot = TRUE;							  \
   143| 	} while (0)
   144| #define LOAD_ERROR do { \
   145| 		break_on_unverified ();								\
   146| 		mono_cfg_set_exception (cfg, MONO_EXCEPTION_TYPE_LOAD); \
   147| 		goto exception_exit;									\
   148| 	} while (0)
   149| #define TYPE_LOAD_ERROR(klass) do { \
   150| 		cfg->exception_ptr = klass; \
   151| 		LOAD_ERROR;					\
   152| 	} while (0)
   153| #define CHECK_CFG_ERROR do {\
   154| 		if (!is_ok (cfg->error)) { \
   155| 			mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);	\
   156| 			goto mono_error_exit; \
   157| 		} \
   158| 	} while (0)
   159| int mono_op_to_op_imm (int opcode);
   160| int mono_op_to_op_imm_noemul (int opcode);
   161| static int inline_method (MonoCompile *cfg, MonoMethod *cmethod, MonoMethodSignature *fsig, MonoInst **sp,
   162| 						  guchar *ip, guint real_offset, gboolean inline_always, gboolean *is_empty);
   163| static MonoInst*
   164| convert_value (MonoCompile *cfg, MonoType *type, MonoInst *ins);
   165| /*
   166|  * Instruction metadata
   167|  */
   168| #ifdef MINI_OP
   169| #undef MINI_OP
   170| #endif
   171| #ifdef MINI_OP3
   172| #undef MINI_OP3
   173| #endif
   174| #define MINI_OP(a,b,dest,src1,src2) dest, src1, src2, ' ',
   175| #define MINI_OP3(a,b,dest,src1,src2,src3) dest, src1, src2, src3,
   176| #define NONE ' '
   177| #define IREG 'i'
   178| #define FREG 'f'
   179| #define VREG 'v'
   180| #define XREG 'x'
   181| #if SIZEOF_REGISTER == 8 && SIZEOF_REGISTER == TARGET_SIZEOF_VOID_P
   182| #define LREG IREG
   183| #else
   184| #define LREG 'l'
   185| #endif
   186| /* keep in sync with the enum in mini.h */
   187| const char
   188| mini_ins_info[] = {
   189| #include "mini-ops.h"
   190| };
   191| #undef MINI_OP
   192| #undef MINI_OP3
   193| #define MINI_OP(a,b,dest,src1,src2) ((src2) != NONE ? 2 : ((src1) != NONE ? 1 : 0)),
   194| #define MINI_OP3(a,b,dest,src1,src2,src3) ((src3) != NONE ? 3 : ((src2) != NONE ? 2 : ((src1) != NONE ? 1 : 0))),
   195| /*
   196|  * This should contain the index of the last sreg + 1. This is not the same
   197|  * as the number of sregs for opcodes like IA64_CMP_EQ_IMM.
   198|  */
   199| const gint8 mini_ins_sreg_counts[] = {
   200| #include "mini-ops.h"
   201| };
   202| #undef MINI_OP
   203| #undef MINI_OP3
   204| guint32
   205| mono_alloc_ireg (MonoCompile *cfg)
   206| {
   207| 	return alloc_ireg (cfg);
   208| }
   209| guint32
   210| mono_alloc_lreg (MonoCompile *cfg)
   211| {
   212| 	return alloc_lreg (cfg);
   213| }
   214| guint32
   215| mono_alloc_freg (MonoCompile *cfg)
   216| {
   217| 	return alloc_freg (cfg);
   218| }
   219| guint32
   220| mono_alloc_preg (MonoCompile *cfg)
   221| {
   222| 	return alloc_preg (cfg);
   223| }
   224| guint32
   225| mono_alloc_dreg (MonoCompile *cfg, MonoStackType stack_type)
   226| {
   227| 	return alloc_dreg (cfg, stack_type);
   228| }
   229| /*
   230|  * mono_alloc_ireg_ref:
   231|  *
   232|  *   Allocate an IREG, and mark it as holding a GC ref.
   233|  */
   234| guint32
   235| mono_alloc_ireg_ref (MonoCompile *cfg)
   236| {
   237| 	return alloc_ireg_ref (cfg);
   238| }
   239| /*
   240|  * mono_alloc_ireg_mp:
   241|  *
   242|  *   Allocate an IREG, and mark it as holding a managed pointer.
   243|  */
   244| guint32
   245| mono_alloc_ireg_mp (MonoCompile *cfg)
   246| {
   247| 	return alloc_ireg_mp (cfg);
   248| }
   249| /*
   250|  * mono_alloc_ireg_copy:
   251|  *
   252|  *   Allocate an IREG with the same GC type as VREG.
   253|  */
   254| guint32
   255| mono_alloc_ireg_copy (MonoCompile *cfg, guint32 vreg)
   256| {
   257| 	if (vreg_is_ref (cfg, vreg))
   258| 		return alloc_ireg_ref (cfg);
   259| 	else if (vreg_is_mp (cfg, vreg))
   260| 		return alloc_ireg_mp (cfg);
   261| 	else
   262| 		return alloc_ireg (cfg);
   263| }
   264| guint
   265| mono_type_to_regmove (MonoCompile *cfg, MonoType *type)
   266| {
   267| 	if (m_type_is_byref (type))
   268| 		return OP_MOVE;
   269| 	type = mini_get_underlying_type (type);
   270| handle_enum:
   271| 	switch (type->type) {
   272| 	case MONO_TYPE_I1:
   273| 	case MONO_TYPE_U1:
   274| 		return OP_MOVE;
   275| 	case MONO_TYPE_I2:
   276| 	case MONO_TYPE_U2:
   277| 		return OP_MOVE;
   278| 	case MONO_TYPE_I4:
   279| 	case MONO_TYPE_U4:
   280| 		return OP_MOVE;
   281| 	case MONO_TYPE_I:
   282| 	case MONO_TYPE_U:
   283| 	case MONO_TYPE_PTR:
   284| 	case MONO_TYPE_FNPTR:
   285| 		return OP_MOVE;
   286| 	case MONO_TYPE_CLASS:
   287| 	case MONO_TYPE_STRING:
   288| 	case MONO_TYPE_OBJECT:
   289| 	case MONO_TYPE_SZARRAY:
   290| 	case MONO_TYPE_ARRAY:
   291| 		return OP_MOVE;
   292| 	case MONO_TYPE_I8:
   293| 	case MONO_TYPE_U8:
   294| #if SIZEOF_REGISTER == 8
   295| 		return OP_MOVE;
   296| #else
   297| 		return OP_LMOVE;
   298| #endif
   299| 	case MONO_TYPE_R4:
   300| 		return cfg->r4fp ? OP_RMOVE : OP_FMOVE;
   301| 	case MONO_TYPE_R8:
   302| 		return OP_FMOVE;
   303| 	case MONO_TYPE_VALUETYPE:
   304| 		if (m_class_is_enumtype (type->data.klass)) {
   305| 			type = mono_class_enum_basetype_internal (type->data.klass);
   306| 			goto handle_enum;
   307| 		}
   308| 		if (mini_class_is_simd (cfg, mono_class_from_mono_type_internal (type)))
   309| 			return OP_XMOVE;
   310| 		return OP_VMOVE;
   311| 	case MONO_TYPE_TYPEDBYREF:
   312| 		return OP_VMOVE;
   313| 	case MONO_TYPE_GENERICINST:
   314| 		if (mini_class_is_simd (cfg, mono_class_from_mono_type_internal (type)))
   315| 			return OP_XMOVE;
   316| 		type = m_class_get_byval_arg (type->data.generic_class->container_class);
   317| 		goto handle_enum;
   318| 	case MONO_TYPE_VAR:
   319| 	case MONO_TYPE_MVAR:
   320| 		g_assert (cfg->gshared);
   321| 		if (mini_type_var_is_vt (type))
   322| 			return OP_VMOVE;
   323| 		else
   324| 			return mono_type_to_regmove (cfg, mini_get_underlying_type (type));
   325| 	default:
   326| 		g_error ("unknown type 0x%02x in type_to_regstore", type->type);
   327| 	}
   328| 	return -1;
   329| }
   330| void
   331| mono_print_bb (MonoBasicBlock *bb, const char *msg)
   332| {
   333| 	int i;
   334| 	MonoInst *tree;
   335| 	GString *str = g_string_new ("");
   336| 	g_string_append_printf (str, "%s %d: [IN: ", msg, bb->block_num);
   337| 	for (i = 0; i < bb->in_count; ++i)
   338| 		g_string_append_printf (str, " BB%d(%d)", bb->in_bb [i]->block_num, bb->in_bb [i]->dfn);
   339| 	g_string_append_printf (str, ", OUT: ");
   340| 	for (i = 0; i < bb->out_count; ++i)
   341| 		g_string_append_printf (str, " BB%d(%d)", bb->out_bb [i]->block_num, bb->out_bb [i]->dfn);
   342| 	g_string_append_printf (str, " ]\n");
   343| 	g_print ("%s", str->str);
   344| 	g_string_free (str, TRUE);
   345| 	for (tree = bb->code; tree; tree = tree->next)
   346| 		mono_print_ins_index (-1, tree);
   347| }
   348| static MONO_NEVER_INLINE gboolean
   349| break_on_unverified (void)
   350| {
   351| 	if (mini_debug_options.break_on_unverified) {
   352| 		G_BREAKPOINT ();
   353| 		return TRUE;
   354| 	}
   355| 	return FALSE;
   356| }
   357| static void
   358| clear_cfg_error (MonoCompile *cfg)
   359| {
   360| 	mono_error_cleanup (cfg->error);
   361| 	error_init (cfg->error);
   362| }
   363| static MONO_NEVER_INLINE void
   364| field_access_failure (MonoCompile *cfg, MonoMethod *method, MonoClassField *field)
   365| {
   366| 	char *method_fname = mono_method_full_name (method, TRUE);
   367| 	char *field_fname = mono_field_full_name (field);
   368| 	mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
   369| 	mono_error_set_generic_error (cfg->error, "System", "FieldAccessException", "Field `%s' is inaccessible from method `%s'\n", field_fname, method_fname);
   370| 	g_free (method_fname);
   371| 	g_free (field_fname);
   372| }
   373| static MONO_NEVER_INLINE void
   374| inline_failure (MonoCompile *cfg, const char *msg)
   375| {
   376| 	if (cfg->verbose_level >= 2)
   377| 		printf ("inline failed: %s\n", msg);
   378| 	mono_cfg_set_exception (cfg, MONO_EXCEPTION_INLINE_FAILED);
   379| }
   380| static MONO_NEVER_INLINE void
   381| gshared_failure (MonoCompile *cfg, int opcode, const char *file, int line)
   382| {
   383| 	if (cfg->verbose_level > 2)
   384| 		printf ("sharing failed for method %s.%s.%s/%d opcode %s line %d\n", m_class_get_name_space (cfg->current_method->klass), m_class_get_name (cfg->current_method->klass), cfg->current_method->name, cfg->current_method->signature->param_count, mono_opcode_name (opcode), line);
   385| 	mono_cfg_set_exception (cfg, MONO_EXCEPTION_GENERIC_SHARING_FAILED);
   386| }
   387| static MONO_NEVER_INLINE void
   388| gsharedvt_failure (MonoCompile *cfg, int opcode, const char *file, int line)
   389| {
   390| 	cfg->exception_message = g_strdup_printf ("gsharedvt failed for method %s.%s.%s/%d opcode %s %s:%d", m_class_get_name_space (cfg->current_method->klass), m_class_get_name (cfg->current_method->klass), cfg->current_method->name, cfg->current_method->signature->param_count, mono_opcode_name ((opcode)), file, line);
   391| 	if (cfg->verbose_level >= 2)
   392| 		printf ("%s\n", cfg->exception_message);
   393| 	mono_cfg_set_exception (cfg, MONO_EXCEPTION_GENERIC_SHARING_FAILED);
   394| }
   395| void
   396| mini_set_inline_failure (MonoCompile *cfg, const char *msg)
   397| {
   398| 	if (cfg->verbose_level >= 2)
   399| 		printf ("inline failed: %s\n", msg);
   400| 	mono_cfg_set_exception (cfg, MONO_EXCEPTION_INLINE_FAILED);
   401| }
   402| /*
   403|  * When using gsharedvt, some instatiations might be verifiable, and some might be not. i.e.
   404|  * foo<T> (int i) { ldarg.0; box T; }
   405|  */
   406| #define UNVERIFIED do { \
   407| 	if (cfg->gsharedvt) { \
   408| 		if (cfg->verbose_level > 2) \
   409| 			printf ("gsharedvt method failed to verify, falling back to instantiation.\n"); \
   410| 		mono_cfg_set_exception (cfg, MONO_EXCEPTION_GENERIC_SHARING_FAILED); \
   411| 		goto exception_exit; \
   412| 	} \
   413| 	break_on_unverified (); \
   414| 	goto unverified; \
   415| } while (0)
   416| #define GET_BBLOCK(cfg,tblock,ip) do { \
   417| 		if ((ip) >= end || (ip) < header->code) { UNVERIFIED; }	\
   418| 		(tblock) = cfg->cil_offset_to_bb [(ip) - cfg->cil_start]; \
   419| 		if (!(tblock)) { \
   420| 			NEW_BBLOCK (cfg, (tblock)); \
   421| 			(tblock)->cil_code = (ip); \
   422| 			ADD_BBLOCK (cfg, (tblock)); \
   423| 		} \
   424| 	} while (0)
   425| /* Emit conversions so both operands of a binary opcode are of the same type */
   426| static void
   427| add_widen_op (MonoCompile *cfg, MonoInst *ins, MonoInst **arg1_ref, MonoInst **arg2_ref)
   428| {
   429| 	MonoInst *arg1 = *arg1_ref;
   430| 	MonoInst *arg2 = *arg2_ref;
   431| 	if (cfg->r4fp &&
   432| 		((arg1->type == STACK_R4 && arg2->type == STACK_R8) ||
   433| 		 (arg1->type == STACK_R8 && arg2->type == STACK_R4))) {
   434| 		MonoInst *conv;
   435| 		/* Mixing r4/r8 is allowed by the spec */
   436| 		if (arg1->type == STACK_R4) {
   437| 			int dreg = alloc_freg (cfg);
   438| 			EMIT_NEW_UNALU (cfg, conv, OP_RCONV_TO_R8, dreg, arg1->dreg);
   439| 			conv->type = STACK_R8;
   440| 			ins->sreg1 = dreg;
   441| 			*arg1_ref = conv;
   442| 		}
   443| 		if (arg2->type == STACK_R4) {
   444| 			int dreg = alloc_freg (cfg);
   445| 			EMIT_NEW_UNALU (cfg, conv, OP_RCONV_TO_R8, dreg, arg2->dreg);
   446| 			conv->type = STACK_R8;
   447| 			ins->sreg2 = dreg;
   448| 			*arg2_ref = conv;
   449| 		}
   450| 	}
   451| #if SIZEOF_REGISTER == 8
   452| 	/* FIXME: Need to add many more cases */
   453| 	if ((arg1)->type == STACK_PTR && (arg2)->type == STACK_I4) {
   454| 		MonoInst *widen;
   455| 		int dr = alloc_preg (cfg);
   456| 		EMIT_NEW_UNALU (cfg, widen, OP_SEXT_I4, dr, (arg2)->dreg);
   457| 		(ins)->sreg2 = widen->dreg;
   458| 	}
   459| #endif
   460| }
   461| #define ADD_UNOP(op) do { \
   462| 		MONO_INST_NEW (cfg, ins, (op)); \
   463| 		sp--; \
   464| 		ins->sreg1 = sp [0]->dreg; \
   465| 		type_from_op (cfg, ins, sp [0], NULL); \
   466| 		CHECK_TYPE (ins); \
   467| 		(ins)->dreg = alloc_dreg ((cfg), (MonoStackType)(ins)->type); \
   468| 		MONO_ADD_INS ((cfg)->cbb, (ins)); \
   469| 		*sp++ = mono_decompose_opcode (cfg, ins); \
   470| 	} while (0)
   471| #define ADD_BINCOND(next_block) do { \
   472| 		MonoInst *cmp; \
   473| 		sp -= 2; \
   474| 		MONO_INST_NEW(cfg, cmp, OP_COMPARE); \
   475| 		cmp->sreg1 = sp [0]->dreg; \
   476| 		cmp->sreg2 = sp [1]->dreg; \
   477| 		add_widen_op (cfg, cmp, &sp [0], &sp [1]); \
   478| 		type_from_op (cfg, cmp, sp [0], sp [1]); \
   479| 		CHECK_TYPE (cmp); \
   480| 		type_from_op (cfg, ins, sp [0], sp [1]); \
   481| 		ins->inst_many_bb = (MonoBasicBlock **)mono_mempool_alloc (cfg->mempool, sizeof(gpointer)*2); \
   482| 		GET_BBLOCK (cfg, tblock, target); \
   483| 		link_bblock (cfg, cfg->cbb, tblock); \
   484| 		ins->inst_true_bb = tblock; \
   485| 		MONO_DISABLE_WARNING(4127) \
   486| 		if ((next_block)) { \
   487| 			link_bblock (cfg, cfg->cbb, (next_block)); \
   488| 			ins->inst_false_bb = (next_block); \
   489| 			start_new_bblock = 1; \
   490| 		} else { \
   491| 			GET_BBLOCK (cfg, tblock, next_ip); \
   492| 			link_bblock (cfg, cfg->cbb, tblock); \
   493| 			ins->inst_false_bb = tblock; \
   494| 			start_new_bblock = 2; \
   495| 		} \
   496| 		MONO_RESTORE_WARNING \
   497| 		if (sp != stack_start) { \
   498| 			handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start)); \
   499| 			CHECK_UNVERIFIABLE (cfg); \
   500| 		} \
   501| 		MONO_ADD_INS (cfg->cbb, cmp); \
   502| 		MONO_ADD_INS (cfg->cbb, ins); \
   503| 	} while (0)
   504| /* *
   505|  * link_bblock: Links two basic blocks
   506|  *
   507|  * links two basic blocks in the control flow graph, the 'from'
   508|  * argument is the starting block and the 'to' argument is the block
   509|  * the control flow ends to after 'from'.
   510|  */
   511| static void
   512| link_bblock (MonoCompile *cfg, MonoBasicBlock *from, MonoBasicBlock* to)
   513| {
   514| 	MonoBasicBlock **newa;
   515| 	int i, found;
   516| #if 0
   517| 	if (from->cil_code) {
   518| 		if (to->cil_code)
   519| 			printf ("edge from IL%04x to IL_%04x\n", from->cil_code - cfg->cil_code, to->cil_code - cfg->cil_code);
   520| 		else
   521| 			printf ("edge from IL%04x to exit\n", from->cil_code - cfg->cil_code);
   522| 	} else {
   523| 		if (to->cil_code)
   524| 			printf ("edge from entry to IL_%04x\n", to->cil_code - cfg->cil_code);
   525| 		else
   526| 			printf ("edge from entry to exit\n");
   527| 	}
   528| #endif
   529| 	found = FALSE;
   530| 	for (i = 0; i < from->out_count; ++i) {
   531| 		if (to == from->out_bb [i]) {
   532| 			found = TRUE;
   533| 			break;
   534| 		}
   535| 	}
   536| 	if (!found) {
   537| 		newa = (MonoBasicBlock **)mono_mempool_alloc (cfg->mempool, sizeof (gpointer) * (from->out_count + 1));
   538| 		for (i = 0; i < from->out_count; ++i) {
   539| 			newa [i] = from->out_bb [i];
   540| 		}
   541| 		newa [i] = to;
   542| 		from->out_count++;
   543| 		from->out_bb = newa;
   544| 	}
   545| 	found = FALSE;
   546| 	for (i = 0; i < to->in_count; ++i) {
   547| 		if (from == to->in_bb [i]) {
   548| 			found = TRUE;
   549| 			break;
   550| 		}
   551| 	}
   552| 	if (!found) {
   553| 		newa = (MonoBasicBlock **)mono_mempool_alloc (cfg->mempool, sizeof (gpointer) * (to->in_count + 1));
   554| 		for (i = 0; i < to->in_count; ++i) {
   555| 			newa [i] = to->in_bb [i];
   556| 		}
   557| 		newa [i] = from;
   558| 		to->in_count++;
   559| 		to->in_bb = newa;
   560| 	}
   561| }
   562| void
   563| mono_link_bblock (MonoCompile *cfg, MonoBasicBlock *from, MonoBasicBlock* to)
   564| {
   565| 	link_bblock (cfg, from, to);
   566| }
   567| static void
   568| mono_create_spvar_for_region (MonoCompile *cfg, int region);
   569| static void
   570| mark_bb_in_region (MonoCompile *cfg, guint region, uint32_t start, uint32_t end)
   571| {
   572| 	MonoBasicBlock *bb = cfg->cil_offset_to_bb [start];
   573| 	g_assert (bb);
   574| 	if (cfg->verbose_level > 1)
   575| 		g_print ("FIRST BB for %d is BB_%d\n", start, bb->block_num);
   576| 	for (; bb && bb->real_offset < end; bb = bb->next_bb) {
   577| 		if (bb->region == -1) {
   578| 			bb->region = region;
   579| 			continue;
   580| 		}
   581| 		if ((bb->region & (0xf << 4)) != MONO_REGION_TRY) {
   582| 			continue;
   583| 		}
   584| 		if ((region & (0xf << 4)) != MONO_REGION_TRY) {
   585| 			bb->region = region;
   586| 		}
   587| 	}
   588| 	if (cfg->spvars)
   589| 		mono_create_spvar_for_region (cfg, region);
   590| }
   591| static void
   592| compute_bb_regions (MonoCompile *cfg)
   593| {
   594| 	MonoMethodHeader *header = cfg->header;
   595| 	for (MonoBasicBlock *bb = cfg->bb_entry; bb; bb = bb->next_bb)
   596| 		bb->region = -1;
   597| 	for (guint i = 0; i < header->num_clauses; ++i) {
   598| 		MonoExceptionClause *clause = &header->clauses [i];
   599| 		if (clause->flags == MONO_EXCEPTION_CLAUSE_FILTER)
   600| 			mark_bb_in_region (cfg, ((i + 1) << 8) | MONO_REGION_FILTER | clause->flags, clause->data.filter_offset, clause->handler_offset);
   601| 		guint handler_region;
   602| 		if (clause->flags == MONO_EXCEPTION_CLAUSE_FINALLY)
   603| 			handler_region = ((i + 1) << 8) | MONO_REGION_FINALLY | clause->flags;
   604| 		else if (clause->flags == MONO_EXCEPTION_CLAUSE_FAULT)
   605| 			handler_region = ((i + 1) << 8) | MONO_REGION_FAULT | clause->flags;
   606| 		else
   607| 			handler_region = ((i + 1) << 8) | MONO_REGION_CATCH | clause->flags;
   608| 		mark_bb_in_region (cfg, handler_region, clause->handler_offset, clause->handler_offset + clause->handler_len);
   609| 		mark_bb_in_region (cfg, ((i + 1) << 8) | clause->flags, clause->try_offset, clause->try_offset + clause->try_len);
   610| 	}
   611| 	if (cfg->verbose_level > 2) {
   612| 		for (MonoBasicBlock *bb = cfg->bb_entry; bb; bb = bb->next_bb)
   613| 			g_print ("REGION BB%d IL_%04x ID_%08X\n", bb->block_num, bb->real_offset, bb->region);
   614| 	}
   615| }
   616| static gboolean
   617| ip_in_finally_clause (MonoCompile *cfg, int offset)
   618| {
   619| 	MonoMethodHeader *header = cfg->header;
   620| 	MonoExceptionClause *clause;
   621| 	for (guint i = 0; i < header->num_clauses; ++i) {
   622| 		clause = &header->clauses [i];
   623| 		if (clause->flags != MONO_EXCEPTION_CLAUSE_FINALLY && clause->flags != MONO_EXCEPTION_CLAUSE_FAULT)
   624| 			continue;
   625| 		if (MONO_OFFSET_IN_HANDLER (clause, GINT_TO_UINT32(offset)))
   626| 			return TRUE;
   627| 	}
   628| 	return FALSE;
   629| }
   630| /* Find clauses between ip and target, from inner to outer */
   631| static GList*
   632| mono_find_leave_clauses (MonoCompile *cfg, guchar *ip, guchar *target)
   633| {
   634| 	MonoMethodHeader *header = cfg->header;
   635| 	MonoExceptionClause *clause;
   636| 	GList *res = NULL;
   637| 	for (guint i = 0; i < header->num_clauses; ++i) {
   638| 		clause = &header->clauses [i];
   639| 		if (MONO_OFFSET_IN_CLAUSE (clause, GPTRDIFF_TO_UINT32(ip - header->code)) &&
   640| 		    (!MONO_OFFSET_IN_CLAUSE (clause, GPTRDIFF_TO_UINT32(target - header->code)))) {
   641| 			MonoLeaveClause *leave = mono_mempool_alloc0 (cfg->mempool, sizeof (MonoLeaveClause));
   642| 			leave->index = i;
   643| 			leave->clause = clause;
   644| 			res = g_list_append_mempool (cfg->mempool, res, leave);
   645| 		}
   646| 	}
   647| 	return res;
   648| }
   649| static void
   650| mono_create_spvar_for_region (MonoCompile *cfg, int region)
   651| {
   652| 	MonoInst *var;
   653| 	var = (MonoInst *)g_hash_table_lookup (cfg->spvars, GINT_TO_POINTER (region));
   654| 	if (var)
   655| 		return;
   656| 	var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
   657| 	/* prevent it from being register allocated */
   658| 	var->flags |= MONO_INST_VOLATILE;
   659| 	g_hash_table_insert (cfg->spvars, GINT_TO_POINTER (region), var);
   660| }
   661| MonoInst *
   662| mono_find_exvar_for_offset (MonoCompile *cfg, int offset)
   663| {
   664| 	return (MonoInst *)g_hash_table_lookup (cfg->exvars, GINT_TO_POINTER (offset));
   665| }
   666| static MonoInst*
   667| mono_create_exvar_for_offset (MonoCompile *cfg, int offset)
   668| {
   669| 	MonoInst *var;
   670| 	var = (MonoInst *)g_hash_table_lookup (cfg->exvars, GINT_TO_POINTER (offset));
   671| 	if (var)
   672| 		return var;
   673| 	var = mono_compile_create_var (cfg, mono_get_object_type (), OP_LOCAL);
   674| 	/* prevent it from being register allocated */
   675| 	var->flags |= MONO_INST_VOLATILE;
   676| 	g_hash_table_insert (cfg->exvars, GINT_TO_POINTER (offset), var);
   677| 	return var;
   678| }
   679| /*
   680|  * Returns the type used in the eval stack when @type is loaded.
   681|  * FIXME: return a MonoType/MonoClass for the byref and VALUETYPE cases.
   682|  */
   683| void
   684| mini_type_to_eval_stack_type (MonoCompile *cfg, MonoType *type, MonoInst *inst)
   685| {
   686| 	MonoClass *klass;
   687| 	type = mini_get_underlying_type (type);
   688| 	inst->klass = klass = mono_class_from_mono_type_internal (type);
   689| 	if (m_type_is_byref (type)) {
   690| 		inst->type = STACK_MP;
   691| 		return;
   692| 	}
   693| handle_enum:
   694| 	switch (type->type) {
   695| 	case MONO_TYPE_VOID:
   696| 		inst->type = STACK_INV;
   697| 		return;
   698| 	case MONO_TYPE_I1:
   699| 	case MONO_TYPE_U1:
   700| 	case MONO_TYPE_I2:
   701| 	case MONO_TYPE_U2:
   702| 	case MONO_TYPE_I4:
   703| 	case MONO_TYPE_U4:
   704| 		inst->type = STACK_I4;
   705| 		return;
   706| 	case MONO_TYPE_I:
   707| 	case MONO_TYPE_U:
   708| 	case MONO_TYPE_PTR:
   709| 	case MONO_TYPE_FNPTR:
   710| 		inst->type = STACK_PTR;
   711| 		return;
   712| 	case MONO_TYPE_CLASS:
   713| 	case MONO_TYPE_STRING:
   714| 	case MONO_TYPE_OBJECT:
   715| 	case MONO_TYPE_SZARRAY:
   716| 	case MONO_TYPE_ARRAY:
   717| 		inst->type = STACK_OBJ;
   718| 		return;
   719| 	case MONO_TYPE_I8:
   720| 	case MONO_TYPE_U8:
   721| 		inst->type = STACK_I8;
   722| 		return;
   723| 	case MONO_TYPE_R4:
   724| 		inst->type = GINT_TO_UINT8 (cfg->r4_stack_type);
   725| 		break;
   726| 	case MONO_TYPE_R8:
   727| 		inst->type = STACK_R8;
   728| 		return;
   729| 	case MONO_TYPE_VALUETYPE:
   730| 		if (m_class_is_enumtype (type->data.klass)) {
   731| 			type = mono_class_enum_basetype_internal (type->data.klass);
   732| 			goto handle_enum;
   733| 		} else {
   734| 			inst->klass = klass;
   735| 			inst->type = STACK_VTYPE;
   736| 			return;
   737| 		}
   738| 	case MONO_TYPE_TYPEDBYREF:
   739| 		inst->klass = mono_defaults.typed_reference_class;
   740| 		inst->type = STACK_VTYPE;
   741| 		return;
   742| 	case MONO_TYPE_GENERICINST:
   743| 		type = m_class_get_byval_arg (type->data.generic_class->container_class);
   744| 		goto handle_enum;
   745| 	case MONO_TYPE_VAR:
   746| 	case MONO_TYPE_MVAR:
   747| 		g_assert (cfg->gshared);
   748| 		if (mini_is_gsharedvt_type (type)) {
   749| 			g_assert (cfg->gsharedvt);
   750| 			inst->type = STACK_VTYPE;
   751| 		} else {
   752| 			mini_type_to_eval_stack_type (cfg, mini_get_underlying_type (type), inst);
   753| 		}
   754| 		return;
   755| 	default:
   756| 		g_error ("unknown type 0x%02x in eval stack type", type->type);
   757| 	}
   758| }
   759| /*
   760|  * The following tables are used to quickly validate the IL code in type_from_op ().
   761|  */
   762| #define IF_P8(v) (SIZEOF_VOID_P == 8 ? v : STACK_INV)
   763| #define IF_P8_I8 IF_P8(STACK_I8)
   764| #define IF_P8_PTR IF_P8(STACK_PTR)
   765| static const char
   766| bin_num_table [STACK_MAX] [STACK_MAX] = {
   767| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   768| 	{STACK_INV, STACK_I4,  IF_P8_I8,  STACK_PTR, STACK_INV, STACK_MP,  STACK_INV, STACK_INV},
   769| 	{STACK_INV, IF_P8_I8,  STACK_I8,  IF_P8_PTR, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   770| 	{STACK_INV, STACK_PTR, IF_P8_PTR, STACK_PTR, STACK_INV, STACK_MP,  STACK_INV, STACK_INV},
   771| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_R8,  STACK_INV, STACK_INV, STACK_INV, STACK_R8},
   772| 	{STACK_INV, STACK_MP,  STACK_INV, STACK_MP,  STACK_INV, STACK_PTR, STACK_INV, STACK_INV},
   773| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   774| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   775| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_R8, STACK_INV, STACK_INV, STACK_INV, STACK_R4}
   776| };
   777| static const char
   778| neg_table [] = {
   779| 	STACK_INV, STACK_I4, STACK_I8, STACK_PTR, STACK_R8, STACK_INV, STACK_INV, STACK_INV, STACK_R4
   780| };
   781| /* reduce the size of this table */
   782| static const char
   783| bin_int_table [STACK_MAX] [STACK_MAX] = {
   784| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   785| 	{STACK_INV, STACK_I4,  IF_P8_I8,  STACK_PTR, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   786| 	{STACK_INV, IF_P8_I8,  STACK_I8,  IF_P8_PTR, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   787| 	{STACK_INV, STACK_PTR, IF_P8_PTR, STACK_PTR, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   788| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   789| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   790| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   791| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV}
   792| };
   793| #define P1 (SIZEOF_VOID_P == 8)
   794| static const char
   795| bin_comp_table [STACK_MAX] [STACK_MAX] = {
   796| /*	Inv i  L  p  F  &  O  vt r4 */
   797| 	{0},
   798| 	{0, 1, 0, 1, 0, 0, 0, 0}, /* i, int32 */
   799| 	{0, 0, 1,P1, 0, 0, 0, 0}, /* L, int64 */
   800| 	{0, 1,P1, 1, 0, 2, 4, 0}, /* p, ptr */
   801| 	{0, 0, 0, 0, 1, 0, 0, 0, 1}, /* F, R8 */
   802| 	{0, 0, 0, 2, 0, 1, 0, 0}, /* &, managed pointer */
   803| 	{0, 0, 0, 4, 0, 0, 3, 0}, /* O, reference */
   804| 	{0, 0, 0, 0, 0, 0, 0, 0}, /* vt value type */
   805| 	{0, 0, 0, 0, 1, 0, 0, 0, 1}, /* r, r4 */
   806| };
   807| #undef P1
   808| /* reduce the size of this table */
   809| static const char
   810| shift_table [STACK_MAX] [STACK_MAX] = {
   811| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   812| 	{STACK_INV, STACK_I4,  STACK_INV, STACK_I4,  STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   813| 	{STACK_INV, STACK_I8,  STACK_INV, STACK_I8,  STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   814| 	{STACK_INV, STACK_PTR, STACK_INV, STACK_PTR, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   815| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   816| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   817| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
   818| 	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV}
   819| };
   820| /*
   821|  * Tables to map from the non-specific opcode to the matching
   822|  * type-specific opcode.
   823|  */
   824| /* handles from CEE_ADD to CEE_SHR_UN (CEE_REM_UN for floats) */
   825| static const guint16
   826| binops_op_map [STACK_MAX] = {
   827| 	0, OP_IADD-CEE_ADD, OP_LADD-CEE_ADD, OP_PADD-CEE_ADD, OP_FADD-CEE_ADD, OP_PADD-CEE_ADD, 0, 0, OP_RADD-CEE_ADD
   828| };
   829| /* handles from CEE_NEG to CEE_CONV_U8 */
   830| static const guint16
   831| unops_op_map [STACK_MAX] = {
   832| 	0, OP_INEG-CEE_NEG, OP_LNEG-CEE_NEG, OP_PNEG-CEE_NEG, OP_FNEG-CEE_NEG, OP_PNEG-CEE_NEG, 0, 0, OP_RNEG-CEE_NEG
   833| };
   834| /* handles from CEE_CONV_U2 to CEE_SUB_OVF_UN */
   835| static const guint16
   836| ovfops_op_map [STACK_MAX] = {
   837| 	0, OP_ICONV_TO_U2-CEE_CONV_U2, OP_LCONV_TO_U2-CEE_CONV_U2, OP_PCONV_TO_U2-CEE_CONV_U2, OP_FCONV_TO_U2-CEE_CONV_U2, OP_PCONV_TO_U2-CEE_CONV_U2, OP_PCONV_TO_U2-CEE_CONV_U2, 0, OP_RCONV_TO_U2-CEE_CONV_U2
   838| };
   839| /* handles from CEE_CONV_OVF_I1_UN to CEE_CONV_OVF_U_UN */
   840| static const guint16
   841| ovf2ops_op_map [STACK_MAX] = {
   842| 	0, OP_ICONV_TO_OVF_I1_UN-CEE_CONV_OVF_I1_UN, OP_LCONV_TO_OVF_I1_UN-CEE_CONV_OVF_I1_UN, OP_PCONV_TO_OVF_I1_UN-CEE_CONV_OVF_I1_UN, OP_FCONV_TO_OVF_I1_UN-CEE_CONV_OVF_I1_UN, OP_PCONV_TO_OVF_I1_UN-CEE_CONV_OVF_I1_UN, 0, 0, OP_RCONV_TO_OVF_I1_UN-CEE_CONV_OVF_I1_UN
   843| };
   844| /* handles from CEE_CONV_OVF_I1 to CEE_CONV_OVF_U8 */
   845| static const guint16
   846| ovf3ops_op_map [STACK_MAX] = {
   847| 	0, OP_ICONV_TO_OVF_I1-CEE_CONV_OVF_I1, OP_LCONV_TO_OVF_I1-CEE_CONV_OVF_I1, OP_PCONV_TO_OVF_I1-CEE_CONV_OVF_I1, OP_FCONV_TO_OVF_I1-CEE_CONV_OVF_I1, OP_PCONV_TO_OVF_I1-CEE_CONV_OVF_I1, 0, 0, OP_RCONV_TO_OVF_I1-CEE_CONV_OVF_I1
   848| };
   849| /* handles from CEE_BEQ to CEE_BLT_UN */
   850| static const guint16
   851| beqops_op_map [STACK_MAX] = {
   852| 	0, OP_IBEQ-CEE_BEQ, OP_LBEQ-CEE_BEQ, OP_PBEQ-CEE_BEQ, OP_FBEQ-CEE_BEQ, OP_PBEQ-CEE_BEQ, OP_PBEQ-CEE_BEQ, 0, OP_FBEQ-CEE_BEQ
   853| };
   854| /* handles from CEE_CEQ to CEE_CLT_UN */
   855| static const guint16
   856| ceqops_op_map [STACK_MAX] = {
   857| 	0, OP_ICEQ-OP_CEQ, OP_LCEQ-OP_CEQ, OP_PCEQ-OP_CEQ, OP_FCEQ-OP_CEQ, OP_PCEQ-OP_CEQ, OP_PCEQ-OP_CEQ, 0, OP_RCEQ-OP_CEQ
   858| };
   859| /*
   860|  * Sets ins->type (the type on the eval stack) according to the
   861|  * type of the opcode and the arguments to it.
   862|  * Invalid IL code is marked by setting ins->type to the invalid value STACK_INV.
   863|  *
   864|  * FIXME: this function sets ins->type unconditionally in some cases, but
   865|  * it should set it to invalid for some types (a conv.x on an object)
   866|  */
   867| static void
   868| type_from_op (MonoCompile *cfg, MonoInst *ins, MonoInst *src1, MonoInst *src2)
   869| {
   870| 	switch (ins->opcode) {
   871| 	/* binops */
   872| 	case MONO_CEE_ADD:
   873| 	case MONO_CEE_SUB:
   874| 	case MONO_CEE_MUL:
   875| 	case MONO_CEE_DIV:
   876| 	case MONO_CEE_REM:
   877| 		/* FIXME: check unverifiable args for STACK_MP */
   878| 		ins->type = bin_num_table [src1->type] [src2->type];
   879| 		ins->opcode += binops_op_map [ins->type];
   880| 		break;
   881| 	case MONO_CEE_DIV_UN:
   882| 	case MONO_CEE_REM_UN:
   883| 	case MONO_CEE_AND:
   884| 	case MONO_CEE_OR:
   885| 	case MONO_CEE_XOR:
   886| 		ins->type = bin_int_table [src1->type] [src2->type];
   887| 		ins->opcode += binops_op_map [ins->type];
   888| 		break;
   889| 	case MONO_CEE_SHL:
   890| 	case MONO_CEE_SHR:
   891| 	case MONO_CEE_SHR_UN:
   892| 		ins->type = shift_table [src1->type] [src2->type];
   893| 		ins->opcode += binops_op_map [ins->type];
   894| 		break;
   895| 	case OP_COMPARE:
   896| 	case OP_LCOMPARE:
   897| 	case OP_ICOMPARE:
   898| 		ins->type = bin_comp_table [src1->type] [src2->type] ? STACK_I4: STACK_INV;
   899| 		if ((src1->type == STACK_I8) || ((TARGET_SIZEOF_VOID_P == 8) && ((src1->type == STACK_PTR) || (src1->type == STACK_OBJ) || (src1->type == STACK_MP))))
   900| 			ins->opcode = OP_LCOMPARE;
   901| 		else if (src1->type == STACK_R4)
   902| 			ins->opcode = OP_RCOMPARE;
   903| 		else if (src1->type == STACK_R8)
   904| 			ins->opcode = OP_FCOMPARE;
   905| 		else
   906| 			ins->opcode = OP_ICOMPARE;
   907| 		break;
   908| 	case OP_ICOMPARE_IMM:
   909| 		ins->type = bin_comp_table [src1->type] [src1->type] ? STACK_I4 : STACK_INV;
   910| 		if ((src1->type == STACK_I8) || ((TARGET_SIZEOF_VOID_P == 8) && ((src1->type == STACK_PTR) || (src1->type == STACK_OBJ) || (src1->type == STACK_MP))))
   911| 			ins->opcode = OP_LCOMPARE_IMM;
   912| 		break;
   913| 	case MONO_CEE_BEQ:
   914| 	case MONO_CEE_BGE:
   915| 	case MONO_CEE_BGT:
   916| 	case MONO_CEE_BLE:
   917| 	case MONO_CEE_BLT:
   918| 	case MONO_CEE_BNE_UN:
   919| 	case MONO_CEE_BGE_UN:
   920| 	case MONO_CEE_BGT_UN:
   921| 	case MONO_CEE_BLE_UN:
   922| 	case MONO_CEE_BLT_UN:
   923| 		ins->opcode += beqops_op_map [src1->type];
   924| 		break;
   925| 	case OP_CEQ:
   926| 		ins->type = bin_comp_table [src1->type] [src2->type] ? STACK_I4: STACK_INV;
   927| 		ins->opcode += ceqops_op_map [src1->type];
   928| 		break;
   929| 	case OP_CGT:
   930| 	case OP_CGT_UN:
   931| 	case OP_CLT:
   932| 	case OP_CLT_UN:
   933| 		ins->type = (bin_comp_table [src1->type] [src2->type] & 1) ? STACK_I4: STACK_INV;
   934| 		ins->opcode += ceqops_op_map [src1->type];
   935| 		break;
   936| 	/* unops */
   937| 	case MONO_CEE_NEG:
   938| 		ins->type = neg_table [src1->type];
   939| 		ins->opcode += unops_op_map [ins->type];
   940| 		break;
   941| 	case MONO_CEE_NOT:
   942| 		if (src1->type >= STACK_I4 && src1->type <= STACK_PTR)
   943| 			ins->type = src1->type;
   944| 		else
   945| 			ins->type = STACK_INV;
   946| 		ins->opcode += unops_op_map [ins->type];
   947| 		break;
   948| 	case MONO_CEE_CONV_I1:
   949| 	case MONO_CEE_CONV_I2:
   950| 	case MONO_CEE_CONV_I4:
   951| 	case MONO_CEE_CONV_U4:
   952| 		ins->type = STACK_I4;
   953| 		ins->opcode += unops_op_map [src1->type];
   954| 		break;
   955| 	case MONO_CEE_CONV_R_UN:
   956| 		ins->type = STACK_R8;
   957| 		switch (src1->type) {
   958| 		case STACK_I4:
   959| #if TARGET_SIZEOF_VOID_P == 4
   960| 		case STACK_PTR:
   961| #endif
   962| 			ins->opcode = OP_ICONV_TO_R_UN;
   963| 			break;
   964| 		case STACK_I8:
   965| #if TARGET_SIZEOF_VOID_P == 8
   966| 		case STACK_PTR:
   967| #endif
   968| 			ins->opcode = OP_LCONV_TO_R_UN;
   969| 			break;
   970| 		case STACK_R4:
   971| 			ins->opcode = OP_RCONV_TO_R8;
   972| 			break;
   973| 		case STACK_R8:
   974| 			ins->opcode = OP_FMOVE;
   975| 			break;
   976| 		}
   977| 		break;
   978| 	case MONO_CEE_CONV_OVF_I1:
   979| 	case MONO_CEE_CONV_OVF_U1:
   980| 	case MONO_CEE_CONV_OVF_I2:
   981| 	case MONO_CEE_CONV_OVF_U2:
   982| 	case MONO_CEE_CONV_OVF_I4:
   983| 	case MONO_CEE_CONV_OVF_U4:
   984| 		ins->type = STACK_I4;
   985| 		ins->opcode += ovf3ops_op_map [src1->type];
   986| 		break;
   987| 	case MONO_CEE_CONV_OVF_I_UN:
   988| 	case MONO_CEE_CONV_OVF_U_UN:
   989| 		ins->type = STACK_PTR;
   990| 		ins->opcode += ovf2ops_op_map [src1->type];
   991| 		break;
   992| 	case MONO_CEE_CONV_OVF_I1_UN:
   993| 	case MONO_CEE_CONV_OVF_I2_UN:
   994| 	case MONO_CEE_CONV_OVF_I4_UN:
   995| 	case MONO_CEE_CONV_OVF_U1_UN:
   996| 	case MONO_CEE_CONV_OVF_U2_UN:
   997| 	case MONO_CEE_CONV_OVF_U4_UN:
   998| 		ins->type = STACK_I4;
   999| 		ins->opcode += ovf2ops_op_map [src1->type];
  1000| 		break;
  1001| 	case MONO_CEE_CONV_U:
  1002| 		ins->type = STACK_PTR;
  1003| 		switch (src1->type) {
  1004| 		case STACK_I4:
  1005| 			ins->opcode = OP_ICONV_TO_U;
  1006| 			break;
  1007| 		case STACK_PTR:
  1008| 		case STACK_MP:
  1009| 		case STACK_OBJ:
  1010| #if TARGET_SIZEOF_VOID_P == 8
  1011| 			ins->opcode = OP_LCONV_TO_U;
  1012| #else
  1013| 			ins->opcode = OP_MOVE;
  1014| #endif
  1015| 			break;
  1016| 		case STACK_I8:
  1017| 			ins->opcode = OP_LCONV_TO_U;
  1018| 			break;
  1019| 		case STACK_R8:
  1020| 			if (TARGET_SIZEOF_VOID_P == 8)
  1021| 				ins->opcode = OP_FCONV_TO_U8;
  1022| 			else
  1023| 				ins->opcode = OP_FCONV_TO_U4;
  1024| 			break;
  1025| 		case STACK_R4:
  1026| 			if (TARGET_SIZEOF_VOID_P == 8)
  1027| 				ins->opcode = OP_RCONV_TO_U8;
  1028| 			else
  1029| 				ins->opcode = OP_RCONV_TO_U4;
  1030| 			break;
  1031| 		}
  1032| 		break;
  1033| 	case MONO_CEE_CONV_I8:
  1034| 	case MONO_CEE_CONV_U8:
  1035| 		ins->type = STACK_I8;
  1036| 		ins->opcode += unops_op_map [src1->type];
  1037| 		break;
  1038| 	case MONO_CEE_CONV_OVF_I8:
  1039| 	case MONO_CEE_CONV_OVF_U8:
  1040| 		ins->type = STACK_I8;
  1041| 		ins->opcode += ovf3ops_op_map [src1->type];
  1042| 		break;
  1043| 	case MONO_CEE_CONV_OVF_U8_UN:
  1044| 	case MONO_CEE_CONV_OVF_I8_UN:
  1045| 		ins->type = STACK_I8;
  1046| 		ins->opcode += ovf2ops_op_map [src1->type];
  1047| 		break;
  1048| 	case MONO_CEE_CONV_R4:
  1049| 		ins->type = GINT_TO_UINT8 (cfg->r4_stack_type);
  1050| 		ins->opcode += unops_op_map [src1->type];
  1051| 		break;
  1052| 	case MONO_CEE_CONV_R8:
  1053| 		ins->type = STACK_R8;
  1054| 		ins->opcode += unops_op_map [src1->type];
  1055| 		break;
  1056| 	case OP_CKFINITE:
  1057| 		ins->type = STACK_R8;
  1058| 		break;
  1059| 	case MONO_CEE_CONV_U2:
  1060| 	case MONO_CEE_CONV_U1:
  1061| 		ins->type = STACK_I4;
  1062| 		ins->opcode += ovfops_op_map [src1->type];
  1063| 		break;
  1064| 	case MONO_CEE_CONV_I:
  1065| 	case MONO_CEE_CONV_OVF_I:
  1066| 	case MONO_CEE_CONV_OVF_U:
  1067| 		ins->type = STACK_PTR;
  1068| 		ins->opcode += ovfops_op_map [src1->type];
  1069| 		switch (ins->opcode) {
  1070| 		case OP_FCONV_TO_I:
  1071| 			ins->opcode = TARGET_SIZEOF_VOID_P == 4 ? OP_FCONV_TO_I4 : OP_FCONV_TO_I8;
  1072| 			break;
  1073| 		case OP_RCONV_TO_I:
  1074| 			ins->opcode = TARGET_SIZEOF_VOID_P == 4 ? OP_RCONV_TO_I4 : OP_RCONV_TO_I8;
  1075| 			break;
  1076| 		default:
  1077| 			break;
  1078| 		}
  1079| 		break;
  1080| 	case MONO_CEE_ADD_OVF:
  1081| 	case MONO_CEE_ADD_OVF_UN:
  1082| 	case MONO_CEE_MUL_OVF:
  1083| 	case MONO_CEE_MUL_OVF_UN:
  1084| 	case MONO_CEE_SUB_OVF:
  1085| 	case MONO_CEE_SUB_OVF_UN:
  1086| 		ins->type = bin_num_table [src1->type] [src2->type];
  1087| 		ins->opcode += ovfops_op_map [src1->type];
  1088| 		if (ins->type == STACK_R8)
  1089| 			ins->type = STACK_INV;
  1090| 		break;
  1091| 	case OP_LOAD_MEMBASE:
  1092| 		ins->type = STACK_PTR;
  1093| 		break;
  1094| 	case OP_LOADI1_MEMBASE:
  1095| 	case OP_LOADU1_MEMBASE:
  1096| 	case OP_LOADI2_MEMBASE:
  1097| 	case OP_LOADU2_MEMBASE:
  1098| 	case OP_LOADI4_MEMBASE:
  1099| 	case OP_LOADU4_MEMBASE:
  1100| 		ins->type = STACK_PTR;
  1101| 		break;
  1102| 	case OP_LOADI8_MEMBASE:
  1103| 		ins->type = STACK_I8;
  1104| 		break;
  1105| 	case OP_LOADR4_MEMBASE:
  1106| 		ins->type = GINT_TO_UINT8 (cfg->r4_stack_type);
  1107| 		break;
  1108| 	case OP_LOADR8_MEMBASE:
  1109| 		ins->type = STACK_R8;
  1110| 		break;
  1111| 	default:
  1112| 		g_error ("opcode 0x%04x not handled in type from op", ins->opcode);
  1113| 		break;
  1114| 	}
  1115| 	if (ins->type == STACK_MP) {
  1116| 		if (src1->type == STACK_MP)
  1117| 			ins->klass = src1->klass;
  1118| 		else
  1119| 			ins->klass = mono_defaults.object_class;
  1120| 	}
  1121| }
  1122| void
  1123| mini_type_from_op (MonoCompile *cfg, MonoInst *ins, MonoInst *src1, MonoInst *src2)
  1124| {
  1125| 	type_from_op (cfg, ins, src1, src2);
  1126| }
  1127| static MonoClass*
  1128| ldind_to_type (int op)
  1129| {
  1130| 	switch (op) {
  1131| 	case MONO_CEE_LDIND_I1: return mono_defaults.sbyte_class;
  1132| 	case MONO_CEE_LDIND_U1: return mono_defaults.byte_class;
  1133| 	case MONO_CEE_LDIND_I2: return mono_defaults.int16_class;
  1134| 	case MONO_CEE_LDIND_U2: return mono_defaults.uint16_class;
  1135| 	case MONO_CEE_LDIND_I4: return mono_defaults.int32_class;
  1136| 	case MONO_CEE_LDIND_U4: return mono_defaults.uint32_class;
  1137| 	case MONO_CEE_LDIND_I8: return mono_defaults.int64_class;
  1138| 	case MONO_CEE_LDIND_I: return mono_defaults.int_class;
  1139| 	case MONO_CEE_LDIND_R4: return mono_defaults.single_class;
  1140| 	case MONO_CEE_LDIND_R8: return mono_defaults.double_class;
  1141| 	case MONO_CEE_LDIND_REF:return mono_defaults.object_class; //FIXME we should try to return a more specific type
  1142| 	default: g_error ("Unknown ldind type %d", op);
  1143| 	}
  1144| }
  1145| static MonoClass*
  1146| stind_to_type (int op)
  1147| {
  1148| 	switch (op) {
  1149| 	case MONO_CEE_STIND_I1: return mono_defaults.sbyte_class;
  1150| 	case MONO_CEE_STIND_I2: return mono_defaults.int16_class;
  1151| 	case MONO_CEE_STIND_I4: return mono_defaults.int32_class;
  1152| 	case MONO_CEE_STIND_I8: return mono_defaults.int64_class;
  1153| 	case MONO_CEE_STIND_I: return mono_defaults.int_class;
  1154| 	case MONO_CEE_STIND_R4: return mono_defaults.single_class;
  1155| 	case MONO_CEE_STIND_R8: return mono_defaults.double_class;
  1156| 	case MONO_CEE_STIND_REF: return mono_defaults.object_class;
  1157| 	default: g_error ("Unknown stind type %d", op);
  1158| 	}
  1159| }
  1160| #if 0
  1161| static const char
  1162| param_table [STACK_MAX] [STACK_MAX] = {
  1163| 	{0},
  1164| };
  1165| static int
  1166| check_values_to_signature (MonoInst *args, MonoType *this_ins, MonoMethodSignature *sig)
  1167| {
  1168| 	int i;
  1169| 	if (sig->hasthis) {
  1170| 		switch (args->type) {
  1171| 		case STACK_I4:
  1172| 		case STACK_I8:
  1173| 		case STACK_R8:
  1174| 		case STACK_VTYPE:
  1175| 		case STACK_INV:
  1176| 			return 0;
  1177| 		}
  1178| 		args++;
  1179| 	}
  1180| 	for (i = 0; i < sig->param_count; ++i) {
  1181| 		switch (args [i].type) {
  1182| 		case STACK_INV:
  1183| 			return 0;
  1184| 		case STACK_MP:
  1185| 			if (m_type_is_byref (!sig->params [i]))
  1186| 				return 0;
  1187| 			continue;
  1188| 		case STACK_OBJ:
  1189| 			if (m_type_is_byref (sig->params [i]))
  1190| 				return 0;
  1191| 			switch (m_type_is_byref (sig->params [i])) {
  1192| 			case MONO_TYPE_CLASS:
  1193| 			case MONO_TYPE_STRING:
  1194| 			case MONO_TYPE_OBJECT:
  1195| 			case MONO_TYPE_SZARRAY:
  1196| 			case MONO_TYPE_ARRAY:
  1197| 				break;
  1198| 			default:
  1199| 				return 0;
  1200| 			}
  1201| 			continue;
  1202| 		case STACK_R8:
  1203| 			if (m_type_is_byref (sig->params [i]))
  1204| 				return 0;
  1205| 			if (sig->params [i]->type != MONO_TYPE_R4 && sig->params [i]->type != MONO_TYPE_R8)
  1206| 				return 0;
  1207| 			continue;
  1208| 		case STACK_PTR:
  1209| 		case STACK_I4:
  1210| 		case STACK_I8:
  1211| 		case STACK_VTYPE:
  1212| 			break;
  1213| 		}
  1214| 		/*if (!param_table [args [i].type] [sig->params [i]->type])
  1215| 			return 0;*/
  1216| 	}
  1217| 	return 1;
  1218| }
  1219| #endif
  1220| /*
  1221|  * The got_var contains the address of the Global Offset Table when AOT
  1222|  * compiling.
  1223|  */
  1224| MonoInst *
  1225| mono_get_got_var (MonoCompile *cfg)
  1226| {
  1227| 	if (!cfg->compile_aot || !cfg->backend->need_got_var || cfg->llvm_only)
  1228| 		return NULL;
  1229| 	if (!cfg->got_var) {
  1230| 		cfg->got_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
  1231| 	}
  1232| 	return cfg->got_var;
  1233| }
  1234| static void
  1235| mono_create_rgctx_var (MonoCompile *cfg)
  1236| {
  1237| 	if (!cfg->rgctx_var) {
  1238| 		cfg->rgctx_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
  1239| 		/* force the var to be stack allocated */
  1240| 		if (!cfg->llvm_only)
  1241| 			cfg->rgctx_var->flags |= MONO_INST_VOLATILE;
  1242| 	}
  1243| }
  1244| static MonoInst *
  1245| mono_get_mrgctx_var (MonoCompile *cfg)
  1246| {
  1247| 	g_assert (cfg->gshared);
  1248| 	mono_create_rgctx_var (cfg);
  1249| 	return cfg->rgctx_var;
  1250| }
  1251| static MonoInst *
  1252| mono_get_vtable_var (MonoCompile *cfg)
  1253| {
  1254| 	g_assert (cfg->gshared);
  1255| 	/* The mrgctx and the vtable are stored in the same var */
  1256| 	mono_create_rgctx_var (cfg);
  1257| 	return cfg->rgctx_var;
  1258| }
  1259| static MonoType*
  1260| type_from_stack_type (MonoInst *ins) {
  1261| 	switch (ins->type) {
  1262| 	case STACK_I4: return mono_get_int32_type ();
  1263| 	case STACK_I8: return m_class_get_byval_arg (mono_defaults.int64_class);
  1264| 	case STACK_PTR: return mono_get_int_type ();
  1265| 	case STACK_R4: return m_class_get_byval_arg (mono_defaults.single_class);
  1266| 	case STACK_R8: return m_class_get_byval_arg (mono_defaults.double_class);
  1267| 	case STACK_MP:
  1268| 		return m_class_get_this_arg (ins->klass);
  1269| 	case STACK_OBJ: return mono_get_object_type ();
  1270| 	case STACK_VTYPE: return m_class_get_byval_arg (ins->klass);
  1271| 	default:
  1272| 		g_error ("stack type %d to monotype not handled\n", ins->type);
  1273| 	}
  1274| 	return NULL;
  1275| }
  1276| MonoStackType
  1277| mini_type_to_stack_type (MonoCompile *cfg, MonoType *t)
  1278| {
  1279| 	t = mini_type_get_underlying_type (t);
  1280| 	switch (t->type) {
  1281| 	case MONO_TYPE_I1:
  1282| 	case MONO_TYPE_U1:
  1283| 	case MONO_TYPE_I2:
  1284| 	case MONO_TYPE_U2:
  1285| 	case MONO_TYPE_I4:
  1286| 	case MONO_TYPE_U4:
  1287| 		return STACK_I4;
  1288| 	case MONO_TYPE_I:
  1289| 	case MONO_TYPE_U:
  1290| 	case MONO_TYPE_PTR:
  1291| 	case MONO_TYPE_FNPTR:
  1292| 		return STACK_PTR;
  1293| 	case MONO_TYPE_CLASS:
  1294| 	case MONO_TYPE_STRING:
  1295| 	case MONO_TYPE_OBJECT:
  1296| 	case MONO_TYPE_SZARRAY:
  1297| 	case MONO_TYPE_ARRAY:
  1298| 		return STACK_OBJ;
  1299| 	case MONO_TYPE_I8:
  1300| 	case MONO_TYPE_U8:
  1301| 		return STACK_I8;
  1302| 	case MONO_TYPE_R4:
  1303| 		return (MonoStackType)cfg->r4_stack_type;
  1304| 	case MONO_TYPE_R8:
  1305| 		return STACK_R8;
  1306| 	case MONO_TYPE_VALUETYPE:
  1307| 	case MONO_TYPE_TYPEDBYREF:
  1308| 		return STACK_VTYPE;
  1309| 	case MONO_TYPE_GENERICINST:
  1310| 		if (mono_type_generic_inst_is_valuetype (t))
  1311| 			return STACK_VTYPE;
  1312| 		else
  1313| 			return STACK_OBJ;
  1314| 		break;
  1315| 	default:
  1316| 		g_assert_not_reached ();
  1317| 	}
  1318| 	return (MonoStackType)-1;
  1319| }
  1320| static MonoClass*
  1321| array_access_to_klass (int opcode)
  1322| {
  1323| 	switch (opcode) {
  1324| 	case MONO_CEE_LDELEM_U1:
  1325| 		return mono_defaults.byte_class;
  1326| 	case MONO_CEE_LDELEM_U2:
  1327| 		return mono_defaults.uint16_class;
  1328| 	case MONO_CEE_LDELEM_I:
  1329| 	case MONO_CEE_STELEM_I:
  1330| 		return mono_defaults.int_class;
  1331| 	case MONO_CEE_LDELEM_I1:
  1332| 	case MONO_CEE_STELEM_I1:
  1333| 		return mono_defaults.sbyte_class;
  1334| 	case MONO_CEE_LDELEM_I2:
  1335| 	case MONO_CEE_STELEM_I2:
  1336| 		return mono_defaults.int16_class;
  1337| 	case MONO_CEE_LDELEM_I4:
  1338| 	case MONO_CEE_STELEM_I4:
  1339| 		return mono_defaults.int32_class;
  1340| 	case MONO_CEE_LDELEM_U4:
  1341| 		return mono_defaults.uint32_class;
  1342| 	case MONO_CEE_LDELEM_I8:
  1343| 	case MONO_CEE_STELEM_I8:
  1344| 		return mono_defaults.int64_class;
  1345| 	case MONO_CEE_LDELEM_R4:
  1346| 	case MONO_CEE_STELEM_R4:
  1347| 		return mono_defaults.single_class;
  1348| 	case MONO_CEE_LDELEM_R8:
  1349| 	case MONO_CEE_STELEM_R8:
  1350| 		return mono_defaults.double_class;
  1351| 	case MONO_CEE_LDELEM_REF:
  1352| 	case MONO_CEE_STELEM_REF:
  1353| 		return mono_defaults.object_class;
  1354| 	default:
  1355| 		g_assert_not_reached ();
  1356| 	}
  1357| 	return NULL;
  1358| }
  1359| /*
  1360|  * We try to share variables when possible
  1361|  */
  1362| static MonoInst *
  1363| mono_compile_get_interface_var (MonoCompile *cfg, int slot, MonoInst *ins)
  1364| {
  1365| 	MonoInst *res;
  1366| 	int pos, vnum;
  1367| 	MonoType *type;
  1368| 	type = type_from_stack_type (ins);
  1369| 	/* inlining can result in deeper stacks */
  1370| 	if (cfg->inline_depth || slot >= cfg->header->max_stack)
  1371| 		return mono_compile_create_var (cfg, type, OP_LOCAL);
  1372| 	pos = ins->type - 1 + slot * STACK_MAX;
  1373| 	switch (ins->type) {
  1374| 	case STACK_I4:
  1375| 	case STACK_I8:
  1376| 	case STACK_R8:
  1377| 	case STACK_PTR:
  1378| 	case STACK_MP:
  1379| 	case STACK_OBJ:
  1380| 		if ((vnum = cfg->intvars [pos]))
  1381| 			return cfg->varinfo [vnum];
  1382| 		res = mono_compile_create_var (cfg, type, OP_LOCAL);
  1383| 		cfg->intvars [pos] = GTMREG_TO_UINT16 (res->inst_c0);
  1384| 		break;
  1385| 	default:
  1386| 		res = mono_compile_create_var (cfg, type, OP_LOCAL);
  1387| 	}
  1388| 	return res;
  1389| }
  1390| static void
  1391| mono_save_token_info (MonoCompile *cfg, MonoImage *image, guint32 token, gpointer key)
  1392| {
  1393| 	/*
  1394| 	 * Don't use this if a generic_context is set, since that means AOT can't
  1395| 	 * look up the method using just the image+token.
  1396| 	 * table == 0 means this is a reference made from a wrapper.
  1397| 	 */
  1398| 	if (cfg->compile_aot && !cfg->generic_context && (mono_metadata_token_table (token) > 0)) {
  1399| 		MonoJumpInfoToken *jump_info_token = (MonoJumpInfoToken *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoJumpInfoToken));
  1400| 		jump_info_token->image = image;
  1401| 		jump_info_token->token = token;
  1402| 		g_hash_table_insert (cfg->token_info_hash, key, jump_info_token);
  1403| 	}
  1404| }
  1405| /*
  1406|  * This function is called to handle items that are left on the evaluation stack
  1407|  * at basic block boundaries. What happens is that we save the values to local variables
  1408|  * and we reload them later when first entering the target basic block (with the
  1409|  * handle_loaded_temps () function).
  1410|  * A single joint point will use the same variables (stored in the array bb->out_stack or
  1411|  * bb->in_stack, if the basic block is before or after the joint point).
  1412|  *
  1413|  * This function needs to be called _before_ emitting the last instruction of
  1414|  * the bb (i.e. before emitting a branch).
  1415|  * If the stack merge fails at a join point, cfg->unverifiable is set.
  1416|  */
  1417| static void
  1418| handle_stack_args (MonoCompile *cfg, MonoInst **sp, int count)
  1419| {
  1420| 	MonoBasicBlock *bb = cfg->cbb;
  1421| 	MonoBasicBlock *outb;
  1422| 	MonoInst *inst, **locals;
  1423| 	gboolean found;
  1424| 	if (!count)
  1425| 		return;
  1426| 	if (cfg->verbose_level > 3)
  1427| 		printf ("%d item(s) on exit from B%d\n", count, bb->block_num);
  1428| 	if (!bb->out_scount) {
  1429| 		bb->out_scount = GINT_TO_UINT16 (count);
  1430| 		found = FALSE;
  1431| 		for (gint16 i = 0; i < bb->out_count; ++i) {
  1432| 			outb = bb->out_bb [i];
  1433| 			/* exception handlers are linked, but they should not be considered for stack args */
  1434| 			if (outb->flags & BB_EXCEPTION_HANDLER)
  1435| 				continue;
  1436| 			if (outb->in_stack) {
  1437| 				found = TRUE;
  1438| 				bb->out_stack = outb->in_stack;
  1439| 				break;
  1440| 			}
  1441| 		}
  1442| 		if (!found) {
  1443| 			bb->out_stack = (MonoInst **)mono_mempool_alloc (cfg->mempool, sizeof (MonoInst*) * count);
  1444| 			for (int i = 0; i < count; ++i) {
  1445| 				/*
  1446| 				 * try to reuse temps already allocated for this purpouse, if they occupy the same
  1447| 				 * stack slot and if they are of the same type.
  1448| 				 * This won't cause conflicts since if 'local' is used to
  1449| 				 * store one of the values in the in_stack of a bblock, then
  1450| 				 * the same variable will be used for the same outgoing stack
  1451| 				 * slot as well.
  1452| 				 * This doesn't work when inlining methods, since the bblocks
  1453| 				 * in the inlined methods do not inherit their in_stack from
  1454| 				 * the bblock they are inlined to. See bug #58863 for an
  1455| 				 * example.
  1456| 				 */
  1457| 				bb->out_stack [i] = mono_compile_get_interface_var (cfg, i, sp [i]);
  1458| 			}
  1459| 		}
  1460| 	}
  1461| 	for (gint16 i = 0; i < bb->out_count; ++i) {
  1462| 		outb = bb->out_bb [i];
  1463| 		/* exception handlers are linked, but they should not be considered for stack args */
  1464| 		if (outb->flags & BB_EXCEPTION_HANDLER)
  1465| 			continue;
  1466| 		if (outb->in_scount) {
  1467| 			if (outb->in_scount != bb->out_scount) {
  1468| 				cfg->unverifiable = TRUE;
  1469| 				return;
  1470| 			}
  1471| 			continue; /* check they are the same locals */
  1472| 		}
  1473| 		outb->in_scount = GINT_TO_UINT16 (count);
  1474| 		outb->in_stack = bb->out_stack;
  1475| 	}
  1476| 	locals = bb->out_stack;
  1477| 	cfg->cbb = bb;
  1478| 	for (int i = 0; i < count; ++i) {
  1479| 		sp [i] = convert_value (cfg, locals [i]->inst_vtype, sp [i]);
  1480| 		EMIT_NEW_TEMPSTORE (cfg, inst, locals [i]->inst_c0, sp [i]);
  1481| 		inst->cil_code = sp [i]->cil_code;
  1482| 		sp [i] = locals [i];
  1483| 		if (cfg->verbose_level > 3)
  1484| 			printf ("storing %d to temp %d\n", i, (int)locals [i]->inst_c0);
  1485| 	}
  1486| 	/*
  1487| 	 * It is possible that the out bblocks already have in_stack assigned, and
  1488| 	 * the in_stacks differ. In this case, we will store to all the different
  1489| 	 * in_stacks.
  1490| 	 */
  1491| 	found = TRUE;
  1492| 	gint16 bindex = 0;
  1493| 	while (found) {
  1494| 		/* Find a bblock which has a different in_stack */
  1495| 		found = FALSE;
  1496| 		while (bindex < bb->out_count) {
  1497| 			outb = bb->out_bb [bindex];
  1498| 			/* exception handlers are linked, but they should not be considered for stack args */
  1499| 			if (outb->flags & BB_EXCEPTION_HANDLER) {
  1500| 				bindex++;
  1501| 				continue;
  1502| 			}
  1503| 			if (outb->in_stack != locals) {
  1504| 				for (int i = 0; i < count; ++i) {
  1505| 					sp [i] = convert_value (cfg, outb->in_stack [i]->inst_vtype, sp [i]);
  1506| 					EMIT_NEW_TEMPSTORE (cfg, inst, outb->in_stack [i]->inst_c0, sp [i]);
  1507| 					inst->cil_code = sp [i]->cil_code;
  1508| 					sp [i] = locals [i];
  1509| 					if (cfg->verbose_level > 3)
  1510| 						printf ("storing %d to temp %d\n", i, (int)outb->in_stack [i]->inst_c0);
  1511| 				}
  1512| 				locals = outb->in_stack;
  1513| 				found = TRUE;
  1514| 				break;
  1515| 			}
  1516| 			bindex ++;
  1517| 		}
  1518| 	}
  1519| }
  1520| MonoInst*
  1521| mini_emit_runtime_constant (MonoCompile *cfg, MonoJumpInfoType patch_type, gpointer data)
  1522| {
  1523| 	MonoInst *ins;
  1524| 	if (cfg->compile_aot) {
  1525| MONO_DISABLE_WARNING (4306) // 'type cast': conversion from 'MonoJumpInfoType' to 'MonoInst *' of greater size
  1526| 		EMIT_NEW_AOTCONST (cfg, ins, patch_type, data);
  1527| MONO_RESTORE_WARNING
  1528| 	} else {
  1529| 		MonoJumpInfo ji;
  1530| 		gpointer target;
  1531| 		ERROR_DECL (error);
  1532| 		ji.type = patch_type;
  1533| 		ji.data.target = data;
  1534| 		target = mono_resolve_patch_target_ext (cfg->mem_manager, NULL, NULL, &ji, FALSE, error);
  1535| 		mono_error_assert_ok (error);
  1536| 		EMIT_NEW_PCONST (cfg, ins, target);
  1537| 	}
  1538| 	return ins;
  1539| }
  1540| static MonoInst*
  1541| mono_create_fast_tls_getter (MonoCompile *cfg, MonoTlsKey key)
  1542| {
  1543| 	if (cfg->compile_aot)
  1544| 		return NULL;
  1545| 	int tls_offset = mono_tls_get_tls_offset (key);
  1546| 	if (tls_offset != -1 && mono_arch_have_fast_tls ()) {
  1547| 		MonoInst *ins;
  1548| 		MONO_INST_NEW (cfg, ins, OP_TLS_GET);
  1549| 		ins->dreg = mono_alloc_preg (cfg);
  1550| 		ins->inst_offset = tls_offset;
  1551| 		return ins;
  1552| 	}
  1553| 	return NULL;
  1554| }
  1555| static MonoInst*
  1556| mono_create_tls_get (MonoCompile *cfg, MonoTlsKey key)
  1557| {
  1558| 	MonoInst *fast_tls = NULL;
  1559| 	if (!mini_debug_options.use_fallback_tls)
  1560| 		fast_tls = mono_create_fast_tls_getter (cfg, key);
  1561| 	if (fast_tls) {
  1562| 		MONO_ADD_INS (cfg->cbb, fast_tls);
  1563| 		return fast_tls;
  1564| 	}
  1565| 	const MonoJitICallId jit_icall_id = mono_get_tls_key_to_jit_icall_id (key);
  1566| 	if (cfg->compile_aot && !cfg->llvm_only) {
  1567| 		MonoInst *addr;
  1568| 		/*
  1569| 		 * tls getters are critical pieces of code and we don't want to resolve them
  1570| 		 * through the standard plt/tramp mechanism since we might expose ourselves
  1571| 		 * to crashes and infinite recursions.
  1572| 		 * Therefore the NOCALL part of MONO_PATCH_INFO_JIT_ICALL_ADDR_NOCALL, FALSE in is_plt_patch.
  1573| 		 */
  1574| 		EMIT_NEW_AOTCONST (cfg, addr, MONO_PATCH_INFO_JIT_ICALL_ADDR_NOCALL, GUINT_TO_POINTER (jit_icall_id));
  1575| 		return mini_emit_calli (cfg, mono_icall_sig_ptr, NULL, addr, NULL, NULL);
  1576| 	} else {
  1577| 		return mono_emit_jit_icall_id (cfg, jit_icall_id, NULL);
  1578| 	}
  1579| }
  1580| /*
  1581|  * emit_push_lmf:
  1582|  *
  1583|  *   Emit IR to push the current LMF onto the LMF stack.
  1584|  */
  1585| static void
  1586| emit_push_lmf (MonoCompile *cfg)
  1587| {
  1588| 	/*
  1589| 	 * Emit IR to push the LMF:
  1590| 	 * lmf_addr = <lmf_addr from tls>
  1591| 	 * lmf->lmf_addr = lmf_addr
  1592| 	 * lmf->prev_lmf = *lmf_addr
  1593| 	 * *lmf_addr = lmf
  1594| 	 */
  1595| 	MonoInst *ins, *lmf_ins;
  1596| 	if (!cfg->lmf_ir)
  1597| 		return;
  1598| 	int lmf_reg, prev_lmf_reg;
  1599| 	/*
  1600| 	 * Store lmf_addr in a variable, so it can be allocated to a global register.
  1601| 	 */
  1602| 	if (!cfg->lmf_addr_var)
  1603| 		cfg->lmf_addr_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
  1604| 	if (!cfg->lmf_var) {
  1605| 		MonoInst *lmf_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
  1606| 		lmf_var->flags |= MONO_INST_VOLATILE;
  1607| 		lmf_var->flags |= MONO_INST_LMF;
  1608| 		cfg->lmf_var = lmf_var;
  1609| 	}
  1610| 	lmf_ins = mono_create_tls_get (cfg, TLS_KEY_LMF_ADDR);
  1611| 	g_assert (lmf_ins);
  1612| 	lmf_ins->dreg = cfg->lmf_addr_var->dreg;
  1613| 	EMIT_NEW_VARLOADA (cfg, ins, cfg->lmf_var, NULL);
  1614| 	lmf_reg = ins->dreg;
  1615| 	prev_lmf_reg = alloc_preg (cfg);
  1616| 	/* Save previous_lmf */
  1617| 	EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, prev_lmf_reg, cfg->lmf_addr_var->dreg, 0);
  1618| 	if (cfg->deopt)
  1619| 		/* Mark this as an LMFExt */
  1620| 		EMIT_NEW_BIALU_IMM (cfg, ins, OP_POR_IMM, prev_lmf_reg, prev_lmf_reg, 2);
  1621| 	EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, lmf_reg, MONO_STRUCT_OFFSET (MonoLMF, previous_lmf), prev_lmf_reg);
  1622| 	/* Set new lmf */
  1623| 	EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, cfg->lmf_addr_var->dreg, 0, lmf_reg);
  1624| }
  1625| /*
  1626|  * emit_pop_lmf:
  1627|  *
  1628|  *   Emit IR to pop the current LMF from the LMF stack.
  1629|  */
  1630| static void
  1631| emit_pop_lmf (MonoCompile *cfg)
  1632| {
  1633| 	int lmf_reg, lmf_addr_reg;
  1634| 	MonoInst *ins;
  1635| 	if (!cfg->lmf_ir)
  1636| 		return;
  1637|  	EMIT_NEW_VARLOADA (cfg, ins, cfg->lmf_var, NULL);
  1638|  	lmf_reg = ins->dreg;
  1639| 	int prev_lmf_reg;
  1640| 	/*
  1641| 	 * Emit IR to pop the LMF:
  1642| 	 * *(lmf->lmf_addr) = lmf->prev_lmf
  1643| 	 */
  1644| 	/* This could be called before emit_push_lmf () */
  1645| 	if (!cfg->lmf_addr_var)
  1646| 		cfg->lmf_addr_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
  1647| 	lmf_addr_reg = cfg->lmf_addr_var->dreg;
  1648| 	prev_lmf_reg = alloc_preg (cfg);
  1649| 	EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, prev_lmf_reg, lmf_reg, MONO_STRUCT_OFFSET (MonoLMF, previous_lmf));
  1650| 	if (cfg->deopt)
  1651| 		/* Clear out the bit set by push_lmf () to mark this as LMFExt */
  1652| 		EMIT_NEW_BIALU_IMM (cfg, ins, OP_PXOR_IMM, prev_lmf_reg, prev_lmf_reg, 2);
  1653| 	EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, lmf_addr_reg, 0, prev_lmf_reg);
  1654| }
  1655| /*
  1656|  * target_type_is_incompatible:
  1657|  * @cfg: MonoCompile context
  1658|  *
  1659|  * Check that the item @arg on the evaluation stack can be stored
  1660|  * in the target type (can be a local, or field, etc).
  1661|  * The cfg arg can be used to check if we need verification or just
  1662|  * validity checks.
  1663|  *
  1664|  * Returns: non-0 value if arg can't be stored on a target.
  1665|  */
  1666| static int
  1667| target_type_is_incompatible (MonoCompile *cfg, MonoType *target, MonoInst *arg)
  1668| {
  1669| 	MonoType *simple_type;
  1670| 	MonoClass *klass;
  1671| 	if (m_type_is_byref (target)) {
  1672| 		/* FIXME: check that the pointed to types match */
  1673| 		if (arg->type == STACK_MP) {
  1674| 			/* This is needed to handle gshared types + ldaddr. We lower the types so we can handle enums and other typedef-like types. */
  1675| 			MonoClass *target_class_lowered = mono_class_from_mono_type_internal (mini_get_underlying_type (m_class_get_byval_arg (mono_class_from_mono_type_internal (target))));
  1676| 			MonoClass *source_class_lowered = mono_class_from_mono_type_internal (mini_get_underlying_type (m_class_get_byval_arg (arg->klass)));
  1677| 			/* if the target is native int& or X* or same type */
  1678| 			if (target->type == MONO_TYPE_I || target->type == MONO_TYPE_PTR || target_class_lowered == source_class_lowered)
  1679| 				return 0;
  1680| 			/* Both are primitive type byrefs and the source points to a larger type that the destination */
  1681| 			if (MONO_TYPE_IS_PRIMITIVE_SCALAR (m_class_get_byval_arg (target_class_lowered)) && MONO_TYPE_IS_PRIMITIVE_SCALAR (m_class_get_byval_arg (source_class_lowered)) &&
  1682| 				mono_class_instance_size (target_class_lowered) <= mono_class_instance_size (source_class_lowered))
  1683| 				return 0;
  1684| 			return 1;
  1685| 		}
  1686| 		if (arg->type == STACK_PTR)
  1687| 			return 0;
  1688| 		return 1;
  1689| 	}
  1690| 	simple_type = mini_get_underlying_type (target);
  1691| 	switch (simple_type->type) {
  1692| 	case MONO_TYPE_VOID:
  1693| 		return 1;
  1694| 	case MONO_TYPE_I1:
  1695| 	case MONO_TYPE_U1:
  1696| 	case MONO_TYPE_I2:
  1697| 	case MONO_TYPE_U2:
  1698| 	case MONO_TYPE_I4:
  1699| 	case MONO_TYPE_U4:
  1700| 		if (arg->type != STACK_I4 && arg->type != STACK_PTR)
  1701| 			return 1;
  1702| 		return 0;
  1703| 	case MONO_TYPE_PTR:
  1704| 		/* STACK_MP is needed when setting pinned locals */
  1705| 		if (arg->type != STACK_I4 && arg->type != STACK_PTR && arg->type != STACK_MP)
  1706| #if SIZEOF_VOID_P == 8
  1707| 			if (arg->type != STACK_I8)
  1708| #endif
  1709| 				return 1;
  1710| 		return 0;
  1711| 	case MONO_TYPE_I:
  1712| 	case MONO_TYPE_U:
  1713| 	case MONO_TYPE_FNPTR:
  1714| 		/*
  1715| 		 * Some opcodes like ldloca returns 'transient pointers' which can be stored in
  1716| 		 * in native int. (#688008).
  1717| 		 */
  1718| 		if (arg->type != STACK_I4 && arg->type != STACK_PTR && arg->type != STACK_MP)
  1719| 			return 1;
  1720| 		return 0;
  1721| 	case MONO_TYPE_CLASS:
  1722| 	case MONO_TYPE_STRING:
  1723| 	case MONO_TYPE_OBJECT:
  1724| 	case MONO_TYPE_SZARRAY:
  1725| 	case MONO_TYPE_ARRAY:
  1726| 		if (arg->type != STACK_OBJ)
  1727| 			return 1;
  1728| 		/* FIXME: check type compatibility */
  1729| 		return 0;
  1730| 	case MONO_TYPE_I8:
  1731| 	case MONO_TYPE_U8:
  1732| 		if (arg->type != STACK_I8)
  1733| #if SIZEOF_VOID_P == 8
  1734| 			if (arg->type != STACK_PTR)
  1735| #endif
  1736| 				return 1;
  1737| 		return 0;
  1738| 	case MONO_TYPE_R4:
  1739| 		if (arg->type != cfg->r4_stack_type)
  1740| 			return 1;
  1741| 		return 0;
  1742| 	case MONO_TYPE_R8:
  1743| 		if (arg->type != STACK_R8)
  1744| 			return 1;
  1745| 		return 0;
  1746| 	case MONO_TYPE_VALUETYPE:
  1747| 		if (arg->type != STACK_VTYPE)
  1748| 			return 1;
  1749| 		klass = mono_class_from_mono_type_internal (simple_type);
  1750| 		if (klass != arg->klass)
  1751| 			return 1;
  1752| 		return 0;
  1753| 	case MONO_TYPE_TYPEDBYREF:
  1754| 		if (arg->type != STACK_VTYPE)
  1755| 			return 1;
  1756| 		klass = mono_class_from_mono_type_internal (simple_type);
  1757| 		if (klass != arg->klass)
  1758| 			return 1;
  1759| 		return 0;
  1760| 	case MONO_TYPE_GENERICINST:
  1761| 		if (mono_type_generic_inst_is_valuetype (simple_type)) {
  1762| 			MonoClass *target_class;
  1763| 			if (arg->type != STACK_VTYPE)
  1764| 				return 1;
  1765| 			klass = mono_class_from_mono_type_internal (simple_type);
  1766| 			target_class = mono_class_from_mono_type_internal (target);
  1767| 			/* The second cases is needed when doing partial sharing */
  1768| 			if (klass != arg->klass && target_class != arg->klass && target_class != mono_class_from_mono_type_internal (mini_get_underlying_type (m_class_get_byval_arg (arg->klass))))
  1769| 				return 1;
  1770| 			return 0;
  1771| 		} else {
  1772| 			if (arg->type != STACK_OBJ)
  1773| 				return 1;
  1774| 			/* FIXME: check type compatibility */
  1775| 			return 0;
  1776| 		}
  1777| 	case MONO_TYPE_VAR:
  1778| 	case MONO_TYPE_MVAR:
  1779| 		g_assert (cfg->gshared);
  1780| 		if (mini_type_var_is_vt (simple_type)) {
  1781| 			if (arg->type != STACK_VTYPE)
  1782| 				return 1;
  1783| 		} else {
  1784| 			if (arg->type != STACK_OBJ)
  1785| 				return 1;
  1786| 		}
  1787| 		return 0;
  1788| 	default:
  1789| 		g_error ("unknown type 0x%02x in target_type_is_incompatible", simple_type->type);
  1790| 	}
  1791| 	return 1;
  1792| }
  1793| /*
  1794|  * convert_value:
  1795|  *
  1796|  *   Emit some implicit conversions which are not part of the .net spec, but are allowed by MS.NET.
  1797|  */
  1798| static MonoInst*
  1799| convert_value (MonoCompile *cfg, MonoType *type, MonoInst *ins)
  1800| {
  1801| 	if (!cfg->r4fp)
  1802| 		return ins;
  1803| 	type = mini_get_underlying_type (type);
  1804| 	switch (type->type) {
  1805| 	case MONO_TYPE_R4:
  1806| 		if (ins->type == STACK_R8) {
  1807| 			int dreg = alloc_freg (cfg);
  1808| 			MonoInst *conv;
  1809| 			EMIT_NEW_UNALU (cfg, conv, OP_FCONV_TO_R4, dreg, ins->dreg);
  1810| 			conv->type = STACK_R4;
  1811| 			return conv;
  1812| 		}
  1813| 		break;
  1814| 	case MONO_TYPE_R8:
  1815| 		if (ins->type == STACK_R4) {
  1816| 			int dreg = alloc_freg (cfg);
  1817| 			MonoInst *conv;
  1818| 			EMIT_NEW_UNALU (cfg, conv, OP_RCONV_TO_R8, dreg, ins->dreg);
  1819| 			conv->type = STACK_R8;
  1820| 			return conv;
  1821| 		}
  1822| 		break;
  1823| 	default:
  1824| 		break;
  1825| 	}
  1826| 	return ins;
  1827| }
  1828| /*
  1829|  * Prepare arguments for passing to a function call.
  1830|  * Return a non-zero value if the arguments can't be passed to the given
  1831|  * signature.
  1832|  * The type checks are not yet complete and some conversions may need
  1833|  * casts on 32 or 64 bit architectures.
  1834|  *
  1835|  * FIXME: implement this using target_type_is_incompatible ()
  1836|  */
  1837| static gboolean
  1838| check_call_signature (MonoCompile *cfg, MonoMethodSignature *sig, MonoInst **args)
  1839| {
  1840| 	MonoType *simple_type;
  1841| 	int i;
  1842| 	if (sig->hasthis) {
  1843| 		if (args [0]->type != STACK_OBJ && args [0]->type != STACK_MP && args [0]->type != STACK_PTR)
  1844| 			return TRUE;
  1845| 		args++;
  1846| 	}
  1847| 	for (i = 0; i < sig->param_count; ++i) {
  1848| 		if (m_type_is_byref (sig->params [i])) {
  1849| 			if (args [i]->type != STACK_MP && args [i]->type != STACK_PTR)
  1850| 				return TRUE;
  1851| 			continue;
  1852| 		}
  1853| 		simple_type = mini_get_underlying_type (sig->params [i]);
  1854| handle_enum:
  1855| 		switch (simple_type->type) {
  1856| 		case MONO_TYPE_VOID:
  1857| 			return TRUE;
  1858| 		case MONO_TYPE_I1:
  1859| 		case MONO_TYPE_U1:
  1860| 		case MONO_TYPE_I2:
  1861| 		case MONO_TYPE_U2:
  1862| 		case MONO_TYPE_I4:
  1863| 		case MONO_TYPE_U4:
  1864| 			if (args [i]->type != STACK_I4 && args [i]->type != STACK_PTR)
  1865| 				return TRUE;
  1866| 			continue;
  1867| 		case MONO_TYPE_I:
  1868| 		case MONO_TYPE_U:
  1869| 			if (args [i]->type != STACK_I4 && args [i]->type != STACK_PTR && args [i]->type != STACK_MP && args [i]->type != STACK_OBJ)
  1870| 				return TRUE;
  1871| 			continue;
  1872| 		case MONO_TYPE_PTR:
  1873| 		case MONO_TYPE_FNPTR:
  1874| 			if (args [i]->type != STACK_I4 && !(SIZEOF_VOID_P == 8 && args [i]->type == STACK_I8) &&
  1875| 				args [i]->type != STACK_PTR && args [i]->type != STACK_MP && args [i]->type != STACK_OBJ)
  1876| 				return TRUE;
  1877| 			continue;
  1878| 		case MONO_TYPE_CLASS:
  1879| 		case MONO_TYPE_STRING:
  1880| 		case MONO_TYPE_OBJECT:
  1881| 		case MONO_TYPE_SZARRAY:
  1882| 		case MONO_TYPE_ARRAY:
  1883| 			if (args [i]->type != STACK_OBJ)
  1884| 				return TRUE;
  1885| 			continue;
  1886| 		case MONO_TYPE_I8:
  1887| 		case MONO_TYPE_U8:
  1888| 			if (args [i]->type != STACK_I8 &&
  1889| 				!(SIZEOF_VOID_P == 8 && (args [i]->type == STACK_I4 || args [i]->type == STACK_PTR)))
  1890| 				return TRUE;
  1891| 			continue;
  1892| 		case MONO_TYPE_R4:
  1893| 			if (args [i]->type != cfg->r4_stack_type)
  1894| 				return TRUE;
  1895| 			continue;
  1896| 		case MONO_TYPE_R8:
  1897| 			if (args [i]->type != STACK_R8)
  1898| 				return TRUE;
  1899| 			continue;
  1900| 		case MONO_TYPE_VALUETYPE:
  1901| 			if (m_class_is_enumtype (simple_type->data.klass)) {
  1902| 				simple_type = mono_class_enum_basetype_internal (simple_type->data.klass);
  1903| 				goto handle_enum;
  1904| 			}
  1905| 			if (args [i]->type != STACK_VTYPE)
  1906| 				return TRUE;
  1907| 			continue;
  1908| 		case MONO_TYPE_TYPEDBYREF:
  1909| 			if (args [i]->type != STACK_VTYPE)
  1910| 				return TRUE;
  1911| 			continue;
  1912| 		case MONO_TYPE_GENERICINST:
  1913| 			simple_type = m_class_get_byval_arg (simple_type->data.generic_class->container_class);
  1914| 			goto handle_enum;
  1915| 		case MONO_TYPE_VAR:
  1916| 		case MONO_TYPE_MVAR:
  1917| 			/* gsharedvt */
  1918| 			if (args [i]->type != STACK_VTYPE)
  1919| 				return TRUE;
  1920| 			continue;
  1921| 		default:
  1922| 			g_error ("unknown type 0x%02x in check_call_signature",
  1923| 				 simple_type->type);
  1924| 		}
  1925| 	}
  1926| 	return FALSE;
  1927| }
  1928| MonoJumpInfo *
  1929| mono_patch_info_new (MonoMemPool *mp, int ip, MonoJumpInfoType type, gconstpointer target)
  1930| {
  1931| 	MonoJumpInfo *ji = (MonoJumpInfo *)mono_mempool_alloc (mp, sizeof (MonoJumpInfo));
  1932| 	ji->ip.i = ip;
  1933| 	ji->type = type;
  1934| 	ji->data.target = target;
  1935| 	return ji;
  1936| }
  1937| int
  1938| mini_class_check_context_used (MonoCompile *cfg, MonoClass *klass)
  1939| {
  1940| 	if (cfg->gshared)
  1941| 		return mono_class_check_context_used (klass);
  1942| 	else
  1943| 		return 0;
  1944| }
  1945| int
  1946| mini_method_check_context_used (MonoCompile *cfg, MonoMethod *method)
  1947| {
  1948| 	if (cfg->gshared)
  1949| 		return mono_method_check_context_used (method);
  1950| 	else
  1951| 		return 0;
  1952| }
  1953| /*
  1954|  * need_mrgctx_arg:
  1955|  *
  1956|  *   Check whenever the mrgctx needs to be passed when calling CMETHOD.
  1957|  */
  1958| static gboolean
  1959| need_mrgctx_arg (MonoCompile *cfg, MonoMethod *cmethod)
  1960| {
  1961| 	gboolean pass_mrgctx = FALSE;
  1962| 	if (((cmethod->flags & METHOD_ATTRIBUTE_STATIC) || m_class_is_valuetype (cmethod->klass)) &&
  1963| 		(mono_class_is_ginst (cmethod->klass) || mono_class_is_gtd (cmethod->klass))) {
  1964| 		gboolean sharable = FALSE;
  1965| 		if (mono_method_is_generic_sharable_full (cmethod, TRUE, TRUE, TRUE))
  1966| 			sharable = TRUE;
  1967| 		/*
  1968| 		 * Pass mrgctx iff target method might
  1969| 		 * be shared, which means that sharing
  1970| 		 * is enabled for its class and its
  1971| 		 * context is sharable (and it's not a
  1972| 		 * generic method).
  1973| 		 */
  1974| 		if (sharable && !(mini_method_get_context (cmethod) && mini_method_get_context (cmethod)->method_inst))
  1975| 			pass_mrgctx = TRUE;
  1976| 	}
  1977| 	if (mini_method_needs_mrgctx (cmethod)) {
  1978| 		if (mono_method_is_generic_sharable_full (cmethod, TRUE, TRUE, TRUE)) {
  1979| 			pass_mrgctx = TRUE;
  1980| 		} else {
  1981| 			if (cfg->gsharedvt && mini_is_gsharedvt_signature (mono_method_signature_internal (cmethod)))
  1982| 				pass_mrgctx = TRUE;
  1983| 		}
  1984| 	}
  1985| 	return pass_mrgctx;
  1986| }
  1987| static gboolean
  1988| direct_icalls_enabled (MonoCompile *cfg, MonoMethod *method)
  1989| {
  1990| 	if (cfg->gen_sdb_seq_points || cfg->disable_direct_icalls)
  1991| 		return FALSE;
  1992| 	if (method && cfg->compile_aot && mono_aot_direct_icalls_enabled_for_method (cfg, method))
  1993| 		return TRUE;
  1994| 	/* LLVM on amd64 can't handle calls to non-32 bit addresses */
  1995| #ifdef TARGET_AMD64
  1996| 	if (cfg->compile_llvm && !cfg->llvm_only)
  1997| 		return FALSE;
  1998| #endif
  1999| 	return FALSE;
  2000| }
  2001| MonoInst*
  2002| mono_emit_jit_icall_by_info (MonoCompile *cfg, int il_offset, MonoJitICallInfo *info, MonoInst **args)
  2003| {
  2004| 	/*
  2005| 	 * Call the jit icall without a wrapper if possible.
  2006| 	 * The wrapper is needed to be able to do stack walks for asynchronously suspended
  2007| 	 * threads when debugging.
  2008| 	 */
  2009| 	if (direct_icalls_enabled (cfg, NULL)) {
  2010| 		int costs;
  2011| 		if (!info->wrapper_method) {
  2012| 			info->wrapper_method = mono_marshal_get_icall_wrapper (info, TRUE);
  2013| 			mono_memory_barrier ();
  2014| 		}
  2015| 		/*
  2016| 		 * Inline the wrapper method, which is basically a call to the C icall, and
  2017| 		 * an exception check.
  2018| 		 */
  2019| 		costs = inline_method (cfg, info->wrapper_method, NULL,
  2020| 							   args, NULL, il_offset, TRUE, NULL);
  2021| 		g_assert (costs > 0);
  2022| 		g_assert (!MONO_TYPE_IS_VOID (info->sig->ret));
  2023| 		return args [0];
  2024| 	}
  2025| 	return mono_emit_jit_icall_id (cfg, mono_jit_icall_info_id (info), args);
  2026| }
  2027| static MonoInst*
  2028| mono_emit_widen_call_res (MonoCompile *cfg, MonoInst *ins, MonoMethodSignature *fsig)
  2029| {
  2030| 	if (!MONO_TYPE_IS_VOID (fsig->ret)) {
  2031| #ifdef MONO_ARCH_LLVM_SUPPORTED
  2032| 		gboolean might_use_llvm = TRUE;
  2033| #else
  2034| 		gboolean might_use_llvm = FALSE;
  2035| #endif
  2036| 		if ((fsig->pinvoke || might_use_llvm) && !m_type_is_byref (fsig->ret)) {
  2037| 			int widen_op = -1;
  2038| 			/*
  2039| 			 * Native code might return non register sized integers
  2040| 			 * without initializing the upper bits.
  2041| 			 */
  2042| 			switch (mono_type_to_load_membase (cfg, fsig->ret)) {
  2043| 			case OP_LOADI1_MEMBASE:
  2044| 				widen_op = OP_ICONV_TO_I1;
  2045| 				break;
  2046| 			case OP_LOADU1_MEMBASE:
  2047| 				widen_op = OP_ICONV_TO_U1;
  2048| 				break;
  2049| 			case OP_LOADI2_MEMBASE:
  2050| 				widen_op = OP_ICONV_TO_I2;
  2051| 				break;
  2052| 			case OP_LOADU2_MEMBASE:
  2053| 				widen_op = OP_ICONV_TO_U2;
  2054| 				break;
  2055| 			default:
  2056| 				break;
  2057| 			}
  2058| 			if (widen_op != -1) {
  2059| 				int dreg = alloc_preg (cfg);
  2060| 				MonoInst *widen;
  2061| 				EMIT_NEW_UNALU (cfg, widen, widen_op, dreg, ins->dreg);
  2062| 				widen->type = ins->type;
  2063| 				ins = widen;
  2064| 			}
  2065| 		}
  2066| 	}
  2067| 	return ins;
  2068| }
  2069| static MonoInst*
  2070| emit_get_rgctx_method (MonoCompile *cfg, int context_used,
  2071| 					   MonoMethod *cmethod, MonoRgctxInfoType rgctx_type);
  2072| static void
  2073| emit_method_access_failure (MonoCompile *cfg, MonoMethod *caller, MonoMethod *callee)
  2074| {
  2075| 	MonoInst *args [2];
  2076| 	args [0] = emit_get_rgctx_method (cfg, mono_method_check_context_used (caller), caller, MONO_RGCTX_INFO_METHOD);
  2077| 	args [1] = emit_get_rgctx_method (cfg, mono_method_check_context_used (callee), callee, MONO_RGCTX_INFO_METHOD);
  2078| 	mono_emit_jit_icall (cfg, mono_throw_method_access, args);
  2079| }
  2080| static void
  2081| emit_bad_image_failure (MonoCompile *cfg, MonoMethod *caller, MonoMethod *callee)
  2082| {
  2083| 	mono_emit_jit_icall (cfg, mono_throw_bad_image, NULL);
  2084| }
  2085| static void
  2086| emit_not_supported_failure (MonoCompile *cfg)
  2087| {
  2088| 	mono_emit_jit_icall (cfg, mono_throw_not_supported, NULL);
  2089| }
  2090| static void
  2091| emit_invalid_program_with_msg (MonoCompile *cfg, MonoError *error_msg, MonoMethod *caller, MonoMethod *callee)
  2092| {
  2093| 	g_assert (!is_ok (error_msg));
  2094| 	char *str = mono_mem_manager_strdup (cfg->mem_manager, mono_error_get_message (error_msg));
  2095| 	MonoInst *iargs[1];
  2096| 	if (cfg->compile_aot)
  2097| 		EMIT_NEW_LDSTRLITCONST (cfg, iargs [0], str);
  2098| 	else
  2099| 		EMIT_NEW_PCONST (cfg, iargs [0], str);
  2100| 	mono_emit_jit_icall (cfg, mono_throw_invalid_program, iargs);
  2101| }
  2102| static MonoMethod*
  2103| get_method_nofail (MonoClass *klass, const char *method_name, int num_params, int flags)
  2104| {
  2105| 	MonoMethod *method;
  2106| 	ERROR_DECL (error);
  2107| 	method = mono_class_get_method_from_name_checked (klass, method_name, num_params, flags, error);
  2108| 	mono_error_assert_ok (error);
  2109| 	g_assertf (method, "Could not lookup method %s in %s", method_name, m_class_get_name (klass));
  2110| 	return method;
  2111| }
  2112| MonoMethod*
  2113| mini_get_memcpy_method (void)
  2114| {
  2115| 	static MonoMethod *memcpy_method = NULL;
  2116| 	if (!memcpy_method) {
  2117| 		memcpy_method = get_method_nofail (mono_defaults.string_class, "memcpy", 3, 0);
  2118| 		if (!memcpy_method)
  2119| 			g_error ("Old corlib found. Install a new one");
  2120| 	}
  2121| 	return memcpy_method;
  2122| }
  2123| MonoInst*
  2124| mini_emit_storing_write_barrier (MonoCompile *cfg, MonoInst *ptr, MonoInst *value)
  2125| {
  2126| 	MonoInst *store;
  2127| 	/*
  2128| 	 * Add a release memory barrier so the object contents are flushed
  2129| 	 * to memory before storing the reference into another object.
  2130| 	 */
  2131| 	if (!mini_debug_options.weak_memory_model)
  2132| 		mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
  2133| 	EMIT_NEW_STORE_MEMBASE (cfg, store, OP_STORE_MEMBASE_REG, ptr->dreg, 0, value->dreg);
  2134| 	mini_emit_write_barrier (cfg, ptr, value);
  2135| 	return store;
  2136| }
  2137| void
  2138| mini_emit_write_barrier (MonoCompile *cfg, MonoInst *ptr, MonoInst *value)
  2139| {
  2140| 	int card_table_shift_bits;
  2141| 	target_mgreg_t card_table_mask;
  2142| 	guint8 *card_table;
  2143| 	MonoInst *dummy_use;
  2144| 	int nursery_shift_bits;
  2145| 	size_t nursery_size;
  2146| 	if (!cfg->gen_write_barriers)
  2147| 		return;
  2148| 	card_table = mono_gc_get_target_card_table (&card_table_shift_bits, &card_table_mask);
  2149| 	mono_gc_get_nursery (&nursery_shift_bits, &nursery_size);
  2150| 	if (cfg->backend->have_card_table_wb && !cfg->compile_aot && card_table && nursery_shift_bits > 0 && !COMPILE_LLVM (cfg)) {
  2151| 		MonoInst *wbarrier;
  2152| 		MONO_INST_NEW (cfg, wbarrier, OP_CARD_TABLE_WBARRIER);
  2153| 		wbarrier->sreg1 = ptr->dreg;
  2154| 		wbarrier->sreg2 = value->dreg;
  2155| 		MONO_ADD_INS (cfg->cbb, wbarrier);
  2156| 	} else if (card_table) {
  2157| 		int offset_reg = alloc_preg (cfg);
  2158| 		int card_reg;
  2159| 		MonoInst *ins;
  2160| 		/*
  2161| 		 * We emit a fast light weight write barrier. This always marks cards as in the concurrent
  2162| 		 * collector case, so, for the serial collector, it might slightly slow down nursery
  2163| 		 * collections. We also expect that the host system and the target system have the same card
  2164| 		 * table configuration, which is the case if they have the same pointer size.
  2165| 		 */
  2166| 		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_SHR_UN_IMM, offset_reg, ptr->dreg, card_table_shift_bits);
  2167| 		if (card_table_mask)
  2168| 			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_PAND_IMM, offset_reg, offset_reg, card_table_mask);
  2169| 		/*We can't use PADD_IMM since the cardtable might end up in high addresses and amd64 doesn't support
  2170| 		 * IMM's larger than 32bits.
  2171| 		 */
  2172| 		ins = mini_emit_runtime_constant (cfg, MONO_PATCH_INFO_GC_CARD_TABLE_ADDR, NULL);
  2173| 		card_reg = ins->dreg;
  2174| 		MONO_EMIT_NEW_BIALU (cfg, OP_PADD, offset_reg, offset_reg, card_reg);
  2175| 		MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STOREI1_MEMBASE_IMM, offset_reg, 0, 1);
  2176| 	} else {
  2177| 		MonoMethod *write_barrier = mono_gc_get_write_barrier ();
  2178| 		mono_emit_method_call (cfg, write_barrier, &ptr, NULL);
  2179| 	}
  2180| 	EMIT_NEW_DUMMY_USE (cfg, dummy_use, value);
  2181| }
  2182| MonoMethod*
  2183| mini_get_memset_method (void)
  2184| {
  2185| 	static MonoMethod *memset_method = NULL;
  2186| 	if (!memset_method) {
  2187| 		memset_method = get_method_nofail (mono_defaults.string_class, "memset", 3, 0);
  2188| 		if (!memset_method)
  2189| 			g_error ("Old corlib found. Install a new one");
  2190| 	}
  2191| 	return memset_method;
  2192| }
  2193| void
  2194| mini_emit_initobj (MonoCompile *cfg, MonoInst *dest, const guchar *ip, MonoClass *klass)
  2195| {
  2196| 	MonoInst *iargs [3];
  2197| 	int n;
  2198| 	guint32 align;
  2199| 	MonoMethod *memset_method;
  2200| 	MonoInst *size_ins = NULL;
  2201| 	MonoInst *bzero_ins = NULL;
  2202| 	static MonoMethod *bzero_method;
  2203| 	/* FIXME: Optimize this for the case when dest is an LDADDR */
  2204| 	mono_class_init_internal (klass);
  2205| 	if (mini_is_gsharedvt_klass (klass)) {
  2206| 		size_ins = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_VALUE_SIZE);
  2207| 		bzero_ins = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_BZERO);
  2208| 		if (!bzero_method)
  2209| 			bzero_method = get_method_nofail (mono_defaults.string_class, "bzero_aligned_1", 2, 0);
  2210| 		g_assert (bzero_method);
  2211| 		iargs [0] = dest;
  2212| 		iargs [1] = size_ins;
  2213| 		mini_emit_calli (cfg, mono_method_signature_internal (bzero_method), iargs, bzero_ins, NULL, NULL);
  2214| 		return;
  2215| 	}
  2216| 	klass = mono_class_from_mono_type_internal (mini_get_underlying_type (m_class_get_byval_arg (klass)));
  2217| 	n = mono_class_value_size (klass, &align);
  2218| 	if (n <= TARGET_SIZEOF_VOID_P * 8) {
  2219| 		mini_emit_memset (cfg, dest->dreg, 0, n, 0, align);
  2220| 	}
  2221| 	else {
  2222| 		memset_method = mini_get_memset_method ();
  2223| 		iargs [0] = dest;
  2224| 		EMIT_NEW_ICONST (cfg, iargs [1], 0);
  2225| 		EMIT_NEW_ICONST (cfg, iargs [2], n);
  2226| 		mono_emit_method_call (cfg, memset_method, iargs, NULL);
  2227| 	}
  2228| }
  2229| static gboolean
  2230| context_used_is_mrgctx (MonoCompile *cfg, int context_used)
  2231| {
  2232| 	if (mono_opt_experimental_gshared_mrgctx)
  2233| 		return context_used != 0;
  2234| 	/* gshared dim methods use an mrgctx */
  2235| 	if (mini_method_is_default_method (cfg->method))
  2236| 		return context_used != 0;
  2237| 	return context_used & MONO_GENERIC_CONTEXT_USED_METHOD;
  2238| }
  2239| /*
  2240|  * emit_get_rgctx:
  2241|  *
  2242|  *   Emit IR to return either the vtable or the mrgctx.
  2243|  */
  2244| static MonoInst*
  2245| emit_get_rgctx (MonoCompile *cfg, int context_used)
  2246| {
  2247| 	g_assert (cfg->gshared);
  2248| 	/* Data whose context contains method type vars is stored in the mrgctx */
  2249| 	if (context_used_is_mrgctx (cfg, context_used) || cfg->gshared_info) {
  2250| 		MonoInst *mrgctx_loc, *mrgctx_var;
  2251| 		g_assert (cfg->rgctx_access == MONO_RGCTX_ACCESS_MRGCTX);
  2252| 		/*
  2253| 		if (!mini_method_is_default_method (method))
  2254| 			g_assert (method->is_inflated && mono_method_get_context (method)->method_inst);
  2255| 		*/
  2256| 		if (cfg->llvm_only) {
  2257| 			mrgctx_var = mono_get_mrgctx_var (cfg);
  2258| 		} else {
  2259| 			/* Volatile */
  2260| 			mrgctx_loc = mono_get_mrgctx_var (cfg);
  2261| 			g_assert (mrgctx_loc->flags & MONO_INST_VOLATILE);
  2262| 			EMIT_NEW_TEMPLOAD (cfg, mrgctx_var, mrgctx_loc->inst_c0);
  2263| 		}
  2264| 		return mrgctx_var;
  2265| 	}
  2266| 	/*
  2267| 	 * The rest of the entries are stored in vtable->runtime_generic_context so
  2268| 	 * have to return a vtable.
  2269| 	 */
  2270| 	if (cfg->rgctx_access == MONO_RGCTX_ACCESS_MRGCTX) {
  2271| 		MonoInst *mrgctx_loc, *mrgctx_var, *vtable_var;
  2272| 		int vtable_reg;
  2273| 		/* We are passed an mrgctx, return mrgctx->class_vtable */
  2274| 		if (cfg->llvm_only) {
  2275| 			mrgctx_var = mono_get_mrgctx_var (cfg);
  2276| 		} else {
  2277| 			mrgctx_loc = mono_get_mrgctx_var (cfg);
  2278| 			g_assert (mrgctx_loc->flags & MONO_INST_VOLATILE);
  2279| 			EMIT_NEW_TEMPLOAD (cfg, mrgctx_var, mrgctx_loc->inst_c0);
  2280| 		}
  2281| 		vtable_reg = alloc_preg (cfg);
  2282| 		EMIT_NEW_LOAD_MEMBASE (cfg, vtable_var, OP_LOAD_MEMBASE, vtable_reg, mrgctx_var->dreg, MONO_STRUCT_OFFSET (MonoMethodRuntimeGenericContext, class_vtable));
  2283| 		vtable_var->type = STACK_PTR;
  2284| 		return vtable_var;
  2285| 	} else {
  2286| 		MonoInst *ins, *this_ins;
  2287| 		int vtable_reg;
  2288| 		/* We are passed a this pointer, return this->vtable */
  2289| 		EMIT_NEW_VARLOAD (cfg, this_ins, cfg->this_arg, mono_get_object_type ());
  2290| 		vtable_reg = alloc_preg (cfg);
  2291| 		EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, vtable_reg, this_ins->dreg, MONO_STRUCT_OFFSET (MonoObject, vtable));
  2292| 		return ins;
  2293| 	}
  2294| }
  2295| static MonoJumpInfoRgctxEntry *
  2296| mono_patch_info_rgctx_entry_new (MonoMemPool *mp, MonoMethod *method, gboolean in_mrgctx, MonoJumpInfoType patch_type, gconstpointer patch_data, MonoRgctxInfoType info_type)
  2297| {
  2298| 	MonoJumpInfoRgctxEntry *res = (MonoJumpInfoRgctxEntry *)mono_mempool_alloc0 (mp, sizeof (MonoJumpInfoRgctxEntry));
  2299| 	if (in_mrgctx)
  2300| 		res->d.method = method;
  2301| 	else
  2302| 		res->d.klass = method->klass;
  2303| 	res->in_mrgctx = in_mrgctx;
  2304| 	res->data = (MonoJumpInfo *)mono_mempool_alloc0 (mp, sizeof (MonoJumpInfo));
  2305| 	res->data->type = patch_type;
  2306| 	res->data->data.target = patch_data;
  2307| 	res->info_type = info_type;
  2308| 	return res;
  2309| }
  2310| static MonoInst*
  2311| emit_get_gsharedvt_info (MonoCompile *cfg, gpointer data, MonoRgctxInfoType rgctx_type);
  2312| /*
  2313|  * get_gshared_info_slot:
  2314|  *
  2315|  *   Return a slot index in the mrgctx. PATCH_INFO describes a runtime structure, while
  2316|  * RGCTX_TYPE is a property of that structure.
  2317|  */
  2318| static int
  2319| get_gshared_info_slot (MonoCompile *cfg, MonoJumpInfo *patch_info, MonoRgctxInfoType rgctx_type)
  2320| {
  2321| 	MonoGSharedMethodInfo *info = cfg->gshared_info;
  2322| 	int idx;
  2323| 	gpointer data;
  2324| 	g_assert (cfg->init_method_rgctx_ins);
  2325| 	g_assert (info);
  2326| 	/* The MonoRuntimeGenericContextInfoTemplate structure contains the 'resolved' patch_info, i.e. a MonoClass pointer etc. */
  2327| 	switch (patch_info->type) {
  2328| 	case MONO_PATCH_INFO_CLASS:
  2329| 		data = m_class_get_byval_arg (patch_info->data.klass);
  2330| 		break;
  2331| 	case MONO_PATCH_INFO_METHODCONST:
  2332| 	case MONO_PATCH_INFO_FIELD:
  2333| 	case MONO_PATCH_INFO_VIRT_METHOD:
  2334| 	case MONO_PATCH_INFO_DELEGATE_INFO:
  2335| 	case MONO_PATCH_INFO_GSHAREDVT_METHOD:
  2336| 	case MONO_PATCH_INFO_GSHAREDVT_CALL:
  2337| 	case MONO_PATCH_INFO_SIGNATURE:
  2338| 		data = (gpointer)patch_info->data.target;
  2339| 		break;
  2340| 	default:
  2341| 		g_assert_not_reached ();
  2342| 		break;
  2343| 	}
  2344| 	g_assert (data);
  2345| 	for (int i = 0; i < info->num_entries; ++i) {
  2346| 		if (info->entries [i].info_type == rgctx_type && info->entries [i].data == data && rgctx_type != MONO_RGCTX_INFO_LOCAL_OFFSET)
  2347| 			return i;
  2348| 	}
  2349| 	if (info->num_entries == info->count_entries) {
  2350| 		MonoRuntimeGenericContextInfoTemplate *new_entries;
  2351| 		int new_count_entries = info->count_entries ? info->count_entries * 2 : 16;
  2352| 		new_entries = (MonoRuntimeGenericContextInfoTemplate *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoRuntimeGenericContextInfoTemplate) * new_count_entries);
  2353| 		memcpy (new_entries, info->entries, sizeof (MonoRuntimeGenericContextInfoTemplate) * info->count_entries);
  2354| 		info->entries = new_entries;
  2355| 		info->count_entries = new_count_entries;
  2356| 	}
  2357| 	idx = info->num_entries;
  2358| 	info->entries [idx].info_type = rgctx_type;
  2359| 	info->entries [idx].data = data;
  2360| 	info->num_entries++;
  2361| 	return idx;
  2362| }
  2363| static MonoInst*
  2364| emit_rgctx_fetch_inline (MonoCompile *cfg, MonoInst *rgctx, MonoJumpInfoRgctxEntry *entry)
  2365| {
  2366| 	MonoInst *call;
  2367| 	MonoInst *slot_ins;
  2368| 	EMIT_NEW_AOTCONST (cfg, slot_ins, MONO_PATCH_INFO_RGCTX_SLOT_INDEX, entry);
  2369| 	if (cfg->disable_inline_rgctx_fetch || cfg->interp_entry_only) {
  2370| 		MonoInst *args [2] = { rgctx, slot_ins };
  2371| 		if (entry->in_mrgctx)
  2372| 			call = mono_emit_jit_icall (cfg, mono_fill_method_rgctx, args);
  2373| 		else
  2374| 			call = mono_emit_jit_icall (cfg, mono_fill_class_rgctx, args);
  2375| 		return call;
  2376| 	}
  2377| 	MonoBasicBlock *slowpath_bb, *end_bb;
  2378| 	MonoInst *ins, *res;
  2379| 	int rgctx_reg, res_reg;
  2380| 	/*
  2381| 	 * rgctx = vtable->runtime_generic_context;
  2382| 	 * if (rgctx) {
  2383| 	 *    val = rgctx [slot + 1];
  2384| 	 *    if (val)
  2385| 	 *       return val;
  2386| 	 * }
  2387| 	 * <slowpath>
  2388| 	 */
  2389| 	NEW_BBLOCK (cfg, end_bb);
  2390| 	NEW_BBLOCK (cfg, slowpath_bb);
  2391| 	if (entry->in_mrgctx) {
  2392| 		rgctx_reg = rgctx->dreg;
  2393| 	} else {
  2394| 		rgctx_reg = alloc_preg (cfg);
  2395| 		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, rgctx_reg, rgctx->dreg, MONO_STRUCT_OFFSET (MonoVTable, runtime_generic_context));
  2396| 		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, rgctx_reg, 0);
  2397| 		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, slowpath_bb);
  2398| 	}
  2399| 	int table_size = mono_class_rgctx_get_array_size (0, entry->in_mrgctx);
  2400| 	if (entry->in_mrgctx)
  2401| 		table_size -= MONO_SIZEOF_METHOD_RUNTIME_GENERIC_CONTEXT / TARGET_SIZEOF_VOID_P;
  2402| 	MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, slot_ins->dreg, table_size - 1);
  2403| 	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBGE, slowpath_bb);
  2404| 	int shifted_slot_reg = alloc_ireg (cfg);
  2405| 	EMIT_NEW_BIALU_IMM (cfg, ins, OP_ISHL_IMM, shifted_slot_reg, slot_ins->dreg, TARGET_SIZEOF_VOID_P == 8 ? 3 : 2);
  2406| 	int addr_reg = alloc_preg (cfg);
  2407| 	EMIT_NEW_UNALU (cfg, ins, OP_MOVE, addr_reg, rgctx_reg);
  2408| 	EMIT_NEW_BIALU (cfg, ins, OP_PADD, addr_reg, addr_reg, shifted_slot_reg);
  2409| 	int val_reg = alloc_preg (cfg);
  2410| 	MONO_EMIT_NEW_LOAD_MEMBASE (cfg, val_reg, addr_reg, TARGET_SIZEOF_VOID_P + (entry->in_mrgctx ? MONO_SIZEOF_METHOD_RUNTIME_GENERIC_CONTEXT : 0));
  2411| 	MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, val_reg, 0);
  2412| 	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, slowpath_bb);
  2413| 	res_reg = alloc_preg (cfg);
  2414| 	EMIT_NEW_UNALU (cfg, ins, OP_MOVE, res_reg, val_reg);
  2415| 	res = ins;
  2416| 	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  2417| 	MONO_START_BB (cfg, slowpath_bb);
  2418| 	slowpath_bb->out_of_line = TRUE;
  2419| 	MonoInst *args[2] = { rgctx, slot_ins };
  2420| 	if (entry->in_mrgctx)
  2421| 		call = mono_emit_jit_icall (cfg, mono_fill_method_rgctx, args);
  2422| 	else
  2423| 		call = mono_emit_jit_icall (cfg, mono_fill_class_rgctx, args);
  2424| 	EMIT_NEW_UNALU (cfg, ins, OP_MOVE, res_reg, call->dreg);
  2425| 	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  2426| 	MONO_START_BB (cfg, end_bb);
  2427| 	return res;
  2428| }
  2429| /*
  2430|  * emit_rgctx_fetch:
  2431|  *
  2432|  *   Emit IR to load the value of the rgctx entry ENTRY from the rgctx.
  2433|  */
  2434| static MonoInst*
  2435| emit_rgctx_fetch (MonoCompile *cfg, int context_used, MonoJumpInfoRgctxEntry *entry)
  2436| {
  2437| 	MonoInst *rgctx = emit_get_rgctx (cfg, context_used);
  2438| 	if (cfg->gshared_info) {
  2439| 		MonoInst *ins;
  2440| 		int dreg, entries_reg, idx;
  2441| 		int ninlines = mono_class_rgctx_get_array_size (0, TRUE);
  2442| 		idx = get_gshared_info_slot (cfg, entry->data, entry->info_type);
  2443| 		/* The first few entries are stored inline, the rest are stored in mrgctx->entries */
  2444| 		if (idx < ninlines) {
  2445| 			/* Load mrgctx->infos [idx] */
  2446| 			dreg = alloc_preg (cfg);
  2447| 			EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, dreg, rgctx->dreg, MONO_STRUCT_OFFSET (MonoMethodRuntimeGenericContext, infos) + idx * TARGET_SIZEOF_VOID_P);
  2448| 		} else {
  2449| 			/* Load mrgctx->entries [idx - ninlines] */
  2450| 			entries_reg = alloc_preg (cfg);
  2451| 			EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, entries_reg, rgctx->dreg, MONO_STRUCT_OFFSET (MonoMethodRuntimeGenericContext, entries));
  2452| 			dreg = alloc_preg (cfg);
  2453| 			EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, dreg, entries_reg, (idx - ninlines) * TARGET_SIZEOF_VOID_P);
  2454| 		}
  2455| 		return ins;
  2456| 	}
  2457| 	if (cfg->llvm_only)
  2458| 		return emit_rgctx_fetch_inline (cfg, rgctx, entry);
  2459| 	else
  2460| 		return mini_emit_abs_call (cfg, MONO_PATCH_INFO_RGCTX_FETCH, entry, mono_icall_sig_ptr_ptr, &rgctx);
  2461| }
  2462| /*
  2463|  * mini_emit_get_rgctx_klass:
  2464|  *
  2465|  *   Emit IR to load the property RGCTX_TYPE of KLASS. If context_used is 0, emit
  2466|  * normal constants, else emit a load from the rgctx.
  2467|  */
  2468| MonoInst*
  2469| mini_emit_get_rgctx_klass (MonoCompile *cfg, int context_used,
  2470| 						   MonoClass *klass, MonoRgctxInfoType rgctx_type)
  2471| {
  2472| 	if (!context_used) {
  2473| 		MonoInst *ins;
  2474| 		switch (rgctx_type) {
  2475| 		case MONO_RGCTX_INFO_KLASS:
  2476| 			EMIT_NEW_CLASSCONST (cfg, ins, klass);
  2477| 			return ins;
  2478| 		case MONO_RGCTX_INFO_VTABLE: {
  2479| 			MonoVTable *vtable = mono_class_vtable_checked (klass, cfg->error);
  2480| 			CHECK_CFG_ERROR;
  2481| 			EMIT_NEW_VTABLECONST (cfg, ins, vtable);
  2482| 			return ins;
  2483| 		}
  2484| 		default:
  2485| 			g_assert_not_reached ();
  2486| 		}
  2487| 	}
  2488| 	if (cfg->llvm_only && cfg->gsharedvt && !cfg->gshared_info)
  2489| 		return mini_emit_get_gsharedvt_info_klass (cfg, klass, rgctx_type);
  2490| 	MonoJumpInfoRgctxEntry *entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_CLASS, klass, rgctx_type);
  2491| 	return emit_rgctx_fetch (cfg, context_used, entry);
  2492| mono_error_exit:
  2493| 	return NULL;
  2494| }
  2495| static MonoInst*
  2496| emit_get_rgctx_sig (MonoCompile *cfg, int context_used,
  2497| 					MonoMethodSignature *sig, MonoRgctxInfoType rgctx_type)
  2498| {
  2499| 	MonoJumpInfoRgctxEntry *entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_SIGNATURE, sig, rgctx_type);
  2500| 	return emit_rgctx_fetch (cfg, context_used, entry);
  2501| }
  2502| static MonoInst*
  2503| emit_get_rgctx_gsharedvt_call (MonoCompile *cfg, int context_used,
  2504| 							   MonoMethodSignature *sig, MonoMethod *cmethod, MonoRgctxInfoType rgctx_type)
  2505| {
  2506| 	MonoJumpInfoGSharedVtCall *call_info;
  2507| 	MonoJumpInfoRgctxEntry *entry;
  2508| 	call_info = (MonoJumpInfoGSharedVtCall *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoJumpInfoGSharedVtCall));
  2509| 	call_info->sig = sig;
  2510| 	call_info->method = cmethod;
  2511| 	entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_GSHAREDVT_CALL, call_info, rgctx_type);
  2512| 	return emit_rgctx_fetch (cfg, context_used, entry);
  2513| }
  2514| /*
  2515|  * emit_get_rgctx_virt_method:
  2516|  *
  2517|  *   Return data for method VIRT_METHOD for a receiver of type KLASS.
  2518|  */
  2519| static MonoInst*
  2520| emit_get_rgctx_virt_method (MonoCompile *cfg, int context_used,
  2521| 							MonoClass *klass, MonoMethod *virt_method, MonoRgctxInfoType rgctx_type)
  2522| {
  2523| 	MonoJumpInfoVirtMethod *info;
  2524| 	MonoJumpInfoRgctxEntry *entry;
  2525| 	if (context_used == -1)
  2526| 		context_used = mono_class_check_context_used (klass) | mono_method_check_context_used (virt_method);
  2527| 	info = (MonoJumpInfoVirtMethod *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoJumpInfoVirtMethod));
  2528| 	info->klass = klass;
  2529| 	info->method = virt_method;
  2530| 	entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_VIRT_METHOD, info, rgctx_type);
  2531| 	return emit_rgctx_fetch (cfg, context_used, entry);
  2532| }
  2533| static MonoInst*
  2534| emit_get_rgctx_gsharedvt_method (MonoCompile *cfg, int context_used,
  2535| 								 MonoMethod *cmethod, MonoGSharedVtMethodInfo *info)
  2536| {
  2537| 	MonoJumpInfoRgctxEntry *entry;
  2538| 	entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_GSHAREDVT_METHOD, info, MONO_RGCTX_INFO_METHOD_GSHAREDVT_INFO);
  2539| 	return emit_rgctx_fetch (cfg, context_used, entry);
  2540| }
  2541| /*
  2542|  * emit_get_rgctx_method:
  2543|  *
  2544|  *   Emit IR to load the property RGCTX_TYPE of CMETHOD. If context_used is 0, emit
  2545|  * normal constants, else emit a load from the rgctx.
  2546|  */
  2547| static MonoInst*
  2548| emit_get_rgctx_method (MonoCompile *cfg, int context_used,
  2549| 					   MonoMethod *cmethod, MonoRgctxInfoType rgctx_type)
  2550| {
  2551| 	if (context_used == -1)
  2552| 		context_used = mono_method_check_context_used (cmethod);
  2553| 	if (!context_used) {
  2554| 		MonoInst *ins;
  2555| 		switch (rgctx_type) {
  2556| 		case MONO_RGCTX_INFO_METHOD:
  2557| 			EMIT_NEW_METHODCONST (cfg, ins, cmethod);
  2558| 			return ins;
  2559| 		case MONO_RGCTX_INFO_METHOD_RGCTX:
  2560| 			EMIT_NEW_METHOD_RGCTX_CONST (cfg, ins, cmethod);
  2561| 			return ins;
  2562| 		case MONO_RGCTX_INFO_METHOD_FTNDESC:
  2563| 			EMIT_NEW_AOTCONST (cfg, ins, MONO_PATCH_INFO_METHOD_FTNDESC, cmethod);
  2564| 			return ins;
  2565| 		case MONO_RGCTX_INFO_LLVMONLY_INTERP_ENTRY:
  2566| 			EMIT_NEW_AOTCONST (cfg, ins, MONO_PATCH_INFO_LLVMONLY_INTERP_ENTRY, cmethod);
  2567| 			return ins;
  2568| 		default:
  2569| 			g_assert_not_reached ();
  2570| 		}
  2571| 	} else {
  2572| 		if (cfg->llvm_only && cfg->gsharedvt && !cfg->gshared_info)
  2573| 			return emit_get_gsharedvt_info (cfg, cmethod, rgctx_type);
  2574| 		MonoJumpInfoRgctxEntry *entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_METHODCONST, cmethod, rgctx_type);
  2575| 		return emit_rgctx_fetch (cfg, context_used, entry);
  2576| 	}
  2577| }
  2578| static MonoInst*
  2579| emit_get_rgctx_field (MonoCompile *cfg, int context_used,
  2580| 					  MonoClassField *field, MonoRgctxInfoType rgctx_type)
  2581| {
  2582| 	if (cfg->llvm_only && cfg->gsharedvt && !cfg->gshared_info)
  2583| 		return emit_get_gsharedvt_info (cfg, field, rgctx_type);
  2584| 	MonoJumpInfoRgctxEntry *entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_FIELD, field, rgctx_type);
  2585| 	return emit_rgctx_fetch (cfg, context_used, entry);
  2586| }
  2587| MonoInst*
  2588| mini_emit_get_rgctx_method (MonoCompile *cfg, int context_used,
  2589| 							MonoMethod *cmethod, MonoRgctxInfoType rgctx_type)
  2590| {
  2591| 	return emit_get_rgctx_method (cfg, context_used, cmethod, rgctx_type);
  2592| }
  2593| static int
  2594| get_gsharedvt_info_slot (MonoCompile *cfg, gpointer data, MonoRgctxInfoType rgctx_type)
  2595| {
  2596| 	MonoGSharedVtMethodInfo *info = cfg->gsharedvt_info;
  2597| 	MonoRuntimeGenericContextInfoTemplate *template_;
  2598| 	int i, idx;
  2599| 	g_assert (info);
  2600| 	for (i = 0; i < info->num_entries; ++i) {
  2601| 		MonoRuntimeGenericContextInfoTemplate *otemplate = &info->entries [i];
  2602| 		if (otemplate->info_type == rgctx_type && otemplate->data == data && rgctx_type != MONO_RGCTX_INFO_LOCAL_OFFSET)
  2603| 			return i;
  2604| 	}
  2605| 	if (info->num_entries == info->count_entries) {
  2606| 		MonoRuntimeGenericContextInfoTemplate *new_entries;
  2607| 		int new_count_entries = info->count_entries ? info->count_entries * 2 : 16;
  2608| 		new_entries = (MonoRuntimeGenericContextInfoTemplate *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoRuntimeGenericContextInfoTemplate) * new_count_entries);
  2609| 		memcpy (new_entries, info->entries, sizeof (MonoRuntimeGenericContextInfoTemplate) * info->count_entries);
  2610| 		info->entries = new_entries;
  2611| 		info->count_entries = new_count_entries;
  2612| 	}
  2613| 	idx = info->num_entries;
  2614| 	template_ = &info->entries [idx];
  2615| 	template_->info_type = rgctx_type;
  2616| 	template_->data = data;
  2617| 	info->num_entries ++;
  2618| 	return idx;
  2619| }
  2620| /*
  2621|  * emit_get_gsharedvt_info:
  2622|  *
  2623|  *   This is similar to emit_get_rgctx_.., but loads the data from the gsharedvt info var instead of calling an rgctx fetch trampoline.
  2624|  */
  2625| static MonoInst*
  2626| emit_get_gsharedvt_info (MonoCompile *cfg, gpointer data, MonoRgctxInfoType rgctx_type)
  2627| {
  2628| 	MonoInst *ins;
  2629| 	int idx, dreg;
  2630| 	idx = get_gsharedvt_info_slot (cfg, data, rgctx_type);
  2631| 	/* Load info->entries [idx] */
  2632| 	dreg = alloc_preg (cfg);
  2633| 	EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, dreg, cfg->gsharedvt_info_var->dreg, MONO_STRUCT_OFFSET (MonoGSharedVtMethodRuntimeInfo, entries) + (idx * TARGET_SIZEOF_VOID_P));
  2634| 	return ins;
  2635| }
  2636| MonoInst*
  2637| mini_emit_get_gsharedvt_info_klass (MonoCompile *cfg, MonoClass *klass, MonoRgctxInfoType rgctx_type)
  2638| {
  2639| 	return emit_get_gsharedvt_info (cfg, m_class_get_byval_arg (klass), rgctx_type);
  2640| }
  2641| /*
  2642|  * On return the caller must check @klass for load errors.
  2643|  */
  2644| static void
  2645| emit_class_init (MonoCompile *cfg, MonoClass *klass, gboolean for_field_access)
  2646| {
  2647| 	MonoInst *vtable_arg;
  2648| 	int context_used;
  2649| 	context_used = mini_class_check_context_used (cfg, klass);
  2650| 	if (cfg->compile_aot && !for_field_access && mono_class_is_before_field_init (klass))
  2651| 		/* Only field accesses trigger initialization */
  2652| 		return;
  2653| 	if (context_used) {
  2654| 		vtable_arg = mini_emit_get_rgctx_klass (cfg, context_used,
  2655| 										   klass, MONO_RGCTX_INFO_VTABLE);
  2656| 	} else {
  2657| 		MonoVTable *vtable = mono_class_vtable_checked (klass, cfg->error);
  2658| 		if (!is_ok (cfg->error)) {
  2659| 			mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
  2660| 			return;
  2661| 		}
  2662| 		EMIT_NEW_VTABLECONST (cfg, vtable_arg, vtable);
  2663| 	}
  2664| 	if (!COMPILE_LLVM (cfg) && cfg->backend->have_op_generic_class_init) {
  2665| 		MonoInst *ins;
  2666| 		/*
  2667| 		 * Using an opcode instead of emitting IR here allows the hiding of the call inside the opcode,
  2668| 		 * so this doesn't have to clobber any regs and it doesn't break basic blocks.
  2669| 		 */
  2670| 		MONO_INST_NEW (cfg, ins, OP_GENERIC_CLASS_INIT);
  2671| 		ins->sreg1 = vtable_arg->dreg;
  2672| 		MONO_ADD_INS (cfg->cbb, ins);
  2673| 	} else {
  2674| 		int inited_reg;
  2675| 		MonoBasicBlock *inited_bb;
  2676| 		inited_reg = alloc_ireg (cfg);
  2677| 		MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADU1_MEMBASE, inited_reg, vtable_arg->dreg, MONO_STRUCT_OFFSET (MonoVTable, initialized));
  2678| 		NEW_BBLOCK (cfg, inited_bb);
  2679| 		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, inited_reg, 0);
  2680| 		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBNE_UN, inited_bb);
  2681| 		cfg->cbb->out_of_line = TRUE;
  2682| 		mono_emit_jit_icall (cfg, mono_generic_class_init, &vtable_arg);
  2683| 		MONO_START_BB (cfg, inited_bb);
  2684| 	}
  2685| }
  2686| static void
  2687| emit_seq_point (MonoCompile *cfg, MonoMethod *method, guint8* ip, gboolean intr_loc, gboolean nonempty_stack)
  2688| {
  2689| 	MonoInst *ins;
  2690| 	if (cfg->gen_seq_points && cfg->method == method) {
  2691| 		NEW_SEQ_POINT (cfg, ins, ip - cfg->header->code, intr_loc);
  2692| 		if (nonempty_stack)
  2693| 			ins->flags |= MONO_INST_NONEMPTY_STACK;
  2694| 		MONO_ADD_INS (cfg->cbb, ins);
  2695| 		cfg->last_seq_point = ins;
  2696| 	}
  2697| }
  2698| void
  2699| mini_save_cast_details (MonoCompile *cfg, MonoClass *klass, int obj_reg, gboolean null_check)
  2700| {
  2701| 	if (mini_debug_options.better_cast_details) {
  2702| 		int vtable_reg = alloc_preg (cfg);
  2703| 		int klass_reg = alloc_preg (cfg);
  2704| 		MonoBasicBlock *is_null_bb = NULL;
  2705| 		MonoInst *tls_get;
  2706| 		if (null_check) {
  2707| 			NEW_BBLOCK (cfg, is_null_bb);
  2708| 			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, obj_reg, 0);
  2709| 			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, is_null_bb);
  2710| 		}
  2711| 		tls_get = mono_create_tls_get (cfg, TLS_KEY_JIT_TLS);
  2712| 		if (!tls_get) {
  2713| 			fprintf (stderr, "error: --debug=casts not supported on this platform.\n.");
  2714| 			exit (1);
  2715| 		}
  2716| 		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, vtable_reg, obj_reg, MONO_STRUCT_OFFSET (MonoObject, vtable));
  2717| 		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, klass_reg, vtable_reg, MONO_STRUCT_OFFSET (MonoVTable, klass));
  2718| 		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, tls_get->dreg, MONO_STRUCT_OFFSET (MonoJitTlsData, class_cast_from), klass_reg);
  2719| 		MonoInst *class_ins = mini_emit_get_rgctx_klass (cfg, mini_class_check_context_used (cfg, klass), klass, MONO_RGCTX_INFO_KLASS);
  2720| 		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, tls_get->dreg, MONO_STRUCT_OFFSET (MonoJitTlsData, class_cast_to), class_ins->dreg);
  2721| 		if (null_check)
  2722| 			MONO_START_BB (cfg, is_null_bb);
  2723| 	}
  2724| }
  2725| void
  2726| mini_reset_cast_details (MonoCompile *cfg)
  2727| {
  2728| 	/* Reset the variables holding the cast details */
  2729| 	if (mini_debug_options.better_cast_details) {
  2730| 		MonoInst *tls_get = mono_create_tls_get (cfg, TLS_KEY_JIT_TLS);
  2731| 		/* It is enough to reset the from field */
  2732| 		MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STORE_MEMBASE_IMM, tls_get->dreg, MONO_STRUCT_OFFSET (MonoJitTlsData, class_cast_from), 0);
  2733| 	}
  2734| }
  2735| /*
  2736|  * On return the caller must check @array_class for load errors
  2737|  */
  2738| static void
  2739| mini_emit_check_array_type (MonoCompile *cfg, MonoInst *obj, MonoClass *array_class)
  2740| {
  2741| 	int vtable_reg = alloc_preg (cfg);
  2742| 	int context_used;
  2743| 	context_used = mini_class_check_context_used (cfg, array_class);
  2744| 	mini_save_cast_details (cfg, array_class, obj->dreg, FALSE);
  2745| 	MONO_EMIT_NEW_LOAD_MEMBASE_FAULT (cfg, vtable_reg, obj->dreg, MONO_STRUCT_OFFSET (MonoObject, vtable));
  2746| 	if (context_used) {
  2747| 		MonoInst *vtable_ins;
  2748| 		vtable_ins = mini_emit_get_rgctx_klass (cfg, context_used, array_class, MONO_RGCTX_INFO_VTABLE);
  2749| 		MONO_EMIT_NEW_BIALU (cfg, OP_COMPARE, -1, vtable_reg, vtable_ins->dreg);
  2750| 	} else {
  2751| 		if (cfg->compile_aot) {
  2752| 			int vt_reg;
  2753| 			MonoVTable *vtable;
  2754| 			if (!(vtable = mono_class_vtable_checked (array_class, cfg->error))) {
  2755| 				mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
  2756| 				return;
  2757| 			}
  2758| 			vt_reg = alloc_preg (cfg);
  2759| 			MONO_EMIT_NEW_VTABLECONST (cfg, vt_reg, vtable);
  2760| 			MONO_EMIT_NEW_BIALU (cfg, OP_COMPARE, -1, vtable_reg, vt_reg);
  2761| 		} else {
  2762| 			MonoVTable *vtable;
  2763| 			if (!(vtable = mono_class_vtable_checked (array_class, cfg->error))) {
  2764| 				mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
  2765| 				return;
  2766| 			}
  2767| 			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, vtable_reg, (gssize)vtable);
  2768| 		}
  2769| 	}
  2770| 	MONO_EMIT_NEW_COND_EXC (cfg, NE_UN, "ArrayTypeMismatchException");
  2771| 	mini_reset_cast_details (cfg);
  2772| }
  2773| /**
  2774|  * Handles unbox of a Nullable<T>. If context_used is non zero, then shared
  2775|  * generic code is generated.
  2776|  */
  2777| static MonoInst*
  2778| handle_unbox_nullable (MonoCompile* cfg, MonoInst* val, MonoClass* klass, int context_used)
  2779| {
  2780| 	MonoMethod* method;
  2781| 	if (m_class_is_enumtype (mono_class_get_nullable_param_internal (klass)))
  2782| 		method = get_method_nofail (klass, "UnboxExact", 1, 0);
  2783| 	else
  2784| 		method = get_method_nofail (klass, "Unbox", 1, 0);
  2785| 	g_assert (method);
  2786| 	if (context_used) {
  2787| 		MonoInst *rgctx, *addr;
  2788| 		/* FIXME: What if the class is shared?  We might not
  2789| 		   have to get the address of the method from the
  2790| 		   RGCTX. */
  2791| 		if (cfg->llvm_only) {
  2792| 			addr = emit_get_rgctx_method (cfg, context_used, method,
  2793| 										  MONO_RGCTX_INFO_METHOD_FTNDESC);
  2794| 			cfg->signatures = g_slist_prepend_mempool (cfg->mempool, cfg->signatures, mono_method_signature_internal (method));
  2795| 			return mini_emit_llvmonly_calli (cfg, mono_method_signature_internal (method), &val, addr);
  2796| 		} else {
  2797| 			addr = emit_get_rgctx_method (cfg, context_used, method,
  2798| 										  MONO_RGCTX_INFO_GENERIC_METHOD_CODE);
  2799| 			rgctx = emit_get_rgctx (cfg, context_used);
  2800| 			return mini_emit_calli (cfg, mono_method_signature_internal (method), &val, addr, NULL, rgctx);
  2801| 		}
  2802| 	} else {
  2803| 		MonoInst *rgctx_arg = NULL;
  2804| 		if (need_mrgctx_arg (cfg, method))
  2805| 			rgctx_arg = emit_get_rgctx_method (cfg, context_used, method,
  2806| 											   MONO_RGCTX_INFO_METHOD_RGCTX);
  2807| 		return mini_emit_method_call_full (cfg, method, NULL, FALSE, &val, NULL, NULL, rgctx_arg);
  2808| 	}
  2809| }
  2810| MonoInst*
  2811| mini_handle_unbox (MonoCompile *cfg, MonoClass *klass, MonoInst *val, int context_used)
  2812| {
  2813| 	MonoInst *add;
  2814| 	int obj_reg;
  2815| 	int vtable_reg = alloc_dreg (cfg ,STACK_PTR);
  2816| 	int klass_reg = alloc_dreg (cfg ,STACK_PTR);
  2817| 	int eclass_reg = alloc_dreg (cfg ,STACK_PTR);
  2818| 	int rank_reg = alloc_dreg (cfg ,STACK_I4);
  2819| 	obj_reg = val->dreg;
  2820| 	MONO_EMIT_NEW_LOAD_MEMBASE_FAULT (cfg, vtable_reg, obj_reg, MONO_STRUCT_OFFSET (MonoObject, vtable));
  2821| 	MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADU1_MEMBASE, rank_reg, vtable_reg, MONO_STRUCT_OFFSET (MonoVTable, rank));
  2822| 	/* FIXME: generics */
  2823| 	g_assert (m_class_get_rank (klass) == 0);
  2824| 	MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, rank_reg, 0);
  2825| 	MONO_EMIT_NEW_COND_EXC (cfg, NE_UN, "InvalidCastException");
  2826| 	MONO_EMIT_NEW_LOAD_MEMBASE (cfg, klass_reg, vtable_reg, MONO_STRUCT_OFFSET (MonoVTable, klass));
  2827| 	MONO_EMIT_NEW_LOAD_MEMBASE (cfg, eclass_reg, klass_reg, m_class_offsetof_element_class ());
  2828| 	if (context_used) {
  2829| 		MonoInst *element_class;
  2830| 		/* This assertion is from the unboxcast insn */
  2831| 		g_assert (m_class_get_rank (klass) == 0);
  2832| 		element_class = mini_emit_get_rgctx_klass (cfg, context_used,
  2833| 				klass, MONO_RGCTX_INFO_ELEMENT_KLASS);
  2834| 		MONO_EMIT_NEW_BIALU (cfg, OP_COMPARE, -1, eclass_reg, element_class->dreg);
  2835| 		MONO_EMIT_NEW_COND_EXC (cfg, NE_UN, "InvalidCastException");
  2836| 	} else {
  2837| 		mini_save_cast_details (cfg, m_class_get_element_class (klass), obj_reg, FALSE);
  2838| 		mini_emit_class_check (cfg, eclass_reg, m_class_get_element_class (klass));
  2839| 		mini_reset_cast_details (cfg);
  2840| 	}
  2841| 	NEW_BIALU_IMM (cfg, add, OP_ADD_IMM, alloc_dreg (cfg, STACK_MP), obj_reg, MONO_ABI_SIZEOF (MonoObject));
  2842| 	MONO_ADD_INS (cfg->cbb, add);
  2843| 	add->type = STACK_MP;
  2844| 	add->klass = klass;
  2845| 	return add;
  2846| }
  2847| static MonoInst*
  2848| handle_unbox_gsharedvt (MonoCompile *cfg, MonoClass *klass, MonoInst *obj)
  2849| {
  2850| 	MonoInst *addr, *klass_inst, *is_ref, *args[16];
  2851| 	MonoBasicBlock *is_ref_bb, *is_nullable_bb, *end_bb;
  2852| 	MonoInst *ins;
  2853| 	int dreg, addr_reg;
  2854| 	klass_inst = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_KLASS);
  2855| 	/* obj */
  2856| 	args [0] = obj;
  2857| 	/* klass */
  2858| 	args [1] = klass_inst;
  2859| 	/* CASTCLASS */
  2860| 	obj = mono_emit_jit_icall (cfg, mono_object_castclass_unbox, args);
  2861| 	NEW_BBLOCK (cfg, is_ref_bb);
  2862| 	NEW_BBLOCK (cfg, is_nullable_bb);
  2863| 	NEW_BBLOCK (cfg, end_bb);
  2864| 	is_ref = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_CLASS_BOX_TYPE);
  2865| 	MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, is_ref->dreg, MONO_GSHAREDVT_BOX_TYPE_REF);
  2866| 	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBEQ, is_ref_bb);
  2867| 	MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, is_ref->dreg, MONO_GSHAREDVT_BOX_TYPE_NULLABLE);
  2868| 	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBEQ, is_nullable_bb);
  2869| 	/* This will contain either the address of the unboxed vtype, or an address of the temporary where the ref is stored */
  2870| 	addr_reg = alloc_dreg (cfg, STACK_MP);
  2871| 	/* Non-ref case */
  2872| 	/* UNBOX */
  2873| 	NEW_BIALU_IMM (cfg, addr, OP_ADD_IMM, addr_reg, obj->dreg, MONO_ABI_SIZEOF (MonoObject));
  2874| 	MONO_ADD_INS (cfg->cbb, addr);
  2875| 	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  2876| 	/* Ref case */
  2877| 	MONO_START_BB (cfg, is_ref_bb);
  2878| 	/* Save the ref to a temporary */
  2879| 	dreg = alloc_ireg (cfg);
  2880| 	EMIT_NEW_VARLOADA_VREG (cfg, addr, dreg, m_class_get_byval_arg (klass));
  2881| 	addr->dreg = addr_reg;
  2882| 	MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, addr->dreg, 0, obj->dreg);
  2883| 	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  2884| 	/* Nullable case */
  2885| 	MONO_START_BB (cfg, is_nullable_bb);
  2886| 	{
  2887| 		MonoInst *unbox_addr = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_NULLABLE_CLASS_UNBOX);
  2888| 		MonoInst *unbox_call;
  2889| 		MonoMethodSignature *unbox_sig;
  2890| 		unbox_sig = (MonoMethodSignature *)mono_mempool_alloc0 (cfg->mempool, MONO_SIZEOF_METHOD_SIGNATURE + (1 * sizeof (MonoType *)));
  2891| 		unbox_sig->ret = m_class_get_byval_arg (klass);
  2892| 		unbox_sig->param_count = 1;
  2893| 		unbox_sig->params [0] = mono_get_object_type ();
  2894| 		if (cfg->llvm_only)
  2895| 			unbox_call = mini_emit_llvmonly_calli (cfg, unbox_sig, &obj, unbox_addr );
  2896| 		else
  2897| 			unbox_call = mini_emit_calli (cfg, unbox_sig, &obj, unbox_addr , NULL, NULL);
  2898| 		EMIT_NEW_VARLOADA_VREG (cfg, unbox_addr , unbox_call->dreg, m_class_get_byval_arg (klass));
  2899| 		unbox_addr ->dreg = addr_reg;
  2900| 	}
  2901| 	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  2902| 	/* End */
  2903| 	MONO_START_BB (cfg, end_bb);
  2904| 	/* LDOBJ */
  2905| 	EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), addr_reg, 0);
  2906| 	return ins;
  2907| }
  2908| /*
  2909|  * Returns NULL and set the cfg exception on error.
  2910|  */
  2911| static MonoInst*
  2912| handle_alloc (MonoCompile *cfg, MonoClass *klass, gboolean for_box, int context_used)
  2913| {
  2914| 	MonoInst *iargs [2];
  2915| 	MonoJitICallId alloc_ftn;
  2916| 	if (mono_class_get_flags (klass) & TYPE_ATTRIBUTE_ABSTRACT) {
  2917| 		char* full_name = mono_type_get_full_name (klass);
  2918| 		mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
  2919| 		mono_error_set_member_access (cfg->error, "Cannot create an abstract class: %s", full_name);
  2920| 		g_free (full_name);
  2921| 		return NULL;
  2922| 	}
  2923| 	if (context_used) {
  2924| 		gboolean known_instance_size = !mini_is_gsharedvt_klass (klass);
  2925| 		MonoMethod *managed_alloc = mono_gc_get_managed_allocator (klass, for_box, known_instance_size);
  2926| 		iargs [0] = mini_emit_get_rgctx_klass (cfg, context_used, klass, MONO_RGCTX_INFO_VTABLE);
  2927| 		alloc_ftn = MONO_JIT_ICALL_ves_icall_object_new_specific;
  2928| 		if (managed_alloc) {
  2929| 			if (known_instance_size) {
  2930| 				int size = mono_class_instance_size (klass);
  2931| 				if (size < MONO_ABI_SIZEOF (MonoObject))
  2932| 					g_error ("Invalid size %d for class %s", size, mono_type_get_full_name (klass));
  2933| 				EMIT_NEW_ICONST (cfg, iargs [1], size);
  2934| 			}
  2935| 			return mono_emit_method_call (cfg, managed_alloc, iargs, NULL);
  2936| 		}
  2937| 		return mono_emit_jit_icall_id (cfg, alloc_ftn, iargs);
  2938| 	}
  2939| 	if (cfg->compile_aot && cfg->cbb->out_of_line && m_class_get_type_token (klass) && m_class_get_image (klass) == mono_defaults.corlib && !mono_class_is_ginst (klass)) {
  2940| 		/* This happens often in argument checking code, eg. throw new FooException... */
  2941| 		/* Avoid relocations and save some space by calling a helper function specialized to mscorlib */
  2942| 		EMIT_NEW_ICONST (cfg, iargs [0], mono_metadata_token_index (m_class_get_type_token (klass)));
  2943| 		alloc_ftn = MONO_JIT_ICALL_mono_helper_newobj_mscorlib;
  2944| 	} else {
  2945| 		MonoVTable *vtable = mono_class_vtable_checked (klass, cfg->error);
  2946| 		if (!is_ok (cfg->error)) {
  2947| 			mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
  2948| 			return NULL;
  2949| 		}
  2950| 		MonoMethod *managed_alloc = mono_gc_get_managed_allocator (klass, for_box, TRUE);
  2951| 		if (managed_alloc) {
  2952| 			int size = mono_class_instance_size (klass);
  2953| 			if (size < MONO_ABI_SIZEOF (MonoObject))
  2954| 				g_error ("Invalid size %d for class %s", size, mono_type_get_full_name (klass));
  2955| 			EMIT_NEW_VTABLECONST (cfg, iargs [0], vtable);
  2956| 			EMIT_NEW_ICONST (cfg, iargs [1], size);
  2957| 			return mono_emit_method_call (cfg, managed_alloc, iargs, NULL);
  2958| 		}
  2959| 		alloc_ftn = MONO_JIT_ICALL_ves_icall_object_new_specific;
  2960| 		EMIT_NEW_VTABLECONST (cfg, iargs [0], vtable);
  2961| 	}
  2962| 	return mono_emit_jit_icall_id (cfg, alloc_ftn, iargs);
  2963| }
  2964| /*
  2965|  * Returns NULL and set the cfg exception on error.
  2966|  */
  2967| MonoInst*
  2968| mini_emit_box (MonoCompile *cfg, MonoInst *val, MonoClass *klass, int context_used)
  2969| {
  2970| 	MonoInst *alloc, *ins;
  2971| 	if (G_UNLIKELY (m_class_is_byreflike (klass))) {
  2972| 		mono_error_set_bad_image (cfg->error, m_class_get_image (cfg->method->klass), "Cannot box IsByRefLike type '%s.%s'", m_class_get_name_space (klass), m_class_get_name (klass));
  2973| 		mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
  2974| 		return NULL;
  2975| 	}
  2976| 	if (mono_class_is_nullable (klass)) {
  2977| 		MonoMethod* method = get_method_nofail (klass, "Box", 1, 0);
  2978| 		if (context_used) {
  2979| 			if (cfg->llvm_only) {
  2980| 				MonoInst *addr;
  2981| 				MonoMethodSignature *sig = mono_method_signature_internal (method);
  2982| 				if (mini_is_gsharedvt_klass (klass))
  2983| 					addr = mini_emit_get_gsharedvt_info_klass (cfg, klass,
  2984| 															   MONO_RGCTX_INFO_NULLABLE_CLASS_BOX);
  2985| 				else
  2986| 					addr = emit_get_rgctx_method (cfg, context_used, method,
  2987| 												  MONO_RGCTX_INFO_METHOD_FTNDESC);
  2988| 				cfg->interp_in_signatures = g_slist_prepend_mempool (cfg->mempool, cfg->interp_in_signatures, sig);
  2989| 				return mini_emit_llvmonly_calli (cfg, sig, &val, addr);
  2990| 			} else {
  2991| 				/* FIXME: What if the class is shared?  We might not
  2992| 				   have to get the method address from the RGCTX. */
  2993| 				MonoInst *addr = emit_get_rgctx_method (cfg, context_used, method,
  2994| 														MONO_RGCTX_INFO_GENERIC_METHOD_CODE);
  2995| 				MonoInst *rgctx = emit_get_rgctx (cfg, context_used);
  2996| 				return mini_emit_calli (cfg, mono_method_signature_internal (method), &val, addr, NULL, rgctx);
  2997| 			}
  2998| 		} else {
  2999| 			MonoInst *rgctx_arg = NULL;
  3000| 			if (need_mrgctx_arg (cfg, method))
  3001| 				rgctx_arg = emit_get_rgctx_method (cfg, context_used, method,
  3002| 												   MONO_RGCTX_INFO_METHOD_RGCTX);
  3003| 			return mini_emit_method_call_full (cfg, method, NULL, FALSE, &val, NULL, NULL, rgctx_arg);
  3004| 		}
  3005| 	}
  3006| 	if (mini_is_gsharedvt_klass (klass)) {
  3007| 		MonoBasicBlock *is_ref_bb, *is_nullable_bb, *end_bb;
  3008| 		MonoInst *res, *is_ref, *src_var, *addr;
  3009| 		int dreg;
  3010| 		dreg = alloc_ireg (cfg);
  3011| 		NEW_BBLOCK (cfg, is_ref_bb);
  3012| 		NEW_BBLOCK (cfg, is_nullable_bb);
  3013| 		NEW_BBLOCK (cfg, end_bb);
  3014| 		is_ref = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_CLASS_BOX_TYPE);
  3015| 		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, is_ref->dreg, MONO_GSHAREDVT_BOX_TYPE_REF);
  3016| 		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBEQ, is_ref_bb);
  3017| 		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, is_ref->dreg, MONO_GSHAREDVT_BOX_TYPE_NULLABLE);
  3018| 		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBEQ, is_nullable_bb);
  3019| 		/* Non-ref case */
  3020| 		alloc = handle_alloc (cfg, klass, TRUE, context_used);
  3021| 		if (!alloc)
  3022| 			return NULL;
  3023| 		EMIT_NEW_STORE_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), alloc->dreg, MONO_ABI_SIZEOF (MonoObject), val->dreg);
  3024| 		ins->opcode = OP_STOREV_MEMBASE;
  3025| 		EMIT_NEW_UNALU (cfg, res, OP_MOVE, dreg, alloc->dreg);
  3026| 		res->type = STACK_OBJ;
  3027| 		res->klass = klass;
  3028| 		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  3029| 		/* Ref case */
  3030| 		MONO_START_BB (cfg, is_ref_bb);
  3031| 		/* val is a vtype, so has to load the value manually */
  3032| 		src_var = get_vreg_to_inst (cfg, val->dreg);
  3033| 		if (!src_var)
  3034| 			src_var = mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (klass), OP_LOCAL, val->dreg);
  3035| 		EMIT_NEW_VARLOADA (cfg, addr, src_var, src_var->inst_vtype);
  3036| 		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, dreg, addr->dreg, 0);
  3037| 		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  3038| 		/* Nullable case */
  3039| 		MONO_START_BB (cfg, is_nullable_bb);
  3040| 		{
  3041| 			MonoInst *box_addr = mini_emit_get_gsharedvt_info_klass (cfg, klass,
  3042| 													MONO_RGCTX_INFO_NULLABLE_CLASS_BOX);
  3043| 			MonoInst *box_call;
  3044| 			MonoMethodSignature *box_sig;
  3045| 			/*
  3046| 			 * klass is Nullable<T>, need to call Nullable<T>.Box () using a gsharedvt signature, but we cannot
  3047| 			 * construct that method at JIT time, so have to do things by hand.
  3048| 			 */
  3049| 			box_sig = (MonoMethodSignature *)mono_mempool_alloc0 (cfg->mempool, MONO_SIZEOF_METHOD_SIGNATURE + (1 * sizeof (MonoType *)));
  3050| 			box_sig->ret = mono_get_object_type ();
  3051| 			box_sig->param_count = 1;
  3052| 			box_sig->params [0] = m_class_get_byval_arg (klass);
  3053| 			if (cfg->llvm_only)
  3054| 				box_call = mini_emit_llvmonly_calli (cfg, box_sig, &val, box_addr);
  3055| 			else
  3056| 				box_call = mini_emit_calli (cfg, box_sig, &val, box_addr, NULL, NULL);
  3057| 			EMIT_NEW_UNALU (cfg, res, OP_MOVE, dreg, box_call->dreg);
  3058| 			res->type = STACK_OBJ;
  3059| 			res->klass = klass;
  3060| 		}
  3061| 		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  3062| 		MONO_START_BB (cfg, end_bb);
  3063| 		return res;
  3064| 	}
  3065| 	alloc = handle_alloc (cfg, klass, TRUE, context_used);
  3066| 	if (!alloc)
  3067| 		return NULL;
  3068| 	EMIT_NEW_STORE_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), alloc->dreg, MONO_ABI_SIZEOF (MonoObject), val->dreg);
  3069| 	return alloc;
  3070| }
  3071| static gboolean
  3072| method_needs_stack_walk (MonoCompile *cfg, MonoMethod *cmethod)
  3073| {
  3074| 	if (cmethod->klass == mono_defaults.systemtype_class) {
  3075| 		if (!strcmp (cmethod->name, "GetType"))
  3076| 			return TRUE;
  3077| 	}
  3078| 	/*
  3079| 	 * In corelib code, methods which need to do a stack walk declare a StackCrawlMark local and pass it as an
  3080| 	 * arguments until it reaches an icall. Its hard to detect which methods do that especially with
  3081| 	 * StackCrawlMark.LookForMyCallersCaller, so for now, just hardcode the classes which contain the public
  3082| 	 * methods whose caller is needed.
  3083| 	 */
  3084| 	if (mono_is_corlib_image (m_class_get_image (cmethod->klass))) {
  3085| 		const char *cname = m_class_get_name (cmethod->klass);
  3086| 		if (!strcmp (cname, "Assembly") ||
  3087| 			!strcmp (cname, "AssemblyLoadContext") ||
  3088| 			(!strcmp (cname, "Activator"))) {
  3089| 			if (!strcmp (cmethod->name, "op_Equality"))
  3090| 				return FALSE;
  3091| 			return TRUE;
  3092| 		}
  3093| 	}
  3094| 	return FALSE;
  3095| }
  3096| G_GNUC_UNUSED MonoInst*
  3097| mini_handle_enum_has_flag (MonoCompile *cfg, MonoClass *klass, MonoInst *enum_this, int enum_val_reg, MonoInst *enum_flag)
  3098| {
  3099| 	MonoType *enum_type = mono_type_get_underlying_type (m_class_get_byval_arg (klass));
  3100| 	guint32 load_opc = mono_type_to_load_membase (cfg, enum_type);
  3101| 	gboolean is_i4;
  3102| 	switch (enum_type->type) {
  3103| 	case MONO_TYPE_I8:
  3104| 	case MONO_TYPE_U8:
  3105| #if SIZEOF_REGISTER == 8
  3106| 	case MONO_TYPE_I:
  3107| 	case MONO_TYPE_U:
  3108| #endif
  3109| 		is_i4 = FALSE;
  3110| 		break;
  3111| 	default:
  3112| 		is_i4 = TRUE;
  3113| 		break;
  3114| 	}
  3115| 	{
  3116| 		MonoInst *load = NULL, *and_, *cmp, *ceq;
  3117| 		int enum_reg = is_i4 ? alloc_ireg (cfg) : alloc_lreg (cfg);
  3118| 		int and_reg = is_i4 ? alloc_ireg (cfg) : alloc_lreg (cfg);
  3119| 		int dest_reg = alloc_ireg (cfg);
  3120| 		if (enum_this) {
  3121| 			EMIT_NEW_LOAD_MEMBASE (cfg, load, load_opc, enum_reg, enum_this->dreg, 0);
  3122| 		} else {
  3123| 			g_assert (enum_val_reg != -1);
  3124| 			enum_reg = enum_val_reg;
  3125| 		}
  3126| 		EMIT_NEW_BIALU (cfg, and_, is_i4 ? OP_IAND : OP_LAND, and_reg, enum_reg, enum_flag->dreg);
  3127| 		EMIT_NEW_BIALU (cfg, cmp, is_i4 ? OP_ICOMPARE : OP_LCOMPARE, -1, and_reg, enum_flag->dreg);
  3128| 		EMIT_NEW_UNALU (cfg, ceq, is_i4 ? OP_ICEQ : OP_LCEQ, dest_reg, -1);
  3129| 		ceq->type = STACK_I4;
  3130| 		if (!is_i4) {
  3131| 			load = load ? mono_decompose_opcode (cfg, load) : NULL;
  3132| 			and_ = mono_decompose_opcode (cfg, and_);
  3133| 			cmp = mono_decompose_opcode (cfg, cmp);
  3134| 			ceq = mono_decompose_opcode (cfg, ceq);
  3135| 		}
  3136| 		return ceq;
  3137| 	}
  3138| }
  3139| static void
  3140| emit_set_deopt_il_offset (MonoCompile *cfg, int offset)
  3141| {
  3142| 	MonoInst *ins;
  3143| 	if (!(cfg->deopt && cfg->method == cfg->current_method))
  3144| 		return;
  3145| 	EMIT_NEW_VARLOADA (cfg, ins, cfg->il_state_var, NULL);
  3146| 	MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STOREI4_MEMBASE_IMM, ins->dreg, MONO_STRUCT_OFFSET (MonoMethodILState, il_offset), offset);
  3147| }
  3148| static MonoInst*
  3149| emit_get_rgctx_dele_tramp_info (MonoCompile *cfg, int context_used,
  3150| 								MonoClass *klass, MonoMethod *method, gboolean is_virtual, MonoRgctxInfoType rgctx_type)
  3151| {
  3152| 	MonoDelegateClassMethodPair *info;
  3153| 	MonoJumpInfoRgctxEntry *entry;
  3154| 	info = (MonoDelegateClassMethodPair *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoDelegateClassMethodPair));
  3155| 	info->klass = klass;
  3156| 	info->method = method;
  3157| 	info->is_virtual = is_virtual;
  3158| 	if (!context_used) {
  3159| 		MonoInst *ins;
  3160| 		g_assert (rgctx_type == MONO_RGCTX_INFO_DELEGATE_TRAMP_INFO);
  3161| 		if (cfg->compile_aot) {
  3162| 			EMIT_NEW_AOTCONST (cfg, ins, MONO_PATCH_INFO_DELEGATE_INFO, info);
  3163| 		} else {
  3164| 			MonoDelegateTrampInfo *tramp_info = mono_create_delegate_trampoline_info (klass, method, is_virtual);
  3165| 			EMIT_NEW_PCONST (cfg, ins, tramp_info);
  3166| 		}
  3167| 		return ins;
  3168| 	}
  3169| 	entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_DELEGATE_INFO, info, rgctx_type);
  3170| 	return emit_rgctx_fetch (cfg, context_used, entry);
  3171| }
  3172| /*
  3173|  * Returns NULL and set the cfg exception on error.
  3174|  */
  3175| static G_GNUC_UNUSED MonoInst*
  3176| handle_delegate_ctor (MonoCompile *cfg, MonoClass *klass, MonoInst *target, MonoMethod *method, int target_method_context_used, int invoke_context_used, gboolean is_virtual)
  3177| {
  3178| 	MonoInst *ptr;
  3179| 	int dreg;
  3180| 	MonoInst *obj, *info_ins;
  3181| 	if (is_virtual && !cfg->llvm_only) {
  3182| 		MonoMethod *invoke = mono_get_delegate_invoke_internal (klass);
  3183| 		g_assert (invoke);
  3184| 		if (invoke_context_used || !mono_get_delegate_virtual_invoke_impl (mono_method_signature_internal (invoke), target_method_context_used ? NULL : method))
  3185| 			return NULL;
  3186| 	}
  3187| 	obj = handle_alloc (cfg, klass, FALSE, invoke_context_used);
  3188| 	if (!obj)
  3189| 		return NULL;
  3190| 	/* Inline the contents of mini_init_delegate */
  3191| 	/* Set target field */
  3192| 	/* Optimize away setting of NULL target */
  3193| 	if (!MONO_INS_IS_PCONST_NULL (target)) {
  3194| 		if (!(method->flags & METHOD_ATTRIBUTE_STATIC)) {
  3195| 			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, target->dreg, 0);
  3196| 			MONO_EMIT_NEW_COND_EXC (cfg, EQ, "NullReferenceException");
  3197| 		}
  3198| 		if (!mini_debug_options.weak_memory_model)
  3199| 			mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
  3200| 		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, target), target->dreg);
  3201| 		if (cfg->gen_write_barriers) {
  3202| 			dreg = alloc_preg (cfg);
  3203| 			EMIT_NEW_BIALU_IMM (cfg, ptr, OP_PADD_IMM, dreg, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, target));
  3204| 			mini_emit_write_barrier (cfg, ptr, target);
  3205| 		}
  3206| 	}
  3207| 	info_ins = emit_get_rgctx_dele_tramp_info (cfg, target_method_context_used | invoke_context_used, klass, method, is_virtual, MONO_RGCTX_INFO_DELEGATE_TRAMP_INFO);
  3208| 	if (cfg->llvm_only) {
  3209| 		MonoInst *args [] = {
  3210| 			obj,
  3211| 			info_ins
  3212| 		};
  3213| 		mono_emit_jit_icall (cfg, mini_llvmonly_init_delegate, args);
  3214| 		return obj;
  3215| 	}
  3216| 	/* Set invoke_info field */
  3217| 	MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, invoke_info), info_ins->dreg);
  3218| 	/* Set method field */
  3219| 	if (target_method_context_used || invoke_context_used) {
  3220| 		dreg = alloc_preg (cfg);
  3221| 		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, dreg, info_ins->dreg, MONO_STRUCT_OFFSET (MonoDelegateTrampInfo, method));
  3222| 		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, method), dreg);
  3223| 	} else {
  3224| 		MonoInst *method_ins = emit_get_rgctx_method (cfg, target_method_context_used, method, MONO_RGCTX_INFO_METHOD);
  3225| 		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, method), method_ins->dreg);
  3226| 	}
  3227| 	/* Set invoke_impl field */
  3228| 	dreg = alloc_preg (cfg);
  3229| 	MONO_EMIT_NEW_LOAD_MEMBASE (cfg, dreg, info_ins->dreg, MONO_STRUCT_OFFSET (MonoDelegateTrampInfo, invoke_impl));
  3230| 	MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, invoke_impl), dreg);
  3231| 	if (!is_virtual) {
  3232| 		dreg = alloc_preg (cfg);
  3233| 		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, dreg, info_ins->dreg, MONO_STRUCT_OFFSET (MonoDelegateTrampInfo, method_ptr));
  3234| 		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, method_ptr), dreg);
  3235| 	}
  3236| 	if (is_virtual) {
  3237| 		dreg = alloc_preg (cfg);
  3238| 		MONO_EMIT_NEW_ICONST (cfg, dreg, 1);
  3239| 		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STOREI1_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, method_is_virtual), dreg);
  3240| 	}
  3241| 	/* All the checks which are in mono_delegate_ctor () are done by the delegate trampoline */
  3242| 	return obj;
  3243| }
  3244| /*
  3245|  * handle_constrained_gsharedvt_call:
  3246|  *
  3247|  *   Handle constrained calls where the receiver is a gsharedvt type.
  3248|  * Return the instruction representing the call. Set the cfg exception on failure.
  3249|  */
  3250| static MonoInst*
  3251| handle_constrained_gsharedvt_call (MonoCompile *cfg, MonoMethod *cmethod, MonoMethodSignature *fsig, MonoInst **sp, MonoClass *constrained_class,
  3252| 								   gboolean *ref_emit_widen)
  3253| {
  3254| 	MonoInst *ins = NULL;
  3255| 	gboolean emit_widen = *ref_emit_widen;
  3256| 	gboolean supported;
  3257| 	MonoJumpInfoVirtMethod *info;
  3258| 	MonoJumpInfoRgctxEntry *entry;
  3259| 	MonoInst *call_info_ins;
  3260| 	int context_used;
  3261| 	MonoBasicBlock *end_bb = NULL, *slowpath_bb = NULL;
  3262| 	MonoInst *calls [2];
  3263| 	MonoInst *args [7];
  3264| 	MonoInst *orig_receiver = sp [0];
  3265| 	/*
  3266| 	 * The calls are of the form:
  3267| 	 * .constrained T_GSHAREDVT
  3268| 	 * callvirt <method>
  3269| 	 *
  3270| 	 * There are 3 basic cases:
  3271| 	 * - T is a vtype and the called method is a vtype method (ie. on T).
  3272| 	 *   In this case a normal call is made.
  3273| 	 * - T is a vtype, and the called method is a method on a reference type
  3274| 	 *   (i.e. a method on Object/Valuetype/Enum)
  3275| 	 *   In this case the receiver needs to be boxed.
  3276| 	 * - T is a reference type.
  3277| 	 *   In this case, it needs to be dereferenced (since its type is T&), and
  3278| 	 *   a virtual call is made based on its runtime type.
  3279| 	 *
  3280| 	 * This is implemented by precomputing some data into an rgctx slot, then
  3281| 	 * passing that data to jit icalls.
  3282| 	 */
  3283| 	supported = ((cmethod->klass == mono_defaults.object_class) || mono_class_is_interface (cmethod->klass) || (!m_class_is_valuetype (cmethod->klass) && m_class_get_image (cmethod->klass) != mono_defaults.corlib));
  3284| 	if (supported)
  3285| 		supported = (MONO_TYPE_IS_VOID (fsig->ret) || MONO_TYPE_IS_PRIMITIVE (fsig->ret) || MONO_TYPE_IS_REFERENCE (fsig->ret) || MONO_TYPE_ISSTRUCT (fsig->ret) || m_class_is_enumtype (mono_class_from_mono_type_internal (fsig->ret)) || mini_is_gsharedvt_type (fsig->ret));
  3286| 	if (supported) {
  3287| 		if (fsig->param_count == 0 || (!fsig->hasthis && fsig->param_count == 1)) {
  3288| 			supported = TRUE;
  3289| 		} else {
  3290| 			supported = TRUE;
  3291| 			for (int i = 0; i < fsig->param_count; ++i) {
  3292| 				if (!(m_type_is_byref (fsig->params [i]) || MONO_TYPE_IS_PRIMITIVE (fsig->params [i]) || MONO_TYPE_IS_REFERENCE (fsig->params [i]) || MONO_TYPE_ISSTRUCT (fsig->params [i]) || mini_is_gsharedvt_type (fsig->params [i])))
  3293| 					supported = FALSE;
  3294| 			}
  3295| 		}
  3296| 	}
  3297| 	if (!supported)
  3298| 		GSHAREDVT_FAILURE (CEE_CALLVIRT);
  3299| 	/* rgctx entry containing precomputed data */
  3300| 	context_used = mono_method_check_context_used (cmethod) | mono_class_check_context_used (constrained_class);
  3301| 	info = (MonoJumpInfoVirtMethod *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoJumpInfoVirtMethod));
  3302| 	info->klass = constrained_class;
  3303| 	info->method = cmethod;
  3304| 	entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_VIRT_METHOD, info, MONO_RGCTX_INFO_GSHAREDVT_CONSTRAINED_CALL_INFO);
  3305| 	call_info_ins = emit_rgctx_fetch (cfg, context_used, entry);
  3306| 	/*
  3307| 	 * Fastpath: call mono_gsharedvt_constrained_call_fast, which returns
  3308| 	 * both the boxed/unboxed etc. receiver and the address to call, then
  3309| 	 * do an indirect call.
  3310| 	 */
  3311| 	calls [0] = NULL;
  3312| 	if (fsig->hasthis && (fsig->ret->type == MONO_TYPE_VOID || MONO_TYPE_IS_PRIMITIVE (fsig->ret) || MONO_TYPE_IS_REFERENCE (fsig->ret)) && !mini_is_gsharedvt_signature (fsig)) {
  3313| 		/* Call mono_gsharedvt_constrained_call_fast (receiver, info, &new_receiver) */
  3314| 		args [0] = sp [0];
  3315| 		args [1] = call_info_ins;
  3316| 		int receiver_vreg = alloc_preg (cfg);
  3317| 		MONO_EMIT_NEW_PCONST (cfg, receiver_vreg, NULL);
  3318| 		EMIT_NEW_VARLOADA_VREG (cfg, args [2], receiver_vreg, mono_get_int_type ());
  3319| 		/* This returns the address/ftndesc to call */
  3320| 		MonoInst *code_ins = mono_emit_jit_icall (cfg, mono_gsharedvt_constrained_call_fast, args);
  3321| 		NEW_BBLOCK (cfg, end_bb);
  3322| 		NEW_BBLOCK (cfg, slowpath_bb);
  3323| 		/* If NULL, go to slowpath */
  3324| 		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, code_ins->dreg, 0);
  3325| 		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, slowpath_bb);
  3326| 		/* Change the receiver to the new receiver returned by mono_gsharedvt_constrained_call_fast () */
  3327| 		int tmp_reg = alloc_preg (cfg);
  3328| 		EMIT_NEW_UNALU (cfg, ins, OP_MOVE, tmp_reg, receiver_vreg);
  3329| 		sp [0] = ins;
  3330| 		if (cfg->llvm_only)
  3331| 			calls [0] = mini_emit_llvmonly_calli (cfg, fsig, sp, code_ins);
  3332| 		else
  3333| 			calls [0] = mini_emit_calli (cfg, fsig, sp, code_ins, NULL, NULL);
  3334| 		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  3335| 		MONO_START_BB (cfg, slowpath_bb);
  3336| 	}
  3337| 	/*
  3338| 	 * Slowpath: store the arguments to an array on the stack, then call
  3339| 	 * mono_gsharedvt_constrained_call () which computes the target method and calls it using
  3340| 	 * runtime invoke.
  3341| 	 */
  3342| 	if (fsig->hasthis)
  3343| 		args [0] = orig_receiver;
  3344| 	else
  3345| 		EMIT_NEW_PCONST (cfg, args [0], NULL);
  3346| 	args [1] = emit_get_rgctx_method (cfg, mono_method_check_context_used (cmethod), cmethod, MONO_RGCTX_INFO_METHOD);
  3347| 	args [2] = mini_emit_get_rgctx_klass (cfg, mono_class_check_context_used (constrained_class), constrained_class, MONO_RGCTX_INFO_KLASS);
  3348| 	args [3] = call_info_ins;
  3349| 	MonoInst *is_gsharedvt_ins = NULL, *args_ins = NULL;
  3350| 	/* !fsig->hasthis is for the wrapper for the Object.GetType () icall or static virtual methods */
  3351| 	if ((fsig->hasthis || m_method_is_static (cmethod)) && fsig->param_count) {
  3352| 		/* Call mono_gsharedvt_constrained_call () */
  3353| 		gboolean has_gsharedvt = FALSE;
  3354| 		for (int i = 0; i < fsig->param_count; ++i) {
  3355| 			if (mini_is_gsharedvt_type (fsig->params [i]))
  3356| 				has_gsharedvt = TRUE;
  3357| 		}
  3358| 		/* Pass an array of bools which signal whenever the corresponding argument is a gsharedvt ref type */
  3359| 		if (has_gsharedvt) {
  3360| 			MONO_INST_NEW (cfg, ins, OP_LOCALLOC_IMM);
  3361| 			ins->dreg = alloc_preg (cfg);
  3362| 			ins->inst_imm = fsig->param_count;
  3363| 			MONO_ADD_INS (cfg->cbb, ins);
  3364| 			is_gsharedvt_ins = ins;
  3365| 		} else {
  3366| 			EMIT_NEW_PCONST (cfg, is_gsharedvt_ins, 0);
  3367| 		}
  3368| 		/* Pass the arguments using a localloc-ed array using the format expected by runtime_invoke () */
  3369| 		MONO_INST_NEW (cfg, ins, OP_LOCALLOC_IMM);
  3370| 		ins->dreg = alloc_preg (cfg);
  3371| 		ins->inst_imm = fsig->param_count * sizeof (target_mgreg_t);
  3372| 		MONO_ADD_INS (cfg->cbb, ins);
  3373| 		args_ins = ins;
  3374| 		for (int i = 0; i < fsig->param_count; ++i) {
  3375| 			int addr_reg;
  3376| 			if (mini_is_gsharedvt_type (fsig->params [i])) {
  3377| 				ins = mini_emit_get_gsharedvt_info_klass (cfg, mono_class_from_mono_type_internal (fsig->params [i]), MONO_RGCTX_INFO_CLASS_BOX_TYPE);
  3378| 				MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STOREI1_MEMBASE_REG, is_gsharedvt_ins->dreg, i, ins->dreg);
  3379| 			} else if (has_gsharedvt) {
  3380| 				MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STOREI1_MEMBASE_IMM, is_gsharedvt_ins->dreg, i, 0);
  3381| 			}
  3382| 			MonoInst *arg = sp [i + fsig->hasthis];
  3383| 			if (mini_is_gsharedvt_type (fsig->params [i]) || MONO_TYPE_IS_PRIMITIVE (fsig->params [i]) || MONO_TYPE_ISSTRUCT (fsig->params [i])) {
  3384| 				EMIT_NEW_VARLOADA_VREG (cfg, ins, arg->dreg, fsig->params [i]);
  3385| 				addr_reg = ins->dreg;
  3386| 				EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, args_ins->dreg, i * sizeof (target_mgreg_t), addr_reg);
  3387| 			} else {
  3388| 				EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, args_ins->dreg, i * sizeof (target_mgreg_t), arg->dreg);
  3389| 			}
  3390| 		}
  3391| 	} else {
  3392| 		EMIT_NEW_ICONST (cfg, is_gsharedvt_ins, 0);
  3393| 		EMIT_NEW_ICONST (cfg, args_ins, 0);
  3394| 	}
  3395| 	args [4] = is_gsharedvt_ins;
  3396| 	args [5] = args_ins;
  3397| 	ins = mono_emit_jit_icall (cfg, mono_gsharedvt_constrained_call, args);
  3398| 	emit_widen = FALSE;
  3399| 	/* Unbox the return value */
  3400| 	if (mini_is_gsharedvt_type (fsig->ret)) {
  3401| 		ins = handle_unbox_gsharedvt (cfg, mono_class_from_mono_type_internal (fsig->ret), ins);
  3402| 	} else if (MONO_TYPE_IS_PRIMITIVE (fsig->ret) || MONO_TYPE_ISSTRUCT (fsig->ret) || m_class_is_enumtype (mono_class_from_mono_type_internal (fsig->ret))) {
  3403| 		MonoInst *add;
  3404| 		/* Unbox */
  3405| 		NEW_BIALU_IMM (cfg, add, OP_ADD_IMM, alloc_dreg (cfg, STACK_MP), ins->dreg, MONO_ABI_SIZEOF (MonoObject));
  3406| 		MONO_ADD_INS (cfg->cbb, add);
  3407| 		/* Load value */
  3408| 		NEW_LOAD_MEMBASE_TYPE (cfg, ins, fsig->ret, add->dreg, 0);
  3409| 		MONO_ADD_INS (cfg->cbb, ins);
  3410| 	}
  3411| 	calls [1] = ins;
  3412| 	/* Merge fastpath/slowpath */
  3413| 	if (slowpath_bb) {
  3414| 		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  3415| 		MONO_START_BB (cfg, end_bb);
  3416| 	}
  3417| 	if (calls [0] && fsig->ret->type != MONO_TYPE_VOID)
  3418| 		calls [0]->dreg = calls [1]->dreg;
  3419| 	*ref_emit_widen = emit_widen;
  3420| 	return calls [1];
  3421|  exception_exit:
  3422| 	return NULL;
  3423| }
  3424| static void
  3425| mono_emit_load_got_addr (MonoCompile *cfg)
  3426| {
  3427| 	MonoInst *getaddr, *dummy_use;
  3428| 	if (!cfg->got_var || cfg->got_var_allocated)
  3429| 		return;
  3430| 	MONO_INST_NEW (cfg, getaddr, OP_LOAD_GOTADDR);
  3431| 	getaddr->cil_code = cfg->header->code;
  3432| 	getaddr->dreg = cfg->got_var->dreg;
  3433| 	/* Add it to the start of the first bblock */
  3434| 	if (cfg->bb_entry->code) {
  3435| 		getaddr->next = cfg->bb_entry->code;
  3436| 		cfg->bb_entry->code = getaddr;
  3437| 	}
  3438| 	else
  3439| 		MONO_ADD_INS (cfg->bb_entry, getaddr);
  3440| 	cfg->got_var_allocated = TRUE;
  3441| 	/*
  3442| 	 * Add a dummy use to keep the got_var alive, since real uses might
  3443| 	 * only be generated by the back ends.
  3444| 	 * Add it to end_bblock, so the variable's lifetime covers the whole
  3445| 	 * method.
  3446| 	 * It would be better to make the usage of the got var explicit in all
  3447| 	 * cases when the backend needs it (i.e. calls, throw etc.), so this
  3448| 	 * wouldn't be needed.
  3449| 	 */
  3450| 	NEW_DUMMY_USE (cfg, dummy_use, cfg->got_var);
  3451| 	MONO_ADD_INS (cfg->bb_exit, dummy_use);
  3452| }
  3453| static MonoMethod*
  3454| get_constrained_method (MonoCompile *cfg, MonoImage *image, guint32 token,
  3455| 						MonoMethod *cil_method, MonoClass *constrained_class,
  3456| 						MonoGenericContext *generic_context)
  3457| {
  3458| 	MonoMethod *cmethod = cil_method;
  3459| 	gboolean constrained_is_generic_param =
  3460| 		m_class_get_byval_arg (constrained_class)->type == MONO_TYPE_VAR ||
  3461| 		m_class_get_byval_arg (constrained_class)->type == MONO_TYPE_MVAR;
  3462| 	if (cfg->current_method->wrapper_type != MONO_WRAPPER_NONE) {
  3463| 		if (cfg->verbose_level > 2)
  3464| 			printf ("DM Constrained call to %s\n", mono_type_get_full_name (constrained_class));
  3465| 		if (!(constrained_is_generic_param &&
  3466| 			  cfg->gshared)) {
  3467| 			cmethod = mono_get_method_constrained_with_method (image, cil_method, constrained_class, generic_context, cfg->error);
  3468| 			CHECK_CFG_ERROR;
  3469| 		}
  3470| 	} else {
  3471| 		if (cfg->verbose_level > 2)
  3472| 			printf ("Constrained call to %s\n", mono_type_get_full_name (constrained_class));
  3473| 		if (constrained_is_generic_param && cfg->gshared) {
  3474| 			/*
  3475| 			 * This is needed since get_method_constrained can't find
  3476| 			 * the method in klass representing a type var.
  3477| 			 * The type var is guaranteed to be a reference type in this
  3478| 			 * case.
  3479| 			 */
  3480| 			if (!mini_is_gsharedvt_klass (constrained_class))
  3481| 				g_assert (!m_class_is_valuetype (cmethod->klass));
  3482| 		} else {
  3483| 			cmethod = mono_get_method_constrained_checked (image, token, constrained_class, generic_context, &cil_method, cfg->error);
  3484| 			CHECK_CFG_ERROR;
  3485| 		}
  3486| 	}
  3487| 	return cmethod;
  3488|  mono_error_exit:
  3489| 	return NULL;
  3490| }
  3491| static gboolean
  3492| method_does_not_return (MonoMethod *method)
  3493| {
  3494| 	return m_class_get_image (method->klass) == mono_defaults.corlib &&
  3495| 		!strcmp (m_class_get_name (method->klass), "ThrowHelper") &&
  3496| 		strstr (method->name, "Throw") == method->name &&
  3497| 		!method->is_inflated;
  3498| }
  3499| static int inline_limit, llvm_jit_inline_limit, llvm_aot_inline_limit;
  3500| static gboolean inline_limit_inited;
  3501| static gboolean
  3502| mono_method_check_inlining (MonoCompile *cfg, MonoMethod *method)
  3503| {
  3504| 	MonoMethodHeaderSummary header;
  3505| 	MonoVTable *vtable;
  3506| 	int limit;
  3507| #ifdef MONO_ARCH_SOFT_FLOAT_FALLBACK
  3508| 	MonoMethodSignature *sig = mono_method_signature_internal (method);
  3509| 	int i;
  3510| #endif
  3511| 	if (cfg->disable_inline)
  3512| 		return FALSE;
  3513| 	if (cfg->gsharedvt)
  3514| 		return FALSE;
  3515| 	if (cfg->inline_depth > 10)
  3516| 		return FALSE;
  3517| 	if (!mono_method_get_header_summary (method, &header))
  3518| 		return FALSE;
  3519| 	/*runtime, icall and pinvoke are checked by summary call*/
  3520| 	if ((method->iflags & METHOD_IMPL_ATTRIBUTE_NOINLINING) ||
  3521| 	    (method->iflags & METHOD_IMPL_ATTRIBUTE_SYNCHRONIZED) ||
  3522| 	    header.has_clauses)
  3523| 		return FALSE;
  3524| 	if (method->flags & METHOD_ATTRIBUTE_REQSECOBJ)
  3525| 		/* Used to mark methods containing StackCrawlMark locals */
  3526| 		return FALSE;
  3527| 	/* also consider num_locals? */
  3528| 	/* Do the size check early to avoid creating vtables */
  3529| 	if (!inline_limit_inited) {
  3530| 		char *inlinelimit;
  3531| 		if ((inlinelimit = g_getenv ("MONO_INLINELIMIT"))) {
  3532| 			inline_limit = atoi (inlinelimit);
  3533| 			llvm_jit_inline_limit = inline_limit;
  3534| 			llvm_aot_inline_limit = inline_limit;
  3535| 			g_free (inlinelimit);
  3536| 		} else {
  3537| 			inline_limit = INLINE_LENGTH_LIMIT;
  3538| 			llvm_jit_inline_limit = LLVM_JIT_INLINE_LENGTH_LIMIT;
  3539| 			llvm_aot_inline_limit = LLVM_AOT_INLINE_LENGTH_LIMIT;
  3540| 		}
  3541| 		inline_limit_inited = TRUE;
  3542| 	}
  3543| 	if (COMPILE_LLVM (cfg)) {
  3544| 		if (cfg->compile_aot)
  3545| 			limit = llvm_aot_inline_limit;
  3546| 		else
  3547| 			limit = llvm_jit_inline_limit;
  3548| 	} else {
  3549| 		limit = inline_limit;
  3550| 	}
  3551| 	if (header.code_size >= GINT_TO_UINT32(limit) && !(method->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING))
  3552| 		return FALSE;
  3553| 	/*
  3554| 	 * if we can initialize the class of the method right away, we do,
  3555| 	 * otherwise we don't allow inlining if the class needs initialization,
  3556| 	 * since it would mean inserting a call to mono_runtime_class_init()
  3557| 	 * inside the inlined code
  3558| 	 */
  3559| 	if (cfg->gshared && m_class_has_cctor (method->klass) && mini_class_check_context_used (cfg, method->klass))
  3560| 		return FALSE;
  3561| 	{
  3562| 		/* The AggressiveInlining hint is a good excuse to force that cctor to run. */
  3563| 		if ((cfg->opt & MONO_OPT_AGGRESSIVE_INLINING) || method->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING) {
  3564| 			if (m_class_has_cctor (method->klass)) {
  3565| 				ERROR_DECL (error);
  3566| 				vtable = mono_class_vtable_checked (method->klass, error);
  3567| 				if (!is_ok (error)) {
  3568| 					mono_error_cleanup (error);
  3569| 					return FALSE;
  3570| 				}
  3571| 				if (!cfg->compile_aot) {
  3572| 					if (!mono_runtime_class_init_full (vtable, error)) {
  3573| 						mono_error_cleanup (error);
  3574| 						return FALSE;
  3575| 					}
  3576| 				}
  3577| 			}
  3578| 		} else if (mono_class_is_before_field_init (method->klass)) {
  3579| 			if (cfg->run_cctors && m_class_has_cctor (method->klass)) {
  3580| 				ERROR_DECL (error);
  3581| 				/*FIXME it would easier and lazier to just use mono_class_try_get_vtable */
  3582| 				if (!m_class_get_runtime_vtable (method->klass))
  3583| 					/* No vtable created yet */
  3584| 					return FALSE;
  3585| 				vtable = mono_class_vtable_checked (method->klass, error);
  3586| 				if (!is_ok (error)) {
  3587| 					mono_error_cleanup (error);
  3588| 					return FALSE;
  3589| 				}
  3590| 				/* This makes so that inline cannot trigger */
  3591| 				/* .cctors: too many apps depend on them */
  3592| 				/* running with a specific order... */
  3593| 				if (! vtable->initialized)
  3594| 					return FALSE;
  3595| 				if (!mono_runtime_class_init_full (vtable, error)) {
  3596| 					mono_error_cleanup (error);
  3597| 					return FALSE;
  3598| 				}
  3599| 			}
  3600| 		} else if (mono_class_needs_cctor_run (method->klass, NULL)) {
  3601| 			ERROR_DECL (error);
  3602| 			if (!m_class_get_runtime_vtable (method->klass))
  3603| 				/* No vtable created yet */
  3604| 				return FALSE;
  3605| 			vtable = mono_class_vtable_checked (method->klass, error);
  3606| 			if (!is_ok (error)) {
  3607| 				mono_error_cleanup (error);
  3608| 				return FALSE;
  3609| 			}
  3610| 			if (!vtable->initialized)
  3611| 				return FALSE;
  3612| 		}
  3613| 	}
  3614| #ifdef MONO_ARCH_SOFT_FLOAT_FALLBACK
  3615| 	if (mono_arch_is_soft_float ()) {
  3616| 		/* FIXME: */
  3617| 		if (sig->ret && sig->ret->type == MONO_TYPE_R4)
  3618| 			return FALSE;
  3619| 		for (i = 0; i < sig->param_count; ++i)
  3620| 			if (!m_type_is_byref (sig->params [i]) && sig->params [i]->type == MONO_TYPE_R4)
  3621| 				return FALSE;
  3622| 	}
  3623| #endif
  3624| 	if (g_list_find (cfg->dont_inline, method))
  3625| 		return FALSE;
  3626| 	if (mono_profiler_get_call_instrumentation_flags (method))
  3627| 		return FALSE;
  3628| 	if (mono_profiler_coverage_instrumentation_enabled (method))
  3629| 		return FALSE;
  3630| 	if (method_does_not_return (method))
  3631| 		return FALSE;
  3632| 	return TRUE;
  3633| }
  3634| static gboolean
  3635| mini_field_access_needs_cctor_run (MonoCompile *cfg, MonoMethod *method, MonoClass *klass, MonoVTable *vtable)
  3636| {
  3637| 	if (!cfg->compile_aot) {
  3638| 		g_assert (vtable);
  3639| 		if (vtable->initialized)
  3640| 			return FALSE;
  3641| 	}
  3642| 	if (mono_class_is_before_field_init (klass)) {
  3643| 		if (cfg->method == method)
  3644| 			return FALSE;
  3645| 	}
  3646| 	if (!mono_class_needs_cctor_run (klass, method))
  3647| 		return FALSE;
  3648| 	if (! (method->flags & METHOD_ATTRIBUTE_STATIC) && (klass == method->klass))
  3649| 		/* The initialization is already done before the method is called */
  3650| 		return FALSE;
  3651| 	return TRUE;
  3652| }
  3653| int
  3654| mini_emit_sext_index_reg (MonoCompile *cfg, MonoInst *index)
  3655| {
  3656| 	int index_reg = index->dreg;
  3657| 	int index2_reg;
  3658| #if SIZEOF_REGISTER == 8
  3659| 	/* The array reg is 64 bits but the index reg is only 32 */
  3660| 	if (COMPILE_LLVM (cfg)) {
  3661| 		/*
  3662| 		 * abcrem can't handle the OP_SEXT_I4, so add this after abcrem,
  3663| 		 * during OP_BOUNDS_CHECK decomposition, and in the implementation
  3664| 		 * of OP_X86_LEA for llvm.
  3665| 		 */
  3666| 		index2_reg = index_reg;
  3667| 	} else {
  3668| 		index2_reg = alloc_preg (cfg);
  3669| 		MONO_EMIT_NEW_UNALU (cfg, OP_SEXT_I4, index2_reg, index_reg);
  3670| 	}
  3671| #else
  3672| 	if (index->type == STACK_I8) {
  3673| 		index2_reg = alloc_preg (cfg);
  3674| 		MONO_EMIT_NEW_UNALU (cfg, OP_LCONV_TO_I4, index2_reg, index_reg);
  3675| 	} else {
  3676| 		index2_reg = index_reg;
  3677| 	}
  3678| #endif
  3679| 	return index2_reg;
  3680| }
  3681| MonoInst*
  3682| mini_emit_ldelema_1_ins (MonoCompile *cfg, MonoClass *klass, MonoInst *arr, MonoInst *index, gboolean bcheck, gboolean bounded)
  3683| {
  3684| 	MonoInst *ins;
  3685| 	guint32 size;
  3686| 	int mult_reg, add_reg, array_reg, index2_reg, bounds_reg, lower_bound_reg, realidx2_reg;
  3687| 	int context_used;
  3688| 	if (mini_is_gsharedvt_variable_klass (klass)) {
  3689| 		size = -1;
  3690| 	} else {
  3691| 		mono_class_init_internal (klass);
  3692| 		size = mono_class_array_element_size (klass);
  3693| 	}
  3694| 	mult_reg = alloc_preg (cfg);
  3695| 	array_reg = arr->dreg;
  3696| 	realidx2_reg = index2_reg = mini_emit_sext_index_reg (cfg, index);
  3697| 	if (bounded) {
  3698| 		bounds_reg = alloc_preg (cfg);
  3699| 		lower_bound_reg = alloc_preg (cfg);
  3700| 		realidx2_reg = alloc_preg (cfg);
  3701| 		MonoBasicBlock *is_null_bb = NULL;
  3702| 		NEW_BBLOCK (cfg, is_null_bb);
  3703| 		MONO_EMIT_NEW_PCONST (cfg, lower_bound_reg, NULL);
  3704| 		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, bounds_reg, arr->dreg, MONO_STRUCT_OFFSET (MonoArray, bounds));
  3705| 		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, bounds_reg, 0);
  3706| 		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, is_null_bb);
  3707| 		MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADI4_MEMBASE, lower_bound_reg, bounds_reg, MONO_STRUCT_OFFSET (MonoArrayBounds, lower_bound));
  3708| 		MONO_START_BB (cfg, is_null_bb);
  3709| 		MONO_EMIT_NEW_BIALU (cfg, OP_PSUB, realidx2_reg, index2_reg, lower_bound_reg);
  3710| 	}
  3711| 	if (bcheck)
  3712| 		MONO_EMIT_BOUNDS_CHECK (cfg, array_reg, MonoArray, max_length, realidx2_reg);
  3713| #if defined(TARGET_X86) || defined(TARGET_AMD64)
  3714| 	if (size == 1 || size == 2 || size == 4 || size == 8) {
  3715| 		static const int fast_log2 [] = { 1, 0, 1, -1, 2, -1, -1, -1, 3 };
  3716| 		EMIT_NEW_X86_LEA (cfg, ins, array_reg, realidx2_reg, fast_log2 [size], MONO_STRUCT_OFFSET (MonoArray, vector));
  3717| 		ins->klass = klass;
  3718| 		ins->type = STACK_MP;
  3719| 		return ins;
  3720| 	}
  3721| #endif
  3722| 	add_reg = alloc_ireg_mp (cfg);
  3723| 	if (size == -1) {
  3724| 		MonoInst *rgctx_ins;
  3725| 		/* gsharedvt */
  3726| 		g_assert (cfg->gshared);
  3727| 		context_used = mini_class_check_context_used (cfg, klass);
  3728| 		g_assert (context_used);
  3729| 		rgctx_ins = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_ARRAY_ELEMENT_SIZE);
  3730| 		MONO_EMIT_NEW_BIALU (cfg, OP_IMUL, mult_reg, realidx2_reg, rgctx_ins->dreg);
  3731| 	} else {
  3732| 		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_MUL_IMM, mult_reg, realidx2_reg, size);
  3733| 	}
  3734| 	MONO_EMIT_NEW_BIALU (cfg, OP_PADD, add_reg, array_reg, mult_reg);
  3735| 	NEW_BIALU_IMM (cfg, ins, OP_PADD_IMM, add_reg, add_reg, MONO_STRUCT_OFFSET (MonoArray, vector));
  3736| 	ins->klass = klass;
  3737| 	ins->type = STACK_MP;
  3738| 	MONO_ADD_INS (cfg->cbb, ins);
  3739| 	return ins;
  3740| }
  3741| static MonoInst*
  3742| mini_emit_ldelema_2_ins (MonoCompile *cfg, MonoClass *klass, MonoInst *arr, MonoInst *index_ins1, MonoInst *index_ins2)
  3743| {
  3744| 	int bounds_reg = alloc_preg (cfg);
  3745| 	int add_reg = alloc_ireg_mp (cfg);
  3746| 	int mult_reg = alloc_preg (cfg);
  3747| 	int mult2_reg = alloc_preg (cfg);
  3748| 	int low1_reg = alloc_preg (cfg);
  3749| 	int low2_reg = alloc_preg (cfg);
  3750| 	int high1_reg = alloc_preg (cfg);
  3751| 	int high2_reg = alloc_preg (cfg);
  3752| 	int realidx1_reg = alloc_preg (cfg);
  3753| 	int realidx2_reg = alloc_preg (cfg);
  3754| 	int sum_reg = alloc_preg (cfg);
  3755| 	int index1, index2;
  3756| 	MonoInst *ins;
  3757| 	guint32 size;
  3758| 	mono_class_init_internal (klass);
  3759| 	size = mono_class_array_element_size (klass);
  3760| 	index1 = index_ins1->dreg;
  3761| 	index2 = index_ins2->dreg;
  3762| #if SIZEOF_REGISTER == 8
  3763| 	/* The array reg is 64 bits but the index reg is only 32 */
  3764| 	if (COMPILE_LLVM (cfg)) {
  3765| 		/* Not needed */
  3766| 	} else {
  3767| 		int tmpreg = alloc_preg (cfg);
  3768| 		MONO_EMIT_NEW_UNALU (cfg, OP_SEXT_I4, tmpreg, index1);
  3769| 		index1 = tmpreg;
  3770| 		tmpreg = alloc_preg (cfg);
  3771| 		MONO_EMIT_NEW_UNALU (cfg, OP_SEXT_I4, tmpreg, index2);
  3772| 		index2 = tmpreg;
  3773| 	}
  3774| #else
  3775| #endif
  3776| 	/* range checking */
  3777| 	MONO_EMIT_NEW_LOAD_MEMBASE_FAULT (cfg, bounds_reg,
  3778| 				       arr->dreg, MONO_STRUCT_OFFSET (MonoArray, bounds));
  3779| 	MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADI4_MEMBASE, low1_reg,
  3780| 				       bounds_reg, MONO_STRUCT_OFFSET (MonoArrayBounds, lower_bound));
  3781| 	MONO_EMIT_NEW_BIALU (cfg, OP_PSUB, realidx1_reg, index1, low1_reg);
  3782| 	MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADI4_MEMBASE, high1_reg,
  3783| 				       bounds_reg, MONO_STRUCT_OFFSET (MonoArrayBounds, length));
  3784| 	MONO_EMIT_NEW_BIALU (cfg, OP_COMPARE, -1, high1_reg, realidx1_reg);
  3785| 	MONO_EMIT_NEW_COND_EXC (cfg, LE_UN, "IndexOutOfRangeException");
  3786| 	MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADI4_MEMBASE, low2_reg,
  3787| 				       bounds_reg, sizeof (MonoArrayBounds) + MONO_STRUCT_OFFSET (MonoArrayBounds, lower_bound));
  3788| 	MONO_EMIT_NEW_BIALU (cfg, OP_PSUB, realidx2_reg, index2, low2_reg);
  3789| 	MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADI4_MEMBASE, high2_reg,
  3790| 				       bounds_reg, sizeof (MonoArrayBounds) + MONO_STRUCT_OFFSET (MonoArrayBounds, length));
  3791| 	MONO_EMIT_NEW_BIALU (cfg, OP_COMPARE, -1, high2_reg, realidx2_reg);
  3792| 	MONO_EMIT_NEW_COND_EXC (cfg, LE_UN, "IndexOutOfRangeException");
  3793| 	MONO_EMIT_NEW_BIALU (cfg, OP_PMUL, mult_reg, high2_reg, realidx1_reg);
  3794| 	MONO_EMIT_NEW_BIALU (cfg, OP_PADD, sum_reg, mult_reg, realidx2_reg);
  3795| 	MONO_EMIT_NEW_BIALU_IMM (cfg, OP_PMUL_IMM, mult2_reg, sum_reg, size);
  3796| 	MONO_EMIT_NEW_BIALU (cfg, OP_PADD, add_reg, mult2_reg, arr->dreg);
  3797| 	NEW_BIALU_IMM (cfg, ins, OP_PADD_IMM, add_reg, add_reg, MONO_STRUCT_OFFSET (MonoArray, vector));
  3798| 	ins->type = STACK_MP;
  3799| 	ins->klass = klass;
  3800| 	MONO_ADD_INS (cfg->cbb, ins);
  3801| 	return ins;
  3802| }
  3803| static MonoInst*
  3804| mini_emit_ldelema_ins (MonoCompile *cfg, MonoMethod *cmethod, MonoInst **sp, guchar *ip, gboolean is_set)
  3805| {
  3806| 	int rank;
  3807| 	MonoInst *addr;
  3808| 	MonoMethod *addr_method;
  3809| 	int element_size;
  3810| 	MonoClass *eclass = m_class_get_element_class (cmethod->klass);
  3811| 	gboolean bounded = m_class_get_byval_arg (cmethod->klass) ? m_class_get_byval_arg (cmethod->klass)->type == MONO_TYPE_ARRAY : FALSE;
  3812| 	rank = mono_method_signature_internal (cmethod)->param_count - (is_set? 1: 0);
  3813| 	if (rank == 1)
  3814| 		return mini_emit_ldelema_1_ins (cfg, eclass, sp [0], sp [1], TRUE, bounded);
  3815| 	/* emit_ldelema_2 depends on OP_LMUL */
  3816| 	if (!cfg->backend->emulate_mul_div && rank == 2 && (cfg->opt & MONO_OPT_INTRINS) && !mini_is_gsharedvt_variable_klass (eclass)) {
  3817| 		return mini_emit_ldelema_2_ins (cfg, eclass, sp [0], sp [1], sp [2]);
  3818| 	}
  3819| 	if (mini_is_gsharedvt_variable_klass (eclass))
  3820| 		element_size = 0;
  3821| 	else
  3822| 		element_size = mono_class_array_element_size (eclass);
  3823| 	addr_method = mono_marshal_get_array_address (rank, element_size);
  3824| 	addr = mono_emit_method_call (cfg, addr_method, sp, NULL);
  3825| 	return addr;
  3826| }
  3827| static gboolean
  3828| mini_class_is_reference (MonoClass *klass)
  3829| {
  3830| 	return mini_type_is_reference (m_class_get_byval_arg (klass));
  3831| }
  3832| MonoInst*
  3833| mini_emit_array_store (MonoCompile *cfg, MonoClass *klass, MonoInst **sp, gboolean safety_checks)
  3834| {
  3835| 	if (safety_checks && mini_class_is_reference (klass) &&
  3836| 		!(MONO_INS_IS_PCONST_NULL (sp [2]))) {
  3837| 		MonoClass *obj_array = mono_array_class_get_cached (mono_defaults.object_class);
  3838| 		MonoMethod *helper;
  3839| 		MonoInst *iargs [3];
  3840| 		if (sp [0]->type != STACK_OBJ)
  3841| 			return NULL;
  3842| 		if (sp [2]->type != STACK_OBJ)
  3843| 			return NULL;
  3844| 		iargs [2] = sp [2];
  3845| 		iargs [1] = sp [1];
  3846| 		iargs [0] = sp [0];
  3847| 		MonoClass *array_class = sp [0]->klass;
  3848| 		if (array_class && m_class_get_rank (array_class) == 1) {
  3849| 			MonoClass *eclass = m_class_get_element_class (array_class);
  3850| 			if (m_class_is_sealed (eclass)) {
  3851| 				helper = mono_marshal_get_virtual_stelemref (array_class);
  3852| 				/* Make a non-virtual call if possible */
  3853| 				return mono_emit_method_call (cfg, helper, iargs, NULL);
  3854| 			}
  3855| 		}
  3856| 		helper = mono_marshal_get_virtual_stelemref (obj_array);
  3857| 		if (!helper->slot)
  3858| 			mono_class_setup_vtable (obj_array);
  3859| 		g_assert (helper->slot);
  3860| 		return mono_emit_method_call (cfg, helper, iargs, sp [0]);
  3861| 	} else {
  3862| 		MonoInst *ins;
  3863| 		if (mini_is_gsharedvt_variable_klass (klass)) {
  3864| 			MonoInst *addr;
  3865| 			addr = mini_emit_ldelema_1_ins (cfg, klass, sp [0], sp [1], TRUE, FALSE);
  3866| 			EMIT_NEW_STORE_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), addr->dreg, 0, sp [2]->dreg);
  3867| 			ins->opcode = OP_STOREV_MEMBASE;
  3868| 		} else if (sp [1]->opcode == OP_ICONST) {
  3869| 			int array_reg = sp [0]->dreg;
  3870| 			int index_reg = sp [1]->dreg;
  3871| 			size_t offset = (mono_class_array_element_size (klass) * sp [1]->inst_c0) + MONO_STRUCT_OFFSET (MonoArray, vector);
  3872| 			if (SIZEOF_REGISTER == 8 && COMPILE_LLVM (cfg) && sp [1]->inst_c0 < 0)
  3873| 				MONO_EMIT_NEW_UNALU (cfg, OP_ZEXT_I4, index_reg, index_reg);
  3874| 			if (safety_checks)
  3875| 				MONO_EMIT_BOUNDS_CHECK (cfg, array_reg, MonoArray, max_length, index_reg);
  3876| 			EMIT_NEW_STORE_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), array_reg, (target_mgreg_t)offset, sp [2]->dreg);
  3877| 		} else {
  3878| 			MonoInst *addr = mini_emit_ldelema_1_ins (cfg, klass, sp [0], sp [1], safety_checks, FALSE);
  3879| 			if (!mini_debug_options.weak_memory_model && mini_class_is_reference (klass))
  3880| 				mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
  3881| 			EMIT_NEW_STORE_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), addr->dreg, 0, sp [2]->dreg);
  3882| 			if (mini_class_is_reference (klass))
  3883| 				mini_emit_write_barrier (cfg, addr, sp [2]);
  3884| 		}
  3885| 		return ins;
  3886| 	}
  3887| }
  3888| MonoInst*
  3889| mini_emit_memory_barrier (MonoCompile *cfg, int kind)
  3890| {
  3891| 	MonoInst *ins = NULL;
  3892| 	MONO_INST_NEW (cfg, ins, OP_MEMORY_BARRIER);
  3893| 	MONO_ADD_INS (cfg->cbb, ins);
  3894| 	ins->backend.memory_barrier_kind = kind;
  3895| 	return ins;
  3896| }
  3897| /*
  3898|  * This entry point could be used later for arbitrary method
  3899|  * redirection.
  3900|  */
  3901| inline static MonoInst*
  3902| mini_redirect_call (MonoCompile *cfg, MonoMethod *method,
  3903| 					MonoMethodSignature *signature, MonoInst **args, MonoInst *this_ins)
  3904| {
  3905| 	if (method->klass == mono_defaults.string_class) {
  3906| 		/* managed string allocation support */
  3907| 		if (strcmp (method->name, "FastAllocateString") == 0) {
  3908| 			MonoInst *iargs [2];
  3909| 			MonoVTable *vtable = mono_class_vtable_checked (method->klass, cfg->error);
  3910| 			MonoMethod *managed_alloc = NULL;
  3911| 			mono_error_assert_ok (cfg->error); /*Should not fail since it System.String*/
  3912| #ifndef MONO_CROSS_COMPILE
  3913| 			managed_alloc = mono_gc_get_managed_allocator (method->klass, FALSE, FALSE);
  3914| #endif
  3915| 			if (!managed_alloc)
  3916| 				return NULL;
  3917| 			EMIT_NEW_VTABLECONST (cfg, iargs [0], vtable);
  3918| 			iargs [1] = args [0];
  3919| 			return mono_emit_method_call (cfg, managed_alloc, iargs, this_ins);
  3920| 		}
  3921| 	}
  3922| 	return NULL;
  3923| }
  3924| static void
  3925| mono_save_args (MonoCompile *cfg, MonoMethodSignature *sig, MonoInst **sp)
  3926| {
  3927| 	MonoInst *store, *temp;
  3928| 	for (guint i = 0; i < sig->param_count + sig->hasthis; ++i) {
  3929| 		MonoType *argtype = (sig->hasthis && (i == 0)) ? type_from_stack_type (*sp) : sig->params [i - sig->hasthis];
  3930| 		/*
  3931| 		 * FIXME: We should use *args++ = sp [0], but that would mean the arg
  3932| 		 * would be different than the MonoInst's used to represent arguments, and
  3933| 		 * the ldelema implementation can't deal with that.
  3934| 		 * Solution: When ldelema is used on an inline argument, create a var for
  3935| 		 * it, emit ldelema on that var, and emit the saving code below in
  3936| 		 * inline_method () if needed.
  3937| 		 */
  3938| 		temp = mono_compile_create_var (cfg, argtype, OP_LOCAL);
  3939| 		cfg->args [i] = temp;
  3940| 		/* This uses cfg->args [i] which is set by the preceding line */
  3941| 		EMIT_NEW_ARGSTORE (cfg, store, i, *sp);
  3942| 		store->cil_code = sp [0]->cil_code;
  3943| 		sp++;
  3944| 	}
  3945| }
  3946| #define MONO_INLINE_CALLED_LIMITED_METHODS 1
  3947| #define MONO_INLINE_CALLER_LIMITED_METHODS 1
  3948| #if (MONO_INLINE_CALLED_LIMITED_METHODS)
  3949| static gboolean
  3950| check_inline_called_method_name_limit (MonoMethod *called_method)
  3951| {
  3952| 	int strncmp_result;
  3953| 	static const char *limit = NULL;
  3954| 	if (limit == NULL) {
  3955| 		const char *limit_string = g_getenv ("MONO_INLINE_CALLED_METHOD_NAME_LIMIT");
  3956| 		if (limit_string != NULL)
  3957| 			limit = limit_string;
  3958| 		else
  3959| 			limit = "";
  3960| 	}
  3961| 	if (limit [0] != '\0') {
  3962| 		char *called_method_name = mono_method_full_name (called_method, TRUE);
  3963| 		strncmp_result = strncmp (called_method_name, limit, strlen (limit));
  3964| 		g_free (called_method_name);
  3965| 		return (strncmp_result == 0);
  3966| 	} else {
  3967| 		return TRUE;
  3968| 	}
  3969| }
  3970| #endif
  3971| #if (MONO_INLINE_CALLER_LIMITED_METHODS)
  3972| static gboolean
  3973| check_inline_caller_method_name_limit (MonoMethod *caller_method)
  3974| {
  3975| 	int strncmp_result;
  3976| 	static const char *limit = NULL;
  3977| 	if (limit == NULL) {
  3978| 		const char *limit_string = g_getenv ("MONO_INLINE_CALLER_METHOD_NAME_LIMIT");
  3979| 		if (limit_string != NULL) {
  3980| 			limit = limit_string;
  3981| 		} else {
  3982| 			limit = "";
  3983| 		}
  3984| 	}
  3985| 	if (limit [0] != '\0') {
  3986| 		char *caller_method_name = mono_method_full_name (caller_method, TRUE);
  3987| 		strncmp_result = strncmp (caller_method_name, limit, strlen (limit));
  3988| 		g_free (caller_method_name);
  3989| 		return (strncmp_result == 0);
  3990| 	} else {
  3991| 		return TRUE;
  3992| 	}
  3993| }
  3994| #endif
  3995| void
  3996| mini_emit_init_rvar (MonoCompile *cfg, int dreg, MonoType *rtype)
  3997| {
  3998| 	static double r8_0 = 0.0;
  3999| 	static float r4_0 = 0.0;
  4000| 	MonoInst *ins;
  4001| 	int t;
  4002| 	rtype = mini_get_underlying_type (rtype);
  4003| 	t = rtype->type;
  4004| 	if (m_type_is_byref (rtype)) {
  4005| 		MONO_EMIT_NEW_PCONST (cfg, dreg, NULL);
  4006| 	} else if (t >= MONO_TYPE_BOOLEAN && t <= MONO_TYPE_U4) {
  4007| 		MONO_EMIT_NEW_ICONST (cfg, dreg, 0);
  4008| 	} else if (t == MONO_TYPE_I8 || t == MONO_TYPE_U8) {
  4009| 		MONO_EMIT_NEW_I8CONST (cfg, dreg, 0);
  4010| 	} else if (cfg->r4fp && t == MONO_TYPE_R4) {
  4011| 		MONO_INST_NEW (cfg, ins, OP_R4CONST);
  4012| 		ins->type = STACK_R4;
  4013| 		ins->inst_p0 = (void*)&r4_0;
  4014| 		ins->dreg = dreg;
  4015| 		MONO_ADD_INS (cfg->cbb, ins);
  4016| 	} else if (t == MONO_TYPE_R4 || t == MONO_TYPE_R8) {
  4017| 		MONO_INST_NEW (cfg, ins, OP_R8CONST);
  4018| 		ins->type = STACK_R8;
  4019| 		ins->inst_p0 = (void*)&r8_0;
  4020| 		ins->dreg = dreg;
  4021| 		MONO_ADD_INS (cfg->cbb, ins);
  4022| 	} else if ((t == MONO_TYPE_VALUETYPE) || (t == MONO_TYPE_TYPEDBYREF) ||
  4023| 		   ((t == MONO_TYPE_GENERICINST) && mono_type_generic_inst_is_valuetype (rtype))) {
  4024| 		MONO_EMIT_NEW_VZERO (cfg, dreg, mono_class_from_mono_type_internal (rtype));
  4025| 	} else if (((t == MONO_TYPE_VAR) || (t == MONO_TYPE_MVAR)) && mini_type_var_is_vt (rtype)) {
  4026| 		MONO_EMIT_NEW_VZERO (cfg, dreg, mono_class_from_mono_type_internal (rtype));
  4027| 	} else {
  4028| 		MONO_EMIT_NEW_PCONST (cfg, dreg, NULL);
  4029| 	}
  4030| }
  4031| static void
  4032| emit_dummy_init_rvar (MonoCompile *cfg, int dreg, MonoType *rtype)
  4033| {
  4034| 	int t;
  4035| 	rtype = mini_get_underlying_type (rtype);
  4036| 	t = rtype->type;
  4037| 	if (m_type_is_byref (rtype)) {
  4038| 		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_PCONST);
  4039| 	} else if (t >= MONO_TYPE_BOOLEAN && t <= MONO_TYPE_U4) {
  4040| 		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_ICONST);
  4041| 	} else if (t == MONO_TYPE_I8 || t == MONO_TYPE_U8) {
  4042| 		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_I8CONST);
  4043| 	} else if (cfg->r4fp && t == MONO_TYPE_R4) {
  4044| 		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_R4CONST);
  4045| 	} else if (t == MONO_TYPE_R4 || t == MONO_TYPE_R8) {
  4046| 		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_R8CONST);
  4047| 	} else if ((t == MONO_TYPE_VALUETYPE) || (t == MONO_TYPE_TYPEDBYREF) ||
  4048| 		   ((t == MONO_TYPE_GENERICINST) && mono_type_generic_inst_is_valuetype (rtype))) {
  4049| 		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_VZERO);
  4050| 	} else if (((t == MONO_TYPE_VAR) || (t == MONO_TYPE_MVAR)) && mini_type_var_is_vt (rtype)) {
  4051| 		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_VZERO);
  4052| 	} else {
  4053| 		mini_emit_init_rvar (cfg, dreg, rtype);
  4054| 	}
  4055| }
  4056| /* If INIT is FALSE, emit dummy initialization statements to keep the IR valid */
  4057| static void
  4058| emit_init_local (MonoCompile *cfg, int local, MonoType *type, gboolean init)
  4059| {
  4060| 	MonoInst *var = cfg->locals [local];
  4061| 	if (COMPILE_SOFT_FLOAT (cfg)) {
  4062| 		MonoInst *store;
  4063| 		int reg = alloc_dreg (cfg, (MonoStackType)var->type);
  4064| 		mini_emit_init_rvar (cfg, reg, type);
  4065| 		EMIT_NEW_LOCSTORE (cfg, store, local, cfg->cbb->last_ins);
  4066| 	} else {
  4067| 		if (init)
  4068| 			mini_emit_init_rvar (cfg, var->dreg, type);
  4069| 		else
  4070| 			emit_dummy_init_rvar (cfg, var->dreg, type);
  4071| 	}
  4072| }
  4073| int
  4074| mini_inline_method (MonoCompile *cfg, MonoMethod *cmethod, MonoMethodSignature *fsig, MonoInst **sp, guchar *ip, guint real_offset, gboolean inline_always)
  4075| {
  4076| 	return inline_method (cfg, cmethod, fsig, sp, ip, real_offset, inline_always, NULL);
  4077| }
  4078| /*
  4079|  * inline_method:
  4080|  *
  4081|  * Return the cost of inlining CMETHOD, or zero if it should not be inlined.
  4082|  */
  4083| static int
  4084| inline_method (MonoCompile *cfg, MonoMethod *cmethod, MonoMethodSignature *fsig, MonoInst **sp,
  4085| 			   guchar *ip, guint real_offset, gboolean inline_always, gboolean *is_empty)
  4086| {
  4087| 	ERROR_DECL (error);
  4088| 	MonoInst *ins, *rvar = NULL;
  4089| 	MonoMethodHeader *cheader;
  4090| 	MonoBasicBlock *ebblock, *sbblock;
  4091| 	int i, costs;
  4092| 	MonoInst **prev_locals, **prev_args;
  4093| 	MonoType **prev_arg_types;
  4094| 	guint prev_real_offset;
  4095| 	GHashTable *prev_cbb_hash;
  4096| 	MonoBasicBlock **prev_cil_offset_to_bb;
  4097| 	MonoBasicBlock *prev_cbb;
  4098| 	const guchar *prev_ip;
  4099| 	guchar *prev_cil_start;
  4100| 	guint32 prev_cil_offset_to_bb_len;
  4101| 	MonoMethod *prev_current_method;
  4102| 	MonoGenericContext *prev_generic_context;
  4103| 	gboolean ret_var_set, prev_ret_var_set, prev_disable_inline, virtual_ = FALSE;
  4104| 	g_assert (cfg->exception_type == MONO_EXCEPTION_NONE);
  4105| #if (MONO_INLINE_CALLED_LIMITED_METHODS)
  4106| 	if ((! inline_always) && ! check_inline_called_method_name_limit (cmethod))
  4107| 		return 0;
  4108| #endif
  4109| #if (MONO_INLINE_CALLER_LIMITED_METHODS)
  4110| 	if ((! inline_always) && ! check_inline_caller_method_name_limit (cfg->method))
  4111| 		return 0;
  4112| #endif
  4113| 	if (!fsig)
  4114| 		fsig = mono_method_signature_internal (cmethod);
  4115| 	if (cfg->verbose_level > 2)
  4116| 		printf ("INLINE START %p %s -> %s\n", cmethod,  mono_method_full_name (cfg->method, TRUE), mono_method_full_name (cmethod, TRUE));
  4117| 	if (!cmethod->inline_info) {
  4118| 		cfg->stat_inlineable_methods++;
  4119| 		cmethod->inline_info = 1;
  4120| 	}
  4121| 	if (is_empty)
  4122| 		*is_empty = FALSE;
  4123| 	/* allocate local variables */
  4124| 	cheader = mono_method_get_header_checked (cmethod, error);
  4125| 	if (!cheader) {
  4126| 		if (inline_always) {
  4127| 			mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
  4128| 			mono_error_move (cfg->error, error);
  4129| 		} else {
  4130| 			mono_error_cleanup (error);
  4131| 		}
  4132| 		return 0;
  4133| 	}
  4134| 	if (is_empty && cheader->code_size == 1 && cheader->code [0] == CEE_RET)
  4135| 		*is_empty = TRUE;
  4136| 	/* allocate space to store the return value */
  4137| 	if (!MONO_TYPE_IS_VOID (fsig->ret)) {
  4138| 		rvar = mono_compile_create_var (cfg, fsig->ret, OP_LOCAL);
  4139| 	}
  4140| 	prev_locals = cfg->locals;
  4141| 	cfg->locals = (MonoInst **)mono_mempool_alloc0 (cfg->mempool, cheader->num_locals * sizeof (MonoInst*));
  4142| 	for (i = 0; i < cheader->num_locals; ++i)
  4143| 		cfg->locals [i] = mono_compile_create_var (cfg, cheader->locals [i], OP_LOCAL);
  4144| 	/* allocate start and end blocks */
  4145| 	/* This is needed so if the inline is aborted, we can clean up */
  4146| 	NEW_BBLOCK (cfg, sbblock);
  4147| 	sbblock->real_offset = real_offset;
  4148| 	NEW_BBLOCK (cfg, ebblock);
  4149| 	ebblock->block_num = cfg->num_bblocks++;
  4150| 	ebblock->real_offset = real_offset;
  4151| 	prev_args = cfg->args;
  4152| 	prev_arg_types = cfg->arg_types;
  4153| 	prev_ret_var_set = cfg->ret_var_set;
  4154| 	prev_real_offset = cfg->real_offset;
  4155| 	prev_cbb_hash = cfg->cbb_hash;
  4156| 	prev_cil_offset_to_bb = cfg->cil_offset_to_bb;
  4157| 	prev_cil_offset_to_bb_len = cfg->cil_offset_to_bb_len;
  4158| 	prev_cil_start = cfg->cil_start;
  4159| 	prev_ip = cfg->ip;
  4160| 	prev_cbb = cfg->cbb;
  4161| 	prev_current_method = cfg->current_method;
  4162| 	prev_generic_context = cfg->generic_context;
  4163| 	prev_disable_inline = cfg->disable_inline;
  4164| 	cfg->ret_var_set = FALSE;
  4165| 	cfg->inline_depth ++;
  4166| 	if (ip && *ip == CEE_CALLVIRT && !(cmethod->flags & METHOD_ATTRIBUTE_STATIC))
  4167| 		virtual_ = TRUE;
  4168| 	costs = mono_method_to_ir (cfg, cmethod, sbblock, ebblock, rvar, sp, real_offset, virtual_);
  4169| 	ret_var_set = cfg->ret_var_set;
  4170| 	cfg->real_offset = prev_real_offset;
  4171| 	cfg->cbb_hash = prev_cbb_hash;
  4172| 	cfg->cil_offset_to_bb = prev_cil_offset_to_bb;
  4173| 	cfg->cil_offset_to_bb_len = prev_cil_offset_to_bb_len;
  4174| 	cfg->cil_start = prev_cil_start;
  4175| 	cfg->ip = prev_ip;
  4176| 	cfg->locals = prev_locals;
  4177| 	cfg->args = prev_args;
  4178| 	cfg->arg_types = prev_arg_types;
  4179| 	cfg->current_method = prev_current_method;
  4180| 	cfg->generic_context = prev_generic_context;
  4181| 	cfg->ret_var_set = prev_ret_var_set;
  4182| 	cfg->disable_inline = prev_disable_inline;
  4183| 	cfg->inline_depth --;
  4184| 	if ((costs >= 0 && costs < 60) || inline_always || (costs >= 0 && (cmethod->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING))) {
  4185| 		if (cfg->verbose_level > 2)
  4186| 			printf ("INLINE END %s -> %s\n", mono_method_full_name (cfg->method, TRUE), mono_method_full_name (cmethod, TRUE));
  4187| 		mono_error_assert_ok (cfg->error);
  4188| 		cfg->stat_inlined_methods++;
  4189| 		/* always add some code to avoid block split failures */
  4190| 		MONO_INST_NEW (cfg, ins, OP_NOP);
  4191| 		MONO_ADD_INS (prev_cbb, ins);
  4192| 		prev_cbb->next_bb = sbblock;
  4193| 		link_bblock (cfg, prev_cbb, sbblock);
  4194| 		/*
  4195| 		 * Get rid of the begin and end bblocks if possible to aid local
  4196| 		 * optimizations.
  4197| 		 */
  4198| 		if (prev_cbb->out_count == 1)
  4199| 			mono_merge_basic_blocks (cfg, prev_cbb, sbblock);
  4200| 		if ((prev_cbb->out_count == 1) && (prev_cbb->out_bb [0]->in_count == 1) && (prev_cbb->out_bb [0] != ebblock))
  4201| 			mono_merge_basic_blocks (cfg, prev_cbb, prev_cbb->out_bb [0]);
  4202| 		if ((ebblock->in_count == 1) && ebblock->in_bb [0]->out_count == 1) {
  4203| 			MonoBasicBlock *prev = ebblock->in_bb [0];
  4204| 			if (prev->next_bb == ebblock) {
  4205| 				mono_merge_basic_blocks (cfg, prev, ebblock);
  4206| 				cfg->cbb = prev;
  4207| 				if ((prev_cbb->out_count == 1) && (prev_cbb->out_bb [0]->in_count == 1) && (prev_cbb->out_bb [0] == prev)) {
  4208| 					mono_merge_basic_blocks (cfg, prev_cbb, prev);
  4209| 					cfg->cbb = prev_cbb;
  4210| 				}
  4211| 			} else {
  4212| 				/* There could be a bblock after 'prev', and making 'prev' the current bb could cause problems */
  4213| 				cfg->cbb = ebblock;
  4214| 			}
  4215| 		} else {
  4216| 			/*
  4217| 			 * Its possible that the rvar is set in some prev bblock, but not in others.
  4218| 			 * (#1835).
  4219| 			 */
  4220| 			if (rvar) {
  4221| 				MonoBasicBlock *bb;
  4222| 				for (i = 0; i < ebblock->in_count; ++i) {
  4223| 					bb = ebblock->in_bb [i];
  4224| 					if (bb->last_ins && bb->last_ins->opcode == OP_NOT_REACHED) {
  4225| 						cfg->cbb = bb;
  4226| 						mini_emit_init_rvar (cfg, rvar->dreg, fsig->ret);
  4227| 					}
  4228| 				}
  4229| 			}
  4230| 			cfg->cbb = ebblock;
  4231| 		}
  4232| 		if (rvar) {
  4233| 			/*
  4234| 			 * If the inlined method contains only a throw, then the ret var is not
  4235| 			 * set, so set it to a dummy value.
  4236| 			 */
  4237| 			if (!ret_var_set)
  4238| 				mini_emit_init_rvar (cfg, rvar->dreg, fsig->ret);
  4239| 			EMIT_NEW_TEMPLOAD (cfg, ins, rvar->inst_c0);
  4240| 			*sp++ = ins;
  4241| 		}
  4242| 		cfg->headers_to_free = g_slist_prepend_mempool (cfg->mempool, cfg->headers_to_free, cheader);
  4243| 		return costs + 1;
  4244| 	} else {
  4245| 		if (cfg->verbose_level > 2) {
  4246| 			const char *msg = mono_error_get_message (cfg->error);
  4247| 			printf ("INLINE ABORTED %s (cost %d) %s\n", mono_method_full_name (cmethod, TRUE), costs, msg ? msg : "");
  4248| 		}
  4249| 		cfg->exception_type = MONO_EXCEPTION_NONE;
  4250| 		clear_cfg_error (cfg);
  4251| 		/* This gets rid of the newly added bblocks */
  4252| 		cfg->cbb = prev_cbb;
  4253| 	}
  4254| 	cfg->headers_to_free = g_slist_prepend_mempool (cfg->mempool, cfg->headers_to_free, cheader);
  4255| 	return 0;
  4256| }
  4257| /*
  4258|  * Some of these comments may well be out-of-date.
  4259|  * Design decisions: we do a single pass over the IL code (and we do bblock
  4260|  * splitting/merging in the few cases when it's required: a back jump to an IL
  4261|  * address that was not already seen as bblock starting point).
  4262|  * Code is validated as we go (full verification is still better left to metadata/verify.c).
  4263|  * Complex operations are decomposed in simpler ones right away. We need to let the
  4264|  * arch-specific code peek and poke inside this process somehow (except when the
  4265|  * optimizations can take advantage of the full semantic info of coarse opcodes).
  4266|  * All the opcodes of the form opcode.s are 'normalized' to opcode.
  4267|  * MonoInst->opcode initially is the IL opcode or some simplification of that
  4268|  * (OP_LOAD, OP_STORE). The arch-specific code may rearrange it to an arch-specific
  4269|  * opcode with value bigger than OP_LAST.
  4270|  * At this point the IR can be handed over to an interpreter, a dumb code generator
  4271|  * or to the optimizing code generator that will translate it to SSA form.
  4272|  *
  4273|  * Profiling directed optimizations.
  4274|  * We may compile by default with few or no optimizations and instrument the code
  4275|  * or the user may indicate what methods to optimize the most either in a config file
  4276|  * or through repeated runs where the compiler applies offline the optimizations to
  4277|  * each method and then decides if it was worth it.
  4278|  */
  4279| #define CHECK_TYPE(ins) if (!(ins)->type) UNVERIFIED
  4280| #define CHECK_STACK(num) if ((sp - stack_start) < (num)) UNVERIFIED
  4281| #define CHECK_STACK_OVF() if (((sp - stack_start) + 1) > header->max_stack) UNVERIFIED
  4282| #define CHECK_ARG(num) if ((unsigned)(num) >= (unsigned)num_args) UNVERIFIED
  4283| #define CHECK_LOCAL(num) if ((unsigned)(num) >= (unsigned)header->num_locals) UNVERIFIED
  4284| #define CHECK_OPSIZE(size) if ((size) < 1 || ip + (size) > end) UNVERIFIED
  4285| #define CHECK_UNVERIFIABLE(cfg) if (cfg->unverifiable) UNVERIFIED
  4286| #define CHECK_TYPELOAD(klass) if (!(klass) || mono_class_has_failure (klass)) TYPE_LOAD_ERROR ((klass))
  4287| /* offset from br.s -> br like opcodes */
  4288| #define BIG_BRANCH_OFFSET 13
  4289| static gboolean
  4290| ip_in_bb (MonoCompile *cfg, MonoBasicBlock *bb, const guint8* ip)
  4291| {
  4292| 	MonoBasicBlock *b = cfg->cil_offset_to_bb [ip - cfg->cil_start];
  4293| 	return b == NULL || b == bb;
  4294| }
  4295| static int
  4296| get_basic_blocks (MonoCompile *cfg, MonoMethodHeader* header, guint real_offset, guchar *start, guchar *end, guchar **pos)
  4297| {
  4298| 	guchar *ip = start;
  4299| 	guchar *target;
  4300| 	int i;
  4301| 	guint cli_addr;
  4302| 	MonoBasicBlock *bblock;
  4303| 	const MonoOpcode *opcode;
  4304| 	while (ip < end) {
  4305| 		cli_addr = GPTRDIFF_TO_UINT (ip - start);
  4306| 		i = mono_opcode_value ((const guint8 **)&ip, end);
  4307| 		if (i < 0)
  4308| 			UNVERIFIED;
  4309| 		opcode = &mono_opcodes [i];
  4310| 		switch (opcode->argument) {
  4311| 		case MonoInlineNone:
  4312| 			ip++;
  4313| 			break;
  4314| 		case MonoInlineString:
  4315| 		case MonoInlineType:
  4316| 		case MonoInlineField:
  4317| 		case MonoInlineMethod:
  4318| 		case MonoInlineTok:
  4319| 		case MonoInlineSig:
  4320| 		case MonoShortInlineR:
  4321| 		case MonoInlineI:
  4322| 			ip += 5;
  4323| 			break;
  4324| 		case MonoInlineVar:
  4325| 			ip += 3;
  4326| 			break;
  4327| 		case MonoShortInlineVar:
  4328| 		case MonoShortInlineI:
  4329| 			ip += 2;
  4330| 			break;
  4331| 		case MonoShortInlineBrTarget:
  4332| 			target = start + cli_addr + 2 + (signed char)ip [1];
  4333| 			GET_BBLOCK (cfg, bblock, target);
  4334| 			ip += 2;
  4335| 			if (ip < end)
  4336| 				GET_BBLOCK (cfg, bblock, ip);
  4337| 			break;
  4338| 		case MonoInlineBrTarget:
  4339| 			target = start + cli_addr + 5 + (gint32)read32 (ip + 1);
  4340| 			GET_BBLOCK (cfg, bblock, target);
  4341| 			ip += 5;
  4342| 			if (ip < end)
  4343| 				GET_BBLOCK (cfg, bblock, ip);
  4344| 			break;
  4345| 		case MonoInlineSwitch: {
  4346| 			guint32 n = read32 (ip + 1);
  4347| 			guint32 j;
  4348| 			ip += 5;
  4349| 			cli_addr += 5 + 4 * n;
  4350| 			target = start + cli_addr;
  4351| 			GET_BBLOCK (cfg, bblock, target);
  4352| 			for (j = 0; j < n; ++j) {
  4353| 				target = start + cli_addr + (gint32)read32 (ip);
  4354| 				GET_BBLOCK (cfg, bblock, target);
  4355| 				ip += 4;
  4356| 			}
  4357| 			break;
  4358| 		}
  4359| 		case MonoInlineR:
  4360| 		case MonoInlineI8:
  4361| 			ip += 9;
  4362| 			break;
  4363| 		default:
  4364| 			g_assert_not_reached ();
  4365| 		}
  4366| 		if (i == CEE_THROW) {
  4367| 			guchar *bb_start = ip - 1;
  4368| 			/* Find the start of the bblock containing the throw */
  4369| 			bblock = NULL;
  4370| 			while ((bb_start >= start) && !bblock) {
  4371| 				bblock = cfg->cil_offset_to_bb [(bb_start) - start];
  4372| 				bb_start --;
  4373| 			}
  4374| 			if (bblock)
  4375| 				bblock->out_of_line = 1;
  4376| 		}
  4377| 	}
  4378| 	return 0;
  4379| unverified:
  4380| exception_exit:
  4381| 	*pos = ip;
  4382| 	return 1;
  4383| }
  4384| static MonoMethod *
  4385| mini_get_method_allow_open (MonoMethod *m, guint32 token, MonoClass *klass, MonoGenericContext *context, MonoError *error)
  4386| {
  4387| 	MonoMethod *method;
  4388| 	error_init (error);
  4389| 	if (m->wrapper_type != MONO_WRAPPER_NONE) {
  4390| 		method = (MonoMethod *)mono_method_get_wrapper_data (m, token);
  4391| 		if (context) {
  4392| 			method = mono_class_inflate_generic_method_checked (method, context, error);
  4393| 		}
  4394| 	} else {
  4395| 		method = mono_get_method_checked (m_class_get_image (m->klass), token, klass, context, error);
  4396| 	}
  4397| 	return method;
  4398| }
  4399| static MonoMethod *
  4400| mini_get_method (MonoCompile *cfg, MonoMethod *m, guint32 token, MonoClass *klass, MonoGenericContext *context)
  4401| {
  4402| 	ERROR_DECL (error);
  4403| 	MonoMethod *method = mini_get_method_allow_open (m, token, klass, context, cfg ? cfg->error : error);
  4404| 	if (method && cfg && !cfg->gshared && mono_class_is_open_constructed_type (m_class_get_byval_arg (method->klass))) {
  4405| 		mono_error_set_bad_image (cfg->error, m_class_get_image (cfg->method->klass), "Method with open type while not compiling gshared");
  4406| 		method = NULL;
  4407| 	}
  4408| 	if (!method && !cfg)
  4409| 		mono_error_cleanup (error); /* FIXME don't swallow the error */
  4410| 	return method;
  4411| }
  4412| static MonoMethodSignature*
  4413| mini_get_signature (MonoMethod *method, guint32 token, MonoGenericContext *context, MonoError *error)
  4414| {
  4415| 	MonoMethodSignature *fsig;
  4416| 	error_init (error);
  4417| 	if (method->wrapper_type != MONO_WRAPPER_NONE) {
  4418| 		fsig = (MonoMethodSignature *)mono_method_get_wrapper_data (method, token);
  4419| 	} else {
  4420| 		fsig = mono_metadata_parse_signature_checked (m_class_get_image (method->klass), token, error);
  4421| 		return_val_if_nok (error, NULL);
  4422| 	}
  4423| 	if (context) {
  4424| 		fsig = mono_inflate_generic_signature(fsig, context, error);
  4425| 	}
  4426| 	return fsig;
  4427| }
  4428| /*
  4429|  * Return the original method is a wrapper is specified. We can only access
  4430|  * the custom attributes from the original method.
  4431|  */
  4432| static MonoMethod*
  4433| get_original_method (MonoMethod *method)
  4434| {
  4435| 	if (method->wrapper_type == MONO_WRAPPER_NONE)
  4436| 		return method;
  4437| 	/* native code (which is like Critical) can call any managed method XXX FIXME XXX to validate all usages */
  4438| 	if (method->wrapper_type == MONO_WRAPPER_NATIVE_TO_MANAGED)
  4439| 		return NULL;
  4440| 	/* in other cases we need to find the original method */
  4441| 	return mono_marshal_method_from_wrapper (method);
  4442| }
  4443| static guchar*
  4444| il_read_op (guchar *ip, guchar *end, guchar first_byte, MonoOpcodeEnum desired_il_op)
  4445| {
  4446| 	if (G_LIKELY (ip < end) && G_UNLIKELY (*ip == first_byte)) {
  4447| 		MonoOpcodeEnum il_op = MonoOpcodeEnum_Invalid;
  4448| 		const guchar *temp_ip = ip;
  4449| 		const int size = mono_opcode_value_and_size (&temp_ip, end, &il_op);
  4450| 		return (G_LIKELY (size > 0) && G_UNLIKELY (il_op == desired_il_op)) ? (ip + size) : NULL;
  4451| 	}
  4452| 	return NULL;
  4453| }
  4454| static guchar*
  4455| il_read_op_and_token (guchar *ip, guchar *end, guchar first_byte, MonoOpcodeEnum desired_il_op, guint32 *token)
  4456| {
  4457| 	ip = il_read_op (ip, end, first_byte, desired_il_op);
  4458| 	if (ip)
  4459| 		*token = read32 (ip - 4); // could be +1 or +2 from start
  4460| 	return ip;
  4461| }
  4462| static guchar*
  4463| il_read_branch_and_target (guchar *ip, guchar *end, guchar first_byte, MonoOpcodeEnum desired_il_op, int size, guchar **target)
  4464| {
  4465| 	ip = il_read_op (ip, end, first_byte, desired_il_op);
  4466| 	if (ip) {
  4467| 		gint32 delta = 0;
  4468| 		switch (size) {
  4469| 		case  1:
  4470| 			delta = (signed char)ip [-1];
  4471| 			break;
  4472| 		case  4:
  4473| 			delta = (gint32)read32 (ip - 4);
  4474| 			break;
  4475| 		}
  4476| 		*target = ip + delta;
  4477| 		return ip;
  4478| 	}
  4479| 	return NULL;
  4480| }
  4481| #define il_read_brtrue(ip, end, target) 	(il_read_branch_and_target (ip, end, CEE_BRTRUE,    MONO_CEE_BRTRUE,    4, target))
  4482| #define il_read_brtrue_s(ip, end, target) 	(il_read_branch_and_target (ip, end, CEE_BRTRUE_S,  MONO_CEE_BRTRUE_S,  1, target))
  4483| #define il_read_brfalse(ip, end, target) 	(il_read_branch_and_target (ip, end, CEE_BRFALSE,   MONO_CEE_BRFALSE,   4, target))
  4484| #define il_read_brfalse_s(ip, end, target) 	(il_read_branch_and_target (ip, end, CEE_BRFALSE_S, MONO_CEE_BRFALSE_S, 1, target))
  4485| #define il_read_dup(ip, end) 			(il_read_op 		   (ip, end, CEE_DUP, MONO_CEE_DUP))
  4486| #define il_read_newobj(ip, end, token) 		(il_read_op_and_token 	   (ip, end, CEE_NEW_OBJ, MONO_CEE_NEWOBJ, token))
  4487| #define il_read_ldtoken(ip, end, token) 	(il_read_op_and_token 	   (ip, end, CEE_LDTOKEN, MONO_CEE_LDTOKEN, token))
  4488| #define il_read_call(ip, end, token) 		(il_read_op_and_token      (ip, end, CEE_CALL, MONO_CEE_CALL, token))
  4489| #define il_read_callvirt(ip, end, token)	(il_read_op_and_token 	   (ip, end, CEE_CALLVIRT, MONO_CEE_CALLVIRT, token))
  4490| #define il_read_initobj(ip, end, token)         (il_read_op_and_token 	   (ip, end, CEE_PREFIX1, MONO_CEE_INITOBJ, token))
  4491| #define il_read_constrained(ip, end, token)     (il_read_op_and_token      (ip, end, CEE_PREFIX1, MONO_CEE_CONSTRAINED_, token))
  4492| #define il_read_unbox_any(ip, end, token)     (il_read_op_and_token      (ip, end, CEE_UNBOX_ANY, MONO_CEE_UNBOX_ANY, token))
  4493| /*
  4494|  * Check that the IL instructions at ip are the array initialization
  4495|  * sequence and return the pointer to the data and the size.
  4496|  */
  4497| static const char*
  4498| initialize_array_data (MonoCompile *cfg, MonoMethod *method, gboolean aot, guchar *ip,
  4499| 		guchar *end, MonoClass *klass, guint32 len, int *out_size,
  4500| 		guint32 *out_field_token, MonoOpcodeEnum *il_op, guchar **next_ip)
  4501| {
  4502| 	/*
  4503| 	 * newarr[System.Int32]
  4504| 	 * dup
  4505| 	 * ldtoken field valuetype ...
  4506| 	 * call void class [mscorlib]System.Runtime.CompilerServices.RuntimeHelpers::InitializeArray(class [mscorlib]System.Array, valuetype [mscorlib]System.RuntimeFieldHandle)
  4507| 	 */
  4508| 	guint32 token;
  4509| 	guint32 field_token;
  4510| 	if  ((ip = il_read_dup (ip, end))
  4511| 			&& ip_in_bb (cfg, cfg->cbb, ip)
  4512| 			&& (ip = il_read_ldtoken (ip, end, &field_token))
  4513| 			&& IS_FIELD_DEF (field_token)
  4514| 			&& ip_in_bb (cfg, cfg->cbb, ip)
  4515| 			&& (ip = il_read_call (ip, end, &token))) {
  4516| 		ERROR_DECL (error);
  4517| 		guint32 rva;
  4518| 		const char *data_ptr;
  4519| 		int size = 0;
  4520| 		MonoMethod *cmethod;
  4521| 		MonoClass *dummy_class;
  4522| 		MonoClassField *field = mono_field_from_token_checked (m_class_get_image (method->klass), field_token, &dummy_class, NULL, error);
  4523| 		int dummy_align;
  4524| 		if (!field) {
  4525| 			mono_error_cleanup (error); /* FIXME don't swallow the error */
  4526| 			return NULL;
  4527| 		}
  4528| 		*out_field_token = field_token;
  4529| 		cmethod = mini_get_method (NULL, method, token, NULL, NULL);
  4530| 		if (!cmethod)
  4531| 			return NULL;
  4532| 		if (strcmp (cmethod->name, "InitializeArray") || strcmp (m_class_get_name (cmethod->klass), "RuntimeHelpers") || m_class_get_image (cmethod->klass) != mono_defaults.corlib)
  4533| 			return NULL;
  4534| 		switch (mini_get_underlying_type (m_class_get_byval_arg (klass))->type) {
  4535| 		case MONO_TYPE_I1:
  4536| 		case MONO_TYPE_U1:
  4537| 			size = 1; break;
  4538| 		/* we need to swap on big endian, so punt. Should we handle R4 and R8 as well? */
  4539| #if TARGET_BYTE_ORDER == G_LITTLE_ENDIAN
  4540| 		case MONO_TYPE_I2:
  4541| 		case MONO_TYPE_U2:
  4542| 			size = 2; break;
  4543| 		case MONO_TYPE_I4:
  4544| 		case MONO_TYPE_U4:
  4545| 		case MONO_TYPE_R4:
  4546| 			size = 4; break;
  4547| 		case MONO_TYPE_R8:
  4548| 		case MONO_TYPE_I8:
  4549| 		case MONO_TYPE_U8:
  4550| 			size = 8; break;
  4551| #endif
  4552| 		default:
  4553| 			return NULL;
  4554| 		}
  4555| 		size *= len;
  4556| 		if (size > mono_type_size (field->type, &dummy_align))
  4557| 		    return NULL;
  4558| 		*out_size = size;
  4559| 		/*g_print ("optimized in %s: size: %d, numelems: %d\n", method->name, size, newarr->inst_newa_len->inst_c0);*/
  4560| 		MonoImage *method_klass_image = m_class_get_image (method->klass);
  4561| 		if (!image_is_dynamic (method_klass_image)) {
  4562| 			guint32 field_index = mono_metadata_token_index (field_token);
  4563| 			mono_metadata_field_info (method_klass_image, field_index - 1, NULL, &rva, NULL);
  4564| 			data_ptr = mono_image_rva_map (method_klass_image, rva);
  4565| 			/*g_print ("field: 0x%08x, rva: %d, rva_ptr: %p\n", read32 (ip + 2), rva, data_ptr);*/
  4566| 			/* for aot code we do the lookup on load */
  4567| 			if (aot && data_ptr)
  4568| 				data_ptr = (const char *)GUINT_TO_POINTER (rva);
  4569| 		} else {
  4570| 			/*FIXME is it possible to AOT a SRE assembly not meant to be saved? */
  4571| 			g_assert (!aot);
  4572| 			data_ptr = mono_field_get_data (field);
  4573| 		}
  4574| 		if (!data_ptr)
  4575| 			return NULL;
  4576| 		*il_op = MONO_CEE_CALL;
  4577| 		*next_ip = ip;
  4578| 		return data_ptr;
  4579| 	}
  4580| 	return NULL;
  4581| }
  4582| static void
  4583| set_exception_type_from_invalid_il (MonoCompile *cfg, MonoMethod *method, guchar *ip)
  4584| {
  4585| 	ERROR_DECL (error);
  4586| 	char *method_fname = mono_method_full_name (method, TRUE);
  4587| 	char *method_code;
  4588| 	MonoMethodHeader *header = mono_method_get_header_checked (method, error);
  4589| 	if (!header) {
  4590| 		method_code = g_strdup_printf ("could not parse method body due to %s", mono_error_get_message (error));
  4591| 		mono_error_cleanup (error);
  4592| 	} else if (header->code_size == 0)
  4593| 		method_code = g_strdup ("method body is empty.");
  4594| 	else
  4595| 		method_code = mono_disasm_code_one (NULL, method, ip, NULL);
  4596| 	mono_cfg_set_exception_invalid_program (cfg, g_strdup_printf ("Invalid IL code in %s: %s\n", method_fname, method_code));
  4597|  	g_free (method_fname);
  4598|  	g_free (method_code);
  4599| 	cfg->headers_to_free = g_slist_prepend_mempool (cfg->mempool, cfg->headers_to_free, header);
  4600| }
  4601| guint32
  4602| mono_type_to_stloc_coerce (MonoType *type)
  4603| {
  4604| 	if (m_type_is_byref (type))
  4605| 		return 0;
  4606| 	type = mini_get_underlying_type (type);
  4607| handle_enum:
  4608| 	switch (type->type) {
  4609| 	case MONO_TYPE_I1:
  4610| 		return OP_ICONV_TO_I1;
  4611| 	case MONO_TYPE_U1:
  4612| 		return OP_ICONV_TO_U1;
  4613| 	case MONO_TYPE_I2:
  4614| 		return OP_ICONV_TO_I2;
  4615| 	case MONO_TYPE_U2:
  4616| 		return OP_ICONV_TO_U2;
  4617| 	case MONO_TYPE_I4:
  4618| 	case MONO_TYPE_U4:
  4619| 	case MONO_TYPE_I:
  4620| 	case MONO_TYPE_U:
  4621| 	case MONO_TYPE_PTR:
  4622| 	case MONO_TYPE_FNPTR:
  4623| 	case MONO_TYPE_CLASS:
  4624| 	case MONO_TYPE_STRING:
  4625| 	case MONO_TYPE_OBJECT:
  4626| 	case MONO_TYPE_SZARRAY:
  4627| 	case MONO_TYPE_ARRAY:
  4628| 	case MONO_TYPE_I8:
  4629| 	case MONO_TYPE_U8:
  4630| 	case MONO_TYPE_R4:
  4631| 	case MONO_TYPE_R8:
  4632| 	case MONO_TYPE_TYPEDBYREF:
  4633| 	case MONO_TYPE_GENERICINST:
  4634| 		return 0;
  4635| 	case MONO_TYPE_VALUETYPE:
  4636| 		if (m_class_is_enumtype (type->data.klass)) {
  4637| 			type = mono_class_enum_basetype_internal (type->data.klass);
  4638| 			goto handle_enum;
  4639| 		}
  4640| 		return 0;
  4641| 	case MONO_TYPE_VAR:
  4642| 	case MONO_TYPE_MVAR: //TODO I believe we don't need to handle gsharedvt as there won't be match and, for example, u1 is not covariant to u32
  4643| 		return 0;
  4644| 	default:
  4645| 		g_error ("unknown type 0x%02x in mono_type_to_stloc_coerce", type->type);
  4646| 	}
  4647| 	return -1;
  4648| }
  4649| static void
  4650| emit_stloc_ir (MonoCompile *cfg, MonoInst **sp, MonoMethodHeader *header, int n)
  4651| {
  4652| 	MonoInst *ins;
  4653| 	guint32 coerce_op = mono_type_to_stloc_coerce (header->locals [n]);
  4654| 	if (coerce_op) {
  4655| 		if (cfg->cbb->last_ins == sp [0] && sp [0]->opcode == coerce_op) {
  4656| 			if (cfg->verbose_level > 2)
  4657| 				printf ("Found existing coercing is enough for stloc\n");
  4658| 		} else {
  4659| 			MONO_INST_NEW (cfg, ins, coerce_op);
  4660| 			ins->dreg = alloc_ireg (cfg);
  4661| 			ins->sreg1 = sp [0]->dreg;
  4662| 			ins->type = STACK_I4;
  4663| 			ins->klass = mono_class_from_mono_type_internal (header->locals [n]);
  4664| 			MONO_ADD_INS (cfg->cbb, ins);
  4665| 			*sp = mono_decompose_opcode (cfg, ins);
  4666| 		}
  4667| 	}
  4668| 	guint32 opcode = mono_type_to_regmove (cfg, header->locals [n]);
  4669| 	if (!cfg->deopt && (opcode == OP_MOVE) && cfg->cbb->last_ins == sp [0]  &&
  4670| 			((sp [0]->opcode == OP_ICONST) || (sp [0]->opcode == OP_I8CONST))) {
  4671| 		/* Optimize reg-reg moves away */
  4672| 		/*
  4673| 		 * Can't optimize other opcodes, since sp[0] might point to
  4674| 		 * the last ins of a decomposed opcode.
  4675| 		 */
  4676| 		sp [0]->dreg = (cfg)->locals [n]->dreg;
  4677| 	} else {
  4678| 		EMIT_NEW_LOCSTORE (cfg, ins, n, *sp);
  4679| 	}
  4680| }
  4681| static void
  4682| emit_starg_ir (MonoCompile *cfg, MonoInst **sp, int n)
  4683| {
  4684| 	MonoInst *ins;
  4685| 	guint32 coerce_op = mono_type_to_stloc_coerce (cfg->arg_types [n]);
  4686| 	if (coerce_op) {
  4687| 		if (cfg->cbb->last_ins == sp [0] && sp [0]->opcode == coerce_op) {
  4688| 			if (cfg->verbose_level > 2)
  4689| 				printf ("Found existing coercing is enough for starg\n");
  4690| 		} else {
  4691| 			MONO_INST_NEW (cfg, ins, coerce_op);
  4692| 			ins->dreg = alloc_ireg (cfg);
  4693| 			ins->sreg1 = sp [0]->dreg;
  4694| 			ins->type = STACK_I4;
  4695| 			ins->klass = mono_class_from_mono_type_internal (cfg->arg_types [n]);
  4696| 			MONO_ADD_INS (cfg->cbb, ins);
  4697| 			*sp = mono_decompose_opcode (cfg, ins);
  4698| 		}
  4699| 	}
  4700| 	EMIT_NEW_ARGSTORE (cfg, ins, n, *sp);
  4701| }
  4702| /*
  4703|  * ldloca inhibits many optimizations so try to get rid of it in common
  4704|  * cases.
  4705|  */
  4706| static guchar *
  4707| emit_optimized_ldloca_ir (MonoCompile *cfg, guchar *ip, guchar *end, int local)
  4708| {
  4709| 	guint32 token;
  4710| 	MonoClass *klass;
  4711| 	MonoType *type;
  4712| 	guchar *start = ip;
  4713| 	if  ((ip = il_read_initobj (ip, end, &token)) && ip_in_bb (cfg, cfg->cbb, start + 1)) {
  4714| 		/* From the INITOBJ case */
  4715| 		klass = mini_get_class (cfg->current_method, token, cfg->generic_context);
  4716| 		CHECK_TYPELOAD (klass);
  4717| 		type = mini_get_underlying_type (m_class_get_byval_arg (klass));
  4718| 		emit_init_local (cfg, local, type, TRUE);
  4719| 		return ip;
  4720| 	}
  4721|  exception_exit:
  4722| 	return NULL;
  4723| }
  4724| static MonoInst*
  4725| handle_call_res_devirt (MonoCompile *cfg, MonoMethod *cmethod, MonoInst *call_res)
  4726| {
  4727| 	MonoClass *ret_klass = mini_handle_call_res_devirt (cmethod);
  4728| 	if (ret_klass) {
  4729| 		MonoInst *typed_objref;
  4730| 		MONO_INST_NEW (cfg, typed_objref, OP_TYPED_OBJREF);
  4731| 		typed_objref->type = STACK_OBJ;
  4732| 		typed_objref->dreg = alloc_ireg_ref (cfg);
  4733| 		typed_objref->sreg1 = call_res->dreg;
  4734| 		typed_objref->klass = ret_klass;
  4735| 		MONO_ADD_INS (cfg->cbb, typed_objref);
  4736| 		call_res = typed_objref;
  4737| 		/* Force decompose */
  4738| 		cfg->flags |= MONO_CFG_NEEDS_DECOMPOSE;
  4739| 		cfg->cbb->needs_decompose = TRUE;
  4740| 	}
  4741| 	return call_res;
  4742| }
  4743| static gboolean
  4744| is_exception_class (MonoClass *klass)
  4745| {
  4746| 	if (G_LIKELY (m_class_get_supertypes (klass)))
  4747| 		return mono_class_has_parent_fast (klass, mono_defaults.exception_class);
  4748| 	while (klass) {
  4749| 		if (klass == mono_defaults.exception_class)
  4750| 			return TRUE;
  4751| 		klass = m_class_get_parent (klass);
  4752| 	}
  4753| 	return FALSE;
  4754| }
  4755| /*
  4756|  * is_jit_optimizer_disabled:
  4757|  *
  4758|  *   Determine whenever M's assembly has a DebuggableAttribute with the
  4759|  * IsJITOptimizerDisabled flag set.
  4760|  */
  4761| static gboolean
  4762| is_jit_optimizer_disabled (MonoMethod *m)
  4763| {
  4764| 	MonoAssembly *ass = m_class_get_image (m->klass)->assembly;
  4765| 	g_assert (ass);
  4766| 	if (ass->jit_optimizer_disabled_inited)
  4767| 		return ass->jit_optimizer_disabled;
  4768| 	return mono_assembly_is_jit_optimizer_disabled (ass);
  4769| }
  4770| gboolean
  4771| mono_is_supported_tailcall_helper (gboolean value, const char *svalue)
  4772| {
  4773| 	if (!value)
  4774| 		mono_tailcall_print ("%s %s\n", __func__, svalue);
  4775| 	return value;
  4776| }
  4777| static gboolean
  4778| mono_is_not_supported_tailcall_helper (gboolean value, const char *svalue, MonoMethod *method, MonoMethod *cmethod)
  4779| {
  4780| 	if (value && mono_tailcall_print_enabled ()) {
  4781| 		const char *lparen = strchr (svalue, ' ') ? "(" : "";
  4782| 		const char *rparen = *lparen ? ")" : "";
  4783| 		mono_tailcall_print ("%s %s -> %s %s%s%s:%d\n", __func__, method->name, cmethod->name, lparen, svalue, rparen, value);
  4784| 	}
  4785| 	return value;
  4786| }
  4787| #define IS_NOT_SUPPORTED_TAILCALL(x) (mono_is_not_supported_tailcall_helper((x), #x, method, cmethod))
  4788| static gboolean
  4789| is_supported_tailcall (MonoCompile *cfg, const guint8 *ip, MonoMethod *method, MonoMethod *cmethod, MonoMethodSignature *fsig,
  4790| 	gboolean virtual_, gboolean extra_arg, gboolean *ptailcall_calli)
  4791| {
  4792| 	gboolean tailcall = TRUE;
  4793| 	gboolean tailcall_calli = TRUE;
  4794| 	if (IS_NOT_SUPPORTED_TAILCALL (virtual_ && !cfg->backend->have_op_tailcall_membase))
  4795| 		tailcall = FALSE;
  4796| 	if (IS_NOT_SUPPORTED_TAILCALL (!cfg->backend->have_op_tailcall_reg))
  4797| 		tailcall_calli = FALSE;
  4798| 	if (!tailcall && !tailcall_calli)
  4799| 		goto exit;
  4800| 	if (       IS_NOT_SUPPORTED_TAILCALL (cmethod && fsig->hasthis && m_class_is_valuetype (cmethod->klass)) // This might point to the current method's stack. Emit range check?
  4801| 		|| IS_NOT_SUPPORTED_TAILCALL (cmethod && (cmethod->flags & METHOD_ATTRIBUTE_PINVOKE_IMPL))
  4802| 		|| IS_NOT_SUPPORTED_TAILCALL (fsig->pinvoke) // i.e. if !cmethod (calli)
  4803| 		|| IS_NOT_SUPPORTED_TAILCALL (cfg->method->save_lmf)
  4804| 		|| IS_NOT_SUPPORTED_TAILCALL (!cmethod && fsig->hasthis) // FIXME could be valuetype to current frame; range check
  4805| 		|| IS_NOT_SUPPORTED_TAILCALL (cmethod && cmethod->wrapper_type && cmethod->wrapper_type != MONO_WRAPPER_DYNAMIC_METHOD)
  4806| 		|| IS_NOT_SUPPORTED_TAILCALL (extra_arg && !cfg->backend->have_volatile_non_param_register)
  4807| 		|| IS_NOT_SUPPORTED_TAILCALL (cfg->gsharedvt)
  4808| 		) {
  4809| 		tailcall_calli = FALSE;
  4810| 		tailcall = FALSE;
  4811| 		goto exit;
  4812| 	}
  4813| 	for (int i = 0; i < fsig->param_count; ++i) {
  4814| 		if (IS_NOT_SUPPORTED_TAILCALL (m_type_is_byref (fsig->params [i]) || fsig->params [i]->type == MONO_TYPE_PTR || fsig->params [i]->type == MONO_TYPE_FNPTR)) {
  4815| 			tailcall_calli = FALSE;
  4816| 			tailcall = FALSE; // These can point to the current method's stack. Emit range check?
  4817| 			goto exit;
  4818| 		}
  4819| 	}
  4820| 	MonoMethodSignature *caller_signature;
  4821| 	MonoMethodSignature *callee_signature;
  4822| 	caller_signature = mono_method_signature_internal (method);
  4823| 	callee_signature = cmethod ? mono_method_signature_internal (cmethod) : fsig;
  4824| 	g_assert (caller_signature);
  4825| 	g_assert (callee_signature);
  4826| 	if (IS_NOT_SUPPORTED_TAILCALL (mini_get_underlying_type (caller_signature->ret)->type != mini_get_underlying_type (callee_signature->ret)->type)
  4827| 		|| IS_NOT_SUPPORTED_TAILCALL (!mono_arch_tailcall_supported (cfg, caller_signature, callee_signature, virtual_))) {
  4828| 		tailcall_calli = FALSE;
  4829| 		tailcall = FALSE;
  4830| 		goto exit;
  4831| 	}
  4832| 	/* Debugging support */
  4833| #if 0
  4834| 	if (!mono_debug_count ()) {
  4835| 		tailcall_calli = FALSE;
  4836| 		tailcall = FALSE;
  4837| 		goto exit;
  4838| 	}
  4839| #endif
  4840| 	if (tailcall_calli && IS_NOT_SUPPORTED_TAILCALL (mini_should_check_stack_pointer (cfg)))
  4841| 		tailcall_calli = FALSE;
  4842| exit:
  4843| 	mono_tailcall_print ("tail.%s %s -> %s tailcall:%d tailcall_calli:%d gshared:%d extra_arg:%d virtual_:%d\n",
  4844| 			mono_opcode_name (*ip), method->name, cmethod ? cmethod->name : "calli", tailcall, tailcall_calli,
  4845| 			cfg->gshared, extra_arg, virtual_);
  4846| 	*ptailcall_calli = tailcall_calli;
  4847| 	return tailcall;
  4848| }
  4849| /*
  4850|  * is_addressable_valuetype_load
  4851|  *
  4852|  *    Returns true if a previous load can be done without doing an extra copy, given the new instruction ip and the type of the object being loaded ldtype
  4853|  */
  4854| static gboolean
  4855| is_addressable_valuetype_load (MonoCompile* cfg, guint8* ip, MonoType* ldtype)
  4856| {
  4857| 	/* Avoid loading a struct just to load one of its fields */
  4858| 	gboolean is_load_instruction = (*ip == CEE_LDFLD);
  4859| 	gboolean is_in_previous_bb = ip_in_bb(cfg, cfg->cbb, ip);
  4860| 	gboolean is_struct = MONO_TYPE_ISSTRUCT(ldtype);
  4861| 	return is_load_instruction && is_in_previous_bb && is_struct;
  4862| }
  4863| /*
  4864|  * check_get_virtual_method_assumptions:
  4865|  * 
  4866|  * This shadows mono_class_get_virtual_method, but instead of actually resolving
  4867|  * the virtual method, this only checks if mono_class_get_virtual_method would
  4868|  * succeed. This is in place because that function fails catastrophically in some
  4869|  * cases, bringing down the entire runtime. Returns TRUE if the function is safe 
  4870|  * to call, FALSE otherwise.
  4871|  */
  4872| static gboolean
  4873| check_get_virtual_method_assumptions (MonoClass* klass, MonoMethod* method)
  4874| {
  4875| 	if (m_class_is_abstract(klass))
  4876| 		return FALSE;
  4877| 	if (((method->flags & METHOD_ATTRIBUTE_FINAL) || !(method->flags & METHOD_ATTRIBUTE_VIRTUAL)))
  4878| 		return TRUE;
  4879| 	mono_class_setup_vtable (klass);
  4880| 	if (m_class_get_vtable (klass) == NULL)
  4881| 		return FALSE;
  4882| 	if (method->slot == -1) {
  4883| 		if (method->is_inflated) {
  4884| 			if (((MonoMethodInflated*)method)->declaring->slot == -1)
  4885| 				return FALSE;
  4886| 		} else {
  4887| 			return FALSE;
  4888| 		}
  4889| 	}
  4890| 	if (method->slot != -1 && mono_class_is_interface (method->klass)) {
  4891| 		gboolean variance_used = FALSE;
  4892| 		int iface_offset = mono_class_interface_offset_with_variance (klass, method->klass, &variance_used);
  4893| 		if (iface_offset <= 0)
  4894| 			return FALSE;
  4895|     }
  4896| 	if (method->is_inflated)
  4897| 		return FALSE;
  4898| 	return TRUE;
  4899| }
  4900| /*
  4901|  * try_prepare_objaddr_callvirt_optimization:
  4902|  * 
  4903|  * Determine in a load+callvirt optimization can be performed and if so,
  4904|  * resolve the callvirt target method, so that it can behave as call.
  4905|  * Returns null, if the optimization cannot be performed.
  4906|  */
  4907| static MonoMethod*
  4908| try_prepare_objaddr_callvirt_optimization (MonoCompile *cfg, guchar *next_ip, guchar* end, MonoMethod *method, MonoGenericContext* generic_context, MonoType *param_type)
  4909| {
  4910| 	g_assert(param_type);
  4911| 	MonoClass *klass = mono_class_from_mono_type_internal (param_type);
  4912| 	if (cfg->compile_aot || cfg->compile_llvm || !klass || !mono_class_is_def (klass))
  4913| 		return NULL;
  4914| 	guchar* callvirt_ip;
  4915| 	guint32 callvirt_proc_token;
  4916| 	if (!(callvirt_ip = il_read_callvirt (next_ip, end, &callvirt_proc_token)) ||
  4917| 		!ip_in_bb (cfg, cfg->cbb, callvirt_ip))
  4918| 		return NULL;
  4919| 	MonoMethod* iface_method = mini_get_method (cfg, method, callvirt_proc_token, NULL, generic_context);
  4920| 	if (!iface_method ||
  4921| 		iface_method->is_generic ||
  4922| 		iface_method->dynamic || 					// Reflection.Emit-generated methods should have this flag
  4923| 		!strcmp (iface_method->name, "GetHashCode") || // the callvirt handler itself optimizes those
  4924| 		(iface_method->iflags & METHOD_IMPL_ATTRIBUTE_RUNTIME))
  4925| 		return NULL;
  4926| 	MonoMethodSignature* iface_method_sig;
  4927| 	if (!((iface_method_sig = mono_method_signature_internal (iface_method)) &&
  4928| 		iface_method_sig->hasthis && 
  4929| 		iface_method_sig->param_count == 0 && 
  4930| 		!iface_method_sig->has_type_parameters &&
  4931| 		iface_method_sig->generic_param_count == 0))
  4932| 		return NULL;
  4933| 	if (!check_get_virtual_method_assumptions (klass, iface_method))
  4934| 		return NULL;
  4935| 	ERROR_DECL (struct_method_error);
  4936| 	MonoMethod* struct_method = mono_class_get_virtual_method (klass, iface_method, struct_method_error);
  4937| 	if (is_ok (struct_method_error)) {
  4938| 		if (!struct_method || !MONO_METHOD_IS_FINAL (struct_method))
  4939| 			return NULL;
  4940| 		MonoMethodSignature* struct_method_sig = mono_method_signature_internal (struct_method);
  4941| 		if (!struct_method_sig ||
  4942| 			struct_method_sig->has_type_parameters ||
  4943| 			!mono_method_can_access_method (method, struct_method)) {
  4944| 			return NULL;
  4945| 			}
  4946| 	} else {
  4947| 		mono_error_cleanup (struct_method_error);
  4948| 		return NULL;
  4949| 	}
  4950| 	return struct_method;
  4951| }
  4952| /*
  4953|  * handle_ctor_call:
  4954|  *
  4955|  *   Handle calls made to ctors from NEWOBJ opcodes.
  4956|  */
  4957| static void
  4958| handle_ctor_call (MonoCompile *cfg, MonoMethod *cmethod, MonoMethodSignature *fsig, int context_used,
  4959| 				  MonoInst **sp, guint8 *ip, int *inline_costs)
  4960| {
  4961| 	MonoInst *rgctx_arg = NULL, *callvirt_this_arg = NULL, *ins;
  4962| 	if (cmethod && (ins = mini_emit_inst_for_ctor (cfg, cmethod, fsig, sp))) {
  4963| 		g_assert (MONO_TYPE_IS_VOID (fsig->ret));
  4964| 		CHECK_CFG_EXCEPTION;
  4965| 		return;
  4966| 	}
  4967| 	if ((cfg->opt & MONO_OPT_INLINE) && mono_method_check_inlining (cfg, cmethod) &&
  4968| 			   !mono_class_is_subclass_of_internal (cmethod->klass, mono_defaults.exception_class, FALSE)) {
  4969| 		int costs;
  4970| 		costs = inline_method (cfg, cmethod, fsig, sp, ip, cfg->real_offset, FALSE, NULL);
  4971| 		if (costs) {
  4972| 			cfg->real_offset += 5;
  4973| 			*inline_costs += costs - 5;
  4974| 			return;
  4975| 		}
  4976| 	}
  4977| 	if (mono_class_generic_sharing_enabled (cmethod->klass) && mono_method_is_generic_sharable (cmethod, TRUE)) {
  4978| 		MonoRgctxAccess access = mini_get_rgctx_access_for_method (cmethod);
  4979| 		if (access == MONO_RGCTX_ACCESS_MRGCTX) {
  4980| 			rgctx_arg = emit_get_rgctx_method (cfg, context_used,
  4981| 												cmethod, MONO_RGCTX_INFO_METHOD_RGCTX);
  4982| 		} else {
  4983| 			g_assert (access == MONO_RGCTX_ACCESS_THIS);
  4984| 		}
  4985| 	}
  4986| 	/* Avoid virtual calls to ctors if possible */
  4987| 	if (!context_used && !rgctx_arg) {
  4988| 		if (!m_method_is_aggressive_inlining (cfg->current_method) && !m_method_is_aggressive_inlining (cmethod))
  4989| 			INLINE_FAILURE ("ctor call");
  4990| 		if (cfg->gsharedvt && mini_is_gsharedvt_signature (fsig))
  4991| 			GSHAREDVT_FAILURE(*ip);
  4992| 		mini_emit_method_call_full (cfg, cmethod, fsig, FALSE, sp, callvirt_this_arg, NULL, NULL);
  4993| 	} else if (cfg->gsharedvt && mini_is_gsharedvt_signature (fsig)) {
  4994| 		MonoInst *addr;
  4995| 		addr = emit_get_rgctx_gsharedvt_call (cfg, context_used, fsig, cmethod, MONO_RGCTX_INFO_METHOD_GSHAREDVT_OUT_TRAMPOLINE);
  4996| 		if (cfg->llvm_only) {
  4997| 			mini_emit_llvmonly_calli (cfg, fsig, sp, addr);
  4998| 		} else {
  4999| 			mini_emit_calli (cfg, fsig, sp, addr, NULL, rgctx_arg);
  5000| 		}
  5001| 	} else if (context_used &&
  5002| 			   ((!mono_method_is_generic_sharable_full (cmethod, TRUE, FALSE, FALSE) ||
  5003| 				 !mono_class_generic_sharing_enabled (cmethod->klass)) || cfg->gsharedvt)) {
  5004| 		MonoInst *cmethod_addr;
  5005| 		/* Generic calls made out of gsharedvt methods cannot be patched, so use an indirect call */
  5006| 		if (cfg->llvm_only) {
  5007| 			MonoInst *addr = emit_get_rgctx_method (cfg, context_used, cmethod,
  5008| 													MONO_RGCTX_INFO_METHOD_FTNDESC);
  5009| 			/* Need wrappers for this signature to be able to enter interpreter */
  5010| 			cfg->interp_in_signatures = g_slist_prepend_mempool (cfg->mempool, cfg->interp_in_signatures, fsig);
  5011| 			mini_emit_llvmonly_calli (cfg, fsig, sp, addr);
  5012| 		} else {
  5013| 			cmethod_addr = emit_get_rgctx_method (cfg, context_used,
  5014| 												  cmethod, MONO_RGCTX_INFO_GENERIC_METHOD_CODE);
  5015| 			mini_emit_calli (cfg, fsig, sp, cmethod_addr, NULL, rgctx_arg);
  5016| 		}
  5017| 	} else {
  5018| 		INLINE_FAILURE ("ctor call");
  5019| 		ins = mini_emit_method_call_full (cfg, cmethod, fsig, FALSE, sp,
  5020| 						  callvirt_this_arg, NULL, rgctx_arg);
  5021| 	}
  5022|  exception_exit:
  5023| 	return;
  5024| }
  5025| typedef struct {
  5026| 	MonoMethod *method;
  5027| 	gboolean inst_tailcall;
  5028| } HandleCallData;
  5029| /*
  5030|  * handle_constrained_call:
  5031|  *
  5032|  *   Handle constrained calls. Return a MonoInst* representing the call or NULL.
  5033|  * May overwrite sp [0] and modify the ref_... parameters.
  5034|  */
  5035| static MonoInst*
  5036| handle_constrained_call (MonoCompile *cfg, MonoMethod *cmethod, MonoMethodSignature *fsig, MonoClass *constrained_class, MonoInst **sp,
  5037| 						 HandleCallData *cdata, MonoMethod **ref_cmethod, gboolean *ref_virtual, gboolean *ref_emit_widen)
  5038| {
  5039| 	MonoInst *ins, *addr;
  5040| 	MonoMethod *method = cdata->method;
  5041| 	gboolean constrained_partial_call = FALSE;
  5042| 	gboolean constrained_is_generic_param =
  5043| 		m_class_get_byval_arg (constrained_class)->type == MONO_TYPE_VAR ||
  5044| 		m_class_get_byval_arg (constrained_class)->type == MONO_TYPE_MVAR;
  5045| 	MonoType *gshared_constraint = NULL;
  5046| 	if (constrained_is_generic_param && cfg->gshared) {
  5047| 		if (!mini_is_gsharedvt_klass (constrained_class)) {
  5048| 			g_assert (!m_class_is_valuetype (cmethod->klass));
  5049| 			if (!mini_type_is_reference (m_class_get_byval_arg (constrained_class)))
  5050| 				constrained_partial_call = TRUE;
  5051| 			MonoType *t = m_class_get_byval_arg (constrained_class);
  5052| 			MonoGenericParam *gparam = t->data.generic_param;
  5053| 			gshared_constraint = gparam->gshared_constraint;
  5054| 		}
  5055| 	}
  5056| 	if (mini_is_gsharedvt_klass (constrained_class)) {
  5057| 		if ((cmethod->klass != mono_defaults.object_class) && m_class_is_valuetype (constrained_class) && m_class_is_valuetype (cmethod->klass)) {
  5058| 			/* The 'Own method' case below */
  5059| 		} else if (m_class_get_image (cmethod->klass) != mono_defaults.corlib && !mono_class_is_interface (cmethod->klass) && !m_class_is_valuetype (cmethod->klass)) {
  5060| 			/* 'The type parameter is instantiated as a reference type' case below. */
  5061| 		} else {
  5062| 			ins = handle_constrained_gsharedvt_call (cfg, cmethod, fsig, sp, constrained_class, ref_emit_widen);
  5063| 			CHECK_CFG_EXCEPTION;
  5064| 			g_assert (ins);
  5065| 			if (cdata->inst_tailcall) // FIXME
  5066| 				mono_tailcall_print ("missed tailcall constrained_class %s -> %s\n", method->name, cmethod->name);
  5067| 			return ins;
  5068| 		}
  5069| 	}
  5070| 	if (m_method_is_static (cmethod)) {
  5071| 		/* Call to an abstract static method, handled normally */
  5072| 		return NULL;
  5073| 	} else if (constrained_partial_call) {
  5074| 		gboolean need_box = TRUE;
  5075| 		/*
  5076| 		 * The receiver is a valuetype, but the exact type is not known at compile time. This means the
  5077| 		 * called method is not known at compile time either. The called method could end up being
  5078| 		 * one of the methods on the parent classes (object/valuetype/enum), in which case we need
  5079| 		 * to box the receiver.
  5080| 		 * A simple solution would be to box always and make a normal virtual call, but that would
  5081| 		 * be bad performance wise.
  5082| 		 */
  5083| 		if (mono_class_is_interface (cmethod->klass) && mono_class_is_ginst (cmethod->klass) &&
  5084| 		    (cmethod->flags & METHOD_ATTRIBUTE_ABSTRACT)) {
  5085| 			/*
  5086| 			 * The parent classes implement no generic interfaces, so the called method will be a vtype method, so no boxing necessary.
  5087| 			 */
  5088| 			/* If the method is not abstract, it's a default interface method, and we need to box */
  5089| 			need_box = FALSE;
  5090| 		}
  5091| 		if (gshared_constraint && MONO_TYPE_IS_PRIMITIVE (gshared_constraint) && cmethod->klass == mono_defaults.object_class &&
  5092| 			!strcmp (cmethod->name, "GetHashCode")) {
  5093| 			/*
  5094| 			 * The receiver is constrained to a primitive type or an enum with the same basetype.
  5095| 			 * Enum.GetHashCode () returns the hash code of the underlying type (see comments in Enum.cs),
  5096| 			 * so the constrained call can be replaced with a normal call to the basetype GetHashCode ()
  5097| 			 * method.
  5098| 			 */
  5099| 			MonoClass *gshared_constraint_class = mono_class_from_mono_type_internal (gshared_constraint);
  5100| 			cmethod = get_method_nofail (gshared_constraint_class, cmethod->name, 0, 0);
  5101| 			g_assert (cmethod);
  5102| 			*ref_cmethod = cmethod;
  5103| 			*ref_virtual = FALSE;
  5104| 			if (cfg->verbose_level)
  5105| 				printf (" -> %s\n", mono_method_get_full_name (cmethod));
  5106| 			return NULL;
  5107| 		}
  5108| 		if (!(cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL) && (cmethod->klass == mono_defaults.object_class || cmethod->klass == m_class_get_parent (mono_defaults.enum_class) || cmethod->klass == mono_defaults.enum_class)) {
  5109| 			/* The called method is not virtual, i.e. Object:GetType (), the receiver is a vtype, has to box */
  5110| 			EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (constrained_class), sp [0]->dreg, 0);
  5111| 			ins->klass = constrained_class;
  5112| 			sp [0] = mini_emit_box (cfg, ins, constrained_class, mono_class_check_context_used (constrained_class));
  5113| 			CHECK_CFG_EXCEPTION;
  5114| 		} else if (need_box) {
  5115| 			MonoInst *box_type;
  5116| 			MonoBasicBlock *is_ref_bb, *end_bb;
  5117| 			MonoInst *nonbox_call, *nonbox_addr;
  5118| 			/*
  5119| 			 * Determine at runtime whenever the called method is defined on object/valuetype/enum, and emit a boxing call
  5120| 			 * if needed.
  5121| 			 * FIXME: It is possible to inline the called method in a lot of cases, i.e. for T_INT,
  5122| 			 * the no-box case goes to a method in Int32, while the box case goes to a method in Enum.
  5123| 			 */
  5124| 			nonbox_addr = emit_get_rgctx_virt_method (cfg, mono_class_check_context_used (constrained_class), constrained_class, cmethod, MONO_RGCTX_INFO_VIRT_METHOD_CODE);
  5125| 			NEW_BBLOCK (cfg, is_ref_bb);
  5126| 			NEW_BBLOCK (cfg, end_bb);
  5127| 			box_type = emit_get_rgctx_virt_method (cfg, mono_class_check_context_used (constrained_class), constrained_class, cmethod, MONO_RGCTX_INFO_VIRT_METHOD_BOX_TYPE);
  5128| 			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, box_type->dreg, MONO_GSHAREDVT_BOX_TYPE_REF);
  5129| 			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBEQ, is_ref_bb);
  5130| 			/* Non-ref case */
  5131| 			if (cfg->llvm_only)
  5132| 				/* nonbox_addr is an ftndesc in this case */
  5133| 				nonbox_call = mini_emit_llvmonly_calli (cfg, fsig, sp, nonbox_addr);
  5134| 			else
  5135| 				nonbox_call = (MonoInst*)mini_emit_calli (cfg, fsig, sp, nonbox_addr, NULL, NULL);
  5136| 			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  5137| 			/* Ref case */
  5138| 			MONO_START_BB (cfg, is_ref_bb);
  5139| 			EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (constrained_class), sp [0]->dreg, 0);
  5140| 			ins->klass = constrained_class;
  5141| 			sp [0] = mini_emit_box (cfg, ins, constrained_class, mono_class_check_context_used (constrained_class));
  5142| 			CHECK_CFG_EXCEPTION;
  5143| 			if (cfg->llvm_only)
  5144| 				ins = mini_emit_llvmonly_calli (cfg, fsig, sp, nonbox_addr);
  5145| 			else
  5146| 				ins = (MonoInst*)mini_emit_calli (cfg, fsig, sp, nonbox_addr, NULL, NULL);
  5147| 			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  5148| 			MONO_START_BB (cfg, end_bb);
  5149| 			cfg->cbb = end_bb;
  5150| 			nonbox_call->dreg = ins->dreg;
  5151| 			if (cdata->inst_tailcall) // FIXME
  5152| 				mono_tailcall_print ("missed tailcall constrained_partial_need_box %s -> %s\n", method->name, cmethod->name);
  5153| 			return ins;
  5154| 		} else {
  5155| 			g_assert (mono_class_is_interface (cmethod->klass));
  5156| 			addr = emit_get_rgctx_virt_method (cfg, mono_class_check_context_used (constrained_class), constrained_class, cmethod, MONO_RGCTX_INFO_VIRT_METHOD_CODE);
  5157| 			if (cfg->llvm_only)
  5158| 				ins = mini_emit_llvmonly_calli (cfg, fsig, sp, addr);
  5159| 			else
  5160| 				ins = (MonoInst*)mini_emit_calli (cfg, fsig, sp, addr, NULL, NULL);
  5161| 			if (cdata->inst_tailcall) // FIXME
  5162| 				mono_tailcall_print ("missed tailcall constrained_partial %s -> %s\n", method->name, cmethod->name);
  5163| 			return ins;
  5164| 		}
  5165| 	} else if (!m_class_is_valuetype (constrained_class)) {
  5166| 		int dreg = alloc_ireg_ref (cfg);
  5167| 		/*
  5168| 		 * The type parameter is instantiated as a reference
  5169| 		 * type.  We have a managed pointer on the stack, so
  5170| 		 * we need to dereference it here.
  5171| 		 */
  5172| 		EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, dreg, sp [0]->dreg, 0);
  5173| 		ins->type = STACK_OBJ;
  5174| 		sp [0] = ins;
  5175| 	} else if (cmethod->klass == mono_defaults.object_class || cmethod->klass == m_class_get_parent (mono_defaults.enum_class) || cmethod->klass == mono_defaults.enum_class) {
  5176| 		/*
  5177| 		 * The type parameter is instantiated as a valuetype,
  5178| 		 * but that type doesn't override the method we're
  5179| 		 * calling, so we need to box `this'.
  5180| 		 */
  5181| 		EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (constrained_class), sp [0]->dreg, 0);
  5182| 		ins->klass = constrained_class;
  5183| 		sp [0] = mini_emit_box (cfg, ins, constrained_class, mono_class_check_context_used (constrained_class));
  5184| 		CHECK_CFG_EXCEPTION;
  5185| 	} else {
  5186| 		if (cmethod->klass != constrained_class) {
  5187| 			/* Enums/default interface methods */
  5188| 			EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (constrained_class), sp [0]->dreg, 0);
  5189| 			ins->klass = constrained_class;
  5190| 			sp [0] = mini_emit_box (cfg, ins, constrained_class, mono_class_check_context_used (constrained_class));
  5191| 			CHECK_CFG_EXCEPTION;
  5192| 		}
  5193| 		*ref_virtual = FALSE;
  5194| 	}
  5195|  exception_exit:
  5196| 	return NULL;
  5197| }
  5198| static void
  5199| emit_setret (MonoCompile *cfg, MonoInst *val)
  5200| {
  5201| 	MonoType *ret_type = mini_get_underlying_type (mono_method_signature_internal (cfg->method)->ret);
  5202| 	MonoInst *ins;
  5203| 	if (mini_type_to_stind (cfg, ret_type) == CEE_STOBJ) {
  5204| 		MonoInst *ret_addr;
  5205| 		if (!cfg->vret_addr) {
  5206| 			EMIT_NEW_VARSTORE (cfg, ins, cfg->ret, ret_type, val);
  5207| 		} else {
  5208| 			EMIT_NEW_RETLOADA (cfg, ret_addr);
  5209| 			MonoClass *ret_class = mono_class_from_mono_type_internal (ret_type);
  5210| 			if (mini_class_is_simd (cfg, ret_class))
  5211| 				EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STOREX_MEMBASE, ret_addr->dreg, 0, val->dreg);
  5212| 			else
  5213| 				EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STOREV_MEMBASE, ret_addr->dreg, 0, val->dreg);
  5214| 			ins->klass = ret_class;
  5215| 		}
  5216| 	} else {
  5217| #ifdef MONO_ARCH_SOFT_FLOAT_FALLBACK
  5218| 		if (COMPILE_SOFT_FLOAT (cfg) && !m_type_is_byref (ret_type) && ret_type->type == MONO_TYPE_R4) {
  5219| 			MonoInst *conv;
  5220| 			MonoInst *iargs [ ] = { val };
  5221| 			conv = mono_emit_jit_icall (cfg, mono_fload_r4_arg, iargs);
  5222| 			mono_arch_emit_setret (cfg, cfg->method, conv);
  5223| 		} else {
  5224| 			mono_arch_emit_setret (cfg, cfg->method, val);
  5225| 		}
  5226| #else
  5227| 		mono_arch_emit_setret (cfg, cfg->method, val);
  5228| #endif
  5229| 	}
  5230| }
  5231| /*
  5232|  * Emit a call to enter the interpreter for methods with filter clauses.
  5233|  */
  5234| static void
  5235| emit_llvmonly_interp_entry (MonoCompile *cfg, MonoMethodHeader *header)
  5236| {
  5237| 	MonoInst *ins;
  5238| 	MonoInst **iargs;
  5239| 	MonoMethodSignature *sig = mono_method_signature_internal (cfg->method);
  5240| 	MonoInst *ftndesc;
  5241| 	cfg->interp_in_signatures = g_slist_prepend_mempool (cfg->mempool, cfg->interp_in_signatures, sig);
  5242| 	/*
  5243| 	 * Emit a call to the interp entry function. We emit it here instead of the llvm backend since
  5244| 	 * calling conventions etc. are easier to handle here. The LLVM backend will only emit the
  5245| 	 * entry/exit bblocks.
  5246| 	 */
  5247| 	g_assert (cfg->cbb == cfg->bb_init);
  5248| 	if (cfg->gsharedvt && mini_is_gsharedvt_variable_signature (sig)) {
  5249| 		/*
  5250| 		 * Would have to generate a gsharedvt out wrapper which calls the interp entry wrapper, but
  5251| 		 * the gsharedvt out wrapper might not exist if the caller is also a gsharedvt method since
  5252| 		 * the concrete signature of the call might not exist in the program.
  5253| 		 * So transition directly to the interpreter without the wrappers.
  5254| 		 */
  5255| 		MonoInst *args_ins;
  5256| 		MONO_INST_NEW (cfg, ins, OP_LOCALLOC_IMM);
  5257| 		ins->dreg = alloc_preg (cfg);
  5258| 		ins->inst_imm = sig->param_count * sizeof (target_mgreg_t);
  5259| 		MONO_ADD_INS (cfg->cbb, ins);
  5260| 		args_ins = ins;
  5261| 		for (unsigned int i = 0; i < sig->hasthis + sig->param_count; ++i) {
  5262| 			MonoInst *arg_addr_ins;
  5263| 			EMIT_NEW_VARLOADA ((cfg), arg_addr_ins, cfg->args [i], cfg->arg_types [i]);
  5264| 			EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, args_ins->dreg, i * sizeof (target_mgreg_t), arg_addr_ins->dreg);
  5265| 		}
  5266| 		MonoInst *ret_var = NULL;
  5267| 		MonoInst *ret_arg_ins;
  5268| 		if (!MONO_TYPE_IS_VOID (sig->ret)) {
  5269| 			ret_var = mono_compile_create_var (cfg, sig->ret, OP_LOCAL);
  5270| 			EMIT_NEW_VARLOADA (cfg, ret_arg_ins, ret_var, sig->ret);
  5271| 		} else {
  5272| 			EMIT_NEW_PCONST (cfg, ret_arg_ins, NULL);
  5273| 		}
  5274| 		iargs = g_newa (MonoInst*, 3);
  5275| 		iargs [0] = emit_get_rgctx_method (cfg, -1, cfg->method, MONO_RGCTX_INFO_INTERP_METHOD);
  5276| 		iargs [1] = ret_arg_ins;
  5277| 		iargs [2] = args_ins;
  5278| 		mono_emit_jit_icall_id (cfg, MONO_JIT_ICALL_mini_llvmonly_interp_entry_gsharedvt, iargs);
  5279| 		if (!MONO_TYPE_IS_VOID (sig->ret))
  5280| 			EMIT_NEW_VARLOAD (cfg, ins, ret_var, sig->ret);
  5281| 		else
  5282| 			ins = NULL;
  5283| 	} else {
  5284| 		/* Obtain the interp entry function */
  5285| 		ftndesc = emit_get_rgctx_method (cfg, -1, cfg->method, MONO_RGCTX_INFO_LLVMONLY_INTERP_ENTRY);
  5286| 		/* Call it */
  5287| 		iargs = g_newa (MonoInst*, sig->param_count + 1);
  5288| 		for (unsigned int i = 0; i < sig->param_count + sig->hasthis; ++i)
  5289| 			EMIT_NEW_ARGLOAD (cfg, iargs [i], i);
  5290| 		ins = mini_emit_llvmonly_calli (cfg, sig, iargs, ftndesc);
  5291| 	}
  5292| 	/* Do a normal return */
  5293| 	if (cfg->ret) {
  5294| 		emit_setret (cfg, ins);
  5295| 		/*
  5296| 		 * Since only bb_entry/bb_exit is emitted if interp_entry_only is set,
  5297| 		 * its possible that the return value becomes an OP_PHI node whose inputs
  5298| 		 * are not emitted. Make it volatile to prevent that.
  5299| 		 */
  5300| 		cfg->ret->flags |= MONO_INST_VOLATILE;
  5301| 	}
  5302| 	MONO_INST_NEW (cfg, ins, OP_BR);
  5303| 	ins->inst_target_bb = cfg->bb_exit;
  5304| 	MONO_ADD_INS (cfg->cbb, ins);
  5305| 	link_bblock (cfg, cfg->cbb, cfg->bb_exit);
  5306| }
  5307| typedef union _MonoOpcodeParameter {
  5308| 	gint32 i32;
  5309| 	gint64 i64;
  5310| 	float f;
  5311| 	double d;
  5312| 	guchar *branch_target;
  5313| } MonoOpcodeParameter;
  5314| typedef struct _MonoOpcodeInfo {
  5315| 	guint constant : 4; // private
  5316| 	gint  pops     : 3; // public -1 means variable
  5317| 	gint  pushes   : 3; // public -1 means variable
  5318| } MonoOpcodeInfo;
  5319| static const MonoOpcodeInfo*
  5320| mono_opcode_decode (guchar *ip, guint op_size, MonoOpcodeEnum il_op, MonoOpcodeParameter *parameter)
  5321| {
  5322| #define Push0 (0)
  5323| #define Pop0 (0)
  5324| #define Push1 (1)
  5325| #define Pop1 (1)
  5326| #define PushI (1)
  5327| #define PopI (1)
  5328| #define PushI8 (1)
  5329| #define PopI8 (1)
  5330| #define PushRef (1)
  5331| #define PopRef (1)
  5332| #define PushR4 (1)
  5333| #define PopR4 (1)
  5334| #define PushR8 (1)
  5335| #define PopR8 (1)
  5336| #define VarPush (-1)
  5337| #define VarPop (-1)
  5338| 	static const MonoOpcodeInfo mono_opcode_info [ ] = {
  5339| #define OPDEF(name, str, pops, pushes, param, param_constant, a, b, c, flow) {param_constant + 1, pops, pushes },
  5340| #include "mono/cil/opcode.def"
  5341| #undef OPDEF
  5342| 	};
  5343| #undef Push0
  5344| #undef Pop0
  5345| #undef Push1
  5346| #undef Pop1
  5347| #undef PushI
  5348| #undef PopI
  5349| #undef PushI8
  5350| #undef PopI8
  5351| #undef PushRef
  5352| #undef PopRef
  5353| #undef PushR4
  5354| #undef PopR4
  5355| #undef PushR8
  5356| #undef PopR8
  5357| #undef VarPush
  5358| #undef VarPop
  5359| 	gint32 delta;
  5360| 	guchar *next_ip = ip + op_size;
  5361| 	const MonoOpcodeInfo *info = &mono_opcode_info [il_op];
  5362| 	switch (mono_opcodes [il_op].argument) {
  5363| 	case MonoInlineNone:
  5364| 		parameter->i32 = (int)info->constant - 1;
  5365| 		break;
  5366| 	case MonoInlineString:
  5367| 	case MonoInlineType:
  5368| 	case MonoInlineField:
  5369| 	case MonoInlineMethod:
  5370| 	case MonoInlineTok:
  5371| 	case MonoInlineSig:
  5372| 	case MonoShortInlineR:
  5373| 	case MonoInlineI:
  5374| 		parameter->i32 = read32 (next_ip - 4);
  5375| 		break;
  5376| 	case MonoShortInlineI:
  5377| 		parameter->i32 = (signed char)next_ip [-1];
  5378| 		break;
  5379| 	case MonoInlineVar:
  5380| 		parameter->i32 = read16 (next_ip - 2);
  5381| 		break;
  5382| 	case MonoShortInlineVar:
  5383| 		parameter->i32 = next_ip [-1];
  5384| 		break;
  5385| 	case MonoInlineR:
  5386| 	case MonoInlineI8:
  5387| 		parameter->i64 = read64 (next_ip - 8);
  5388| 		break;
  5389| 	case MonoShortInlineBrTarget:
  5390| 		delta = (signed char)next_ip [-1];
  5391| 		goto branch_target;
  5392| 	case MonoInlineBrTarget:
  5393| 		delta = (gint32)read32 (next_ip - 4);
  5394| branch_target:
  5395| 		parameter->branch_target = delta + next_ip;
  5396| 		break;
  5397| 	case MonoInlineSwitch: // complicated
  5398| 		break;
  5399| 	default:
  5400| 		g_error ("%s %d %d\n", __func__, il_op, mono_opcodes [il_op].argument);
  5401| 	}
  5402| 	return info;
  5403| }
  5404| /*
  5405|  * mono_method_to_ir:
  5406|  *
  5407|  * Translate the .net IL into linear IR.
  5408|  *
  5409|  * @start_bblock: if not NULL, the starting basic block, used during inlining.
  5410|  * @end_bblock: if not NULL, the ending basic block, used during inlining.
  5411|  * @return_var: if not NULL, the place where the return value is stored, used during inlining.
  5412|  * @inline_args: if not NULL, contains the arguments to the inline call
  5413|  * @inline_offset: if not zero, the real offset from the inline call, or zero otherwise.
  5414|  * @is_virtual_call: whether this method is being called as a result of a call to callvirt
  5415|  *
  5416|  * This method is used to turn ECMA IL into Mono's internal Linear IR
  5417|  * reprensetation.  It is used both for entire methods, as well as
  5418|  * inlining existing methods.  In the former case, the @start_bblock,
  5419|  * @end_bblock, @return_var, @inline_args are all set to NULL, and the
  5420|  * inline_offset is set to zero.
  5421|  *
  5422|  * Returns: the inline cost, or -1 if there was an error processing this method.
  5423|  */
  5424| int
  5425| mono_method_to_ir (MonoCompile *cfg, MonoMethod *method, MonoBasicBlock *start_bblock, MonoBasicBlock *end_bblock,
  5426| 		   MonoInst *return_var, MonoInst **inline_args,
  5427| 		   guint inline_offset, gboolean is_virtual_call)
  5428| {
  5429| 	ERROR_DECL (error);
  5430| 	MonoInst *array_new_localalloc_ins = NULL;
  5431| 	MonoInst *ins, **sp, **stack_start;
  5432| 	MonoBasicBlock *tblock = NULL;
  5433| 	MonoBasicBlock *init_localsbb = NULL, *init_localsbb2 = NULL;
  5434| 	MonoSimpleBasicBlock *bb = NULL, *original_bb = NULL;
  5435| 	MonoMethod *method_definition;
  5436| 	MonoInst **arg_array;
  5437| 	MonoMethodHeader *header;
  5438| 	MonoImage *image;
  5439| 	guint32 token, ins_flag;
  5440| 	MonoClass *klass;
  5441| 	MonoClass *constrained_class = NULL;
  5442| 	gboolean save_last_error = FALSE;
  5443| 	guchar *ip, *end, *target, *err_pos;
  5444| 	MonoMethodSignature *sig;
  5445| 	MonoGenericContext *generic_context = NULL;
  5446| 	MonoGenericContainer *generic_container = NULL;
  5447| 	MonoType **param_types;
  5448| 	int n, start_new_bblock;
  5449| 	int num_calls = 0, inline_costs = 0;
  5450| 	guint num_args;
  5451| 	GSList *class_inits = NULL;
  5452| 	gboolean dont_verify, dont_verify_stloc, readonly = FALSE;
  5453| 	int context_used;
  5454| 	gboolean init_locals, seq_points, skip_dead_blocks;
  5455| 	gboolean sym_seq_points = FALSE;
  5456| 	MonoDebugMethodInfo *minfo;
  5457| 	MonoBitSet *seq_point_locs = NULL;
  5458| 	MonoBitSet *seq_point_set_locs = NULL;
  5459| 	const char *ovf_exc = NULL;
  5460| 	gboolean emitted_funccall_seq_point = FALSE;
  5461| 	gboolean detached_before_ret = FALSE;
  5462| 	gboolean ins_has_side_effect;
  5463| 	MonoMethod* cmethod_override = NULL; // this is ised in call/callvirt handler to override the method to be called (e.g. from box handler)
  5464| 	if (!cfg->disable_inline)
  5465| 		cfg->disable_inline = (method->iflags & METHOD_IMPL_ATTRIBUTE_NOOPTIMIZATION) || is_jit_optimizer_disabled (method);
  5466| 	cfg->current_method = method;
  5467| 	image = m_class_get_image (method->klass);
  5468| 	/* serialization and xdomain stuff may need access to private fields and methods */
  5469| 	dont_verify = FALSE;
  5470|  	dont_verify |= method->wrapper_type == MONO_WRAPPER_MANAGED_TO_NATIVE; /* bug #77896 */
  5471| 	dont_verify |= method->wrapper_type == MONO_WRAPPER_COMINTEROP;
  5472| 	dont_verify |= method->wrapper_type == MONO_WRAPPER_COMINTEROP_INVOKE;
  5473| 	/* still some type unsafety issues in marshal wrappers... (unknown is PtrToStructure) */
  5474| 	dont_verify_stloc = method->wrapper_type == MONO_WRAPPER_MANAGED_TO_NATIVE;
  5475| 	dont_verify_stloc |= method->wrapper_type == MONO_WRAPPER_OTHER;
  5476| 	dont_verify_stloc |= method->wrapper_type == MONO_WRAPPER_NATIVE_TO_MANAGED;
  5477| 	dont_verify_stloc |= method->wrapper_type == MONO_WRAPPER_STELEMREF;
  5478| 	header = mono_method_get_header_checked (method, cfg->error);
  5479| 	if (!header) {
  5480| 		mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
  5481| 		goto exception_exit;
  5482| 	} else {
  5483| 		cfg->headers_to_free = g_slist_prepend_mempool (cfg->mempool, cfg->headers_to_free, header);
  5484| 	}
  5485| 	generic_container = mono_method_get_generic_container (method);
  5486| 	sig = mono_method_signature_internal (method);
  5487| 	num_args = sig->hasthis + sig->param_count;
  5488| 	ip = (guchar*)header->code;
  5489| 	cfg->cil_start = ip;
  5490| 	end = ip + header->code_size;
  5491| 	cfg->stat_cil_code_size += header->code_size;
  5492| 	seq_points = cfg->gen_seq_points && cfg->method == method;
  5493| 	if (method->wrapper_type == MONO_WRAPPER_NATIVE_TO_MANAGED) {
  5494| 		/* We could hit a seq point before attaching to the JIT (#8338) */
  5495| 		seq_points = FALSE;
  5496| 	}
  5497| 	if (method->wrapper_type == MONO_WRAPPER_OTHER)	{
  5498| 		WrapperInfo *info = mono_marshal_get_wrapper_info (method);
  5499| 		if (info->subtype == WRAPPER_SUBTYPE_INTERP_IN) {
  5500| 			/* We could hit a seq point before attaching to the JIT (#8338) */
  5501| 			seq_points = FALSE;
  5502| 		}
  5503| 	}
  5504| 	if (cfg->prof_coverage) {
  5505| 		if (cfg->compile_aot)
  5506| 			g_error ("Coverage profiling is not supported with AOT.");
  5507| 		INLINE_FAILURE ("coverage profiling");
  5508| 		cfg->coverage_info = mono_profiler_coverage_alloc (cfg->method, header->code_size);
  5509| 	}
  5510| 	if ((cfg->gen_sdb_seq_points && cfg->method == method) || cfg->prof_coverage) {
  5511| 		minfo = mono_debug_lookup_method (method);
  5512| 		if (minfo) {
  5513| 			MonoSymSeqPoint *sps;
  5514| 			int n_il_offsets;
  5515| 			mono_debug_get_seq_points (minfo, NULL, NULL, NULL, &sps, &n_il_offsets);
  5516| 			seq_point_locs = mono_bitset_mem_new (mono_mempool_alloc0 (cfg->mempool, mono_bitset_alloc_size (header->code_size, 0)), header->code_size, 0);
  5517| 			seq_point_set_locs = mono_bitset_mem_new (mono_mempool_alloc0 (cfg->mempool, mono_bitset_alloc_size (header->code_size, 0)), header->code_size, 0);
  5518| 			sym_seq_points = TRUE;
  5519| 			for (int i = 0; i < n_il_offsets; ++i) {
  5520| 				if (GINT_TO_UINT32(sps [i].il_offset) < header->code_size)
  5521| 					mono_bitset_set_fast (seq_point_locs, sps [i].il_offset);
  5522| 			}
  5523| 			g_free (sps);
  5524| 			MonoDebugMethodAsyncInfo* asyncMethod = mono_debug_lookup_method_async_debug_info (method);
  5525| 			if (asyncMethod) {
  5526| 				for (int i = 0; asyncMethod != NULL && i < asyncMethod->num_awaits; i++)
  5527| 				{
  5528| 					mono_bitset_set_fast (seq_point_locs, asyncMethod->resume_offsets[i]);
  5529| 					mono_bitset_set_fast (seq_point_locs, asyncMethod->yield_offsets[i]);
  5530| 				}
  5531| 				mono_debug_free_method_async_debug_info (asyncMethod);
  5532| 			}
  5533| 		} else if (!method->wrapper_type && !method->dynamic && mono_debug_image_has_debug_info (m_class_get_image (method->klass))) {
  5534| 			/* Methods without line number info like auto-generated property accessors */
  5535| 			seq_point_locs = mono_bitset_mem_new (mono_mempool_alloc0 (cfg->mempool, mono_bitset_alloc_size (header->code_size, 0)), header->code_size, 0);
  5536| 			seq_point_set_locs = mono_bitset_mem_new (mono_mempool_alloc0 (cfg->mempool, mono_bitset_alloc_size (header->code_size, 0)), header->code_size, 0);
  5537| 			sym_seq_points = TRUE;
  5538| 		}
  5539| 	}
  5540| 	/*
  5541| 	 * Methods without init_locals set could cause asserts in various passes
  5542| 	 * (#497220). To work around this, we emit dummy initialization opcodes
  5543| 	 * (OP_DUMMY_ICONST etc.) which generate no code. These are only supported
  5544| 	 * on some platforms.
  5545| 	 */
  5546| 	if (cfg->opt & MONO_OPT_UNSAFE)
  5547| 		init_locals = header->init_locals;
  5548| 	else
  5549| 		init_locals = TRUE;
  5550| 	method_definition = method;
  5551| 	while (method_definition->is_inflated) {
  5552| 		MonoMethodInflated *imethod = (MonoMethodInflated *) method_definition;
  5553| 		method_definition = imethod->declaring;
  5554| 	}
  5555| 	if (sig->is_inflated)
  5556| 		generic_context = mono_method_get_context (method);
  5557| 	else if (generic_container)
  5558| 		generic_context = &generic_container->context;
  5559| 	cfg->generic_context = generic_context;
  5560| 	if (!cfg->gshared) {
  5561| 		gboolean check_type_parameter = TRUE;
  5562| 		if (method->wrapper_type == MONO_WRAPPER_OTHER) {
  5563| 			WrapperInfo *info = mono_marshal_get_wrapper_info (method);
  5564| 			g_assert (info);
  5565| 			if (info->subtype == WRAPPER_SUBTYPE_UNSAFE_ACCESSOR)
  5566| 				check_type_parameter = FALSE;
  5567| 		}
  5568| 		if (check_type_parameter)
  5569| 			g_assert (!sig->has_type_parameters);
  5570| 	}
  5571| 	if (sig->generic_param_count && method->wrapper_type == MONO_WRAPPER_NONE) {
  5572| 		g_assert (method->is_inflated);
  5573| 		g_assert (mono_method_get_context (method)->method_inst);
  5574| 	}
  5575| 	if (method->is_inflated && mono_method_get_context (method)->method_inst)
  5576| 		g_assert (sig->generic_param_count);
  5577| 	if (cfg->method == method) {
  5578| 		cfg->real_offset = 0;
  5579| 	} else {
  5580| 		cfg->real_offset = inline_offset;
  5581| 	}
  5582| 	cfg->cil_offset_to_bb = (MonoBasicBlock **)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoBasicBlock*) * header->code_size);
  5583| 	cfg->cil_offset_to_bb_len = header->code_size;
  5584| 	if (cfg->verbose_level > 2)
  5585| 		printf ("method to IR %s\n", mono_method_full_name (method, TRUE));
  5586| 	param_types = (MonoType **)mono_mempool_alloc (cfg->mempool, sizeof (MonoType*) * num_args);
  5587| 	if (sig->hasthis)
  5588| 		param_types [0] = m_class_is_valuetype (method->klass) ? m_class_get_this_arg (method->klass) : m_class_get_byval_arg (method->klass);
  5589| 	for (n = 0; n < sig->param_count; ++n)
  5590| 		param_types [n + sig->hasthis] = sig->params [n];
  5591| 	cfg->arg_types = param_types;
  5592| 	cfg->dont_inline = g_list_prepend (cfg->dont_inline, method);
  5593| 	if (cfg->method == method) {
  5594| 		/* ENTRY BLOCK */
  5595| 		NEW_BBLOCK (cfg, start_bblock);
  5596| 		cfg->bb_entry = start_bblock;
  5597| 		start_bblock->cil_code = NULL;
  5598| 		start_bblock->cil_length = 0;
  5599| 		/* EXIT BLOCK */
  5600| 		NEW_BBLOCK (cfg, end_bblock);
  5601| 		cfg->bb_exit = end_bblock;
  5602| 		end_bblock->cil_code = NULL;
  5603| 		end_bblock->cil_length = 0;
  5604| 		end_bblock->flags |= BB_INDIRECT_JUMP_TARGET;
  5605| 		g_assert (cfg->num_bblocks == 2);
  5606| 		arg_array = cfg->args;
  5607| 		if (header->num_clauses) {
  5608| 			cfg->spvars = g_hash_table_new (NULL, NULL);
  5609| 			cfg->exvars = g_hash_table_new (NULL, NULL);
  5610| 		}
  5611| 		cfg->clause_is_dead = mono_mempool_alloc0 (cfg->mempool, sizeof (gboolean) * header->num_clauses);
  5612| 		/* handle exception clauses */
  5613| 		for (unsigned int i = 0; i < header->num_clauses; ++i) {
  5614| 			MonoBasicBlock *try_bb;
  5615| 			MonoExceptionClause *clause = &header->clauses [i];
  5616| 			GET_BBLOCK (cfg, try_bb, ip + clause->try_offset);
  5617| 			try_bb->real_offset = clause->try_offset;
  5618| 			try_bb->try_start = TRUE;
  5619| 			GET_BBLOCK (cfg, tblock, ip + clause->handler_offset);
  5620| 			tblock->real_offset = clause->handler_offset;
  5621| 			tblock->flags |= BB_EXCEPTION_HANDLER;
  5622| 			if (clause->flags == MONO_EXCEPTION_CLAUSE_FINALLY)
  5623| 				mono_create_exvar_for_offset (cfg, clause->handler_offset);
  5624| 			/*
  5625| 			 * Linking the try block with the EH block hinders inlining as we won't be able to
  5626| 			 * merge the bblocks from inlining and produce an artificial hole for no good reason.
  5627| 			 */
  5628| 			if (COMPILE_LLVM (cfg))
  5629| 				link_bblock (cfg, try_bb, tblock);
  5630| 			if (*(ip + clause->handler_offset) == CEE_POP)
  5631| 				tblock->flags |= BB_EXCEPTION_DEAD_OBJ;
  5632| 			if (clause->flags == MONO_EXCEPTION_CLAUSE_FINALLY ||
  5633| 			    clause->flags == MONO_EXCEPTION_CLAUSE_FILTER ||
  5634| 			    clause->flags == MONO_EXCEPTION_CLAUSE_FAULT) {
  5635| 				MONO_INST_NEW (cfg, ins, OP_START_HANDLER);
  5636| 				MONO_ADD_INS (tblock, ins);
  5637| 				if (seq_points && clause->flags != MONO_EXCEPTION_CLAUSE_FINALLY && clause->flags != MONO_EXCEPTION_CLAUSE_FILTER) {
  5638| 					/* finally clauses already have a seq point */
  5639| 					/* seq points for filter clauses are emitted below */
  5640| 					NEW_SEQ_POINT (cfg, ins, clause->handler_offset, TRUE);
  5641| 					MONO_ADD_INS (tblock, ins);
  5642| 				}
  5643| 				/* todo: is a fault block unsafe to optimize? */
  5644| 				if (clause->flags == MONO_EXCEPTION_CLAUSE_FAULT)
  5645| 					tblock->flags |= BB_EXCEPTION_UNSAFE;
  5646| 			}
  5647| 			/*printf ("clause try IL_%04x to IL_%04x handler %d at IL_%04x to IL_%04x\n", clause->try_offset, clause->try_offset + clause->try_len, clause->flags, clause->handler_offset, clause->handler_offset + clause->handler_len);
  5648| 			  while (p < end) {
  5649| 			  printf ("%s", mono_disasm_code_one (NULL, method, p, &p));
  5650| 			  }*/
  5651| 			/* catch and filter blocks get the exception object on the stack */
  5652| 			if (clause->flags == MONO_EXCEPTION_CLAUSE_NONE ||
  5653| 			    clause->flags == MONO_EXCEPTION_CLAUSE_FILTER) {
  5654| 				/* mostly like handle_stack_args (), but just sets the input args */
  5655| 				/* printf ("handling clause at IL_%04x\n", clause->handler_offset); */
  5656| 				tblock->in_scount = 1;
  5657| 				tblock->in_stack = (MonoInst **)mono_mempool_alloc (cfg->mempool, sizeof (MonoInst*));
  5658| 				tblock->in_stack [0] = mono_create_exvar_for_offset (cfg, clause->handler_offset);
  5659| 				cfg->cbb = tblock;
  5660| #ifdef MONO_CONTEXT_SET_LLVM_EXC_REG
  5661| 				/* The EH code passes in the exception in a register to both JITted and LLVM compiled code */
  5662| 				if (!cfg->compile_llvm) {
  5663| 					MONO_INST_NEW (cfg, ins, OP_GET_EX_OBJ);
  5664| 					ins->dreg = tblock->in_stack [0]->dreg;
  5665| 					MONO_ADD_INS (tblock, ins);
  5666| 				}
  5667| #else
  5668| 				MonoInst *dummy_use;
  5669| 				/*
  5670| 				 * Add a dummy use for the exvar so its liveness info will be
  5671| 				 * correct.
  5672| 				 */
  5673| 				EMIT_NEW_DUMMY_USE (cfg, dummy_use, tblock->in_stack [0]);
  5674| #endif
  5675| 				if (seq_points && clause->flags == MONO_EXCEPTION_CLAUSE_FILTER) {
  5676| 					NEW_SEQ_POINT (cfg, ins, clause->handler_offset, TRUE);
  5677| 					MONO_ADD_INS (tblock, ins);
  5678| 				}
  5679| 				if (clause->flags == MONO_EXCEPTION_CLAUSE_FILTER) {
  5680| 					GET_BBLOCK (cfg, tblock, ip + clause->data.filter_offset);
  5681| 					tblock->flags |= BB_EXCEPTION_HANDLER;
  5682| 					tblock->real_offset = clause->data.filter_offset;
  5683| 					tblock->in_scount = 1;
  5684| 					tblock->in_stack = (MonoInst **)mono_mempool_alloc (cfg->mempool, sizeof (MonoInst*));
  5685| 					/* The filter block shares the exvar with the handler block */
  5686| 					tblock->in_stack [0] = mono_create_exvar_for_offset (cfg, clause->handler_offset);
  5687| 					MONO_INST_NEW (cfg, ins, OP_START_HANDLER);
  5688| 					MONO_ADD_INS (tblock, ins);
  5689| 				}
  5690| 			}
  5691| 			if (clause->flags != MONO_EXCEPTION_CLAUSE_FILTER &&
  5692| 					clause->data.catch_class &&
  5693| 					cfg->gshared &&
  5694| 					mono_class_check_context_used (clause->data.catch_class)) {
  5695| 				/*
  5696| 				 * In shared generic code with catch
  5697| 				 * clauses containing type variables
  5698| 				 * the exception handling code has to
  5699| 				 * be able to get to the rgctx.
  5700| 				 * Therefore we have to make sure that
  5701| 				 * the vtable/mrgctx argument (for
  5702| 				 * static or generic methods) or the
  5703| 				 * "this" argument (for non-static
  5704| 				 * methods) are live.
  5705| 				 */
  5706| 				if ((method->flags & METHOD_ATTRIBUTE_STATIC) ||
  5707| 						mini_method_get_context (method)->method_inst ||
  5708| 						m_class_is_valuetype (method->klass)) {
  5709| 					mono_get_vtable_var (cfg);
  5710| 				} else {
  5711| 					MonoInst *dummy_use;
  5712| 					EMIT_NEW_DUMMY_USE (cfg, dummy_use, arg_array [0]);
  5713| 				}
  5714| 			}
  5715| 		}
  5716| 	} else {
  5717| 		arg_array = g_newa (MonoInst*, num_args);
  5718| 		cfg->cbb = start_bblock;
  5719| 		cfg->args = arg_array;
  5720| 		mono_save_args (cfg, sig, inline_args);
  5721| 	}
  5722| 	if (cfg->method == method && cfg->self_init && cfg->compile_aot && !COMPILE_LLVM (cfg)) {
  5723| 		MonoMethod *wrapper;
  5724| 		MonoInst *args [2];
  5725| 		int idx;
  5726| 		/*
  5727| 		 * Emit code to initialize this method by calling the init wrapper emitted by LLVM.
  5728| 		 * This is not efficient right now, but its only used for the methods which fail
  5729| 		 * LLVM compilation.
  5730| 		 * FIXME: Optimize this
  5731| 		 */
  5732| 		g_assert (!cfg->gshared);
  5733| 		wrapper = mono_marshal_get_aot_init_wrapper (AOT_INIT_METHOD);
  5734| 		/* Emit this into the entry bb so it comes before the GC safe point which depends on an inited GOT */
  5735| 		cfg->cbb = cfg->bb_entry;
  5736| 		idx = mono_aot_get_method_index (cfg->method);
  5737| 		EMIT_NEW_ICONST (cfg, args [0], idx);
  5738| 		/* Dummy */
  5739| 		EMIT_NEW_ICONST (cfg, args [1], 0);
  5740| 		mono_emit_method_call (cfg, wrapper, args, NULL);
  5741| 	}
  5742| 	if (cfg->llvm_only)
  5743| 		g_assert (cfg->interp);
  5744| 	if (cfg->llvm_only && cfg->interp && cfg->method == method && !cfg->deopt && !cfg->interp_entry_only) {
  5745| 		if (header->num_clauses) {
  5746| 			for (guint i = 0; i < header->num_clauses; ++i) {
  5747| 				MonoExceptionClause *clause = &header->clauses [i];
  5748| 				/* Finally clauses are checked after the remove_finally pass */
  5749| 				if (clause->flags != MONO_EXCEPTION_CLAUSE_FINALLY)
  5750| 					cfg->interp_entry_only = TRUE;
  5751| 			}
  5752| 		}
  5753| 	}
  5754| 	/* we use a separate basic block for the initialization code */
  5755| 	NEW_BBLOCK (cfg, init_localsbb);
  5756| 	if (cfg->method == method)
  5757| 		cfg->bb_init = init_localsbb;
  5758| 	init_localsbb->real_offset = cfg->real_offset;
  5759| 	start_bblock->next_bb = init_localsbb;
  5760| 	link_bblock (cfg, start_bblock, init_localsbb);
  5761| 	init_localsbb2 = init_localsbb;
  5762| 	cfg->cbb = init_localsbb;
  5763| 	/*
  5764| 	 * If the method receives an mrgctx, store all rgctx entries in mrgctx->entries instead of in the
  5765| 	 * class rgctx.
  5766| 	 * Disable for gsharedvt for now since the handling of gsharedvt related rgctx entries for
  5767| 	 * MONO_PATCH_INFO_GSHARED_METHOD_INFO is not implemented yet.
  5768| 	 */
  5769| 	if (cfg->gshared && cfg->method == method && cfg->rgctx_access == MONO_RGCTX_ACCESS_MRGCTX) {
  5770| 		MonoGSharedMethodInfo *info;
  5771| 		MonoInst *args [2];
  5772| 		/* Allocate into permanent memory since its the key in MonoJumpInfo */
  5773| 		info = (MonoGSharedMethodInfo *)mono_mem_manager_alloc0 (cfg->mem_manager, sizeof (MonoGSharedMethodInfo));
  5774| 		/* Will be copied into permanent memory in mini_method_compile () */
  5775| 		info->method = cfg->method;
  5776| 		info->count_entries = 16;
  5777| 		info->entries = (MonoRuntimeGenericContextInfoTemplate *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoRuntimeGenericContextInfoTemplate) * info->count_entries);
  5778| 		cfg->gshared_info = info;
  5779| 		args [0] = mono_get_mrgctx_var (cfg);
  5780| 		if (cfg->compile_aot)
  5781| 			args [1] = mini_emit_runtime_constant (cfg, MONO_PATCH_INFO_GSHARED_METHOD_INFO, info);
  5782| 		else
  5783| 			EMIT_NEW_PCONST (cfg, args [1], info);
  5784| 		cfg->init_method_rgctx_ins_arg = args [1];
  5785| 		if (COMPILE_LLVM (cfg) || cfg->backend->have_init_mrgctx) {
  5786| 			MONO_INST_NEW (cfg, ins, OP_INIT_MRGCTX);
  5787| 			ins->sreg1 = args [0]->dreg;
  5788| 			ins->sreg2 = args [1]->dreg;
  5789| 			MONO_ADD_INS (cfg->cbb, ins);
  5790| 			cfg->init_method_rgctx_ins = ins;
  5791| 		} else {
  5792| 			cfg->init_method_rgctx_ins = mono_emit_jit_icall (cfg, mini_init_method_rgctx, args);
  5793| 		}
  5794| 	}
  5795| 	if (cfg->gsharedvt && cfg->method == method) {
  5796| 		MonoGSharedVtMethodInfo *info;
  5797| 		MonoInst *var, *locals_var;
  5798| 		int dreg;
  5799| 		info = (MonoGSharedVtMethodInfo *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoGSharedVtMethodInfo));
  5800| 		info->method = cfg->method;
  5801| 		info->count_entries = 16;
  5802| 		info->entries = (MonoRuntimeGenericContextInfoTemplate *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoRuntimeGenericContextInfoTemplate) * info->count_entries);
  5803| 		cfg->gsharedvt_info = info;
  5804| 		var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
  5805| 		/*
  5806| 		 * Decomposing ldaddr creates uses for this and gsharedvt_locals_var, so
  5807| 		 * when we emit an ldaddr, we emit dummy uses for these in handle_gsharedvt_ldaddr ().
  5808| 		 */
  5809| 		cfg->gsharedvt_info_var = var;
  5810| 		ins = emit_get_rgctx_gsharedvt_method (cfg, mini_method_check_context_used (cfg, method), method, info);
  5811| 		MONO_EMIT_NEW_UNALU (cfg, OP_MOVE, var->dreg, ins->dreg);
  5812| 		/* Allocate locals */
  5813| 		locals_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
  5814| 		/* prevent it from being register allocated */
  5815| 		cfg->gsharedvt_locals_var = locals_var;
  5816| 		dreg = alloc_ireg (cfg);
  5817| 		MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADI4_MEMBASE, dreg, var->dreg, MONO_STRUCT_OFFSET (MonoGSharedVtMethodRuntimeInfo, locals_size));
  5818| 		MONO_INST_NEW (cfg, ins, OP_LOCALLOC);
  5819| 		ins->dreg = locals_var->dreg;
  5820| 		ins->sreg1 = dreg;
  5821| 		MONO_ADD_INS (cfg->cbb, ins);
  5822| 		cfg->gsharedvt_locals_var_ins = ins;
  5823| 		cfg->flags |= MONO_CFG_HAS_ALLOCA;
  5824| 		/*
  5825| 		if (init_locals)
  5826| 			ins->flags |= MONO_INST_INIT;
  5827| 		*/
  5828| 		if (cfg->llvm_only) {
  5829| 			init_localsbb = cfg->cbb;
  5830| 			init_localsbb2 = cfg->cbb;
  5831| 		}
  5832| 	}
  5833| 	if (cfg->deopt) {
  5834| 		/*
  5835| 		 * Push an LMFExt frame which points to a MonoMethodILState structure.
  5836| 		 */
  5837| 		emit_push_lmf (cfg);
  5838| 		/* The type doesn't matter, the llvm backend will use the correct type */
  5839| 		MonoInst *il_state_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
  5840| 		il_state_var->flags |= MONO_INST_VOLATILE;
  5841| 		cfg->il_state_var = il_state_var;
  5842| 		EMIT_NEW_VARLOADA (cfg, ins, cfg->il_state_var, NULL);
  5843| 		int il_state_addr_reg = ins->dreg;
  5844| 		/* il_state->method = method */
  5845| 		MonoInst *method_ins = emit_get_rgctx_method (cfg, -1, cfg->method, MONO_RGCTX_INFO_METHOD);
  5846| 		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, il_state_addr_reg, MONO_STRUCT_OFFSET (MonoMethodILState, method), method_ins->dreg);
  5847| 		EMIT_NEW_VARLOADA (cfg, ins, cfg->lmf_var, NULL);
  5848| 		int lmf_reg = ins->dreg;
  5849| 		/* lmf->kind = MONO_LMFEXT_IL_STATE */
  5850| 		MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STOREI4_MEMBASE_IMM, lmf_reg, MONO_STRUCT_OFFSET (MonoLMFExt, kind), MONO_LMFEXT_IL_STATE);
  5851| 		/* lmf->il_state = il_state */
  5852| 		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, lmf_reg, MONO_STRUCT_OFFSET (MonoLMFExt, il_state), il_state_addr_reg);
  5853| 		/* emit_get_rgctx_method () might create new bblocks */
  5854| 		if (cfg->llvm_only) {
  5855| 			init_localsbb = cfg->cbb;
  5856| 			init_localsbb2 = cfg->cbb;
  5857| 		}
  5858| 	}
  5859| 	if (cfg->llvm_only && cfg->interp && cfg->method == method) {
  5860| 		if (cfg->interp_entry_only)
  5861| 			emit_llvmonly_interp_entry (cfg, header);
  5862| 	}
  5863| 	/* FIRST CODE BLOCK */
  5864| 	NEW_BBLOCK (cfg, tblock);
  5865| 	tblock->cil_code = ip;
  5866| 	cfg->cbb = tblock;
  5867| 	cfg->ip = ip;
  5868| 	init_localsbb->next_bb = cfg->cbb;
  5869| 	link_bblock (cfg, init_localsbb, cfg->cbb);
  5870| 	ADD_BBLOCK (cfg, tblock);
  5871| 	CHECK_CFG_EXCEPTION;
  5872| 	if (header->code_size == 0)
  5873| 		UNVERIFIED;
  5874| 	if (get_basic_blocks (cfg, header, cfg->real_offset, ip, end, &err_pos)) {
  5875| 		ip = err_pos;
  5876| 		UNVERIFIED;
  5877| 	}
  5878| 	if (cfg->method == method) {
  5879| 		int breakpoint_id = mono_debugger_method_has_breakpoint (method);
  5880| 		if (breakpoint_id) {
  5881| 			if (COMPILE_LLVM (cfg)) {
  5882| 				mono_emit_jit_icall (cfg, mono_break, NULL);
  5883| 			} else {
  5884| 				MONO_INST_NEW (cfg, ins, OP_BREAK);
  5885| 				MONO_ADD_INS (cfg->cbb, ins);
  5886| 			}
  5887| 		}
  5888| 		mono_debug_init_method (cfg, cfg->cbb, breakpoint_id);
  5889| 	}
  5890| 	for (n = 0; n < header->num_locals; ++n) {
  5891| 		if (header->locals [n]->type == MONO_TYPE_VOID && !m_type_is_byref (header->locals [n]))
  5892| 			UNVERIFIED;
  5893| 	}
  5894| 	class_inits = NULL;
  5895| 	/* We force the vtable variable here for all shared methods
  5896| 	   for the possibility that they might show up in a stack
  5897| 	   trace where their exact instantiation is needed. */
  5898| 	if (cfg->gshared && method == cfg->method) {
  5899| 		if ((method->flags & METHOD_ATTRIBUTE_STATIC) ||
  5900| 				mini_method_get_context (method)->method_inst ||
  5901| 				m_class_is_valuetype (method->klass)) {
  5902| 			mono_get_vtable_var (cfg);
  5903| 		} else {
  5904| 			/* FIXME: Is there a better way to do this?
  5905| 			   We need the variable live for the duration
  5906| 			   of the whole method. */
  5907| 			if (!COMPILE_LLVM (cfg))
  5908| 				cfg->args [0]->flags |= MONO_INST_VOLATILE;
  5909| 		}
  5910| 	}
  5911| 	/* add a check for this != NULL to inlined methods */
  5912| 	if (is_virtual_call) {
  5913| 		MonoInst *arg_ins;
  5914| 		if (!(cfg->llvm_only && m_class_is_valuetype (method->klass) && header->code_size == 1 && header->code [0] == CEE_RET)) {
  5915| 			NEW_ARGLOAD (cfg, arg_ins, 0);
  5916| 			MONO_ADD_INS (cfg->cbb, arg_ins);
  5917| 			MONO_EMIT_NEW_CHECK_THIS (cfg, arg_ins->dreg);
  5918| 		}
  5919| 	}
  5920| 	skip_dead_blocks = !dont_verify;
  5921| 	if (skip_dead_blocks) {
  5922| 		original_bb = bb = mono_basic_block_split (method, cfg->error, header);
  5923| 		CHECK_CFG_ERROR;
  5924| 		g_assert (bb);
  5925| 	}
  5926| 	if (cfg->gsharedvt_min) {
  5927| 		if (mini_is_gsharedvt_variable_signature (sig))
  5928| 			GSHAREDVT_FAILURE (*cfg->cil_start);
  5929| 		for (int i = 0; i < header->num_locals; ++i) {
  5930| 			if (mini_is_gsharedvt_variable_type (header->locals [i]))
  5931| 				GSHAREDVT_FAILURE (*cfg->cil_start);
  5932| 		}
  5933| 	}
  5934| 	/* we use a spare stack slot in SWITCH and NEWOBJ and others */
  5935| 	stack_start = sp = (MonoInst **)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoInst*) * (header->max_stack + 1));
  5936| 	ins_flag = 0;
  5937| 	start_new_bblock = 0;
  5938| 	MonoOpcodeEnum il_op; il_op = MonoOpcodeEnum_Invalid;
  5939| 	emit_set_deopt_il_offset (cfg, GPTRDIFF_TO_INT (ip - cfg->cil_start));
  5940| 	for (guchar *next_ip = ip; ip < end; ip = next_ip) {
  5941| 		MonoOpcodeEnum previous_il_op = il_op;
  5942| 		const guchar *tmp_ip = ip;
  5943| 		const int op_size = mono_opcode_value_and_size (&tmp_ip, end, &il_op);
  5944| 		CHECK_OPSIZE (op_size);
  5945| 		next_ip += op_size;
  5946| 		if (cfg->method == method)
  5947| 			cfg->real_offset = GPTRDIFF_TO_UINT (ip - header->code);
  5948| 		else
  5949| 			cfg->real_offset = inline_offset;
  5950| 		cfg->ip = ip;
  5951| 		context_used = 0;
  5952| 		if (start_new_bblock) {
  5953| 			cfg->cbb->cil_length = GPTRDIFF_TO_INT32 (ip - cfg->cbb->cil_code);
  5954| 			if (start_new_bblock == 2) {
  5955| 				g_assert (ip == tblock->cil_code);
  5956| 			} else {
  5957| 				GET_BBLOCK (cfg, tblock, ip);
  5958| 			}
  5959| 			cfg->cbb->next_bb = tblock;
  5960| 			cfg->cbb = tblock;
  5961| 			start_new_bblock = 0;
  5962| 			for (int i = 0; i < cfg->cbb->in_scount; ++i) {
  5963| 				if (cfg->verbose_level > 3)
  5964| 					printf ("loading %d from temp %d\n", i, (int)cfg->cbb->in_stack [i]->inst_c0);
  5965| 				EMIT_NEW_TEMPLOAD (cfg, ins, cfg->cbb->in_stack [i]->inst_c0);
  5966| 				*sp++ = ins;
  5967| 			}
  5968| 			if (class_inits)
  5969| 				g_slist_free (class_inits);
  5970| 			class_inits = NULL;
  5971| 			emit_set_deopt_il_offset (cfg, GPTRDIFF_TO_INT (ip - cfg->cil_start));
  5972| 		} else {
  5973| 			if ((tblock = cfg->cil_offset_to_bb [ip - cfg->cil_start]) && (tblock != cfg->cbb)) {
  5974| 				link_bblock (cfg, cfg->cbb, tblock);
  5975| 				if (sp != stack_start) {
  5976| 					handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start));
  5977| 					sp = stack_start;
  5978| 					CHECK_UNVERIFIABLE (cfg);
  5979| 				}
  5980| 				cfg->cbb->next_bb = tblock;
  5981| 				cfg->cbb = tblock;
  5982| 				for (int i = 0; i < cfg->cbb->in_scount; ++i) {
  5983| 					if (cfg->verbose_level > 3)
  5984| 						printf ("loading %d from temp %d\n", i, (int)cfg->cbb->in_stack [i]->inst_c0);
  5985| 					EMIT_NEW_TEMPLOAD (cfg, ins, cfg->cbb->in_stack [i]->inst_c0);
  5986| 					*sp++ = ins;
  5987| 				}
  5988| 				g_slist_free (class_inits);
  5989| 				class_inits = NULL;
  5990| 				emit_set_deopt_il_offset (cfg, GPTRDIFF_TO_INT (ip - cfg->cil_start));
  5991| 			}
  5992| 		}
  5993| 		/*
  5994| 		 * Methods with AggressiveInline flag could be inlined even if the class has a cctor.
  5995| 		 * This might create a branch so emit it in the first code bblock instead of into initlocals_bb.
  5996| 		 */
  5997| 		if (ip - header->code == 0 && cfg->method != method && cfg->compile_aot && (method->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING) && mono_class_needs_cctor_run (method->klass, method))
  5998| 			emit_class_init (cfg, method->klass, FALSE);
  5999| 		if (skip_dead_blocks) {
  6000| 			int ip_offset = GPTRDIFF_TO_INT (ip - header->code);
  6001| 			if (ip_offset == bb->end)
  6002| 				bb = bb->next;
  6003| 			if (bb->dead) {
  6004| 				g_assert (op_size > 0); /*The BB formation pass must catch all bad ops*/
  6005| 				if (cfg->verbose_level > 3) printf ("SKIPPING DEAD OP at %x\n", ip_offset);
  6006| 				if (ip_offset + op_size == bb->end) {
  6007| 					MONO_INST_NEW (cfg, ins, OP_NOP);
  6008| 					MONO_ADD_INS (cfg->cbb, ins);
  6009| 					start_new_bblock = 1;
  6010| 				}
  6011| 				continue;
  6012| 			}
  6013| 		}
  6014| 		/*
  6015| 		 * Sequence points are points where the debugger can place a breakpoint.
  6016| 		 * Currently, we generate these automatically at points where the IL
  6017| 		 * stack is empty.
  6018| 		 */
  6019| 		if (seq_points && ((!sym_seq_points && (sp == stack_start)) || (sym_seq_points && mono_bitset_test_fast (seq_point_locs, ip - header->code)))) {
  6020| 			/*
  6021| 			 * Make methods interruptible at the beginning, and at the targets of
  6022| 			 * backward branches.
  6023| 			 * Also, do this at the start of every bblock in methods with clauses too,
  6024| 			 * to be able to handle instructions with inprecise control flow like
  6025| 			 * throw/endfinally.
  6026| 			 * Backward branches are handled at the end of method-to-ir ().
  6027| 			 */
  6028| 			gboolean intr_loc = ip == header->code || (!cfg->cbb->last_ins && cfg->header->num_clauses);
  6029| 			gboolean sym_seq_point = sym_seq_points && mono_bitset_test_fast (seq_point_locs, ip - header->code);
  6030| 			/* Avoid sequence points on empty IL like .volatile */
  6031| 			NEW_SEQ_POINT (cfg, ins, ip - header->code, intr_loc);
  6032| 			if ((sp != stack_start) && !sym_seq_point)
  6033| 				ins->flags |= MONO_INST_NONEMPTY_STACK;
  6034| 			MONO_ADD_INS (cfg->cbb, ins);
  6035| 			if (sym_seq_points)
  6036| 				mono_bitset_set_fast (seq_point_set_locs, ip - header->code);
  6037| 			if (cfg->prof_coverage) {
  6038| 				ptrdiff_t cil_offset = ip - header->code;
  6039| 				gpointer counter = &cfg->coverage_info->data [cil_offset].count;
  6040| 				cfg->coverage_info->data [cil_offset].cil_code = ip;
  6041| 				if (mono_arch_opcode_supported (OP_ATOMIC_ADD_I4)) {
  6042| 					MonoInst *one_ins, *load_ins;
  6043| 					EMIT_NEW_PCONST (cfg, load_ins, counter);
  6044| 					EMIT_NEW_ICONST (cfg, one_ins, 1);
  6045| 					MONO_INST_NEW (cfg, ins, OP_ATOMIC_ADD_I4);
  6046| 					ins->dreg = mono_alloc_ireg (cfg);
  6047| 					ins->inst_basereg = load_ins->dreg;
  6048| 					ins->inst_offset = 0;
  6049| 					ins->sreg2 = one_ins->dreg;
  6050| 					ins->type = STACK_I4;
  6051| 					MONO_ADD_INS (cfg->cbb, ins);
  6052| 				} else {
  6053| 					EMIT_NEW_PCONST (cfg, ins, counter);
  6054| 					MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STORE_MEMBASE_IMM, ins->dreg, 0, 1);
  6055| 				}
  6056| 			}
  6057| 		}
  6058| 		cfg->cbb->real_offset = cfg->real_offset;
  6059| 		if (cfg->verbose_level > 3)
  6060| 			printf ("converting (in B%d: stack: %d) %s", cfg->cbb->block_num, GPTRDIFF_TO_INT (sp - stack_start), mono_disasm_code_one (NULL, method, ip, NULL));
  6061| 		/*
  6062| 		 * This is used to compute BB_HAS_SIDE_EFFECTS, which is used for the elimination of
  6063| 		 * foreach finally clauses, so only IL opcodes which occur in such clauses
  6064| 		 * need to set this.
  6065| 		 */
  6066| 		ins_has_side_effect = TRUE;
  6067| 		gboolean emit_widen = TRUE;
  6068| 		gboolean tailcall = FALSE;
  6069| 		gboolean common_call = FALSE;
  6070| 		MonoInst *keep_this_alive = NULL;
  6071| 		MonoMethod *cmethod = NULL;
  6072| 		MonoMethodSignature *fsig = NULL;
  6073| 		gboolean need_seq_point = FALSE;
  6074| 		gboolean push_res = TRUE;
  6075| 		gboolean skip_ret = FALSE;
  6076| 		gboolean tailcall_remove_ret = FALSE;
  6077| 		MonoOpcodeParameter parameter;
  6078| 		const MonoOpcodeInfo* info = mono_opcode_decode (ip, op_size, il_op, &parameter);
  6079| 		g_assert (info);
  6080| 		n = parameter.i32;
  6081| 		token = parameter.i32;
  6082| 		target = parameter.branch_target;
  6083| 		const int pushes = info->pushes;
  6084| 		const int pops = info->pops;
  6085| 		if (pushes >= 0 && pops >= 0) {
  6086| 			g_assert (pushes - pops <= 1);
  6087| 			if (pushes - pops == 1)
  6088| 				CHECK_STACK_OVF ();
  6089| 		}
  6090| 		if (pops >= 0)
  6091| 			CHECK_STACK (pops);
  6092| 		switch (il_op) {
  6093| 		case MONO_CEE_NOP:
  6094| 			if (seq_points && !sym_seq_points && sp != stack_start) {
  6095| 				/*
  6096| 				 * The C# compiler uses these nops to notify the JIT that it should
  6097| 				 * insert seq points.
  6098| 				 */
  6099| 				NEW_SEQ_POINT (cfg, ins, ip - header->code, FALSE);
  6100| 				MONO_ADD_INS (cfg->cbb, ins);
  6101| 			}
  6102| 			if (cfg->keep_cil_nops)
  6103| 				MONO_INST_NEW (cfg, ins, OP_HARD_NOP);
  6104| 			else
  6105| 				MONO_INST_NEW (cfg, ins, OP_NOP);
  6106| 			MONO_ADD_INS (cfg->cbb, ins);
  6107| 			emitted_funccall_seq_point = FALSE;
  6108| 			ins_has_side_effect = FALSE;
  6109| 			break;
  6110| 		case MONO_CEE_BREAK:
  6111| 			if (mini_should_insert_breakpoint (cfg->method)) {
  6112| 				ins = mono_emit_jit_icall (cfg, mono_debugger_agent_user_break, NULL);
  6113| 			} else {
  6114| 				MONO_INST_NEW (cfg, ins, OP_NOP);
  6115| 				MONO_ADD_INS (cfg->cbb, ins);
  6116| 			}
  6117| 			break;
  6118| 		case MONO_CEE_LDARG_0:
  6119| 		case MONO_CEE_LDARG_1:
  6120| 		case MONO_CEE_LDARG_2:
  6121| 		case MONO_CEE_LDARG_3:
  6122| 		case MONO_CEE_LDARG_S:
  6123| 		case MONO_CEE_LDARG:
  6124| 			CHECK_ARG (n);
  6125| 			if (next_ip < end && is_addressable_valuetype_load (cfg, next_ip, cfg->arg_types [n])) {
  6126| 				EMIT_NEW_ARGLOADA (cfg, ins, n);
  6127| 			} else {
  6128| 				EMIT_NEW_ARGLOAD (cfg, ins, n);
  6129| 			}
  6130| 			*sp++ = ins;
  6131| 			/*if (!m_method_is_icall (method)) */{
  6132| 				MonoMethod* callvirt_target = try_prepare_objaddr_callvirt_optimization (cfg, next_ip, end, method, generic_context, param_types [n]);
  6133| 				if (callvirt_target)
  6134| 					cmethod_override = callvirt_target;
  6135| 			}
  6136| 			break;
  6137| 		case MONO_CEE_LDLOC_0:
  6138| 		case MONO_CEE_LDLOC_1:
  6139| 		case MONO_CEE_LDLOC_2:
  6140| 		case MONO_CEE_LDLOC_3:
  6141| 		case MONO_CEE_LDLOC_S:
  6142| 		case MONO_CEE_LDLOC:
  6143| 			CHECK_LOCAL (n);
  6144| 			if (next_ip < end && is_addressable_valuetype_load (cfg, next_ip, header->locals [n])) {
  6145| 				EMIT_NEW_LOCLOADA (cfg, ins, n);
  6146| 			} else {
  6147| 				EMIT_NEW_LOCLOAD (cfg, ins, n);
  6148| 			}
  6149| 			*sp++ = ins;
  6150| 			break;
  6151| 		case MONO_CEE_STLOC_0:
  6152| 		case MONO_CEE_STLOC_1:
  6153| 		case MONO_CEE_STLOC_2:
  6154| 		case MONO_CEE_STLOC_3:
  6155| 		case MONO_CEE_STLOC_S:
  6156| 		case MONO_CEE_STLOC:
  6157| 			CHECK_LOCAL (n);
  6158| 			--sp;
  6159| 			*sp = convert_value (cfg, header->locals [n], *sp);
  6160| 			if (!dont_verify_stloc && target_type_is_incompatible (cfg, header->locals [n], *sp))
  6161| 				UNVERIFIED;
  6162| 			emit_stloc_ir (cfg, sp, header, n);
  6163| 			inline_costs += 1;
  6164| 			break;
  6165| 		case MONO_CEE_LDARGA_S:
  6166| 		case MONO_CEE_LDARGA:
  6167| 			CHECK_ARG (n);
  6168| 			NEW_ARGLOADA (cfg, ins, n);
  6169| 			MONO_ADD_INS (cfg->cbb, ins);
  6170| 			*sp++ = ins;
  6171| 			break;
  6172| 		case MONO_CEE_STARG_S:
  6173| 		case MONO_CEE_STARG:
  6174| 			--sp;
  6175| 			CHECK_ARG (n);
  6176| 			*sp = convert_value (cfg, param_types [n], *sp);
  6177| 			if (!dont_verify_stloc && target_type_is_incompatible (cfg, param_types [n], *sp))
  6178| 				UNVERIFIED;
  6179| 			emit_starg_ir (cfg, sp, n);
  6180| 			break;
  6181| 		case MONO_CEE_LDLOCA:
  6182| 		case MONO_CEE_LDLOCA_S: {
  6183| 			guchar *ldloca_ip;
  6184| 			CHECK_LOCAL (n);
  6185| 			if ((ldloca_ip = emit_optimized_ldloca_ir (cfg, next_ip, end, n))) {
  6186| 				next_ip = ldloca_ip;
  6187| 				il_op = MONO_CEE_INITOBJ;
  6188| 				inline_costs += 1;
  6189| 				break;
  6190| 			}
  6191| 			ins_has_side_effect = FALSE;
  6192| 			EMIT_NEW_LOCLOADA (cfg, ins, n);
  6193| 			*sp++ = ins;
  6194| 			break;
  6195| 		}
  6196| 		case MONO_CEE_LDNULL:
  6197| 			EMIT_NEW_PCONST (cfg, ins, NULL);
  6198| 			ins->type = STACK_OBJ;
  6199| 			*sp++ = ins;
  6200| 			break;
  6201| 		case MONO_CEE_LDC_I4_M1:
  6202| 		case MONO_CEE_LDC_I4_0:
  6203| 		case MONO_CEE_LDC_I4_1:
  6204| 		case MONO_CEE_LDC_I4_2:
  6205| 		case MONO_CEE_LDC_I4_3:
  6206| 		case MONO_CEE_LDC_I4_4:
  6207| 		case MONO_CEE_LDC_I4_5:
  6208| 		case MONO_CEE_LDC_I4_6:
  6209| 		case MONO_CEE_LDC_I4_7:
  6210| 		case MONO_CEE_LDC_I4_8:
  6211| 		case MONO_CEE_LDC_I4_S:
  6212| 		case MONO_CEE_LDC_I4:
  6213| 			EMIT_NEW_ICONST (cfg, ins, n);
  6214| 			*sp++ = ins;
  6215| 			break;
  6216| 		case MONO_CEE_LDC_I8:
  6217| 			MONO_INST_NEW (cfg, ins, OP_I8CONST);
  6218| 			ins->type = STACK_I8;
  6219| 			ins->dreg = alloc_dreg (cfg, STACK_I8);
  6220| 			ins->inst_l = parameter.i64;
  6221| 			MONO_ADD_INS (cfg->cbb, ins);
  6222| 			*sp++ = ins;
  6223| 			break;
  6224| 		case MONO_CEE_LDC_R4: {
  6225| 			float *f;
  6226| 			gboolean use_aotconst = FALSE;
  6227| #ifdef TARGET_POWERPC
  6228| 			/* FIXME: Clean this up */
  6229| 			if (cfg->compile_aot)
  6230| 				use_aotconst = TRUE;
  6231| #endif
  6232| 			/* FIXME: we should really allocate this only late in the compilation process */
  6233| 			f = (float *)mono_mem_manager_alloc (cfg->mem_manager, sizeof (float));
  6234| 			if (use_aotconst) {
  6235| 				MonoInst *cons;
  6236| 				int dreg;
  6237| 				EMIT_NEW_AOTCONST (cfg, cons, MONO_PATCH_INFO_R4, f);
  6238| 				dreg = alloc_freg (cfg);
  6239| 				EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOADR4_MEMBASE, dreg, cons->dreg, 0);
  6240| 				ins->type = GINT_TO_UINT8 (cfg->r4_stack_type);
  6241| 			} else {
  6242| 				MONO_INST_NEW (cfg, ins, OP_R4CONST);
  6243| 				ins->type = GINT_TO_UINT8 (cfg->r4_stack_type);
  6244| 				ins->dreg = alloc_dreg (cfg, STACK_R8);
  6245| 				ins->inst_p0 = f;
  6246| 				MONO_ADD_INS (cfg->cbb, ins);
  6247| 			}
  6248| 			*f = parameter.f;
  6249| 			*sp++ = ins;
  6250| 			break;
  6251| 		}
  6252| 		case MONO_CEE_LDC_R8: {
  6253| 			double *d;
  6254| 			gboolean use_aotconst = FALSE;
  6255| #ifdef TARGET_POWERPC
  6256| 			/* FIXME: Clean this up */
  6257| 			if (cfg->compile_aot)
  6258| 				use_aotconst = TRUE;
  6259| #endif
  6260| 			/* FIXME: we should really allocate this only late in the compilation process */
  6261| 			d = (double *)mono_mem_manager_alloc (cfg->mem_manager, sizeof (double));
  6262| 			if (use_aotconst) {
  6263| 				MonoInst *cons;
  6264| 				int dreg;
  6265| 				EMIT_NEW_AOTCONST (cfg, cons, MONO_PATCH_INFO_R8, d);
  6266| 				dreg = alloc_freg (cfg);
  6267| 				EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOADR8_MEMBASE, dreg, cons->dreg, 0);
  6268| 				ins->type = STACK_R8;
  6269| 			} else {
  6270| 				MONO_INST_NEW (cfg, ins, OP_R8CONST);
  6271| 				ins->type = STACK_R8;
  6272| 				ins->dreg = alloc_dreg (cfg, STACK_R8);
  6273| 				ins->inst_p0 = d;
  6274| 				MONO_ADD_INS (cfg->cbb, ins);
  6275| 			}
  6276| 			*d = parameter.d;
  6277| 			*sp++ = ins;
  6278| 			break;
  6279| 		}
  6280| 		case MONO_CEE_DUP: {
  6281| 			MonoInst *temp, *store;
  6282| 			sp--;
  6283| 			ins = *sp;
  6284| 			klass = ins->klass;
  6285| 			temp = mono_compile_create_var (cfg, type_from_stack_type (ins), OP_LOCAL);
  6286| 			EMIT_NEW_TEMPSTORE (cfg, store, temp->inst_c0, ins);
  6287| 			EMIT_NEW_TEMPLOAD (cfg, ins, temp->inst_c0);
  6288| 			ins->klass = klass;
  6289| 			*sp++ = ins;
  6290| 			EMIT_NEW_TEMPLOAD (cfg, ins, temp->inst_c0);
  6291| 			ins->klass = klass;
  6292| 			*sp++ = ins;
  6293| 			inline_costs += 2;
  6294| 			break;
  6295| 		}
  6296| 		case MONO_CEE_POP:
  6297| 			--sp;
  6298| 			break;
  6299| 		case MONO_CEE_JMP: {
  6300| 			MonoCallInst *call;
  6301| 			INLINE_FAILURE ("jmp");
  6302| 			GSHAREDVT_FAILURE (il_op);
  6303| 			if (stack_start != sp)
  6304| 				UNVERIFIED;
  6305| 			/* FIXME: check the signature matches */
  6306| 			cmethod = mini_get_method (cfg, method, token, NULL, generic_context);
  6307| 			CHECK_CFG_ERROR;
  6308| 			if (cfg->gshared && mono_method_check_context_used (cmethod))
  6309| 				GENERIC_SHARING_FAILURE (CEE_JMP);
  6310| 			mini_profiler_emit_tail_call (cfg, cmethod);
  6311| 			fsig = mono_method_signature_internal (cmethod);
  6312| 			int nargs = fsig->param_count + fsig->hasthis;
  6313| 			if (cfg->llvm_only) {
  6314| 				MonoInst **args;
  6315| 				args = (MonoInst **)mono_mempool_alloc (cfg->mempool, sizeof (MonoInst*) * nargs);
  6316| 				for (int i = 0; i < nargs; ++i)
  6317| 					EMIT_NEW_ARGLOAD (cfg, args [i], i);
  6318| 				ins = mini_emit_method_call_full (cfg, cmethod, fsig, TRUE, args, NULL, NULL, NULL);
  6319| 				/*
  6320| 				 * The code in mono-basic-block.c treats the rest of the code as dead, but we
  6321| 				 * have to emit a normal return since llvm expects it.
  6322| 				 */
  6323| 				if (cfg->ret)
  6324| 					emit_setret (cfg, ins);
  6325| 				MONO_INST_NEW (cfg, ins, OP_BR);
  6326| 				ins->inst_target_bb = end_bblock;
  6327| 				MONO_ADD_INS (cfg->cbb, ins);
  6328| 				link_bblock (cfg, cfg->cbb, end_bblock);
  6329| 				break;
  6330| 			} else {
  6331| 				/* Handle tailcalls similarly to calls */
  6332| 				DISABLE_AOT (cfg);
  6333| 				mini_emit_tailcall_parameters (cfg, fsig);
  6334| 				MONO_INST_NEW_CALL (cfg, call, OP_TAILCALL);
  6335| 				call->method = cmethod;
  6336| 				call->tailcall = TRUE;
  6337| 				call->signature = fsig;
  6338| 				call->args = (MonoInst **)mono_mempool_alloc (cfg->mempool, sizeof (MonoInst*) * nargs);
  6339| 				call->inst.inst_p0 = cmethod;
  6340| 				for (int i = 0; i < nargs; ++i)
  6341| 					EMIT_NEW_ARGLOAD (cfg, call->args [i], i);
  6342| 				if (mini_type_is_vtype (mini_get_underlying_type (call->signature->ret)))
  6343| 					call->vret_var = cfg->vret_addr;
  6344| 				mono_arch_emit_call (cfg, call);
  6345| 				cfg->param_area = MAX(cfg->param_area, call->stack_usage);
  6346| 				MONO_ADD_INS (cfg->cbb, (MonoInst*)call);
  6347| 			}
  6348| 			start_new_bblock = 1;
  6349| 			break;
  6350| 		}
  6351| 		case MONO_CEE_CALLI: {
  6352| 			MonoInst *addr;
  6353| 			MonoInst *callee = NULL;
  6354| 			common_call = TRUE; // i.e. skip_ret/push_res/seq_point logic
  6355| 			cmethod = NULL;
  6356| 			gboolean const inst_tailcall = G_UNLIKELY (debug_tailcall_try_all
  6357| 							? (next_ip < end && next_ip [0] == CEE_RET)
  6358| 							: ((ins_flag & MONO_INST_TAILCALL) != 0));
  6359| 			ins = NULL;
  6360| 			CHECK_STACK (1);
  6361| 			--sp;
  6362| 			addr = *sp;
  6363| 			g_assert (addr);
  6364| 			fsig = mini_get_signature (method, token, generic_context, cfg->error);
  6365| 			CHECK_CFG_ERROR;
  6366| 			if (cfg->gsharedvt_min && mini_is_gsharedvt_variable_signature (fsig))
  6367| 				GSHAREDVT_FAILURE (il_op);
  6368| 			if (method->dynamic && fsig->pinvoke) {
  6369| 				MonoInst *args [3];
  6370| 				/*
  6371| 				 * This is a call through a function pointer using a pinvoke
  6372| 				 * signature. Have to create a wrapper and call that instead.
  6373| 				 * FIXME: This is very slow, need to create a wrapper at JIT time
  6374| 				 * instead based on the signature.
  6375| 				 */
  6376| 				EMIT_NEW_IMAGECONST (cfg, args [0], ((MonoDynamicMethod*)method)->assembly->image);
  6377| 				EMIT_NEW_PCONST (cfg, args [1], fsig);
  6378| 				args [2] = addr;
  6379| 				addr = mono_emit_jit_icall (cfg, mono_get_native_calli_wrapper, args);
  6380| 			}
  6381| 			if (!method->dynamic && fsig->pinvoke &&
  6382| 			    !method->wrapper_type) {
  6383| 				/* MONO_WRAPPER_DYNAMIC_METHOD dynamic method handled above in the
  6384| 				method->dynamic case; for other wrapper types assume the code knows
  6385| 				what its doing and added its own GC transitions */
  6386| 				gboolean skip_gc_trans = fsig->suppress_gc_transition;
  6387| 				if (!skip_gc_trans) {
  6388| #if 0
  6389| 					fprintf (stderr, "generating wrapper for calli in method %s with wrapper type %s\n", method->name, mono_wrapper_type_to_str (method->wrapper_type));
  6390| #endif
  6391| 					if (cfg->compile_aot)
  6392| 						cfg->pinvoke_calli_signatures = g_slist_prepend_mempool (cfg->mempool, cfg->pinvoke_calli_signatures, fsig);
  6393| 					/* Call the wrapper that will do the GC transition instead */
  6394| 					MonoMethod *wrapper = mono_marshal_get_native_func_wrapper_indirect (method->klass, fsig, cfg->compile_aot);
  6395| 					fsig = mono_method_signature_internal (wrapper);
  6396| 					n = fsig->param_count - 1; /* wrapper has extra fnptr param */
  6397| 					CHECK_STACK (n);
  6398| 					/* move the args to allow room for 'this' in the first position */
  6399| 					while (n--) {
  6400| 						--sp;
  6401| 						sp [1] = sp [0];
  6402| 					}
  6403| 					sp[0] = addr; /* n+1 args, first arg is the address of the indirect method to call */
  6404| 					g_assert (!fsig->hasthis && !fsig->pinvoke);
  6405| 					ins = mono_emit_method_call (cfg, wrapper, /*args*/sp, NULL);
  6406| 					goto calli_end;
  6407| 				}
  6408| 			}
  6409| 			n = fsig->param_count + fsig->hasthis;
  6410| 			CHECK_STACK (n);
  6411| 			if (n == 0 && fsig->call_convention == MONO_CALL_THISCALL)
  6412| 				mono_cfg_set_exception_invalid_program(cfg, "thiscall with 0 arguments");
  6413| 			sp -= n;
  6414| 			if (!(cfg->method->wrapper_type && cfg->method->wrapper_type != MONO_WRAPPER_DYNAMIC_METHOD) && check_call_signature (cfg, fsig, sp)) {
  6415| 				if (break_on_unverified ())
  6416| 					check_call_signature (cfg, fsig, sp); // Again, step through it.
  6417| 				UNVERIFIED;
  6418| 			}
  6419| 			inline_costs += CALL_COST * MIN(10, num_calls++);
  6420| 			/*
  6421| 			 * Making generic calls out of gsharedvt methods.
  6422| 			 * This needs to be used for all generic calls, not just ones with a gsharedvt signature, to avoid
  6423| 			 * patching gshared method addresses into a gsharedvt method.
  6424| 			 */
  6425| 			if (cfg->gsharedvt && mini_is_gsharedvt_signature (fsig)) {
  6426| 				/*
  6427| 				 * We pass the address to the gsharedvt trampoline in the rgctx reg
  6428| 				 */
  6429| 				callee = addr;
  6430| 				g_assert (addr); // Doubles as boolean after tailcall check.
  6431| 			}
  6432| 			inst_tailcall && is_supported_tailcall (cfg, ip, method, NULL, fsig,
  6433| 						FALSE/*virtual irrelevant*/, addr != NULL, &tailcall);
  6434| 			if (save_last_error)
  6435| 				mono_emit_jit_icall (cfg, mono_marshal_clear_last_error, NULL);
  6436| 			if (callee) {
  6437| 				if (method->wrapper_type != MONO_WRAPPER_DELEGATE_INVOKE)
  6438| 					/* Not tested */
  6439| 					GSHAREDVT_FAILURE (il_op);
  6440| 				if (cfg->llvm_only)
  6441| 					GSHAREDVT_FAILURE (il_op);
  6442| 				addr = emit_get_rgctx_sig (cfg, context_used, fsig, MONO_RGCTX_INFO_SIG_GSHAREDVT_OUT_TRAMPOLINE_CALLI);
  6443| 				ins = (MonoInst*)mini_emit_calli_full (cfg, fsig, sp, addr, NULL, callee, tailcall);
  6444| 				goto calli_end;
  6445| 			}
  6446| 			/* Prevent inlining of methods with indirect calls */
  6447| 			INLINE_FAILURE ("indirect call");
  6448| 			if (addr->opcode == OP_PCONST || addr->opcode == OP_AOTCONST || addr->opcode == OP_GOT_ENTRY) {
  6449| 				MonoJumpInfoType info_type;
  6450| 				gpointer info_data;
  6451| 				/*
  6452| 				 * Instead of emitting an indirect call, emit a direct call
  6453| 				 * with the contents of the aotconst as the patch info.
  6454| 				 */
  6455| 				if (addr->opcode == OP_PCONST || addr->opcode == OP_AOTCONST) {
  6456| 					info_type = (MonoJumpInfoType)addr->inst_c1;
  6457| 					info_data = addr->inst_p0;
  6458| 				} else {
  6459| 					info_type = (MonoJumpInfoType)addr->inst_right->inst_c1;
  6460| 					info_data = addr->inst_right->inst_left;
  6461| 				}
  6462| 				if (info_type == MONO_PATCH_INFO_ICALL_ADDR) {
  6463| 					tailcall = FALSE;
  6464| 					ins = (MonoInst*)mini_emit_abs_call (cfg, MONO_PATCH_INFO_ICALL_ADDR_CALL, info_data, fsig, sp);
  6465| 					NULLIFY_INS (addr);
  6466| 					goto calli_end;
  6467| 				} else if (info_type == MONO_PATCH_INFO_JIT_ICALL_ADDR
  6468| 						|| info_type == MONO_PATCH_INFO_SPECIFIC_TRAMPOLINE_LAZY_FETCH_ADDR) {
  6469| 					tailcall = FALSE;
  6470| 					ins = (MonoInst*)mini_emit_abs_call (cfg, info_type, info_data, fsig, sp);
  6471| 					NULLIFY_INS (addr);
  6472| 					goto calli_end;
  6473| 				}
  6474| 			}
  6475| 			/* Some wrappers use calli with ftndesc-es */
  6476| 			if (cfg->llvm_only && !(cfg->method->wrapper_type &&
  6477| 									cfg->method->wrapper_type != MONO_WRAPPER_DYNAMIC_METHOD &&
  6478| 									cfg->method->wrapper_type != MONO_WRAPPER_DELEGATE_INVOKE))
  6479| 				ins = mini_emit_llvmonly_calli (cfg, fsig, sp, addr);
  6480| 			else
  6481| 				ins = (MonoInst*)mini_emit_calli_full (cfg, fsig, sp, addr, NULL, NULL, tailcall);
  6482| 			goto calli_end;
  6483| 		}
  6484| 		case MONO_CEE_CALL:
  6485| 		case MONO_CEE_CALLVIRT: {
  6486| 			MonoInst *addr; addr = NULL;
  6487| 			int array_rank; array_rank = 0;
  6488| 			gboolean virtual_; virtual_ = il_op == MONO_CEE_CALLVIRT;
  6489| 			gboolean pass_imt_from_rgctx; pass_imt_from_rgctx = FALSE;
  6490| 			MonoInst *imt_arg; imt_arg = NULL;
  6491| 			gboolean pass_mrgctx; pass_mrgctx = FALSE;
  6492| 			MonoInst *vtable_arg; vtable_arg = NULL;
  6493| 			gboolean check_this; check_this = FALSE;
  6494| 			gboolean delegate_invoke; delegate_invoke = FALSE;
  6495| 			gboolean direct_icall; direct_icall = FALSE;
  6496| 			gboolean tailcall_calli; tailcall_calli = FALSE;
  6497| 			gboolean noreturn; noreturn = FALSE;
  6498| 			gboolean gshared_static_virtual; gshared_static_virtual = FALSE;
  6499| #ifdef TARGET_WASM
  6500| 			gboolean needs_stack_walk; needs_stack_walk = FALSE;
  6501| #endif
  6502| 			common_call = FALSE;
  6503| 			gboolean called_is_supported_tailcall; called_is_supported_tailcall = FALSE;
  6504| 			MonoMethod *tailcall_method; tailcall_method = NULL;
  6505| 			MonoMethod *tailcall_cmethod; tailcall_cmethod = NULL;
  6506| 			MonoMethodSignature *tailcall_fsig; tailcall_fsig = NULL;
  6507| 			gboolean tailcall_virtual; tailcall_virtual = FALSE;
  6508| 			gboolean tailcall_extra_arg; tailcall_extra_arg = FALSE;
  6509| 			gboolean inst_tailcall; inst_tailcall = G_UNLIKELY (debug_tailcall_try_all
  6510| 							? (next_ip < end && next_ip [0] == CEE_RET)
  6511| 							: ((ins_flag & MONO_INST_TAILCALL) != 0));
  6512| 			gboolean make_generic_call_out_of_gsharedvt_method = FALSE;
  6513| 			gboolean will_have_imt_arg = FALSE;
  6514| 			ins = NULL;
  6515| 			/* Used to pass arguments to called functions */
  6516| 			HandleCallData cdata;
  6517| 			memset (&cdata, 0, sizeof (HandleCallData));
  6518| 			if (cmethod_override) {
  6519| 				cmethod = cmethod_override;
  6520| 				cmethod_override = NULL;
  6521| 				virtual_ = FALSE;
  6522| 			} else {
  6523| 				cmethod = mini_get_method (cfg, method, token, NULL, generic_context);
  6524| 			}
  6525| 			CHECK_CFG_ERROR;
  6526| 			if (cfg->verbose_level > 3)
  6527| 				printf ("cmethod = %s\n", mono_method_get_full_name (cmethod));
  6528| 			MonoMethod *cil_method; cil_method = cmethod;
  6529| 			if (constrained_class) {
  6530| 				if (m_method_is_static (cil_method) && mini_class_check_context_used (cfg, constrained_class)) {
  6531| 					/* get_constrained_method () doesn't work on the gparams used by generic sharing */
  6532| 					gshared_static_virtual = TRUE;
  6533| 					if (!cfg->gsharedvt)
  6534| 						/*
  6535| 						 * We can't resolve these calls at compile time, and they are used in
  6536| 						 * perf-sensitive code in the BCL, so ask the AOT compiler to try to use specific instances
  6537| 						 * instead of this gshared method.
  6538| 						 */
  6539| 						cfg->prefer_instances = TRUE;
  6540| 				} else {
  6541| 					cmethod = get_constrained_method (cfg, image, token, cil_method, constrained_class, generic_context);
  6542| 					CHECK_CFG_ERROR;
  6543| 					if (mono_class_has_dim_conflicts (constrained_class) &&
  6544| 							mono_class_is_method_ambiguous (constrained_class, cil_method))
  6545| 						mono_emit_jit_icall (cfg, mono_throw_ambiguous_implementation, NULL);
  6546| 					if (m_class_is_enumtype (constrained_class) && !strcmp (cmethod->name, "GetHashCode")) {
  6547| 						/* Use the corresponding method from the base type to avoid boxing */
  6548| 						MonoType *base_type = mono_class_enum_basetype_internal (constrained_class);
  6549| 						g_assert (base_type);
  6550| 						constrained_class = mono_class_from_mono_type_internal (base_type);
  6551| 						cmethod = get_method_nofail (constrained_class, cmethod->name, 0, 0);
  6552| 						g_assert (cmethod);
  6553| 					}
  6554| 				}
  6555| 			}
  6556| 			if (!dont_verify && !cfg->skip_visibility) {
  6557| 				MonoMethod *target_method = cil_method;
  6558| 				if (method->is_inflated) {
  6559| 					MonoGenericContainer *container = mono_method_get_generic_container(method_definition);
  6560| 					MonoGenericContext *context = (container != NULL ? &container->context : NULL);
  6561| 					target_method = mini_get_method_allow_open (method, token, NULL, context, cfg->error);
  6562| 					CHECK_CFG_ERROR;
  6563| 				}
  6564| 				if (!mono_method_can_access_method (method_definition, target_method) &&
  6565| 					!mono_method_can_access_method (method, cil_method))
  6566| 					emit_method_access_failure (cfg, method, cil_method);
  6567| 			}
  6568| 			if (cfg->llvm_only && cmethod && method_needs_stack_walk (cfg, cmethod)) {
  6569| 				if (cfg->interp && !cfg->interp_entry_only) {
  6570| 					/* Use the interpreter instead */
  6571| 					cfg->exception_message = g_strdup ("stack walk");
  6572| 					cfg->disable_llvm = TRUE;
  6573| 				}
  6574| #ifdef TARGET_WASM
  6575| 				else {
  6576| 					needs_stack_walk = TRUE;
  6577| 				}
  6578| #endif
  6579| 			}
  6580| 			if (!virtual_ && (cmethod->flags & METHOD_ATTRIBUTE_ABSTRACT) && !gshared_static_virtual) {
  6581| 				if (!mono_class_is_interface (method->klass))
  6582| 					emit_bad_image_failure (cfg, method, cil_method);
  6583| 				else
  6584| 					virtual_ = TRUE;
  6585| 			}
  6586| 			if (!m_class_is_inited (cmethod->klass))
  6587| 				if (!mono_class_init_internal (cmethod->klass))
  6588| 					TYPE_LOAD_ERROR (cmethod->klass);
  6589| 			fsig = mono_method_signature_internal (cmethod);
  6590| 			if (!fsig)
  6591| 				LOAD_ERROR;
  6592| 			if (cmethod->iflags & METHOD_IMPL_ATTRIBUTE_INTERNAL_CALL &&
  6593| 				mini_class_is_system_array (cmethod->klass)) {
  6594| 				array_rank = m_class_get_rank (cmethod->klass);
  6595| 			} else if ((cmethod->iflags & METHOD_IMPL_ATTRIBUTE_INTERNAL_CALL) && direct_icalls_enabled (cfg, cmethod)) {
  6596| 				direct_icall = TRUE;
  6597| 			} else if (fsig->pinvoke) {
  6598| 				if (cmethod->flags & METHOD_ATTRIBUTE_PINVOKE_IMPL) {
  6599| 					/*
  6600| 					 * Avoid calling mono_marshal_get_native_wrapper () too early, it might call managed
  6601| 					 * callbacks on netcore.
  6602| 					 */
  6603| 					fsig = mono_metadata_signature_dup_mempool (cfg->mempool, fsig);
  6604| 					fsig->pinvoke = FALSE;
  6605| 				} else {
  6606| 					MonoMethod *wrapper = mono_marshal_get_native_wrapper (cmethod, TRUE, cfg->compile_aot);
  6607| 					fsig = mono_method_signature_internal (wrapper);
  6608| 				}
  6609| 			} else if (constrained_class) {
  6610| 			} else {
  6611| 				fsig = mono_method_get_signature_checked (cmethod, image, token, generic_context, cfg->error);
  6612| 				CHECK_CFG_ERROR;
  6613| 			}
  6614| 			if (cfg->llvm_only && !cfg->method->wrapper_type && (!cmethod || cmethod->is_inflated))
  6615| 				cfg->signatures = g_slist_prepend_mempool (cfg->mempool, cfg->signatures, fsig);
  6616| 			if (cfg->gsharedvt_min && mini_is_gsharedvt_variable_signature (fsig))
  6617| 				GSHAREDVT_FAILURE (il_op);
  6618| 			/* See code below */
  6619| 			if (cmethod->klass == mono_defaults.monitor_class && !strcmp (cmethod->name, "Enter") && mono_method_signature_internal (cmethod)->param_count == 1) {
  6620| 				MonoBasicBlock *tbb;
  6621| 				GET_BBLOCK (cfg, tbb, next_ip);
  6622| 				if (tbb->try_start && MONO_REGION_FLAGS(tbb->region) == MONO_EXCEPTION_CLAUSE_FINALLY) {
  6623| 					/*
  6624| 					 * We want to extend the try block to cover the call, but we can't do it if the
  6625| 					 * call is made directly since its followed by an exception check.
  6626| 					 */
  6627| 					direct_icall = FALSE;
  6628| 				}
  6629| 			}
  6630| 			mono_save_token_info (cfg, image, token, cil_method);
  6631| 			if (!(seq_point_locs && mono_bitset_test_fast (seq_point_locs, next_ip - header->code)))
  6632| 				need_seq_point = TRUE;
  6633| 			/* Don't support calls made using type arguments for now */
  6634| 			/*
  6635| 			  if (cfg->gsharedvt) {
  6636| 			  if (mini_is_gsharedvt_signature (fsig))
  6637| 			  GSHAREDVT_FAILURE (il_op);
  6638| 			  }
  6639| 			*/
  6640| 			if (cmethod->string_ctor && method->wrapper_type != MONO_WRAPPER_RUNTIME_INVOKE)
  6641| 				g_assert_not_reached ();
  6642| 			n = fsig->param_count + fsig->hasthis;
  6643| 			if (!cfg->gshared && mono_class_is_gtd (cmethod->klass))
  6644| 				UNVERIFIED;
  6645| 			if (!cfg->gshared)
  6646| 				g_assert (!mono_method_check_context_used (cmethod));
  6647| 			CHECK_STACK (n);
  6648| 			sp -= n;
  6649| 			if (virtual_ && cmethod && sp [0] && sp [0]->opcode == OP_TYPED_OBJREF) {
  6650| 				error_init_reuse (error);
  6651| 				MonoMethod *new_cmethod = mono_class_get_virtual_method (sp [0]->klass, cmethod, error);
  6652| 				if (is_ok (error)) {
  6653| 					cmethod = new_cmethod;
  6654| 					virtual_ = FALSE;
  6655| 				} else {
  6656| 					mono_error_cleanup (error);
  6657| 				}
  6658| 			}
  6659| 			if (cmethod && method_does_not_return (cmethod)) {
  6660| 				cfg->cbb->out_of_line = TRUE;
  6661| 				noreturn = TRUE;
  6662| 			}
  6663| 			cdata.method = method;
  6664| 			cdata.inst_tailcall = inst_tailcall;
  6665| 			/*
  6666| 			 * We have the `constrained.' prefix opcode.
  6667| 			 */
  6668| 			if (constrained_class) {
  6669| 				ins = handle_constrained_call (cfg, cmethod, fsig, constrained_class, sp, &cdata, &cmethod, &virtual_, &emit_widen);
  6670| 				CHECK_CFG_EXCEPTION;
  6671| 				if (!gshared_static_virtual)
  6672| 					constrained_class = NULL;
  6673| 				if (ins)
  6674| 					goto call_end;
  6675| 			}
  6676| 			for (int i = 0; i < fsig->param_count; ++i)
  6677| 				sp [i + fsig->hasthis] = convert_value (cfg, fsig->params [i], sp [i + fsig->hasthis]);
  6678| 			if (check_call_signature (cfg, fsig, sp)) {
  6679| 				if (break_on_unverified ())
  6680| 					check_call_signature (cfg, fsig, sp); // Again, step through it.
  6681| 				UNVERIFIED;
  6682| 			}
  6683| 			if ((m_class_get_parent (cmethod->klass) == mono_defaults.multicastdelegate_class) && !strcmp (cmethod->name, "Invoke"))
  6684| 				delegate_invoke = TRUE;
  6685| 			/*
  6686| 			 * Implement a workaround for the inherent races involved in locking:
  6687| 			 * Monitor.Enter ()
  6688| 			 * try {
  6689| 			 * } finally {
  6690| 			 *    Monitor.Exit ()
  6691| 			 * }
  6692| 			 * If a thread abort happens between the call to Monitor.Enter () and the start of the
  6693| 			 * try block, the Exit () won't be executed, see:
  6694| 			 * http://www.bluebytesoftware.com/blog/2007/01/30/MonitorEnterThreadAbortsAndOrphanedLocks.aspx
  6695| 			 * To work around this, we extend such try blocks to include the last x bytes
  6696| 			 * of the Monitor.Enter () call.
  6697| 			 */
  6698| 			if (cmethod->klass == mono_defaults.monitor_class && !strcmp (cmethod->name, "Enter") && mono_method_signature_internal (cmethod)->param_count == 1) {
  6699| 				MonoBasicBlock *tbb;
  6700| 				GET_BBLOCK (cfg, tbb, next_ip);
  6701| 				/*
  6702| 				 * Only extend try blocks with a finally, to avoid catching exceptions thrown
  6703| 				 * from Monitor.Enter like ArgumentNullException.
  6704| 				 */
  6705| 				if (tbb->try_start && MONO_REGION_FLAGS(tbb->region) == MONO_EXCEPTION_CLAUSE_FINALLY) {
  6706| 					/* Mark this bblock as needing to be extended */
  6707| 					tbb->extend_try_block = TRUE;
  6708| 				}
  6709| 			}
  6710| 			/* Conversion to a JIT intrinsic */
  6711| 			gboolean ins_type_initialized;
  6712| 			if ((ins = mini_emit_inst_for_method (cfg, cmethod, fsig, sp, &ins_type_initialized))) {
  6713| 				if (!MONO_TYPE_IS_VOID (fsig->ret)) {
  6714| 					if (!ins_type_initialized)
  6715| 						mini_type_to_eval_stack_type ((cfg), fsig->ret, ins);
  6716| 					emit_widen = FALSE;
  6717| 				}
  6718| 				if (inst_tailcall) // FIXME
  6719| 					mono_tailcall_print ("missed tailcall intrins %s -> %s\n", method->name, cmethod->name);
  6720| 				goto call_end;
  6721| 			}
  6722| 			CHECK_CFG_ERROR;
  6723| 			/*
  6724| 			 * If the callee is a shared method, then its static cctor
  6725| 			 * might not get called after the call was patched.
  6726| 			 */
  6727| 			if (cfg->gshared && cmethod->klass != method->klass && mono_class_is_ginst (cmethod->klass) && mono_method_is_generic_sharable (cmethod, TRUE) && mono_class_needs_cctor_run (cmethod->klass, method)) {
  6728| 				emit_class_init (cfg, cmethod->klass, FALSE);
  6729| 				CHECK_TYPELOAD (cmethod->klass);
  6730| 			}
  6731| 			/* Inlining */
  6732| 			if ((cfg->opt & MONO_OPT_INLINE) && !inst_tailcall && !gshared_static_virtual &&
  6733| 				(!virtual_ || !(cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL) || MONO_METHOD_IS_FINAL (cmethod)) &&
  6734| 			    mono_method_check_inlining (cfg, cmethod)) {
  6735| 				int costs;
  6736| 				gboolean always = FALSE;
  6737| 				gboolean is_empty = FALSE;
  6738| 				if (cmethod->iflags & METHOD_IMPL_ATTRIBUTE_INTERNAL_CALL) {
  6739| 					/* Prevent inlining of methods that call wrappers */
  6740| 					INLINE_FAILURE ("wrapper call");
  6741| 					cmethod = mono_marshal_get_native_wrapper (cmethod, TRUE, FALSE);
  6742| 					always = TRUE;
  6743| 				}
  6744| 				costs = inline_method (cfg, cmethod, fsig, sp, ip, cfg->real_offset, always, &is_empty);
  6745| 				if (costs) {
  6746| 					cfg->real_offset += 5;
  6747| 					if (!MONO_TYPE_IS_VOID (fsig->ret))
  6748| 						/* *sp is already set by inline_method */
  6749| 						ins = *sp;
  6750| 					inline_costs += costs;
  6751| 					if (inst_tailcall) // FIXME
  6752| 						mono_tailcall_print ("missed tailcall inline %s -> %s\n", method->name, cmethod->name);
  6753| 					if (is_empty)
  6754| 						ins_has_side_effect = FALSE;
  6755| 					goto call_end;
  6756| 				}
  6757| 			}
  6758| 			pass_mrgctx = need_mrgctx_arg (cfg, cmethod);
  6759| 			if (cfg->gshared) {
  6760| 				MonoGenericContext *cmethod_context = mono_method_get_context (cmethod);
  6761| 				context_used = mini_method_check_context_used (cfg, cmethod);
  6762| 				if (!context_used && gshared_static_virtual)
  6763| 					context_used = mini_class_check_context_used (cfg, constrained_class);
  6764| 				if (context_used && mono_class_is_interface (cmethod->klass) && !m_method_is_static (cmethod)) {
  6765| 					/* Generic method interface
  6766| 					   calls are resolved via a
  6767| 					   helper function and don't
  6768| 					   need an imt. */
  6769| 					if (!cmethod_context || !cmethod_context->method_inst)
  6770| 						pass_imt_from_rgctx = TRUE;
  6771| 				}
  6772| 				/*
  6773| 				 * If a shared method calls another
  6774| 				 * shared method then the caller must
  6775| 				 * have a generic sharing context
  6776| 				 * because the magic trampoline
  6777| 				 * requires it.  FIXME: We shouldn't
  6778| 				 * have to force the vtable/mrgctx
  6779| 				 * variable here.  Instead there
  6780| 				 * should be a flag in the cfg to
  6781| 				 * request a generic sharing context.
  6782| 				 */
  6783| 				if (context_used &&
  6784| 				    ((cfg->method->flags & METHOD_ATTRIBUTE_STATIC) || m_class_is_valuetype (cfg->method->klass)))
  6785| 					mono_get_vtable_var (cfg);
  6786| 			}
  6787| 			if (pass_mrgctx) {
  6788| 				g_assert (!vtable_arg);
  6789| 				if (!cfg->compile_aot) {
  6790| 					/*
  6791| 					 * emit_get_rgctx_method () calls mono_class_vtable () so check
  6792| 					 * for type load errors before.
  6793| 					 */
  6794| 					mono_class_setup_vtable (cmethod->klass);
  6795| 					CHECK_TYPELOAD (cmethod->klass);
  6796| 				}
  6797| 				vtable_arg = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_METHOD_RGCTX);
  6798| 				if ((!(cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL) || MONO_METHOD_IS_FINAL (cmethod)) && !delegate_invoke) {
  6799| 					if (virtual_)
  6800| 						check_this = TRUE;
  6801| 					virtual_ = FALSE;
  6802| 				}
  6803| 			}
  6804| 			if (pass_imt_from_rgctx) {
  6805| 				imt_arg = emit_get_rgctx_method (cfg, context_used,
  6806| 					cmethod, MONO_RGCTX_INFO_METHOD);
  6807| 				g_assert (imt_arg);
  6808| 			}
  6809| 			if (check_this)
  6810| 				MONO_EMIT_NEW_CHECK_THIS (cfg, sp [0]->dreg);
  6811| 			/* Calling virtual generic methods */
  6812| 			gboolean virtual_generic; virtual_generic = FALSE;
  6813| 			gboolean virtual_generic_imt; virtual_generic_imt = FALSE;
  6814| 			if (virtual_ && (cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL) &&
  6815| 			    !MONO_METHOD_IS_FINAL (cmethod) &&
  6816| 			    fsig->generic_param_count &&
  6817| 				!(cfg->gsharedvt && mini_is_gsharedvt_signature (fsig)) &&
  6818| 				!cfg->llvm_only) {
  6819| 				g_assert (fsig->is_inflated);
  6820| 				virtual_generic = TRUE;
  6821| 				/* Prevent inlining of methods that contain indirect calls */
  6822| 				INLINE_FAILURE ("virtual generic call");
  6823| 				if (cfg->gsharedvt && mini_is_gsharedvt_signature (fsig))
  6824| 					GSHAREDVT_FAILURE (il_op);
  6825| 				if (cfg->backend->have_generalized_imt_trampoline && cfg->backend->gshared_supported && cmethod->wrapper_type == MONO_WRAPPER_NONE) {
  6826| 					virtual_generic_imt = TRUE;
  6827| 					g_assert (!imt_arg);
  6828| 					if (!context_used)
  6829| 						g_assert (cmethod->is_inflated);
  6830| 					imt_arg = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_METHOD);
  6831| 					g_assert (imt_arg);
  6832| 					virtual_ = TRUE;
  6833| 					vtable_arg = NULL;
  6834| 				}
  6835| 			}
  6836| 			/*
  6837| 			 * Making generic calls out of gsharedvt methods.
  6838| 			 * This needs to be used for all generic calls, not just ones with a gsharedvt signature, to avoid
  6839| 			 * patching gshared method addresses into a gsharedvt method.
  6840| 			 */
  6841| 			if (cfg->gsharedvt && (mini_is_gsharedvt_signature (fsig) || cmethod->is_inflated || mono_class_is_ginst (cmethod->klass)) &&
  6842| 				!(m_class_get_rank (cmethod->klass) && m_class_get_byval_arg (cmethod->klass)->type != MONO_TYPE_SZARRAY) &&
  6843| 				(!(cfg->llvm_only && virtual_ && (cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL)))) {
  6844| 				make_generic_call_out_of_gsharedvt_method = TRUE;
  6845| 				if (virtual_) {
  6846| 					if (fsig->generic_param_count) {
  6847| 						will_have_imt_arg = TRUE;
  6848| 					} else if (mono_class_is_interface (cmethod->klass) && !imt_arg) {
  6849| 						will_have_imt_arg = TRUE;
  6850| 					}
  6851| 				}
  6852| 			}
  6853| 			/* Tail prefix / tailcall optimization */
  6854| 			/* FIXME: Enabling TAILC breaks some inlining/stack trace/etc tests.
  6855| 				  Inlining and stack traces are not guaranteed however. */
  6856| 			/* FIXME: runtime generic context pointer for jumps? */
  6857| 			/* FIXME: handle this for generic sharing eventually */
  6858| 			tailcall_extra_arg = vtable_arg || imt_arg || will_have_imt_arg || mono_class_is_interface (cmethod->klass);
  6859| 			tailcall = inst_tailcall && is_supported_tailcall (cfg, ip, method, cmethod, fsig,
  6860| 						virtual_, tailcall_extra_arg, &tailcall_calli);
  6861| 			called_is_supported_tailcall = TRUE;
  6862| 			tailcall_method = method;
  6863| 			tailcall_cmethod = cmethod;
  6864| 			tailcall_fsig = fsig;
  6865| 			tailcall_virtual = virtual_;
  6866| 			if (virtual_generic) {
  6867| 				if (virtual_generic_imt) {
  6868| 					if (tailcall) {
  6869| 						/* Prevent inlining of methods with tailcalls (the call stack would be altered) */
  6870| 						INLINE_FAILURE ("tailcall");
  6871| 					}
  6872| 					common_call = TRUE;
  6873| 					goto call_end;
  6874| 				}
  6875| 				MonoInst *this_temp, *this_arg_temp, *store;
  6876| 				MonoInst *iargs [4];
  6877| 				this_temp = mono_compile_create_var (cfg, type_from_stack_type (sp [0]), OP_LOCAL);
  6878| 				NEW_TEMPSTORE (cfg, store, this_temp->inst_c0, sp [0]);
  6879| 				MONO_ADD_INS (cfg->cbb, store);
  6880| 				/* FIXME: This should be a managed pointer */
  6881| 				this_arg_temp = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
  6882| 				EMIT_NEW_TEMPLOAD (cfg, iargs [0], this_temp->inst_c0);
  6883| 				iargs [1] = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_METHOD);
  6884| 				EMIT_NEW_TEMPLOADA (cfg, iargs [2], this_arg_temp->inst_c0);
  6885| 				addr = mono_emit_jit_icall (cfg, mono_helper_compile_generic_method, iargs);
  6886| 				EMIT_NEW_TEMPLOAD (cfg, sp [0], this_arg_temp->inst_c0);
  6887| 				ins = (MonoInst*)mini_emit_calli (cfg, fsig, sp, addr, NULL, NULL);
  6888| 				if (inst_tailcall) // FIXME
  6889| 					mono_tailcall_print ("missed tailcall virtual generic %s -> %s\n", method->name, cmethod->name);
  6890| 				goto call_end;
  6891| 			}
  6892| 			CHECK_CFG_ERROR;
  6893| 			/* Tail recursion elimination */
  6894| 			if (((cfg->opt & MONO_OPT_TAILCALL) || inst_tailcall) && il_op == MONO_CEE_CALL && cmethod == method && next_ip < end && next_ip [0] == CEE_RET && !vtable_arg) {
  6895| 				gboolean has_vtargs = FALSE;
  6896| 				int i;
  6897| 				/* Prevent inlining of methods with tailcalls (the call stack would be altered) */
  6898| 				INLINE_FAILURE ("tailcall");
  6899| 				/* keep it simple */
  6900| 				for (i = fsig->param_count - 1; !has_vtargs && i >= 0; i--)
  6901| 					has_vtargs = MONO_TYPE_ISSTRUCT (mono_method_signature_internal (cmethod)->params [i]);
  6902| 				if (!has_vtargs) {
  6903| 					if (need_seq_point) {
  6904| 						emit_seq_point (cfg, method, ip, FALSE, TRUE);
  6905| 						need_seq_point = FALSE;
  6906| 					}
  6907| 					for (i = 0; i < n; ++i)
  6908| 						EMIT_NEW_ARGSTORE (cfg, ins, i, sp [i]);
  6909| 					mini_profiler_emit_tail_call (cfg, cmethod);
  6910| 					MONO_INST_NEW (cfg, ins, OP_BR);
  6911| 					MONO_ADD_INS (cfg->cbb, ins);
  6912| 					tblock = start_bblock->out_bb [0];
  6913| 					link_bblock (cfg, cfg->cbb, tblock);
  6914| 					ins->inst_target_bb = tblock;
  6915| 					start_new_bblock = 1;
  6916| 					/* skip the CEE_RET, too */
  6917| 					if (ip_in_bb (cfg, cfg->cbb, next_ip))
  6918| 						skip_ret = TRUE;
  6919| 					push_res = FALSE;
  6920| 					need_seq_point = FALSE;
  6921| 					goto call_end;
  6922| 				}
  6923| 			}
  6924| 			inline_costs += CALL_COST * MIN(10, num_calls++);
  6925| 			/*
  6926| 			 * Synchronized wrappers.
  6927| 			 * Its hard to determine where to replace a method with its synchronized
  6928| 			 * wrapper without causing an infinite recursion. The current solution is
  6929| 			 * to add the synchronized wrapper in the trampolines, and to
  6930| 			 * change the called method to a dummy wrapper, and resolve that wrapper
  6931| 			 * to the real method in mono_jit_compile_method ().
  6932| 			 */
  6933| 			if (cfg->method->wrapper_type == MONO_WRAPPER_SYNCHRONIZED) {
  6934| 				MonoMethod *orig = mono_marshal_method_from_wrapper (cfg->method);
  6935| 				if (cmethod == orig || (cmethod->is_inflated && mono_method_get_declaring_generic_method (cmethod) == orig)) {
  6936| 					cmethod = mono_marshal_get_synchronized_inner_wrapper (cmethod);
  6937| 				}
  6938| 			}
  6939| 			/*
  6940| 			 * Making generic calls out of gsharedvt methods.
  6941| 			 * This needs to be used for all generic calls, not just ones with a gsharedvt signature, to avoid
  6942| 			 * patching gshared method addresses into a gsharedvt method.
  6943| 			 */
  6944| 			if (make_generic_call_out_of_gsharedvt_method) {
  6945| 				if (virtual_) {
  6946| 					if (fsig->hasthis && method->klass == mono_defaults.object_class)
  6947| 						GSHAREDVT_FAILURE (il_op);
  6948| 					if (fsig->generic_param_count) {
  6949| 						/* virtual generic call */
  6950| 						g_assert (!imt_arg);
  6951| 						g_assert (will_have_imt_arg);
  6952| 						/* Same as the virtual generic case above */
  6953| 						imt_arg = emit_get_rgctx_method (cfg, context_used,
  6954| 														 cmethod, MONO_RGCTX_INFO_METHOD);
  6955| 						g_assert (imt_arg);
  6956| 					} else if (mono_class_is_interface (cmethod->klass) && !imt_arg) {
  6957| 						/* This can happen when we call a fully instantiated iface method */
  6958| 						g_assert (will_have_imt_arg);
  6959| 						imt_arg = emit_get_rgctx_method (cfg, context_used,
  6960| 														 cmethod, MONO_RGCTX_INFO_METHOD);
  6961| 						g_assert (imt_arg);
  6962| 					}
  6963| 					/* This is not needed, as the trampoline code will pass one, and it might be passed in the same reg as the imt arg */
  6964| 					vtable_arg = NULL;
  6965| 				}
  6966| 				if ((m_class_get_parent (cmethod->klass) == mono_defaults.multicastdelegate_class) && (!strcmp (cmethod->name, "Invoke")))
  6967| 					keep_this_alive = sp [0];
  6968| 				MonoRgctxInfoType info_type;
  6969| 				if (virtual_ && (cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL))
  6970| 					info_type = MONO_RGCTX_INFO_METHOD_GSHAREDVT_OUT_TRAMPOLINE_VIRT;
  6971| 				else
  6972| 					info_type = MONO_RGCTX_INFO_METHOD_GSHAREDVT_OUT_TRAMPOLINE;
  6973| 				addr = emit_get_rgctx_gsharedvt_call (cfg, context_used, fsig, cmethod, info_type);
  6974| 				if (cfg->llvm_only) {
  6975| 					ins = mini_emit_llvmonly_calli (cfg, fsig, sp, addr);
  6976| 					if (inst_tailcall) // FIXME
  6977| 						mono_tailcall_print ("missed tailcall llvmonly gsharedvt %s -> %s\n", method->name, cmethod->name);
  6978| 				} else {
  6979| 					tailcall = tailcall_calli;
  6980| 					ins = (MonoInst*)mini_emit_calli_full (cfg, fsig, sp, addr, imt_arg, vtable_arg, tailcall);
  6981| 					tailcall_remove_ret |= tailcall;
  6982| 				}
  6983| 				goto call_end;
  6984| 			}
  6985| 			/* Generic sharing */
  6986| 			/*
  6987| 			 * Calls to generic methods from shared code cannot go through the trampoline infrastructure
  6988| 			 * in some cases, because the called method might end up being different on every call.
  6989| 			 * Load the called method address from the rgctx and do an indirect call in these cases.
  6990| 			 * Use this if the callee is gsharedvt sharable too, since
  6991| 			 * at runtime we might find an instantiation so the call cannot
  6992| 			 * be patched (the 'no_patch' code path in mini-trampolines.c).
  6993| 			 */
  6994| 			gboolean gshared_indirect;
  6995| 			gshared_indirect = context_used && !imt_arg && !array_rank && !delegate_invoke;
  6996| 			if (gshared_indirect)
  6997| 				gshared_indirect = (!mono_method_is_generic_sharable_full (cmethod, TRUE, FALSE, FALSE) ||
  6998| 									!mono_class_generic_sharing_enabled (cmethod->klass) ||
  6999| 									gshared_static_virtual);
  7000| 			if (gshared_indirect)
  7001| 				gshared_indirect = (!virtual_ || MONO_METHOD_IS_FINAL (cmethod) ||
  7002| 									!(cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL));
  7003| 			if (gshared_indirect) {
  7004| 				INLINE_FAILURE ("gshared");
  7005| 				g_assert (cfg->gshared && cmethod);
  7006| 				g_assert (!addr);
  7007| 				if (fsig->hasthis)
  7008| 					MONO_EMIT_NEW_CHECK_THIS (cfg, sp [0]->dreg);
  7009| 				if (cfg->llvm_only) {
  7010| 					if (cfg->gsharedvt && mini_is_gsharedvt_variable_signature (fsig)) {
  7011| 						/* Handled in handle_constrained_gsharedvt_call () */
  7012| 						g_assert (!gshared_static_virtual);
  7013| 						addr = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_GSHAREDVT_OUT_WRAPPER);
  7014| 					} else {
  7015| 						if (gshared_static_virtual)
  7016| 							addr = emit_get_rgctx_virt_method (cfg, -1, constrained_class, cmethod, MONO_RGCTX_INFO_VIRT_METHOD_CODE);
  7017| 						else
  7018| 							addr = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_METHOD_FTNDESC);
  7019| 					}
  7020| 					ins = mini_emit_llvmonly_calli (cfg, fsig, sp, addr);
  7021| 					if (inst_tailcall) // FIXME
  7022| 						mono_tailcall_print ("missed tailcall context_used_llvmonly %s -> %s\n", method->name, cmethod->name);
  7023| 				} else {
  7024| 					if (gshared_static_virtual) {
  7025| 						/*
  7026| 						 * cmethod is a static interface method, the actual called method at runtime
  7027| 						 * needs to be computed using constrained_class and cmethod.
  7028| 						 */
  7029| 						addr = emit_get_rgctx_virt_method (cfg, -1, constrained_class, cmethod, MONO_RGCTX_INFO_VIRT_METHOD_CODE);
  7030| 					} else {
  7031| 						addr = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_GENERIC_METHOD_CODE);
  7032| 					}
  7033| 					if (inst_tailcall)
  7034| 						mono_tailcall_print ("%s tailcall_calli#2 %s -> %s\n", tailcall_calli ? "making" : "missed", method->name, cmethod->name);
  7035| 					tailcall = tailcall_calli;
  7036| 					ins = (MonoInst*)mini_emit_calli_full (cfg, fsig, sp, addr, imt_arg, vtable_arg, tailcall);
  7037| 					tailcall_remove_ret |= tailcall;
  7038| 				}
  7039| 				goto call_end;
  7040| 			}
  7041| 			/* Direct calls to icalls */
  7042| 			if (direct_icall) {
  7043| 				MonoMethod *wrapper;
  7044| 				int costs;
  7045| 				/* Inline the wrapper */
  7046| 				wrapper = mono_marshal_get_native_wrapper (cmethod, TRUE, cfg->compile_aot);
  7047| 				costs = inline_method (cfg, wrapper, fsig, sp, ip, cfg->real_offset, TRUE, NULL);
  7048| 				g_assert (costs > 0);
  7049| 				cfg->real_offset += 5;
  7050| 				if (!MONO_TYPE_IS_VOID (fsig->ret))
  7051| 					/* *sp is already set by inline_method */
  7052| 					ins = *sp;
  7053| 				inline_costs += costs;
  7054| 				if (inst_tailcall) // FIXME
  7055| 					mono_tailcall_print ("missed tailcall direct_icall %s -> %s\n", method->name, cmethod->name);
  7056| 				goto call_end;
  7057| 			}
  7058| 			/* Array methods */
  7059| 			if (array_rank) {
  7060| 				MonoInst *ldelema_addr;
  7061| 				if (strcmp (cmethod->name, "Set") == 0) { /* array Set */
  7062| 					MonoInst *val = sp [fsig->param_count];
  7063| 					if (val->type == STACK_OBJ) {
  7064| 						MonoInst *iargs [ ] = { sp [0], val };
  7065| 						mono_emit_jit_icall (cfg, mono_helper_stelem_ref_check, iargs);
  7066| 					}
  7067| 					ldelema_addr = mini_emit_ldelema_ins (cfg, cmethod, sp, ip, TRUE);
  7068| 					if (!mini_debug_options.weak_memory_model && val->type == STACK_OBJ)
  7069| 						mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
  7070| 					EMIT_NEW_STORE_MEMBASE_TYPE (cfg, ins, fsig->params [fsig->param_count - 1], ldelema_addr->dreg, 0, val->dreg);
  7071| 					if (cfg->gen_write_barriers && val->type == STACK_OBJ && !MONO_INS_IS_PCONST_NULL (val))
  7072| 						mini_emit_write_barrier (cfg, ldelema_addr, val);
  7073| 					if (cfg->gen_write_barriers && mini_is_gsharedvt_klass (cmethod->klass))
  7074| 						GSHAREDVT_FAILURE (il_op);
  7075| 				} else if (strcmp (cmethod->name, "Get") == 0) { /* array Get */
  7076| 					ldelema_addr = mini_emit_ldelema_ins (cfg, cmethod, sp, ip, FALSE);
  7077| 					EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, fsig->ret, ldelema_addr->dreg, 0);
  7078| 				} else if (strcmp (cmethod->name, "Address") == 0) { /* array Address */
  7079| 					if (!m_class_is_valuetype (m_class_get_element_class (cmethod->klass)) && !readonly)
  7080| 						mini_emit_check_array_type (cfg, sp [0], cmethod->klass);
  7081| 					CHECK_TYPELOAD (cmethod->klass);
  7082| 					readonly = FALSE;
  7083| 					ldelema_addr = mini_emit_ldelema_ins (cfg, cmethod, sp, ip, FALSE);
  7084| 					ins = ldelema_addr;
  7085| 				} else {
  7086| 					g_assert_not_reached ();
  7087| 				}
  7088| 				emit_widen = FALSE;
  7089| 				if (inst_tailcall) // FIXME
  7090| 					mono_tailcall_print ("missed tailcall array_rank %s -> %s\n", method->name, cmethod->name);
  7091| 				goto call_end;
  7092| 			}
  7093| 			ins = mini_redirect_call (cfg, cmethod, fsig, sp, virtual_ ? sp [0] : NULL);
  7094| 			if (ins) {
  7095| 				if (inst_tailcall) // FIXME
  7096| 					mono_tailcall_print ("missed tailcall redirect %s -> %s\n", method->name, cmethod->name);
  7097| 				goto call_end;
  7098| 			}
  7099| 			/* Tail prefix / tailcall optimization */
  7100| 			if (tailcall) {
  7101| 				/* Prevent inlining of methods with tailcalls (the call stack would be altered) */
  7102| 				INLINE_FAILURE ("tailcall");
  7103| 			}
  7104| 			/*
  7105| 			 * Virtual calls in llvm-only mode.
  7106| 			 */
  7107| 			if (cfg->llvm_only && virtual_ && cmethod && (cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL)) {
  7108| 				ins = mini_emit_llvmonly_virtual_call (cfg, cmethod, fsig, context_used, sp);
  7109| 				goto call_end;
  7110| 			}
  7111| 			/* Common call */
  7112| 			if (!(cfg->opt & MONO_OPT_AGGRESSIVE_INLINING) && !(method->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING) && !(cmethod->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING) && !method_does_not_return (cmethod))
  7113| 				INLINE_FAILURE ("call");
  7114| 			common_call = TRUE;
  7115| #ifdef TARGET_WASM
  7116| 			/* Push an LMF so these frames can be enumerated during stack walks by mono_arch_unwind_frame () */
  7117| 			if (needs_stack_walk && !cfg->deopt) {
  7118| 				MonoInst *method_ins;
  7119| 				int lmf_reg;
  7120| 				emit_push_lmf (cfg);
  7121| 				EMIT_NEW_VARLOADA (cfg, ins, cfg->lmf_var, NULL);
  7122| 				lmf_reg = ins->dreg;
  7123| 				/* The lmf->method field will be used to look up the MonoJitInfo for this method */
  7124| 				method_ins = emit_get_rgctx_method (cfg, mono_method_check_context_used (cfg->method), cfg->method, MONO_RGCTX_INFO_METHOD);
  7125| 				EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, lmf_reg, MONO_STRUCT_OFFSET (MonoLMF, method), method_ins->dreg);
  7126| 			}
  7127| #endif
  7128| call_end:
  7129| 			g_assert (!called_is_supported_tailcall || tailcall_method == method);
  7130| 			g_assert (!called_is_supported_tailcall || !tailcall || tailcall_cmethod == cmethod);
  7131| 			g_assert (!called_is_supported_tailcall || tailcall_fsig == fsig);
  7132| 			g_assert (!called_is_supported_tailcall || tailcall_virtual == virtual_);
  7133| 			if (common_call) // FIXME goto call_end && !common_call often skips tailcall processing.
  7134| 				ins = mini_emit_method_call_full (cfg, cmethod, fsig, tailcall, sp, virtual_ ? sp [0] : NULL,
  7135| 												  imt_arg, vtable_arg);
  7136| 			/*
  7137| 			 * Handle devirt of some A.B.C calls by replacing the result of A.B with a OP_TYPED_OBJREF instruction, so the .C
  7138| 			 * call can be devirtualized above.
  7139| 			 */
  7140| 			if (cmethod)
  7141| 				ins = handle_call_res_devirt (cfg, cmethod, ins);
  7142| #ifdef TARGET_WASM
  7143| 			if (common_call && needs_stack_walk && !cfg->deopt)
  7144| 				emit_pop_lmf (cfg);
  7145| #endif
  7146| 			if (noreturn) {
  7147| 				MONO_INST_NEW (cfg, ins, OP_NOT_REACHED);
  7148| 				MONO_ADD_INS (cfg->cbb, ins);
  7149| 			}
  7150| calli_end:
  7151| 			if ((tailcall_remove_ret || (common_call && tailcall)) && !cfg->llvm_only) {
  7152| 				link_bblock (cfg, cfg->cbb, end_bblock);
  7153| 				start_new_bblock = 1;
  7154| 				/*
  7155| 				 * OP_TAILCALL has no return value, so skip the CEE_RET if it is
  7156| 				 * only reachable from this call.
  7157| 				 */
  7158| 				GET_BBLOCK (cfg, tblock, next_ip);
  7159| 				if (tblock == cfg->cbb || tblock->in_count == 0)
  7160| 					skip_ret = TRUE;
  7161| 				push_res = FALSE;
  7162| 				need_seq_point = FALSE;
  7163| 			}
  7164| 			if (ins_flag & MONO_INST_TAILCALL)
  7165| 				mini_test_tailcall (cfg, tailcall);
  7166| 			/* End of call, INS should contain the result of the call, if any */
  7167| 			if (push_res && !MONO_TYPE_IS_VOID (fsig->ret)) {
  7168| 				g_assert (ins);
  7169| 				if (emit_widen)
  7170| 					*sp++ = mono_emit_widen_call_res (cfg, ins, fsig);
  7171| 				else
  7172| 					*sp++ = ins;
  7173| 			}
  7174| 			if (save_last_error) {
  7175| 				save_last_error = FALSE;
  7176| #ifdef TARGET_WIN32
  7177| 				MONO_INST_NEW (cfg, ins, OP_GET_LAST_ERROR);
  7178| 				ins->dreg = alloc_dreg (cfg, STACK_I4);
  7179| 				ins->type = STACK_I4;
  7180| 				MONO_ADD_INS (cfg->cbb, ins);
  7181| 				mono_emit_jit_icall (cfg, mono_marshal_set_last_error_windows, &ins);
  7182| #else
  7183| 				mono_emit_jit_icall (cfg, mono_marshal_set_last_error, NULL);
  7184| #endif
  7185| 			}
  7186| 			if (keep_this_alive) {
  7187| 				MonoInst *dummy_use;
  7188| 				/* See mini_emit_method_call_full () */
  7189| 				EMIT_NEW_DUMMY_USE (cfg, dummy_use, keep_this_alive);
  7190| 			}
  7191| 			if (cfg->llvm_only && cmethod && method_needs_stack_walk (cfg, cmethod)) {
  7192| 				/*
  7193| 				 * Clang can convert these calls to tailcalls which screw up the stack
  7194| 				 * walk. This happens even when the -fno-optimize-sibling-calls
  7195| 				 * option is passed to clang.
  7196| 				 * Work around this by emitting a dummy call.
  7197| 				 */
  7198| 				mono_emit_jit_icall (cfg, mono_dummy_jit_icall, NULL);
  7199| 			}
  7200| 			CHECK_CFG_EXCEPTION;
  7201| 			if (skip_ret) {
  7202| 				g_assert (next_ip [0] == CEE_RET);
  7203| 				next_ip += 1;
  7204| 				il_op = MonoOpcodeEnum_Invalid; // Call or ret? Unclear.
  7205| 			}
  7206| 			ins_flag = 0;
  7207| 			constrained_class = NULL;
  7208| 			if (need_seq_point) {
  7209| 				if (!(method->flags & METHOD_IMPL_ATTRIBUTE_NATIVE)) {
  7210| 					if (emitted_funccall_seq_point) {
  7211| 						if (cfg->last_seq_point)
  7212| 							cfg->last_seq_point->flags |= MONO_INST_NESTED_CALL;
  7213| 					}
  7214| 					else
  7215| 						emitted_funccall_seq_point = TRUE;
  7216| 				}
  7217| 				emit_seq_point (cfg, method, next_ip, FALSE, TRUE);
  7218| 			}
  7219| 			break;
  7220| 		}
  7221| 		case MONO_CEE_RET:
  7222| 			if (!detached_before_ret)
  7223| 				mini_profiler_emit_leave (cfg, sig->ret->type != MONO_TYPE_VOID ? sp [-1] : NULL);
  7224| 			g_assert (!method_does_not_return (method));
  7225| 			if (cfg->method != method) {
  7226| 				/* return from inlined method */
  7227| 				/*
  7228| 				 * If in_count == 0, that means the ret is unreachable due to
  7229| 				 * being preceded by a throw. In that case, inline_method () will
  7230| 				 * handle setting the return value
  7231| 				 * (test case: test_0_inline_throw ()).
  7232| 				 */
  7233| 				if (return_var && cfg->cbb->in_count) {
  7234| 					MonoType *ret_type = mono_method_signature_internal (method)->ret;
  7235| 					MonoInst *store;
  7236| 					CHECK_STACK (1);
  7237| 					--sp;
  7238| 					*sp = convert_value (cfg, ret_type, *sp);
  7239| 					if ((method->wrapper_type == MONO_WRAPPER_DYNAMIC_METHOD || method->wrapper_type == MONO_WRAPPER_NONE) && target_type_is_incompatible (cfg, ret_type, *sp))
  7240| 						UNVERIFIED;
  7241| 					EMIT_NEW_TEMPSTORE (cfg, store, return_var->inst_c0, *sp);
  7242| 					cfg->ret_var_set = TRUE;
  7243| 				}
  7244| 			} else {
  7245| 				if (cfg->lmf_var && cfg->cbb->in_count && (!cfg->llvm_only || cfg->deopt))
  7246| 					emit_pop_lmf (cfg);
  7247| 				if (cfg->ret) {
  7248| 					MonoType *ret_type = mini_get_underlying_type (mono_method_signature_internal (method)->ret);
  7249| 					if (seq_points && !sym_seq_points) {
  7250| 						/*
  7251| 						 * Place a seq point here too even through the IL stack is not
  7252| 						 * empty, so a step over on
  7253| 						 * call <FOO>
  7254| 						 * ret
  7255| 						 * will work correctly.
  7256| 						 */
  7257| 						NEW_SEQ_POINT (cfg, ins, ip - header->code, TRUE);
  7258| 						MONO_ADD_INS (cfg->cbb, ins);
  7259| 					}
  7260| 					g_assert (!return_var);
  7261| 					CHECK_STACK (1);
  7262| 					--sp;
  7263| 					*sp = convert_value (cfg, ret_type, *sp);
  7264| 					if ((method->wrapper_type == MONO_WRAPPER_DYNAMIC_METHOD || method->wrapper_type == MONO_WRAPPER_NONE) && target_type_is_incompatible (cfg, ret_type, *sp))
  7265| 						UNVERIFIED;
  7266| 					emit_setret (cfg, *sp);
  7267| 				}
  7268| 			}
  7269| 			if (sp != stack_start)
  7270| 				UNVERIFIED;
  7271| 			MONO_INST_NEW (cfg, ins, OP_BR);
  7272| 			ins->inst_target_bb = end_bblock;
  7273| 			MONO_ADD_INS (cfg->cbb, ins);
  7274| 			link_bblock (cfg, cfg->cbb, end_bblock);
  7275| 			start_new_bblock = 1;
  7276| 			break;
  7277| 		case MONO_CEE_BR_S:
  7278| 			MONO_INST_NEW (cfg, ins, OP_BR);
  7279| 			GET_BBLOCK (cfg, tblock, target);
  7280| 			link_bblock (cfg, cfg->cbb, tblock);
  7281| 			ins->inst_target_bb = tblock;
  7282| 			if (sp != stack_start) {
  7283| 				handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start));
  7284| 				sp = stack_start;
  7285| 				CHECK_UNVERIFIABLE (cfg);
  7286| 			}
  7287| 			MONO_ADD_INS (cfg->cbb, ins);
  7288| 			start_new_bblock = 1;
  7289| 			inline_costs += BRANCH_COST;
  7290| 			break;
  7291| 		case MONO_CEE_BEQ_S:
  7292| 		case MONO_CEE_BGE_S:
  7293| 		case MONO_CEE_BGT_S:
  7294| 		case MONO_CEE_BLE_S:
  7295| 		case MONO_CEE_BLT_S:
  7296| 		case MONO_CEE_BNE_UN_S:
  7297| 		case MONO_CEE_BGE_UN_S:
  7298| 		case MONO_CEE_BGT_UN_S:
  7299| 		case MONO_CEE_BLE_UN_S:
  7300| 		case MONO_CEE_BLT_UN_S:
  7301| 			MONO_INST_NEW (cfg, ins, il_op + BIG_BRANCH_OFFSET);
  7302| 			ADD_BINCOND (NULL);
  7303| 			sp = stack_start;
  7304| 			inline_costs += BRANCH_COST;
  7305| 			break;
  7306| 		case MONO_CEE_BR:
  7307| 			MONO_INST_NEW (cfg, ins, OP_BR);
  7308| 			GET_BBLOCK (cfg, tblock, target);
  7309| 			link_bblock (cfg, cfg->cbb, tblock);
  7310| 			ins->inst_target_bb = tblock;
  7311| 			if (sp != stack_start) {
  7312| 				handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start));
  7313| 				sp = stack_start;
  7314| 				CHECK_UNVERIFIABLE (cfg);
  7315| 			}
  7316| 			MONO_ADD_INS (cfg->cbb, ins);
  7317| 			start_new_bblock = 1;
  7318| 			inline_costs += BRANCH_COST;
  7319| 			break;
  7320| 		case MONO_CEE_BRFALSE_S:
  7321| 		case MONO_CEE_BRTRUE_S:
  7322| 		case MONO_CEE_BRFALSE:
  7323| 		case MONO_CEE_BRTRUE: {
  7324| 			MonoInst *cmp;
  7325| 			gboolean is_true = il_op == MONO_CEE_BRTRUE_S || il_op == MONO_CEE_BRTRUE;
  7326| 			if (sp [-1]->type == STACK_VTYPE || sp [-1]->type == STACK_R8)
  7327| 				UNVERIFIED;
  7328| 			sp--;
  7329| 			GET_BBLOCK (cfg, tblock, target);
  7330| 			link_bblock (cfg, cfg->cbb, tblock);
  7331| 			GET_BBLOCK (cfg, tblock, next_ip);
  7332| 			link_bblock (cfg, cfg->cbb, tblock);
  7333| 			if (sp != stack_start) {
  7334| 				handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start));
  7335| 				CHECK_UNVERIFIABLE (cfg);
  7336| 			}
  7337| 			MONO_INST_NEW(cfg, cmp, OP_ICOMPARE_IMM);
  7338| 			cmp->sreg1 = sp [0]->dreg;
  7339| 			type_from_op (cfg, cmp, sp [0], NULL);
  7340| 			CHECK_TYPE (cmp);
  7341| #if SIZEOF_REGISTER == 4
  7342| 			if (cmp->opcode == OP_LCOMPARE_IMM) {
  7343| 				/* Convert it to OP_LCOMPARE */
  7344| 				MONO_INST_NEW (cfg, ins, OP_I8CONST);
  7345| 				ins->type = STACK_I8;
  7346| 				ins->dreg = alloc_dreg (cfg, STACK_I8);
  7347| 				ins->inst_l = 0;
  7348| 				MONO_ADD_INS (cfg->cbb, ins);
  7349| 				cmp->opcode = OP_LCOMPARE;
  7350| 				cmp->sreg2 = ins->dreg;
  7351| 			}
  7352| #endif
  7353| 			MONO_ADD_INS (cfg->cbb, cmp);
  7354| 			MONO_INST_NEW (cfg, ins, is_true ? CEE_BNE_UN : CEE_BEQ);
  7355| 			type_from_op (cfg, ins, sp [0], NULL);
  7356| 			MONO_ADD_INS (cfg->cbb, ins);
  7357| 			ins->inst_many_bb = (MonoBasicBlock **)mono_mempool_alloc (cfg->mempool, sizeof (gpointer) * 2);
  7358| 			GET_BBLOCK (cfg, tblock, target);
  7359| 			ins->inst_true_bb = tblock;
  7360| 			GET_BBLOCK (cfg, tblock, next_ip);
  7361| 			ins->inst_false_bb = tblock;
  7362| 			start_new_bblock = 2;
  7363| 			sp = stack_start;
  7364| 			inline_costs += BRANCH_COST;
  7365| 			break;
  7366| 		}
  7367| 		case MONO_CEE_BEQ:
  7368| 		case MONO_CEE_BGE:
  7369| 		case MONO_CEE_BGT:
  7370| 		case MONO_CEE_BLE:
  7371| 		case MONO_CEE_BLT:
  7372| 		case MONO_CEE_BNE_UN:
  7373| 		case MONO_CEE_BGE_UN:
  7374| 		case MONO_CEE_BGT_UN:
  7375| 		case MONO_CEE_BLE_UN:
  7376| 		case MONO_CEE_BLT_UN:
  7377| 			MONO_INST_NEW (cfg, ins, il_op);
  7378| 			ADD_BINCOND (NULL);
  7379| 			sp = stack_start;
  7380| 			inline_costs += BRANCH_COST;
  7381| 			break;
  7382| 		case MONO_CEE_SWITCH: {
  7383| 			MonoInst *src1;
  7384| 			MonoBasicBlock **targets;
  7385| 			MonoBasicBlock *default_bblock;
  7386| 			MonoJumpInfoBBTable *table;
  7387| 			int offset_reg = alloc_preg (cfg);
  7388| 			int target_reg = alloc_preg (cfg);
  7389| 			int table_reg = alloc_preg (cfg);
  7390| 			int sum_reg = alloc_preg (cfg);
  7391| 			gboolean use_op_switch;
  7392| 			n = read32 (ip + 1);
  7393| 			--sp;
  7394| 			src1 = sp [0];
  7395| 			if ((src1->type != STACK_I4) && (src1->type != STACK_PTR))
  7396| 				UNVERIFIED;
  7397| 			ip += 5;
  7398| 			GET_BBLOCK (cfg, default_bblock, next_ip);
  7399| 			default_bblock->flags |= BB_INDIRECT_JUMP_TARGET;
  7400| 			targets = (MonoBasicBlock **)mono_mempool_alloc (cfg->mempool, sizeof (MonoBasicBlock*) * n);
  7401| 			for (int i = 0; i < n; ++i) {
  7402| 				GET_BBLOCK (cfg, tblock, next_ip + (gint32)read32 (ip));
  7403| 				targets [i] = tblock;
  7404| 				targets [i]->flags |= BB_INDIRECT_JUMP_TARGET;
  7405| 				ip += 4;
  7406| 			}
  7407| 			if (sp != stack_start) {
  7408| 				/*
  7409| 				 * Link the current bb with the targets as well, so handle_stack_args
  7410| 				 * will set their in_stack correctly.
  7411| 				 */
  7412| 				link_bblock (cfg, cfg->cbb, default_bblock);
  7413| 				for (int i = 0; i < n; ++i)
  7414| 					link_bblock (cfg, cfg->cbb, targets [i]);
  7415| 				handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start));
  7416| 				sp = stack_start;
  7417| 				CHECK_UNVERIFIABLE (cfg);
  7418| 				/* Undo the links */
  7419| 				mono_unlink_bblock (cfg, cfg->cbb, default_bblock);
  7420| 				for (int i = 0; i < n; ++i)
  7421| 					mono_unlink_bblock (cfg, cfg->cbb, targets [i]);
  7422| 			}
  7423| 			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_ICOMPARE_IMM, -1, src1->dreg, n);
  7424| 			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBGE_UN, default_bblock);
  7425| 			for (int i = 0; i < n; ++i)
  7426| 				link_bblock (cfg, cfg->cbb, targets [i]);
  7427| 			table = (MonoJumpInfoBBTable *)mono_mempool_alloc (cfg->mempool, sizeof (MonoJumpInfoBBTable));
  7428| 			table->table = targets;
  7429| 			table->table_size = n;
  7430| 			use_op_switch = FALSE;
  7431| #ifdef TARGET_ARM
  7432| 			/* ARM implements SWITCH statements differently */
  7433| 			/* FIXME: Make it use the generic implementation */
  7434| 			if (!cfg->compile_aot)
  7435| 				use_op_switch = TRUE;
  7436| #endif
  7437| 			if (COMPILE_LLVM (cfg))
  7438| 				use_op_switch = TRUE;
  7439| 			cfg->cbb->has_jump_table = 1;
  7440| 			if (use_op_switch) {
  7441| 				MONO_INST_NEW (cfg, ins, OP_SWITCH);
  7442| 				ins->sreg1 = src1->dreg;
  7443| 				ins->inst_p0 = table;
  7444| 				ins->inst_many_bb = targets;
  7445| 				ins->klass = (MonoClass *)GUINT_TO_POINTER (n);
  7446| 				MONO_ADD_INS (cfg->cbb, ins);
  7447| 			} else {
  7448| 				if (TARGET_SIZEOF_VOID_P == 8)
  7449| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_SHL_IMM, offset_reg, src1->dreg, 3);
  7450| 				else
  7451| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_SHL_IMM, offset_reg, src1->dreg, 2);
  7452| #if SIZEOF_REGISTER == 8
  7453| 				/* The upper word might not be zero, and we add it to a 64 bit address later */
  7454| 				MONO_EMIT_NEW_UNALU (cfg, OP_ZEXT_I4, offset_reg, offset_reg);
  7455| #endif
  7456| 				if (cfg->compile_aot) {
  7457| 					MONO_EMIT_NEW_AOTCONST (cfg, table_reg, table, MONO_PATCH_INFO_SWITCH);
  7458| 				} else {
  7459| 					MONO_INST_NEW (cfg, ins, OP_JUMP_TABLE);
  7460| 					ins->inst_c1 = MONO_PATCH_INFO_SWITCH;
  7461| 					ins->inst_p0 = table;
  7462| 					ins->dreg = table_reg;
  7463| 					MONO_ADD_INS (cfg->cbb, ins);
  7464| 				}
  7465| 				/* FIXME: Use load_memindex */
  7466| 				MONO_EMIT_NEW_BIALU (cfg, OP_PADD, sum_reg, table_reg, offset_reg);
  7467| 				MONO_EMIT_NEW_LOAD_MEMBASE (cfg, target_reg, sum_reg, 0);
  7468| 				MONO_EMIT_NEW_UNALU (cfg, OP_BR_REG, -1, target_reg);
  7469| 			}
  7470| 			start_new_bblock = 1;
  7471| 			inline_costs += BRANCH_COST * 2;
  7472| 			break;
  7473| 		}
  7474| 		case MONO_CEE_LDIND_I1:
  7475| 		case MONO_CEE_LDIND_U1:
  7476| 		case MONO_CEE_LDIND_I2:
  7477| 		case MONO_CEE_LDIND_U2:
  7478| 		case MONO_CEE_LDIND_I4:
  7479| 		case MONO_CEE_LDIND_U4:
  7480| 		case MONO_CEE_LDIND_I8:
  7481| 		case MONO_CEE_LDIND_I:
  7482| 		case MONO_CEE_LDIND_R4:
  7483| 		case MONO_CEE_LDIND_R8:
  7484| 		case MONO_CEE_LDIND_REF:
  7485| 			--sp;
  7486| 			if (!(ins_flag & MONO_INST_NONULLCHECK))
  7487| 				MONO_EMIT_NULL_CHECK (cfg, sp [0]->dreg, FALSE);
  7488| 			ins = mini_emit_memory_load (cfg, m_class_get_byval_arg (ldind_to_type (il_op)), sp [0], 0, ins_flag);
  7489| 			*sp++ = ins;
  7490| 			ins_flag = 0;
  7491| 			break;
  7492| 		case MONO_CEE_STIND_REF:
  7493| 		case MONO_CEE_STIND_I1:
  7494| 		case MONO_CEE_STIND_I2:
  7495| 		case MONO_CEE_STIND_I4:
  7496| 		case MONO_CEE_STIND_I8:
  7497| 		case MONO_CEE_STIND_R4:
  7498| 		case MONO_CEE_STIND_R8:
  7499| 		case MONO_CEE_STIND_I: {
  7500| 			sp -= 2;
  7501| 			if (il_op == MONO_CEE_STIND_REF && sp [1]->type != STACK_OBJ) {
  7502| 				/* stind.ref must only be used with object references. */
  7503| 				UNVERIFIED;
  7504| 			}
  7505| 			if (il_op == MONO_CEE_STIND_R4 && sp [1]->type == STACK_R8)
  7506| 				sp [1] = convert_value (cfg, m_class_get_byval_arg (mono_defaults.single_class), sp [1]);
  7507| 			mini_emit_memory_store (cfg, m_class_get_byval_arg (stind_to_type (il_op)), sp [0], sp [1], ins_flag);
  7508| 			ins_flag = 0;
  7509| 			inline_costs += 1;
  7510| 			break;
  7511| 		}
  7512| 		case MONO_CEE_MUL: {
  7513| 			MONO_INST_NEW (cfg, ins, il_op);
  7514| 			sp -= 2;
  7515| 			ins->sreg1 = sp [0]->dreg;
  7516| 			ins->sreg2 = sp [1]->dreg;
  7517| 			type_from_op (cfg, ins, sp [0], sp [1]);
  7518| 			CHECK_TYPE (ins);
  7519| 			ins->dreg = alloc_dreg ((cfg), (MonoStackType)(ins)->type);
  7520| 			/* Use the immediate opcodes if possible */
  7521| 			int imm_opcode; imm_opcode = mono_op_to_op_imm_noemul (ins->opcode);
  7522| 			if ((sp [1]->opcode == OP_ICONST) && mono_arch_is_inst_imm (ins->opcode, imm_opcode, sp [1]->inst_c0)) {
  7523| 				if (imm_opcode != -1) {
  7524| 					ins->opcode = GINT_TO_OPCODE (imm_opcode);
  7525| 					ins->inst_p1 = (gpointer)(gssize)(sp [1]->inst_c0);
  7526| 					ins->sreg2 = -1;
  7527| 					NULLIFY_INS (sp [1]);
  7528| 				}
  7529| 			}
  7530| 			MONO_ADD_INS ((cfg)->cbb, (ins));
  7531| 			*sp++ = mono_decompose_opcode (cfg, ins);
  7532| 			break;
  7533| 		}
  7534| 		case MONO_CEE_ADD:
  7535| 		case MONO_CEE_SUB:
  7536| 		case MONO_CEE_DIV:
  7537| 		case MONO_CEE_DIV_UN:
  7538| 		case MONO_CEE_REM:
  7539| 		case MONO_CEE_REM_UN:
  7540| 		case MONO_CEE_AND:
  7541| 		case MONO_CEE_OR:
  7542| 		case MONO_CEE_XOR:
  7543| 		case MONO_CEE_SHL:
  7544| 		case MONO_CEE_SHR:
  7545| 		case MONO_CEE_SHR_UN: {
  7546| 			MONO_INST_NEW (cfg, ins, il_op);
  7547| 			sp -= 2;
  7548| 			ins->sreg1 = sp [0]->dreg;
  7549| 			ins->sreg2 = sp [1]->dreg;
  7550| 			type_from_op (cfg, ins, sp [0], sp [1]);
  7551| 			CHECK_TYPE (ins);
  7552| 			add_widen_op (cfg, ins, &sp [0], &sp [1]);
  7553| 			ins->dreg = alloc_dreg ((cfg), (MonoStackType)(ins)->type);
  7554| 			/* Use the immediate opcodes if possible */
  7555| 			int imm_opcode; imm_opcode = mono_op_to_op_imm_noemul (ins->opcode);
  7556| 			if (((sp [1]->opcode == OP_ICONST) || (sp [1]->opcode == OP_I8CONST)) &&
  7557| 			    mono_arch_is_inst_imm (ins->opcode, imm_opcode, sp [1]->opcode == OP_ICONST ? sp [1]->inst_c0 : sp [1]->inst_l)) {
  7558| 				if (imm_opcode != -1) {
  7559| 					ins->opcode = GINT_TO_OPCODE (imm_opcode);
  7560| 					if (sp [1]->opcode == OP_I8CONST) {
  7561| #if SIZEOF_REGISTER == 8
  7562| 						ins->inst_imm = sp [1]->inst_l;
  7563| #else
  7564| 						ins->inst_l = sp [1]->inst_l;
  7565| #endif
  7566| 					} else {
  7567| 						ins->inst_imm = (gssize)(sp [1]->inst_c0);
  7568| 					}
  7569| 					ins->sreg2 = -1;
  7570| 					/* Might be followed by an instruction added by add_widen_op */
  7571| 					if (sp [1]->next == NULL)
  7572| 						NULLIFY_INS (sp [1]);
  7573| 				}
  7574| 			}
  7575| 			MONO_ADD_INS ((cfg)->cbb, (ins));
  7576| 			*sp++ = mono_decompose_opcode (cfg, ins);
  7577| 			break;
  7578| 		}
  7579| 		case MONO_CEE_NEG:
  7580| 		case MONO_CEE_NOT:
  7581| 		case MONO_CEE_CONV_I1:
  7582| 		case MONO_CEE_CONV_I2:
  7583| 		case MONO_CEE_CONV_I4:
  7584| 		case MONO_CEE_CONV_R4:
  7585| 		case MONO_CEE_CONV_R8:
  7586| 		case MONO_CEE_CONV_U4:
  7587| 		case MONO_CEE_CONV_I8:
  7588| 		case MONO_CEE_CONV_U8:
  7589| 		case MONO_CEE_CONV_OVF_I8:
  7590| 		case MONO_CEE_CONV_OVF_U8:
  7591| 		case MONO_CEE_CONV_R_UN:
  7592| 			/* Special case this earlier so we have long constants in the IR */
  7593| 			if ((il_op == MONO_CEE_CONV_I8 || il_op == MONO_CEE_CONV_U8) && (sp [-1]->opcode == OP_ICONST)) {
  7594| 				int data = GTMREG_TO_INT (sp [-1]->inst_c0);
  7595| 				sp [-1]->opcode = OP_I8CONST;
  7596| 				sp [-1]->type = STACK_I8;
  7597| #if SIZEOF_REGISTER == 8
  7598| 				if (il_op == MONO_CEE_CONV_U8)
  7599| 					sp [-1]->inst_c0 = (guint32)data;
  7600| 				else
  7601| 					sp [-1]->inst_c0 = data;
  7602| #else
  7603| 				if (il_op == MONO_CEE_CONV_U8)
  7604| 					sp [-1]->inst_l = (guint32)data;
  7605| 				else
  7606| 					sp [-1]->inst_l = data;
  7607| #endif
  7608| 				sp [-1]->dreg = alloc_dreg (cfg, STACK_I8);
  7609| 			}
  7610| 			else {
  7611| 				ADD_UNOP (il_op);
  7612| 			}
  7613| 			break;
  7614| 		case MONO_CEE_CONV_OVF_I4:
  7615| 		case MONO_CEE_CONV_OVF_I1:
  7616| 		case MONO_CEE_CONV_OVF_I2:
  7617| 		case MONO_CEE_CONV_OVF_I:
  7618| 		case MONO_CEE_CONV_OVF_I1_UN:
  7619| 		case MONO_CEE_CONV_OVF_I2_UN:
  7620| 		case MONO_CEE_CONV_OVF_I4_UN:
  7621| 		case MONO_CEE_CONV_OVF_I8_UN:
  7622| 		case MONO_CEE_CONV_OVF_I_UN:
  7623| 			if (sp [-1]->type == STACK_R8 || sp [-1]->type == STACK_R4) {
  7624| 				/* floats are always signed, _UN has no effect */
  7625| 				ADD_UNOP (CEE_CONV_OVF_I8);
  7626| 				if (il_op == MONO_CEE_CONV_OVF_I1_UN)
  7627| 					ADD_UNOP (MONO_CEE_CONV_OVF_I1);
  7628| 				else if (il_op == MONO_CEE_CONV_OVF_I2_UN)
  7629| 					ADD_UNOP (MONO_CEE_CONV_OVF_I2);
  7630| 				else if (il_op == MONO_CEE_CONV_OVF_I4_UN)
  7631| 					ADD_UNOP (MONO_CEE_CONV_OVF_I4);
  7632| 				else if (il_op == MONO_CEE_CONV_OVF_I8_UN)
  7633| 					;
  7634| 				else
  7635| 					ADD_UNOP (il_op);
  7636| 			} else {
  7637| 				ADD_UNOP (il_op);
  7638| 			}
  7639| 			break;
  7640| 		case MONO_CEE_CONV_OVF_U1:
  7641| 		case MONO_CEE_CONV_OVF_U2:
  7642| 		case MONO_CEE_CONV_OVF_U4:
  7643| 		case MONO_CEE_CONV_OVF_U:
  7644| 		case MONO_CEE_CONV_OVF_U1_UN:
  7645| 		case MONO_CEE_CONV_OVF_U2_UN:
  7646| 		case MONO_CEE_CONV_OVF_U4_UN:
  7647| 		case MONO_CEE_CONV_OVF_U8_UN:
  7648| 		case MONO_CEE_CONV_OVF_U_UN:
  7649| 			if (sp [-1]->type == STACK_R8 || sp [-1]->type == STACK_R4) {
  7650| 				/* floats are always signed, _UN has no effect */
  7651| 				ADD_UNOP (CEE_CONV_OVF_U8);
  7652| 				if (TARGET_SIZEOF_VOID_P == 8 && il_op == MONO_CEE_CONV_OVF_U)
  7653| 					sp [-1]->type = STACK_PTR; // no additional conversion needed
  7654| 				else
  7655| 					ADD_UNOP (il_op);
  7656| 			} else {
  7657| 				ADD_UNOP (il_op);
  7658| 			}
  7659| 			break;
  7660| 		case MONO_CEE_CONV_U2:
  7661| 		case MONO_CEE_CONV_U1:
  7662| 		case MONO_CEE_CONV_U:
  7663| 		case MONO_CEE_CONV_I:
  7664| 			ADD_UNOP (il_op);
  7665| 			CHECK_CFG_EXCEPTION;
  7666| 			break;
  7667| 		case MONO_CEE_ADD_OVF:
  7668| 		case MONO_CEE_ADD_OVF_UN:
  7669| 		case MONO_CEE_MUL_OVF:
  7670| 		case MONO_CEE_MUL_OVF_UN:
  7671| 		case MONO_CEE_SUB_OVF:
  7672| 		case MONO_CEE_SUB_OVF_UN:
  7673| 			MONO_INST_NEW (cfg, ins, il_op);
  7674| 			sp -= 2;
  7675| 			ins->sreg1 = sp [0]->dreg;
  7676| 			ins->sreg2 = sp [1]->dreg;
  7677| 			type_from_op (cfg, ins, sp [0], sp [1]);
  7678| 			CHECK_TYPE (ins);
  7679| 			if (ovf_exc)
  7680| 				ins->inst_exc_name = ovf_exc;
  7681| 			else
  7682| 				ins->inst_exc_name = "OverflowException";
  7683| 			/* Have to insert a widening op */
  7684| 			add_widen_op (cfg, ins, &sp [0], &sp [1]);
  7685| 			ins->dreg = alloc_dreg (cfg, (MonoStackType)(ins)->type);
  7686| 			MONO_ADD_INS ((cfg)->cbb, ins);
  7687| 			/* The opcode might be emulated, so need to special case this */
  7688| 			if (ovf_exc && mono_find_jit_opcode_emulation (ins->opcode)) {
  7689| 				switch (ins->opcode) {
  7690| 				case OP_IMUL_OVF_UN:
  7691| 					/* This opcode is just a placeholder, it will be emulated also */
  7692| 					ins->opcode = OP_IMUL_OVF_UN_OOM;
  7693| 					break;
  7694| 				case OP_LMUL_OVF_UN:
  7695| 					/* This opcode is just a placeholder, it will be emulated also */
  7696| 					ins->opcode = OP_LMUL_OVF_UN_OOM;
  7697| 					break;
  7698| 				default:
  7699| 					g_assert_not_reached ();
  7700| 				}
  7701| 			}
  7702| 			ovf_exc = NULL;
  7703| 			*sp++ = mono_decompose_opcode (cfg, ins);
  7704| 			break;
  7705| 		case MONO_CEE_CPOBJ:
  7706| 			GSHAREDVT_FAILURE (il_op);
  7707| 			GSHAREDVT_FAILURE (*ip);
  7708| 			klass = mini_get_class (method, token, generic_context);
  7709| 			CHECK_TYPELOAD (klass);
  7710| 			sp -= 2;
  7711| 			mini_emit_memory_copy (cfg, sp [0], sp [1], klass, FALSE, ins_flag);
  7712| 			ins_flag = 0;
  7713| 			break;
  7714| 		case MONO_CEE_LDOBJ: {
  7715| 			int loc_index = -1;
  7716| 			int stloc_len = 0;
  7717| 			--sp;
  7718| 			klass = mini_get_class (method, token, generic_context);
  7719| 			CHECK_TYPELOAD (klass);
  7720| 			/* Optimize the common ldobj+stloc combination */
  7721| 			if (next_ip < end) {
  7722| 				switch (next_ip [0]) {
  7723| 				case MONO_CEE_STLOC_S:
  7724| 					CHECK_OPSIZE (7);
  7725| 					loc_index = next_ip [1];
  7726| 					stloc_len = 2;
  7727| 					break;
  7728| 				case MONO_CEE_STLOC_0:
  7729| 				case MONO_CEE_STLOC_1:
  7730| 				case MONO_CEE_STLOC_2:
  7731| 				case MONO_CEE_STLOC_3:
  7732| 					loc_index = next_ip [0] - CEE_STLOC_0;
  7733| 					stloc_len = 1;
  7734| 					break;
  7735| 				default:
  7736| 					break;
  7737| 				}
  7738| 			}
  7739| 			if ((loc_index != -1) && ip_in_bb (cfg, cfg->cbb, next_ip)) {
  7740| 				CHECK_LOCAL (loc_index);
  7741| 				EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), sp [0]->dreg, 0);
  7742| 				ins->dreg = cfg->locals [loc_index]->dreg;
  7743| 				ins->flags |= ins_flag;
  7744| 				il_op = (MonoOpcodeEnum)next_ip [0];
  7745| 				next_ip += stloc_len;
  7746| 				if (ins_flag & MONO_INST_VOLATILE) {
  7747| 					/* Volatile loads have acquire semantics, see 12.6.7 in Ecma 335 */
  7748| 					mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_ACQ);
  7749| 				}
  7750| 				ins_flag = 0;
  7751| 				break;
  7752| 			}
  7753| 			/* Optimize the ldobj+stobj combination */
  7754| 			if (next_ip + 4 < end && next_ip [0] == CEE_STOBJ && ip_in_bb (cfg, cfg->cbb, next_ip) && read32 (next_ip + 1) == token) {
  7755| 				CHECK_STACK (1);
  7756| 				sp --;
  7757| 				mini_emit_memory_copy (cfg, sp [0], sp [1], klass, FALSE, ins_flag);
  7758| 				il_op = (MonoOpcodeEnum)next_ip [0];
  7759| 				next_ip += 5;
  7760| 				ins_flag = 0;
  7761| 				break;
  7762| 			}
  7763| 			if (!(ins_flag & MONO_INST_NONULLCHECK))
  7764| 				MONO_EMIT_NULL_CHECK (cfg, sp [0]->dreg, FALSE);
  7765| 			ins = mini_emit_memory_load (cfg, m_class_get_byval_arg (klass), sp [0], 0, ins_flag);
  7766| 			*sp++ = ins;
  7767| 			ins_flag = 0;
  7768| 			inline_costs += 1;
  7769| 			break;
  7770| 		}
  7771| 		case MONO_CEE_LDSTR:
  7772| 			if (method->wrapper_type == MONO_WRAPPER_DYNAMIC_METHOD) {
  7773| 				EMIT_NEW_PCONST (cfg, ins, mono_method_get_wrapper_data (method, n));
  7774| 				ins->type = STACK_OBJ;
  7775| 				*sp = ins;
  7776| 			}
  7777| 			else if (method->wrapper_type != MONO_WRAPPER_NONE) {
  7778| 				MonoInst *iargs [1];
  7779| 				char *str = (char *)mono_method_get_wrapper_data (method, n);
  7780| 				if (cfg->compile_aot)
  7781| 					EMIT_NEW_LDSTRLITCONST (cfg, iargs [0], str);
  7782| 				else
  7783| 					EMIT_NEW_PCONST (cfg, iargs [0], str);
  7784| 				*sp = mono_emit_jit_icall (cfg, mono_string_new_wrapper_internal, iargs);
  7785| 			} else {
  7786| 				{
  7787| 					if (cfg->cbb->out_of_line) {
  7788| 						MonoInst *iargs [2];
  7789| 						if (image == mono_defaults.corlib) {
  7790| 							/*
  7791| 							 * Avoid relocations in AOT and save some space by using a
  7792| 							 * version of helper_ldstr specialized to mscorlib.
  7793| 							 */
  7794| 							EMIT_NEW_ICONST (cfg, iargs [0], mono_metadata_token_index (n));
  7795| 							*sp = mono_emit_jit_icall (cfg, mono_helper_ldstr_mscorlib, iargs);
  7796| 						} else {
  7797| 							/* Avoid creating the string object */
  7798| 							EMIT_NEW_IMAGECONST (cfg, iargs [0], image);
  7799| 							EMIT_NEW_ICONST (cfg, iargs [1], mono_metadata_token_index (n));
  7800| 							*sp = mono_emit_jit_icall (cfg, mono_helper_ldstr, iargs);
  7801| 						}
  7802| 					}
  7803| 					else
  7804| 					if (cfg->compile_aot) {
  7805| 						NEW_LDSTRCONST (cfg, ins, image, n);
  7806| 						*sp = ins;
  7807| 						MONO_ADD_INS (cfg->cbb, ins);
  7808| 					}
  7809| 					else {
  7810| 						NEW_PCONST (cfg, ins, NULL);
  7811| 						ins->type = STACK_OBJ;
  7812| 						ins->inst_p0 = mono_ldstr_checked (image, mono_metadata_token_index (n), cfg->error);
  7813| 						CHECK_CFG_ERROR;
  7814| 						if (!ins->inst_p0)
  7815| 							OUT_OF_MEMORY_FAILURE;
  7816| 						*sp = ins;
  7817| 						MONO_ADD_INS (cfg->cbb, ins);
  7818| 					}
  7819| 				}
  7820| 			}
  7821| 			sp++;
  7822| 			break;
  7823| 		case MONO_CEE_NEWOBJ: {
  7824| 			MonoInst *iargs [2];
  7825| 			MonoInst this_ins;
  7826| 			MonoInst *alloc;
  7827| 			MonoInst *vtable_arg = NULL;
  7828| 			cmethod = mini_get_method (cfg, method, token, NULL, generic_context);
  7829| 			CHECK_CFG_ERROR;
  7830| 			fsig = mono_method_get_signature_checked (cmethod, image, token, generic_context, cfg->error);
  7831| 			CHECK_CFG_ERROR;
  7832| 			mono_save_token_info (cfg, image, token, cmethod);
  7833| 			if (!mono_class_init_internal (cmethod->klass))
  7834| 				TYPE_LOAD_ERROR (cmethod->klass);
  7835| 			context_used = mini_method_check_context_used (cfg, cmethod);
  7836| 			if (!dont_verify && !cfg->skip_visibility) {
  7837| 				MonoMethod *cil_method = cmethod;
  7838| 				MonoMethod *target_method = cil_method;
  7839| 				if (method->is_inflated) {
  7840| 					MonoGenericContainer *container = mono_method_get_generic_container(method_definition);
  7841| 					MonoGenericContext *context = (container != NULL ? &container->context : NULL);
  7842| 					target_method = mini_get_method_allow_open (method, token, NULL, context, cfg->error);
  7843| 					CHECK_CFG_ERROR;
  7844| 				}
  7845| 				if (!mono_method_can_access_method (method_definition, target_method) &&
  7846| 					!mono_method_can_access_method (method, cil_method))
  7847| 					emit_method_access_failure (cfg, method, cil_method);
  7848| 			}
  7849| 			if (cfg->gshared && cmethod && cmethod->klass != method->klass && mono_class_is_ginst (cmethod->klass) && mono_method_is_generic_sharable (cmethod, TRUE) && mono_class_needs_cctor_run (cmethod->klass, method)) {
  7850| 				emit_class_init (cfg, cmethod->klass, FALSE);
  7851| 				CHECK_TYPELOAD (cmethod->klass);
  7852| 			}
  7853| 			/*
  7854| 			if (cfg->gsharedvt) {
  7855| 				if (mini_is_gsharedvt_variable_signature (sig))
  7856| 					GSHAREDVT_FAILURE (il_op);
  7857| 			}
  7858| 			*/
  7859| 			n = fsig->param_count;
  7860| 			CHECK_STACK (n);
  7861| 			if (cfg->gsharedvt_min && mini_is_gsharedvt_variable_signature (fsig))
  7862| 				GSHAREDVT_FAILURE (il_op);
  7863| 			/*
  7864| 			 * Generate smaller code for the common newobj <exception> instruction in
  7865| 			 * argument checking code.
  7866| 			 */
  7867| 			if (cfg->cbb->out_of_line && m_class_get_image (cmethod->klass) == mono_defaults.corlib &&
  7868| 				is_exception_class (cmethod->klass) && n <= 2 &&
  7869| 			    ((n < 1) || (!m_type_is_byref (fsig->params [0]) && fsig->params [0]->type == MONO_TYPE_STRING)) &&
  7870| 			    ((n < 2) || (!m_type_is_byref (fsig->params [1]) && fsig->params [1]->type == MONO_TYPE_STRING))) {
  7871| 				MonoInst *ex_iargs [3];
  7872| 				sp -= n;
  7873| 				EMIT_NEW_ICONST (cfg, ex_iargs [0], m_class_get_type_token (cmethod->klass));
  7874| 				switch (n) {
  7875| 				case 0:
  7876| 					*sp ++ = mono_emit_jit_icall (cfg, mono_create_corlib_exception_0, ex_iargs);
  7877| 					break;
  7878| 				case 1:
  7879| 					ex_iargs [1] = sp [0];
  7880| 					*sp ++ = mono_emit_jit_icall (cfg, mono_create_corlib_exception_1, ex_iargs);
  7881| 					break;
  7882| 				case 2:
  7883| 					ex_iargs [1] = sp [0];
  7884| 					ex_iargs [2] = sp [1];
  7885| 					*sp ++ = mono_emit_jit_icall (cfg, mono_create_corlib_exception_2, ex_iargs);
  7886| 					break;
  7887| 				default:
  7888| 					g_assert_not_reached ();
  7889| 				}
  7890| 				inline_costs += 5;
  7891| 				break;
  7892| 			}
  7893| 			/* move the args to allow room for 'this' in the first position */
  7894| 			while (n--) {
  7895| 				--sp;
  7896| 				sp [1] = sp [0];
  7897| 			}
  7898| 			for (int i = 0; i < fsig->param_count; ++i)
  7899| 				sp [i + fsig->hasthis] = convert_value (cfg, fsig->params [i], sp [i + fsig->hasthis]);
  7900| 			/* check_call_signature () requires sp[0] to be set */
  7901| 			this_ins.type = STACK_OBJ;
  7902| 			sp [0] = &this_ins;
  7903| 			if (check_call_signature (cfg, fsig, sp))
  7904| 				UNVERIFIED;
  7905| 			iargs [0] = NULL;
  7906| 			if (mini_class_is_system_array (cmethod->klass)) {
  7907| 				*sp = emit_get_rgctx_method (cfg, context_used,
  7908| 											 cmethod, MONO_RGCTX_INFO_METHOD);
  7909| 				MonoJitICallId function = MONO_JIT_ICALL_ZeroIsReserved;
  7910| 				int rank = m_class_get_rank (cmethod->klass);
  7911| 				int param_count = fsig->param_count;
  7912| 				/* Optimize the common cases, use ctor using length for each rank (no lbound). */
  7913| 				if (param_count == rank) {
  7914| 					switch (param_count) {
  7915| 					case 1: function = MONO_JIT_ICALL_mono_array_new_1;
  7916| 						break;
  7917| 					case 2: function = MONO_JIT_ICALL_mono_array_new_2;
  7918| 						break;
  7919| 					case 3: function = MONO_JIT_ICALL_mono_array_new_3;
  7920| 						break;
  7921| 					case 4: function = MONO_JIT_ICALL_mono_array_new_4;
  7922| 						break;
  7923| 					default:
  7924| 						break;
  7925| 					}
  7926| 				}
  7927| 				/* Regular case, rank > 4 or legnth, lbound specified per rank. */
  7928| 				if (function == MONO_JIT_ICALL_ZeroIsReserved) {
  7929| 					if  (!array_new_localalloc_ins) {
  7930| 						MONO_INST_NEW (cfg, array_new_localalloc_ins, OP_LOCALLOC_IMM);
  7931| 						array_new_localalloc_ins->dreg = alloc_preg (cfg);
  7932| 						cfg->flags |= MONO_CFG_HAS_ALLOCA;
  7933| 						MONO_ADD_INS (init_localsbb, array_new_localalloc_ins);
  7934| 					}
  7935| 					array_new_localalloc_ins->inst_imm = MAX (array_new_localalloc_ins->inst_imm, param_count * GUINT_TO_INT(sizeof (target_mgreg_t)));
  7936| 					int dreg = array_new_localalloc_ins->dreg;
  7937| 					if (2 * rank == param_count) {
  7938| 						/* [lbound, length, lbound, length, ...]
  7939| 						 * mono_array_new_n_icall expects a non-interleaved list of
  7940| 						 * lbounds and lengths, so deinterleave here.
  7941| 						 */
  7942| 						for (int l = 0; l < 2; ++l) {
  7943| 							int src = l;
  7944| 							int dst = l * rank;
  7945| 							for (int r = 0; r < rank; ++r, src += 2, ++dst) {
  7946| 								NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, dreg, dst * sizeof (target_mgreg_t), sp [src + 1]->dreg);
  7947| 								MONO_ADD_INS (cfg->cbb, ins);
  7948| 							}
  7949| 						}
  7950| 					} else {
  7951| 						/* [length, length, length, ...] */
  7952| 						for (int i = 0; i < param_count; ++i) {
  7953| 							NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, dreg, i * sizeof (target_mgreg_t), sp [i + 1]->dreg);
  7954| 							MONO_ADD_INS (cfg->cbb, ins);
  7955| 						}
  7956| 					}
  7957| 					EMIT_NEW_ICONST (cfg, ins, param_count);
  7958| 					sp [1] = ins;
  7959| 					EMIT_NEW_UNALU (cfg, ins, OP_MOVE, alloc_preg (cfg), dreg);
  7960| 					ins->type = STACK_PTR;
  7961| 					sp [2] = ins;
  7962| 					function = MONO_JIT_ICALL_mono_array_new_n_icall;
  7963| 				}
  7964| 				alloc = mono_emit_jit_icall_id (cfg, function, sp);
  7965| 			} else if (cmethod->string_ctor) {
  7966| 				g_assert (!context_used);
  7967| 				g_assert (!vtable_arg);
  7968| 				/* we simply pass a null pointer */
  7969| 				EMIT_NEW_PCONST (cfg, *sp, NULL);
  7970| 				/* now call the string ctor */
  7971| 				alloc = mini_emit_method_call_full (cfg, cmethod, fsig, FALSE, sp, NULL, NULL, NULL);
  7972| 			} else {
  7973| 				if (m_class_is_valuetype (cmethod->klass)) {
  7974| 					iargs [0] = mono_compile_create_var (cfg, m_class_get_byval_arg (cmethod->klass), OP_LOCAL);
  7975| 					mini_emit_init_rvar (cfg, iargs [0]->dreg, m_class_get_byval_arg (cmethod->klass));
  7976| 					EMIT_NEW_TEMPLOADA (cfg, *sp, iargs [0]->inst_c0);
  7977| 					alloc = NULL;
  7978| 					/*
  7979| 					 * The code generated by mini_emit_virtual_call () expects
  7980| 					 * iargs [0] to be a boxed instance, but luckily the vcall
  7981| 					 * will be transformed into a normal call there.
  7982| 					 */
  7983| 				} else if (context_used) {
  7984| 					alloc = handle_alloc (cfg, cmethod->klass, FALSE, context_used);
  7985| 					*sp = alloc;
  7986| 				} else {
  7987| 					MonoVTable *vtable = NULL;
  7988| 					if (!cfg->compile_aot)
  7989| 						vtable = mono_class_vtable_checked (cmethod->klass, cfg->error);
  7990| 					CHECK_CFG_ERROR;
  7991| 					CHECK_TYPELOAD (cmethod->klass);
  7992| 					/*
  7993| 					 * TypeInitializationExceptions thrown from the mono_runtime_class_init
  7994| 					 * call in mono_jit_runtime_invoke () can abort the finalizer thread.
  7995| 					 * As a workaround, we call class cctors before allocating objects.
  7996| 					 */
  7997| 					if (mini_field_access_needs_cctor_run (cfg, method, cmethod->klass, vtable) && !(g_slist_find (class_inits, cmethod->klass))) {
  7998| 						emit_class_init (cfg, cmethod->klass, TRUE);
  7999| 						if (cfg->verbose_level > 2)
  8000| 							printf ("class %s.%s needs init call for ctor\n", m_class_get_name_space (cmethod->klass), m_class_get_name (cmethod->klass));
  8001| 						class_inits = g_slist_prepend (class_inits, cmethod->klass);
  8002| 					}
  8003| 					alloc = handle_alloc (cfg, cmethod->klass, FALSE, 0);
  8004| 					*sp = alloc;
  8005| 				}
  8006| 				CHECK_CFG_EXCEPTION; /*for handle_alloc*/
  8007| 				if (alloc)
  8008| 					MONO_EMIT_NEW_UNALU (cfg, OP_NOT_NULL, -1, alloc->dreg);
  8009| 				/* Now call the actual ctor */
  8010| 				int ctor_inline_costs = 0;
  8011| 				handle_ctor_call (cfg, cmethod, fsig, context_used, sp, ip, &ctor_inline_costs);
  8012| 				if (!COMPILE_LLVM(cfg) || !(cmethod->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING))
  8013| 					inline_costs += ctor_inline_costs;
  8014| 				CHECK_CFG_EXCEPTION;
  8015| 			}
  8016| 			if (alloc == NULL) {
  8017| 				/* Valuetype */
  8018| 				EMIT_NEW_TEMPLOAD (cfg, ins, iargs [0]->inst_c0);
  8019| 				mini_type_to_eval_stack_type (cfg, m_class_get_byval_arg (ins->klass), ins);
  8020| 				*sp++= ins;
  8021| 			} else {
  8022| 				*sp++ = alloc;
  8023| 			}
  8024| 			inline_costs += 5;
  8025| 			if (!(seq_point_locs && mono_bitset_test_fast (seq_point_locs, next_ip - header->code)))
  8026| 				emit_seq_point (cfg, method, next_ip, FALSE, TRUE);
  8027| 			break;
  8028| 		}
  8029| 		case MONO_CEE_CASTCLASS:
  8030| 		case MONO_CEE_ISINST: {
  8031| 			--sp;
  8032| 			klass = mini_get_class (method, token, generic_context);
  8033| 			CHECK_TYPELOAD (klass);
  8034| 			if (sp [0]->type != STACK_OBJ)
  8035| 				UNVERIFIED;
  8036| 			MONO_INST_NEW (cfg, ins, (il_op == MONO_CEE_ISINST) ? OP_ISINST : OP_CASTCLASS);
  8037| 			ins->dreg = alloc_preg (cfg);
  8038| 			ins->sreg1 = (*sp)->dreg;
  8039| 			ins->klass = klass;
  8040| 			ins->type = STACK_OBJ;
  8041| 			MONO_ADD_INS (cfg->cbb, ins);
  8042| 			CHECK_CFG_EXCEPTION;
  8043| 			*sp++ = ins;
  8044| 			cfg->flags |= MONO_CFG_HAS_TYPE_CHECK;
  8045| 			break;
  8046| 		}
  8047| 		case MONO_CEE_UNBOX_ANY: {
  8048| 			MonoInst *res, *addr;
  8049| 			--sp;
  8050| 			klass = mini_get_class (method, token, generic_context);
  8051| 			CHECK_TYPELOAD (klass);
  8052| 			mono_save_token_info (cfg, image, token, klass);
  8053| 			context_used = mini_class_check_context_used (cfg, klass);
  8054| 			if (cfg->gsharedvt_min && mini_is_gsharedvt_variable_klass (klass))
  8055| 				GSHAREDVT_FAILURE (il_op);
  8056| 			if (mini_is_gsharedvt_klass (klass)) {
  8057| 				res = handle_unbox_gsharedvt (cfg, klass, *sp);
  8058| 				inline_costs += 2;
  8059| 			} else if (mini_class_is_reference (klass)) {
  8060| 				if (MONO_INS_IS_PCONST_NULL (*sp)) {
  8061| 					EMIT_NEW_PCONST (cfg, res, NULL);
  8062| 					res->type = STACK_OBJ;
  8063| 				} else {
  8064| 					MONO_INST_NEW (cfg, res, OP_CASTCLASS);
  8065| 					res->dreg = alloc_preg (cfg);
  8066| 					res->sreg1 = (*sp)->dreg;
  8067| 					res->klass = klass;
  8068| 					res->type = STACK_OBJ;
  8069| 					MONO_ADD_INS (cfg->cbb, res);
  8070| 					cfg->flags |= MONO_CFG_HAS_TYPE_CHECK;
  8071| 				}
  8072| 			} else if (mono_class_is_nullable (klass)) {
  8073| 				res = handle_unbox_nullable (cfg, *sp, klass, context_used);
  8074| 			} else {
  8075| 				addr = mini_handle_unbox (cfg, klass, *sp, context_used);
  8076| 				/* LDOBJ */
  8077| 				EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), addr->dreg, 0);
  8078| 				res = ins;
  8079| 				inline_costs += 2;
  8080| 			}
  8081| 			*sp ++ = res;
  8082| 			break;
  8083| 		}
  8084| 		case MONO_CEE_BOX: {
  8085| 			MonoInst *val;
  8086| 			MonoClass *enum_class;
  8087| 			MonoMethod *has_flag;
  8088| 			MonoMethodSignature *has_flag_sig;
  8089| 			--sp;
  8090| 			val = *sp;
  8091| 			klass = mini_get_class (method, token, generic_context);
  8092| 			CHECK_TYPELOAD (klass);
  8093| 			mono_save_token_info (cfg, image, token, klass);
  8094| 			context_used = mini_class_check_context_used (cfg, klass);
  8095| 			if (mini_class_is_reference (klass)) {
  8096| 				*sp++ = val;
  8097| 				break;
  8098| 			}
  8099| 			val = convert_value (cfg, m_class_get_byval_arg (klass), val);
  8100| 			if (klass == mono_defaults.void_class)
  8101| 				UNVERIFIED;
  8102| 			if (target_type_is_incompatible (cfg, m_class_get_byval_arg (klass), val))
  8103| 				UNVERIFIED;
  8104| 			/* frequent check in generic code: box (struct), brtrue */
  8105| 			/*
  8106| 			 * Look for:
  8107| 			 *
  8108| 			 *   <push int/long ptr>
  8109| 			 *   <push int/long>
  8110| 			 *   box MyFlags
  8111| 			 *   constrained. MyFlags
  8112| 			 *   callvirt instance bool class [mscorlib] System.Enum::HasFlag (class [mscorlib] System.Enum)
  8113| 			 *
  8114| 			 * If we find this sequence and the operand types on box and constrained
  8115| 			 * are equal, we can emit a specialized instruction sequence instead of
  8116| 			 * the very slow HasFlag () call.
  8117| 			 * This code sequence is generated by older mcs/csc, the newer one is handled in
  8118| 			 * emit_inst_for_method ().
  8119| 			 */
  8120| 			guint32 constrained_token;
  8121| 			guint32 callvirt_token;
  8122| 			if ((cfg->opt & MONO_OPT_INTRINS) &&
  8123| 			    next_ip < end && ip_in_bb (cfg, cfg->cbb, next_ip) &&
  8124| 			    (ip = il_read_constrained (next_ip, end, &constrained_token)) &&
  8125| 			    ip_in_bb (cfg, cfg->cbb, ip) &&
  8126| 			    (ip = il_read_callvirt (ip, end, &callvirt_token)) &&
  8127| 			    ip_in_bb (cfg, cfg->cbb, ip) &&
  8128| 			    m_class_is_enumtype (klass) &&
  8129| 			    (enum_class = mini_get_class (method, constrained_token, generic_context)) &&
  8130| 			    (has_flag = mini_get_method (cfg, method, callvirt_token, NULL, generic_context)) &&
  8131| 			    has_flag->klass == mono_defaults.enum_class &&
  8132| 			    !strcmp (has_flag->name, "HasFlag") &&
  8133| 			    (has_flag_sig = mono_method_signature_internal (has_flag)) &&
  8134| 			    has_flag_sig->hasthis &&
  8135| 			    has_flag_sig->param_count == 1) {
  8136| 				CHECK_TYPELOAD (enum_class);
  8137| 				if (enum_class == klass) {
  8138| 					MonoInst *enum_this, *enum_flag;
  8139| 					next_ip = ip;
  8140| 					il_op = MONO_CEE_CALLVIRT;
  8141| 					--sp;
  8142| 					enum_this = sp [0];
  8143| 					enum_flag = sp [1];
  8144| 					*sp++ = mini_handle_enum_has_flag (cfg, klass, enum_this, -1, enum_flag);
  8145| 					break;
  8146| 				}
  8147| 			}
  8148| 			guint32 unbox_any_token;
  8149| 			/*
  8150| 			 * Common in generic code:
  8151| 			 * box T1, unbox.any T2.
  8152| 			 */
  8153| 			if ((cfg->opt & MONO_OPT_INTRINS) &&
  8154| 			    next_ip < end && ip_in_bb (cfg, cfg->cbb, next_ip) &&
  8155| 			    (ip = il_read_unbox_any (next_ip, end, &unbox_any_token))) {
  8156| 				MonoClass *unbox_klass = mini_get_class (method, unbox_any_token, generic_context);
  8157| 				CHECK_TYPELOAD (unbox_klass);
  8158| 				if (klass == unbox_klass) {
  8159| 					next_ip = ip;
  8160| 					*sp++ = val;
  8161| 					break;
  8162| 				}
  8163| 			}
  8164| 			guint32 gettype_token;
  8165| 			if ((ip = il_read_call(next_ip, end, &gettype_token)) && ip_in_bb (cfg, cfg->cbb, ip)) {
  8166| 				MonoMethod* gettype_method = mini_get_method (cfg, method, gettype_token, NULL, generic_context);
  8167| 				if (!strcmp (gettype_method->name, "GetType") && gettype_method->klass == mono_defaults.object_class) {
  8168| 					mono_class_init_internal(klass);
  8169| 					if (mono_class_get_checked (m_class_get_image (klass), m_class_get_type_token (klass), error) == klass) {
  8170| 						if (cfg->compile_aot) {
  8171| 							EMIT_NEW_TYPE_FROM_HANDLE_CONST (cfg, ins, m_class_get_image (klass), m_class_get_type_token (klass), generic_context);
  8172| 						} else {
  8173| 							MonoType *klass_type = m_class_get_byval_arg (klass);
  8174| 							MonoReflectionType* reflection_type = mono_type_get_object_checked (klass_type, cfg->error);
  8175| 							EMIT_NEW_PCONST (cfg, ins, reflection_type);
  8176| 						}
  8177| 						ins->type = STACK_OBJ;
  8178| 						ins->klass = mono_defaults.systemtype_class;
  8179| 						*sp++ = ins;
  8180| 						next_ip = ip;
  8181| 						break;
  8182| 					}
  8183| 				}
  8184| 			}
  8185| 			guchar* ldnull_ip;
  8186| 			if ((ldnull_ip = il_read_op (next_ip, end, CEE_LDNULL, MONO_CEE_LDNULL)) && ip_in_bb (cfg, cfg->cbb, ldnull_ip)) {
  8187| 				gboolean is_eq = FALSE, is_neq = FALSE;
  8188| 				if ((ip = il_read_op (ldnull_ip, end, CEE_PREFIX1, MONO_CEE_CEQ)))
  8189| 					is_eq = TRUE;
  8190| 				else if ((ip = il_read_op (ldnull_ip, end, CEE_PREFIX1, MONO_CEE_CGT_UN)))
  8191| 					is_neq = TRUE;
  8192| 				if ((is_eq || is_neq) && ip_in_bb (cfg, cfg->cbb, ip) &&
  8193| 					!mono_class_is_nullable (klass) && !mini_is_gsharedvt_klass (klass)) {
  8194| 					next_ip = ip;
  8195| 					il_op = (MonoOpcodeEnum) (is_eq ? CEE_LDC_I4_0 : CEE_LDC_I4_1);
  8196| 					EMIT_NEW_ICONST (cfg, ins, is_eq ? 0 : 1);
  8197| 					ins->type = STACK_I4;
  8198| 					*sp++ = ins;
  8199| 					break;
  8200| 				}
  8201| 			}
  8202| 			guint32 isinst_tk = 0;
  8203| 			if ((ip = il_read_op_and_token (next_ip, end, CEE_ISINST, MONO_CEE_ISINST, &isinst_tk)) &&
  8204| 				ip_in_bb (cfg, cfg->cbb, ip)) {
  8205| 				MonoClass *isinst_class = mini_get_class (method, isinst_tk, generic_context);
  8206| 				if (!mono_class_is_nullable (klass) && !mono_class_is_nullable (isinst_class) &&
  8207| 					!mini_is_gsharedvt_variable_klass (klass) && !mini_is_gsharedvt_variable_klass (isinst_class) &&
  8208| 					!mono_class_is_open_constructed_type (m_class_get_byval_arg (klass)) &&
  8209| 					!mono_class_is_open_constructed_type (m_class_get_byval_arg (isinst_class))) {
  8210| 					guchar* br_ip = NULL;
  8211| 					if ((br_ip = il_read_brtrue (ip, end, &target)) || (br_ip = il_read_brtrue_s (ip, end, &target)) ||
  8212| 						(br_ip = il_read_brfalse (ip, end, &target)) || (br_ip = il_read_brfalse_s (ip, end, &target))) {
  8213| 						gboolean isinst = mono_class_is_assignable_from_internal (isinst_class, klass);
  8214| 						next_ip = ip;
  8215| 						il_op = (MonoOpcodeEnum) (isinst ? CEE_LDC_I4_1 : CEE_LDC_I4_0);
  8216| 						EMIT_NEW_ICONST (cfg, ins, isinst ? 1 : 0);
  8217| 						ins->type = STACK_I4;
  8218| 						*sp++ = ins;
  8219| 						break;
  8220| 					}
  8221| 					ldnull_ip = NULL;
  8222| 					if ((ldnull_ip = il_read_op (ip, end, CEE_LDNULL, MONO_CEE_LDNULL)) && ip_in_bb (cfg, cfg->cbb, ldnull_ip)) {
  8223| 						gboolean is_eq = FALSE, is_neq = FALSE;
  8224| 						if ((ip = il_read_op (ldnull_ip, end, CEE_PREFIX1, MONO_CEE_CEQ)))
  8225| 							is_eq = TRUE;
  8226| 						else if ((ip = il_read_op (ldnull_ip, end, CEE_PREFIX1, MONO_CEE_CGT_UN)))
  8227| 							is_neq = TRUE;
  8228| 						if ((is_eq || is_neq) && ip_in_bb (cfg, cfg->cbb, ip) &&
  8229| 							!mono_class_is_nullable (klass) && !mini_is_gsharedvt_klass (klass)) {
  8230| 							gboolean isinst = mono_class_is_assignable_from_internal (isinst_class, klass);
  8231| 							next_ip = ip;
  8232| 							if (is_eq)
  8233| 								isinst = !isinst;
  8234| 							il_op = (MonoOpcodeEnum) (isinst ? CEE_LDC_I4_1 : CEE_LDC_I4_0);
  8235| 							EMIT_NEW_ICONST (cfg, ins, isinst ? 1 : 0);
  8236| 							ins->type = STACK_I4;
  8237| 							*sp++ = ins;
  8238| 							break;
  8239| 						}
  8240| 					}
  8241| 					guchar* unbox_ip = NULL;
  8242| 					guint32 unbox_token = 0;
  8243| 					if ((unbox_ip = il_read_unbox_any (ip, end, &unbox_token)) && ip_in_bb (cfg, cfg->cbb, unbox_ip)) {
  8244| 						MonoClass *unbox_klass = mini_get_class (method, unbox_token, generic_context);
  8245| 						CHECK_TYPELOAD (unbox_klass);
  8246| 						if (!mono_class_is_nullable (unbox_klass) &&
  8247| 							!mini_is_gsharedvt_klass (unbox_klass) &&
  8248| 							klass == isinst_class &&
  8249| 							klass == unbox_klass)
  8250| 						{
  8251| 							*sp++ = val;
  8252| 							next_ip = unbox_ip;
  8253| 							break;
  8254| 						}
  8255| 					}
  8256| 				}
  8257| 			}
  8258| 			guint32 callvirt_proc_token;
  8259| 			if (!((cfg->compile_aot || cfg->compile_llvm) && !mono_class_is_def(klass)) && // we cannot devirtualize in AOT when using generics
  8260| 				next_ip < end &&
  8261| 				il_read_callvirt (next_ip, end, &callvirt_proc_token) &&
  8262| 				ip_in_bb (cfg, cfg->cbb, next_ip) ) {
  8263| 				MonoMethod* iface_method;
  8264| 				MonoMethodSignature* iface_method_sig;
  8265| 				if (val &&
  8266| 					val->flags != MONO_INST_FAULT && // not null
  8267| 					!mono_class_is_nullable (klass) &&
  8268| 					!mini_is_gsharedvt_klass (klass) &&
  8269| 					(iface_method = mini_get_method (cfg, method, callvirt_proc_token, NULL, generic_context)) &&
  8270| 					(iface_method_sig = mono_method_signature_internal (iface_method)) && // callee signture is healthy
  8271| 					iface_method_sig->hasthis && 
  8272| 					iface_method_sig->param_count == 0 && // the callee has no args (other than this)
  8273| 					!iface_method_sig->has_type_parameters &&
  8274| 					iface_method_sig->generic_param_count == 0) { // and no type params, apparently virtual generic methods require special handling
  8275| 					if (!m_class_is_inited (iface_method->klass)) {
  8276| 						if (!mono_class_init_internal (iface_method->klass))
  8277| 							TYPE_LOAD_ERROR (iface_method->klass);
  8278| 					}
  8279| 					ERROR_DECL (struct_method_error);
  8280| 					MonoMethod* struct_method = mono_class_get_virtual_method (klass, iface_method, struct_method_error);
  8281| 					if (is_ok (struct_method_error)) {
  8282| 						MonoMethodSignature* struct_method_sig = mono_method_signature_internal (struct_method);
  8283| 						if (!struct_method ||
  8284| 							!MONO_METHOD_IS_FINAL (struct_method) ||
  8285| 							!struct_method_sig ||
  8286| 							struct_method_sig->has_type_parameters ||
  8287| 							!mono_method_can_access_method (method, struct_method)) {
  8288| 						} else if (val->opcode == OP_TYPED_OBJREF) {
  8289| 							*sp++ = val;
  8290| 							cmethod_override = struct_method;
  8291| 							break;
  8292| 						} else {
  8293| 							MonoInst* srcvar = get_vreg_to_inst (cfg, val->dreg);
  8294| 							if (!srcvar)
  8295| 								srcvar = mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (klass), OP_LOCAL, val->dreg);
  8296| 							EMIT_NEW_VARLOADA (cfg, ins, srcvar, m_class_get_byval_arg (klass));
  8297| 							*sp++= ins;
  8298| 							cmethod_override = struct_method;
  8299| 							break;
  8300| 						}
  8301| 					} else {
  8302| 						mono_error_cleanup (struct_method_error);
  8303| 					}
  8304| 				} 
  8305| 			}			
  8306| 			gboolean is_true;
  8307| 			if (!mono_class_is_nullable (klass) &&
  8308| 				!mini_is_gsharedvt_klass (klass) &&
  8309| 				next_ip < end && ip_in_bb (cfg, cfg->cbb, next_ip) &&
  8310| 				( (is_true = !!(ip = il_read_brtrue   (next_ip, end, &target))) ||
  8311| 				  (is_true = !!(ip = il_read_brtrue_s (next_ip, end, &target))) ||
  8312| 					       (ip = il_read_brfalse  (next_ip, end, &target))  ||
  8313| 					       (ip = il_read_brfalse_s (next_ip, end, &target)))) {
  8314| 				int dreg;
  8315| 				MonoBasicBlock *true_bb, *false_bb;
  8316| 				il_op = (MonoOpcodeEnum)next_ip [0];
  8317| 				next_ip = ip;
  8318| 				if (cfg->verbose_level > 3) {
  8319| 					printf ("converting (in B%d: stack: %d) %s", cfg->cbb->block_num, GPTRDIFF_TO_INT (sp - stack_start), mono_disasm_code_one (NULL, method, ip, NULL));
  8320| 					printf ("<box+brtrue opt>\n");
  8321| 				}
  8322| 				/*
  8323| 				 * We need to link both bblocks, since it is needed for handling stack
  8324| 				 * arguments correctly (See test_0_box_brtrue_opt_regress_81102).
  8325| 				 * Branching to only one of them would lead to inconsistencies, so
  8326| 				 * generate an ICONST+BRTRUE, the branch opts will get rid of them.
  8327| 				 */
  8328| 				GET_BBLOCK (cfg, true_bb, target);
  8329| 				GET_BBLOCK (cfg, false_bb, next_ip);
  8330| 				mono_link_bblock (cfg, cfg->cbb, true_bb);
  8331| 				mono_link_bblock (cfg, cfg->cbb, false_bb);
  8332| 				if (sp != stack_start) {
  8333| 					handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start));
  8334| 					sp = stack_start;
  8335| 					CHECK_UNVERIFIABLE (cfg);
  8336| 				}
  8337| 				if (COMPILE_LLVM (cfg)) {
  8338| 					dreg = alloc_ireg (cfg);
  8339| 					MONO_EMIT_NEW_ICONST (cfg, dreg, 0);
  8340| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, dreg, is_true ? 0 : 1);
  8341| 					MONO_EMIT_NEW_BRANCH_BLOCK2 (cfg, OP_IBEQ, true_bb, false_bb);
  8342| 				} else {
  8343| 					/* The JIT can't eliminate the iconst+compare */
  8344| 					MONO_INST_NEW (cfg, ins, OP_BR);
  8345| 					ins->inst_target_bb = is_true ? true_bb : false_bb;
  8346| 					MONO_ADD_INS (cfg->cbb, ins);
  8347| 				}
  8348| 				start_new_bblock = 1;
  8349| 				break;
  8350| 			}
  8351| 			if (m_class_is_enumtype (klass) && !mini_is_gsharedvt_klass (klass) && !(val->type == STACK_I8 && TARGET_SIZEOF_VOID_P == 4)) {
  8352| 				/* Can't do this with 64 bit enums on 32 bit since the vtype decomp pass is ran after the long decomp pass */
  8353| 				if (val->opcode == OP_ICONST) {
  8354| 					MONO_INST_NEW (cfg, ins, OP_BOX_ICONST);
  8355| 					ins->type = STACK_OBJ;
  8356| 					ins->klass = klass;
  8357| 					ins->inst_c0 = val->inst_c0;
  8358| 					ins->dreg = alloc_dreg (cfg, (MonoStackType)val->type);
  8359| 				} else {
  8360| 					MONO_INST_NEW (cfg, ins, OP_BOX);
  8361| 					ins->type = STACK_OBJ;
  8362| 					ins->klass = klass;
  8363| 					ins->sreg1 = val->dreg;
  8364| 					ins->dreg = alloc_dreg (cfg, (MonoStackType)val->type);
  8365| 				}
  8366| 				MONO_ADD_INS (cfg->cbb, ins);
  8367| 				*sp++ = ins;
  8368| 			} else {
  8369| 				*sp++ = mini_emit_box (cfg, val, klass, context_used);
  8370| 			}
  8371| 			CHECK_CFG_EXCEPTION;
  8372| 			inline_costs += 1;
  8373| 			break;
  8374| 		}
  8375| 		case MONO_CEE_UNBOX: {
  8376| 			--sp;
  8377| 			klass = mini_get_class (method, token, generic_context);
  8378| 			CHECK_TYPELOAD (klass);
  8379| 			mono_save_token_info (cfg, image, token, klass);
  8380| 			context_used = mini_class_check_context_used (cfg, klass);
  8381| 			if (mono_class_is_nullable (klass)) {
  8382| 				MonoInst *val;
  8383| 				val = handle_unbox_nullable (cfg, *sp, klass, context_used);
  8384| 				EMIT_NEW_VARLOADA (cfg, ins, get_vreg_to_inst (cfg, val->dreg), m_class_get_byval_arg (val->klass));
  8385| 				*sp++= ins;
  8386| 			} else {
  8387| 				ins = mini_handle_unbox (cfg, klass, *sp, context_used);
  8388| 				*sp++ = ins;
  8389| 			}
  8390| 			inline_costs += 2;
  8391| 			break;
  8392| 		}
  8393| 		case MONO_CEE_LDFLD:
  8394| 		case MONO_CEE_LDFLDA:
  8395| 		case MONO_CEE_STFLD:
  8396| 		case MONO_CEE_LDSFLD:
  8397| 		case MONO_CEE_LDSFLDA:
  8398| 		case MONO_CEE_STSFLD: {
  8399| 			MonoClassField *field;
  8400| 			guint foffset;
  8401| 			gboolean is_instance;
  8402| 			gpointer addr = NULL;
  8403| 			gboolean is_special_static;
  8404| 			MonoType *ftype;
  8405| 			MonoInst *store_val = NULL;
  8406| 			MonoInst *thread_ins;
  8407| 			is_instance = (il_op == MONO_CEE_LDFLD || il_op == MONO_CEE_LDFLDA || il_op == MONO_CEE_STFLD);
  8408| 			if (is_instance) {
  8409| 				if (il_op == MONO_CEE_STFLD) {
  8410| 					sp -= 2;
  8411| 					store_val = sp [1];
  8412| 				} else {
  8413| 					--sp;
  8414| 				}
  8415| 				if (sp [0]->type == STACK_I4 || sp [0]->type == STACK_I8 || sp [0]->type == STACK_R8)
  8416| 					UNVERIFIED;
  8417| 				if (il_op != MONO_CEE_LDFLD && sp [0]->type == STACK_VTYPE)
  8418| 					UNVERIFIED;
  8419| 			} else {
  8420| 				if (il_op == MONO_CEE_STSFLD) {
  8421| 					sp--;
  8422| 					store_val = sp [0];
  8423| 				}
  8424| 			}
  8425| 			if (method->wrapper_type != MONO_WRAPPER_NONE) {
  8426| 				field = (MonoClassField *)mono_method_get_wrapper_data (method, token);
  8427| 				klass = m_field_get_parent (field);
  8428| 			}
  8429| 			else {
  8430| 				klass = NULL;
  8431| 				field = mono_field_from_token_checked (image, token, &klass, generic_context, cfg->error);
  8432| 				if (!field)
  8433| 					CHECK_TYPELOAD (klass);
  8434| 				CHECK_CFG_ERROR;
  8435| 			}
  8436| 			if (!dont_verify && !cfg->skip_visibility && !mono_method_can_access_field (method, field))
  8437| 				FIELD_ACCESS_FAILURE (method, field);
  8438| 			mono_class_init_internal (klass);
  8439| 			mono_class_setup_fields (klass);
  8440| 			ftype = mono_field_get_type_internal (field);
  8441| 			/*
  8442| 			 * LDFLD etc. is usable on static fields as well, so convert those cases to
  8443| 			 * the static case.
  8444| 			 */
  8445| 			if (is_instance && ftype->attrs & FIELD_ATTRIBUTE_STATIC) {
  8446| 				switch (il_op) {
  8447| 				case MONO_CEE_LDFLD:
  8448| 					il_op = MONO_CEE_LDSFLD;
  8449| 					break;
  8450| 				case MONO_CEE_STFLD:
  8451| 					il_op = MONO_CEE_STSFLD;
  8452| 					break;
  8453| 				case MONO_CEE_LDFLDA:
  8454| 					il_op = MONO_CEE_LDSFLDA;
  8455| 					break;
  8456| 				default:
  8457| 					g_assert_not_reached ();
  8458| 				}
  8459| 				is_instance = FALSE;
  8460| 			}
  8461| 			context_used = mini_class_check_context_used (cfg, klass);
  8462| 			if (il_op == MONO_CEE_LDSFLD) {
  8463| 				ins = mini_emit_inst_for_field_load (cfg, field);
  8464| 				if (ins) {
  8465| 					*sp++ = ins;
  8466| 					goto field_access_end;
  8467| 				}
  8468| 			}
  8469| 			/* INSTANCE CASE */
  8470| 			if (is_instance)
  8471| 				g_assert (field->offset);
  8472| 			/* metadata-update: no hot reload in the JIT.  But if it was supported,
  8473| 			 * field->offset here could be wrong for added (m_field_is_from_update)
  8474| 			 * fields */
  8475| 			foffset = m_class_is_valuetype (klass) ? field->offset - MONO_ABI_SIZEOF (MonoObject): field->offset;
  8476| 			if (il_op == MONO_CEE_STFLD) {
  8477| 				sp [1] = convert_value (cfg, field->type, sp [1]);
  8478| 				if (target_type_is_incompatible (cfg, field->type, sp [1]))
  8479| 					UNVERIFIED;
  8480| 				{
  8481| 					MonoInst *store;
  8482| 					MONO_EMIT_NULL_CHECK (cfg, sp [0]->dreg, foffset > mono_target_pagesize ());
  8483| 					if (ins_flag & MONO_INST_VOLATILE) {
  8484| 						/* Volatile stores have release semantics, see 12.6.7 in Ecma 335 */
  8485| 						mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
  8486| 					}
  8487| 					if (mini_is_gsharedvt_klass (klass)) {
  8488| 						MonoInst *offset_ins;
  8489| 						context_used = mini_class_check_context_used (cfg, klass);
  8490| 						offset_ins = emit_get_gsharedvt_info (cfg, field, MONO_RGCTX_INFO_FIELD_OFFSET);
  8491| 						/* The value is offset by 1 */
  8492| 						EMIT_NEW_BIALU_IMM (cfg, ins, OP_PSUB_IMM, offset_ins->dreg, offset_ins->dreg, 1);
  8493| 						int dreg = alloc_ireg_mp (cfg);
  8494| 						EMIT_NEW_BIALU (cfg, ins, OP_PADD, dreg, sp [0]->dreg, offset_ins->dreg);
  8495| 						if (cfg->gen_write_barriers && mini_type_to_stind (cfg, field->type) == CEE_STIND_REF && !MONO_INS_IS_PCONST_NULL (sp [1])) {
  8496| 							store = mini_emit_storing_write_barrier (cfg, ins, sp [1]);
  8497| 						} else {
  8498| 							/* The decomposition will call mini_emit_memory_copy () which will emit a wbarrier if needed */
  8499| 							EMIT_NEW_STORE_MEMBASE_TYPE (cfg, store, field->type, dreg, 0, sp [1]->dreg);
  8500| 						}
  8501| 					} else {
  8502| 						if (cfg->gen_write_barriers && mini_type_to_stind (cfg, field->type) == CEE_STIND_REF && !MONO_INS_IS_PCONST_NULL (sp [1])) {
  8503| 							/* insert call to write barrier */
  8504| 							MonoInst *ptr;
  8505| 							int dreg;
  8506| 							dreg = alloc_ireg_mp (cfg);
  8507| 							EMIT_NEW_BIALU_IMM (cfg, ptr, OP_PADD_IMM, dreg, sp [0]->dreg, foffset);
  8508| 							store = mini_emit_storing_write_barrier (cfg, ptr, sp [1]);
  8509| 						} else {
  8510| 							if (MONO_TYPE_ISSTRUCT (field->type))
  8511| 								/* The decomposition might end up calling a copy/wbarrier function which doesn't do null checks */
  8512| 								MONO_EMIT_EXPLICIT_NULL_CHECK (cfg, sp [0]->dreg);
  8513| 							EMIT_NEW_STORE_MEMBASE_TYPE (cfg, store, field->type, sp [0]->dreg, foffset, sp [1]->dreg);
  8514| 						}
  8515| 					}
  8516| 					if (sp [0]->opcode != OP_LDADDR)
  8517| 						store->flags |= MONO_INST_FAULT;
  8518| 					store->flags |= ins_flag;
  8519| 				}
  8520| 				goto field_access_end;
  8521| 			}
  8522| 			if (is_instance) {
  8523| 				if (sp [0]->type == STACK_VTYPE) {
  8524| 					MonoInst *var;
  8525| 					/* Have to compute the address of the variable */
  8526| 					var = get_vreg_to_inst (cfg, sp [0]->dreg);
  8527| 					if (!var)
  8528| 						var = mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (klass), OP_LOCAL, sp [0]->dreg);
  8529| 					else
  8530| 						g_assert (var->klass == klass);
  8531| 					EMIT_NEW_VARLOADA (cfg, ins, var, m_class_get_byval_arg (var->klass));
  8532| 					sp [0] = ins;
  8533| 				}
  8534| 				if (il_op == MONO_CEE_LDFLDA) {
  8535| 					if (sp [0]->type == STACK_OBJ || sp [0]->type == STACK_PTR) {
  8536| 						MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, sp [0]->dreg, 0);
  8537| 						MONO_EMIT_NEW_COND_EXC (cfg, EQ, "NullReferenceException");
  8538| 					}
  8539| 					int dreg = alloc_ireg_mp (cfg);
  8540| 					if (mini_is_gsharedvt_klass (klass)) {
  8541| 						MonoInst *offset_ins;
  8542| 						offset_ins = emit_get_gsharedvt_info (cfg, field, MONO_RGCTX_INFO_FIELD_OFFSET);
  8543| 						/* The value is offset by 1 */
  8544| 						EMIT_NEW_BIALU_IMM (cfg, ins, OP_PSUB_IMM, offset_ins->dreg, offset_ins->dreg, 1);
  8545| 						EMIT_NEW_BIALU (cfg, ins, OP_PADD, dreg, sp [0]->dreg, offset_ins->dreg);
  8546| 					} else {
  8547| 						EMIT_NEW_BIALU_IMM (cfg, ins, OP_PADD_IMM, dreg, sp [0]->dreg, foffset);
  8548| 					}
  8549| 					ins->klass = mono_class_from_mono_type_internal (field->type);
  8550| 					ins->type = STACK_MP;
  8551| 					*sp++ = ins;
  8552| 				} else {
  8553| 					MonoInst *load;
  8554| 					MONO_EMIT_NULL_CHECK (cfg, sp [0]->dreg, foffset > mono_target_pagesize ());
  8555| #ifdef MONO_ARCH_SIMD_INTRINSICS
  8556| 					if (sp [0]->opcode == OP_LDADDR && m_class_is_simd_type (klass) && cfg->opt & MONO_OPT_SIMD) {
  8557| 						ins = mono_emit_simd_field_load (cfg, field, sp [0]);
  8558| 						if (ins) {
  8559| 							*sp++ = ins;
  8560| 							goto field_access_end;
  8561| 						}
  8562| 					}
  8563| #endif
  8564| 					MonoInst *field_add_inst = sp [0];
  8565| 					if (mini_is_gsharedvt_klass (klass)) {
  8566| 						MonoInst *offset_ins;
  8567| 						offset_ins = emit_get_gsharedvt_info (cfg, field, MONO_RGCTX_INFO_FIELD_OFFSET);
  8568| 						/* The value is offset by 1 */
  8569| 						EMIT_NEW_BIALU_IMM (cfg, ins, OP_PSUB_IMM, offset_ins->dreg, offset_ins->dreg, 1);
  8570| 						EMIT_NEW_BIALU (cfg, field_add_inst, OP_PADD, alloc_ireg_mp (cfg), sp [0]->dreg, offset_ins->dreg);
  8571| 						foffset = 0;
  8572| 					}
  8573| 					load = mini_emit_memory_load (cfg, field->type, field_add_inst, foffset, ins_flag);
  8574| 					if (sp [0]->opcode != OP_LDADDR)
  8575| 						load->flags |= MONO_INST_FAULT;
  8576| 					*sp++ = load;
  8577| 				}
  8578| 			}
  8579| 			if (is_instance)
  8580| 				goto field_access_end;
  8581| 			/* STATIC CASE */
  8582| 			context_used = mini_class_check_context_used (cfg, klass);
  8583| 			if (ftype->attrs & FIELD_ATTRIBUTE_LITERAL) {
  8584| 				mono_error_set_field_missing (cfg->error, m_field_get_parent (field), field->name, NULL, "Using static instructions with literal field");
  8585| 				CHECK_CFG_ERROR;
  8586| 			}
  8587| 			/* The special_static_fields field is init'd in mono_class_vtable, so it needs
  8588| 			 * to be called here.
  8589| 			 */
  8590| 			if (!context_used) {
  8591| 				mono_class_vtable_checked (klass, cfg->error);
  8592| 				CHECK_CFG_ERROR;
  8593| 				CHECK_TYPELOAD (klass);
  8594| 			}
  8595| 			is_special_static = mono_class_field_is_special_static (field);
  8596| 			if (is_special_static) {
  8597| 				addr = mono_special_static_field_get_offset (field, cfg->error);
  8598| 				CHECK_CFG_ERROR;
  8599| 				CHECK_TYPELOAD (klass);
  8600| 			} else {
  8601| 				addr = NULL;
  8602| 			}
  8603| 			if (is_special_static && ((gsize)addr & 0x80000000) == 0)
  8604| 				thread_ins = mono_create_tls_get (cfg, TLS_KEY_THREAD);
  8605| 			else
  8606| 				thread_ins = NULL;
  8607| 			/* Generate IR to compute the field address */
  8608| 			if (is_special_static && ((gsize)addr & 0x80000000) == 0 && thread_ins &&
  8609| 				!(context_used && cfg->gsharedvt && mini_is_gsharedvt_klass (klass))) {
  8610| 				/*
  8611| 				 * Fast access to TLS data
  8612| 				 * Inline version of get_thread_static_data () in
  8613| 				 * threads.c.
  8614| 				 */
  8615| 				guint32 offset;
  8616| 				int idx, static_data_reg, array_reg, dreg;
  8617| 				static_data_reg = alloc_ireg (cfg);
  8618| 				MONO_EMIT_NEW_LOAD_MEMBASE (cfg, static_data_reg, thread_ins->dreg, MONO_STRUCT_OFFSET (MonoInternalThread, static_data));
  8619| 				if (cfg->compile_aot || context_used) {
  8620| 					int offset_reg, offset2_reg, idx_reg;
  8621| 					/* For TLS variables, this will return the TLS offset */
  8622| 					if (context_used) {
  8623| 						MonoInst *addr_ins = emit_get_rgctx_field (cfg, context_used, field, MONO_RGCTX_INFO_FIELD_OFFSET);
  8624| 						/* The value is offset by 1 */
  8625| 						EMIT_NEW_BIALU_IMM (cfg, ins, OP_PSUB_IMM, addr_ins->dreg, addr_ins->dreg, 1);
  8626| 					} else {
  8627| 						EMIT_NEW_SFLDACONST (cfg, ins, field);
  8628| 					}
  8629| 					offset_reg = ins->dreg;
  8630| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_IAND_IMM, offset_reg, offset_reg, 0x7fffffff);
  8631| 					idx_reg = alloc_ireg (cfg);
  8632| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_IAND_IMM, idx_reg, offset_reg, 0x3f);
  8633| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_ISHL_IMM, idx_reg, idx_reg, TARGET_SIZEOF_VOID_P == 8 ? 3 : 2);
  8634| 					MONO_EMIT_NEW_BIALU (cfg, OP_PADD, static_data_reg, static_data_reg, idx_reg);
  8635| 					array_reg = alloc_ireg (cfg);
  8636| 					MONO_EMIT_NEW_LOAD_MEMBASE (cfg, array_reg, static_data_reg, 0);
  8637| 					offset2_reg = alloc_ireg (cfg);
  8638| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_ISHR_UN_IMM, offset2_reg, offset_reg, 6);
  8639| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_IAND_IMM, offset2_reg, offset2_reg, 0x1ffffff);
  8640| 					dreg = alloc_ireg (cfg);
  8641| 					EMIT_NEW_BIALU (cfg, ins, OP_PADD, dreg, array_reg, offset2_reg);
  8642| 				} else {
  8643| 					offset = (gsize)addr & 0x7fffffff;
  8644| 					idx = offset & 0x3f;
  8645| 					array_reg = alloc_ireg (cfg);
  8646| 					MONO_EMIT_NEW_LOAD_MEMBASE (cfg, array_reg, static_data_reg, idx * TARGET_SIZEOF_VOID_P);
  8647| 					dreg = alloc_ireg (cfg);
  8648| 					EMIT_NEW_BIALU_IMM (cfg, ins, OP_ADD_IMM, dreg, array_reg, ((offset >> 6) & 0x1ffffff));
  8649| 				}
  8650| 			} else if ((cfg->compile_aot && is_special_static) ||
  8651| 					(context_used && is_special_static)) {
  8652| 				MonoInst *iargs [1];
  8653| 				g_assert (m_field_get_parent (field));
  8654| 				if (context_used) {
  8655| 					iargs [0] = emit_get_rgctx_field (cfg, context_used,
  8656| 						field, MONO_RGCTX_INFO_CLASS_FIELD);
  8657| 				} else {
  8658| 					EMIT_NEW_FIELDCONST (cfg, iargs [0], field);
  8659| 				}
  8660| 				ins = mono_emit_jit_icall (cfg, mono_class_static_field_address, iargs);
  8661| 			} else if (context_used) {
  8662| 				MonoInst *static_data;
  8663| 				/*
  8664| 				g_print ("sharing static field access in %s.%s.%s - depth %d offset %d\n",
  8665| 					method->klass->name_space, method->klass->name, method->name,
  8666| 					depth, field->offset);
  8667| 				*/
  8668| 				if (mono_class_needs_cctor_run (klass, method))
  8669| 					emit_class_init (cfg, klass, TRUE);
  8670| 				/*
  8671| 				 * The pointer we're computing here is
  8672| 				 *
  8673| 				 *   super_info.static_data + field->offset
  8674| 				 */
  8675| 				static_data = mini_emit_get_rgctx_klass (cfg, context_used,
  8676| 					klass, MONO_RGCTX_INFO_STATIC_DATA);
  8677| 				if (mini_is_gsharedvt_klass (klass)) {
  8678| 					MonoInst *offset_ins;
  8679| 					offset_ins = emit_get_rgctx_field (cfg, context_used, field, MONO_RGCTX_INFO_FIELD_OFFSET);
  8680| 					/* The value is offset by 1 */
  8681| 					EMIT_NEW_BIALU_IMM (cfg, ins, OP_PSUB_IMM, offset_ins->dreg, offset_ins->dreg, 1);
  8682| 					int dreg = alloc_ireg_mp (cfg);
  8683| 					EMIT_NEW_BIALU (cfg, ins, OP_PADD, dreg, static_data->dreg, offset_ins->dreg);
  8684| 				} else if (field->offset == 0) {
  8685| 					ins = static_data;
  8686| 				} else {
  8687| 					int addr_reg = mono_alloc_preg (cfg);
  8688| 					EMIT_NEW_BIALU_IMM (cfg, ins, OP_PADD_IMM, addr_reg, static_data->dreg, field->offset);
  8689| 				}
  8690| 			} else if (cfg->compile_aot && addr) {
  8691| 				MonoInst *iargs [1];
  8692| 				g_assert (m_field_get_parent (field));
  8693| 				EMIT_NEW_FIELDCONST (cfg, iargs [0], field);
  8694| 				ins = mono_emit_jit_icall (cfg, mono_class_static_field_address, iargs);
  8695| 			} else {
  8696| 				MonoVTable *vtable = NULL;
  8697| 				if (!cfg->compile_aot)
  8698| 					vtable = mono_class_vtable_checked (klass, cfg->error);
  8699| 				CHECK_CFG_ERROR;
  8700| 				CHECK_TYPELOAD (klass);
  8701| 				if (!addr) {
  8702| 					if (mini_field_access_needs_cctor_run (cfg, method, klass, vtable)) {
  8703| 						if (!(g_slist_find (class_inits, klass))) {
  8704| 							emit_class_init (cfg, klass, TRUE);
  8705| 							if (cfg->verbose_level > 2)
  8706| 								printf ("class %s.%s needs init call for %s\n", m_class_get_name_space (klass), m_class_get_name (klass), mono_field_get_name (field));
  8707| 							class_inits = g_slist_prepend (class_inits, klass);
  8708| 						}
  8709| 					} else {
  8710| 						if (cfg->run_cctors) {
  8711| 							/* This makes so that inline cannot trigger */
  8712| 							/* .cctors: too many apps depend on them */
  8713| 							/* running with a specific order... */
  8714| 							g_assert (vtable);
  8715| 							if (!vtable->initialized && m_class_has_cctor (vtable->klass))
  8716| 								INLINE_FAILURE ("class init");
  8717| 							if (!mono_runtime_class_init_full (vtable, cfg->error)) {
  8718| 								mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
  8719| 								goto exception_exit;
  8720| 							}
  8721| 						}
  8722| 					}
  8723| 					if (cfg->compile_aot)
  8724| 						EMIT_NEW_SFLDACONST (cfg, ins, field);
  8725| 					else {
  8726| 						g_assert (vtable);
  8727| 						addr = mono_static_field_get_addr (vtable, field);
  8728| 						g_assert (addr);
  8729| 						EMIT_NEW_PCONST (cfg, ins, addr);
  8730| 					}
  8731| 				} else {
  8732| 					MonoInst *iargs [1];
  8733| 					EMIT_NEW_ICONST (cfg, iargs [0], GPOINTER_TO_UINT (addr));
  8734| 					ins = mono_emit_jit_icall (cfg, mono_get_special_static_data, iargs);
  8735| 				}
  8736| 			}
  8737| 			/* Generate IR to do the actual load/store operation */
  8738| 			if ((il_op == MONO_CEE_STFLD || il_op == MONO_CEE_STSFLD)) {
  8739| 				if (ins_flag & MONO_INST_VOLATILE) {
  8740| 					/* Volatile stores have release semantics, see 12.6.7 in Ecma 335 */
  8741| 					mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
  8742| 				} else if (!mini_debug_options.weak_memory_model && mini_type_is_reference (ftype)) {
  8743| 					mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
  8744| 				}
  8745| 			}
  8746| 			if (il_op == MONO_CEE_LDSFLDA) {
  8747| 				ins->klass = mono_class_from_mono_type_internal (ftype);
  8748| 				ins->type = STACK_PTR;
  8749| 				*sp++ = ins;
  8750| 			} else if (il_op == MONO_CEE_STSFLD) {
  8751| 				MonoInst *store;
  8752| 				if (m_class_get_mem_manager (m_field_get_parent (field))->collectible && (mini_type_is_reference (ftype) || m_class_has_references (mono_class_from_mono_type_internal (ftype)))) {
  8753| 					/* These are stored on the GC heap, so they need GC barriers */
  8754| 					mini_emit_memory_store (cfg, ftype, ins, store_val, 0);
  8755| 				} else {
  8756| 					EMIT_NEW_STORE_MEMBASE_TYPE (cfg, store, ftype, ins->dreg, 0, store_val->dreg);
  8757| 					store->flags |= ins_flag;
  8758| 				}
  8759| 			} else {
  8760| 				gboolean is_const = FALSE;
  8761| 				MonoVTable *vtable = NULL;
  8762| 				addr = NULL;
  8763| 				if (!context_used) {
  8764| 					vtable = mono_class_vtable_checked (klass, cfg->error);
  8765| 					CHECK_CFG_ERROR;
  8766| 					CHECK_TYPELOAD (klass);
  8767| 				}
  8768| 				if ((ftype->attrs & FIELD_ATTRIBUTE_INIT_ONLY) && (((addr = mono_aot_readonly_field_override (field)) != NULL) ||
  8769| 						(!context_used && !cfg->compile_aot && vtable->initialized))) {
  8770| 					int ro_type = ftype->type;
  8771| 					if (!addr)
  8772| 						addr = mono_static_field_get_addr (vtable, field);
  8773| 					if (ro_type == MONO_TYPE_VALUETYPE && m_class_is_enumtype (ftype->data.klass)) {
  8774| 						ro_type = mono_class_enum_basetype_internal (ftype->data.klass)->type;
  8775| 					}
  8776| 					GSHAREDVT_FAILURE (il_op);
  8777| 					/* printf ("RO-FIELD %s.%s:%s\n", klass->name_space, klass->name, mono_field_get_name (field));*/
  8778| 					is_const = TRUE;
  8779| 					switch (ro_type) {
  8780| 					case MONO_TYPE_BOOLEAN:
  8781| 					case MONO_TYPE_U1:
  8782| 						EMIT_NEW_ICONST (cfg, *sp, *((guint8 *)addr));
  8783| 						sp++;
  8784| 						break;
  8785| 					case MONO_TYPE_I1:
  8786| 						EMIT_NEW_ICONST (cfg, *sp, *((gint8 *)addr));
  8787| 						sp++;
  8788| 						break;
  8789| 					case MONO_TYPE_CHAR:
  8790| 					case MONO_TYPE_U2:
  8791| 						EMIT_NEW_ICONST (cfg, *sp, *((guint16 *)addr));
  8792| 						sp++;
  8793| 						break;
  8794| 					case MONO_TYPE_I2:
  8795| 						EMIT_NEW_ICONST (cfg, *sp, *((gint16 *)addr));
  8796| 						sp++;
  8797| 						break;
  8798| 						break;
  8799| 					case MONO_TYPE_I4:
  8800| 						EMIT_NEW_ICONST (cfg, *sp, *((gint32 *)addr));
  8801| 						sp++;
  8802| 						break;
  8803| 					case MONO_TYPE_U4:
  8804| 						EMIT_NEW_ICONST (cfg, *sp, *((guint32 *)addr));
  8805| 						sp++;
  8806| 						break;
  8807| 					case MONO_TYPE_I:
  8808| 					case MONO_TYPE_U:
  8809| 					case MONO_TYPE_PTR:
  8810| 					case MONO_TYPE_FNPTR:
  8811| 						EMIT_NEW_PCONST (cfg, *sp, *((gpointer *)addr));
  8812| 						mini_type_to_eval_stack_type ((cfg), field->type, *sp);
  8813| 						sp++;
  8814| 						break;
  8815| 					case MONO_TYPE_STRING:
  8816| 					case MONO_TYPE_OBJECT:
  8817| 					case MONO_TYPE_CLASS:
  8818| 					case MONO_TYPE_SZARRAY:
  8819| 					case MONO_TYPE_ARRAY:
  8820| 						if (!mono_gc_is_moving ()) {
  8821| 							EMIT_NEW_PCONST (cfg, *sp, *((gpointer *)addr));
  8822| 							mini_type_to_eval_stack_type ((cfg), field->type, *sp);
  8823| 							sp++;
  8824| 						} else {
  8825| 							is_const = FALSE;
  8826| 						}
  8827| 						break;
  8828| 					case MONO_TYPE_I8:
  8829| 					case MONO_TYPE_U8:
  8830| 						EMIT_NEW_I8CONST (cfg, *sp, *((gint64 *)addr));
  8831| 						sp++;
  8832| 						break;
  8833| 					case MONO_TYPE_R4:
  8834| 					case MONO_TYPE_R8:
  8835| 					case MONO_TYPE_VALUETYPE:
  8836| 					default:
  8837| 						is_const = FALSE;
  8838| 						break;
  8839| 					}
  8840| 				}
  8841| 				if (!is_const) {
  8842| 					MonoInst *load;
  8843| 					EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, load, field->type, ins->dreg, 0);
  8844| 					load->flags |= ins_flag;
  8845| 					*sp++ = load;
  8846| 				}
  8847| 			}
  8848| field_access_end:
  8849| 			if ((il_op == MONO_CEE_LDFLD || il_op == MONO_CEE_LDSFLD) && (ins_flag & MONO_INST_VOLATILE)) {
  8850| 				/* Volatile loads have acquire semantics, see 12.6.7 in Ecma 335 */
  8851| 				mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_ACQ);
  8852| 			}
  8853| 			ins_flag = 0;
  8854| 			break;
  8855| 		}
  8856| 		case MONO_CEE_STOBJ:
  8857| 			sp -= 2;
  8858| 			klass = mini_get_class (method, token, generic_context);
  8859| 			CHECK_TYPELOAD (klass);
  8860| 			/* FIXME: should check item at sp [1] is compatible with the type of the store. */
  8861| 			mini_emit_memory_store (cfg, m_class_get_byval_arg (klass), sp [0], sp [1], ins_flag);
  8862| 			ins_flag = 0;
  8863| 			inline_costs += 1;
  8864| 			break;
  8865| 			/*
  8866| 			 * Array opcodes
  8867| 			 */
  8868| 		case MONO_CEE_NEWARR: {
  8869| 			MonoInst *len_ins;
  8870| 			const char *data_ptr;
  8871| 			int data_size = 0;
  8872| 			guint32 field_token;
  8873| 			--sp;
  8874| 			klass = mini_get_class (method, token, generic_context);
  8875| 			CHECK_TYPELOAD (klass);
  8876| 			if (m_class_get_byval_arg (klass)->type == MONO_TYPE_VOID)
  8877| 				UNVERIFIED;
  8878| 			context_used = mini_class_check_context_used (cfg, klass);
  8879| #ifndef TARGET_S390X
  8880| 			if (sp [0]->type == STACK_I8 && TARGET_SIZEOF_VOID_P == 4) {
  8881| 				MONO_INST_NEW (cfg, ins, OP_LCONV_TO_OVF_U4);
  8882| 				ins->sreg1 = sp [0]->dreg;
  8883| 				ins->type = STACK_I4;
  8884| 				ins->dreg = alloc_ireg (cfg);
  8885| 				MONO_ADD_INS (cfg->cbb, ins);
  8886| 				*sp = mono_decompose_opcode (cfg, ins);
  8887| 			}
  8888| #else
  8889| 			/* The array allocator expects a 64-bit input, and we cannot rely
  8890| 			   on the high bits of a 32-bit result, so we have to extend.  */
  8891| 			if (sp [0]->type == STACK_I4 && TARGET_SIZEOF_VOID_P == 8) {
  8892| 				MONO_INST_NEW (cfg, ins, OP_ICONV_TO_I8);
  8893| 				ins->sreg1 = sp [0]->dreg;
  8894| 				ins->type = STACK_I8;
  8895| 				ins->dreg = alloc_ireg (cfg);
  8896| 				MONO_ADD_INS (cfg->cbb, ins);
  8897| 				*sp = mono_decompose_opcode (cfg, ins);
  8898| 			}
  8899| #endif
  8900| 			if (context_used) {
  8901| 				MonoInst *args [3];
  8902| 				MonoClass *array_class = mono_class_create_array (klass, 1);
  8903| 				MonoMethod *managed_alloc = mono_gc_get_managed_array_allocator (array_class);
  8904| 				/* FIXME: Use OP_NEWARR and decompose later to help abcrem */
  8905| 				/* vtable */
  8906| 				args [0] = mini_emit_get_rgctx_klass (cfg, context_used,
  8907| 					array_class, MONO_RGCTX_INFO_VTABLE);
  8908| 				/* array len */
  8909| 				args [1] = sp [0];
  8910| 				if (managed_alloc)
  8911| 					ins = mono_emit_method_call (cfg, managed_alloc, args, NULL);
  8912| 				else
  8913| 					ins = mono_emit_jit_icall (cfg, ves_icall_array_new_specific, args);
  8914| 			} else {
  8915| 				/* Decompose later since it is needed by abcrem */
  8916| 				MonoClass *array_type = mono_class_create_array (klass, 1);
  8917| 				mono_class_vtable_checked (array_type, cfg->error);
  8918| 				CHECK_CFG_ERROR;
  8919| 				CHECK_TYPELOAD (array_type);
  8920| 				MONO_INST_NEW (cfg, ins, OP_NEWARR);
  8921| 				ins->dreg = alloc_ireg_ref (cfg);
  8922| 				ins->sreg1 = sp [0]->dreg;
  8923| 				ins->inst_newa_class = klass;
  8924| 				ins->type = STACK_OBJ;
  8925| 				ins->klass = array_type;
  8926| 				MONO_ADD_INS (cfg->cbb, ins);
  8927| 				cfg->flags |= MONO_CFG_NEEDS_DECOMPOSE;
  8928| 				cfg->cbb->needs_decompose = TRUE;
  8929| 				/* Needed so mono_emit_load_get_addr () gets called */
  8930| 				mono_get_got_var (cfg);
  8931| 			}
  8932| 			len_ins = sp [0];
  8933| 			ip += 5;
  8934| 			*sp++ = ins;
  8935| 			inline_costs += 1;
  8936| 			/*
  8937| 			 * we inline/optimize the initialization sequence if possible.
  8938| 			 * we should also allocate the array as not cleared, since we spend as much time clearing to 0 as initializing
  8939| 			 * for small sizes open code the memcpy
  8940| 			 * ensure the rva field is big enough
  8941| 			 */
  8942| 			if ((cfg->opt & MONO_OPT_INTRINS) && next_ip < end
  8943| 					&& ip_in_bb (cfg, cfg->cbb, next_ip)
  8944| 					&& (len_ins->opcode == OP_ICONST)
  8945| 					&& (data_ptr = initialize_array_data (cfg, method,
  8946| 						cfg->compile_aot, next_ip, end, klass,
  8947| 						GTMREG_TO_UINT32 (len_ins->inst_c0), &data_size, &field_token,
  8948| 						&il_op, &next_ip))) {
  8949| 				MonoMethod *memcpy_method = mini_get_memcpy_method ();
  8950| 				MonoInst *iargs [3];
  8951| 				int add_reg = alloc_ireg_mp (cfg);
  8952| 				EMIT_NEW_BIALU_IMM (cfg, iargs [0], OP_PADD_IMM, add_reg, ins->dreg, MONO_STRUCT_OFFSET (MonoArray, vector));
  8953| 				if (cfg->compile_aot) {
  8954| 					EMIT_NEW_AOTCONST_TOKEN (cfg, iargs [1], MONO_PATCH_INFO_RVA, m_class_get_image (method->klass), field_token, STACK_PTR, NULL);
  8955| 				} else {
  8956| 					EMIT_NEW_PCONST (cfg, iargs [1], (char*)data_ptr);
  8957| 				}
  8958| 				EMIT_NEW_ICONST (cfg, iargs [2], data_size);
  8959| 				mono_emit_method_call (cfg, memcpy_method, iargs, NULL);
  8960| 			}
  8961| 			break;
  8962| 		}
  8963| 		case MONO_CEE_LDLEN:
  8964| 			--sp;
  8965| 			if (sp [0]->type != STACK_OBJ)
  8966| 				UNVERIFIED;
  8967| 			MONO_INST_NEW (cfg, ins, OP_LDLEN);
  8968| 			ins->dreg = alloc_preg (cfg);
  8969| 			ins->sreg1 = sp [0]->dreg;
  8970| 			ins->inst_imm = MONO_STRUCT_OFFSET (MonoArray, max_length);
  8971| 			ins->type = STACK_I4;
  8972| 			/* This flag will be inherited by the decomposition */
  8973| 			ins->flags |= MONO_INST_FAULT | MONO_INST_INVARIANT_LOAD;
  8974| 			MONO_ADD_INS (cfg->cbb, ins);
  8975| 			cfg->flags |= MONO_CFG_NEEDS_DECOMPOSE;
  8976| 			cfg->cbb->needs_decompose = TRUE;
  8977| 			MONO_EMIT_NEW_UNALU (cfg, OP_NOT_NULL, -1, sp [0]->dreg);
  8978| 			*sp++ = ins;
  8979| 			break;
  8980| 		case MONO_CEE_LDELEMA:
  8981| 			sp -= 2;
  8982| 			if (sp [0]->type != STACK_OBJ)
  8983| 				UNVERIFIED;
  8984| 			cfg->flags |= MONO_CFG_HAS_LDELEMA;
  8985| 			klass = mini_get_class (method, token, generic_context);
  8986| 			CHECK_TYPELOAD (klass);
  8987| 			/* we need to make sure that this array is exactly the type it needs
  8988| 			 * to be for correctness. the wrappers are lax with their usage
  8989| 			 * so we need to ignore them here
  8990| 			 */
  8991| 			if (!m_class_is_valuetype (klass) && method->wrapper_type == MONO_WRAPPER_NONE && !readonly) {
  8992| 				MonoClass *array_class = mono_class_create_array (klass, 1);
  8993| 				mini_emit_check_array_type (cfg, sp [0], array_class);
  8994| 				CHECK_TYPELOAD (array_class);
  8995| 			}
  8996| 			readonly = FALSE;
  8997| 			ins = mini_emit_ldelema_1_ins (cfg, klass, sp [0], sp [1], TRUE, FALSE);
  8998| 			*sp++ = ins;
  8999| 			break;
  9000| 		case MONO_CEE_LDELEM:
  9001| 		case MONO_CEE_LDELEM_I1:
  9002| 		case MONO_CEE_LDELEM_U1:
  9003| 		case MONO_CEE_LDELEM_I2:
  9004| 		case MONO_CEE_LDELEM_U2:
  9005| 		case MONO_CEE_LDELEM_I4:
  9006| 		case MONO_CEE_LDELEM_U4:
  9007| 		case MONO_CEE_LDELEM_I8:
  9008| 		case MONO_CEE_LDELEM_I:
  9009| 		case MONO_CEE_LDELEM_R4:
  9010| 		case MONO_CEE_LDELEM_R8:
  9011| 		case MONO_CEE_LDELEM_REF: {
  9012| 			MonoInst *addr;
  9013| 			sp -= 2;
  9014| 			if (il_op == MONO_CEE_LDELEM) {
  9015| 				klass = mini_get_class (method, token, generic_context);
  9016| 				CHECK_TYPELOAD (klass);
  9017| 				mono_class_init_internal (klass);
  9018| 			}
  9019| 			else
  9020| 				klass = array_access_to_klass (il_op);
  9021| 			if (sp [0]->type != STACK_OBJ)
  9022| 				UNVERIFIED;
  9023| 			cfg->flags |= MONO_CFG_HAS_LDELEMA;
  9024| 			if (mini_is_gsharedvt_variable_klass (klass)) {
  9025| 				addr = mini_emit_ldelema_1_ins (cfg, klass, sp [0], sp [1], TRUE, FALSE);
  9026| 				EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), addr->dreg, 0);
  9027| 				ins->opcode = OP_LOADV_MEMBASE;
  9028| 			} else if (sp [1]->opcode == OP_ICONST) {
  9029| 				int array_reg = sp [0]->dreg;
  9030| 				int index_reg = sp [1]->dreg;
  9031| 				size_t offset = (mono_class_array_element_size (klass) * sp [1]->inst_c0) + MONO_STRUCT_OFFSET (MonoArray, vector);
  9032| 				if (SIZEOF_REGISTER == 8 && COMPILE_LLVM (cfg))
  9033| 					MONO_EMIT_NEW_UNALU (cfg, OP_ZEXT_I4, index_reg, index_reg);
  9034| 				MONO_EMIT_BOUNDS_CHECK (cfg, array_reg, MonoArray, max_length, index_reg);
  9035| 				EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), array_reg, (target_mgreg_t)offset);
  9036| 			} else {
  9037| 				addr = mini_emit_ldelema_1_ins (cfg, klass, sp [0], sp [1], TRUE, FALSE);
  9038| 				EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), addr->dreg, 0);
  9039| 			}
  9040| 			*sp++ = ins;
  9041| 			break;
  9042| 		}
  9043| 		case MONO_CEE_STELEM_I:
  9044| 		case MONO_CEE_STELEM_I1:
  9045| 		case MONO_CEE_STELEM_I2:
  9046| 		case MONO_CEE_STELEM_I4:
  9047| 		case MONO_CEE_STELEM_I8:
  9048| 		case MONO_CEE_STELEM_R4:
  9049| 		case MONO_CEE_STELEM_R8:
  9050| 		case MONO_CEE_STELEM_REF:
  9051| 		case MONO_CEE_STELEM: {
  9052| 			sp -= 3;
  9053| 			cfg->flags |= MONO_CFG_HAS_LDELEMA;
  9054| 			if (il_op == MONO_CEE_STELEM) {
  9055| 				klass = mini_get_class (method, token, generic_context);
  9056| 				CHECK_TYPELOAD (klass);
  9057| 				mono_class_init_internal (klass);
  9058| 			}
  9059| 			else
  9060| 				klass = array_access_to_klass (il_op);
  9061| 			if (sp [0]->type != STACK_OBJ)
  9062| 				UNVERIFIED;
  9063| 			sp [2] = convert_value (cfg, m_class_get_byval_arg (klass), sp [2]);
  9064| 			mini_emit_array_store (cfg, klass, sp, TRUE);
  9065| 			inline_costs += 1;
  9066| 			break;
  9067| 		}
  9068| 		case MONO_CEE_CKFINITE: {
  9069| 			--sp;
  9070| 			if (cfg->llvm_only) {
  9071| 				MonoInst *iargs [1];
  9072| 				iargs [0] = sp [0];
  9073| 				*sp++ = mono_emit_jit_icall (cfg, mono_ckfinite, iargs);
  9074| 			} else  {
  9075| 				sp [0] = convert_value (cfg, m_class_get_byval_arg (mono_defaults.double_class), sp [0]);
  9076| 				MONO_INST_NEW (cfg, ins, OP_CKFINITE);
  9077| 				ins->sreg1 = sp [0]->dreg;
  9078| 				ins->dreg = alloc_freg (cfg);
  9079| 				ins->type = STACK_R8;
  9080| 				MONO_ADD_INS (cfg->cbb, ins);
  9081| 				*sp++ = mono_decompose_opcode (cfg, ins);
  9082| 			}
  9083| 			break;
  9084| 		}
  9085| 		case MONO_CEE_REFANYVAL: {
  9086| 			MonoInst *src_var, *src;
  9087| 			int klass_reg = alloc_preg (cfg);
  9088| 			int dreg = alloc_preg (cfg);
  9089| 			GSHAREDVT_FAILURE (il_op);
  9090| 			MONO_INST_NEW (cfg, ins, il_op);
  9091| 			--sp;
  9092| 			klass = mini_get_class (method, token, generic_context);
  9093| 			CHECK_TYPELOAD (klass);
  9094| 			context_used = mini_class_check_context_used (cfg, klass);
  9095| 			src_var = get_vreg_to_inst (cfg, sp [0]->dreg);
  9096| 			if (!src_var)
  9097| 				src_var = mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (mono_defaults.typed_reference_class), OP_LOCAL, sp [0]->dreg);
  9098| 			EMIT_NEW_VARLOADA (cfg, src, src_var, src_var->inst_vtype);
  9099| 			MONO_EMIT_NEW_LOAD_MEMBASE (cfg, klass_reg, src->dreg, MONO_STRUCT_OFFSET (MonoTypedRef, klass));
  9100| 			if (context_used) {
  9101| 				MonoInst *klass_ins;
  9102| 				klass_ins = mini_emit_get_rgctx_klass (cfg, context_used,
  9103| 						klass, MONO_RGCTX_INFO_KLASS);
  9104| 				MONO_EMIT_NEW_BIALU (cfg, OP_COMPARE, -1, klass_reg, klass_ins->dreg);
  9105| 				MONO_EMIT_NEW_COND_EXC (cfg, NE_UN, "InvalidCastException");
  9106| 			} else {
  9107| 				mini_emit_class_check (cfg, klass_reg, klass);
  9108| 			}
  9109| 			EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, dreg, src->dreg, MONO_STRUCT_OFFSET (MonoTypedRef, value));
  9110| 			ins->type = STACK_MP;
  9111| 			ins->klass = klass;
  9112| 			*sp++ = ins;
  9113| 			break;
  9114| 		}
  9115| 		case MONO_CEE_MKREFANY: {
  9116| 			MonoInst *loc, *addr;
  9117| 			GSHAREDVT_FAILURE (il_op);
  9118| 			MONO_INST_NEW (cfg, ins, il_op);
  9119| 			--sp;
  9120| 			klass = mini_get_class (method, token, generic_context);
  9121| 			CHECK_TYPELOAD (klass);
  9122| 			context_used = mini_class_check_context_used (cfg, klass);
  9123| 			loc = mono_compile_create_var (cfg, m_class_get_byval_arg (mono_defaults.typed_reference_class), OP_LOCAL);
  9124| 			EMIT_NEW_TEMPLOADA (cfg, addr, loc->inst_c0);
  9125| 			MonoInst *const_ins = mini_emit_get_rgctx_klass (cfg, context_used, klass, MONO_RGCTX_INFO_KLASS);
  9126| 			int type_reg = alloc_preg (cfg);
  9127| 			MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STOREP_MEMBASE_REG, addr->dreg, MONO_STRUCT_OFFSET (MonoTypedRef, klass), const_ins->dreg);
  9128| 			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_ADD_IMM, type_reg, const_ins->dreg, m_class_offsetof_byval_arg ());
  9129| 			MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STOREP_MEMBASE_REG, addr->dreg, MONO_STRUCT_OFFSET (MonoTypedRef, type), type_reg);
  9130| 			MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STOREP_MEMBASE_REG, addr->dreg, MONO_STRUCT_OFFSET (MonoTypedRef, value), sp [0]->dreg);
  9131| 			EMIT_NEW_TEMPLOAD (cfg, ins, loc->inst_c0);
  9132| 			ins->type = STACK_VTYPE;
  9133| 			ins->klass = mono_defaults.typed_reference_class;
  9134| 			*sp++ = ins;
  9135| 			break;
  9136| 		}
  9137| 		case MONO_CEE_LDTOKEN: {
  9138| 			gpointer handle;
  9139| 			MonoClass *handle_class;
  9140| 			if (method->wrapper_type == MONO_WRAPPER_DYNAMIC_METHOD ||
  9141| 					method->wrapper_type == MONO_WRAPPER_SYNCHRONIZED) {
  9142| 				handle = mono_method_get_wrapper_data (method, n);
  9143| 				handle_class = (MonoClass *)mono_method_get_wrapper_data (method, n + 1);
  9144| 				if (handle_class == mono_defaults.typehandle_class)
  9145| 					handle = m_class_get_byval_arg ((MonoClass*)handle);
  9146| 			}
  9147| 			else {
  9148| 				handle = mono_ldtoken_checked (image, n, &handle_class, generic_context, cfg->error);
  9149| 				CHECK_CFG_ERROR;
  9150| 			}
  9151| 			if (!handle)
  9152| 				LOAD_ERROR;
  9153| 			mono_class_init_internal (handle_class);
  9154| 			if (cfg->gshared) {
  9155| 				if (mono_metadata_token_table (n) == MONO_TABLE_TYPEDEF ||
  9156| 						mono_metadata_token_table (n) == MONO_TABLE_TYPEREF) {
  9157| 					/* This case handles ldtoken
  9158| 					   of an open type, like for
  9159| 					   typeof(Gen<>). */
  9160| 					context_used = 0;
  9161| 				} else if (handle_class == mono_defaults.typehandle_class) {
  9162| 					context_used = mini_class_check_context_used (cfg, mono_class_from_mono_type_internal ((MonoType *)handle));
  9163| 				} else if (handle_class == mono_defaults.fieldhandle_class)
  9164| 					context_used = mini_class_check_context_used (cfg, m_field_get_parent (((MonoClassField*)handle)));
  9165| 				else if (handle_class == mono_defaults.methodhandle_class)
  9166| 					context_used = mini_method_check_context_used (cfg, (MonoMethod *)handle);
  9167| 				else
  9168| 					g_assert_not_reached ();
  9169| 			}
  9170| 			{
  9171| 				if ((next_ip + 4 < end) && ip_in_bb (cfg, cfg->cbb, next_ip) &&
  9172| 					((next_ip [0] == CEE_CALL) || (next_ip [0] == CEE_CALLVIRT)) &&
  9173| 					(cmethod = mini_get_method (cfg, method, read32 (next_ip + 1), NULL, generic_context)) &&
  9174| 					(cmethod->klass == mono_defaults.systemtype_class) &&
  9175| 					(strcmp (cmethod->name, "GetTypeFromHandle") == 0)) {
  9176| 					MonoClass *tclass = mono_class_from_mono_type_internal ((MonoType *)handle);
  9177| 					mono_class_init_internal (tclass);
  9178| 					guchar *is_vt_ip;
  9179| 					guint32 is_vt_token;
  9180| 					if ((is_vt_ip = il_read_call (next_ip + 5, end, &is_vt_token)) && ip_in_bb (cfg, cfg->cbb, is_vt_ip)) {
  9181| 						MonoMethod *is_vt_method = mini_get_method (cfg, method, is_vt_token, NULL, generic_context);
  9182| 						if (is_vt_method->klass == mono_defaults.systemtype_class &&
  9183| 							!mini_is_gsharedvt_variable_klass (tclass) &&
  9184| 							!mono_class_is_open_constructed_type (m_class_get_byval_arg (tclass)) &&
  9185| 							!strcmp ("get_IsValueType", is_vt_method->name)) {
  9186| 							next_ip = is_vt_ip;
  9187| 							EMIT_NEW_ICONST (cfg, ins, m_class_is_valuetype (tclass) ? 1 : 0);
  9188| 							ins->type = STACK_I4;
  9189| 							*sp++ = ins;
  9190| 							break;
  9191| 						}
  9192| 					}
  9193| 					if (context_used) {
  9194| 						MONO_INST_NEW (cfg, ins, OP_RTTYPE);
  9195| 						ins->dreg = alloc_ireg_ref (cfg);
  9196| 						ins->inst_p0 = tclass;
  9197| 						ins->type = STACK_OBJ;
  9198| 						MONO_ADD_INS (cfg->cbb, ins);
  9199| 						cfg->flags |= MONO_CFG_NEEDS_DECOMPOSE;
  9200| 						cfg->cbb->needs_decompose = TRUE;
  9201| 					} else if (cfg->compile_aot) {
  9202| 						if (method->wrapper_type) {
  9203| 							error_init (error); //got to do it since there are multiple conditionals below
  9204| 							if (mono_class_get_checked (m_class_get_image (tclass), m_class_get_type_token (tclass), error) == tclass && !generic_context) {
  9205| 								/* Special case for static synchronized wrappers */
  9206| 								EMIT_NEW_TYPE_FROM_HANDLE_CONST (cfg, ins, m_class_get_image (tclass), m_class_get_type_token (tclass), generic_context);
  9207| 							} else {
  9208| 								mono_error_cleanup (error); /* FIXME don't swallow the error */
  9209| 								/* FIXME: n is not a normal token */
  9210| 								DISABLE_AOT (cfg);
  9211| 								EMIT_NEW_PCONST (cfg, ins, NULL);
  9212| 							}
  9213| 						} else {
  9214| 							EMIT_NEW_TYPE_FROM_HANDLE_CONST (cfg, ins, image, n, generic_context);
  9215| 						}
  9216| 					} else {
  9217| 						MonoReflectionType *rt = mono_type_get_object_checked ((MonoType *)handle, cfg->error);
  9218| 						CHECK_CFG_ERROR;
  9219| 						EMIT_NEW_PCONST (cfg, ins, rt);
  9220| 					}
  9221| 					ins->type = STACK_OBJ;
  9222| 					ins->klass = mono_defaults.runtimetype_class;
  9223| 					il_op = (MonoOpcodeEnum)next_ip [0];
  9224| 					next_ip += 5;
  9225| 				} else {
  9226| 					MonoInst *addr, *vtvar;
  9227| 					vtvar = mono_compile_create_var (cfg, m_class_get_byval_arg (handle_class), OP_LOCAL);
  9228| 					if (context_used) {
  9229| 						if (handle_class == mono_defaults.typehandle_class) {
  9230| 							ins = mini_emit_get_rgctx_klass (cfg, context_used,
  9231| 									mono_class_from_mono_type_internal ((MonoType *)handle),
  9232| 									MONO_RGCTX_INFO_TYPE);
  9233| 						} else if (handle_class == mono_defaults.methodhandle_class) {
  9234| 							ins = emit_get_rgctx_method (cfg, context_used,
  9235| 									(MonoMethod *)handle, MONO_RGCTX_INFO_METHOD);
  9236| 						} else if (handle_class == mono_defaults.fieldhandle_class) {
  9237| 							ins = emit_get_rgctx_field (cfg, context_used,
  9238| 									(MonoClassField *)handle, MONO_RGCTX_INFO_CLASS_FIELD);
  9239| 						} else {
  9240| 							g_assert_not_reached ();
  9241| 						}
  9242| 					} else if (cfg->compile_aot) {
  9243| 						EMIT_NEW_LDTOKENCONST (cfg, ins, image, n, generic_context);
  9244| 					} else {
  9245| 						EMIT_NEW_PCONST (cfg, ins, handle);
  9246| 					}
  9247| 					EMIT_NEW_TEMPLOADA (cfg, addr, vtvar->inst_c0);
  9248| 					MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, addr->dreg, 0, ins->dreg);
  9249| 					EMIT_NEW_TEMPLOAD (cfg, ins, vtvar->inst_c0);
  9250| 					if (handle_class == mono_defaults.fieldhandle_class) {
  9251| 						ins->opcode = OP_LDTOKEN_FIELD;
  9252| 						ins->inst_c0 = n;
  9253| 						ins->inst_p1 = handle;
  9254| 						cfg->flags |= MONO_CFG_NEEDS_DECOMPOSE;
  9255| 						cfg->cbb->needs_decompose = TRUE;
  9256| 					}
  9257| 				}
  9258| 			}
  9259| 			*sp++ = ins;
  9260| 			break;
  9261| 		}
  9262| 		case MONO_CEE_THROW:
  9263| 			if (sp [-1]->type != STACK_OBJ)
  9264| 				UNVERIFIED;
  9265| 			MONO_INST_NEW (cfg, ins, OP_THROW);
  9266| 			--sp;
  9267| 			ins->sreg1 = sp [0]->dreg;
  9268| 			cfg->cbb->out_of_line = TRUE;
  9269| 			MONO_ADD_INS (cfg->cbb, ins);
  9270| 			MONO_INST_NEW (cfg, ins, OP_NOT_REACHED);
  9271| 			MONO_ADD_INS (cfg->cbb, ins);
  9272| 			sp = stack_start;
  9273| 			link_bblock (cfg, cfg->cbb, end_bblock);
  9274| 			start_new_bblock = 1;
  9275| 			/* This can complicate code generation for llvm since the return value might not be defined */
  9276| 			if (COMPILE_LLVM (cfg))
  9277| 				INLINE_FAILURE ("throw");
  9278| 			break;
  9279| 		case MONO_CEE_ENDFINALLY:
  9280| 			if (!ip_in_finally_clause (cfg, GPTRDIFF_TO_INT (ip - header->code)))
  9281| 				UNVERIFIED;
  9282| 			/* mono_save_seq_point_info () depends on this */
  9283| 			if (sp != stack_start)
  9284| 				emit_seq_point (cfg, method, ip, FALSE, FALSE);
  9285| 			MONO_INST_NEW (cfg, ins, OP_ENDFINALLY);
  9286| 			MONO_ADD_INS (cfg->cbb, ins);
  9287| 			start_new_bblock = 1;
  9288| 			ins_has_side_effect = FALSE;
  9289| 			/*
  9290| 			 * Control will leave the method so empty the stack, otherwise
  9291| 			 * the next basic block will start with a nonempty stack.
  9292| 			 */
  9293| 			while (sp != stack_start) {
  9294| 				sp--;
  9295| 			}
  9296| 			break;
  9297| 		case MONO_CEE_LEAVE:
  9298| 		case MONO_CEE_LEAVE_S: {
  9299| 			GList *handlers;
  9300| 			/* empty the stack */
  9301| 			g_assert (sp >= stack_start);
  9302| 			sp = stack_start;
  9303| 			/*
  9304| 			 * If this leave statement is in a catch block, check for a
  9305| 			 * pending exception, and rethrow it if necessary.
  9306| 			 * We avoid doing this in runtime invoke wrappers, since those are called
  9307| 			 * by native code which excepts the wrapper to catch all exceptions.
  9308| 			 */
  9309| 			for (unsigned int i = 0; i < header->num_clauses; ++i) {
  9310| 				MonoExceptionClause *clause = &header->clauses [i];
  9311| 				/*
  9312| 				 * Use <= in the final comparison to handle clauses with multiple
  9313| 				 * leave statements, like in bug #78024.
  9314| 				 * The ordering of the exception clauses guarantees that we find the
  9315| 				 * innermost clause.
  9316| 				 */
  9317| 				if (MONO_OFFSET_IN_HANDLER (clause, GPTRDIFF_TO_UINT32(ip - header->code)) && (clause->flags == MONO_EXCEPTION_CLAUSE_NONE) && GPTRDIFF_TO_UINT32(ip - header->code + ((il_op == MONO_CEE_LEAVE) ? 5 : 2)) <= (clause->handler_offset + clause->handler_len) && method->wrapper_type != MONO_WRAPPER_RUNTIME_INVOKE) {
  9318| 					MonoInst *exc_ins;
  9319| 					MonoBasicBlock *dont_throw;
  9320| 					/*
  9321| 					  MonoInst *load;
  9322| 					  NEW_TEMPLOAD (cfg, load, mono_find_exvar_for_offset (cfg, clause->handler_offset)->inst_c0);
  9323| 					*/
  9324| 					exc_ins = mono_emit_jit_icall (cfg, mono_thread_get_undeniable_exception, NULL);
  9325| 					NEW_BBLOCK (cfg, dont_throw);
  9326| 					/*
  9327| 					 * Currently, we always rethrow the abort exception, despite the
  9328| 					 * fact that this is not correct. See thread6.cs for an example.
  9329| 					 * But propagating the abort exception is more important than
  9330| 					 * getting the semantics right.
  9331| 					 */
  9332| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, exc_ins->dreg, 0);
  9333| 					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, dont_throw);
  9334| 					MONO_EMIT_NEW_UNALU (cfg, OP_THROW, -1, exc_ins->dreg);
  9335| 					MONO_START_BB (cfg, dont_throw);
  9336| 				}
  9337| 			}
  9338| #ifdef ENABLE_LLVM
  9339| 			cfg->cbb->try_end = (intptr_t)(ip - header->code);
  9340| #endif
  9341| 			if ((handlers = mono_find_leave_clauses (cfg, ip, target))) {
  9342| 				GList *tmp;
  9343| 				/*
  9344| 				 * For each finally clause that we exit we need to invoke the finally block.
  9345| 				 * After each invocation we need to add try holes for all the clauses that
  9346| 				 * we already exited.
  9347| 				 */
  9348| 				for (tmp = handlers; tmp; tmp = tmp->next) {
  9349| 					MonoLeaveClause *leave = (MonoLeaveClause *) tmp->data;
  9350| 					MonoExceptionClause *clause = leave->clause;
  9351| 					if (clause->flags != MONO_EXCEPTION_CLAUSE_FINALLY)
  9352| 						continue;
  9353| 					MonoInst *abort_exc = (MonoInst *)mono_find_exvar_for_offset (cfg, clause->handler_offset);
  9354| 					MonoBasicBlock *dont_throw;
  9355| 					/*
  9356| 					 * Emit instrumentation code before linking the basic blocks below as this
  9357| 					 * will alter cfg->cbb.
  9358| 					 */
  9359| 					mini_profiler_emit_call_finally (cfg, header, ip, leave->index, clause);
  9360| 					tblock = cfg->cil_offset_to_bb [clause->handler_offset];
  9361| 					g_assert (tblock);
  9362| 					link_bblock (cfg, cfg->cbb, tblock);
  9363| 					MONO_EMIT_NEW_PCONST (cfg, abort_exc->dreg, 0);
  9364| 					MONO_INST_NEW (cfg, ins, OP_CALL_HANDLER);
  9365| 					ins->inst_target_bb = tblock;
  9366| 					ins->inst_eh_blocks = tmp;
  9367| 					MONO_ADD_INS (cfg->cbb, ins);
  9368| 					cfg->cbb->has_call_handler = 1;
  9369| 					/* Throw exception if exvar is set */
  9370| 					/* FIXME Do we need this for calls from catch/filter ? */
  9371| 					NEW_BBLOCK (cfg, dont_throw);
  9372| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, abort_exc->dreg, 0);
  9373| 					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, dont_throw);
  9374| 					mono_emit_jit_icall (cfg, ves_icall_thread_finish_async_abort, NULL);
  9375| 					cfg->cbb->clause_holes = tmp;
  9376| 					MONO_START_BB (cfg, dont_throw);
  9377| 					cfg->cbb->clause_holes = tmp;
  9378| 					if (COMPILE_LLVM (cfg)) {
  9379| 						MonoBasicBlock *target_bb;
  9380| 						/*
  9381| 						 * Link the finally bblock with the target, since it will
  9382| 						 * conceptually branch there.
  9383| 						 */
  9384| 						GET_BBLOCK (cfg, tblock, cfg->cil_start + clause->handler_offset + clause->handler_len - 1);
  9385| 						GET_BBLOCK (cfg, target_bb, target);
  9386| 						link_bblock (cfg, tblock, target_bb);
  9387| 					}
  9388| 				}
  9389| 			}
  9390| 			MONO_INST_NEW (cfg, ins, OP_BR);
  9391| 			MONO_ADD_INS (cfg->cbb, ins);
  9392| 			GET_BBLOCK (cfg, tblock, target);
  9393| 			link_bblock (cfg, cfg->cbb, tblock);
  9394| 			ins->inst_target_bb = tblock;
  9395| 			start_new_bblock = 1;
  9396| 			break;
  9397| 		}
  9398| 		/*
  9399| 		 * Mono specific opcodes
  9400| 		 */
  9401| 		case MONO_CEE_MONO_ICALL: {
  9402| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9403| 			const MonoJitICallId jit_icall_id = (MonoJitICallId)token;
  9404| 			MonoJitICallInfo * const jit_icall_info = mono_find_jit_icall_info (jit_icall_id);
  9405| 			CHECK_STACK (jit_icall_info->sig->param_count);
  9406| 			sp -= jit_icall_info->sig->param_count;
  9407| 			if (token == MONO_JIT_ICALL_mono_threads_attach_coop) {
  9408| 				MonoInst *addr;
  9409| 				MonoBasicBlock *next_bb;
  9410| 				if (cfg->compile_aot) {
  9411| 					/*
  9412| 					 * This is called on unattached threads, so it cannot go through the trampoline
  9413| 					 * infrastructure. Use an indirect call through a got slot initialized at load time
  9414| 					 * instead.
  9415| 					 */
  9416| 					EMIT_NEW_AOTCONST (cfg, addr, MONO_PATCH_INFO_JIT_ICALL_ADDR_NOCALL, GUINT_TO_POINTER (jit_icall_id));
  9417| 					ins = mini_emit_calli (cfg, jit_icall_info->sig, sp, addr, NULL, NULL);
  9418| 				} else {
  9419| 					ins = mono_emit_jit_icall_id (cfg, jit_icall_id, sp);
  9420| 				}
  9421| 				/*
  9422| 				 * Parts of the initlocals code needs to come after this, since it might call methods like memset.
  9423| 				 * Also profiling needs to be after attach.
  9424| 				 */
  9425| 				init_localsbb2 = cfg->cbb;
  9426| 				NEW_BBLOCK (cfg, next_bb);
  9427| 				MONO_START_BB (cfg, next_bb);
  9428| 			} else {
  9429| 				if (token == MONO_JIT_ICALL_mono_threads_detach_coop) {
  9430| 					/* can't emit profiling code after a detach, so emit it now */
  9431| 					mini_profiler_emit_leave (cfg, NULL);
  9432| 					detached_before_ret = TRUE;
  9433| 				}
  9434| 				ins = mono_emit_jit_icall_id (cfg, jit_icall_id, sp);
  9435| 			}
  9436| 			if (!MONO_TYPE_IS_VOID (jit_icall_info->sig->ret))
  9437| 				*sp++ = ins;
  9438| 			inline_costs += CALL_COST * MIN(10, num_calls++);
  9439| 			break;
  9440| 		}
  9441| 		MonoJumpInfoType ldptr_type;
  9442| 		case MONO_CEE_MONO_LDPTR_CARD_TABLE:
  9443| 			ldptr_type = MONO_PATCH_INFO_GC_CARD_TABLE_ADDR;
  9444| 			goto mono_ldptr;
  9445| 		case MONO_CEE_MONO_LDPTR_NURSERY_START:
  9446| 			ldptr_type = MONO_PATCH_INFO_GC_NURSERY_START;
  9447| 			goto mono_ldptr;
  9448| 		case MONO_CEE_MONO_LDPTR_NURSERY_BITS:
  9449| 			ldptr_type = MONO_PATCH_INFO_GC_NURSERY_BITS;
  9450| 			goto mono_ldptr;
  9451| 		case MONO_CEE_MONO_LDPTR_INT_REQ_FLAG:
  9452| 			ldptr_type = MONO_PATCH_INFO_INTERRUPTION_REQUEST_FLAG;
  9453| 			goto mono_ldptr;
  9454| 		case MONO_CEE_MONO_LDPTR_PROFILER_ALLOCATION_COUNT:
  9455| 			ldptr_type = MONO_PATCH_INFO_PROFILER_ALLOCATION_COUNT;
  9456| mono_ldptr:
  9457| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9458| 			ins = mini_emit_runtime_constant (cfg, ldptr_type, NULL);
  9459| 			*sp++ = ins;
  9460| 			inline_costs += CALL_COST * MIN(10, num_calls++);
  9461| 			break;
  9462| 		case MONO_CEE_MONO_LDPTR: {
  9463| 			gpointer ptr;
  9464| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9465| 			ptr = mono_method_get_wrapper_data (method, token);
  9466| 			EMIT_NEW_PCONST (cfg, ins, ptr);
  9467| 			*sp++ = ins;
  9468| 			inline_costs += CALL_COST * MIN(10, num_calls++);
  9469| 			/* Can't embed random pointers into AOT code */
  9470| 			DISABLE_AOT (cfg);
  9471| 			break;
  9472| 		}
  9473| 		case MONO_CEE_MONO_JIT_ICALL_ADDR:
  9474| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9475| 			EMIT_NEW_JIT_ICALL_ADDRCONST (cfg, ins, GUINT_TO_POINTER (token));
  9476| 			*sp++ = ins;
  9477| 			inline_costs += CALL_COST * MIN(10, num_calls++);
  9478| 			break;
  9479| 		case MONO_CEE_MONO_ICALL_ADDR: {
  9480| 			gpointer ptr;
  9481| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9482| 			cmethod = (MonoMethod *)mono_method_get_wrapper_data (method, token);
  9483| 			if (cfg->compile_aot) {
  9484| 				if (cfg->direct_pinvoke && ip + 6 < end && (ip [6] == CEE_POP)) {
  9485| 					/*
  9486| 					 * This is generated by emit_native_wrapper () to resolve the pinvoke address
  9487| 					 * before the call, its not needed when using direct pinvoke.
  9488| 					 * This is not an optimization, but its used to avoid looking up pinvokes
  9489| 					 * on platforms which don't support dlopen ().
  9490| 					 */
  9491| 					EMIT_NEW_PCONST (cfg, ins, NULL);
  9492| 				} else {
  9493| 					EMIT_NEW_AOTCONST (cfg, ins, MONO_PATCH_INFO_ICALL_ADDR, cmethod);
  9494| 				}
  9495| 			} else {
  9496| 				ptr = mono_lookup_internal_call (cmethod);
  9497| 				g_assert (ptr);
  9498| 				EMIT_NEW_PCONST (cfg, ins, ptr);
  9499| 			}
  9500| 			*sp++ = ins;
  9501| 			break;
  9502| 		}
  9503| 		case MONO_CEE_MONO_VTADDR: {
  9504| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9505| 			MonoInst *src_var, *src;
  9506| 			--sp;
  9507| 			src_var = get_vreg_to_inst (cfg, sp [0]->dreg);
  9508| 			EMIT_NEW_VARLOADA ((cfg), (src), src_var, src_var->inst_vtype);
  9509| 			*sp++ = src;
  9510| 			break;
  9511| 		}
  9512| 		case MONO_CEE_MONO_NEWOBJ: {
  9513| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9514| 			MonoInst *iargs [2];
  9515| 			klass = (MonoClass *)mono_method_get_wrapper_data (method, token);
  9516| 			mono_class_init_internal (klass);
  9517| 			NEW_CLASSCONST (cfg, iargs [0], klass);
  9518| 			MONO_ADD_INS (cfg->cbb, iargs [0]);
  9519| 			*sp++ = mono_emit_jit_icall (cfg, ves_icall_object_new, iargs);
  9520| 			inline_costs += CALL_COST * MIN(10, num_calls++);
  9521| 			break;
  9522| 		}
  9523| 		case MONO_CEE_MONO_OBJADDR:
  9524| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9525| 			--sp;
  9526| 			MONO_INST_NEW (cfg, ins, OP_MOVE);
  9527| 			ins->dreg = alloc_ireg_mp (cfg);
  9528| 			ins->sreg1 = sp [0]->dreg;
  9529| 			ins->type = STACK_MP;
  9530| 			MONO_ADD_INS (cfg->cbb, ins);
  9531| 			*sp++ = ins;
  9532| 			break;
  9533| 		case MONO_CEE_MONO_LDNATIVEOBJ:
  9534| 			/*
  9535| 			 * Similar to LDOBJ, but instead load the unmanaged
  9536| 			 * representation of the vtype to the stack.
  9537| 			 */
  9538| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9539| 			--sp;
  9540| 			klass = (MonoClass *)mono_method_get_wrapper_data (method, token);
  9541| 			g_assert (m_class_is_valuetype (klass));
  9542| 			mono_class_init_internal (klass);
  9543| 			{
  9544| 				MonoInst *src, *dest, *temp;
  9545| 				src = sp [0];
  9546| 				temp = mono_compile_create_var (cfg, m_class_get_byval_arg (klass), OP_LOCAL);
  9547| 				temp->backend.is_pinvoke = 1;
  9548| 				EMIT_NEW_TEMPLOADA (cfg, dest, temp->inst_c0);
  9549| 				mini_emit_memory_copy (cfg, dest, src, klass, TRUE, 0);
  9550| 				EMIT_NEW_TEMPLOAD (cfg, dest, temp->inst_c0);
  9551| 				dest->type = STACK_VTYPE;
  9552| 				dest->klass = klass;
  9553| 				*sp ++ = dest;
  9554| 			}
  9555| 			break;
  9556| 		case MONO_CEE_MONO_RETOBJ: {
  9557| 			/*
  9558| 			 * Same as RET, but return the native representation of a vtype
  9559| 			 * to the caller.
  9560| 			 */
  9561| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9562| 			g_assert (cfg->ret);
  9563| 			g_assert (mono_method_signature_internal (method)->pinvoke);
  9564| 			--sp;
  9565| 			klass = (MonoClass *)mono_method_get_wrapper_data (method, token);
  9566| 			if (!cfg->vret_addr) {
  9567| 				g_assert (cfg->ret_var_is_local);
  9568| 				EMIT_NEW_VARLOADA (cfg, ins, cfg->ret, cfg->ret->inst_vtype);
  9569| 			} else {
  9570| 				EMIT_NEW_RETLOADA (cfg, ins);
  9571| 			}
  9572| 			mini_emit_memory_copy (cfg, ins, sp [0], klass, TRUE, 0);
  9573| 			if (sp != stack_start)
  9574| 				UNVERIFIED;
  9575| 			if (!detached_before_ret)
  9576| 				mini_profiler_emit_leave (cfg, sp [0]);
  9577| 			MONO_INST_NEW (cfg, ins, OP_BR);
  9578| 			ins->inst_target_bb = end_bblock;
  9579| 			MONO_ADD_INS (cfg->cbb, ins);
  9580| 			link_bblock (cfg, cfg->cbb, end_bblock);
  9581| 			start_new_bblock = 1;
  9582| 			break;
  9583| 		}
  9584| 		case MONO_CEE_MONO_SAVE_LMF:
  9585| 		case MONO_CEE_MONO_RESTORE_LMF:
  9586| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9587| 			break;
  9588| 		case MONO_CEE_MONO_CLASSCONST:
  9589| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9590| 			EMIT_NEW_CLASSCONST (cfg, ins, mono_method_get_wrapper_data (method, token));
  9591| 			*sp++ = ins;
  9592| 			inline_costs += CALL_COST * MIN(10, num_calls++);
  9593| 			break;
  9594| 		case MONO_CEE_MONO_METHODCONST:
  9595| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9596| 			EMIT_NEW_METHODCONST (cfg, ins, mono_method_get_wrapper_data (method, token));
  9597| 			*sp++ = ins;
  9598| 			break;
  9599| 		case MONO_CEE_MONO_PINVOKE_ADDR_CACHE: {
  9600| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9601| 			MonoMethod *pinvoke_method = (MonoMethod*)mono_method_get_wrapper_data (method, token);
  9602| 			/* This is a memory slot used by the wrapper */
  9603| 			if (cfg->compile_aot) {
  9604| 				EMIT_NEW_AOTCONST (cfg, ins, MONO_PATCH_INFO_METHOD_PINVOKE_ADDR_CACHE, pinvoke_method);
  9605| 			} else {
  9606| 				gpointer addr = mono_mem_manager_alloc0 (cfg->mem_manager, sizeof (gpointer));
  9607| 				EMIT_NEW_PCONST (cfg, ins, addr);
  9608| 			}
  9609| 			*sp++ = ins;
  9610| 			break;
  9611| 		}
  9612| 		case MONO_CEE_MONO_NOT_TAKEN:
  9613| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9614| 			cfg->cbb->out_of_line = TRUE;
  9615| 			break;
  9616| 		case MONO_CEE_MONO_TLS: {
  9617| 			MonoTlsKey key;
  9618| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9619| 			key = (MonoTlsKey)n;
  9620| 			g_assert (key < TLS_KEY_NUM);
  9621| 			ins = mono_create_tls_get (cfg, key);
  9622| 			g_assert (ins);
  9623| 			ins->type = STACK_PTR;
  9624| 			*sp++ = ins;
  9625| 			break;
  9626| 		}
  9627| 		case MONO_CEE_MONO_DYN_CALL: {
  9628| 			MonoCallInst *call;
  9629| 			/* It would be easier to call a trampoline, but that would put an
  9630| 			 * extra frame on the stack, confusing exception handling. So
  9631| 			 * implement it inline using an opcode for now.
  9632| 			 */
  9633| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9634| 			if (!cfg->dyn_call_var) {
  9635| 				cfg->dyn_call_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
  9636| 				/* prevent it from being register allocated */
  9637| 				cfg->dyn_call_var->flags |= MONO_INST_VOLATILE;
  9638| 			}
  9639| 			/* Has to use a call inst since local regalloc expects it */
  9640| 			MONO_INST_NEW_CALL (cfg, call, OP_DYN_CALL);
  9641| 			ins = (MonoInst*)call;
  9642| 			sp -= 2;
  9643| 			ins->sreg1 = sp [0]->dreg;
  9644| 			ins->sreg2 = sp [1]->dreg;
  9645| 			MONO_ADD_INS (cfg->cbb, ins);
  9646| 			cfg->param_area = MAX (cfg->param_area, GINT_TO_UINT(cfg->backend->dyn_call_param_area));
  9647| 			/* OP_DYN_CALL might need to allocate a dynamically sized param area */
  9648| 			cfg->flags |= MONO_CFG_HAS_ALLOCA;
  9649| 			inline_costs += CALL_COST * MIN(10, num_calls++);
  9650| 			break;
  9651| 		}
  9652| 		case MONO_CEE_MONO_MEMORY_BARRIER: {
  9653| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9654| 			mini_emit_memory_barrier (cfg, (int)n);
  9655| 			break;
  9656| 		}
  9657| 		case MONO_CEE_MONO_ATOMIC_STORE_I4: {
  9658| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9659| 			g_assert (mono_arch_opcode_supported (OP_ATOMIC_STORE_I4));
  9660| 			sp -= 2;
  9661| 			MONO_INST_NEW (cfg, ins, OP_ATOMIC_STORE_I4);
  9662| 			ins->dreg = sp [0]->dreg;
  9663| 			ins->sreg1 = sp [1]->dreg;
  9664| 			ins->backend.memory_barrier_kind = (int)n;
  9665| 			MONO_ADD_INS (cfg->cbb, ins);
  9666| 			break;
  9667| 		}
  9668| 		case MONO_CEE_MONO_LD_DELEGATE_METHOD_PTR: {
  9669| 			CHECK_STACK (1);
  9670| 			--sp;
  9671| 			int dreg = alloc_preg (cfg);
  9672| 			EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, dreg, sp [0]->dreg, MONO_STRUCT_OFFSET (MonoDelegate, method_ptr));
  9673| 			*sp++ = ins;
  9674| 			break;
  9675| 		}
  9676| 		case MONO_CEE_MONO_CALLI_EXTRA_ARG: {
  9677| 			MonoInst *addr;
  9678| 			MonoInst *arg;
  9679| 			/*
  9680| 			 * This is the same as CEE_CALLI, but passes an additional argument
  9681| 			 * to the called method in llvmonly mode.
  9682| 			 * This is only used by delegate invoke wrappers to call the
  9683| 			 * actual delegate method.
  9684| 			 */
  9685| 			g_assert (method->wrapper_type == MONO_WRAPPER_DELEGATE_INVOKE);
  9686| 			ins = NULL;
  9687| 			cmethod = NULL;
  9688| 			CHECK_STACK (1);
  9689| 			--sp;
  9690| 			addr = *sp;
  9691| 			fsig = mini_get_signature (method, token, generic_context, cfg->error);
  9692| 			CHECK_CFG_ERROR;
  9693| 			if (cfg->llvm_only)
  9694| 				cfg->signatures = g_slist_prepend_mempool (cfg->mempool, cfg->signatures, fsig);
  9695| 			n = fsig->param_count + fsig->hasthis + 1;
  9696| 			CHECK_STACK (n);
  9697| 			sp -= n;
  9698| 			arg = sp [n - 1];
  9699| 			if (cfg->llvm_only) {
  9700| 				/*
  9701| 				 * The lowest bit of 'arg' determines whenever the callee uses the gsharedvt
  9702| 				 * cconv. This is set by mono_init_delegate ().
  9703| 				 */
  9704| 				if (cfg->gsharedvt && mini_is_gsharedvt_variable_signature (fsig)) {
  9705| 					MonoInst *callee = addr;
  9706| 					MonoInst *call, *localloc_ins;
  9707| 					MonoBasicBlock *is_gsharedvt_bb, *end_bb;
  9708| 					int low_bit_reg = alloc_preg (cfg);
  9709| 					NEW_BBLOCK (cfg, is_gsharedvt_bb);
  9710| 					NEW_BBLOCK (cfg, end_bb);
  9711| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_PAND_IMM, low_bit_reg, arg->dreg, 1);
  9712| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, low_bit_reg, 0);
  9713| 					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBNE_UN, is_gsharedvt_bb);
  9714| 					/* Normal case: callee uses a normal cconv, have to add an out wrapper */
  9715| 					addr = emit_get_rgctx_sig (cfg, context_used,
  9716| 											   fsig, MONO_RGCTX_INFO_SIG_GSHAREDVT_OUT_TRAMPOLINE_CALLI);
  9717| 					/*
  9718| 					 * ADDR points to a gsharedvt-out wrapper, have to pass <callee, arg> as an extra arg.
  9719| 					 */
  9720| 					MONO_INST_NEW (cfg, ins, OP_LOCALLOC_IMM);
  9721| 					ins->dreg = alloc_preg (cfg);
  9722| 					ins->inst_imm = 2 * TARGET_SIZEOF_VOID_P;
  9723| 					MONO_ADD_INS (cfg->cbb, ins);
  9724| 					localloc_ins = ins;
  9725| 					cfg->flags |= MONO_CFG_HAS_ALLOCA;
  9726| 					MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, localloc_ins->dreg, 0, callee->dreg);
  9727| 					MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, localloc_ins->dreg, TARGET_SIZEOF_VOID_P, arg->dreg);
  9728| 					call = mini_emit_extra_arg_calli (cfg, fsig, sp, localloc_ins->dreg, addr);
  9729| 					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  9730| 					/* Gsharedvt case: callee uses a gsharedvt cconv, no conversion is needed */
  9731| 					MONO_START_BB (cfg, is_gsharedvt_bb);
  9732| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_PXOR_IMM, arg->dreg, arg->dreg, 1);
  9733| 					ins = mini_emit_extra_arg_calli (cfg, fsig, sp, arg->dreg, callee);
  9734| 					ins->dreg = call->dreg;
  9735| 					MONO_START_BB (cfg, end_bb);
  9736| 				} else {
  9737| 					/* Caller uses a normal calling conv */
  9738| 					MonoInst *callee = addr;
  9739| 					MonoInst *call, *localloc_ins;
  9740| 					MonoBasicBlock *is_gsharedvt_bb, *end_bb;
  9741| 					int low_bit_reg = alloc_preg (cfg);
  9742| 					NEW_BBLOCK (cfg, is_gsharedvt_bb);
  9743| 					NEW_BBLOCK (cfg, end_bb);
  9744| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_PAND_IMM, low_bit_reg, arg->dreg, 1);
  9745| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, low_bit_reg, 0);
  9746| 					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBNE_UN, is_gsharedvt_bb);
  9747| 					/* Normal case: callee uses a normal cconv, no conversion is needed */
  9748| 					call = mini_emit_extra_arg_calli (cfg, fsig, sp, arg->dreg, callee);
  9749| 					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  9750| 					/* Gsharedvt case: callee uses a gsharedvt cconv, have to add an in wrapper */
  9751| 					MONO_START_BB (cfg, is_gsharedvt_bb);
  9752| 					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_PXOR_IMM, arg->dreg, arg->dreg, 1);
  9753| 					NEW_AOTCONST (cfg, addr, MONO_PATCH_INFO_GSHAREDVT_IN_WRAPPER, fsig);
  9754| 					MONO_ADD_INS (cfg->cbb, addr);
  9755| 					/*
  9756| 					 * ADDR points to a gsharedvt-in wrapper, have to pass <callee, arg> as an extra arg.
  9757| 					 */
  9758| 					MONO_INST_NEW (cfg, ins, OP_LOCALLOC_IMM);
  9759| 					ins->dreg = alloc_preg (cfg);
  9760| 					ins->inst_imm = 2 * TARGET_SIZEOF_VOID_P;
  9761| 					MONO_ADD_INS (cfg->cbb, ins);
  9762| 					localloc_ins = ins;
  9763| 					cfg->flags |= MONO_CFG_HAS_ALLOCA;
  9764| 					MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, localloc_ins->dreg, 0, callee->dreg);
  9765| 					MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, localloc_ins->dreg, TARGET_SIZEOF_VOID_P, arg->dreg);
  9766| 					ins = mini_emit_extra_arg_calli (cfg, fsig, sp, localloc_ins->dreg, addr);
  9767| 					ins->dreg = call->dreg;
  9768| 					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
  9769| 					MONO_START_BB (cfg, end_bb);
  9770| 				}
  9771| 			} else {
  9772| 				/* Same as CEE_CALLI */
  9773| 				if (cfg->gsharedvt && mini_is_gsharedvt_signature (fsig)) {
  9774| 					/*
  9775| 					 * We pass the address to the gsharedvt trampoline in the rgctx reg
  9776| 					 */
  9777| 					MonoInst *callee = addr;
  9778| 					addr = emit_get_rgctx_sig (cfg, context_used,
  9779| 											   fsig, MONO_RGCTX_INFO_SIG_GSHAREDVT_OUT_TRAMPOLINE_CALLI);
  9780| 					ins = (MonoInst*)mini_emit_calli (cfg, fsig, sp, addr, NULL, callee);
  9781| 				} else {
  9782| 					ins = (MonoInst*)mini_emit_calli (cfg, fsig, sp, addr, NULL, NULL);
  9783| 				}
  9784| 			}
  9785| 			if (!MONO_TYPE_IS_VOID (fsig->ret))
  9786| 				*sp++ = mono_emit_widen_call_res (cfg, ins, fsig);
  9787| 			CHECK_CFG_EXCEPTION;
  9788| 			ins_flag = 0;
  9789| 			constrained_class = NULL;
  9790| 			break;
  9791| 		}
  9792| 		case MONO_CEE_MONO_LDDOMAIN: {
  9793| 			MonoDomain *domain = mono_get_root_domain ();
  9794| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9795| 			EMIT_NEW_PCONST (cfg, ins, cfg->compile_aot ? NULL : domain);
  9796| 			*sp++ = ins;
  9797| 			break;
  9798| 		}
  9799| 		case MONO_CEE_MONO_SAVE_LAST_ERROR:
  9800| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9801| 			save_last_error = TRUE;
  9802| 			break;
  9803| 		case MONO_CEE_MONO_GET_RGCTX_ARG:
  9804| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9805| 			mono_create_rgctx_var (cfg);
  9806| 			MONO_INST_NEW (cfg, ins, OP_MOVE);
  9807| 			ins->dreg = alloc_dreg (cfg, STACK_PTR);
  9808| 			ins->sreg1 = cfg->rgctx_var->dreg;
  9809| 			ins->type = STACK_PTR;
  9810| 			MONO_ADD_INS (cfg->cbb, ins);
  9811| 			*sp++ = ins;
  9812| 			break;
  9813| 		case MONO_CEE_MONO_GET_SP: {
  9814| 			/* Used by COOP only, so this is good enough */
  9815| 			MonoInst *var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
  9816| 			EMIT_NEW_VARLOADA (cfg, ins, var, NULL);
  9817| 			*sp++ = ins;
  9818| 			break;
  9819| 		}
  9820| 		case MONO_CEE_MONO_REMAP_OVF_EXC:
  9821| 			/* Remap the exception thrown by the next _OVF opcode */
  9822| 			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
  9823| 			ovf_exc = (const char*)mono_method_get_wrapper_data (method, token);
  9824| 			break;
  9825| 		case MONO_CEE_ARGLIST: {
  9826| 			/* somewhat similar to LDTOKEN */
  9827| 			MonoInst *addr, *vtvar;
  9828| 			vtvar = mono_compile_create_var (cfg, m_class_get_byval_arg (mono_defaults.argumenthandle_class), OP_LOCAL);
  9829| 			EMIT_NEW_TEMPLOADA (cfg, addr, vtvar->inst_c0);
  9830| 			EMIT_NEW_UNALU (cfg, ins, OP_ARGLIST, -1, addr->dreg);
  9831| 			EMIT_NEW_TEMPLOAD (cfg, ins, vtvar->inst_c0);
  9832| 			ins->type = STACK_VTYPE;
  9833| 			ins->klass = mono_defaults.argumenthandle_class;
  9834| 			*sp++ = ins;
  9835| 			break;
  9836| 		}
  9837| 		case MONO_CEE_CEQ:
  9838| 		case MONO_CEE_CGT:
  9839| 		case MONO_CEE_CGT_UN:
  9840| 		case MONO_CEE_CLT:
  9841| 		case MONO_CEE_CLT_UN: {
  9842| 			MonoInst *cmp, *arg1, *arg2;
  9843| 			sp -= 2;
  9844| 			arg1 = sp [0];
  9845| 			arg2 = sp [1];
  9846| 			/*
  9847| 			 * The following transforms:
  9848| 			 *    CEE_CEQ    into OP_CEQ
  9849| 			 *    CEE_CGT    into OP_CGT
  9850| 			 *    CEE_CGT_UN into OP_CGT_UN
  9851| 			 *    CEE_CLT    into OP_CLT
  9852| 			 *    CEE_CLT_UN into OP_CLT_UN
  9853| 			 */
  9854| 			MONO_INST_NEW (cfg, cmp, (OP_CEQ - CEE_CEQ) + ip [1]);
  9855| 			MONO_INST_NEW (cfg, ins, cmp->opcode);
  9856| 			cmp->sreg1 = arg1->dreg;
  9857| 			cmp->sreg2 = arg2->dreg;
  9858| 			type_from_op (cfg, cmp, arg1, arg2);
  9859| 			CHECK_TYPE (cmp);
  9860| 			add_widen_op (cfg, cmp, &arg1, &arg2);
  9861| 			if ((arg1->type == STACK_I8) || ((TARGET_SIZEOF_VOID_P == 8) && ((arg1->type == STACK_PTR) || (arg1->type == STACK_OBJ) || (arg1->type == STACK_MP))))
  9862| 				cmp->opcode = OP_LCOMPARE;
  9863| 			else if (arg1->type == STACK_R4)
  9864| 				cmp->opcode = OP_RCOMPARE;
  9865| 			else if (arg1->type == STACK_R8)
  9866| 				cmp->opcode = OP_FCOMPARE;
  9867| 			else
  9868| 				cmp->opcode = OP_ICOMPARE;
  9869| 			MONO_ADD_INS (cfg->cbb, cmp);
  9870| 			ins->type = STACK_I4;
  9871| 			ins->dreg = alloc_dreg (cfg, (MonoStackType)ins->type);
  9872| 			type_from_op (cfg, ins, arg1, arg2);
  9873| 			if (cmp->opcode == OP_FCOMPARE || cmp->opcode == OP_RCOMPARE) {
  9874| 				/*
  9875| 				 * The backends expect the fceq opcodes to do the
  9876| 				 * comparison too.
  9877| 				 */
  9878| 				ins->sreg1 = cmp->sreg1;
  9879| 				ins->sreg2 = cmp->sreg2;
  9880| 				NULLIFY_INS (cmp);
  9881| 			}
  9882| 			MONO_ADD_INS (cfg->cbb, ins);
  9883| 			*sp++ = ins;
  9884| 			break;
  9885| 		}
  9886| 		case MONO_CEE_LDFTN: {
  9887| 			MonoInst *argconst;
  9888| 			MonoMethod *cil_method;
  9889| 			gboolean gshared_static_virtual = FALSE;
  9890| 			cil_method = cmethod = mini_get_method (cfg, method, n, NULL, generic_context);
  9891| 			CHECK_CFG_ERROR;
  9892| 			if (!dont_verify && !cfg->skip_visibility && !mono_method_can_access_method (method, cmethod))
  9893| 				emit_method_access_failure (cfg, method, cil_method);
  9894| 			if (constrained_class) {
  9895| 				if (m_method_is_static (cmethod) && mini_class_check_context_used (cfg, constrained_class)) {
  9896| 					gshared_static_virtual = TRUE;
  9897| 				} else {
  9898| 					cmethod = get_constrained_method (cfg, image, n, cmethod, constrained_class, generic_context);
  9899| 					CHECK_CFG_ERROR;
  9900| 					if (mono_class_has_dim_conflicts (constrained_class) &&
  9901| 							mono_class_is_method_ambiguous (constrained_class, cil_method))
  9902| 						mono_emit_jit_icall (cfg, mono_throw_ambiguous_implementation, NULL);
  9903| 					constrained_class = NULL;
  9904| 				}
  9905| 			} else {
  9906| 				mono_save_token_info (cfg, image, n, cmethod);
  9907| 			}
  9908| 			mono_class_init_internal (cmethod->klass);
  9909| 			context_used = mini_method_check_context_used (cfg, cmethod);
  9910| 			const gboolean has_unmanaged_callers_only =
  9911| 				cmethod->wrapper_type == MONO_WRAPPER_NONE &&
  9912| 				mono_method_has_unmanaged_callers_only_attribute (cmethod);
  9913| 			/*
  9914| 			 * Optimize the common case of ldftn+delegate creation
  9915| 			 */
  9916| 			if (!gshared_static_virtual && (sp > stack_start) && (next_ip + 4 < end) && ip_in_bb (cfg, cfg->cbb, next_ip) && (next_ip [0] == CEE_NEWOBJ)) {
  9917| 				MonoMethod *ctor_method = mini_get_method (cfg, method, read32 (next_ip + 1), NULL, generic_context);
  9918| 				if (ctor_method && (m_class_get_parent (ctor_method->klass) == mono_defaults.multicastdelegate_class)) {
  9919| 					MonoInst *target_ins, *handle_ins;
  9920| 					MonoMethod *invoke;
  9921| 					int invoke_context_used;
  9922| 					if (G_UNLIKELY (has_unmanaged_callers_only)) {
  9923| 						mono_error_set_not_supported (cfg->error, "Cannot create delegate from method with UnmanagedCallersOnlyAttribute");
  9924| 						CHECK_CFG_ERROR;
  9925| 					}
  9926| 					invoke = mono_get_delegate_invoke_internal (ctor_method->klass);
  9927| 					if (!invoke || !mono_method_signature_internal (invoke))
  9928| 						LOAD_ERROR;
  9929| 					invoke_context_used = mini_method_check_context_used (cfg, invoke);
  9930| 					target_ins = sp [-1];
  9931| 					if (!(cmethod->flags & METHOD_ATTRIBUTE_STATIC)) {
  9932| 						/*BAD IMPL: We must not add a null check for virtual invoke delegates.*/
  9933| 						if (mono_method_signature_internal (invoke)->param_count == mono_method_signature_internal (cmethod)->param_count) {
  9934| 							MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, target_ins->dreg, 0);
  9935| 							MONO_EMIT_NEW_COND_EXC (cfg, EQ, "ArgumentException");
  9936| 						}
  9937| 					}
  9938| 					if ((invoke_context_used == 0 || !cfg->gsharedvt) || cfg->llvm_only) {
  9939| 						if (cfg->verbose_level > 3)
  9940| 							g_print ("converting (in B%d: stack: %d) %s", cfg->cbb->block_num, GPTRDIFF_TO_INT (sp - stack_start), mono_disasm_code_one (NULL, method, ip + 6, NULL));
  9941| 						if ((handle_ins = handle_delegate_ctor (cfg, ctor_method->klass, target_ins, cmethod, context_used, invoke_context_used, FALSE))) {
  9942| 							sp --;
  9943| 							*sp = handle_ins;
  9944| 							CHECK_CFG_EXCEPTION;
  9945| 							sp ++;
  9946| 							next_ip += 5;
  9947| 							il_op = MONO_CEE_NEWOBJ;
  9948| 							break;
  9949| 						} else {
  9950| 							CHECK_CFG_ERROR;
  9951| 						}
  9952| 					}
  9953| 				}
  9954| 			}
  9955| 			/* UnmanagedCallersOnlyAttribute means ldftn should return a method callable from native */
  9956| 			if (G_UNLIKELY (has_unmanaged_callers_only)) {
  9957| 				if (G_UNLIKELY (cmethod->flags & METHOD_ATTRIBUTE_PINVOKE_IMPL)) {
  9958| 					emit_not_supported_failure (cfg);
  9959| 					EMIT_NEW_PCONST (cfg, ins, NULL);
  9960| 					*sp++ = ins;
  9961| 					inline_costs += CALL_COST * MIN(10, num_calls++);
  9962| 					break;
  9963| 				}
  9964| 				MonoClass *delegate_klass = NULL;
  9965| 				MonoGCHandle target_handle = 0;
  9966| 				ERROR_DECL (wrapper_error);
  9967| 				MonoMethod *wrapped_cmethod;
  9968| 				wrapped_cmethod = mono_marshal_get_managed_wrapper (cmethod, delegate_klass, target_handle, wrapper_error);
  9969| 				if (!is_ok (wrapper_error)) {
  9970| 					/* if we couldn't create a wrapper because cmethod isn't supposed to have an
  9971| 					UnmanagedCallersOnly attribute, follow CoreCLR behavior and throw when the
  9972| 					method with the ldftn is executing, not when it is being compiled. */
  9973| 					emit_invalid_program_with_msg (cfg, wrapper_error, method, cmethod);
  9974| 					mono_error_cleanup (wrapper_error);
  9975| 					EMIT_NEW_PCONST (cfg, ins, NULL);
  9976| 					*sp++ = ins;
  9977| 					inline_costs += CALL_COST * MIN(10, num_calls++);
  9978| 					break;
  9979| 				} else {
  9980| 					cmethod = wrapped_cmethod;
  9981| 				}
  9982| 			}
  9983| 			if (gshared_static_virtual) {
  9984| 				argconst = emit_get_rgctx_virt_method (cfg, -1, constrained_class, cmethod, MONO_RGCTX_INFO_VIRT_METHOD);
  9985| 				constrained_class = NULL;
  9986| 			} else {
  9987| 				argconst = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_METHOD);
  9988| 			}
  9989| 			ins = mono_emit_jit_icall (cfg, mono_ldftn, &argconst);
  9990| 			*sp++ = ins;
  9991| 			inline_costs += CALL_COST * MIN(10, num_calls++);
  9992| 			break;
  9993| 		}
  9994| 		case MONO_CEE_LDVIRTFTN: {
  9995| 			MonoInst *args [2];
  9996| 			cmethod = mini_get_method (cfg, method, n, NULL, generic_context);
  9997| 			CHECK_CFG_ERROR;
  9998| 			mono_class_init_internal (cmethod->klass);
  9999| 			context_used = mini_method_check_context_used (cfg, cmethod);
 10000| 			/*
 10001| 			 * Optimize the common case of ldvirtftn+delegate creation
 10002| 			 */
 10003| 			if (previous_il_op == MONO_CEE_DUP && (sp > stack_start) && (next_ip + 4 < end) && ip_in_bb (cfg, cfg->cbb, next_ip) && (next_ip [0] == CEE_NEWOBJ)) {
 10004| 				MonoMethod *ctor_method = mini_get_method (cfg, method, read32 (next_ip + 1), NULL, generic_context);
 10005| 				if (ctor_method && (m_class_get_parent (ctor_method->klass) == mono_defaults.multicastdelegate_class)) {
 10006| 					MonoInst *target_ins, *handle_ins;
 10007| 					MonoMethod *invoke;
 10008| 					int invoke_context_used;
 10009| 					const gboolean is_virtual = (cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL) != 0;
 10010| 					invoke = mono_get_delegate_invoke_internal (ctor_method->klass);
 10011| 					if (!invoke || !mono_method_signature_internal (invoke))
 10012| 						LOAD_ERROR;
 10013| 					invoke_context_used = mini_method_check_context_used (cfg, invoke);
 10014| 					target_ins = sp [-1];
 10015| 					if (invoke_context_used == 0 || !cfg->gsharedvt || cfg->llvm_only) {
 10016| 						if (cfg->verbose_level > 3)
 10017| 							g_print ("converting (in B%d: stack: %d) %s", cfg->cbb->block_num, GPTRDIFF_TO_INT (sp - stack_start), mono_disasm_code_one (NULL, method, ip + 6, NULL));
 10018| 						if ((handle_ins = handle_delegate_ctor (cfg, ctor_method->klass, target_ins, cmethod, context_used, invoke_context_used, is_virtual))) {
 10019| 							sp -= 2;
 10020| 							*sp = handle_ins;
 10021| 							CHECK_CFG_EXCEPTION;
 10022| 							next_ip += 5;
 10023| 							previous_il_op = MONO_CEE_NEWOBJ;
 10024| 							sp ++;
 10025| 							break;
 10026| 						} else {
 10027| 							CHECK_CFG_ERROR;
 10028| 						}
 10029| 					}
 10030| 				}
 10031| 			}
 10032| 			--sp;
 10033| 			args [0] = *sp;
 10034| 			args [1] = emit_get_rgctx_method (cfg, context_used,
 10035| 											  cmethod, MONO_RGCTX_INFO_METHOD);
 10036| 			if (context_used)
 10037| 				*sp++ = mono_emit_jit_icall (cfg, mono_ldvirtfn_gshared, args);
 10038| 			else
 10039| 				*sp++ = mono_emit_jit_icall (cfg, mono_ldvirtfn, args);
 10040| 			inline_costs += CALL_COST * MIN(10, num_calls++);
 10041| 			break;
 10042| 		}
 10043| 		case MONO_CEE_LOCALLOC: {
 10044| 			MonoBasicBlock *non_zero_bb, *end_bb;
 10045| 			int alloc_ptr = alloc_preg (cfg);
 10046| 			--sp;
 10047| 			if (sp != stack_start)
 10048| 				UNVERIFIED;
 10049| 			if (cfg->method != method)
 10050| 				/*
 10051| 				 * Inlining this into a loop in a parent could lead to
 10052| 				 * stack overflows which is different behavior than the
 10053| 				 * non-inlined case, thus disable inlining in this case.
 10054| 				 */
 10055| 				INLINE_FAILURE("localloc");
 10056| 			NEW_BBLOCK (cfg, non_zero_bb);
 10057| 			NEW_BBLOCK (cfg, end_bb);
 10058| 			/* if size != zero */
 10059| 			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, sp [0]->dreg, 0);
 10060| 			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBNE_UN, non_zero_bb);
 10061| 			MONO_EMIT_NEW_PCONST (cfg, alloc_ptr, NULL);
 10062| 			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
 10063| 			MONO_START_BB (cfg, non_zero_bb);
 10064| 			MONO_INST_NEW (cfg, ins, OP_LOCALLOC);
 10065| 			ins->dreg = alloc_ptr;
 10066| 			ins->sreg1 = sp [0]->dreg;
 10067| 			ins->type = STACK_PTR;
 10068| 			MONO_ADD_INS (cfg->cbb, ins);
 10069| 			cfg->flags |= MONO_CFG_HAS_ALLOCA;
 10070| 			if (header->init_locals)
 10071| 				ins->flags |= MONO_INST_INIT;
 10072| 			MONO_START_BB (cfg, end_bb);
 10073| 			EMIT_NEW_UNALU (cfg, ins, OP_MOVE, alloc_preg (cfg), alloc_ptr);
 10074| 			ins->type = STACK_PTR;
 10075| 			*sp++ = ins;
 10076| 			break;
 10077| 		}
 10078| 		case MONO_CEE_ENDFILTER: {
 10079| 			MonoExceptionClause *clause, *nearest;
 10080| 			--sp;
 10081| 			if ((sp != stack_start) || (sp [0]->type != STACK_I4))
 10082| 				UNVERIFIED;
 10083| 			MONO_INST_NEW (cfg, ins, OP_ENDFILTER);
 10084| 			ins->sreg1 = (*sp)->dreg;
 10085| 			MONO_ADD_INS (cfg->cbb, ins);
 10086| 			start_new_bblock = 1;
 10087| 			nearest = NULL;
 10088| 			for (guint cc = 0; cc < header->num_clauses; ++cc) {
 10089| 				clause = &header->clauses [cc];
 10090| 				if ((clause->flags & MONO_EXCEPTION_CLAUSE_FILTER) &&
 10091| 					(GPTRDIFF_TO_UINT32(next_ip - header->code) > clause->data.filter_offset && GPTRDIFF_TO_UINT32(next_ip - header->code) <= clause->handler_offset) &&
 10092| 				    (!nearest || (clause->data.filter_offset < nearest->data.filter_offset)))
 10093| 					nearest = clause;
 10094| 			}
 10095| 			g_assert (nearest);
 10096| 			if ((next_ip - header->code) != nearest->handler_offset)
 10097| 				UNVERIFIED;
 10098| 			break;
 10099| 		}
 10100| 		case MONO_CEE_UNALIGNED_:
 10101| 			ins_flag |= MONO_INST_UNALIGNED;
 10102| 			/* FIXME: record alignment? we can assume 1 for now */
 10103| 			break;
 10104| 		case MONO_CEE_VOLATILE_:
 10105| 			ins_flag |= MONO_INST_VOLATILE;
 10106| 			break;
 10107| 		case MONO_CEE_TAIL_:
 10108| 			ins_flag   |= MONO_INST_TAILCALL;
 10109| 			cfg->flags |= MONO_CFG_HAS_TAILCALL;
 10110| 			/* Can't inline tailcalls at this time */
 10111| 			inline_costs += 100000;
 10112| 			break;
 10113| 		case MONO_CEE_INITOBJ:
 10114| 			--sp;
 10115| 			klass = mini_get_class (method, token, generic_context);
 10116| 			CHECK_TYPELOAD (klass);
 10117| 			if (mini_class_is_reference (klass))
 10118| 				MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STORE_MEMBASE_IMM, sp [0]->dreg, 0, 0);
 10119| 			else
 10120| 				mini_emit_initobj (cfg, *sp, NULL, klass);
 10121| 			inline_costs += 1;
 10122| 			break;
 10123| 		case MONO_CEE_CONSTRAINED_:
 10124| 			constrained_class = mini_get_class (method, token, generic_context);
 10125| 			CHECK_TYPELOAD (constrained_class);
 10126| 			ins_has_side_effect = FALSE;
 10127| 			break;
 10128| 		case MONO_CEE_CPBLK:
 10129| 			sp -= 3;
 10130| 			mini_emit_memory_copy_bytes (cfg, sp [0], sp [1], sp [2], ins_flag);
 10131| 			ins_flag = 0;
 10132| 			inline_costs += 1;
 10133| 			break;
 10134| 		case MONO_CEE_INITBLK:
 10135| 			sp -= 3;
 10136| 			mini_emit_memory_init_bytes (cfg, sp [0], sp [1], sp [2], ins_flag);
 10137| 			ins_flag = 0;
 10138| 			inline_costs += 1;
 10139| 			break;
 10140| 		case MONO_CEE_NO_:
 10141| 			if (ip [2] & CEE_NO_TYPECHECK)
 10142| 				ins_flag |= MONO_INST_NOTYPECHECK;
 10143| 			if (ip [2] & CEE_NO_RANGECHECK)
 10144| 				ins_flag |= MONO_INST_NORANGECHECK;
 10145| 			if (ip [2] & CEE_NO_NULLCHECK)
 10146| 				ins_flag |= MONO_INST_NONULLCHECK;
 10147| 			break;
 10148| 		case MONO_CEE_RETHROW: {
 10149| 			MonoInst *load;
 10150| 			int handler_offset = -1;
 10151| 			for (unsigned int i = 0; i < header->num_clauses; ++i) {
 10152| 				MonoExceptionClause *clause = &header->clauses [i];
 10153| 				if (MONO_OFFSET_IN_HANDLER (clause, GPTRDIFF_TO_UINT32(ip - header->code)) && !(clause->flags & MONO_EXCEPTION_CLAUSE_FINALLY)) {
 10154| 					handler_offset = clause->handler_offset;
 10155| 					break;
 10156| 				}
 10157| 			}
 10158| 			cfg->cbb->flags |= BB_EXCEPTION_UNSAFE;
 10159| 			if (handler_offset == -1)
 10160| 				UNVERIFIED;
 10161| 			EMIT_NEW_TEMPLOAD (cfg, load, mono_find_exvar_for_offset (cfg, handler_offset)->inst_c0);
 10162| 			MONO_INST_NEW (cfg, ins, OP_RETHROW);
 10163| 			ins->sreg1 = load->dreg;
 10164| 			MONO_ADD_INS (cfg->cbb, ins);
 10165| 			MONO_INST_NEW (cfg, ins, OP_NOT_REACHED);
 10166| 			MONO_ADD_INS (cfg->cbb, ins);
 10167| 			sp = stack_start;
 10168| 			link_bblock (cfg, cfg->cbb, end_bblock);
 10169| 			start_new_bblock = 1;
 10170| 			break;
 10171| 		}
 10172| 		case MONO_CEE_MONO_RETHROW: {
 10173| 			if (sp [-1]->type != STACK_OBJ)
 10174| 				UNVERIFIED;
 10175| 			MONO_INST_NEW (cfg, ins, OP_RETHROW);
 10176| 			--sp;
 10177| 			ins->sreg1 = sp [0]->dreg;
 10178| 			cfg->cbb->out_of_line = TRUE;
 10179| 			MONO_ADD_INS (cfg->cbb, ins);
 10180| 			MONO_INST_NEW (cfg, ins, OP_NOT_REACHED);
 10181| 			MONO_ADD_INS (cfg->cbb, ins);
 10182| 			sp = stack_start;
 10183| 			link_bblock (cfg, cfg->cbb, end_bblock);
 10184| 			start_new_bblock = 1;
 10185| 			/* This can complicate code generation for llvm since the return value might not be defined */
 10186| 			if (COMPILE_LLVM (cfg))
 10187| 				INLINE_FAILURE ("mono_rethrow");
 10188| 			break;
 10189| 		}
 10190| 		case MONO_CEE_SIZEOF: {
 10191| 			guint32 val;
 10192| 			int ialign;
 10193| 			if (mono_metadata_token_table (token) == MONO_TABLE_TYPESPEC && !image_is_dynamic (m_class_get_image (method->klass)) && !generic_context) {
 10194| 				MonoType *type = mono_type_create_from_typespec_checked (image, token, cfg->error);
 10195| 				CHECK_CFG_ERROR;
 10196| 				val = mono_type_size (type, &ialign);
 10197| 				EMIT_NEW_ICONST (cfg, ins, val);
 10198| 			} else {
 10199| 				klass = mini_get_class (method, token, generic_context);
 10200| 				CHECK_TYPELOAD (klass);
 10201| 				if (mini_is_gsharedvt_klass (klass)) {
 10202| 					ins = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_CLASS_SIZEOF);
 10203| 					ins->type = STACK_I4;
 10204| 				} else {
 10205| 					val = mono_type_size (m_class_get_byval_arg (klass), &ialign);
 10206| 					EMIT_NEW_ICONST (cfg, ins, val);
 10207| 				}
 10208| 			}
 10209| 			*sp++ = ins;
 10210| 			break;
 10211| 		}
 10212| 		case MONO_CEE_REFANYTYPE: {
 10213| 			MonoInst *src_var, *src;
 10214| 			GSHAREDVT_FAILURE (il_op);
 10215| 			--sp;
 10216| 			src_var = get_vreg_to_inst (cfg, sp [0]->dreg);
 10217| 			if (!src_var)
 10218| 				src_var = mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (mono_defaults.typed_reference_class), OP_LOCAL, sp [0]->dreg);
 10219| 			EMIT_NEW_VARLOADA (cfg, src, src_var, src_var->inst_vtype);
 10220| 			EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (mono_defaults.typehandle_class), src->dreg, MONO_STRUCT_OFFSET (MonoTypedRef, type));
 10221| 			*sp++ = ins;
 10222| 			break;
 10223| 		}
 10224| 		case MONO_CEE_READONLY_:
 10225| 			readonly = TRUE;
 10226| 			break;
 10227| 		case MONO_CEE_UNUSED56:
 10228| 		case MONO_CEE_UNUSED57:
 10229| 		case MONO_CEE_UNUSED70:
 10230| 		case MONO_CEE_UNUSED:
 10231| 		case MONO_CEE_UNUSED99:
 10232| 		case MONO_CEE_UNUSED58:
 10233| 		case MONO_CEE_UNUSED1:
 10234| 			UNVERIFIED;
 10235| 		default:
 10236| 			g_warning ("opcode 0x%02x not handled", il_op);
 10237| 			UNVERIFIED;
 10238| 		}
 10239| 		if (ins_has_side_effect)
 10240| 			cfg->cbb->flags |= BB_HAS_SIDE_EFFECTS;
 10241| 	}
 10242| 	if (start_new_bblock != 1)
 10243| 		UNVERIFIED;
 10244| 	cfg->cbb->cil_length = GPTRDIFF_TO_INT32 (ip - cfg->cbb->cil_code);
 10245| 	if (cfg->cbb->next_bb) {
 10246| 		/* This could already be set because of inlining, #693905 */
 10247| 		MonoBasicBlock *cbb = cfg->cbb;
 10248| 		while (cbb->next_bb)
 10249| 			cbb = cbb->next_bb;
 10250| 		cbb->next_bb = end_bblock;
 10251| 	} else {
 10252| 		cfg->cbb->next_bb = end_bblock;
 10253| 	}
 10254| #if defined(TARGET_POWERPC) || defined(TARGET_X86)
 10255| 	if (cfg->compile_aot)
 10256| 		/* FIXME: The plt slots require a GOT var even if the method doesn't use it */
 10257| 		mono_get_got_var (cfg);
 10258| #endif
 10259| #ifdef TARGET_WASM
 10260| 	if (cfg->lmf_var && !cfg->deopt) {
 10261| 		cfg->cbb = init_localsbb;
 10262| 		EMIT_NEW_VARLOADA (cfg, ins, cfg->lmf_var, NULL);
 10263| 		int lmf_reg = ins->dreg;
 10264| 		EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_IMM, lmf_reg, MONO_STRUCT_OFFSET (MonoLMF, previous_lmf), 0);
 10265| 	}
 10266| #endif
 10267| 	if (cfg->method == method && cfg->got_var)
 10268| 		mono_emit_load_got_addr (cfg);
 10269| 	if (init_localsbb) {
 10270| 		cfg->cbb = init_localsbb;
 10271| 		cfg->ip = NULL;
 10272| 		for (int i = 0; i < header->num_locals; ++i) {
 10273| 			/*
 10274| 			 * Vtype initialization might need to be done after CEE_JIT_ATTACH, since it can make calls to memset (),
 10275| 			 * which need the trampoline code to work.
 10276| 			 */
 10277| 			if (MONO_TYPE_ISSTRUCT (header->locals [i]))
 10278| 				cfg->cbb = init_localsbb2;
 10279| 			else
 10280| 				cfg->cbb = init_localsbb;
 10281| 			emit_init_local (cfg, i, header->locals [i], init_locals);
 10282| 		}
 10283| 	}
 10284| 	if (cfg->init_ref_vars && cfg->method == method) {
 10285| 		/* Emit initialization for ref vars */
 10286| 		for (guint i = 0; i < cfg->num_varinfo; ++i) {
 10287| 			MonoInst *var_ins = cfg->varinfo [i];
 10288| 			if (var_ins->opcode == OP_LOCAL && var_ins->type == STACK_OBJ)
 10289| 				MONO_EMIT_NEW_PCONST (cfg, var_ins->dreg, NULL);
 10290| 		}
 10291| 	}
 10292| 	if (cfg->lmf_var && cfg->method == method && !cfg->llvm_only) {
 10293| 		cfg->cbb = init_localsbb;
 10294| 		emit_push_lmf (cfg);
 10295| 	}
 10296| 	/* emit profiler enter code after a jit attach if there is one */
 10297| 	cfg->cbb = init_localsbb2;
 10298| 	mini_profiler_emit_enter (cfg);
 10299| 	cfg->cbb = init_localsbb;
 10300| 	if (seq_points) {
 10301| 		MonoBasicBlock *cbb;
 10302| 		/*
 10303| 		 * Make seq points at backward branch targets interruptible.
 10304| 		 */
 10305| 		for (cbb = cfg->bb_entry; cbb; cbb = cbb->next_bb)
 10306| 			if (cbb->code && cbb->in_count > 1 && cbb->code->opcode == OP_SEQ_POINT)
 10307| 				cbb->code->flags |= MONO_INST_SINGLE_STEP_LOC;
 10308| 	}
 10309| 	/* Add a sequence point for method entry/exit events */
 10310| 	if (seq_points && cfg->gen_sdb_seq_points) {
 10311| 		NEW_SEQ_POINT (cfg, ins, METHOD_ENTRY_IL_OFFSET, FALSE);
 10312| 		MONO_ADD_INS (init_localsbb, ins);
 10313| 		NEW_SEQ_POINT (cfg, ins, METHOD_EXIT_IL_OFFSET, FALSE);
 10314| 		MONO_ADD_INS (cfg->bb_exit, ins);
 10315| 	}
 10316| 	/*
 10317| 	 * Add seq points for IL offsets which have line number info, but wasn't generated a seq point during JITting because
 10318| 	 * the code they refer to was dead (#11880).
 10319| 	 */
 10320| 	if (sym_seq_points) {
 10321| 		for (guint32 i = 0; i < header->code_size; ++i) {
 10322| 			if (mono_bitset_test_fast (seq_point_locs, i) && !mono_bitset_test_fast (seq_point_set_locs, i)) {
 10323| 				MonoInst *seq_point_ins;
 10324| 				NEW_SEQ_POINT (cfg, seq_point_ins, i, FALSE);
 10325| 				mono_add_seq_point (cfg, NULL, seq_point_ins, SEQ_POINT_NATIVE_OFFSET_DEAD_CODE);
 10326| 			}
 10327| 		}
 10328| 	}
 10329| 	cfg->ip = NULL;
 10330| 	if (cfg->method == method) {
 10331| 		compute_bb_regions (cfg);
 10332| 	} else {
 10333| 		MonoBasicBlock *cbb;
 10334| 		/* get_most_deep_clause () in mini-llvm.c depends on this for inlined bblocks */
 10335| 		for (cbb = start_bblock; cbb != end_bblock; cbb  = cbb->next_bb) {
 10336| 			cbb->real_offset = inline_offset;
 10337| 		}
 10338| 	}
 10339| 	if (inline_costs < 0) {
 10340| 		char *mname;
 10341| 		/* Method is too large */
 10342| 		mname = mono_method_full_name (method, TRUE);
 10343| 		mono_cfg_set_exception_invalid_program (cfg, g_strdup_printf ("Method %s is too complex.", mname));
 10344| 		g_free (mname);
 10345| 	}
 10346| 	if ((cfg->verbose_level > 2) && (cfg->method == method))
 10347| 		mono_print_code (cfg, "AFTER METHOD-TO-IR");
 10348| 	goto cleanup;
 10349| mono_error_exit:
 10350| 	if (cfg->verbose_level > 3)
 10351| 		g_print ("exiting due to error\n");
 10352| 	g_assert (!is_ok (cfg->error));
 10353| 	goto cleanup;
 10354|  exception_exit:
 10355| 	if (cfg->verbose_level > 3)
 10356| 		g_print ("exiting due to exception\n");
 10357| 	g_assert (cfg->exception_type != MONO_EXCEPTION_NONE);
 10358| 	goto cleanup;
 10359|  unverified:
 10360| 	if (cfg->verbose_level > 3)
 10361| 		g_print ("exiting due to invalid il\n");
 10362| 	set_exception_type_from_invalid_il (cfg, method, ip);
 10363| 	goto cleanup;
 10364|  cleanup:
 10365| 	g_slist_free (class_inits);
 10366| 	mono_basic_block_free (original_bb);
 10367| 	cfg->dont_inline = g_list_remove (cfg->dont_inline, method);
 10368| 	if (cfg->exception_type)
 10369| 		return -1;
 10370| 	else
 10371| 		return inline_costs;
 10372| }
 10373| static int
 10374| store_membase_reg_to_store_membase_imm (int opcode)
 10375| {
 10376| 	switch (opcode) {
 10377| 	case OP_STORE_MEMBASE_REG:
 10378| 		return OP_STORE_MEMBASE_IMM;
 10379| 	case OP_STOREI1_MEMBASE_REG:
 10380| 		return OP_STOREI1_MEMBASE_IMM;
 10381| 	case OP_STOREI2_MEMBASE_REG:
 10382| 		return OP_STOREI2_MEMBASE_IMM;
 10383| 	case OP_STOREI4_MEMBASE_REG:
 10384| 		return OP_STOREI4_MEMBASE_IMM;
 10385| 	case OP_STOREI8_MEMBASE_REG:
 10386| 		return OP_STOREI8_MEMBASE_IMM;
 10387| 	default:
 10388| 		g_assert_not_reached ();
 10389| 	}
 10390| 	return -1;
 10391| }
 10392| int
 10393| mono_op_to_op_imm (int opcode)
 10394| {
 10395| 	switch (opcode) {
 10396| 	case OP_IADD:
 10397| 		return OP_IADD_IMM;
 10398| 	case OP_ISUB:
 10399| 		return OP_ISUB_IMM;
 10400| 	case OP_IDIV:
 10401| 		return OP_IDIV_IMM;
 10402| 	case OP_IDIV_UN:
 10403| 		return OP_IDIV_UN_IMM;
 10404| 	case OP_IREM:
 10405| 		return OP_IREM_IMM;
 10406| 	case OP_IREM_UN:
 10407| 		return OP_IREM_UN_IMM;
 10408| 	case OP_IMUL:
 10409| 		return OP_IMUL_IMM;
 10410| 	case OP_IAND:
 10411| 		return OP_IAND_IMM;
 10412| 	case OP_IOR:
 10413| 		return OP_IOR_IMM;
 10414| 	case OP_IXOR:
 10415| 		return OP_IXOR_IMM;
 10416| 	case OP_ISHL:
 10417| 		return OP_ISHL_IMM;
 10418| 	case OP_ISHR:
 10419| 		return OP_ISHR_IMM;
 10420| 	case OP_ISHR_UN:
 10421| 		return OP_ISHR_UN_IMM;
 10422| 	case OP_LADD:
 10423| 		return OP_LADD_IMM;
 10424| 	case OP_LSUB:
 10425| 		return OP_LSUB_IMM;
 10426| 	case OP_LAND:
 10427| 		return OP_LAND_IMM;
 10428| 	case OP_LOR:
 10429| 		return OP_LOR_IMM;
 10430| 	case OP_LXOR:
 10431| 		return OP_LXOR_IMM;
 10432| 	case OP_LSHL:
 10433| 		return OP_LSHL_IMM;
 10434| 	case OP_LSHR:
 10435| 		return OP_LSHR_IMM;
 10436| 	case OP_LSHR_UN:
 10437| 		return OP_LSHR_UN_IMM;
 10438| #if SIZEOF_REGISTER == 8
 10439| 	case OP_LMUL:
 10440| 		return OP_LMUL_IMM;
 10441| 	case OP_LREM:
 10442| 		return OP_LREM_IMM;
 10443| #endif
 10444| 	case OP_COMPARE:
 10445| 		return OP_COMPARE_IMM;
 10446| 	case OP_ICOMPARE:
 10447| 		return OP_ICOMPARE_IMM;
 10448| 	case OP_LCOMPARE:
 10449| 		return OP_LCOMPARE_IMM;
 10450| 	case OP_STORE_MEMBASE_REG:
 10451| 		return OP_STORE_MEMBASE_IMM;
 10452| 	case OP_STOREI1_MEMBASE_REG:
 10453| 		return OP_STOREI1_MEMBASE_IMM;
 10454| 	case OP_STOREI2_MEMBASE_REG:
 10455| 		return OP_STOREI2_MEMBASE_IMM;
 10456| 	case OP_STOREI4_MEMBASE_REG:
 10457| 		return OP_STOREI4_MEMBASE_IMM;
 10458| #if defined(TARGET_X86) || defined (TARGET_AMD64)
 10459| 	case OP_X86_PUSH:
 10460| 		return OP_X86_PUSH_IMM;
 10461| 	case OP_X86_COMPARE_MEMBASE_REG:
 10462| 		return OP_X86_COMPARE_MEMBASE_IMM;
 10463| #endif
 10464| #if defined(TARGET_AMD64)
 10465| 	case OP_AMD64_ICOMPARE_MEMBASE_REG:
 10466| 		return OP_AMD64_ICOMPARE_MEMBASE_IMM;
 10467| #endif
 10468| 	case OP_VOIDCALL_REG:
 10469| 		return OP_VOIDCALL;
 10470| 	case OP_CALL_REG:
 10471| 		return OP_CALL;
 10472| 	case OP_LCALL_REG:
 10473| 		return OP_LCALL;
 10474| 	case OP_FCALL_REG:
 10475| 		return OP_FCALL;
 10476| 	case OP_LOCALLOC:
 10477| 		return OP_LOCALLOC_IMM;
 10478| 	}
 10479| 	return -1;
 10480| }
 10481| int
 10482| mono_load_membase_to_load_mem (int opcode)
 10483| {
 10484| #if defined(TARGET_X86) || defined(TARGET_AMD64)
 10485| 	switch (opcode) {
 10486| 	case OP_LOAD_MEMBASE:
 10487| 		return OP_LOAD_MEM;
 10488| 	case OP_LOADU1_MEMBASE:
 10489| 		return OP_LOADU1_MEM;
 10490| 	case OP_LOADU2_MEMBASE:
 10491| 		return OP_LOADU2_MEM;
 10492| 	case OP_LOADI4_MEMBASE:
 10493| 		return OP_LOADI4_MEM;
 10494| 	case OP_LOADU4_MEMBASE:
 10495| 		return OP_LOADU4_MEM;
 10496| #if SIZEOF_REGISTER == 8
 10497| 	case OP_LOADI8_MEMBASE:
 10498| 		return OP_LOADI8_MEM;
 10499| #endif
 10500| 	}
 10501| #endif
 10502| 	return -1;
 10503| }
 10504| static int
 10505| op_to_op_dest_membase (int store_opcode, int opcode)
 10506| {
 10507| #if defined(TARGET_X86)
 10508| 	if (!((store_opcode == OP_STORE_MEMBASE_REG) || (store_opcode == OP_STOREI4_MEMBASE_REG)))
 10509| 		return -1;
 10510| 	switch (opcode) {
 10511| 	case OP_IADD:
 10512| 		return OP_X86_ADD_MEMBASE_REG;
 10513| 	case OP_ISUB:
 10514| 		return OP_X86_SUB_MEMBASE_REG;
 10515| 	case OP_IAND:
 10516| 		return OP_X86_AND_MEMBASE_REG;
 10517| 	case OP_IOR:
 10518| 		return OP_X86_OR_MEMBASE_REG;
 10519| 	case OP_IXOR:
 10520| 		return OP_X86_XOR_MEMBASE_REG;
 10521| 	case OP_ADD_IMM:
 10522| 	case OP_IADD_IMM:
 10523| 		return OP_X86_ADD_MEMBASE_IMM;
 10524| 	case OP_SUB_IMM:
 10525| 	case OP_ISUB_IMM:
 10526| 		return OP_X86_SUB_MEMBASE_IMM;
 10527| 	case OP_AND_IMM:
 10528| 	case OP_IAND_IMM:
 10529| 		return OP_X86_AND_MEMBASE_IMM;
 10530| 	case OP_OR_IMM:
 10531| 	case OP_IOR_IMM:
 10532| 		return OP_X86_OR_MEMBASE_IMM;
 10533| 	case OP_XOR_IMM:
 10534| 	case OP_IXOR_IMM:
 10535| 		return OP_X86_XOR_MEMBASE_IMM;
 10536| 	case OP_MOVE:
 10537| 		return OP_NOP;
 10538| 	}
 10539| #endif
 10540| #if defined(TARGET_AMD64)
 10541| 	if (!((store_opcode == OP_STORE_MEMBASE_REG) || (store_opcode == OP_STOREI4_MEMBASE_REG) || (store_opcode == OP_STOREI8_MEMBASE_REG)))
 10542| 		return -1;
 10543| 	switch (opcode) {
 10544| 	case OP_IADD:
 10545| 		return OP_X86_ADD_MEMBASE_REG;
 10546| 	case OP_ISUB:
 10547| 		return OP_X86_SUB_MEMBASE_REG;
 10548| 	case OP_IAND:
 10549| 		return OP_X86_AND_MEMBASE_REG;
 10550| 	case OP_IOR:
 10551| 		return OP_X86_OR_MEMBASE_REG;
 10552| 	case OP_IXOR:
 10553| 		return OP_X86_XOR_MEMBASE_REG;
 10554| 	case OP_IADD_IMM:
 10555| 		return OP_X86_ADD_MEMBASE_IMM;
 10556| 	case OP_ISUB_IMM:
 10557| 		return OP_X86_SUB_MEMBASE_IMM;
 10558| 	case OP_IAND_IMM:
 10559| 		return OP_X86_AND_MEMBASE_IMM;
 10560| 	case OP_IOR_IMM:
 10561| 		return OP_X86_OR_MEMBASE_IMM;
 10562| 	case OP_IXOR_IMM:
 10563| 		return OP_X86_XOR_MEMBASE_IMM;
 10564| 	case OP_LADD:
 10565| 		return OP_AMD64_ADD_MEMBASE_REG;
 10566| 	case OP_LSUB:
 10567| 		return OP_AMD64_SUB_MEMBASE_REG;
 10568| 	case OP_LAND:
 10569| 		return OP_AMD64_AND_MEMBASE_REG;
 10570| 	case OP_LOR:
 10571| 		return OP_AMD64_OR_MEMBASE_REG;
 10572| 	case OP_LXOR:
 10573| 		return OP_AMD64_XOR_MEMBASE_REG;
 10574| 	case OP_ADD_IMM:
 10575| 	case OP_LADD_IMM:
 10576| 		return OP_AMD64_ADD_MEMBASE_IMM;
 10577| 	case OP_SUB_IMM:
 10578| 	case OP_LSUB_IMM:
 10579| 		return OP_AMD64_SUB_MEMBASE_IMM;
 10580| 	case OP_AND_IMM:
 10581| 	case OP_LAND_IMM:
 10582| 		return OP_AMD64_AND_MEMBASE_IMM;
 10583| 	case OP_OR_IMM:
 10584| 	case OP_LOR_IMM:
 10585| 		return OP_AMD64_OR_MEMBASE_IMM;
 10586| 	case OP_XOR_IMM:
 10587| 	case OP_LXOR_IMM:
 10588| 		return OP_AMD64_XOR_MEMBASE_IMM;
 10589| 	case OP_MOVE:
 10590| 		return OP_NOP;
 10591| 	}
 10592| #endif
 10593| 	return -1;
 10594| }
 10595| static int
 10596| op_to_op_store_membase (int store_opcode, int opcode)
 10597| {
 10598| #if defined(TARGET_X86) || defined(TARGET_AMD64)
 10599| 	switch (opcode) {
 10600| 	case OP_ICEQ:
 10601| 		if (store_opcode == OP_STOREI1_MEMBASE_REG)
 10602| 			return OP_X86_SETEQ_MEMBASE;
 10603| 	case OP_CNE:
 10604| 		if (store_opcode == OP_STOREI1_MEMBASE_REG)
 10605| 			return OP_X86_SETNE_MEMBASE;
 10606| 	}
 10607| #endif
 10608| 	return -1;
 10609| }
 10610| static int
 10611| op_to_op_src1_membase (MonoCompile *cfg, int load_opcode, int opcode)
 10612| {
 10613| #ifdef TARGET_X86
 10614| 	/* FIXME: This has sign extension issues */
 10615| 	/*
 10616| 	if ((opcode == OP_ICOMPARE_IMM) && (load_opcode == OP_LOADU1_MEMBASE))
 10617| 		return OP_X86_COMPARE_MEMBASE8_IMM;
 10618| 	*/
 10619| 	if (!((load_opcode == OP_LOAD_MEMBASE) || (load_opcode == OP_LOADI4_MEMBASE) || (load_opcode == OP_LOADU4_MEMBASE)))
 10620| 		return -1;
 10621| 	switch (opcode) {
 10622| 	case OP_X86_PUSH:
 10623| 		return OP_X86_PUSH_MEMBASE;
 10624| 	case OP_COMPARE_IMM:
 10625| 	case OP_ICOMPARE_IMM:
 10626| 		return OP_X86_COMPARE_MEMBASE_IMM;
 10627| 	case OP_COMPARE:
 10628| 	case OP_ICOMPARE:
 10629| 		return OP_X86_COMPARE_MEMBASE_REG;
 10630| 	}
 10631| #endif
 10632| #ifdef TARGET_AMD64
 10633| 	/* FIXME: This has sign extension issues */
 10634| 	/*
 10635| 	if ((opcode == OP_ICOMPARE_IMM) && (load_opcode == OP_LOADU1_MEMBASE))
 10636| 		return OP_X86_COMPARE_MEMBASE8_IMM;
 10637| 	*/
 10638| 	switch (opcode) {
 10639| 	case OP_X86_PUSH:
 10640| 		if ((load_opcode == OP_LOAD_MEMBASE && !cfg->backend->ilp32) || (load_opcode == OP_LOADI8_MEMBASE))
 10641| 			return OP_X86_PUSH_MEMBASE;
 10642| 		break;
 10643| 		/* FIXME: This only works for 32 bit immediates
 10644| 	case OP_COMPARE_IMM:
 10645| 	case OP_LCOMPARE_IMM:
 10646| 		if ((load_opcode == OP_LOAD_MEMBASE) || (load_opcode == OP_LOADI8_MEMBASE))
 10647| 			return OP_AMD64_COMPARE_MEMBASE_IMM;
 10648| 		*/
 10649| 	case OP_ICOMPARE_IMM:
 10650| 		if ((load_opcode == OP_LOADI4_MEMBASE) || (load_opcode == OP_LOADU4_MEMBASE))
 10651| 			return OP_AMD64_ICOMPARE_MEMBASE_IMM;
 10652| 		break;
 10653| 	case OP_COMPARE:
 10654| 	case OP_LCOMPARE:
 10655| 		if (cfg->backend->ilp32 && load_opcode == OP_LOAD_MEMBASE)
 10656| 			return OP_AMD64_ICOMPARE_MEMBASE_REG;
 10657| 		if ((load_opcode == OP_LOAD_MEMBASE && !cfg->backend->ilp32) || (load_opcode == OP_LOADI8_MEMBASE))
 10658| 			return OP_AMD64_COMPARE_MEMBASE_REG;
 10659| 		break;
 10660| 	case OP_ICOMPARE:
 10661| 		if ((load_opcode == OP_LOADI4_MEMBASE) || (load_opcode == OP_LOADU4_MEMBASE))
 10662| 			return OP_AMD64_ICOMPARE_MEMBASE_REG;
 10663| 		break;
 10664| 	}
 10665| #endif
 10666| 	return -1;
 10667| }
 10668| static int
 10669| op_to_op_src2_membase (MonoCompile *cfg, int load_opcode, int opcode)
 10670| {
 10671| #ifdef TARGET_X86
 10672| 	if (!((load_opcode == OP_LOAD_MEMBASE) || (load_opcode == OP_LOADI4_MEMBASE) || (load_opcode == OP_LOADU4_MEMBASE)))
 10673| 		return -1;
 10674| 	switch (opcode) {
 10675| 	case OP_COMPARE:
 10676| 	case OP_ICOMPARE:
 10677| 		return OP_X86_COMPARE_REG_MEMBASE;
 10678| 	case OP_IADD:
 10679| 		return OP_X86_ADD_REG_MEMBASE;
 10680| 	case OP_ISUB:
 10681| 		return OP_X86_SUB_REG_MEMBASE;
 10682| 	case OP_IAND:
 10683| 		return OP_X86_AND_REG_MEMBASE;
 10684| 	case OP_IOR:
 10685| 		return OP_X86_OR_REG_MEMBASE;
 10686| 	case OP_IXOR:
 10687| 		return OP_X86_XOR_REG_MEMBASE;
 10688| 	}
 10689| #endif
 10690| #ifdef TARGET_AMD64
 10691| 	if ((load_opcode == OP_LOADI4_MEMBASE) || (load_opcode == OP_LOADU4_MEMBASE) || (load_opcode == OP_LOAD_MEMBASE && cfg->backend->ilp32)) {
 10692| 		switch (opcode) {
 10693| 		case OP_ICOMPARE:
 10694| 			return OP_AMD64_ICOMPARE_REG_MEMBASE;
 10695| 		case OP_IADD:
 10696| 			return OP_X86_ADD_REG_MEMBASE;
 10697| 		case OP_ISUB:
 10698| 			return OP_X86_SUB_REG_MEMBASE;
 10699| 		case OP_IAND:
 10700| 			return OP_X86_AND_REG_MEMBASE;
 10701| 		case OP_IOR:
 10702| 			return OP_X86_OR_REG_MEMBASE;
 10703| 		case OP_IXOR:
 10704| 			return OP_X86_XOR_REG_MEMBASE;
 10705| 		}
 10706| 	} else if ((load_opcode == OP_LOADI8_MEMBASE) || (load_opcode == OP_LOAD_MEMBASE && !cfg->backend->ilp32)) {
 10707| 		switch (opcode) {
 10708| 		case OP_COMPARE:
 10709| 		case OP_LCOMPARE:
 10710| 			return OP_AMD64_COMPARE_REG_MEMBASE;
 10711| 		case OP_LADD:
 10712| 			return OP_AMD64_ADD_REG_MEMBASE;
 10713| 		case OP_LSUB:
 10714| 			return OP_AMD64_SUB_REG_MEMBASE;
 10715| 		case OP_LAND:
 10716| 			return OP_AMD64_AND_REG_MEMBASE;
 10717| 		case OP_LOR:
 10718| 			return OP_AMD64_OR_REG_MEMBASE;
 10719| 		case OP_LXOR:
 10720| 			return OP_AMD64_XOR_REG_MEMBASE;
 10721| 		}
 10722| 	}
 10723| #endif
 10724| 	return -1;
 10725| }
 10726| int
 10727| mono_op_to_op_imm_noemul (int opcode)
 10728| {
 10729| MONO_DISABLE_WARNING(4065) // switch with default but no case
 10730| 	switch (opcode) {
 10731| #if SIZEOF_REGISTER == 4 && !defined(MONO_ARCH_NO_EMULATE_LONG_SHIFT_OPS)
 10732| 	case OP_LSHR:
 10733| 	case OP_LSHL:
 10734| 	case OP_LSHR_UN:
 10735| 		return -1;
 10736| #endif
 10737| #if defined(MONO_ARCH_EMULATE_MUL_DIV) || defined(MONO_ARCH_EMULATE_DIV)
 10738| 	case OP_IDIV:
 10739| 	case OP_IDIV_UN:
 10740| 	case OP_IREM:
 10741| 	case OP_IREM_UN:
 10742| 		return -1;
 10743| #endif
 10744| #if defined(MONO_ARCH_EMULATE_MUL_DIV)
 10745| 	case OP_IMUL:
 10746| 		return -1;
 10747| #endif
 10748| 	default:
 10749| 		return mono_op_to_op_imm (opcode);
 10750| 	}
 10751| MONO_RESTORE_WARNING
 10752| }
 10753| gboolean
 10754| mono_op_no_side_effects (int opcode)
 10755| {
 10756| 	/* FIXME: Add more instructions */
 10757| 	/* INEG sets the condition codes, and the OP_LNEG decomposition depends on this on x86 */
 10758| 	switch (opcode) {
 10759| 	case OP_MOVE:
 10760| 	case OP_FMOVE:
 10761| 	case OP_VMOVE:
 10762| 	case OP_XMOVE:
 10763| 	case OP_RMOVE:
 10764| 	case OP_VZERO:
 10765| 	case OP_XZERO:
 10766| 	case OP_XONES:
 10767| 	case OP_XCONST:
 10768| 	case OP_ICONST:
 10769| 	case OP_I8CONST:
 10770| 	case OP_ADD_IMM:
 10771| 	case OP_R8CONST:
 10772| 	case OP_LADD_IMM:
 10773| 	case OP_ISUB_IMM:
 10774| 	case OP_IADD_IMM:
 10775| 	case OP_LNEG:
 10776| 	case OP_ISUB:
 10777| 	case OP_CMOV_IGE:
 10778| 	case OP_ISHL_IMM:
 10779| 	case OP_ISHR_IMM:
 10780| 	case OP_ISHR_UN_IMM:
 10781| 	case OP_IAND_IMM:
 10782| 	case OP_ICONV_TO_U1:
 10783| 	case OP_ICONV_TO_I1:
 10784| 	case OP_SEXT_I4:
 10785| 	case OP_LCONV_TO_U1:
 10786| 	case OP_ICONV_TO_U2:
 10787| 	case OP_ICONV_TO_I2:
 10788| 	case OP_LCONV_TO_I2:
 10789| 	case OP_LDADDR:
 10790| 	case OP_PHI:
 10791| 	case OP_NOP:
 10792| 	case OP_ZEXT_I4:
 10793| 	case OP_NOT_NULL:
 10794| 	case OP_IL_SEQ_POINT:
 10795| 	case OP_RTTYPE:
 10796| 		return TRUE;
 10797| 	default:
 10798| 		return FALSE;
 10799| 	}
 10800| }
 10801| gboolean
 10802| mono_ins_no_side_effects (MonoInst *ins)
 10803| {
 10804| 	if (mono_op_no_side_effects (ins->opcode))
 10805| 		return TRUE;
 10806| 	if (ins->opcode == OP_AOTCONST) {
 10807| 		MonoJumpInfoType type = (MonoJumpInfoType)(intptr_t)ins->inst_p1;
 10808| 		switch (type) {
 10809| 		case MONO_PATCH_INFO_TYPE_FROM_HANDLE:
 10810| 		case MONO_PATCH_INFO_LDSTR:
 10811| 		case MONO_PATCH_INFO_VTABLE:
 10812| 		case MONO_PATCH_INFO_METHOD_RGCTX:
 10813| 			return TRUE;
 10814| 		}
 10815| 	}
 10816| 	return FALSE;
 10817| }
 10818| /**
 10819|  * mono_handle_global_vregs:
 10820|  *
 10821|  *   Make vregs used in more than one bblock 'global', i.e. allocate a variable
 10822|  * for them.
 10823|  */
 10824| void
 10825| mono_handle_global_vregs (MonoCompile *cfg)
 10826| {
 10827| 	gint32 *vreg_to_bb;
 10828| 	MonoBasicBlock *bb;
 10829| 	vreg_to_bb = (gint32 *)mono_mempool_alloc0 (cfg->mempool, sizeof (gint32*) * cfg->next_vreg + 1);
 10830| 	/* Find local vregs used in more than one bb */
 10831| 	for (bb = cfg->bb_entry; bb; bb = bb->next_bb) {
 10832| 		MonoInst *ins = bb->code;
 10833| 		int block_num = bb->block_num;
 10834| 		if (cfg->verbose_level > 2)
 10835| 			printf ("\nHANDLE-GLOBAL-VREGS BLOCK %d:\n", bb->block_num);
 10836| 		cfg->cbb = bb;
 10837| 		for (; ins; ins = ins->next) {
 10838| 			const char *spec = INS_INFO (ins->opcode);
 10839| 			int regtype = 0, regindex;
 10840| 			gint32 prev_bb;
 10841| 			if (G_UNLIKELY (cfg->verbose_level > 2))
 10842| 				mono_print_ins (ins);
 10843| 			g_assert (ins->opcode >= MONO_CEE_LAST);
 10844| 			for (regindex = 0; regindex < 4; regindex ++) {
 10845| 				int vreg = 0;
 10846| 				if (regindex == 0) {
 10847| 					regtype = spec [MONO_INST_DEST];
 10848| 					if (regtype == ' ')
 10849| 						continue;
 10850| 					vreg = ins->dreg;
 10851| 				} else if (regindex == 1) {
 10852| 					regtype = spec [MONO_INST_SRC1];
 10853| 					if (regtype == ' ')
 10854| 						continue;
 10855| 					vreg = ins->sreg1;
 10856| 				} else if (regindex == 2) {
 10857| 					regtype = spec [MONO_INST_SRC2];
 10858| 					if (regtype == ' ')
 10859| 						continue;
 10860| 					vreg = ins->sreg2;
 10861| 				} else if (regindex == 3) {
 10862| 					regtype = spec [MONO_INST_SRC3];
 10863| 					if (regtype == ' ')
 10864| 						continue;
 10865| 					vreg = ins->sreg3;
 10866| 				}
 10867| #if SIZEOF_REGISTER == 4
 10868| 				/* In the LLVM case, the long opcodes are not decomposed */
 10869| 				if (regtype == 'l' && !COMPILE_LLVM (cfg)) {
 10870| 					/*
 10871| 					 * Since some instructions reference the original long vreg,
 10872| 					 * and some reference the two component vregs, it is quite hard
 10873| 					 * to determine when it needs to be global. So be conservative.
 10874| 					 */
 10875| 					if (!get_vreg_to_inst (cfg, vreg)) {
 10876| 						mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (mono_defaults.int64_class), OP_LOCAL, vreg);
 10877| 						if (cfg->verbose_level > 2)
 10878| 							printf ("LONG VREG R%d made global.\n", vreg);
 10879| 					}
 10880| 					/*
 10881| 					 * Make the component vregs volatile since the optimizations can
 10882| 					 * get confused otherwise.
 10883| 					 */
 10884| 					get_vreg_to_inst (cfg, MONO_LVREG_LS (vreg))->flags |= MONO_INST_VOLATILE;
 10885| 					get_vreg_to_inst (cfg, MONO_LVREG_MS (vreg))->flags |= MONO_INST_VOLATILE;
 10886| 				}
 10887| #endif
 10888| 				g_assert (vreg != -1);
 10889| 				prev_bb = vreg_to_bb [vreg];
 10890| 				if (prev_bb == 0) {
 10891| 					/* 0 is a valid block num */
 10892| 					vreg_to_bb [vreg] = block_num + 1;
 10893| 				} else if ((prev_bb != block_num + 1) && (prev_bb != -1)) {
 10894| 					if (((regtype == 'i' && (vreg < MONO_MAX_IREGS))) || (regtype == 'f' && (vreg < MONO_MAX_FREGS)))
 10895| 						continue;
 10896| 					if (!get_vreg_to_inst (cfg, vreg)) {
 10897| 						if (G_UNLIKELY (cfg->verbose_level > 2))
 10898| 							printf ("VREG R%d used in BB%d and BB%d made global.\n", vreg, vreg_to_bb [vreg], block_num);
 10899| 						switch (regtype) {
 10900| 						case 'i':
 10901| 							if (vreg_is_ref (cfg, vreg))
 10902| 								mono_compile_create_var_for_vreg (cfg, mono_get_object_type (), OP_LOCAL, vreg);
 10903| 							else
 10904| 								mono_compile_create_var_for_vreg (cfg, mono_get_int_type (), OP_LOCAL, vreg);
 10905| 							break;
 10906| 						case 'l':
 10907| 							mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (mono_defaults.int64_class), OP_LOCAL, vreg);
 10908| 							break;
 10909| 						case 'f':
 10910| 							mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (mono_defaults.double_class), OP_LOCAL, vreg);
 10911| 							break;
 10912| 						case 'v':
 10913| 						case 'x':
 10914| 							mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (ins->klass), OP_LOCAL, vreg);
 10915| 							break;
 10916| 						default:
 10917| 							g_assert_not_reached ();
 10918| 						}
 10919| 					}
 10920| 					/* Flag as having been used in more than one bb */
 10921| 					vreg_to_bb [vreg] = -1;
 10922| 				}
 10923| 			}
 10924| 		}
 10925| 	}
 10926| 	/* If a variable is used in only one bblock, convert it into a local vreg */
 10927| 	for (guint i = 0; i < cfg->num_varinfo; i++) {
 10928| 		MonoInst *var = cfg->varinfo [i];
 10929| 		MonoMethodVar *vmv = MONO_VARINFO (cfg, i);
 10930| 		switch (var->type) {
 10931| 		case STACK_I4:
 10932| 		case STACK_OBJ:
 10933| 		case STACK_PTR:
 10934| 		case STACK_MP:
 10935| 		case STACK_VTYPE:
 10936| #if SIZEOF_REGISTER == 8
 10937| 		case STACK_I8:
 10938| #endif
 10939| #if !defined(TARGET_X86)
 10940| 		/* Enabling this screws up the fp stack on x86 */
 10941| 		case STACK_R8:
 10942| #endif
 10943| 			if (mono_arch_is_soft_float ())
 10944| 				break;
 10945| 			/*
 10946| 			if (var->type == STACK_VTYPE && cfg->gsharedvt && mini_is_gsharedvt_variable_type (var->inst_vtype))
 10947| 				break;
 10948| 			*/
 10949| 			/* Arguments are implicitly global */
 10950| 			/* Putting R4 vars into registers doesn't work currently */
 10951| 			/* The gsharedvt vars are implicitly referenced by ldaddr opcodes, but those opcodes are only generated later */
 10952| 			if ((var->opcode != OP_ARG) && (var != cfg->ret) && !(var->flags & (MONO_INST_VOLATILE|MONO_INST_INDIRECT)) && (vreg_to_bb [var->dreg] != -1) && (m_class_get_byval_arg (var->klass)->type != MONO_TYPE_R4) && !cfg->disable_vreg_to_lvreg && var != cfg->gsharedvt_info_var && var != cfg->gsharedvt_locals_var && var != cfg->lmf_addr_var) {
 10953| 				/*
 10954| 				 * Make that the variable's liveness interval doesn't contain a call, since
 10955| 				 * that would cause the lvreg to be spilled, making the whole optimization
 10956| 				 * useless.
 10957| 				 */
 10958| 				/* This is too slow for JIT compilation */
 10959| #if 0
 10960| 				if (cfg->compile_aot && vreg_to_bb [var->dreg]) {
 10961| 					MonoInst *ins;
 10962| 					int def_index, call_index, ins_index;
 10963| 					gboolean spilled = FALSE;
 10964| 					def_index = -1;
 10965| 					call_index = -1;
 10966| 					ins_index = 0;
 10967| 					for (ins = vreg_to_bb [var->dreg]->code; ins; ins = ins->next) {
 10968| 						const char *spec = INS_INFO (ins->opcode);
 10969| 						if ((spec [MONO_INST_DEST] != ' ') && (ins->dreg == var->dreg))
 10970| 							def_index = ins_index;
 10971| 						if (((spec [MONO_INST_SRC1] != ' ') && (ins->sreg1 == var->dreg)) ||
 10972| 							((spec [MONO_INST_SRC1] != ' ') && (ins->sreg1 == var->dreg))) {
 10973| 							if (call_index > def_index) {
 10974| 								spilled = TRUE;
 10975| 								break;
 10976| 							}
 10977| 						}
 10978| 						if (MONO_IS_CALL (ins))
 10979| 							call_index = ins_index;
 10980| 						ins_index ++;
 10981| 					}
 10982| 					if (spilled)
 10983| 						break;
 10984| 				}
 10985| #endif
 10986| 				if (G_UNLIKELY (cfg->verbose_level > 2))
 10987| 					printf ("CONVERTED R%d(%d) TO VREG.\n", var->dreg, vmv->idx);
 10988| 				var->flags |= MONO_INST_IS_DEAD;
 10989| 				cfg->vreg_to_inst [var->dreg] = NULL;
 10990| 			}
 10991| 			break;
 10992| 		}
 10993| 	}
 10994| 	/*
 10995| 	 * Compress the varinfo and vars tables so the liveness computation is faster and
 10996| 	 * takes up less space.
 10997| 	 */
 10998| 	guint pos = 0;
 10999| 	for (guint i = 0; i < cfg->num_varinfo; ++i) {
 11000| 		MonoInst *var = cfg->varinfo [i];
 11001| 		if (pos < i && cfg->locals_start == i)
 11002| 			cfg->locals_start = pos;
 11003| 		if (!(var->flags & MONO_INST_IS_DEAD)) {
 11004| 			if (pos < i) {
 11005| 				cfg->varinfo [pos] = cfg->varinfo [i];
 11006| 				cfg->varinfo [pos]->inst_c0 = pos;
 11007| 				memcpy (&cfg->vars [pos], &cfg->vars [i], sizeof (MonoMethodVar));
 11008| 				cfg->vars [pos].idx = pos;
 11009| #if SIZEOF_REGISTER == 4
 11010| 				if (cfg->varinfo [pos]->type == STACK_I8) {
 11011| 					/* Modify the two component vars too */
 11012| 					MonoInst *var1;
 11013| 					var1 = get_vreg_to_inst (cfg, MONO_LVREG_LS (cfg->varinfo [pos]->dreg));
 11014| 					var1->inst_c0 = pos;
 11015| 					var1 = get_vreg_to_inst (cfg, MONO_LVREG_MS (cfg->varinfo [pos]->dreg));
 11016| 					var1->inst_c0 = pos;
 11017| 				}
 11018| #endif
 11019| 			}
 11020| 			pos ++;
 11021| 		}
 11022| 	}
 11023| 	cfg->num_varinfo = pos;
 11024| 	if (cfg->locals_start > cfg->num_varinfo)
 11025| 		cfg->locals_start = cfg->num_varinfo;
 11026| }
 11027| /*
 11028|  * mono_allocate_gsharedvt_vars:
 11029|  *
 11030|  *   Allocate variables with gsharedvt types to entries in the MonoGSharedVtMethodRuntimeInfo.entries array.
 11031|  * Initialize cfg->gsharedvt_vreg_to_idx with the mapping between vregs and indexes.
 11032|  */
 11033| void
 11034| mono_allocate_gsharedvt_vars (MonoCompile *cfg)
 11035| {
 11036| 	cfg->gsharedvt_vreg_to_idx = (int *)mono_mempool_alloc0 (cfg->mempool, sizeof (int) * cfg->next_vreg);
 11037| 	for (guint i = 0; i < cfg->num_varinfo; ++i) {
 11038| 		MonoInst *ins = cfg->varinfo [i];
 11039| 		int idx;
 11040| 		if (mini_is_gsharedvt_variable_type (ins->inst_vtype)) {
 11041| 			if (i >= cfg->locals_start) {
 11042| 				/* Local */
 11043| 				idx = get_gsharedvt_info_slot (cfg, ins->inst_vtype, MONO_RGCTX_INFO_LOCAL_OFFSET);
 11044| 				cfg->gsharedvt_vreg_to_idx [ins->dreg] = idx + 1;
 11045| 				ins->opcode = OP_GSHAREDVT_LOCAL;
 11046| 				ins->inst_imm = idx;
 11047| 			} else {
 11048| 				/* Arg */
 11049| 				cfg->gsharedvt_vreg_to_idx [ins->dreg] = -1;
 11050| 				ins->opcode = OP_GSHAREDVT_ARG_REGOFFSET;
 11051| 			}
 11052| 		}
 11053| 	}
 11054| }
 11055| /**
 11056|  * mono_spill_global_vars:
 11057|  *
 11058|  *   Generate spill code for variables which are not allocated to registers,
 11059|  * and replace vregs with their allocated hregs. *need_local_opts is set to TRUE if
 11060|  * code is generated which could be optimized by the local optimization passes.
 11061|  */
 11062| void
 11063| mono_spill_global_vars (MonoCompile *cfg, gboolean *need_local_opts)
 11064| {
 11065| 	MonoBasicBlock *bb;
 11066| 	char spec2 [16];
 11067| 	int orig_next_vreg;
 11068| 	guint32 *vreg_to_lvreg;
 11069| 	guint32 *lvregs;
 11070| 	guint32 i, lvregs_len, lvregs_size;
 11071| 	gboolean dest_has_lvreg = FALSE;
 11072| 	MonoStackType stacktypes [128];
 11073| 	MonoInst **live_range_start, **live_range_end;
 11074| 	MonoBasicBlock **live_range_start_bb, **live_range_end_bb;
 11075| 	*need_local_opts = FALSE;
 11076| 	memset (spec2, 0, sizeof (spec2));
 11077| 	/* FIXME: Move this function to mini.c */
 11078| 	stacktypes [(int)'i'] = STACK_PTR;
 11079| 	stacktypes [(int)'l'] = STACK_I8;
 11080| 	stacktypes [(int)'f'] = STACK_R8;
 11081| #ifdef MONO_ARCH_SIMD_INTRINSICS
 11082| 	stacktypes [(int)'x'] = STACK_VTYPE;
 11083| #endif
 11084| #if SIZEOF_REGISTER == 4
 11085| 	/* Create MonoInsts for longs */
 11086| 	for (i = 0; i < cfg->num_varinfo; i++) {
 11087| 		MonoInst *ins = cfg->varinfo [i];
 11088| 		if ((ins->opcode != OP_REGVAR) && !(ins->flags & MONO_INST_IS_DEAD)) {
 11089| 			switch (ins->type) {
 11090| 			case STACK_R8:
 11091| 			case STACK_I8: {
 11092| 				MonoInst *tree;
 11093| 				if (ins->type == STACK_R8 && !COMPILE_SOFT_FLOAT (cfg))
 11094| 					break;
 11095| 				g_assert (ins->opcode == OP_REGOFFSET);
 11096| 				tree = get_vreg_to_inst (cfg, MONO_LVREG_LS (ins->dreg));
 11097| 				g_assert (tree);
 11098| 				tree->opcode = OP_REGOFFSET;
 11099| 				tree->inst_basereg = ins->inst_basereg;
 11100| 				tree->inst_offset = ins->inst_offset + MINI_LS_WORD_OFFSET;
 11101| 				tree = get_vreg_to_inst (cfg, MONO_LVREG_MS (ins->dreg));
 11102| 				g_assert (tree);
 11103| 				tree->opcode = OP_REGOFFSET;
 11104| 				tree->inst_basereg = ins->inst_basereg;
 11105| 				tree->inst_offset = ins->inst_offset + MINI_MS_WORD_OFFSET;
 11106| 				break;
 11107| 			}
 11108| 			default:
 11109| 				break;
 11110| 			}
 11111| 		}
 11112| 	}
 11113| #endif
 11114| 	if (cfg->compute_gc_maps) {
 11115| 		/* registers need liveness info even for !non refs */
 11116| 		for (i = 0; i < cfg->num_varinfo; i++) {
 11117| 			MonoInst *ins = cfg->varinfo [i];
 11118| 			if (ins->opcode == OP_REGVAR)
 11119| 				ins->flags |= MONO_INST_GC_TRACK;
 11120| 		}
 11121| 	}
 11122| 	/* FIXME: widening and truncation */
 11123| 	/*
 11124| 	 * As an optimization, when a variable allocated to the stack is first loaded into
 11125| 	 * an lvreg, we will remember the lvreg and use it the next time instead of loading
 11126| 	 * the variable again.
 11127| 	 */
 11128| 	orig_next_vreg = cfg->next_vreg;
 11129| 	vreg_to_lvreg = (guint32 *)mono_mempool_alloc0 (cfg->mempool, sizeof (guint32) * cfg->next_vreg);
 11130| 	lvregs_size = 1024;
 11131| 	lvregs = (guint32 *)mono_mempool_alloc (cfg->mempool, sizeof (guint32) * lvregs_size);
 11132| 	lvregs_len = 0;
 11133| 	/*
 11134| 	 * These arrays contain the first and last instructions accessing a given
 11135| 	 * variable.
 11136| 	 * Since we emit bblocks in the same order we process them here, and we
 11137| 	 * don't split live ranges, these will precisely describe the live range of
 11138| 	 * the variable, i.e. the instruction range where a valid value can be found
 11139| 	 * in the variables location.
 11140| 	 * The live range is computed using the liveness info computed by the liveness pass.
 11141| 	 * We can't use vmv->range, since that is an abstract live range, and we need
 11142| 	 * one which is instruction precise.
 11143| 	 * FIXME: Variables used in out-of-line bblocks have a hole in their live range.
 11144| 	 */
 11145| 	/* FIXME: Only do this if debugging info is requested */
 11146| 	live_range_start = g_new0 (MonoInst*, cfg->next_vreg);
 11147| 	live_range_end = g_new0 (MonoInst*, cfg->next_vreg);
 11148| 	live_range_start_bb = g_new (MonoBasicBlock*, cfg->next_vreg);
 11149| 	live_range_end_bb = g_new (MonoBasicBlock*, cfg->next_vreg);
 11150| 	/* Add spill loads/stores */
 11151| 	for (bb = cfg->bb_entry; bb; bb = bb->next_bb) {
 11152| 		MonoInst *ins;
 11153| 		if (cfg->verbose_level > 2)
 11154| 			printf ("\nSPILL BLOCK %d:\n", bb->block_num);
 11155| 		/* Clear vreg_to_lvreg array */
 11156| 		for (i = 0; i < lvregs_len; i++)
 11157| 			vreg_to_lvreg [lvregs [i]] = 0;
 11158| 		lvregs_len = 0;
 11159| 		cfg->cbb = bb;
 11160| 		MONO_BB_FOR_EACH_INS (bb, ins) {
 11161| 			const char *spec = INS_INFO (ins->opcode);
 11162| 			int regtype, srcindex, sreg, tmp_reg, prev_dreg, num_sregs;
 11163| 			gboolean store, no_lvreg;
 11164| 			int sregs [MONO_MAX_SRC_REGS];
 11165| 			if (G_UNLIKELY (cfg->verbose_level > 2))
 11166| 				mono_print_ins (ins);
 11167| 			if (ins->opcode == OP_NOP)
 11168| 				continue;
 11169| 			/*
 11170| 			 * We handle LDADDR here as well, since it can only be decomposed
 11171| 			 * when variable addresses are known.
 11172| 			 */
 11173| 			if (ins->opcode == OP_LDADDR) {
 11174| 				MonoInst *var = (MonoInst *)ins->inst_p0;
 11175| 				if (var->opcode == OP_VTARG_ADDR) {
 11176| 					/* Happens on SPARC/S390 where vtypes are passed by reference */
 11177| 					MonoInst *vtaddr = var->inst_left;
 11178| 					if (vtaddr->opcode == OP_REGVAR) {
 11179| 						ins->opcode = OP_MOVE;
 11180| 						ins->sreg1 = vtaddr->dreg;
 11181| 					}
 11182| 					else if (var->inst_left->opcode == OP_REGOFFSET) {
 11183| 						ins->opcode = OP_LOAD_MEMBASE;
 11184| 						ins->inst_basereg = vtaddr->inst_basereg;
 11185| 						ins->inst_offset = vtaddr->inst_offset;
 11186| 					} else
 11187| 						NOT_IMPLEMENTED;
 11188| 				} else if (cfg->gsharedvt && cfg->gsharedvt_vreg_to_idx [var->dreg] < 0) {
 11189| 					/* gsharedvt arg passed by ref */
 11190| 					g_assert (var->opcode == OP_GSHAREDVT_ARG_REGOFFSET);
 11191| 					ins->opcode = OP_LOAD_MEMBASE;
 11192| 					ins->inst_basereg = var->inst_basereg;
 11193| 					ins->inst_offset = var->inst_offset;
 11194| 				} else if (cfg->gsharedvt && cfg->gsharedvt_vreg_to_idx [var->dreg]) {
 11195| 					MonoInst *load, *load2, *load3;
 11196| 					int idx = cfg->gsharedvt_vreg_to_idx [var->dreg] - 1;
 11197| 					int reg1, reg2, reg3;
 11198| 					MonoInst *info_var = cfg->gsharedvt_info_var;
 11199| 					MonoInst *locals_var = cfg->gsharedvt_locals_var;
 11200| 					/*
 11201| 					 * gsharedvt local.
 11202| 					 * Compute the address of the local as gsharedvt_locals_var + gsharedvt_info_var->locals_offsets [idx].
 11203| 					 */
 11204| 					g_assert (var->opcode == OP_GSHAREDVT_LOCAL);
 11205| 					g_assert (info_var);
 11206| 					g_assert (locals_var);
 11207| 					/* Mark the instruction used to compute the locals var as used */
 11208| 					cfg->gsharedvt_locals_var_ins = NULL;
 11209| 					/* Load the offset */
 11210| 					if (info_var->opcode == OP_REGOFFSET) {
 11211| 						reg1 = alloc_ireg (cfg);
 11212| 						NEW_LOAD_MEMBASE (cfg, load, OP_LOAD_MEMBASE, reg1, info_var->inst_basereg, info_var->inst_offset);
 11213| 					} else if (info_var->opcode == OP_REGVAR) {
 11214| 						load = NULL;
 11215| 						reg1 = info_var->dreg;
 11216| 					} else {
 11217| 						g_assert_not_reached ();
 11218| 					}
 11219| 					reg2 = alloc_ireg (cfg);
 11220| 					NEW_LOAD_MEMBASE (cfg, load2, OP_LOADI4_MEMBASE, reg2, reg1, MONO_STRUCT_OFFSET (MonoGSharedVtMethodRuntimeInfo, entries) + (idx * TARGET_SIZEOF_VOID_P));
 11221| 					/* Load the locals area address */
 11222| 					reg3 = alloc_ireg (cfg);
 11223| 					if (locals_var->opcode == OP_REGOFFSET) {
 11224| 						NEW_LOAD_MEMBASE (cfg, load3, OP_LOAD_MEMBASE, reg3, locals_var->inst_basereg, locals_var->inst_offset);
 11225| 					} else if (locals_var->opcode == OP_REGVAR) {
 11226| 						NEW_UNALU (cfg, load3, OP_MOVE, reg3, locals_var->dreg);
 11227| 					} else {
 11228| 						g_assert_not_reached ();
 11229| 					}
 11230| 					/* Compute the address */
 11231| 					ins->opcode = OP_PADD;
 11232| 					ins->sreg1 = reg3;
 11233| 					ins->sreg2 = reg2;
 11234| 					mono_bblock_insert_before_ins (bb, ins, load3);
 11235| 					mono_bblock_insert_before_ins (bb, load3, load2);
 11236| 					if (load)
 11237| 						mono_bblock_insert_before_ins (bb, load2, load);
 11238| 				} else {
 11239| 					g_assert (var->opcode == OP_REGOFFSET);
 11240| 					ins->opcode = OP_ADD_IMM;
 11241| 					ins->sreg1 = var->inst_basereg;
 11242| 					ins->inst_imm = var->inst_offset;
 11243| 				}
 11244| 				*need_local_opts = TRUE;
 11245| 				spec = INS_INFO (ins->opcode);
 11246| 			}
 11247| 			if (ins->opcode < MONO_CEE_LAST) {
 11248| 				mono_print_ins (ins);
 11249| 				g_assert_not_reached ();
 11250| 			}
 11251| 			/*
 11252| 			 * Store opcodes have destbasereg in the dreg, but in reality, it is an
 11253| 			 * src register.
 11254| 			 * FIXME:
 11255| 			 */
 11256| 			if (MONO_IS_STORE_MEMBASE (ins)) {
 11257| 				tmp_reg = ins->dreg;
 11258| 				ins->dreg = ins->sreg2;
 11259| 				ins->sreg2 = tmp_reg;
 11260| 				store = TRUE;
 11261| 				spec2 [MONO_INST_DEST] = ' ';
 11262| 				spec2 [MONO_INST_SRC1] = spec [MONO_INST_SRC1];
 11263| 				spec2 [MONO_INST_SRC2] = spec [MONO_INST_DEST];
 11264| 				spec2 [MONO_INST_SRC3] = ' ';
 11265| 				spec = spec2;
 11266| 			} else if (MONO_IS_STORE_MEMINDEX (ins))
 11267| 				g_assert_not_reached ();
 11268| 			else
 11269| 				store = FALSE;
 11270| 			no_lvreg = FALSE;
 11271| 			if (G_UNLIKELY (cfg->verbose_level > 2)) {
 11272| 				printf ("\t %.3s %d", spec, ins->dreg);
 11273| 				num_sregs = mono_inst_get_src_registers (ins, sregs);
 11274| 				for (srcindex = 0; srcindex < num_sregs; ++srcindex)
 11275| 					printf (" %d", sregs [srcindex]);
 11276| 				printf ("\n");
 11277| 			}
 11278| 			/***************/
 11279| 			/*    DREG     */
 11280| 			/***************/
 11281| 			regtype = spec [MONO_INST_DEST];
 11282| 			g_assert (((ins->dreg == -1) && (regtype == ' ')) || ((ins->dreg != -1) && (regtype != ' ')));
 11283| 			prev_dreg = -1;
 11284| 			int dreg_using_dest_to_membase_op = -1;
 11285| 			if ((ins->dreg != -1) && get_vreg_to_inst (cfg, ins->dreg)) {
 11286| 				MonoInst *var = get_vreg_to_inst (cfg, ins->dreg);
 11287| 				MonoInst *store_ins;
 11288| 				int store_opcode;
 11289| 				MonoInst *def_ins = ins;
 11290| 				int dreg = ins->dreg; /* The original vreg */
 11291| 				store_opcode = mono_type_to_store_membase (cfg, var->inst_vtype);
 11292| 				if (var->opcode == OP_REGVAR) {
 11293| 					ins->dreg = var->dreg;
 11294| 				} else if ((ins->dreg == ins->sreg1) && (spec [MONO_INST_DEST] == 'i') && (spec [MONO_INST_SRC1] == 'i') && !vreg_to_lvreg [ins->dreg] && (op_to_op_dest_membase (store_opcode, ins->opcode) != -1)) {
 11295| 					/*
 11296| 					 * Instead of emitting a load+store, use a _membase opcode.
 11297| 					 */
 11298| 					g_assert (var->opcode == OP_REGOFFSET);
 11299| 					if (ins->opcode == OP_MOVE) {
 11300| 						NULLIFY_INS (ins);
 11301| 						def_ins = NULL;
 11302| 					} else {
 11303| 						dreg_using_dest_to_membase_op = ins->dreg;
 11304| 						ins->opcode = GINT_TO_OPCODE (op_to_op_dest_membase (store_opcode, ins->opcode));
 11305| 						ins->inst_basereg = var->inst_basereg;
 11306| 						ins->inst_offset = var->inst_offset;
 11307| 						ins->dreg = -1;
 11308| 					}
 11309| 					spec = INS_INFO (ins->opcode);
 11310| 				} else {
 11311| 					guint32 lvreg;
 11312| 					g_assert (var->opcode == OP_REGOFFSET);
 11313| 					prev_dreg = ins->dreg;
 11314| 					/* Invalidate any previous lvreg for this vreg */
 11315| 					vreg_to_lvreg [ins->dreg] = 0;
 11316| 					lvreg = 0;
 11317| 					if (COMPILE_SOFT_FLOAT (cfg) && store_opcode == OP_STORER8_MEMBASE_REG) {
 11318| 						regtype = 'l';
 11319| 						store_opcode = OP_STOREI8_MEMBASE_REG;
 11320| 					}
 11321| 					ins->dreg = alloc_dreg (cfg, stacktypes [regtype]);
 11322| #if SIZEOF_REGISTER != 8
 11323| 					if (regtype == 'l') {
 11324| 						NEW_STORE_MEMBASE (cfg, store_ins, OP_STOREI4_MEMBASE_REG, var->inst_basereg, var->inst_offset + MINI_LS_WORD_OFFSET, MONO_LVREG_LS (ins->dreg));
 11325| 						mono_bblock_insert_after_ins (bb, ins, store_ins);
 11326| 						NEW_STORE_MEMBASE (cfg, store_ins, OP_STOREI4_MEMBASE_REG, var->inst_basereg, var->inst_offset + MINI_MS_WORD_OFFSET, MONO_LVREG_MS (ins->dreg));
 11327| 						mono_bblock_insert_after_ins (bb, ins, store_ins);
 11328| 						def_ins = store_ins;
 11329| 					}
 11330| 					else
 11331| #endif
 11332| 					{
 11333| 						g_assert (store_opcode != OP_STOREV_MEMBASE);
 11334| 						/* Try to fuse the store into the instruction itself */
 11335| 						/* FIXME: Add more instructions */
 11336| 						if (!lvreg && ((ins->opcode == OP_ICONST) || ((ins->opcode == OP_I8CONST) && (ins->inst_c0 == 0)))) {
 11337| 							ins->opcode = GINT_TO_OPCODE (store_membase_reg_to_store_membase_imm (store_opcode));
 11338| 							ins->inst_imm = ins->inst_c0;
 11339| 							ins->inst_destbasereg = var->inst_basereg;
 11340| 							ins->inst_offset = var->inst_offset;
 11341| 							spec = INS_INFO (ins->opcode);
 11342| 						} else if (!lvreg && ((ins->opcode == OP_MOVE) || (ins->opcode == OP_FMOVE) || (ins->opcode == OP_LMOVE) || (ins->opcode == OP_RMOVE))) {
 11343| 							ins->opcode = GINT_TO_OPCODE (store_opcode);
 11344| 							ins->inst_destbasereg = var->inst_basereg;
 11345| 							ins->inst_offset = var->inst_offset;
 11346| 							no_lvreg = TRUE;
 11347| 							tmp_reg = ins->dreg;
 11348| 							ins->dreg = ins->sreg2;
 11349| 							ins->sreg2 = tmp_reg;
 11350| 							store = TRUE;
 11351| 							spec2 [MONO_INST_DEST] = ' ';
 11352| 							spec2 [MONO_INST_SRC1] = spec [MONO_INST_SRC1];
 11353| 							spec2 [MONO_INST_SRC2] = spec [MONO_INST_DEST];
 11354| 							spec2 [MONO_INST_SRC3] = ' ';
 11355| 							spec = spec2;
 11356| 						} else if (!lvreg && (op_to_op_store_membase (store_opcode, ins->opcode) != -1)) {
 11357| 							ins->opcode = GINT_TO_OPCODE (op_to_op_store_membase (store_opcode, ins->opcode));
 11358| 							ins->dreg = -1;
 11359| 							ins->inst_basereg = var->inst_basereg;
 11360| 							ins->inst_offset = var->inst_offset;
 11361| 							spec = INS_INFO (ins->opcode);
 11362| 						} else {
 11363| 							/* printf ("INS: "); mono_print_ins (ins); */
 11364| 							/* Create a store instruction */
 11365| 							NEW_STORE_MEMBASE (cfg, store_ins, store_opcode, var->inst_basereg, var->inst_offset, ins->dreg);
 11366| 							if (store_ins->opcode == OP_STOREX_MEMBASE)
 11367| 								mini_type_to_eval_stack_type (cfg, var->inst_vtype, store_ins);
 11368| 							/* Insert it after the instruction */
 11369| 							mono_bblock_insert_after_ins (bb, ins, store_ins);
 11370| 							def_ins = store_ins;
 11371| 							/*
 11372| 							 * We can't assign ins->dreg to var->dreg here, since the
 11373| 							 * sregs could use it. So set a flag, and do it after
 11374| 							 * the sregs.
 11375| 							 */
 11376| 							if (!((var)->flags & (MONO_INST_VOLATILE|MONO_INST_INDIRECT)))
 11377| 								dest_has_lvreg = TRUE;
 11378| 						}
 11379| 					}
 11380| 				}
 11381| 				if (def_ins && !live_range_start [dreg]) {
 11382| 					live_range_start [dreg] = def_ins;
 11383| 					live_range_start_bb [dreg] = bb;
 11384| 				}
 11385| 				if (cfg->compute_gc_maps && def_ins && (var->flags & MONO_INST_GC_TRACK)) {
 11386| 					MonoInst *tmp;
 11387| 					MONO_INST_NEW (cfg, tmp, OP_GC_LIVENESS_DEF);
 11388| 					tmp->inst_c1 = dreg;
 11389| 					mono_bblock_insert_after_ins (bb, def_ins, tmp);
 11390| 				}
 11391| 			}
 11392| 			/************/
 11393| 			/*  SREGS   */
 11394| 			/************/
 11395| 			num_sregs = mono_inst_get_src_registers (ins, sregs);
 11396| 			for (srcindex = 0; srcindex < 3; ++srcindex) {
 11397| 				regtype = spec [MONO_INST_SRC1 + srcindex];
 11398| 				sreg = sregs [srcindex];
 11399| 				g_assert (((sreg == -1) && (regtype == ' ')) || ((sreg != -1) && (regtype != ' ')));
 11400| 				if ((sreg != -1) && get_vreg_to_inst (cfg, sreg)) {
 11401| 					MonoInst *var = get_vreg_to_inst (cfg, sreg);
 11402| 					MonoInst *use_ins = ins;
 11403| 					MonoInst *load_ins;
 11404| 					guint32 load_opcode;
 11405| 					if (var->opcode == OP_REGVAR) {
 11406| 						sregs [srcindex] = var->dreg;
 11407| 						live_range_end [sreg] = use_ins;
 11408| 						live_range_end_bb [sreg] = bb;
 11409| 						if (cfg->compute_gc_maps && var->dreg < orig_next_vreg && (var->flags & MONO_INST_GC_TRACK)) {
 11410| 							MonoInst *tmp;
 11411| 							MONO_INST_NEW (cfg, tmp, OP_GC_LIVENESS_USE);
 11412| 							/* var->dreg is a hreg */
 11413| 							tmp->inst_c1 = sreg;
 11414| 							mono_bblock_insert_after_ins (bb, ins, tmp);
 11415| 						}
 11416| 						continue;
 11417| 					}
 11418| 					g_assert (var->opcode == OP_REGOFFSET);
 11419| 					load_opcode = mono_type_to_load_membase (cfg, var->inst_vtype);
 11420| 					g_assert (load_opcode != OP_LOADV_MEMBASE);
 11421| 					if (vreg_to_lvreg [sreg]) {
 11422| 						g_assert (vreg_to_lvreg [sreg] != -1);
 11423| 						/* The variable is already loaded to an lvreg */
 11424| 						if (G_UNLIKELY (cfg->verbose_level > 2))
 11425| 							printf ("\t\tUse lvreg R%d for R%d.\n", vreg_to_lvreg [sreg], sreg);
 11426| 						sregs [srcindex] = vreg_to_lvreg [sreg];
 11427| 						continue;
 11428| 					}
 11429| 					/* Try to fuse the load into the instruction */
 11430| 					if ((srcindex == 0) && (op_to_op_src1_membase (cfg, load_opcode, ins->opcode) != -1)) {
 11431| 						ins->opcode = GINT_TO_OPCODE (op_to_op_src1_membase (cfg, load_opcode, ins->opcode));
 11432| 						sregs [0] = var->inst_basereg;
 11433| 						ins->inst_offset = var->inst_offset;
 11434| 					} else if ((srcindex == 1) && (op_to_op_src2_membase (cfg, load_opcode, ins->opcode) != -1)) {
 11435| 						ins->opcode = GINT_TO_OPCODE (op_to_op_src2_membase (cfg, load_opcode, ins->opcode));
 11436| 						sregs [1] = var->inst_basereg;
 11437| 						ins->inst_offset = var->inst_offset;
 11438| 					} else {
 11439| 						if (MONO_IS_REAL_MOVE (ins)) {
 11440| 							ins->opcode = OP_NOP;
 11441| 							sreg = ins->dreg;
 11442| 						} else {
 11443| 							sreg = alloc_dreg (cfg, stacktypes [regtype]);
 11444| 							if (!((var)->flags & (MONO_INST_VOLATILE|MONO_INST_INDIRECT)) && !no_lvreg) {
 11445| 								if (var->dreg == prev_dreg) {
 11446| 									/*
 11447| 									 * sreg refers to the value loaded by the load
 11448| 									 * emitted below, but we need to use ins->dreg
 11449| 									 * since it refers to the store emitted earlier.
 11450| 									 */
 11451| 									sreg = ins->dreg;
 11452| 								}
 11453| 								g_assert (sreg != -1);
 11454| 								if (var->dreg == dreg_using_dest_to_membase_op) {
 11455| 									if (cfg->verbose_level > 2)
 11456| 										printf ("\tCan't cache R%d because it's part of a dreg dest_membase optimization\n", var->dreg);
 11457| 								} else {
 11458| 									vreg_to_lvreg [var->dreg] = sreg;
 11459| 								}
 11460| 								if (lvregs_len >= lvregs_size) {
 11461| 									guint32 *new_lvregs = mono_mempool_alloc0 (cfg->mempool, sizeof (guint32) * lvregs_size * 2);
 11462| 									memcpy (new_lvregs, lvregs, sizeof (guint32) * lvregs_size);
 11463| 									lvregs = new_lvregs;
 11464| 									lvregs_size *= 2;
 11465| 								}
 11466| 								lvregs [lvregs_len ++] = var->dreg;
 11467| 							}
 11468| 						}
 11469| 						sregs [srcindex] = sreg;
 11470| #if SIZEOF_REGISTER != 8
 11471| 						if (regtype == 'l') {
 11472| 							NEW_LOAD_MEMBASE (cfg, load_ins, OP_LOADI4_MEMBASE, MONO_LVREG_MS (sreg), var->inst_basereg, var->inst_offset + MINI_MS_WORD_OFFSET);
 11473| 							mono_bblock_insert_before_ins (bb, ins, load_ins);
 11474| 							NEW_LOAD_MEMBASE (cfg, load_ins, OP_LOADI4_MEMBASE, MONO_LVREG_LS (sreg), var->inst_basereg, var->inst_offset + MINI_LS_WORD_OFFSET);
 11475| 							mono_bblock_insert_before_ins (bb, ins, load_ins);
 11476| 							use_ins = load_ins;
 11477| 						}
 11478| 						else
 11479| #endif
 11480| 						{
 11481| #if SIZEOF_REGISTER == 4
 11482| 							g_assert (load_opcode != OP_LOADI8_MEMBASE);
 11483| #endif
 11484| 							NEW_LOAD_MEMBASE (cfg, load_ins, load_opcode, sreg, var->inst_basereg, var->inst_offset);
 11485| 							if (load_ins->opcode == OP_LOADX_MEMBASE)
 11486| 								mini_type_to_eval_stack_type (cfg, var->inst_vtype, load_ins);
 11487| 							mono_bblock_insert_before_ins (bb, ins, load_ins);
 11488| 							use_ins = load_ins;
 11489| 						}
 11490| 						if (cfg->verbose_level > 2)
 11491| 							mono_print_ins_index (0, use_ins);
 11492| 					}
 11493| 					if (var->dreg < orig_next_vreg) {
 11494| 						live_range_end [var->dreg] = use_ins;
 11495| 						live_range_end_bb [var->dreg] = bb;
 11496| 					}
 11497| 					if (cfg->compute_gc_maps && var->dreg < orig_next_vreg && (var->flags & MONO_INST_GC_TRACK)) {
 11498| 						MonoInst *tmp;
 11499| 						MONO_INST_NEW (cfg, tmp, OP_GC_LIVENESS_USE);
 11500| 						tmp->inst_c1 = var->dreg;
 11501| 						mono_bblock_insert_after_ins (bb, ins, tmp);
 11502| 					}
 11503| 				}
 11504| 			}
 11505| 			mono_inst_set_src_registers (ins, sregs);
 11506| 			if (dest_has_lvreg) {
 11507| 				g_assert (ins->dreg != -1);
 11508| 				vreg_to_lvreg [prev_dreg] = ins->dreg;
 11509| 				if (lvregs_len >= lvregs_size) {
 11510| 					guint32 *new_lvregs = mono_mempool_alloc0 (cfg->mempool, sizeof (guint32) * lvregs_size * 2);
 11511| 					memcpy (new_lvregs, lvregs, sizeof (guint32) * lvregs_size);
 11512| 					lvregs = new_lvregs;
 11513| 					lvregs_size *= 2;
 11514| 				}
 11515| 				lvregs [lvregs_len ++] = prev_dreg;
 11516| 				dest_has_lvreg = FALSE;
 11517| 			}
 11518| 			if (store) {
 11519| 				tmp_reg = ins->dreg;
 11520| 				ins->dreg = ins->sreg2;
 11521| 				ins->sreg2 = tmp_reg;
 11522| 			}
 11523| 			if (MONO_IS_CALL (ins)) {
 11524| 				/* Clear vreg_to_lvreg array */
 11525| 				for (i = 0; i < lvregs_len; i++)
 11526| 					vreg_to_lvreg [lvregs [i]] = 0;
 11527| 				lvregs_len = 0;
 11528| 			} else if (ins->opcode == OP_NOP) {
 11529| 				ins->dreg = -1;
 11530| 				MONO_INST_NULLIFY_SREGS (ins);
 11531| 			}
 11532| 			if (cfg->verbose_level > 2)
 11533| 				mono_print_ins_index (1, ins);
 11534| 		}
 11535| 		/* Extend the live range based on the liveness info */
 11536| 		if (cfg->compute_precise_live_ranges && bb->live_out_set && bb->code) {
 11537| 			for (i = 0; i < cfg->num_varinfo; i ++) {
 11538| 				MonoMethodVar *vi = MONO_VARINFO (cfg, i);
 11539| 				if (vreg_is_volatile (cfg, vi->vreg))
 11540| 					/* The liveness info is incomplete */
 11541| 					continue;
 11542| 				if (mono_bitset_test_fast (bb->live_in_set, i) && !live_range_start [vi->vreg]) {
 11543| 					/* Live from at least the first ins of this bb */
 11544| 					live_range_start [vi->vreg] = bb->code;
 11545| 					live_range_start_bb [vi->vreg] = bb;
 11546| 				}
 11547| 				if (mono_bitset_test_fast (bb->live_out_set, i)) {
 11548| 					/* Live at least until the last ins of this bb */
 11549| 					live_range_end [vi->vreg] = bb->last_ins;
 11550| 					live_range_end_bb [vi->vreg] = bb;
 11551| 				}
 11552| 			}
 11553| 		}
 11554| 	}
 11555| 	/*
 11556| 	 * Emit LIVERANGE_START/LIVERANGE_END opcodes, the backend will implement them
 11557| 	 * by storing the current native offset into MonoMethodVar->live_range_start/end.
 11558| 	 */
 11559| 	if (cfg->compute_precise_live_ranges && cfg->comp_done & MONO_COMP_LIVENESS) {
 11560| 		for (i = 0; i < cfg->num_varinfo; ++i) {
 11561| 			int vreg = MONO_VARINFO (cfg, i)->vreg;
 11562| 			MonoInst *ins;
 11563| 			if (live_range_start [vreg]) {
 11564| 				MONO_INST_NEW (cfg, ins, OP_LIVERANGE_START);
 11565| 				ins->inst_c0 = i;
 11566| 				ins->inst_c1 = vreg;
 11567| 				mono_bblock_insert_after_ins (live_range_start_bb [vreg], live_range_start [vreg], ins);
 11568| 			}
 11569| 			if (live_range_end [vreg]) {
 11570| 				MONO_INST_NEW (cfg, ins, OP_LIVERANGE_END);
 11571| 				ins->inst_c0 = i;
 11572| 				ins->inst_c1 = vreg;
 11573| 				if (live_range_end [vreg] == live_range_end_bb [vreg]->last_ins)
 11574| 					mono_add_ins_to_end (live_range_end_bb [vreg], ins);
 11575| 				else
 11576| 					mono_bblock_insert_after_ins (live_range_end_bb [vreg], live_range_end [vreg], ins);
 11577| 			}
 11578| 		}
 11579| 	}
 11580| 	if (cfg->gsharedvt_locals_var_ins) {
 11581| 		/* Nullify if unused */
 11582| 		cfg->gsharedvt_locals_var_ins->opcode = OP_PCONST;
 11583| 		cfg->gsharedvt_locals_var_ins->inst_imm = 0;
 11584| 	}
 11585| 	g_free (live_range_start);
 11586| 	g_free (live_range_end);
 11587| 	g_free (live_range_start_bb);
 11588| 	g_free (live_range_end_bb);
 11589| }
 11590| /**
 11591|  * FIXME:
 11592|  * - use 'iadd' instead of 'int_add'
 11593|  * - handling ovf opcodes: decompose in method_to_ir.
 11594|  * - unify iregs/fregs
 11595|  *   -> partly done, the missing parts are:
 11596|  *   - a more complete unification would involve unifying the hregs as well, so
 11597|  *     code wouldn't need if (fp) all over the place. but that would mean the hregs
 11598|  *     would no longer map to the machine hregs, so the code generators would need to
 11599|  *     be modified. Also, on ia64 for example, niregs + nfregs > 256 -> bitmasks
 11600|  *     wouldn't work any more. Duplicating the code in mono_local_regalloc () into
 11601|  *     fp/non-fp branches speeds it up by about 15%.
 11602|  * - use sext/zext opcodes instead of shifts
 11603|  * - add OP_ICALL
 11604|  * - get rid of TEMPLOADs if possible and use vregs instead
 11605|  * - clean up usage of OP_P/OP_ opcodes
 11606|  * - cleanup usage of DUMMY_USE
 11607|  * - cleanup the setting of ins->type for MonoInst's which are pushed on the
 11608|  *   stack
 11609|  * - set the stack type and allocate a dreg in the EMIT_NEW macros
 11610|  * - get rid of all the <foo>2 stuff when the new JIT is ready.
 11611|  * - make sure handle_stack_args () is called before the branch is emitted
 11612|  * - when the new IR is done, get rid of all unused stuff
 11613|  * - COMPARE/BEQ as separate instructions or unify them ?
 11614|  *   - keeping them separate allows specialized compare instructions like
 11615|  *     compare_imm, compare_membase
 11616|  *   - most back ends unify fp compare+branch, fp compare+ceq
 11617|  * - integrate mono_save_args into inline_method
 11618|  * - get rid of the empty bblocks created by MONO_EMIT_NEW_BRACH_BLOCK2
 11619|  * - handle long shift opts on 32 bit platforms somehow: they require
 11620|  *   3 sregs (2 for arg1 and 1 for arg2)
 11621|  * - make byref a 'normal' type.
 11622|  * - use vregs for bb->out_stacks if possible, handle_global_vreg will make them a
 11623|  *   variable if needed.
 11624|  * - do not start a new IL level bblock when cfg->cbb is changed by a function call
 11625|  *   like inline_method.
 11626|  * - remove inlining restrictions
 11627|  * - fix LNEG and enable cfold of INEG
 11628|  * - generalize x86 optimizations like ldelema as a peephole optimization
 11629|  * - add store_mem_imm for amd64
 11630|  * - optimize the loading of the interruption flag in the managed->native wrappers
 11631|  * - avoid special handling of OP_NOP in passes
 11632|  * - move code inserting instructions into one function/macro.
 11633|  * - try a coalescing phase after liveness analysis
 11634|  * - add float -> vreg conversion + local optimizations on !x86
 11635|  * - figure out how to handle decomposed branches during optimizations, ie.
 11636|  *   compare+branch, op_jump_table+op_br etc.
 11637|  * - promote RuntimeXHandles to vregs
 11638|  * - vtype cleanups:
 11639|  *   - add a NEW_VARLOADA_VREG macro
 11640|  * - the vtype optimizations are blocked by the LDADDR opcodes generated for
 11641|  *   accessing vtype fields.
 11642|  * - get rid of I8CONST on 64 bit platforms
 11643|  * - dealing with the increase in code size due to branches created during opcode
 11644|  *   decomposition:
 11645|  *   - use extended basic blocks
 11646|  *     - all parts of the JIT
 11647|  *     - handle_global_vregs () && local regalloc
 11648|  *   - avoid introducing global vregs during decomposition, like 'vtable' in isinst
 11649|  * - sources of increase in code size:
 11650|  *   - vtypes
 11651|  *   - long compares
 11652|  *   - isinst and castclass
 11653|  *   - lvregs not allocated to global registers even if used multiple times
 11654|  * - call cctors outside the JIT, to make -v output more readable and JIT timings more
 11655|  *   meaningful.
 11656|  * - check for fp stack leakage in other opcodes too. (-> 'exceptions' optimization)
 11657|  * - add all micro optimizations from the old JIT
 11658|  * - put tree optimizations into the deadce pass
 11659|  * - decompose op_start_handler/op_endfilter/op_endfinally earlier using an arch
 11660|  *   specific function.
 11661|  * - unify the float comparison opcodes with the other comparison opcodes, i.e.
 11662|  *   fcompare + branchCC.
 11663|  * - create a helper function for allocating a stack slot, taking into account
 11664|  *   MONO_CFG_HAS_SPILLUP.
 11665|  * - merge r68207.
 11666|  * - optimize mono_regstate2_alloc_int/float.
 11667|  * - fix the pessimistic handling of variables accessed in exception handler blocks.
 11668|  * - need to write a tree optimization pass, but the creation of trees is difficult, i.e.
 11669|  *   parts of the tree could be separated by other instructions, killing the tree
 11670|  *   arguments, or stores killing loads etc. Also, should we fold loads into other
 11671|  *   instructions if the result of the load is used multiple times ?
 11672|  * - make the REM_IMM optimization in mini-x86.c arch-independent.
 11673|  * - LAST MERGE: 108395.
 11674|  * - when returning vtypes in registers, generate IR and append it to the end of the
 11675|  *   last bb instead of doing it in the epilog.
 11676|  * - change the store opcodes so they use sreg1 instead of dreg to store the base register.
 11677|  */
 11678| /*
 11679| NOTES
 11680| -----
 11681| - When to decompose opcodes:
 11682|   - earlier: this makes some optimizations hard to implement, since the low level IR
 11683|   no longer contains the necessary information. But it is easier to do.
 11684|   - later: harder to implement, enables more optimizations.
 11685| - Branches inside bblocks:
 11686|   - created when decomposing complex opcodes.
 11687|     - branches to another bblock: harmless, but not tracked by the branch
 11688|       optimizations, so need to branch to a label at the start of the bblock.
 11689|     - branches to inside the same bblock: very problematic, trips up the local
 11690|       reg allocator. Can be fixed by spitting the current bblock, but that is a
 11691|       complex operation, since some local vregs can become global vregs etc.
 11692| - Local/global vregs:
 11693|   - local vregs: temporary vregs used inside one bblock. Assigned to hregs by the
 11694|     local register allocator.
 11695|   - global vregs: used in more than one bblock. Have an associated MonoMethodVar
 11696|     structure, created by mono_create_var (). Assigned to hregs or the stack by
 11697|     the global register allocator.
 11698| - When to do optimizations like alu->alu_imm:
 11699|   - earlier -> saves work later on since the IR will be smaller/simpler
 11700|   - later -> can work on more instructions
 11701| - Handling of valuetypes:
 11702|   - When a vtype is pushed on the stack, a new temporary is created, an
 11703|     instruction computing its address (LDADDR) is emitted and pushed on
 11704|     the stack. Need to optimize cases when the vtype is used immediately as in
 11705|     argument passing, stloc etc.
 11706| - Instead of the to_end stuff in the old JIT, simply call the function handling
 11707|   the values on the stack before emitting the last instruction of the bb.
 11708| */
 11709| #else /* !DISABLE_JIT */
 11710| MONO_EMPTY_SOURCE_FILE (method_to_ir);
 11711| #endif /* !DISABLE_JIT */


# ====================================================================
# FILE: src/tasks/Microsoft.NET.Sdk.WebAssembly.Pack.Tasks/ComputeWasmBuildAssets.cs
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-226 ---
     1| // Licensed to the .NET Foundation under one or more agreements.
     2| using System;
     3| using System.Collections.Generic;
     4| using System.IO;
     5| using System.Linq;
     6| using Microsoft.Build.Framework;
     7| using Microsoft.Build.Utilities;
     8| using Microsoft.NET.Sdk.WebAssembly;
     9| namespace Microsoft.NET.Sdk.WebAssembly;
    10| public class ComputeWasmBuildAssets : Task
    11| {
    12|     [Required]
    13|     public ITaskItem[] Candidates { get; set; }
    14|     public ITaskItem CustomIcuCandidate { get; set; }
    15|     [Required]
    16|     public ITaskItem[] ProjectAssembly { get; set; }
    17|     [Required]
    18|     public ITaskItem[] ProjectDebugSymbols { get; set; }
    19|     [Required]
    20|     public ITaskItem[] SatelliteAssemblies { get; set; }
    21|     [Required]
    22|     public ITaskItem[] ProjectSatelliteAssemblies { get; set; }
    23|     [Required]
    24|     public string DotNetJsVersion { get; set; }
    25|     [Required]
    26|     public string OutputPath { get; set; }
    27|     [Required]
    28|     public bool TimeZoneSupport { get; set; }
    29|     [Required]
    30|     public bool InvariantGlobalization { get; set; }
    31|     [Required]
    32|     public bool HybridGlobalization { get; set; }
    33|     [Required]
    34|     public bool LoadFullICUData { get; set; }
    35|     [Required]
    36|     public bool CopySymbols { get; set; }
    37|     public bool FingerprintDotNetJs { get; set; }
    38|     public bool EnableThreads { get; set; }
    39|     public bool EmitSourceMap { get; set; }
    40|     [Output]
    41|     public ITaskItem[] AssetCandidates { get; set; }
    42|     [Output]
    43|     public ITaskItem[] FilesToRemove { get; set; }
    44|     public override bool Execute()
    45|     {
    46|         var filesToRemove = new List<ITaskItem>();
    47|         var assetCandidates = new List<ITaskItem>();
    48|         try
    49|         {
    50|             if (ProjectAssembly.Length != 1)
    51|             {
    52|                 Log.LogError("Invalid number of project assemblies '{0}'", string.Join("," + Environment.NewLine, ProjectAssembly.Select(a => a.ItemSpec)));
    53|                 return true;
    54|             }
    55|             if (ProjectDebugSymbols.Length > 1)
    56|             {
    57|                 Log.LogError("Invalid number of symbol assemblies '{0}'", string.Join("," + Environment.NewLine, ProjectDebugSymbols.Select(a => a.ItemSpec)));
    58|                 return true;
    59|             }
    60|             if (!AssetsComputingHelper.TryGetAssetFilename(CustomIcuCandidate, out string customIcuCandidateFilename))
    61|             {
    62|                 Log.LogMessage(MessageImportance.Low, "Custom icu asset was passed as empty.");
    63|             }
    64|             for (int i = 0; i < Candidates.Length; i++)
    65|             {
    66|                 var candidate = Candidates[i];
    67|                 if (AssetsComputingHelper.ShouldFilterCandidate(candidate, TimeZoneSupport, InvariantGlobalization, HybridGlobalization, LoadFullICUData, CopySymbols, customIcuCandidateFilename, EnableThreads, EmitSourceMap, out var reason))
    68|                 {
    69|                     Log.LogMessage(MessageImportance.Low, "Skipping asset '{0}' because '{1}'", candidate.ItemSpec, reason);
    70|                     filesToRemove.Add(candidate);
    71|                     continue;
    72|                 }
    73|                 var satelliteAssembly = SatelliteAssemblies.FirstOrDefault(s => s.ItemSpec == candidate.ItemSpec);
    74|                 if (satelliteAssembly != null)
    75|                 {
    76|                     var inferredCulture = satelliteAssembly.GetMetadata("DestinationSubDirectory").Trim('\\', '/');
    77|                     Log.LogMessage(MessageImportance.Low, "Found satellite assembly '{0}' asset for candidate '{1}' with inferred culture '{2}'", satelliteAssembly.ItemSpec, candidate.ItemSpec, inferredCulture);
    78|                     var assetCandidate = new TaskItem(satelliteAssembly);
    79|                     assetCandidate.SetMetadata("AssetKind", "Build");
    80|                     assetCandidate.SetMetadata("AssetRole", "Related");
    81|                     assetCandidate.SetMetadata("AssetTraitName", "Culture");
    82|                     assetCandidate.SetMetadata("AssetTraitValue", inferredCulture);
    83|                     assetCandidate.SetMetadata("RelativePath", $"_framework/{inferredCulture}/{satelliteAssembly.GetMetadata("FileName")}{satelliteAssembly.GetMetadata("Extension")}");
    84|                     var resolvedFrom = assetCandidate.GetMetadata("ResolvedFrom");
    85|                     if (resolvedFrom == "{RawFileName}") // Satellite assembly found from `<Reference />` element
    86|                         resolvedFrom = candidate.GetMetadata("OriginalItemSpec");
    87|                     assetCandidate.SetMetadata("RelatedAsset", Path.GetFullPath(Path.Combine(OutputPath, "wwwroot", "_framework", Path.GetFileName(resolvedFrom))));
    88|                     assetCandidates.Add(assetCandidate);
    89|                     continue;
    90|                 }
    91|                 string candidateFileName = candidate.GetMetadata("FileName");
    92|                 if (candidateFileName.StartsWith("dotnet") && candidate.GetMetadata("Extension") == ".js")
    93|                 {
    94|                     string newDotnetJSFileName = null;
    95|                     string newDotNetJSFullPath = null;
    96|                     if (candidateFileName != "dotnet" || FingerprintDotNetJs)
    97|                     {
    98|                         var itemHash = FileHasher.GetFileHash(candidate.ItemSpec);
    99|                         newDotnetJSFileName = $"{candidateFileName}.{DotNetJsVersion}.{itemHash}.js";
   100|                         var originalFileFullPath = Path.GetFullPath(candidate.ItemSpec);
   101|                         var originalFileDirectory = Path.GetDirectoryName(originalFileFullPath);
   102|                         newDotNetJSFullPath = Path.Combine(originalFileDirectory, newDotnetJSFileName);
   103|                     }
   104|                     else
   105|                     {
   106|                         newDotNetJSFullPath = candidate.ItemSpec;
   107|                         newDotnetJSFileName = Path.GetFileName(newDotNetJSFullPath);
   108|                     }
   109|                     var newDotNetJs = new TaskItem(newDotNetJSFullPath, candidate.CloneCustomMetadata());
   110|                     newDotNetJs.SetMetadata("OriginalItemSpec", candidate.ItemSpec);
   111|                     var newRelativePath = $"_framework/{newDotnetJSFileName}";
   112|                     newDotNetJs.SetMetadata("RelativePath", newRelativePath);
   113|                     newDotNetJs.SetMetadata("AssetTraitName", "WasmResource");
   114|                     newDotNetJs.SetMetadata("AssetTraitValue", "native");
   115|                     assetCandidates.Add(newDotNetJs);
   116|                     continue;
   117|                 }
   118|                 else
   119|                 {
   120|                     string relativePath = AssetsComputingHelper.GetCandidateRelativePath(candidate);
   121|                     candidate.SetMetadata("RelativePath", relativePath);
   122|                 }
   123|                 if (candidate.GetMetadata("ReferenceSourceTarget") == "ProjectReference")
   124|                 {
   125|                     candidate.SetMetadata("OriginalItemSpec", candidate.ItemSpec);
   126|                 }
   127|                 var culture = candidate.GetMetadata("Culture");
   128|                 if (!string.IsNullOrEmpty(culture))
   129|                 {
   130|                     candidate.SetMetadata("AssetKind", "Build");
   131|                     candidate.SetMetadata("AssetRole", "Related");
   132|                     candidate.SetMetadata("AssetTraitName", "Culture");
   133|                     candidate.SetMetadata("AssetTraitValue", culture);
   134|                     var fileName = candidate.GetMetadata("FileName");
   135|                     var suffixIndex = fileName.Length - ".resources".Length;
   136|                     var relatedAssetPath = Path.GetFullPath(Path.Combine(
   137|                         OutputPath,
   138|                         "wwwroot",
   139|                         "_framework",
   140|                         fileName.Substring(0, suffixIndex) + ProjectAssembly[0].GetMetadata("Extension")));
   141|                     candidate.SetMetadata("RelatedAsset", relatedAssetPath);
   142|                     Log.LogMessage(MessageImportance.Low, "Found satellite assembly '{0}' asset for inferred candidate '{1}' with culture '{2}'", candidate.ItemSpec, relatedAssetPath, culture);
   143|                 }
   144|                 assetCandidates.Add(candidate);
   145|             }
   146|             var intermediateAssembly = new TaskItem(ProjectAssembly[0]);
   147|             intermediateAssembly.SetMetadata("RelativePath", $"_framework/{intermediateAssembly.GetMetadata("FileName")}{intermediateAssembly.GetMetadata("Extension")}");
   148|             assetCandidates.Add(intermediateAssembly);
   149|             if (ProjectDebugSymbols.Length > 0)
   150|             {
   151|                 var debugSymbols = new TaskItem(ProjectDebugSymbols[0]);
   152|                 debugSymbols.SetMetadata("RelativePath", $"_framework/{debugSymbols.GetMetadata("FileName")}{debugSymbols.GetMetadata("Extension")}");
   153|                 assetCandidates.Add(debugSymbols);
   154|             }
   155|             for (int i = 0; i < ProjectSatelliteAssemblies.Length; i++)
   156|             {
   157|                 var projectSatelliteAssembly = ProjectSatelliteAssemblies[i];
   158|                 var candidateCulture = projectSatelliteAssembly.GetMetadata("Culture");
   159|                 Log.LogMessage(
   160|                     "Found satellite assembly '{0}' asset for project '{1}' with culture '{2}'",
   161|                     projectSatelliteAssembly.ItemSpec,
   162|                     intermediateAssembly.ItemSpec,
   163|                     candidateCulture);
   164|                 var assetCandidate = new TaskItem(Path.GetFullPath(projectSatelliteAssembly.ItemSpec), projectSatelliteAssembly.CloneCustomMetadata());
   165|                 var projectAssemblyAssetPath = Path.GetFullPath(Path.Combine(
   166|                     OutputPath,
   167|                     "wwwroot",
   168|                     "_framework",
   169|                     ProjectAssembly[0].GetMetadata("FileName") + ProjectAssembly[0].GetMetadata("Extension")));
   170|                 var normalizedPath = assetCandidate.GetMetadata("TargetPath").Replace('\\', '/');
   171|                 assetCandidate.SetMetadata("AssetKind", "Build");
   172|                 assetCandidate.SetMetadata("AssetRole", "Related");
   173|                 assetCandidate.SetMetadata("AssetTraitName", "Culture");
   174|                 assetCandidate.SetMetadata("AssetTraitValue", candidateCulture);
   175|                 assetCandidate.SetMetadata("RelativePath", Path.Combine("_framework", normalizedPath));
   176|                 assetCandidate.SetMetadata("RelatedAsset", projectAssemblyAssetPath);
   177|                 assetCandidates.Add(assetCandidate);
   178|             }
   179|             for (var i = 0; i < assetCandidates.Count; i++)
   180|             {
   181|                 var candidate = assetCandidates[i];
   182|                 ApplyUniqueMetadataProperties(candidate);
   183|             }
   184|         }
   185|         catch (Exception ex)
   186|         {
   187|             Log.LogError(ex.ToString());
   188|             return false;
   189|         }
   190|         FilesToRemove = filesToRemove.ToArray();
   191|         AssetCandidates = assetCandidates.ToArray();
   192|         return !Log.HasLoggedErrors;
   193|     }
   194|     private static void ApplyUniqueMetadataProperties(ITaskItem candidate)
   195|     {
   196|         var extension = candidate.GetMetadata("Extension");
   197|         var filename = candidate.GetMetadata("FileName");
   198|         switch (extension)
   199|         {
   200|             case ".dll":
   201|                 if (string.IsNullOrEmpty(candidate.GetMetadata("AssetTraitName")))
   202|                 {
   203|                     candidate.SetMetadata("AssetTraitName", "WasmResource");
   204|                     candidate.SetMetadata("AssetTraitValue", "runtime");
   205|                 }
   206|                 if (string.Equals(candidate.GetMetadata("ResolvedFrom"), "{HintPathFromItem}", StringComparison.Ordinal))
   207|                 {
   208|                     candidate.RemoveMetadata("OriginalItemSpec");
   209|                 }
   210|                 break;
   211|             case ".wasm":
   212|             case ".blat":
   213|             case ".dat" when filename.StartsWith("icudt"):
   214|                 candidate.SetMetadata("AssetTraitName", "WasmResource");
   215|                 candidate.SetMetadata("AssetTraitValue", "native");
   216|                 break;
   217|             case ".pdb":
   218|                 candidate.SetMetadata("AssetTraitName", "WasmResource");
   219|                 candidate.SetMetadata("AssetTraitValue", "symbol");
   220|                 candidate.RemoveMetadata("OriginalItemSpec");
   221|                 break;
   222|             default:
   223|                 break;
   224|         }
   225|     }
   226| }

