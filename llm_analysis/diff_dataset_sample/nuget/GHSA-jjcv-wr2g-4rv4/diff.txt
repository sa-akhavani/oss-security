--- a//dev/null
+++ b/.devcontainer/scripts/onCreateCommand.sh
@@ -0,0 +1,15 @@
+set -e
+opt=$1
+case "$opt" in
+    libraries)
+        ./build.sh libs+clr -rc Release
+        ./build.sh libs.tests -restore
+    ;;
+    wasm)
+        make -C src/mono/wasm provision-wasm
+        export EMSDK_PATH=$PWD/src/mono/wasm/emsdk
+        ./build.sh mono+libs -os browser -c Release
+        ./dotnet.sh tool install dotnet-serve --tool-path ./.dotnet-tools-global
+    ;;
+esac
+git rev-parse HEAD > ./artifacts/prebuild.sha

--- a//dev/null
+++ b/eng/build.sh
@@ -0,0 +1,495 @@
+set -ue
+source="${BASH_SOURCE[0]}"
+while [[ -h "$source" ]]; do
+  scriptroot="$( cd -P "$( dirname "$source" )" && pwd )"
+  source="$(readlink "$source")"
+  [[ $source != /* ]] && source="$scriptroot/$source"
+done
+scriptroot="$( cd -P "$( dirname "$source" )" && pwd )"
+usage()
+{
+  echo "Common settings:"
+  echo "  --arch (-a)                     Target platform: x86, x64, arm, armv6, armel, arm64, loongarch64, riscv64, s390x, ppc64le or wasm."
+  echo "                                  [Default: Your machine's architecture.]"
+  echo "  --binaryLog (-bl)               Output binary log."
+  echo "  --cross                         Optional argument to signify cross compilation."
+  echo "  --configuration (-c)            Build configuration: Debug, Release or Checked."
+  echo "                                  Checked is exclusive to the CLR subset. It is the same as Debug, except code is"
+  echo "                                  compiled with optimizations enabled."
+  echo "                                  [Default: Debug]"
+  echo "  --help (-h)                     Print help and exit."
+  echo "  --hostConfiguration (-hc)       Host build configuration: Debug, Release or Checked."
+  echo "                                  [Default: Debug]"
+  echo "  --librariesConfiguration (-lc)  Libraries build configuration: Debug or Release."
+  echo "                                  [Default: Debug]"
+  echo "  --os                            Target operating system: windows, linux, freebsd, osx, maccatalyst, tvos,"
+  echo "                                  tvossimulator, ios, iossimulator, android, browser, wasi, netbsd, illumos, solaris"
+  echo "                                  linux-musl, linux-bionic or haiku."
+  echo "                                  [Default: Your machine's OS.]"
+  echo "  --outputrid <rid>               Optional argument that overrides the target rid name."
+  echo "  --projects <value>              Project or solution file(s) to build."
+  echo "  --runtimeConfiguration (-rc)    Runtime build configuration: Debug, Release or Checked."
+  echo "                                  Checked is exclusive to the CLR runtime. It is the same as Debug, except code is"
+  echo "                                  compiled with optimizations enabled."
+  echo "                                  [Default: Debug]"
+  echo "  -runtimeFlavor (-rf)            Runtime flavor: CoreCLR or Mono."
+  echo "                                  [Default: CoreCLR]"
+  echo "  --subset (-s)                   Build a subset, print available subsets with -subset help."
+  echo "                                 '--subset' can be omitted if the subset is given as the first argument."
+  echo "                                  [Default: Builds the entire repo.]"
+  echo "  --usemonoruntime                Product a .NET runtime with Mono as the underlying runtime."
+  echo "  --verbosity (-v)                MSBuild verbosity: q[uiet], m[inimal], n[ormal], d[etailed], and diag[nostic]."
+  echo "                                  [Default: Minimal]"
+  echo ""
+  echo "Actions (defaults to --restore --build):"
+  echo "  --build (-b)               Build all source projects."
+  echo "                             This assumes --restore has been run already."
+  echo "  --clean                    Clean the solution."
+  echo "  --pack                     Package build outputs into NuGet packages."
+  echo "  --publish                  Publish artifacts (e.g. symbols)."
+  echo "                             This assumes --build has been run already."
+  echo "  --rebuild                  Rebuild all source projects."
+  echo "  --restore (-r)             Restore dependencies."
+  echo "  --sign                     Sign build outputs."
+  echo "  --test (-t)                Incrementally builds and runs tests."
+  echo "                             Use in conjunction with --testnobuild to only run tests."
+  echo ""
+  echo "Libraries settings:"
+  echo "  --allconfigurations        Build packages for all build configurations."
+  echo "  --coverage                 Collect code coverage when testing."
+  echo "  --framework (-f)           Build framework: net8.0 or net48."
+  echo "                             [Default: net8.0]"
+  echo "  --testnobuild              Skip building tests when invoking -test."
+  echo "  --testscope                Test scope, allowed values: innerloop, outerloop, all."
+  echo ""
+  echo "Native build settings:"
+  echo "  --clang                    Optional argument to build using clang in PATH (default)."
+  echo "  --clangx                   Optional argument to build using clang version x (used for Clang 7 and newer)."
+  echo "  --clangx.y                 Optional argument to build using clang version x.y (used for Clang 6 and older)."
+  echo "  --cmakeargs                User-settable additional arguments passed to CMake."
+  echo "  --gcc                      Optional argument to build using gcc in PATH (default)."
+  echo "  --gccx.y                   Optional argument to build using gcc version x.y."
+  echo "  --portablebuild            Optional argument: set to false to force a non-portable build."
+  echo "  --keepnativesymbols        Optional argument: set to true to keep native symbols/debuginfo in generated binaries."
+  echo "  --ninja                    Optional argument: set to true to use Ninja instead of Make to run the native build."
+  echo "  --pgoinstrument            Optional argument: build PGO-instrumented runtime"
+  echo "  --fsanitize                Optional argument: Specify native sanitizers to instrument the native build with. Supported values are: 'address'."
+  echo ""
+  echo "Command line arguments starting with '/p:' are passed through to MSBuild."
+  echo "Arguments can also be passed in with a single hyphen."
+  echo ""
+  echo "Here are some quick examples. These assume you are on a Linux x64 machine:"
+  echo ""
+  echo "* Build CoreCLR for Linux x64 on Release configuration:"
+  echo "./build.sh clr -c release"
+  echo ""
+  echo "* Build Debug libraries with a Release runtime for Linux x64."
+  echo "./build.sh clr+libs -rc release"
+  echo ""
+  echo "* Build Release libraries and their tests with a Checked runtime for Linux x64, and run the tests."
+  echo "./build.sh clr+libs+libs.tests -rc checked -lc release -test"
+  echo ""
+  echo "* Build CoreCLR for Linux x64 on Debug configuration using Clang 9."
+  echo "./build.sh clr -clang9"
+  echo ""
+  echo "* Build CoreCLR for Linux x64 on Debug configuration using GCC 8.4."
+  echo "./build.sh clr -gcc8.4"
+  echo ""
+  echo "* Build CoreCLR for Linux x64 using extra compiler flags (-fstack-clash-protection)."
+  echo "EXTRA_CFLAGS=-fstack-clash-protection EXTRA_CXXFLAGS=-fstack-clash-protection ./build.sh clr"
+  echo ""
+  echo "* Cross-compile CoreCLR runtime for Linux ARM64 on Release configuration."
+  echo "./build.sh clr.runtime -arch arm64 -c release -cross"
+  echo ""
+  echo "However, for this example, you need to already have ROOTFS_DIR set up."
+  echo "Further information on this can be found here:"
+  echo "https://github.com/dotnet/runtime/blob/main/docs/workflow/building/coreclr/linux-instructions.md"
+  echo ""
+  echo "* Build Mono runtime for Linux x64 on Release configuration."
+  echo "./build.sh mono -c release"
+  echo ""
+  echo "* Build Release coreclr corelib, crossgen corelib and update Debug libraries testhost to run test on an updated corelib."
+  echo "./build.sh clr.corelib+clr.nativecorelib+libs.pretest -rc release"
+  echo ""
+  echo "* Build Debug mono corelib and update Release libraries testhost to run test on an updated corelib."
+  echo "./build.sh mono.corelib+libs.pretest -rc debug -c release"
+  echo ""
+  echo ""
+  echo "For more general information, check out https://github.com/dotnet/runtime/blob/main/docs/workflow/README.md"
+}
+initDistroRid()
+{
+    source "$scriptroot"/native/init-distro-rid.sh
+    local passedRootfsDir=""
+    local targetOs="$1"
+    local targetArch="$2"
+    local isCrossBuild="$3"
+    local isPortableBuild="$4"
+    if [[ $isCrossBuild == 1 && "$targetOs" != "osx" && "$targetOs" != "ios" && "$targetOs" != "iossimulator" && "$targetOs" != "tvos" && "$targetOs" != "tvossimulator" && "$targetOs" != "maccatalyst" ]]; then
+        passedRootfsDir=${ROOTFS_DIR}
+    fi
+    initDistroRidGlobal "${targetOs}" "${targetArch}" "${isPortableBuild}" "${passedRootfsDir}"
+}
+showSubsetHelp()
+{
+  "$scriptroot/common/build.sh" "-restore" "-build" "/p:Subset=help" "/clp:nosummary"
+}
+arguments=''
+cmakeargs=''
+extraargs=''
+crossBuild=0
+portableBuild=1
+source $scriptroot/native/init-os-and-arch.sh
+hostArch=$arch
+declare -a actions=("b" "build" "r" "restore" "rebuild" "testnobuild" "sign" "publish" "clean")
+actInt=($(comm -12 <(printf '%s\n' "${actions[@]/#/-}" | sort) <(printf '%s\n' "${@/#--/-}" | sort)))
+firstArgumentChecked=0
+while [[ $# > 0 ]]; do
+  opt="$(echo "${1/#--/-}" | tr "[:upper:]" "[:lower:]")"
+  if [[ $firstArgumentChecked -eq 0 && $opt =~ ^[a-zA-Z.+]+$ ]]; then
+    if [[ "$opt" == "help" ]]; then
+      showSubsetHelp
+      exit 0
+    fi
+    arguments="$arguments /p:Subset=$1"
+    shift 1
+    continue
+  fi
+  firstArgumentChecked=1
+  case "$opt" in
+     -help|-h|-\?|/?)
+      usage
+      exit 0
+      ;;
+     -subset|-s)
+      if [ -z ${2+x} ]; then
+        showSubsetHelp
+        exit 0
+      else
+        passedSubset="$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+        if [[ "$passedSubset" == "help" ]]; then
+          showSubsetHelp
+          exit 0
+        fi
+        arguments="$arguments /p:Subset=$2"
+        shift 2
+      fi
+      ;;
+     -arch|-a)
+      if [ -z ${2+x} ]; then
+        echo "No architecture supplied. See help (--help) for supported architectures." 1>&2
+        exit 1
+      fi
+      passedArch="$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+      case "$passedArch" in
+        x64|x86|arm|armv6|armel|arm64|loongarch64|riscv64|s390x|ppc64le|wasm)
+          arch=$passedArch
+          ;;
+        *)
+          echo "Unsupported target architecture '$2'."
+          echo "The allowed values are x86, x64, arm, armv6, armel, arm64, loongarch64, riscv64, s390x, ppc64le and wasm."
+          exit 1
+          ;;
+      esac
+      shift 2
+      ;;
+     -configuration|-c)
+      if [ -z ${2+x} ]; then
+        echo "No configuration supplied. See help (--help) for supported configurations." 1>&2
+        exit 1
+      fi
+      passedConfig="$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+      case "$passedConfig" in
+        debug|release|checked)
+          val="$(tr '[:lower:]' '[:upper:]' <<< ${passedConfig:0:1})${passedConfig:1}"
+          ;;
+        *)
+          echo "Unsupported target configuration '$2'."
+          echo "The allowed values are Debug, Release, and Checked."
+          exit 1
+          ;;
+      esac
+      arguments="$arguments -configuration $val"
+      shift 2
+      ;;
+     -framework|-f)
+      if [ -z ${2+x} ]; then
+        echo "No framework supplied. See help (--help) for supported frameworks." 1>&2
+        exit 1
+      fi
+      val="$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+      arguments="$arguments /p:BuildTargetFramework=$val"
+      shift 2
+      ;;
+     -os)
+      if [ -z ${2+x} ]; then
+        echo "No target operating system supplied. See help (--help) for supported target operating systems." 1>&2
+        exit 1
+      fi
+      passedOS="$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+      case "$passedOS" in
+        windows)
+          os="windows" ;;
+        linux)
+          os="linux" ;;
+        freebsd)
+          os="freebsd" ;;
+        osx)
+          os="osx" ;;
+        maccatalyst)
+          os="maccatalyst" ;;
+        tvos)
+          os="tvos" ;;
+        tvossimulator)
+          os="tvossimulator" ;;
+        ios)
+          os="ios" ;;
+        iossimulator)
+          os="iossimulator" ;;
+        android)
+          os="android" ;;
+        browser)
+          os="browser" ;;
+        wasi)
+          os="wasi" ;;
+        illumos)
+          os="illumos" ;;
+        solaris)
+          os="solaris" ;;
+        linux-bionic)
+          os="linux"
+          __PortableTargetOS=linux-bionic
+          ;;
+        linux-musl)
+          os="linux"
+          __PortableTargetOS=linux-musl
+          ;;
+        haiku)
+          os="haiku" ;;
+        *)
+          echo "Unsupported target OS '$2'."
+          echo "Try 'build.sh --help' for values supported by '--os'."
+          exit 1
+          ;;
+      esac
+      arguments="$arguments /p:TargetOS=$os"
+      shift 2
+      ;;
+     -allconfigurations)
+      arguments="$arguments /p:BuildAllConfigurations=true"
+      shift 1
+      ;;
+     -testscope)
+      if [ -z ${2+x} ]; then
+        echo "No test scope supplied. See help (--help) for supported test scope values." 1>&2
+        exit 1
+      fi
+      arguments="$arguments /p:TestScope=$2"
+      shift 2
+      ;;
+     -testnobuild)
+      arguments="$arguments /p:TestNoBuild=true"
+      shift 1
+      ;;
+     -coverage)
+      arguments="$arguments /p:Coverage=true"
+      shift 1
+      ;;
+     -runtimeconfiguration|-rc)
+      if [ -z ${2+x} ]; then
+        echo "No runtime configuration supplied. See help (--help) for supported runtime configurations." 1>&2
+        exit 1
+      fi
+      passedRuntimeConf="$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+      case "$passedRuntimeConf" in
+        debug|release|checked)
+          val="$(tr '[:lower:]' '[:upper:]' <<< ${passedRuntimeConf:0:1})${passedRuntimeConf:1}"
+          ;;
+        *)
+          echo "Unsupported runtime configuration '$2'."
+          echo "The allowed values are Debug, Release, and Checked."
+          exit 1
+          ;;
+      esac
+      arguments="$arguments /p:RuntimeConfiguration=$val"
+      shift 2
+      ;;
+     -runtimeflavor|-rf)
+      if [ -z ${2+x} ]; then
+        echo "No runtime flavor supplied. See help (--help) for supported runtime flavors." 1>&2
+        exit 1
+      fi
+      passedRuntimeFlav="$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+      case "$passedRuntimeFlav" in
+        coreclr|mono)
+          val="$(tr '[:lower:]' '[:upper:]' <<< ${passedRuntimeFlav:0:1})${passedRuntimeFlav:1}"
+          ;;
+        *)
+          echo "Unsupported runtime flavor '$2'."
+          echo "The allowed values are CoreCLR and Mono."
+          exit 1
+          ;;
+      esac
+      arguments="$arguments /p:RuntimeFlavor=$val"
+      shift 2
+      ;;
+     -usemonoruntime)
+      arguments="$arguments /p:PrimaryRuntimeFlavor=Mono"
+      shift 1
+      ;;
+     -librariesconfiguration|-lc)
+      if [ -z ${2+x} ]; then
+        echo "No libraries configuration supplied. See help (--help) for supported libraries configurations." 1>&2
+        exit 1
+      fi
+      passedLibConf="$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+      case "$passedLibConf" in
+        debug|release)
+          val="$(tr '[:lower:]' '[:upper:]' <<< ${passedLibConf:0:1})${passedLibConf:1}"
+          ;;
+        *)
+          echo "Unsupported libraries configuration '$2'."
+          echo "The allowed values are Debug and Release."
+          exit 1
+          ;;
+      esac
+      arguments="$arguments /p:LibrariesConfiguration=$val"
+      shift 2
+      ;;
+     -hostconfiguration|-hc)
+      if [ -z ${2+x} ]; then
+        echo "No host configuration supplied. See help (--help) for supported host configurations." 1>&2
+        exit 1
+      fi
+      passedHostConf="$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+      case "$passedHostConf" in
+        debug|release|checked)
+          val="$(tr '[:lower:]' '[:upper:]' <<< ${passedHostConf:0:1})${passedHostConf:1}"
+          ;;
+        *)
+          echo "Unsupported host configuration '$2'."
+          echo "The allowed values are Debug, Release, and Checked."
+          exit 1
+          ;;
+      esac
+      arguments="$arguments /p:HostConfiguration=$val"
+      shift 2
+      ;;
+     -cross)
+      crossBuild=1
+      arguments="$arguments /p:CrossBuild=True"
+      shift 1
+      ;;
+     *crossbuild=true*)
+      crossBuild=1
+      extraargs="$extraargs $1"
+      shift 1
+      ;;
+     -clang*)
+      compiler="${opt/#-/}" # -clang-9 => clang-9 or clang-9 => (unchanged)
+      arguments="$arguments /p:Compiler=$compiler /p:CppCompilerAndLinker=$compiler"
+      shift 1
+      ;;
+     -cmakeargs)
+      if [ -z ${2+x} ]; then
+        echo "No cmake args supplied." 1>&2
+        exit 1
+      fi
+      cmakeargs="${cmakeargs} $2"
+      shift 2
+      ;;
+     -gcc*)
+      compiler="${opt/#-/}" # -gcc-9 => gcc-9 or gcc-9 => (unchanged)
+      arguments="$arguments /p:Compiler=$compiler /p:CppCompilerAndLinker=$compiler"
+      shift 1
+      ;;
+     -outputrid)
+      if [ -z ${2+x} ]; then
+        echo "No value for outputrid is supplied. See help (--help) for supported values." 1>&2
+        exit 1
+      fi
+      arguments="$arguments /p:OutputRID=$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+      shift 2
+      ;;
+     -portablebuild)
+      if [ -z ${2+x} ]; then
+        echo "No value for portablebuild is supplied. See help (--help) for supported values." 1>&2
+        exit 1
+      fi
+      passedPortable="$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+      if [ "$passedPortable" = false ]; then
+        portableBuild=0
+        arguments="$arguments /p:PortableBuild=false"
+      fi
+      shift 2
+      ;;
+     -keepnativesymbols)
+      if [ -z ${2+x} ]; then
+        echo "No value for keepNativeSymbols is supplied. See help (--help) for supported values." 1>&2
+        exit 1
+      fi
+      passedKeepNativeSymbols="$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+      if [ "$passedKeepNativeSymbols" = true ]; then
+        arguments="$arguments /p:KeepNativeSymbols=true"
+      fi
+      shift 2
+      ;;
+      -ninja)
+      if [ -z ${2+x} ]; then
+        arguments="$arguments /p:Ninja=true"
+        shift 1
+      else
+        ninja="$(echo "$2" | tr "[:upper:]" "[:lower:]")"
+        if [ "$ninja" = true ]; then
+          arguments="$arguments /p:Ninja=true"
+          shift 2
+        elif [ "$ninja" = false ]; then
+          arguments="$arguments /p:Ninja=false"
+          shift 2
+        else
+          arguments="$arguments /p:Ninja=true"
+          shift 1
+        fi
+      fi
+      ;;
+      -pgoinstrument)
+      arguments="$arguments /p:PgoInstrument=true"
+      shift 1
+      ;;
+      -fsanitize)
+      if [ -z ${2+x} ]; then
+        echo "No value for -fsanitize is supplied. See help (--help) for supported values." 1>&2
+        exit 1
+      fi
+      arguments="$arguments /p:EnableNativeSanitizers=$2"
+      shift 2
+      ;;
+      -fsanitize=*)
+      sanitizers="${opt/#-fsanitize=/}" # -fsanitize=address => address
+      arguments="$arguments /p:EnableNativeSanitizers=$sanitizers"
+      shift 2
+      ;;
+      *)
+      extraargs="$extraargs $1"
+      shift 1
+      ;;
+  esac
+done
+if [ ${#actInt[@]} -eq 0 ]; then
+    arguments="-restore -build $arguments"
+fi
+if [[ "$os" == "browser" ]]; then
+    arch=wasm
+fi
+if [[ "$os" == "wasi" ]]; then
+    arch=wasm
+fi
+if [[ "${TreatWarningsAsErrors:-}" == "false" ]]; then
+    arguments="$arguments -warnAsError 0"
+fi
+initDistroRid "$os" "$arch" "$crossBuild" "$portableBuild"
+export DOTNETSDK_ALLOW_TARGETING_PACK_CACHING=0
+cmakeargs="${cmakeargs// /%20}"
+arguments="$arguments /p:TargetArchitecture=$arch /p:BuildArchitecture=$hostArch"
+arguments="$arguments /p:CMakeArgs=\"$cmakeargs\" $extraargs"
+"$scriptroot/common/build.sh" $arguments

--- a//dev/null
+++ b/eng/common/SetupNugetSources.sh
@@ -0,0 +1,110 @@
+ConfigFile=$1
+CredToken=$2
+NL='\n'
+TB='    '
+source="${BASH_SOURCE[0]}"
+while [[ -h "$source" ]]; do
+  scriptroot="$( cd -P "$( dirname "$source" )" && pwd )"
+  source="$(readlink "$source")"
+  [[ $source != /* ]] && source="$scriptroot/$source"
+done
+scriptroot="$( cd -P "$( dirname "$source" )" && pwd )"
+. "$scriptroot/tools.sh"
+if [ ! -f "$ConfigFile" ]; then
+    Write-PipelineTelemetryError -Category 'Build' "Error: Eng/common/SetupNugetSources.sh returned a non-zero exit code. Couldn't find the NuGet config file: $ConfigFile"
+    ExitWithExitCode 1
+fi
+if [ -z "$CredToken" ]; then
+    Write-PipelineTelemetryError -category 'Build' "Error: Eng/common/SetupNugetSources.sh returned a non-zero exit code. Please supply a valid PAT"
+    ExitWithExitCode 1
+fi
+if [[ `uname -s` == "Darwin" ]]; then
+    NL=$'\\\n'
+    TB=''
+fi
+grep -i "<packageSources>" $ConfigFile
+if [ "$?" != "0" ]; then
+    echo "Adding <packageSources>...</packageSources> section."
+    ConfigNodeHeader="<configuration>"
+    PackageSourcesTemplate="${TB}<packageSources>${NL}${TB}</packageSources>"
+    sed -i.bak "s|$ConfigNodeHeader|$ConfigNodeHeader${NL}$PackageSourcesTemplate|" $ConfigFile
+fi
+grep -i "<packageSourceCredentials>" $ConfigFile
+if [ "$?" != "0" ]; then
+    echo "Adding <packageSourceCredentials>...</packageSourceCredentials> section."
+    PackageSourcesNodeFooter="</packageSources>"
+    PackageSourceCredentialsTemplate="${TB}<packageSourceCredentials>${NL}${TB}</packageSourceCredentials>"
+    sed -i.bak "s|$PackageSourcesNodeFooter|$PackageSourcesNodeFooter${NL}$PackageSourceCredentialsTemplate|" $ConfigFile
+fi
+PackageSources=()
+grep -i "<add key=\"dotnet3.1\"" $ConfigFile
+if [ "$?" == "0" ]; then
+    grep -i "<add key=\"dotnet3.1-internal\"" $ConfigFile
+    if [ "$?" != "0" ]; then
+        echo "Adding dotnet3.1-internal to the packageSources."
+        PackageSourcesNodeFooter="</packageSources>"
+        PackageSourceTemplate="${TB}<add key=\"dotnet3.1-internal\" value=\"https://pkgs.dev.azure.com/dnceng/_packaging/dotnet3.1-internal/nuget/v2\" />"
+        sed -i.bak "s|$PackageSourcesNodeFooter|$PackageSourceTemplate${NL}$PackageSourcesNodeFooter|" $ConfigFile
+    fi
+    PackageSources+=('dotnet3.1-internal')
+    grep -i "<add key=\"dotnet3.1-internal-transport\">" $ConfigFile
+    if [ "$?" != "0" ]; then
+        echo "Adding dotnet3.1-internal-transport to the packageSources."
+        PackageSourcesNodeFooter="</packageSources>"
+        PackageSourceTemplate="${TB}<add key=\"dotnet3.1-internal-transport\" value=\"https://pkgs.dev.azure.com/dnceng/_packaging/dotnet3.1-internal-transport/nuget/v2\" />"
+        sed -i.bak "s|$PackageSourcesNodeFooter|$PackageSourceTemplate${NL}$PackageSourcesNodeFooter|" $ConfigFile
+    fi
+    PackageSources+=('dotnet3.1-internal-transport')
+fi
+DotNetVersions=('5' '6' '7' '8')
+for DotNetVersion in ${DotNetVersions[@]} ; do
+    FeedPrefix="dotnet${DotNetVersion}";
+    grep -i "<add key=\"$FeedPrefix\"" $ConfigFile
+    if [ "$?" == "0" ]; then
+        grep -i "<add key=\"$FeedPrefix-internal\"" $ConfigFile
+        if [ "$?" != "0" ]; then
+            echo "Adding $FeedPrefix-internal to the packageSources."
+            PackageSourcesNodeFooter="</packageSources>"
+            PackageSourceTemplate="${TB}<add key=\"$FeedPrefix-internal\" value=\"https://pkgs.dev.azure.com/dnceng/internal/_packaging/$FeedPrefix-internal/nuget/v2\" />"
+            sed -i.bak "s|$PackageSourcesNodeFooter|$PackageSourceTemplate${NL}$PackageSourcesNodeFooter|" $ConfigFile
+        fi
+        PackageSources+=("$FeedPrefix-internal")
+        grep -i "<add key=\"$FeedPrefix-internal-transport\">" $ConfigFile
+        if [ "$?" != "0" ]; then
+            echo "Adding $FeedPrefix-internal-transport to the packageSources."
+            PackageSourcesNodeFooter="</packageSources>"
+            PackageSourceTemplate="${TB}<add key=\"$FeedPrefix-internal-transport\" value=\"https://pkgs.dev.azure.com/dnceng/internal/_packaging/$FeedPrefix-internal-transport/nuget/v2\" />"
+            sed -i.bak "s|$PackageSourcesNodeFooter|$PackageSourceTemplate${NL}$PackageSourcesNodeFooter|" $ConfigFile
+        fi
+        PackageSources+=("$FeedPrefix-internal-transport")
+    fi
+done
+PrevIFS=$IFS
+IFS=$'\n'
+PackageSources+="$IFS"
+PackageSources+=$(grep -oh '"darc-int-[^"]*"' $ConfigFile | tr -d '"')
+IFS=$PrevIFS
+for FeedName in ${PackageSources[@]} ; do
+    grep -i "<$FeedName>" $ConfigFile 
+    if [ "$?" != "0" ]; then
+        echo "Adding credentials for $FeedName."
+        PackageSourceCredentialsNodeFooter="</packageSourceCredentials>"
+        NewCredential="${TB}${TB}<$FeedName>${NL}<add key=\"Username\" value=\"dn-bot\" />${NL}<add key=\"ClearTextPassword\" value=\"$CredToken\" />${NL}</$FeedName>"
+        sed -i.bak "s|$PackageSourceCredentialsNodeFooter|$NewCredential${NL}$PackageSourceCredentialsNodeFooter|" $ConfigFile
+    fi
+done
+grep -i "<disabledPackageSources>" $ConfigFile
+if [ "$?" == "0" ]; then
+    DisabledDarcIntSources=()
+    echo "Re-enabling any disabled \"darc-int\" package sources in $ConfigFile"
+    DisabledDarcIntSources+=$(grep -oh '"darc-int-[^"]*" value="true"' $ConfigFile  | tr -d '"')
+    for DisabledSourceName in ${DisabledDarcIntSources[@]} ; do
+        if [[ $DisabledSourceName == darc-int* ]]
+            then
+                OldDisableValue="<add key=\"$DisabledSourceName\" value=\"true\" />"
+                NewDisableValue="<!-- Reenabled for build : $DisabledSourceName -->"
+                sed -i.bak "s|$OldDisableValue|$NewDisableValue|" $ConfigFile
+                echo "Neutralized disablePackageSources entry for '$DisabledSourceName'"
+        fi
+    done
+fi

--- a//dev/null
+++ b/eng/common/cross/build-android-rootfs.sh
@@ -0,0 +1,105 @@
+set -e
+__NDK_Version=r21
+usage()
+{
+    echo "Creates a toolchain and sysroot used for cross-compiling for Android."
+    echo.
+    echo "Usage: $0 [BuildArch] [ApiLevel]"
+    echo.
+    echo "BuildArch is the target architecture of Android. Currently only arm64 is supported."
+    echo "ApiLevel is the target Android API level. API levels usually match to Android releases. See https://source.android.com/source/build-numbers.html"
+    echo.
+    echo "By default, the toolchain and sysroot will be generated in cross/android-rootfs/toolchain/[BuildArch]. You can change this behavior"
+    echo "by setting the TOOLCHAIN_DIR environment variable"
+    echo.
+    echo "By default, the NDK will be downloaded into the cross/android-rootfs/android-ndk-$__NDK_Version directory. If you already have an NDK installation,"
+    echo "you can set the NDK_DIR environment variable to have this script use that installation of the NDK."
+    echo "By default, this script will generate a file, android_platform, in the root of the ROOTFS_DIR directory that contains the RID for the supported and tested Android build: android.28-arm64. This file is to replace '/etc/os-release', which is not available for Android."
+    exit 1
+}
+__ApiLevel=28 # The minimum platform for arm64 is API level 21 but the minimum version that support glob(3) is 28. See $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/glob.h
+__BuildArch=arm64
+__AndroidArch=aarch64
+__AndroidToolchain=aarch64-linux-android
+for i in "$@"
+    do
+        lowerI="$(echo $i | tr "[:upper:]" "[:lower:]")"
+        case $lowerI in
+        -?|-h|--help)
+            usage
+            exit 1
+            ;;
+        arm64)
+            __BuildArch=arm64
+            __AndroidArch=aarch64
+            __AndroidToolchain=aarch64-linux-android
+            ;;
+        arm)
+            __BuildArch=arm
+            __AndroidArch=arm
+            __AndroidToolchain=arm-linux-androideabi
+            ;;
+        *[0-9])
+            __ApiLevel=$i
+            ;;
+        *)
+            __UnprocessedBuildArgs="$__UnprocessedBuildArgs $i"
+            ;;
+    esac
+done
+__ScriptBaseDir="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
+__CrossDir="$__ScriptBaseDir/../../../.tools/android-rootfs"
+if [[ ! -f "$__CrossDir" ]]; then
+    mkdir -p "$__CrossDir"
+fi
+__CrossDir="$( cd "$__CrossDir" && pwd )"
+__NDK_Dir="$__CrossDir/android-ndk-$__NDK_Version"
+__lldb_Dir="$__CrossDir/lldb"
+__ToolchainDir="$__CrossDir/android-ndk-$__NDK_Version"
+if [[ -n "$TOOLCHAIN_DIR" ]]; then
+    __ToolchainDir=$TOOLCHAIN_DIR
+fi
+if [[ -n "$NDK_DIR" ]]; then
+    __NDK_Dir=$NDK_DIR
+fi
+echo "Target API level: $__ApiLevel"
+echo "Target architecture: $__BuildArch"
+echo "NDK location: $__NDK_Dir"
+echo "Target Toolchain location: $__ToolchainDir"
+if [ ! -d $__NDK_Dir ]; then
+    echo Downloading the NDK into $__NDK_Dir
+    mkdir -p $__NDK_Dir
+    wget -q --progress=bar:force:noscroll --show-progress https://dl.google.com/android/repository/android-ndk-$__NDK_Version-linux-x86_64.zip -O $__CrossDir/android-ndk-$__NDK_Version-linux-x86_64.zip
+    unzip -q $__CrossDir/android-ndk-$__NDK_Version-linux-x86_64.zip -d $__CrossDir
+fi
+if [ ! -d $__lldb_Dir ]; then
+    mkdir -p $__lldb_Dir
+    echo Downloading LLDB into $__lldb_Dir
+    wget -q --progress=bar:force:noscroll --show-progress https://dl.google.com/android/repository/lldb-2.3.3614996-linux-x86_64.zip -O $__CrossDir/lldb-2.3.3614996-linux-x86_64.zip
+    unzip -q $__CrossDir/lldb-2.3.3614996-linux-x86_64.zip -d $__lldb_Dir
+fi
+echo "Download dependencies..."
+__TmpDir=$__CrossDir/tmp/$__BuildArch/
+mkdir -p "$__TmpDir"
+__AndroidPackages="libicu"
+__AndroidPackages+=" libandroid-glob"
+__AndroidPackages+=" liblzma"
+__AndroidPackages+=" krb5"
+__AndroidPackages+=" openssl"
+for path in $(wget -qO- https://packages.termux.dev/termux-main-21/dists/stable/main/binary-$__AndroidArch/Packages |\
+    grep -A15 "Package: \(${__AndroidPackages// /\\|}\)" | grep -v "static\|tool" | grep Filename); do
+    if [[ "$path" != "Filename:" ]]; then
+        echo "Working on: $path"
+        wget -qO- https://packages.termux.dev/termux-main-21/$path | dpkg -x - "$__TmpDir"
+    fi
+done
+cp -R "$__TmpDir/data/data/com.termux/files/usr/"* "$__ToolchainDir/sysroot/usr/"
+echo "Generating platform file..."
+echo "RID=android.${__ApiLevel}-${__BuildArch}" > $__ToolchainDir/sysroot/android_platform
+echo "Now to build coreclr, libraries and installers; run:"
+echo ROOTFS_DIR=\$\(realpath $__ToolchainDir/sysroot\) ./build.sh --cross --arch $__BuildArch \
+    --subsetCategory coreclr
+echo ROOTFS_DIR=\$\(realpath $__ToolchainDir/sysroot\) ./build.sh --cross --arch $__BuildArch \
+    --subsetCategory libraries
+echo ROOTFS_DIR=\$\(realpath $__ToolchainDir/sysroot\) ./build.sh --cross --arch $__BuildArch \
+    --subsetCategory installer

--- a//dev/null
+++ b/eng/common/cross/build-rootfs.sh
@@ -0,0 +1,561 @@
+set -e
+usage()
+{
+    echo "Usage: $0 [BuildArch] [CodeName] [lldbx.y] [llvmx[.y]] [--skipunmount] --rootfsdir <directory>]"
+    echo "BuildArch can be: arm(default), arm64, armel, armv6, ppc64le, riscv64, s390x, x64, x86"
+    echo "CodeName - optional, Code name for Linux, can be: xenial(default), zesty, bionic, alpine"
+    echo "                               for alpine can be specified with version: alpineX.YY or alpineedge"
+    echo "                               for FreeBSD can be: freebsd12, freebsd13"
+    echo "                               for illumos can be: illumos"
+    echo "                               for Haiku can be: haiku."
+    echo "lldbx.y - optional, LLDB version, can be: lldb3.9(default), lldb4.0, lldb5.0, lldb6.0 no-lldb. Ignored for alpine and FreeBSD"
+    echo "llvmx[.y] - optional, LLVM version for LLVM related packages."
+    echo "--skipunmount - optional, will skip the unmount of rootfs folder."
+    echo "--skipsigcheck - optional, will skip package signature checks (allowing untrusted packages)."
+    echo "--use-mirror - optional, use mirror URL to fetch resources, when available."
+    echo "--jobs N - optional, restrict to N jobs."
+    exit 1
+}
+__CodeName=xenial
+__CrossDir=$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )
+__BuildArch=arm
+__AlpineArch=armv7
+__FreeBSDArch=arm
+__FreeBSDMachineArch=armv7
+__IllumosArch=arm7
+__HaikuArch=arm
+__QEMUArch=arm
+__UbuntuArch=armhf
+__UbuntuRepo="http://ports.ubuntu.com/"
+__LLDB_Package="liblldb-3.9-dev"
+__SkipUnmount=0
+__UbuntuPackages="build-essential"
+__AlpinePackages="alpine-base"
+__AlpinePackages+=" build-base"
+__AlpinePackages+=" linux-headers"
+__AlpinePackages+=" lldb-dev"
+__AlpinePackages+=" python3"
+__AlpinePackages+=" libedit"
+__UbuntuPackages+=" symlinks"
+__UbuntuPackages+=" libicu-dev"
+__UbuntuPackages+=" liblttng-ust-dev"
+__UbuntuPackages+=" libunwind8-dev"
+__UbuntuPackages+=" libnuma-dev"
+__AlpinePackages+=" gettext-dev"
+__AlpinePackages+=" icu-dev"
+__AlpinePackages+=" libunwind-dev"
+__AlpinePackages+=" lttng-ust-dev"
+__AlpinePackages+=" compiler-rt"
+__AlpinePackages+=" numactl-dev"
+__UbuntuPackages+=" libcurl4-openssl-dev"
+__UbuntuPackages+=" libkrb5-dev"
+__UbuntuPackages+=" libssl-dev"
+__UbuntuPackages+=" zlib1g-dev"
+__AlpinePackages+=" curl-dev"
+__AlpinePackages+=" krb5-dev"
+__AlpinePackages+=" openssl-dev"
+__AlpinePackages+=" zlib-dev"
+__FreeBSDBase="12.4-RELEASE"
+__FreeBSDPkg="1.17.0"
+__FreeBSDABI="12"
+__FreeBSDPackages="libunwind"
+__FreeBSDPackages+=" icu"
+__FreeBSDPackages+=" libinotify"
+__FreeBSDPackages+=" openssl"
+__FreeBSDPackages+=" krb5"
+__FreeBSDPackages+=" terminfo-db"
+__IllumosPackages="icu"
+__IllumosPackages+=" mit-krb5"
+__IllumosPackages+=" openssl"
+__IllumosPackages+=" zlib"
+__HaikuPackages="gcc_syslibs"
+__HaikuPackages+=" gcc_syslibs_devel"
+__HaikuPackages+=" gmp"
+__HaikuPackages+=" gmp_devel"
+__HaikuPackages+=" icu66"
+__HaikuPackages+=" icu66_devel"
+__HaikuPackages+=" krb5"
+__HaikuPackages+=" krb5_devel"
+__HaikuPackages+=" libiconv"
+__HaikuPackages+=" libiconv_devel"
+__HaikuPackages+=" llvm12_libunwind"
+__HaikuPackages+=" llvm12_libunwind_devel"
+__HaikuPackages+=" mpfr"
+__HaikuPackages+=" mpfr_devel"
+__HaikuPackages+=" openssl"
+__HaikuPackages+=" openssl_devel"
+__HaikuPackages+=" zlib"
+__HaikuPackages+=" zlib_devel"
+__UbuntuPackages+=" libomp5"
+__UbuntuPackages+=" libomp-dev"
+__AlpineKeys='
+4a6a0840:MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA1yHJxQgsHQREclQu4Ohe\nqxTxd1tHcNnvnQTu/UrTky8wWvgXT+jpveroeWWnzmsYlDI93eLI2ORakxb3gA2O\nQ0Ry4ws8vhaxLQGC74uQR5+/yYrLuTKydFzuPaS1dK19qJPXB8GMdmFOijnXX4SA\njixuHLe1WW7kZVtjL7nufvpXkWBGjsfrvskdNA/5MfxAeBbqPgaq0QMEfxMAn6/R\nL5kNepi/Vr4S39Xvf2DzWkTLEK8pcnjNkt9/aafhWqFVW7m3HCAII6h/qlQNQKSo\nGuH34Q8GsFG30izUENV9avY7hSLq7nggsvknlNBZtFUcmGoQrtx3FmyYsIC8/R+B\nywIDAQAB
+5243ef4b:MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvNijDxJ8kloskKQpJdx+\nmTMVFFUGDoDCbulnhZMJoKNkSuZOzBoFC94omYPtxnIcBdWBGnrm6ncbKRlR+6oy\nDO0W7c44uHKCFGFqBhDasdI4RCYP+fcIX/lyMh6MLbOxqS22TwSLhCVjTyJeeH7K\naA7vqk+QSsF4TGbYzQDDpg7+6aAcNzg6InNePaywA6hbT0JXbxnDWsB+2/LLSF2G\nmnhJlJrWB1WGjkz23ONIWk85W4S0XB/ewDefd4Ly/zyIciastA7Zqnh7p3Ody6Q0\nsS2MJzo7p3os1smGjUF158s6m/JbVh4DN6YIsxwl2OjDOz9R0OycfJSDaBVIGZzg\ncQIDAQAB
+524d27bb:MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAr8s1q88XpuJWLCZALdKj\nlN8wg2ePB2T9aIcaxryYE/Jkmtu+ZQ5zKq6BT3y/udt5jAsMrhHTwroOjIsF9DeG\ne8Y3vjz+Hh4L8a7hZDaw8jy3CPag47L7nsZFwQOIo2Cl1SnzUc6/owoyjRU7ab0p\niWG5HK8IfiybRbZxnEbNAfT4R53hyI6z5FhyXGS2Ld8zCoU/R4E1P0CUuXKEN4p0\n64dyeUoOLXEWHjgKiU1mElIQj3k/IF02W89gDj285YgwqA49deLUM7QOd53QLnx+\nxrIrPv3A+eyXMFgexNwCKQU9ZdmWa00MjjHlegSGK8Y2NPnRoXhzqSP9T9i2HiXL\nVQIDAQAB
+5261cecb:MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwlzMkl7b5PBdfMzGdCT0\ncGloRr5xGgVmsdq5EtJvFkFAiN8Ac9MCFy/vAFmS8/7ZaGOXoCDWbYVLTLOO2qtX\nyHRl+7fJVh2N6qrDDFPmdgCi8NaE+3rITWXGrrQ1spJ0B6HIzTDNEjRKnD4xyg4j\ng01FMcJTU6E+V2JBY45CKN9dWr1JDM/nei/Pf0byBJlMp/mSSfjodykmz4Oe13xB\nCa1WTwgFykKYthoLGYrmo+LKIGpMoeEbY1kuUe04UiDe47l6Oggwnl+8XD1MeRWY\nsWgj8sF4dTcSfCMavK4zHRFFQbGp/YFJ/Ww6U9lA3Vq0wyEI6MCMQnoSMFwrbgZw\nwwIDAQAB
+58199dcc:MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA3v8/ye/V/t5xf4JiXLXa\nhWFRozsnmn3hobON20GdmkrzKzO/eUqPOKTpg2GtvBhK30fu5oY5uN2ORiv2Y2ht\neLiZ9HVz3XP8Fm9frha60B7KNu66FO5P2o3i+E+DWTPqqPcCG6t4Znk2BypILcit\nwiPKTsgbBQR2qo/cO01eLLdt6oOzAaF94NH0656kvRewdo6HG4urbO46tCAizvCR\nCA7KGFMyad8WdKkTjxh8YLDLoOCtoZmXmQAiwfRe9pKXRH/XXGop8SYptLqyVVQ+\ntegOD9wRs2tOlgcLx4F/uMzHN7uoho6okBPiifRX+Pf38Vx+ozXh056tjmdZkCaV\naQIDAQAB
+58cbb476:MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAoSPnuAGKtRIS5fEgYPXD\n8pSGvKAmIv3A08LBViDUe+YwhilSHbYXUEAcSH1KZvOo1WT1x2FNEPBEFEFU1Eyc\n+qGzbA03UFgBNvArurHQ5Z/GngGqE7IarSQFSoqewYRtFSfp+TL9CUNBvM0rT7vz\n2eMu3/wWG+CBmb92lkmyWwC1WSWFKO3x8w+Br2IFWvAZqHRt8oiG5QtYvcZL6jym\nY8T6sgdDlj+Y+wWaLHs9Fc+7vBuyK9C4O1ORdMPW15qVSl4Lc2Wu1QVwRiKnmA+c\nDsH/m7kDNRHM7TjWnuj+nrBOKAHzYquiu5iB3Qmx+0gwnrSVf27Arc3ozUmmJbLj\nzQIDAQAB
+58e4f17d:MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvBxJN9ErBgdRcPr5g4hV\nqyUSGZEKuvQliq2Z9SRHLh2J43+EdB6A+yzVvLnzcHVpBJ+BZ9RV30EM9guck9sh\nr+bryZcRHyjG2wiIEoduxF2a8KeWeQH7QlpwGhuobo1+gA8L0AGImiA6UP3LOirl\nI0G2+iaKZowME8/tydww4jx5vG132JCOScMjTalRsYZYJcjFbebQQolpqRaGB4iG\nWqhytWQGWuKiB1A22wjmIYf3t96l1Mp+FmM2URPxD1gk/BIBnX7ew+2gWppXOK9j\n1BJpo0/HaX5XoZ/uMqISAAtgHZAqq+g3IUPouxTphgYQRTRYpz2COw3NF43VYQrR\nbQIDAQAB
+60ac2099:MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwR4uJVtJOnOFGchnMW5Y\nj5/waBdG1u5BTMlH+iQMcV5+VgWhmpZHJCBz3ocD+0IGk2I68S5TDOHec/GSC0lv\n6R9o6F7h429GmgPgVKQsc8mPTPtbjJMuLLs4xKc+viCplXc0Nc0ZoHmCH4da6fCV\ntdpHQjVe6F9zjdquZ4RjV6R6JTiN9v924dGMAkbW/xXmamtz51FzondKC52Gh8Mo\n/oA0/T0KsCMCi7tb4QNQUYrf+Xcha9uus4ww1kWNZyfXJB87a2kORLiWMfs2IBBJ\nTmZ2Fnk0JnHDb8Oknxd9PvJPT0mvyT8DA+KIAPqNvOjUXP4bnjEHJcoCP9S5HkGC\nIQIDAQAB
+6165ee59:MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAutQkua2CAig4VFSJ7v54\nALyu/J1WB3oni7qwCZD3veURw7HxpNAj9hR+S5N/pNeZgubQvJWyaPuQDm7PTs1+\ntFGiYNfAsiibX6Rv0wci3M+z2XEVAeR9Vzg6v4qoofDyoTbovn2LztaNEjTkB+oK\ntlvpNhg1zhou0jDVYFniEXvzjckxswHVb8cT0OMTKHALyLPrPOJzVtM9C1ew2Nnc\n3848xLiApMu3NBk0JqfcS3Bo5Y2b1FRVBvdt+2gFoKZix1MnZdAEZ8xQzL/a0YS5\nHd0wj5+EEKHfOd3A75uPa/WQmA+o0cBFfrzm69QDcSJSwGpzWrD1ScH3AK8nWvoj\nv7e9gukK/9yl1b4fQQ00vttwJPSgm9EnfPHLAtgXkRloI27H6/PuLoNvSAMQwuCD\nhQRlyGLPBETKkHeodfLoULjhDi1K2gKJTMhtbnUcAA7nEphkMhPWkBpgFdrH+5z4\nLxy+3ek0cqcI7K68EtrffU8jtUj9LFTUC8dERaIBs7NgQ/LfDbDfGh9g6qVj1hZl\nk9aaIPTm/xsi8v3u+0qaq7KzIBc9s59JOoA8TlpOaYdVgSQhHHLBaahOuAigH+VI\nisbC9vmqsThF2QdDtQt37keuqoda2E6sL7PUvIyVXDRfwX7uMDjlzTxHTymvq2Ck\nhtBqojBnThmjJQFgZXocHG8CAwEAAQ==
+61666e3f:MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAlEyxkHggKCXC2Wf5Mzx4\nnZLFZvU2bgcA3exfNPO/g1YunKfQY+Jg4fr6tJUUTZ3XZUrhmLNWvpvSwDS19ZmC\nIXOu0+V94aNgnhMsk9rr59I8qcbsQGIBoHzuAl8NzZCgdbEXkiY90w1skUw8J57z\nqCsMBydAueMXuWqF5nGtYbi5vHwK42PffpiZ7G5Kjwn8nYMW5IZdL6ZnMEVJUWC9\nI4waeKg0yskczYDmZUEAtrn3laX9677ToCpiKrvmZYjlGl0BaGp3cxggP2xaDbUq\nqfFxWNgvUAb3pXD09JM6Mt6HSIJaFc9vQbrKB9KT515y763j5CC2KUsilszKi3mB\nHYe5PoebdjS7D1Oh+tRqfegU2IImzSwW3iwA7PJvefFuc/kNIijfS/gH/cAqAK6z\nbhdOtE/zc7TtqW2Wn5Y03jIZdtm12CxSxwgtCF1NPyEWyIxAQUX9ACb3M0FAZ61n\nfpPrvwTaIIxxZ01L3IzPLpbc44x/DhJIEU+iDt6IMTrHOphD9MCG4631eIdB0H1b\n6zbNX1CXTsafqHRFV9XmYYIeOMggmd90s3xIbEujA6HKNP/gwzO6CDJ+nHFDEqoF\nSkxRdTkEqjTjVKieURW7Swv7zpfu5PrsrrkyGnsRrBJJzXlm2FOOxnbI2iSL1B5F\nrO5kbUxFeZUIDq+7Yv4kLWcCAwEAAQ==
+616a9724:MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAnC+bR4bHf/L6QdU4puhQ\ngl1MHePszRC38bzvVFDUJsmCaMCL2suCs2A2yxAgGb9pu9AJYLAmxQC4mM3jNqhg\n/E7yuaBbek3O02zN/ctvflJ250wZCy+z0ZGIp1ak6pu1j14IwHokl9j36zNfGtfv\nADVOcdpWITFFlPqwq1qt/H3UsKVmtiF3BNWWTeUEQwKvlU8ymxgS99yn0+4OPyNT\nL3EUeS+NQJtDS01unau0t7LnjUXn+XIneWny8bIYOQCuVR6s/gpIGuhBaUqwaJOw\n7jkJZYF2Ij7uPb4b5/R3vX2FfxxqEHqssFSg8FFUNTZz3qNZs0CRVyfA972g9WkJ\nhPfn31pQYil4QGRibCMIeU27YAEjXoqfJKEPh4UWMQsQLrEfdGfb8VgwrPbniGfU\nL3jKJR3VAafL9330iawzVQDlIlwGl6u77gEXMl9K0pfazunYhAp+BMP+9ot5ckK+\nosmrqj11qMESsAj083GeFdfV3pXEIwUytaB0AKEht9DbqUfiE/oeZ/LAXgySMtVC\nsbC4ESmgVeY2xSBIJdDyUap7FR49GGrw0W49NUv9gRgQtGGaNVQQO9oGL2PBC41P\niWF9GLoX30HIz1P8PF/cZvicSSPkQf2Z6TV+t0ebdGNS5DjapdnCrq8m9Z0pyKsQ\nuxAL2a7zX8l5i1CZh1ycUGsCAwEAAQ==
+616abc23:MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA0MfCDrhODRCIxR9Dep1s\neXafh5CE5BrF4WbCgCsevyPIdvTeyIaW4vmO3bbG4VzhogDZju+R3IQYFuhoXP5v\nY+zYJGnwrgz3r5wYAvPnLEs1+dtDKYOgJXQj+wLJBW1mzRDL8FoRXOe5iRmn1EFS\nwZ1DoUvyu7/J5r0itKicZp3QKED6YoilXed+1vnS4Sk0mzN4smuMR9eO1mMCqNp9\n9KTfRDHTbakIHwasECCXCp50uXdoW6ig/xUAFanpm9LtK6jctNDbXDhQmgvAaLXZ\nLvFqoaYJ/CvWkyYCgL6qxvMvVmPoRv7OPcyni4xR/WgWa0MSaEWjgPx3+yj9fiMA\n1S02pFWFDOr5OUF/O4YhFJvUCOtVsUPPfA/Lj6faL0h5QI9mQhy5Zb9TTaS9jB6p\nLw7u0dJlrjFedk8KTJdFCcaGYHP6kNPnOxMylcB/5WcztXZVQD5WpCicGNBxCGMm\nW64SgrV7M07gQfL/32QLsdqPUf0i8hoVD8wfQ3EpbQzv6Fk1Cn90bZqZafg8XWGY\nwddhkXk7egrr23Djv37V2okjzdqoyLBYBxMz63qQzFoAVv5VoY2NDTbXYUYytOvG\nGJ1afYDRVWrExCech1mX5ZVUB1br6WM+psFLJFoBFl6mDmiYt0vMYBddKISsvwLl\nIJQkzDwtXzT2cSjoj3T5QekCAwEAAQ==
+616ac3bc:MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAvaaoSLab+IluixwKV5Od\n0gib2YurjPatGIbn5Ov2DLUFYiebj2oJINXJSwUOO+4WcuHFEqiL/1rya+k5hLZt\nhnPL1tn6QD4rESznvGSasRCQNT2vS/oyZbTYJRyAtFkEYLlq0t3S3xBxxHWuvIf0\nqVxVNYpQWyM3N9RIeYBR/euXKJXileSHk/uq1I5wTC0XBIHWcthczGN0m9wBEiWS\n0m3cnPk4q0Ea8mUJ91Rqob19qETz6VbSPYYpZk3qOycjKosuwcuzoMpwU8KRiMFd\n5LHtX0Hx85ghGsWDVtS0c0+aJa4lOMGvJCAOvDfqvODv7gKlCXUpgumGpLdTmaZ8\n1RwqspAe3IqBcdKTqRD4m2mSg23nVx2FAY3cjFvZQtfooT7q1ItRV5RgH6FhQSl7\n+6YIMJ1Bf8AAlLdRLpg+doOUGcEn+pkDiHFgI8ylH1LKyFKw+eXaAml/7DaWZk1d\ndqggwhXOhc/UUZFQuQQ8A8zpA13PcbC05XxN2hyP93tCEtyynMLVPtrRwDnHxFKa\nqKzs3rMDXPSXRn3ZZTdKH3069ApkEjQdpcwUh+EmJ1Ve/5cdtzT6kKWCjKBFZP/s\n91MlRrX2BTRdHaU5QJkUheUtakwxuHrdah2F94lRmsnQlpPr2YseJu6sIE+Dnx4M\nCfhdVbQL2w54R645nlnohu8CAwEAAQ==
+616adfeb:MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAq0BFD1D4lIxQcsqEpQzU\npNCYM3aP1V/fxxVdT4DWvSI53JHTwHQamKdMWtEXetWVbP5zSROniYKFXd/xrD9X\n0jiGHey3lEtylXRIPxe5s+wXoCmNLcJVnvTcDtwx/ne2NLHxp76lyc25At+6RgE6\nADjLVuoD7M4IFDkAsd8UQ8zM0Dww9SylIk/wgV3ZkifecvgUQRagrNUdUjR56EBZ\nraQrev4hhzOgwelT0kXCu3snbUuNY/lU53CoTzfBJ5UfEJ5pMw1ij6X0r5S9IVsy\nKLWH1hiO0NzU2c8ViUYCly4Fe9xMTFc6u2dy/dxf6FwERfGzETQxqZvSfrRX+GLj\n/QZAXiPg5178hT/m0Y3z5IGenIC/80Z9NCi+byF1WuJlzKjDcF/TU72zk0+PNM/H\nKuppf3JT4DyjiVzNC5YoWJT2QRMS9KLP5iKCSThwVceEEg5HfhQBRT9M6KIcFLSs\nmFjx9kNEEmc1E8hl5IR3+3Ry8G5/bTIIruz14jgeY9u5jhL8Vyyvo41jgt9sLHR1\n/J1TxKfkgksYev7PoX6/ZzJ1ksWKZY5NFoDXTNYUgzFUTOoEaOg3BAQKadb3Qbbq\nXIrxmPBdgrn9QI7NCgfnAY3Tb4EEjs3ON/BNyEhUENcXOH6I1NbcuBQ7g9P73kE4\nVORdoc8MdJ5eoKBpO8Ww8HECAwEAAQ==
+616ae350:MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAyduVzi1mWm+lYo2Tqt/0\nXkCIWrDNP1QBMVPrE0/ZlU2bCGSoo2Z9FHQKz/mTyMRlhNqTfhJ5qU3U9XlyGOPJ\npiM+b91g26pnpXJ2Q2kOypSgOMOPA4cQ42PkHBEqhuzssfj9t7x47ppS94bboh46\nxLSDRff/NAbtwTpvhStV3URYkxFG++cKGGa5MPXBrxIp+iZf9GnuxVdST5PGiVGP\nODL/b69sPJQNbJHVquqUTOh5Ry8uuD2WZuXfKf7/C0jC/ie9m2+0CttNu9tMciGM\nEyKG1/Xhk5iIWO43m4SrrT2WkFlcZ1z2JSf9Pjm4C2+HovYpihwwdM/OdP8Xmsnr\nDzVB4YvQiW+IHBjStHVuyiZWc+JsgEPJzisNY0Wyc/kNyNtqVKpX6dRhMLanLmy+\nf53cCSI05KPQAcGj6tdL+D60uKDkt+FsDa0BTAobZ31OsFVid0vCXtsbplNhW1IF\nHwsGXBTVcfXg44RLyL8Lk/2dQxDHNHzAUslJXzPxaHBLmt++2COa2EI1iWlvtznk\nOk9WP8SOAIj+xdqoiHcC4j72BOVVgiITIJNHrbppZCq6qPR+fgXmXa+sDcGh30m6\n9Wpbr28kLMSHiENCWTdsFij+NQTd5S47H7XTROHnalYDuF1RpS+DpQidT5tUimaT\nJZDr++FjKrnnijbyNF8b98UCAwEAAQ==
+616db30d:MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAnpUpyWDWjlUk3smlWeA0\nlIMW+oJ38t92CRLHH3IqRhyECBRW0d0aRGtq7TY8PmxjjvBZrxTNDpJT6KUk4LRm\na6A6IuAI7QnNK8SJqM0DLzlpygd7GJf8ZL9SoHSH+gFsYF67Cpooz/YDqWrlN7Vw\ntO00s0B+eXy+PCXYU7VSfuWFGK8TGEv6HfGMALLjhqMManyvfp8hz3ubN1rK3c8C\nUS/ilRh1qckdbtPvoDPhSbTDmfU1g/EfRSIEXBrIMLg9ka/XB9PvWRrekrppnQzP\nhP9YE3x/wbFc5QqQWiRCYyQl/rgIMOXvIxhkfe8H5n1Et4VAorkpEAXdsfN8KSVv\nLSMazVlLp9GYq5SUpqYX3KnxdWBgN7BJoZ4sltsTpHQ/34SXWfu3UmyUveWj7wp0\nx9hwsPirVI00EEea9AbP7NM2rAyu6ukcm4m6ATd2DZJIViq2es6m60AE6SMCmrQF\nwmk4H/kdQgeAELVfGOm2VyJ3z69fQuywz7xu27S6zTKi05Qlnohxol4wVb6OB7qG\nLPRtK9ObgzRo/OPumyXqlzAi/Yvyd1ZQk8labZps3e16bQp8+pVPiumWioMFJDWV\nGZjCmyMSU8V6MB6njbgLHoyg2LCukCAeSjbPGGGYhnKLm1AKSoJh3IpZuqcKCk5C\n8CM1S15HxV78s9dFntEqIokCAwEAAQ==
+'
+__Keyring=
+__SkipSigCheck=0
+__UseMirror=0
+__UnprocessedBuildArgs=
+while :; do
+    if [[ "$#" -le 0 ]]; then
+        break
+    fi
+    lowerI="$(echo "$1" | tr "[:upper:]" "[:lower:]")"
+    case $lowerI in
+        -\?|-h|--help)
+            usage
+            exit 1
+            ;;
+        arm)
+            __BuildArch=arm
+            __UbuntuArch=armhf
+            __AlpineArch=armv7
+            __QEMUArch=arm
+            ;;
+        arm64)
+            __BuildArch=arm64
+            __UbuntuArch=arm64
+            __AlpineArch=aarch64
+            __QEMUArch=aarch64
+            __FreeBSDArch=arm64
+            __FreeBSDMachineArch=aarch64
+            ;;
+        armel)
+            __BuildArch=armel
+            __UbuntuArch=armel
+            __UbuntuRepo="http://ftp.debian.org/debian/"
+            __CodeName=jessie
+            ;;
+        armv6)
+            __BuildArch=armv6
+            __UbuntuArch=armhf
+            __QEMUArch=arm
+            __UbuntuRepo="http://raspbian.raspberrypi.org/raspbian/"
+            __CodeName=buster
+            __LLDB_Package="liblldb-6.0-dev"
+            if [[ -e "/usr/share/keyrings/raspbian-archive-keyring.gpg" ]]; then
+                __Keyring="--keyring /usr/share/keyrings/raspbian-archive-keyring.gpg"
+            fi
+            ;;
+        riscv64)
+            __BuildArch=riscv64
+            __AlpineArch=riscv64
+            __AlpinePackages="${__AlpinePackages// lldb-dev/}"
+            __QEMUArch=riscv64
+            __UbuntuArch=riscv64
+            __UbuntuRepo="http://deb.debian.org/debian-ports"
+            __UbuntuPackages="${__UbuntuPackages// libunwind8-dev/}"
+            unset __LLDB_Package
+            if [[ -e "/usr/share/keyrings/debian-ports-archive-keyring.gpg" ]]; then
+                __Keyring="--keyring /usr/share/keyrings/debian-ports-archive-keyring.gpg --include=debian-ports-archive-keyring"
+            fi
+            ;;
+        ppc64le)
+            __BuildArch=ppc64le
+            __AlpineArch=ppc64le
+            __QEMUArch=ppc64le
+            __UbuntuArch=ppc64el
+            __UbuntuRepo="http://ports.ubuntu.com/ubuntu-ports/"
+            __UbuntuPackages="${__UbuntuPackages// libunwind8-dev/}"
+            __UbuntuPackages="${__UbuntuPackages// libomp-dev/}"
+            __UbuntuPackages="${__UbuntuPackages// libomp5/}"
+            unset __LLDB_Package
+            ;;
+        s390x)
+            __BuildArch=s390x
+            __AlpineArch=s390x
+            __QEMUArch=s390x
+            __UbuntuArch=s390x
+            __UbuntuRepo="http://ports.ubuntu.com/ubuntu-ports/"
+            __UbuntuPackages="${__UbuntuPackages// libunwind8-dev/}"
+            __UbuntuPackages="${__UbuntuPackages// libomp-dev/}"
+            __UbuntuPackages="${__UbuntuPackages// libomp5/}"
+            unset __LLDB_Package
+            ;;
+        x64)
+            __BuildArch=x64
+            __AlpineArch=x86_64
+            __UbuntuArch=amd64
+            __FreeBSDArch=amd64
+            __FreeBSDMachineArch=amd64
+            __illumosArch=x86_64
+            __HaikuArch=x86_64
+            __UbuntuRepo="http://archive.ubuntu.com/ubuntu/"
+            ;;
+        x86)
+            __BuildArch=x86
+            __UbuntuArch=i386
+            __AlpineArch=x86
+            __UbuntuRepo="http://archive.ubuntu.com/ubuntu/"
+            ;;
+        lldb*)
+            version="${lowerI/lldb/}"
+            parts=(${version//./ })
+            if [[ "${parts[0]}" -gt 6 ]]; then
+                version="${parts[0]}"
+            fi
+            __LLDB_Package="liblldb-${version}-dev"
+            ;;
+        no-lldb)
+            unset __LLDB_Package
+            ;;
+        llvm*)
+            version="${lowerI/llvm/}"
+            parts=(${version//./ })
+            __LLVM_MajorVersion="${parts[0]}"
+            __LLVM_MinorVersion="${parts[1]}"
+            if [[ -z "$__LLVM_MinorVersion" && "$__LLVM_MajorVersion" -le 6 ]]; then
+                __LLVM_MinorVersion=0;
+            fi
+            ;;
+        xenial) # Ubuntu 16.04
+            if [[ "$__CodeName" != "jessie" ]]; then
+                __CodeName=xenial
+            fi
+            ;;
+        zesty) # Ubuntu 17.04
+            if [[ "$__CodeName" != "jessie" ]]; then
+                __CodeName=zesty
+            fi
+            ;;
+        bionic) # Ubuntu 18.04
+            if [[ "$__CodeName" != "jessie" ]]; then
+                __CodeName=bionic
+            fi
+            ;;
+        focal) # Ubuntu 20.04
+            if [[ "$__CodeName" != "jessie" ]]; then
+                __CodeName=focal
+            fi
+            ;;
+        jammy) # Ubuntu 22.04
+            if [[ "$__CodeName" != "jessie" ]]; then
+                __CodeName=jammy
+            fi
+            ;;
+        jessie) # Debian 8
+            __CodeName=jessie
+            if [[ -z "$__UbuntuRepo" ]]; then
+                __UbuntuRepo="http://ftp.debian.org/debian/"
+            fi
+            ;;
+        stretch) # Debian 9
+            __CodeName=stretch
+            __LLDB_Package="liblldb-6.0-dev"
+            if [[ -z "$__UbuntuRepo" ]]; then
+                __UbuntuRepo="http://ftp.debian.org/debian/"
+            fi
+            ;;
+        buster) # Debian 10
+            __CodeName=buster
+            __LLDB_Package="liblldb-6.0-dev"
+            if [[ -z "$__UbuntuRepo" ]]; then
+                __UbuntuRepo="http://ftp.debian.org/debian/"
+            fi
+            ;;
+        bullseye) # Debian 11
+            __CodeName=bullseye
+            if [[ -z "$__UbuntuRepo" ]]; then
+                __UbuntuRepo="http://ftp.debian.org/debian/"
+            fi
+            ;;
+        sid) # Debian sid
+            __CodeName=sid
+            if [[ -z "$__UbuntuRepo" ]]; then
+                __UbuntuRepo="http://ftp.debian.org/debian/"
+            fi
+            ;;
+        tizen)
+            __CodeName=
+            __UbuntuRepo=
+            __Tizen=tizen
+            ;;
+        alpine*)
+            __CodeName=alpine
+            __UbuntuRepo=
+            version="${lowerI/alpine/}"
+            if [[ "$version" == "edge" ]]; then
+                __AlpineVersion=edge
+            else
+                parts=(${version//./ })
+                __AlpineMajorVersion="${parts[0]}"
+                __AlpineMinoVersion="${parts[1]}"
+                __AlpineVersion="$__AlpineMajorVersion.$__AlpineMinoVersion"
+            fi
+            ;;
+        freebsd12)
+            __CodeName=freebsd
+            __SkipUnmount=1
+            ;;
+        freebsd13)
+            __CodeName=freebsd
+            __FreeBSDBase="13.2-RELEASE"
+            __FreeBSDABI="13"
+            __SkipUnmount=1
+            ;;
+        illumos)
+            __CodeName=illumos
+            __SkipUnmount=1
+            ;;
+        haiku)
+            __CodeName=haiku
+            __SkipUnmount=1
+            ;;
+        --skipunmount)
+            __SkipUnmount=1
+            ;;
+        --skipsigcheck)
+            __SkipSigCheck=1
+            ;;
+        --rootfsdir|-rootfsdir)
+            shift
+            __RootfsDir="$1"
+            ;;
+        --use-mirror)
+            __UseMirror=1
+            ;;
+        --use-jobs)
+            shift
+            MAXJOBS=$1
+            ;;
+        *)
+            __UnprocessedBuildArgs="$__UnprocessedBuildArgs $1"
+            ;;
+    esac
+    shift
+done
+case "$__AlpineVersion" in
+    3.14) __AlpinePackages+=" llvm11-libs" ;;
+    3.15) __AlpinePackages+=" llvm12-libs" ;;
+    3.16) __AlpinePackages+=" llvm13-libs" ;;
+    3.17) __AlpinePackages+=" llvm15-libs" ;;
+    edge) __AlpineLlvmLibsLookup=1 ;;
+    *)
+        if [[ "$__AlpineArch" =~ s390x|ppc64le ]]; then
+            __AlpineVersion=3.15 # minimum version that supports lldb-dev
+            __AlpinePackages+=" llvm12-libs"
+        elif [[ "$__AlpineArch" == "x86" ]]; then
+            __AlpineVersion=3.17 # minimum version that supports lldb-dev
+            __AlpinePackages+=" llvm15-libs"
+        elif [[ "$__AlpineArch" == "riscv64" ]]; then
+            __AlpineLlvmLibsLookup=1
+            __AlpineVersion=edge # minimum version with APKINDEX.tar.gz (packages archive)
+        else
+            __AlpineVersion=3.13 # 3.13 to maximize compatibility
+            __AlpinePackages+=" llvm10-libs"
+            if [[ "$__AlpineArch" == "armv7" ]]; then
+                __AlpinePackages="${__AlpinePackages//numactl-dev/}"
+            fi
+        fi
+esac
+if [[ "$__AlpineVersion" =~ 3\.1[345] ]]; then
+    __AlpinePackages="${__AlpinePackages/compiler-rt/compiler-rt-static}"
+fi
+if [[ "$__BuildArch" == "armel" ]]; then
+    __LLDB_Package="lldb-3.5-dev"
+fi
+if [[ "$__CodeName" == "xenial" && "$__UbuntuArch" == "armhf" ]]; then
+    __UbuntuPackages="${__UbuntuPackages//libnuma-dev/}"
+fi
+__UbuntuPackages+=" ${__LLDB_Package:-}"
+if [[ -n "$__LLVM_MajorVersion" ]]; then
+    __UbuntuPackages+=" libclang-common-${__LLVM_MajorVersion}${__LLVM_MinorVersion:+.$__LLVM_MinorVersion}-dev"
+fi
+if [[ -z "$__RootfsDir" && -n "$ROOTFS_DIR" ]]; then
+    __RootfsDir="$ROOTFS_DIR"
+fi
+if [[ -z "$__RootfsDir" ]]; then
+    __RootfsDir="$__CrossDir/../../../.tools/rootfs/$__BuildArch"
+fi
+if [[ -d "$__RootfsDir" ]]; then
+    if [[ "$__SkipUnmount" == "0" ]]; then
+        umount "$__RootfsDir"/* || true
+    fi
+    rm -rf "$__RootfsDir"
+fi
+mkdir -p "$__RootfsDir"
+__RootfsDir="$( cd "$__RootfsDir" && pwd )"
+if [[ "$__CodeName" == "alpine" ]]; then
+    __ApkToolsVersion=2.12.11
+    __ApkToolsSHA512SUM=53e57b49230da07ef44ee0765b9592580308c407a8d4da7125550957bb72cb59638e04f8892a18b584451c8d841d1c7cb0f0ab680cc323a3015776affaa3be33
+    __ApkToolsDir="$(mktemp -d)"
+    __ApkKeysDir="$(mktemp -d)"
+    wget "https://gitlab.alpinelinux.org/api/v4/projects/5/packages/generic//v$__ApkToolsVersion/x86_64/apk.static" -P "$__ApkToolsDir"
+    echo "$__ApkToolsSHA512SUM $__ApkToolsDir/apk.static" | sha512sum -c
+    chmod +x "$__ApkToolsDir/apk.static"
+    if [[ -f "/usr/bin/qemu-$__QEMUArch-static" ]]; then
+        mkdir -p "$__RootfsDir"/usr/bin
+        cp -v "/usr/bin/qemu-$__QEMUArch-static" "$__RootfsDir/usr/bin"
+    fi
+    if [[ "$__AlpineVersion" == "edge" ]]; then
+        version=edge
+    else
+        version="v$__AlpineVersion"
+    fi
+    for line in $__AlpineKeys; do
+        id="${line%%:*}"
+        content="${line#*:}"
+        echo -e "-----BEGIN PUBLIC KEY-----\n$content\n-----END PUBLIC KEY-----" > "$__ApkKeysDir/alpine-devel@lists.alpinelinux.org-$id.rsa.pub"
+    done
+    if [[ "$__SkipSigCheck" == "1" ]]; then
+        __ApkSignatureArg="--allow-untrusted"
+    else
+        __ApkSignatureArg="--keys-dir $__ApkKeysDir"
+    fi
+    "$__ApkToolsDir/apk.static" \
+        -X "http://dl-cdn.alpinelinux.org/alpine/$version/main" \
+        -X "http://dl-cdn.alpinelinux.org/alpine/$version/community" \
+        -U $__ApkSignatureArg --root "$__RootfsDir" --arch "$__AlpineArch" --initdb add
+    if [[ "$__AlpineLlvmLibsLookup" == 1 ]]; then
+        __AlpinePackages+=" $("$__ApkToolsDir/apk.static" \
+            -X "http://dl-cdn.alpinelinux.org/alpine/$version/main" \
+            -X "http://dl-cdn.alpinelinux.org/alpine/$version/community" \
+            -U $__ApkSignatureArg --root "$__RootfsDir" --arch "$__AlpineArch" \
+            search 'llvm*-libs' | sort | tail -1 | sed 's/-[^-]*//2g')"
+    fi
+    "$__ApkToolsDir/apk.static" \
+        -X "http://dl-cdn.alpinelinux.org/alpine/$version/main" \
+        -X "http://dl-cdn.alpinelinux.org/alpine/$version/community" \
+        -U $__ApkSignatureArg --root "$__RootfsDir" --arch "$__AlpineArch" \
+        add $__AlpinePackages
+    rm -r "$__ApkToolsDir"
+elif [[ "$__CodeName" == "freebsd" ]]; then
+    mkdir -p "$__RootfsDir"/usr/local/etc
+    JOBS=${MAXJOBS:="$(getconf _NPROCESSORS_ONLN)"}
+    wget -O - "https://download.freebsd.org/ftp/releases/${__FreeBSDArch}/${__FreeBSDMachineArch}/${__FreeBSDBase}/base.txz" | tar -C "$__RootfsDir" -Jxf - ./lib ./usr/lib ./usr/libdata ./usr/include ./usr/share/keys ./etc ./bin/freebsd-version
+    echo "ABI = \"FreeBSD:${__FreeBSDABI}:${__FreeBSDMachineArch}\"; FINGERPRINTS = \"${__RootfsDir}/usr/share/keys\"; REPOS_DIR = [\"${__RootfsDir}/etc/pkg\"]; REPO_AUTOUPDATE = NO; RUN_SCRIPTS = NO;" > "${__RootfsDir}"/usr/local/etc/pkg.conf
+    echo "FreeBSD: { url: \"pkg+http://pkg.FreeBSD.org/\${ABI}/quarterly\", mirror_type: \"srv\", signature_type: \"fingerprints\", fingerprints: \"${__RootfsDir}/usr/share/keys/pkg\", enabled: yes }" > "${__RootfsDir}"/etc/pkg/FreeBSD.conf
+    mkdir -p "$__RootfsDir"/tmp
+    wget -O - "https://github.com/freebsd/pkg/archive/${__FreeBSDPkg}.tar.gz" | tar -C "$__RootfsDir"/tmp -zxf -
+    cd "$__RootfsDir/tmp/pkg-${__FreeBSDPkg}"
+    mkdir -p "$__RootfsDir"/host/etc
+    ./autogen.sh && ./configure --prefix="$__RootfsDir"/host && make -j "$JOBS" && make install
+    rm -rf "$__RootfsDir/tmp/pkg-${__FreeBSDPkg}"
+    INSTALL_AS_USER=$(whoami) "$__RootfsDir"/host/sbin/pkg -r "$__RootfsDir" -C "$__RootfsDir"/usr/local/etc/pkg.conf update
+    INSTALL_AS_USER=$(whoami) "$__RootfsDir"/host/sbin/pkg -r "$__RootfsDir" -C "$__RootfsDir"/usr/local/etc/pkg.conf install --yes $__FreeBSDPackages
+elif [[ "$__CodeName" == "illumos" ]]; then
+    mkdir "$__RootfsDir/tmp"
+    pushd "$__RootfsDir/tmp"
+    JOBS=${MAXJOBS:="$(getconf _NPROCESSORS_ONLN)"}
+    echo "Downloading sysroot."
+    wget -O - https://github.com/illumos/sysroot/releases/download/20181213-de6af22ae73b-v1/illumos-sysroot-i386-20181213-de6af22ae73b-v1.tar.gz | tar -C "$__RootfsDir" -xzf -
+    echo "Building binutils. Please wait.."
+    wget -O - https://ftp.gnu.org/gnu/binutils/binutils-2.33.1.tar.bz2 | tar -xjf -
+    mkdir build-binutils && cd build-binutils
+    ../binutils-2.33.1/configure --prefix="$__RootfsDir" --target="${__illumosArch}-sun-solaris2.10" --program-prefix="${__illumosArch}-illumos-" --with-sysroot="$__RootfsDir"
+    make -j "$JOBS" && make install && cd ..
+    echo "Building gcc. Please wait.."
+    wget -O - https://ftp.gnu.org/gnu/gcc/gcc-8.4.0/gcc-8.4.0.tar.xz | tar -xJf -
+    CFLAGS="-fPIC"
+    CXXFLAGS="-fPIC"
+    CXXFLAGS_FOR_TARGET="-fPIC"
+    CFLAGS_FOR_TARGET="-fPIC"
+    export CFLAGS CXXFLAGS CXXFLAGS_FOR_TARGET CFLAGS_FOR_TARGET
+    mkdir build-gcc && cd build-gcc
+    ../gcc-8.4.0/configure --prefix="$__RootfsDir" --target="${__illumosArch}-sun-solaris2.10" --program-prefix="${__illumosArch}-illumos-" --with-sysroot="$__RootfsDir" --with-gnu-as       \
+        --with-gnu-ld --disable-nls --disable-libgomp --disable-libquadmath --disable-libssp --disable-libvtv --disable-libcilkrts --disable-libada --disable-libsanitizer \
+        --disable-libquadmath-support --disable-shared --enable-tls
+    make -j "$JOBS" && make install && cd ..
+    BaseUrl=https://pkgsrc.smartos.org
+    if [[ "$__UseMirror" == 1 ]]; then
+        BaseUrl=https://pkgsrc.smartos.skylime.net
+    fi
+    BaseUrl="$BaseUrl/packages/SmartOS/trunk/${__illumosArch}/All"
+    echo "Downloading manifest"
+    wget "$BaseUrl"
+    echo "Downloading dependencies."
+    read -ra array <<<"$__IllumosPackages"
+    for package in "${array[@]}"; do
+        echo "Installing '$package'"
+        package="$(sed -En '/.*href="('"$package"'-[0-9].*).tgz".*/h;$!d;g;s//\1/p' All)"
+        echo "Resolved name '$package'"
+        wget "$BaseUrl"/"$package".tgz
+        ar -x "$package".tgz
+        tar --skip-old-files -xzf "$package".tmp.tg* -C "$__RootfsDir" 2>/dev/null
+    done
+    echo "Cleaning up temporary files."
+    popd
+    rm -rf "$__RootfsDir"/{tmp,+*}
+    mkdir -p "$__RootfsDir"/usr/include/net
+    mkdir -p "$__RootfsDir"/usr/include/netpacket
+    wget -P "$__RootfsDir"/usr/include/net https://raw.githubusercontent.com/illumos/illumos-gate/master/usr/src/uts/common/io/bpf/net/bpf.h
+    wget -P "$__RootfsDir"/usr/include/net https://raw.githubusercontent.com/illumos/illumos-gate/master/usr/src/uts/common/io/bpf/net/dlt.h
+    wget -P "$__RootfsDir"/usr/include/netpacket https://raw.githubusercontent.com/illumos/illumos-gate/master/usr/src/uts/common/inet/sockmods/netpacket/packet.h
+    wget -P "$__RootfsDir"/usr/include/sys https://raw.githubusercontent.com/illumos/illumos-gate/master/usr/src/uts/common/sys/sdt.h
+elif [[ "$__CodeName" == "haiku" ]]; then
+    JOBS=${MAXJOBS:="$(getconf _NPROCESSORS_ONLN)"}
+    echo "Building Haiku sysroot for $__HaikuArch"
+    mkdir -p "$__RootfsDir/tmp"
+    pushd "$__RootfsDir/tmp"
+    mkdir "$__RootfsDir/tmp/download"
+    echo "Downloading Haiku package tool"
+    git clone https://github.com/haiku/haiku-toolchains-ubuntu --depth 1 $__RootfsDir/tmp/script
+    wget -O "$__RootfsDir/tmp/download/hosttools.zip" $($__RootfsDir/tmp/script/fetch.sh --hosttools)
+    unzip -o "$__RootfsDir/tmp/download/hosttools.zip" -d "$__RootfsDir/tmp/bin"
+    DepotBaseUrl="https://depot.haiku-os.org/__api/v2/pkg/get-pkg"
+    HpkgBaseUrl="https://eu.hpkg.haiku-os.org/haiku/master/$__HaikuArch/current"
+    echo "Downloading Haiku packages"
+    read -ra array <<<"$__HaikuPackages"
+    for package in "${array[@]}"; do
+        echo "Downloading $package..."
+        hpkgDownloadUrl="$(wget -qO- --post-data='{"name":"'"$package"'","repositorySourceCode":"haikuports_'$__HaikuArch'","versionType":"LATEST","naturalLanguageCode":"en"}' \
+            --header='Content-Type:application/json' "$DepotBaseUrl" | jq -r '.result.versions[].hpkgDownloadURL')"
+        wget -P "$__RootfsDir/tmp/download" "$hpkgDownloadUrl"
+    done
+    for package in haiku haiku_devel; do
+        echo "Downloading $package..."
+        hpkgVersion="$(wget -qO- $HpkgBaseUrl | sed -n 's/^.*version: "\([^"]*\)".*$/\1/p')"
+        wget -P "$__RootfsDir/tmp/download" "$HpkgBaseUrl/packages/$package-$hpkgVersion-1-$__HaikuArch.hpkg"
+    done
+    echo "Setting up sysroot and extracting required packages"
+    mkdir -p "$__RootfsDir/boot/system"
+    for file in "$__RootfsDir/tmp/download/"*.hpkg; do
+        echo "Extracting $file..."
+        LD_LIBRARY_PATH="$__RootfsDir/tmp/bin" "$__RootfsDir/tmp/bin/package" extract -C "$__RootfsDir/boot/system" "$file"
+    done
+    echo "Downloading Haiku buildtools"
+    wget -O "$__RootfsDir/tmp/download/buildtools.zip" $($__RootfsDir/tmp/script/fetch.sh --buildtools --arch=$__HaikuArch)
+    unzip -o "$__RootfsDir/tmp/download/buildtools.zip" -d "$__RootfsDir"
+    echo "Cleaning up temporary files"
+    popd
+    rm -rf "$__RootfsDir/tmp"
+elif [[ -n "$__CodeName" ]]; then
+    if [[ "$__SkipSigCheck" == "0" ]]; then
+        __Keyring="$__Keyring --force-check-gpg"
+    fi
+    debootstrap "--variant=minbase" $__Keyring --arch "$__UbuntuArch" "$__CodeName" "$__RootfsDir" "$__UbuntuRepo"
+    cp "$__CrossDir/$__BuildArch/sources.list.$__CodeName" "$__RootfsDir/etc/apt/sources.list"
+    chroot "$__RootfsDir" apt-get update
+    chroot "$__RootfsDir" apt-get -f -y install
+    chroot "$__RootfsDir" apt-get -y install $__UbuntuPackages
+    chroot "$__RootfsDir" symlinks -cr /usr
+    chroot "$__RootfsDir" apt-get clean
+    if [[ "$__SkipUnmount" == "0" ]]; then
+        umount "$__RootfsDir"/* || true
+    fi
+    if [[ "$__BuildArch" == "armel" && "$__CodeName" == "jessie" ]]; then
+        pushd "$__RootfsDir"
+        patch -p1 < "$__CrossDir/$__BuildArch/armel.jessie.patch"
+        popd
+    fi
+elif [[ "$__Tizen" == "tizen" ]]; then
+    ROOTFS_DIR="$__RootfsDir" "$__CrossDir/tizen-build-rootfs.sh" "$__BuildArch"
+else
+    echo "Unsupported target platform."
+    usage;
+    exit 1
+fi

--- a//dev/null
+++ b/eng/common/cross/tizen-build-rootfs.sh
@@ -0,0 +1,48 @@
+set -e
+ARCH=$1
+LINK_ARCH=$ARCH
+case "$ARCH" in
+    arm)
+        TIZEN_ARCH="armv7hl"
+        ;;
+    armel)
+        TIZEN_ARCH="armv7l"
+        LINK_ARCH="arm"
+        ;;
+    arm64)
+        TIZEN_ARCH="aarch64"
+        ;;
+    x86)
+        TIZEN_ARCH="i686"
+        ;;
+    x64)
+        TIZEN_ARCH="x86_64"
+        LINK_ARCH="x86"
+        ;;
+    *)
+        echo "Unsupported architecture for tizen: $ARCH"
+        exit 1
+esac
+__CrossDir=$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )
+__TIZEN_CROSSDIR="$__CrossDir/${ARCH}/tizen"
+if [[ -z "$ROOTFS_DIR" ]]; then
+    echo "ROOTFS_DIR is not defined."
+    exit 1;
+fi
+TIZEN_TMP_DIR=$ROOTFS_DIR/tizen_tmp
+mkdir -p $TIZEN_TMP_DIR
+echo ">>Start downloading files"
+VERBOSE=1 $__CrossDir/tizen-fetch.sh $TIZEN_TMP_DIR $TIZEN_ARCH
+echo "<<Finish downloading files"
+echo ">>Start constructing Tizen rootfs"
+TIZEN_RPM_FILES=`ls $TIZEN_TMP_DIR/*.rpm`
+cd $ROOTFS_DIR
+for f in $TIZEN_RPM_FILES; do
+    rpm2cpio $f  | cpio -idm --quiet
+done
+echo "<<Finish constructing Tizen rootfs"
+rm -rf $TIZEN_TMP_DIR
+echo ">>Start configuring Tizen rootfs"
+ln -sfn asm-${LINK_ARCH} ./usr/include/asm
+patch -p1 < $__TIZEN_CROSSDIR/tizen.patch
+echo "<<Finish configuring Tizen rootfs"

--- a//dev/null
+++ b/eng/common/cross/tizen-fetch.sh
@@ -0,0 +1,135 @@
+set -e
+if [[ -z "${VERBOSE// }" ]] || [ "$VERBOSE" -ne "$VERBOSE" ] 2>/dev/null; then
+    VERBOSE=0
+fi
+Log()
+{
+    if [ $VERBOSE -ge $1 ]; then
+        echo ${@:2}
+    fi
+}
+Inform()
+{
+    Log 1 -e "\x1B[0;34m$@\x1B[m"
+}
+Debug()
+{
+    Log 2 -e "\x1B[0;32m$@\x1B[m"
+}
+Error()
+{
+    >&2 Log 0 -e "\x1B[0;31m$@\x1B[m"
+}
+Fetch()
+{
+    URL=$1
+    FILE=$2
+    PROGRESS=$3
+    if [ $VERBOSE -ge 1 ] && [ $PROGRESS ]; then
+        CURL_OPT="--progress-bar"
+    else
+        CURL_OPT="--silent"
+    fi
+    curl $CURL_OPT $URL > $FILE
+}
+hash curl 2> /dev/null || { Error "Require 'curl' Aborting."; exit 1; }
+hash xmllint 2> /dev/null || { Error "Require 'xmllint' Aborting."; exit 1; }
+hash sha256sum 2> /dev/null || { Error "Require 'sha256sum' Aborting."; exit 1; }
+TMPDIR=$1
+if [ ! -d $TMPDIR ]; then
+    TMPDIR=./tizen_tmp
+    Debug "Create temporary directory : $TMPDIR"
+    mkdir -p $TMPDIR
+fi
+TIZEN_ARCH=$2
+TIZEN_URL=http://download.tizen.org/snapshots/TIZEN/Tizen
+BUILD_XML=build.xml
+REPOMD_XML=repomd.xml
+PRIMARY_XML=primary.xml
+TARGET_URL="http://__not_initialized"
+Xpath_get()
+{
+    XPATH_RESULT=''
+    XPATH=$1
+    XML_FILE=$2
+    RESULT=$(xmllint --xpath $XPATH $XML_FILE)
+    if [[ -z ${RESULT// } ]]; then
+        Error "Can not find target from $XML_FILE"
+        Debug "Xpath = $XPATH"
+        exit 1
+    fi
+    XPATH_RESULT=$RESULT
+}
+fetch_tizen_pkgs_init()
+{
+    TARGET=$1
+    PROFILE=$2
+    Debug "Initialize TARGET=$TARGET, PROFILE=$PROFILE"
+    TMP_PKG_DIR=$TMPDIR/tizen_${PROFILE}_pkgs
+    if [ -d $TMP_PKG_DIR ]; then rm -rf $TMP_PKG_DIR; fi
+    mkdir -p $TMP_PKG_DIR
+    PKG_URL=$TIZEN_URL/$PROFILE/latest
+    BUILD_XML_URL=$PKG_URL/$BUILD_XML
+    TMP_BUILD=$TMP_PKG_DIR/$BUILD_XML
+    TMP_REPOMD=$TMP_PKG_DIR/$REPOMD_XML
+    TMP_PRIMARY=$TMP_PKG_DIR/$PRIMARY_XML
+    TMP_PRIMARYGZ=${TMP_PRIMARY}.gz
+    Fetch $BUILD_XML_URL $TMP_BUILD
+    Debug "fetch $BUILD_XML_URL to $TMP_BUILD"
+    TARGET_XPATH="//build/buildtargets/buildtarget[@name=\"$TARGET\"]/repo[@type=\"binary\"]/text()"
+    Xpath_get $TARGET_XPATH $TMP_BUILD
+    TARGET_PATH=$XPATH_RESULT
+    TARGET_URL=$PKG_URL/$TARGET_PATH
+    REPOMD_URL=$TARGET_URL/repodata/repomd.xml
+    PRIMARY_XPATH='string(//*[local-name()="data"][@type="primary"]/*[local-name()="location"]/@href)'
+    Fetch $REPOMD_URL $TMP_REPOMD
+    Debug "fetch $REPOMD_URL to $TMP_REPOMD"
+    Xpath_get $PRIMARY_XPATH $TMP_REPOMD
+    PRIMARY_XML_PATH=$XPATH_RESULT
+    PRIMARY_URL=$TARGET_URL/$PRIMARY_XML_PATH
+    Fetch $PRIMARY_URL $TMP_PRIMARYGZ
+    Debug "fetch $PRIMARY_URL to $TMP_PRIMARYGZ"
+    gunzip $TMP_PRIMARYGZ
+    Debug "unzip $TMP_PRIMARYGZ to $TMP_PRIMARY"
+}
+fetch_tizen_pkgs()
+{
+    ARCH=$1
+    PACKAGE_XPATH_TPL='string(//*[local-name()="metadata"]/*[local-name()="package"][*[local-name()="name"][text()="_PKG_"]][*[local-name()="arch"][text()="_ARCH_"]]/*[local-name()="location"]/@href)'
+    PACKAGE_CHECKSUM_XPATH_TPL='string(//*[local-name()="metadata"]/*[local-name()="package"][*[local-name()="name"][text()="_PKG_"]][*[local-name()="arch"][text()="_ARCH_"]]/*[local-name()="checksum"]/text())'
+    for pkg in ${@:2}
+    do
+        Inform "Fetching... $pkg"
+        XPATH=${PACKAGE_XPATH_TPL/_PKG_/$pkg}
+        XPATH=${XPATH/_ARCH_/$ARCH}
+        Xpath_get $XPATH $TMP_PRIMARY
+        PKG_PATH=$XPATH_RESULT
+        XPATH=${PACKAGE_CHECKSUM_XPATH_TPL/_PKG_/$pkg}
+        XPATH=${XPATH/_ARCH_/$ARCH}
+        Xpath_get $XPATH $TMP_PRIMARY
+        CHECKSUM=$XPATH_RESULT
+        PKG_URL=$TARGET_URL/$PKG_PATH
+        PKG_FILE=$(basename $PKG_PATH)
+        PKG_PATH=$TMPDIR/$PKG_FILE
+        Debug "Download $PKG_URL to $PKG_PATH"
+        Fetch $PKG_URL $PKG_PATH true
+        echo "$CHECKSUM $PKG_PATH" | sha256sum -c - > /dev/null
+        if [ $? -ne 0 ]; then
+            Error "Fail to fetch $PKG_URL to $PKG_PATH"
+            Debug "Checksum = $CHECKSUM"
+            exit 1
+        fi
+    done
+}
+Inform "Initialize ${TIZEN_ARCH} base"
+fetch_tizen_pkgs_init standard Tizen-Base
+Inform "fetch common packages"
+fetch_tizen_pkgs ${TIZEN_ARCH} gcc gcc-devel-static glibc glibc-devel libicu libicu-devel libatomic linux-glibc-devel keyutils keyutils-devel libkeyutils
+Inform "fetch coreclr packages"
+fetch_tizen_pkgs ${TIZEN_ARCH} lldb lldb-devel libgcc libstdc++ libstdc++-devel libunwind libunwind-devel lttng-ust-devel lttng-ust userspace-rcu-devel userspace-rcu
+Inform "fetch corefx packages"
+fetch_tizen_pkgs ${TIZEN_ARCH} libcom_err libcom_err-devel zlib zlib-devel libopenssl11 libopenssl1.1-devel krb5 krb5-devel
+Inform "Initialize standard unified"
+fetch_tizen_pkgs_init standard Tizen-Unified
+Inform "fetch corefx packages"
+fetch_tizen_pkgs ${TIZEN_ARCH} gssdp gssdp-devel tizen-release

--- a//dev/null
+++ b/eng/common/dotnet-install.sh
@@ -0,0 +1,78 @@
+source="${BASH_SOURCE[0]}"
+while [[ -h "$source" ]]; do
+  scriptroot="$( cd -P "$( dirname "$source" )" && pwd )"
+  source="$(readlink "$source")"
+  [[ $source != /* ]] && source="$scriptroot/$source"
+done
+scriptroot="$( cd -P "$( dirname "$source" )" && pwd )"
+. "$scriptroot/tools.sh"
+version='Latest'
+architecture=''
+runtime='dotnet'
+runtimeSourceFeed=''
+runtimeSourceFeedKey=''
+while [[ $# > 0 ]]; do
+  opt="$(echo "$1" | tr "[:upper:]" "[:lower:]")"
+  case "$opt" in
+    -version|-v)
+      shift
+      version="$1"
+      ;;
+    -architecture|-a)
+      shift
+      architecture="$1"
+      ;;
+    -runtime|-r)
+      shift
+      runtime="$1"
+      ;;
+    -runtimesourcefeed)
+      shift
+      runtimeSourceFeed="$1"
+      ;;
+    -runtimesourcefeedkey)
+      shift
+      runtimeSourceFeedKey="$1"
+      ;;
+    *)
+      Write-PipelineTelemetryError -Category 'Build' -Message "Invalid argument: $1"
+      exit 1
+      ;;
+  esac
+  shift
+done
+cpuname=$(uname -m)
+case $cpuname in
+  arm64|aarch64)
+    buildarch=arm64
+    if [ "$(getconf LONG_BIT)" -lt 64 ]; then
+        buildarch=arm
+    fi
+    ;;
+  loongarch64)
+    buildarch=loongarch64
+    ;;
+  amd64|x86_64)
+    buildarch=x64
+    ;;
+  armv*l)
+    buildarch=arm
+    ;;
+  i[3-6]86)
+    buildarch=x86
+    ;;
+  *)
+    echo "Unknown CPU $cpuname detected, treating it as x64"
+    buildarch=x64
+    ;;
+esac
+dotnetRoot="${repo_root}.dotnet"
+if [[ $architecture != "" ]] && [[ $architecture != $buildarch ]]; then
+  dotnetRoot="$dotnetRoot/$architecture"
+fi
+InstallDotNet $dotnetRoot $version "$architecture" $runtime true $runtimeSourceFeed $runtimeSourceFeedKey || {
+  local exit_code=$?
+  Write-PipelineTelemetryError -Category 'InitializeToolset' -Message "dotnet-install.sh failed (exit code '$exit_code')." >&2
+  ExitWithExitCode $exit_code
+}
+ExitWithExitCode 0

--- a//dev/null
+++ b/eng/common/native/init-compiler.sh
@@ -0,0 +1,106 @@
+if [ -z "$build_arch" ] || [ -z "$compiler" ]; then
+  echo "Usage..."
+  echo "build_arch=<ARCH> compiler=<NAME> init-compiler.sh"
+  echo "Specify the target architecture."
+  echo "Specify the name of compiler (clang or gcc)."
+  exit 1
+fi
+case "$compiler" in
+    clang*|-clang*|--clang*)
+        version="$(echo "$compiler" | tr -d '[:alpha:]-=')"
+        majorVersion="${version%%.*}"
+        [ -z "${version##*.*}" ] && minorVersion="${version#*.}"
+        if [ -z "$minorVersion" ] && [ -n "$majorVersion" ] && [ "$majorVersion" -le 6 ]; then
+            minorVersion=0;
+        fi
+        compiler=clang
+        ;;
+    gcc*|-gcc*|--gcc*)
+        version="$(echo "$compiler" | tr -d '[:alpha:]-=')"
+        majorVersion="${version%%.*}"
+        [ -z "${version##*.*}" ] && minorVersion="${version#*.}"
+        compiler=gcc
+        ;;
+esac
+cxxCompiler="$compiler++"
+CC=
+CXX=
+LDFLAGS=
+if [ "$compiler" = "gcc" ]; then cxxCompiler="g++"; fi
+check_version_exists() {
+    desired_version=-1
+    if command -v "$compiler-$1.$2" > /dev/null; then
+        desired_version="-$1.$2"
+    elif command -v "$compiler$1$2" > /dev/null; then
+        desired_version="$1$2"
+    elif command -v "$compiler-$1$2" > /dev/null; then
+        desired_version="-$1$2"
+    fi
+    echo "$desired_version"
+}
+if [ -z "$CLR_CC" ]; then
+    if [ -z "$majorVersion" ]; then
+        if [ "$compiler" = "clang" ]; then versions="17 16 15 14 13 12 11 10 9 8 7 6.0 5.0 4.0 3.9 3.8 3.7 3.6 3.5"
+        elif [ "$compiler" = "gcc" ]; then versions="13 12 11 10 9 8 7 6 5 4.9"; fi
+        for version in $versions; do
+            _major="${version%%.*}"
+            [ -z "${version##*.*}" ] && _minor="${version#*.}"
+            desired_version="$(check_version_exists "$_major" "$_minor")"
+            if [ "$desired_version" != "-1" ]; then majorVersion="$_major"; break; fi
+        done
+        if [ -z "$majorVersion" ]; then
+            if command -v "$compiler" > /dev/null; then
+                if [ "$(uname)" != "Darwin" ]; then
+                    echo "Warning: Specific version of $compiler not found, falling back to use the one in PATH."
+                fi
+                CC="$(command -v "$compiler")"
+                CXX="$(command -v "$cxxCompiler")"
+            else
+                echo "No usable version of $compiler found."
+                exit 1
+            fi
+        else
+            if [ "$compiler" = "clang" ] && [ "$majorVersion" -lt 5 ]; then
+                if [ "$build_arch" = "arm" ] || [ "$build_arch" = "armel" ]; then
+                    if command -v "$compiler" > /dev/null; then
+                        echo "Warning: Found clang version $majorVersion which is not supported on arm/armel architectures, falling back to use clang from PATH."
+                        CC="$(command -v "$compiler")"
+                        CXX="$(command -v "$cxxCompiler")"
+                    else
+                        echo "Found clang version $majorVersion which is not supported on arm/armel architectures, and there is no clang in PATH."
+                        exit 1
+                    fi
+                fi
+            fi
+        fi
+    else
+        desired_version="$(check_version_exists "$majorVersion" "$minorVersion")"
+        if [ "$desired_version" = "-1" ]; then
+            echo "Could not find specific version of $compiler: $majorVersion $minorVersion."
+            exit 1
+        fi
+    fi
+    if [ -z "$CC" ]; then
+        CC="$(command -v "$compiler$desired_version")"
+        CXX="$(command -v "$cxxCompiler$desired_version")"
+        if [ -z "$CXX" ]; then CXX="$(command -v "$cxxCompiler")"; fi
+    fi
+else
+    if [ ! -f "$CLR_CC" ]; then
+        echo "CLR_CC is set but path '$CLR_CC' does not exist"
+        exit 1
+    fi
+    CC="$CLR_CC"
+    CXX="$CLR_CXX"
+fi
+if [ -z "$CC" ]; then
+    echo "Unable to find $compiler."
+    exit 1
+fi
+if [ "$compiler" = "clang" ] && [ -n "$majorVersion" ] && [ "$majorVersion" -ge 9 ] && [ "$build_arch" != "s390x" ]; then
+    if "$CC" -fuse-ld=lld -Wl,--version >/dev/null 2>&1; then
+        LDFLAGS="-fuse-ld=lld"
+    fi
+fi
+SCAN_BUILD_COMMAND="$(command -v "scan-build$desired_version")"
+export CC CXX LDFLAGS SCAN_BUILD_COMMAND

--- a//dev/null
+++ b/eng/common/native/init-distro-rid.sh
@@ -0,0 +1,81 @@
+getNonPortableDistroRid()
+{
+    local targetOs="$1"
+    local targetArch="$2"
+    local rootfsDir="$3"
+    local nonPortableRid=""
+    if [ "$targetOs" = "linux" ]; then
+        if [ -e "${rootfsDir}/etc/os-release" ]; then
+            source "${rootfsDir}/etc/os-release"
+            if [[ "${ID}" == "rhel" || "${ID}" == "rocky" || "${ID}" == "alpine" ]]; then
+                VERSION_ID="${VERSION_ID%.*}"
+            fi
+            if [[ "${VERSION_ID:-}" =~ ^([[:digit:]]|\.)+$ ]]; then
+                nonPortableRid="${ID}.${VERSION_ID}-${targetArch}"
+            else
+                nonPortableRid="${ID}-${targetArch}"
+            fi
+        elif [ -e "${rootfsDir}/android_platform" ]; then
+            source "$rootfsDir"/android_platform
+            nonPortableRid="$RID"
+        fi
+    fi
+    if [ "$targetOs" = "freebsd" ]; then
+        __freebsd_major_version=$($rootfsDir/bin/freebsd-version | { read v; echo "${v%%.*}"; })
+        nonPortableRid="freebsd.$__freebsd_major_version-${targetArch}"
+    elif command -v getprop && getprop ro.product.system.model 2>&1 | grep -qi android; then
+        __android_sdk_version=$(getprop ro.build.version.sdk)
+        nonPortableRid="android.$__android_sdk_version-${targetArch}"
+    elif [ "$targetOs" = "illumos" ]; then
+        __uname_version=$(uname -v)
+        case "$__uname_version" in
+            omnios-*)
+                __omnios_major_version=$(echo "${__uname_version:8:2}")
+                nonPortableRid=omnios."$__omnios_major_version"-"$targetArch"
+            ;;
+            joyent_*)
+                __smartos_major_version=$(echo "${__uname_version:7:4}")
+                nonPortableRid=smartos."$__smartos_major_version"-"$targetArch"
+            ;;
+            illumos_*)
+                nonPortableRid=openindiana-"$targetArch"
+            ;;
+        esac
+    elif [ "$targetOs" = "solaris" ]; then
+        __uname_version=$(uname -v)
+        __solaris_major_version=$(echo "${__uname_version%.*}")
+        nonPortableRid=solaris."$__solaris_major_version"-"$targetArch"
+    elif [ "$targetOs" = "haiku" ]; then
+        __uname_release=$(uname -r)
+        nonPortableRid=haiku.r"$__uname_release"-"$targetArch"
+    fi
+    echo "$(echo $nonPortableRid | tr '[:upper:]' '[:lower:]')"
+}
+initDistroRidGlobal()
+{
+    local targetOs="$1"
+    local targetArch="$2"
+    local rootfsDir=""
+    if [ "$#" -ge 3 ]; then
+        rootfsDir="$3"
+    fi
+    if [ -n "${rootfsDir}" ]; then
+        if [ ! -e "${rootfsDir}" ]; then
+            echo "Error rootfsDir has been passed, but the location is not valid."
+            exit 1
+        fi
+    fi
+    __DistroRid=$(getNonPortableDistroRid "${targetOs}" "${targetArch}" "${rootfsDir}")
+    if [ -z "${__PortableTargetOS:-}" ]; then
+        __PortableTargetOS="$targetOs"
+        STRINGS="$(command -v strings || true)"
+        if [ -z "$STRINGS" ]; then
+            STRINGS="$(command -v llvm-strings || true)"
+        fi
+        if "${rootfsDir}/usr/bin/ldd" --version 2>&1 | grep -q musl ||
+                ( [ -n "$STRINGS" ] && "$STRINGS" "${rootfsDir}/usr/bin/ldd" 2>&1 | grep -q musl ); then
+            __PortableTargetOS="linux-musl"
+        fi
+    fi
+    export __DistroRid __PortableTargetOS
+}

--- a//dev/null
+++ b/eng/common/native/init-os-and-arch.sh
@@ -0,0 +1,61 @@
+OSName=$(uname -s | tr '[:upper:]' '[:lower:]')
+if command -v getprop && getprop ro.product.system.model 2>&1 | grep -qi android; then
+    OSName="android"
+fi
+case "$OSName" in
+freebsd|linux|netbsd|openbsd|sunos|android|haiku)
+    os="$OSName" ;;
+darwin)
+    os=osx ;;
+*)
+    echo "Unsupported OS $OSName detected!"
+    exit 1 ;;
+esac
+if [ "$os" = "sunos" ]; then
+    if uname -o 2>&1 | grep -q illumos; then
+        os="illumos"
+    else
+        os="solaris"
+    fi
+    CPUName=$(isainfo -n)
+else
+    CPUName=$(uname -m)
+fi
+case "$CPUName" in
+    arm64|aarch64)
+        arch=arm64
+        ;;
+    loongarch64)
+        arch=loongarch64
+        ;;
+    riscv64)
+        arch=riscv64
+        ;;
+    amd64|x86_64)
+        arch=x64
+        ;;
+    armv7l|armv8l)
+        if (NAME=""; . /etc/os-release; test "$NAME" = "Tizen"); then
+            arch=armel
+        else
+            arch=arm
+        fi
+        ;;
+    armv6l)
+        arch=armv6
+        ;;
+    i[3-6]86)
+        echo "Unsupported CPU $CPUName detected, build might not succeed!"
+        arch=x86
+        ;;
+    s390x)
+        arch=s390x
+        ;;
+    ppc64le)
+        arch=ppc64le
+        ;;
+    *)
+        echo "Unknown CPU $CPUName detected!"
+        exit 1
+        ;;
+esac

--- a//dev/null
+++ b/eng/common/tools.sh
@@ -0,0 +1,420 @@
+ci=${ci:-false}
+if [[ "$ci" == true ]]; then
+  pipelines_log=${pipelines_log:-true}
+else
+  pipelines_log=${pipelines_log:-false}
+fi
+configuration=${configuration:-'Debug'}
+exclude_ci_binary_log=${exclude_ci_binary_log:-false}
+if [[ "$ci" == true && "$exclude_ci_binary_log" == false ]]; then
+  binary_log_default=true
+else
+  binary_log_default=false
+fi
+binary_log=${binary_log:-$binary_log_default}
+prepare_machine=${prepare_machine:-false}
+restore=${restore:-true}
+verbosity=${verbosity:-'minimal'}
+if [[ "$ci" == true ]]; then
+  node_reuse=${node_reuse:-false}
+else
+  node_reuse=${node_reuse:-true}
+fi
+warn_as_error=${warn_as_error:-true}
+use_installed_dotnet_cli=${use_installed_dotnet_cli:-true}
+dotnetInstallScriptVersion=${dotnetInstallScriptVersion:-'v1'}
+if [[ "$ci" == true ]]; then
+  use_global_nuget_cache=${use_global_nuget_cache:-false}
+else
+  use_global_nuget_cache=${use_global_nuget_cache:-true}
+fi
+runtime_source_feed=${runtime_source_feed:-''}
+runtime_source_feed_key=${runtime_source_feed_key:-''}
+function ResolvePath {
+  local path=$1
+  while [[ -h $path ]]; do
+    local dir="$( cd -P "$( dirname "$path" )" && pwd )"
+    path="$(readlink "$path")"
+    [[ $path != /* ]] && path="$dir/$path"
+  done
+  _ResolvePath="$path"
+}
+function ReadGlobalVersion {
+  local key=$1
+  if command -v jq &> /dev/null; then
+    _ReadGlobalVersion="$(jq -r ".[] | select(has(\"$key\")) | .\"$key\"" "$global_json_file")"
+  elif [[ "$(cat "$global_json_file")" =~ \"$key\"[[:space:]\:]*\"([^\"]+) ]]; then
+    _ReadGlobalVersion=${BASH_REMATCH[1]}
+  fi
+  if [[ -z "$_ReadGlobalVersion" ]]; then
+    Write-PipelineTelemetryError -category 'Build' "Error: Cannot find \"$key\" in $global_json_file"
+    ExitWithExitCode 1
+  fi
+}
+function InitializeDotNetCli {
+  if [[ -n "${_InitializeDotNetCli:-}" ]]; then
+    return
+  fi
+  local install=$1
+  export DOTNET_MULTILEVEL_LOOKUP=0
+  export DOTNET_SKIP_FIRST_TIME_EXPERIENCE=1
+  if [[ $ci == true ]]; then
+    export DOTNET_CLI_TELEMETRY_OPTOUT=1
+  fi
+  export LTTNG_HOME="$HOME"
+  if [[ -n "${DotNetCoreSdkDir:-}" ]]; then
+    export DOTNET_INSTALL_DIR="$DotNetCoreSdkDir"
+  fi
+  if [[ "$use_installed_dotnet_cli" == true && $global_json_has_runtimes == false && -z "${DOTNET_INSTALL_DIR:-}" ]]; then
+    local dotnet_path=`command -v dotnet`
+    if [[ -n "$dotnet_path" ]]; then
+      ResolvePath "$dotnet_path"
+      export DOTNET_INSTALL_DIR=`dirname "$_ResolvePath"`
+    fi
+  fi
+  ReadGlobalVersion "dotnet"
+  local dotnet_sdk_version=$_ReadGlobalVersion
+  local dotnet_root=""
+  if [[ $global_json_has_runtimes == false && -n "${DOTNET_INSTALL_DIR:-}" && -d "$DOTNET_INSTALL_DIR/sdk/$dotnet_sdk_version" ]]; then
+    dotnet_root="$DOTNET_INSTALL_DIR"
+  else
+    dotnet_root="$repo_root/.dotnet"
+    export DOTNET_INSTALL_DIR="$dotnet_root"
+    if [[ ! -d "$DOTNET_INSTALL_DIR/sdk/$dotnet_sdk_version" ]]; then
+      if [[ "$install" == true ]]; then
+        InstallDotNetSdk "$dotnet_root" "$dotnet_sdk_version"
+      else
+        Write-PipelineTelemetryError -category 'InitializeToolset' "Unable to find dotnet with SDK version '$dotnet_sdk_version'"
+        ExitWithExitCode 1
+      fi
+    fi
+  fi
+  Write-PipelinePrependPath -path "$dotnet_root"
+  Write-PipelineSetVariable -name "DOTNET_MULTILEVEL_LOOKUP" -value "0"
+  Write-PipelineSetVariable -name "DOTNET_SKIP_FIRST_TIME_EXPERIENCE" -value "1"
+  _InitializeDotNetCli="$dotnet_root"
+}
+function InstallDotNetSdk {
+  local root=$1
+  local version=$2
+  local architecture="unset"
+  if [[ $# -ge 3 ]]; then
+    architecture=$3
+  fi
+  InstallDotNet "$root" "$version" $architecture 'sdk' 'true' $runtime_source_feed $runtime_source_feed_key
+}
+function InstallDotNet {
+  local root=$1
+  local version=$2
+  local runtime=$4
+  local dotnetVersionLabel="'$runtime v$version'"
+  if [[ -n "${4:-}" ]] && [ "$4" != 'sdk' ]; then
+    runtimePath="$root"
+    runtimePath="$runtimePath/shared"
+    case "$runtime" in
+      dotnet)
+        runtimePath="$runtimePath/Microsoft.NETCore.App"
+        ;;
+      aspnetcore)
+        runtimePath="$runtimePath/Microsoft.AspNetCore.App"
+        ;;
+      windowsdesktop)
+        runtimePath="$runtimePath/Microsoft.WindowsDesktop.App"
+        ;;
+      *)
+        ;;
+    esac
+    runtimePath="$runtimePath/$version"
+    dotnetVersionLabel="runtime toolset '$runtime/$architecture v$version'"
+    if [ -d "$runtimePath" ]; then
+      echo "  Runtime toolset '$runtime/$architecture v$version' already installed."
+      local installSuccess=1
+      return
+    fi
+  fi
+  GetDotNetInstallScript "$root"
+  local install_script=$_GetDotNetInstallScript
+  local installParameters=(--version $version --install-dir "$root")
+  if [[ -n "${3:-}" ]] && [ "$3" != 'unset' ]; then
+    installParameters+=(--architecture $3)
+  fi
+  if [[ -n "${4:-}" ]] && [ "$4" != 'sdk' ]; then
+    installParameters+=(--runtime $4)
+  fi
+  if [[ "$#" -ge "5" ]] && [[ "$5" != 'false' ]]; then
+    installParameters+=(--skip-non-versioned-files)
+  fi
+  local variations=() # list of variable names with parameter arrays in them
+  local public_location=("${installParameters[@]}")
+  variations+=(public_location)
+  local dotnetbuilds=("${installParameters[@]}" --azure-feed "https://dotnetbuilds.azureedge.net/public")
+  variations+=(dotnetbuilds)
+  if [[ -n "${6:-}" ]]; then
+    variations+=(private_feed)
+    local private_feed=("${installParameters[@]}" --azure-feed $6)
+    if [[ -n "${7:-}" ]]; then
+      decodeArg="--decode"
+      if base64 --help 2>&1 | grep -q "BusyBox"; then
+          decodeArg="-d"
+      fi
+      decodedFeedKey=`echo $7 | base64 $decodeArg`
+      private_feed+=(--feed-credential $decodedFeedKey)
+    fi
+  fi
+  local installSuccess=0
+  for variationName in "${variations[@]}"; do
+    local name="$variationName[@]"
+    local variation=("${!name}")
+    echo "  Attempting to install $dotnetVersionLabel from $variationName."
+    bash "$install_script" "${variation[@]}" && installSuccess=1
+    if [[ "$installSuccess" -eq 1 ]]; then
+      break
+    fi
+    echo "  Failed to install $dotnetVersionLabel from $variationName."
+  done
+  if [[ "$installSuccess" -eq 0 ]]; then
+    Write-PipelineTelemetryError -category 'InitializeToolset' "Failed to install $dotnetVersionLabel from any of the specified locations."
+    ExitWithExitCode 1
+  fi
+}
+function with_retries {
+  local maxRetries=5
+  local retries=1
+  echo "Trying to run '$@' for maximum of $maxRetries attempts."
+  while [[ $((retries++)) -le $maxRetries ]]; do
+    "$@"
+    if [[ $? == 0 ]]; then
+      echo "Ran '$@' successfully."
+      return 0
+    fi
+    timeout=$((3**$retries-1))
+    echo "Failed to execute '$@'. Waiting $timeout seconds before next attempt ($retries out of $maxRetries)." 1>&2
+    sleep $timeout
+  done
+  echo "Failed to execute '$@' for $maxRetries times." 1>&2
+  return 1
+}
+function GetDotNetInstallScript {
+  local root=$1
+  local install_script="$root/dotnet-install.sh"
+  local install_script_url="https://dotnet.microsoft.com/download/dotnet/scripts/$dotnetInstallScriptVersion/dotnet-install.sh"
+  if [[ ! -a "$install_script" ]]; then
+    mkdir -p "$root"
+    echo "Downloading '$install_script_url'"
+    if command -v curl > /dev/null; then
+      curl "$install_script_url" -sSL --retry 10 --create-dirs -o "$install_script" || {
+        if command -v openssl &> /dev/null; then
+          echo "Curl failed; dumping some information about dotnet.microsoft.com for later investigation"
+          echo | openssl s_client -showcerts -servername dotnet.microsoft.com  -connect dotnet.microsoft.com:443
+        fi
+        echo "Will now retry the same URL with verbose logging."
+        with_retries curl "$install_script_url" -sSL --verbose --retry 10 --create-dirs -o "$install_script" || {
+          local exit_code=$?
+          Write-PipelineTelemetryError -category 'InitializeToolset' "Failed to acquire dotnet install script (exit code '$exit_code')."
+          ExitWithExitCode $exit_code
+        }
+      }
+    else
+      with_retries wget -v -O "$install_script" "$install_script_url" || {
+        local exit_code=$?
+        Write-PipelineTelemetryError -category 'InitializeToolset' "Failed to acquire dotnet install script (exit code '$exit_code')."
+        ExitWithExitCode $exit_code
+      }
+    fi
+  fi
+  _GetDotNetInstallScript="$install_script"
+}
+function InitializeBuildTool {
+  if [[ -n "${_InitializeBuildTool:-}" ]]; then
+    return
+  fi
+  InitializeDotNetCli $restore
+  _InitializeBuildTool="$_InitializeDotNetCli/dotnet"
+  _InitializeBuildToolCommand="msbuild"
+  _InitializeBuildToolFramework="net8.0"
+}
+function GetNuGetPackageCachePath {
+  if [[ -z ${NUGET_PACKAGES:-} ]]; then
+    if [[ "$use_global_nuget_cache" == true ]]; then
+      export NUGET_PACKAGES="$HOME/.nuget/packages"
+    else
+      export NUGET_PACKAGES="$repo_root/.packages"
+      export RESTORENOCACHE=true
+    fi
+  fi
+  _GetNuGetPackageCachePath=$NUGET_PACKAGES
+}
+function InitializeNativeTools() {
+  if [[ -n "${DisableNativeToolsetInstalls:-}" ]]; then
+    return
+  fi
+  if grep -Fq "native-tools" $global_json_file
+  then
+    local nativeArgs=""
+    if [[ "$ci" == true ]]; then
+      nativeArgs="--installDirectory $tools_dir"
+    fi
+    "$_script_dir/init-tools-native.sh" $nativeArgs
+  fi
+}
+function InitializeToolset {
+  if [[ -n "${_InitializeToolset:-}" ]]; then
+    return
+  fi
+  GetNuGetPackageCachePath
+  ReadGlobalVersion "Microsoft.DotNet.Arcade.Sdk"
+  local toolset_version=$_ReadGlobalVersion
+  local toolset_location_file="$toolset_dir/$toolset_version.txt"
+  if [[ -a "$toolset_location_file" ]]; then
+    local path=`cat "$toolset_location_file"`
+    if [[ -a "$path" ]]; then
+      _InitializeToolset="$path"
+      return
+    fi
+  fi
+  if [[ "$restore" != true ]]; then
+    Write-PipelineTelemetryError -category 'InitializeToolset' "Toolset version $toolset_version has not been restored."
+    ExitWithExitCode 2
+  fi
+  local proj="$toolset_dir/restore.proj"
+  local bl=""
+  if [[ "$binary_log" == true ]]; then
+    bl="/bl:$log_dir/ToolsetRestore.binlog"
+  fi
+  echo '<Project Sdk="Microsoft.DotNet.Arcade.Sdk"/>' > "$proj"
+  MSBuild-Core "$proj" $bl /t:__WriteToolsetLocation /clp:ErrorsOnly\;NoSummary /p:__ToolsetLocationOutputFile="$toolset_location_file"
+  local toolset_build_proj=`cat "$toolset_location_file"`
+  if [[ ! -a "$toolset_build_proj" ]]; then
+    Write-PipelineTelemetryError -category 'Build' "Invalid toolset path: $toolset_build_proj"
+    ExitWithExitCode 3
+  fi
+  _InitializeToolset="$toolset_build_proj"
+}
+function ExitWithExitCode {
+  if [[ "$ci" == true && "$prepare_machine" == true ]]; then
+    StopProcesses
+  fi
+  exit $1
+}
+function StopProcesses {
+  echo "Killing running build processes..."
+  pkill -9 "dotnet" || true
+  pkill -9 "vbcscompiler" || true
+  return 0
+}
+function MSBuild {
+  local args=$@
+  if [[ "$pipelines_log" == true ]]; then
+    InitializeBuildTool
+    InitializeToolset
+    if [[ "$ci" == true ]]; then
+      export NUGET_PLUGIN_HANDSHAKE_TIMEOUT_IN_SECONDS=20
+      export NUGET_PLUGIN_REQUEST_TIMEOUT_IN_SECONDS=20
+      Write-PipelineSetVariable -name "NUGET_PLUGIN_HANDSHAKE_TIMEOUT_IN_SECONDS" -value "20"
+      Write-PipelineSetVariable -name "NUGET_PLUGIN_REQUEST_TIMEOUT_IN_SECONDS" -value "20"
+    fi
+    local toolset_dir="${_InitializeToolset%/*}"
+    local selectedPath=
+    local possiblePaths=()
+    possiblePaths+=( "$toolset_dir/$_InitializeBuildToolFramework/Microsoft.DotNet.ArcadeLogging.dll" )
+    possiblePaths+=( "$toolset_dir/$_InitializeBuildToolFramework/Microsoft.DotNet.Arcade.Sdk.dll" )
+    possiblePaths+=( "$toolset_dir/netcoreapp2.1/Microsoft.DotNet.ArcadeLogging.dll" )
+    possiblePaths+=( "$toolset_dir/netcoreapp2.1/Microsoft.DotNet.Arcade.Sdk.dll" )
+    possiblePaths+=( "$toolset_dir/netcoreapp3.1/Microsoft.DotNet.ArcadeLogging.dll" )
+    possiblePaths+=( "$toolset_dir/netcoreapp3.1/Microsoft.DotNet.Arcade.Sdk.dll" )
+    possiblePaths+=( "$toolset_dir/net7.0/Microsoft.DotNet.ArcadeLogging.dll" )
+    possiblePaths+=( "$toolset_dir/net7.0/Microsoft.DotNet.Arcade.Sdk.dll" )
+    for path in "${possiblePaths[@]}"; do
+      if [[ -f $path ]]; then
+        selectedPath=$path
+        break
+      fi
+    done
+    if [[ -z "$selectedPath" ]]; then
+      Write-PipelineTelemetryError -category 'Build'  "Unable to find arcade sdk logger assembly."
+      ExitWithExitCode 1
+    fi
+    args+=( "-logger:$selectedPath" )
+  fi
+  MSBuild-Core ${args[@]}
+}
+function MSBuild-Core {
+  if [[ "$ci" == true ]]; then
+    if [[ "$binary_log" != true && "$exclude_ci_binary_log" != true ]]; then
+      Write-PipelineTelemetryError -category 'Build'  "Binary log must be enabled in CI build, or explicitly opted-out from with the -noBinaryLog switch."
+      ExitWithExitCode 1
+    fi
+    if [[ "$node_reuse" == true ]]; then
+      Write-PipelineTelemetryError -category 'Build'  "Node reuse must be disabled in CI build."
+      ExitWithExitCode 1
+    fi
+  fi
+  InitializeBuildTool
+  local warnaserror_switch=""
+  if [[ $warn_as_error == true ]]; then
+    warnaserror_switch="/warnaserror"
+  fi
+  function RunBuildTool {
+    export ARCADE_BUILD_TOOL_COMMAND="$_InitializeBuildTool $@"
+    "$_InitializeBuildTool" "$@" || {
+      local exit_code=$?
+      echo "Build failed with exit code $exit_code. Check errors above."
+      if [[ "$ci" == "true" && -n ${SYSTEM_TEAMPROJECT:-} ]]; then
+        Write-PipelineSetResult -result "Failed" -message "msbuild execution failed."
+        ExitWithExitCode 0
+      else
+        ExitWithExitCode $exit_code
+      fi
+    }
+  }
+  RunBuildTool "$_InitializeBuildToolCommand" /m /nologo /clp:Summary /v:$verbosity /nr:$node_reuse $warnaserror_switch /p:TreatWarningsAsErrors=$warn_as_error /p:ContinuousIntegrationBuild=$ci "$@"
+}
+function GetDarc {
+    darc_path="$temp_dir/darc"
+    version="$1"
+    if [[ -n "$version" ]]; then
+      version="--darcversion $version"
+    fi
+    "$eng_root/common/darc-init.sh" --toolpath "$darc_path" $version
+}
+ResolvePath "${BASH_SOURCE[0]}"
+_script_dir=`dirname "$_ResolvePath"`
+. "$_script_dir/pipeline-logging-functions.sh"
+eng_root=`cd -P "$_script_dir/.." && pwd`
+repo_root=`cd -P "$_script_dir/../.." && pwd`
+repo_root="${repo_root}/"
+artifacts_dir="${repo_root}artifacts"
+toolset_dir="$artifacts_dir/toolset"
+tools_dir="${repo_root}.tools"
+log_dir="$artifacts_dir/log/$configuration"
+temp_dir="$artifacts_dir/tmp/$configuration"
+global_json_file="${repo_root}global.json"
+global_json_has_runtimes=false
+if command -v jq &> /dev/null; then
+  if jq -e '.tools | has("runtimes")' "$global_json_file" &> /dev/null; then
+    global_json_has_runtimes=true
+  fi
+elif [[ "$(cat "$global_json_file")" =~ \"runtimes\"[[:space:]\:]*\{ ]]; then
+  global_json_has_runtimes=true
+fi
+if [[ -z $HOME ]]; then
+  export HOME="${repo_root}artifacts/.home/"
+  mkdir -p "$HOME"
+fi
+mkdir -p "$toolset_dir"
+mkdir -p "$temp_dir"
+mkdir -p "$log_dir"
+Write-PipelineSetVariable -name "Artifacts" -value "$artifacts_dir"
+Write-PipelineSetVariable -name "Artifacts.Toolset" -value "$toolset_dir"
+Write-PipelineSetVariable -name "Artifacts.Log" -value "$log_dir"
+Write-PipelineSetVariable -name "Temp" -value "$temp_dir"
+Write-PipelineSetVariable -name "TMP" -value "$temp_dir"
+if [ -z "${disable_configure_toolset_import:-}" ]; then
+  configure_toolset_script="$eng_root/configure-toolset.sh"
+  if [[ -a "$configure_toolset_script" ]]; then
+    . "$configure_toolset_script"
+  fi
+fi
+if [[ -n "${useInstalledDotNetCli:-}" ]]; then
+  use_installed_dotnet_cli="$useInstalledDotNetCli"
+fi

--- a//dev/null
+++ b/eng/formatting/download-tools.sh
@@ -0,0 +1,40 @@
+set -ue
+source="${BASH_SOURCE[0]}"
+while [[ -h "$source" ]]; do
+  scriptroot="$( cd -P "$( dirname "$source" )" && pwd )"
+  source="$(readlink "$source")"
+  [[ $source != /* ]] && source="$scriptroot/$source"
+done
+scriptroot="$( cd -P "$( dirname "$source" )" && pwd )"
+function DownloadClangTool {
+    targetPlatform=$(dotnet --info |grep RID:)
+    targetPlatform=${targetPlatform##*RID:* }
+    echo "dotnet RID: ${targetPlatform}"
+    case $targetPlatform in
+        osx.*-x64)
+            targetPlatform=osx.10.15-x64
+            ;;
+        ubuntu.*-x64)
+            targetPlatform=ubuntu.18.04-x64
+            ;;
+    esac
+    toolUrl=https://clrjit.blob.core.windows.net/clang-tools/${targetPlatform}/$1
+    toolOutput=$2/$1
+    echo "Downloading $1 from ${toolUrl} to ${toolOutput}"
+    if [[ ! -x "$toolOutput" ]]; then
+        curl --silent --retry 5 --fail -o "${toolOutput}" "$toolUrl"
+        chmod 751 $toolOutput
+    fi
+    if [[ ! -x "$toolOutput" ]]; then
+        echo "Failed to download $1"
+        exit 1
+    fi
+}
+engFolder="$(cd -P "$( dirname "$scriptroot" )" && pwd )"
+downloadPathFolder="$(cd -P "$( dirname "$engFolder" )" && pwd )/artifacts/tools"
+mkdir -p "$downloadPathFolder"
+. "$scriptroot/../common/tools.sh"
+InitializeDotNetCli true
+DownloadClangTool "clang-format" "$downloadPathFolder"
+DownloadClangTool "clang-tidy" "$downloadPathFolder"
+export PATH=$downloadPathFolder:$PATH

--- a//dev/null
+++ b/eng/install-native-dependencies.sh
@@ -0,0 +1,30 @@
+set -e
+os="$(echo "$1" | tr "[:upper:]" "[:lower:]")"
+if [ -z "$os" ]; then
+    . "$(dirname "$0")"/native/init-os-and-arch.sh
+fi
+case "$os" in
+    linux)
+        if [ -e /etc/os-release ]; then
+            . /etc/os-release
+        fi
+        if [ "$ID" != "debian" ] && [ "$ID_LIKE" != "debian" ]; then
+            echo "Unsupported distro. distro: $ID"
+            exit 1
+        fi
+        apt update
+        apt install -y build-essential gettext locales cmake llvm clang lldb liblldb-dev libunwind8-dev libicu-dev liblttng-ust-dev \
+            libssl-dev libkrb5-dev zlib1g-dev
+        localedef -i en_US -c -f UTF-8 -A /usr/share/locale/locale.alias en_US.UTF-8
+        ;;
+    osx|maccatalyst|ios|iossimulator|tvos|tvossimulator)
+        echo "Installed xcode version: $(xcode-select -p)"
+        export HOMEBREW_NO_INSTALL_CLEANUP=1
+        export HOMEBREW_NO_INSTALLED_DEPENDENTS_CHECK=1
+        brew bundle --no-upgrade --no-lock --file "$(dirname "$0")/Brewfile"
+        ;;
+    *)
+        echo "Unsupported platform. OS: $os"
+        exit 1
+        ;;
+esac

--- a//dev/null
+++ b/eng/native/build-commons.sh
@@ -0,0 +1,455 @@
+initTargetDistroRid()
+{
+    source "$__RepoRootDir/eng/native/init-distro-rid.sh"
+    local passedRootfsDir=""
+    if [[ "$__CrossBuild" == 1 && "$platform" != "darwin" ]]; then
+        passedRootfsDir="$ROOTFS_DIR"
+    fi
+    initDistroRidGlobal "$__TargetOS" "$__TargetArch" "$__PortableBuild" "$passedRootfsDir"
+}
+setup_dirs()
+{
+    echo Setting up directories for build
+    mkdir -p "$__RootBinDir"
+    mkdir -p "$__BinDir"
+    mkdir -p "$__IntermediatesDir"
+}
+check_prereqs()
+{
+    echo "Checking prerequisites..."
+    if [[ "$__HostOS" == "osx" ]]; then
+        command -v pkg-config 2>/dev/null || { echo >&2 "Please install pkg-config before running this script, see https://github.com/dotnet/runtime/blob/main/docs/workflow/requirements/macos-requirements.md"; exit 1; }
+        if ! pkg-config openssl ; then
+            export PKG_CONFIG_PATH=$(brew --prefix)/opt/openssl@3/lib/pkgconfig:$(brew --prefix)/opt/openssl@1.1/lib/pkgconfig:$(brew --prefix)/opt/openssl/lib/pkgconfig
+            pkg-config openssl || { echo >&2 "Please install openssl before running this script, see https://github.com/dotnet/runtime/blob/main/docs/workflow/requirements/macos-requirements.md"; exit 1; }
+        fi
+    fi
+    if [[ "$__UseNinja" == 1 ]]; then
+        command -v ninja 2>/dev/null || command -v ninja-build 2>/dev/null || { echo "Unable to locate ninja!"; exit 1; }
+    fi
+}
+build_native()
+{
+    if [[ ! -e "$__RepoRootDir/artifacts/obj/_version.c" ]]; then
+        eval "$__RepoRootDir/eng/native/version/copy_version_files.sh"
+    fi
+    targetOS="$1"
+    hostArch="$2"
+    cmakeDir="$3"
+    intermediatesDir="$4"
+    target="$5"
+    cmakeArgs="$6"
+    message="$7"
+    echo "Commencing build of \"$target\" target in \"$message\" for $__TargetOS.$__TargetArch.$__BuildType in $intermediatesDir"
+    if [[ "$targetOS" == osx || "$targetOS" == maccatalyst ]]; then
+        if [[ "$hostArch" == x64 ]]; then
+            cmakeArgs="-DCMAKE_OSX_ARCHITECTURES=\"x86_64\" $cmakeArgs"
+        elif [[ "$hostArch" == arm64 ]]; then
+            cmakeArgs="-DCMAKE_OSX_ARCHITECTURES=\"arm64\" $cmakeArgs"
+        else
+            echo "Error: Unknown OSX architecture $hostArch."
+            exit 1
+        fi
+    fi
+    if [[ "$targetOS" == maccatalyst ]]; then
+        cmakeArgs="-DCMAKE_SYSTEM_VARIANT=maccatalyst $cmakeArgs"
+    fi
+    if [[ "$targetOS" == android || "$targetOS" == linux-bionic ]]; then
+        if [[ -z "$ANDROID_NDK_ROOT" ]]; then
+            echo "Error: You need to set the ANDROID_NDK_ROOT environment variable pointing to the Android NDK root."
+            exit 1
+        fi
+        cmakeArgs="-C $__RepoRootDir/eng/native/tryrun.cmake $cmakeArgs"
+        cmakeArgs="-DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK_ROOT/build/cmake/android.toolchain.cmake -DANDROID_PLATFORM=android-21 $cmakeArgs"
+        __Compiler="default"
+        if [[ "$hostArch" == x64 ]]; then
+            cmakeArgs="-DANDROID_ABI=x86_64 $cmakeArgs"
+        elif [[ "$hostArch" == x86 ]]; then
+            cmakeArgs="-DANDROID_ABI=x86 $cmakeArgs"
+        elif [[ "$hostArch" == arm64 ]]; then
+            cmakeArgs="-DANDROID_ABI=arm64-v8a $cmakeArgs"
+        elif [[ "$hostArch" == arm ]]; then
+            cmakeArgs="-DANDROID_ABI=armeabi-v7a $cmakeArgs"
+        else
+            echo "Error: Unknown Android architecture $hostArch."
+            exit 1
+        fi
+    elif [[ "$targetOS" == iossimulator ]]; then
+        cmakeArgs="-C $__RepoRootDir/eng/native/tryrun_ios_tvos.cmake $cmakeArgs"
+        cmakeArgs="-DCMAKE_SYSTEM_NAME=iOS -DCMAKE_OSX_SYSROOT=iphonesimulator -DCMAKE_OSX_DEPLOYMENT_TARGET=11.0 $cmakeArgs"
+        if [[ "$__TargetArch" == x64 ]]; then
+            cmakeArgs="-DCMAKE_OSX_ARCHITECTURES=\"x86_64\" $cmakeArgs"
+        elif [[ "$__TargetArch" == arm64 ]]; then
+            cmakeArgs="-DCMAKE_OSX_ARCHITECTURES=\"arm64\" $cmakeArgs"
+        else
+            echo "Error: Unknown iOS Simulator architecture $__TargetArch."
+            exit 1
+        fi
+    elif [[ "$targetOS" == ios ]]; then
+        cmakeArgs="-C $__RepoRootDir/eng/native/tryrun_ios_tvos.cmake $cmakeArgs"
+        cmakeArgs="-DCMAKE_SYSTEM_NAME=iOS -DCMAKE_OSX_SYSROOT=iphoneos -DCMAKE_OSX_DEPLOYMENT_TARGET=11.0 $cmakeArgs"
+        if [[ "$__TargetArch" == arm64 ]]; then
+            cmakeArgs="-DCMAKE_OSX_ARCHITECTURES=\"arm64\" $cmakeArgs"
+        else
+            echo "Error: Unknown iOS architecture $__TargetArch."
+            exit 1
+        fi
+    elif [[ "$targetOS" == tvossimulator ]]; then
+        cmakeArgs="-C $__RepoRootDir/eng/native/tryrun_ios_tvos.cmake $cmakeArgs"
+        cmakeArgs="-DCMAKE_SYSTEM_NAME=tvOS -DCMAKE_OSX_SYSROOT=appletvsimulator -DCMAKE_OSX_DEPLOYMENT_TARGET=11.0 $cmakeArgs"
+        if [[ "$__TargetArch" == x64 ]]; then
+            cmakeArgs="-DCMAKE_OSX_ARCHITECTURES=\"x86_64\" $cmakeArgs"
+        elif [[ "$__TargetArch" == arm64 ]]; then
+            cmakeArgs="-DCMAKE_OSX_ARCHITECTURES=\"arm64\" $cmakeArgs"
+        else
+            echo "Error: Unknown tvOS Simulator architecture $__TargetArch."
+            exit 1
+        fi
+    elif [[ "$targetOS" == tvos ]]; then
+        cmakeArgs="-C $__RepoRootDir/eng/native/tryrun_ios_tvos.cmake $cmakeArgs"
+        cmakeArgs="-DCMAKE_SYSTEM_NAME=tvOS -DCMAKE_OSX_SYSROOT=appletvos -DCMAKE_OSX_DEPLOYMENT_TARGET=11.0 $cmakeArgs"
+        if [[ "$__TargetArch" == arm64 ]]; then
+            cmakeArgs="-DCMAKE_OSX_ARCHITECTURES=\"arm64\" $cmakeArgs"
+        else
+            echo "Error: Unknown tvOS architecture $__TargetArch."
+            exit 1
+        fi
+    fi
+    if [[ "$__UseNinja" == 1 ]]; then
+        generator="ninja"
+        buildTool="$(command -v ninja || command -v ninja-build)"
+    else
+        buildTool="make"
+    fi
+    if [[ "$__SkipConfigure" == 0 ]]; then
+        if [[ "$__StaticAnalyzer" == 1 ]]; then
+            scan_build=scan-build
+        fi
+        nextCommand="\"$__RepoRootDir/eng/native/gen-buildsys.sh\" \"$cmakeDir\" \"$intermediatesDir\" $hostArch $targetOS $__Compiler $__BuildType \"$generator\" $scan_build $cmakeArgs"
+        echo "Invoking $nextCommand"
+        eval $nextCommand
+        local exit_code="$?"
+        if [[ "$exit_code" != 0  ]]; then
+            echo "${__ErrMsgPrefix}Failed to generate \"$message\" build project!"
+            exit "$exit_code"
+        fi
+    fi
+    if [[ ! -f "$intermediatesDir/CMakeCache.txt" ]]; then
+        echo "${__ErrMsgPrefix}Unable to find generated build files for \"$message\" project!"
+        exit 1
+    fi
+    if [[ "$__ConfigureOnly" == 1 ]]; then
+        echo "Finish configuration & skipping \"$message\" build."
+        return
+    fi
+    SAVED_CFLAGS="${CFLAGS}"
+    SAVED_CXXFLAGS="${CXXFLAGS}"
+    SAVED_LDFLAGS="${LDFLAGS}"
+    export CFLAGS="${CFLAGS} ${EXTRA_CFLAGS}"
+    export CXXFLAGS="${CXXFLAGS} ${EXTRA_CXXFLAGS}"
+    export LDFLAGS="${LDFLAGS} ${EXTRA_LDFLAGS}"
+    local exit_code
+    if [[ "$__StaticAnalyzer" == 1 ]]; then
+        pushd "$intermediatesDir"
+        buildTool="$SCAN_BUILD_COMMAND -o $__BinDir/scan-build-log $buildTool"
+        echo "Executing $buildTool $target -j $__NumProc"
+        "$buildTool" $target -j "$__NumProc"
+        exit_code="$?"
+        popd
+    else
+        cmake_command=cmake
+        if [[ "$build_arch" == "wasm" && "$__TargetOS" == "browser" ]]; then
+            cmake_command="emcmake cmake"
+            echo "Executing $cmake_command --build \"$intermediatesDir\" --target $target -- -j $__NumProc"
+            $cmake_command --build "$intermediatesDir" --target $target -- -j "$__NumProc"
+            exit_code="$?"
+        else
+            pushd "$intermediatesDir"
+            echo "Executing $buildTool $target -j $__NumProc"
+            "$buildTool" $target -j "$__NumProc"
+            exit_code="$?"
+            popd
+        fi
+    fi
+    CFLAGS="${SAVED_CFLAGS}"
+    CXXFLAGS="${SAVED_CXXFLAGS}"
+    LDFLAGS="${SAVED_LDFLAGS}"
+    if [[ "$exit_code" != 0 ]]; then
+        echo "${__ErrMsgPrefix}Failed to build \"$message\"."
+        exit "$exit_code"
+    fi
+}
+usage()
+{
+    echo "Usage: $0 <options>"
+    echo ""
+    echo "Common Options:"
+    echo ""
+    echo "BuildArch can be: -arm, -armv6, -armel, -arm64, -loongarch64, -riscv64, -s390x, -ppc64le, x64, x86, -wasm"
+    echo "BuildType can be: -debug, -checked, -release"
+    echo "-os: target OS (defaults to running OS)"
+    echo "-bindir: output directory (defaults to $__ProjectRoot/artifacts)"
+    echo "-ci: indicates if this is a CI build."
+    echo "-clang: optional argument to build using clang in PATH (default)."
+    echo "-clangx.y: optional argument to build using clang version x.y."
+    echo "-cmakeargs: user-settable additional arguments passed to CMake."
+    echo "-configureonly: do not perform any builds; just configure the build."
+    echo "-cross: optional argument to signify cross compilation,"
+    echo "        will use ROOTFS_DIR environment variable if set."
+    echo "-gcc: optional argument to build using gcc in PATH."
+    echo "-gccx.y: optional argument to build using gcc version x.y."
+    echo "-ninja: target ninja instead of GNU make"
+    echo "-numproc: set the number of build processes."
+    echo "-outputrid: optional argument that overrides the target rid name."
+    echo "-portablebuild: pass -portablebuild=false to force a non-portable build."
+    echo "-skipconfigure: skip build configuration."
+    echo "-keepnativesymbols: keep native/unmanaged debug symbols."
+    echo "-fsanitize: Enable native sanitizers"
+    echo "-verbose: optional argument to enable verbose build output."
+    echo ""
+    echo "Additional Options:"
+    echo ""
+    for i in "${!usage_list[@]}"; do
+        echo "${usage_list[${i}]}"
+    done
+    echo ""
+    exit 1
+}
+source "$__RepoRootDir/eng/native/init-os-and-arch.sh"
+__TargetArch=$arch
+__TargetOS=$os
+__OutputRid=''
+platform="$(uname -s | tr '[:upper:]' '[:lower:]')"
+if [[ "$platform" == "freebsd" ]]; then
+  __NumProc="$(($(sysctl -n hw.ncpu)+1))"
+elif [[ "$platform" == "netbsd" || "$platform" == "sunos" ]]; then
+  __NumProc="$(($(getconf NPROCESSORS_ONLN)+1))"
+elif [[ "$platform" == "darwin" ]]; then
+  __NumProc="$(($(getconf _NPROCESSORS_ONLN)+1))"
+elif command -v nproc > /dev/null 2>&1; then
+  __NumProc="$(nproc)"
+elif (NAME=""; . /etc/os-release; test "$NAME" = "Tizen"); then
+  __NumProc="$(getconf _NPROCESSORS_ONLN)"
+else
+  __NumProc=1
+fi
+while :; do
+    if [[ "$#" -le 0 ]]; then
+        break
+    fi
+    lowerI="$(echo "${1/--/-}" | tr "[:upper:]" "[:lower:]")"
+    case "$lowerI" in
+        -\?|-h|--help)
+            usage
+            exit 1
+            ;;
+        arm|-arm)
+            __TargetArch=arm
+            ;;
+        armv6|-armv6)
+            __TargetArch=armv6
+            ;;
+        arm64|-arm64)
+            __TargetArch=arm64
+            ;;
+        armel|-armel)
+            __TargetArch=armel
+            ;;
+        bindir|-bindir)
+            if [[ -n "$2" ]]; then
+                __RootBinDir="$2"
+                if [[ ! -d "$__RootBinDir" ]]; then
+                    mkdir "$__RootBinDir"
+                fi
+                __RootBinParent=$(dirname "$__RootBinDir")
+                __RootBinName="${__RootBinDir##*/}"
+                __RootBinDir="$(cd "$__RootBinParent" &>/dev/null && printf %s/%s "$PWD" "$__RootBinName")"
+                shift
+            else
+                echo "ERROR: 'bindir' requires a non-empty option argument"
+                exit 1
+            fi
+            ;;
+        checked|-checked)
+            __BuildType=Checked
+            ;;
+        ci|-ci)
+            __ArcadeScriptArgs="--ci"
+            __ErrMsgPrefix="##vso[task.logissue type=error]"
+            ;;
+        clang*|-clang*)
+            __Compiler="$lowerI"
+            ;;
+        cmakeargs|-cmakeargs)
+            if [[ -n "$2" ]]; then
+                __CMakeArgs="$2 $__CMakeArgs"
+                shift
+            else
+                echo "ERROR: 'cmakeargs' requires a non-empty option argument"
+                exit 1
+            fi
+            ;;
+        configureonly|-configureonly)
+            __ConfigureOnly=1
+            __SkipMSCorLib=1
+            __SkipNuget=1
+            ;;
+        cross|-cross)
+            __CrossBuild=1
+            ;;
+        debug|-debug)
+            __BuildType=Debug
+            ;;
+        gcc*|-gcc*)
+            __Compiler="$lowerI"
+            ;;
+        keepnativesymbols|-keepnativesymbols)
+            __CMakeArgs="$__CMakeArgs -DCLR_CMAKE_KEEP_NATIVE_SYMBOLS=true"
+            ;;
+        -fsanitize)
+            __CMakeArgs="$__CMakeArgs -DCLR_CMAKE_ENABLE_SANITIZERS=$2"
+            EnableNativeSanitizers=$2
+            shift
+            ;;
+        -fsanitize=*)
+            sanitizers="${lowerI/#-fsanitize=/}" # -fsanitize=address => address
+            __CMakeArgs="$__CMakeArgs -DCLR_CMAKE_ENABLE_SANITIZERS=$sanitizers"
+            EnableNativeSanitizers=$sanitizers
+            ;;
+        ninja|-ninja)
+            __UseNinja=1
+            ;;
+        numproc|-numproc)
+            if [[ -n "$2" ]]; then
+              __NumProc="$2"
+              shift
+            else
+              echo "ERROR: 'numproc' requires a non-empty option argument"
+              exit 1
+            fi
+            ;;
+        portablebuild=false|-portablebuild=false)
+            __PortableBuild=0
+            ;;
+        release|-release)
+            __BuildType=Release
+            ;;
+        skipconfigure|-skipconfigure)
+            __SkipConfigure=1
+            ;;
+        verbose|-verbose)
+            __VerboseBuild=1
+            ;;
+        x86|-x86)
+            __TargetArch=x86
+            ;;
+        x64|-x64)
+            __TargetArch=x64
+            ;;
+        loongarch64|-loongarch64)
+            __TargetArch=loongarch64
+            ;;
+        riscv64|-riscv64)
+            __TargetArch=riscv64
+            ;;
+        s390x|-s390x)
+            __TargetArch=s390x
+            ;;
+        wasm|-wasm)
+            __TargetArch=wasm
+            ;;
+        outputrid|-outputrid)
+            if [[ -n "$2" ]]; then
+                __OutputRid="$2"
+                shift
+            else
+                echo "ERROR: 'outputrid' requires a non-empty option argument"
+                exit 1
+            fi
+            ;;
+        ppc64le|-ppc64le)
+            __TargetArch=ppc64le
+            ;;
+        os|-os)
+            if [[ -n "$2" ]]; then
+                __TargetOS=$(echo "$2" | tr '[:upper:]' '[:lower:]')
+                shift
+            else
+                echo "ERROR: 'os' requires a non-empty option argument"
+                exit 1
+            fi
+            ;;
+        hostarch|-hostarch)
+            if [[ -n "$2" ]]; then
+                __HostArch="$2"
+                __ExplicitHostArch=1
+                shift
+            else
+                echo "ERROR: 'hostarch' requires a non-empty option argument"
+                exit 1
+            fi
+            ;;
+        hostos|-hostos)
+            if [[ -n "$2" ]]; then
+                __HostOS="$2"
+                shift
+            else
+                echo "ERROR: 'hostos' requires a non-empty option argument"
+                exit 1
+            fi
+            ;;
+        *)
+            handle_arguments "$1" "$2"
+            if [[ "$__ShiftArgs" == 1 ]]; then
+                shift
+                __ShiftArgs=0
+            fi
+            ;;
+    esac
+    shift
+done
+if [[ -z "$__HostArch" ]]; then
+    __HostArch=$__TargetArch
+fi
+if [[ -z "$__HostOS" ]]; then
+    __HostOS=$__TargetOS
+fi
+__CommonMSBuildArgs="/p:TargetArchitecture=$__TargetArch /p:Configuration=$__BuildType /p:TargetOS=$__TargetOS /nodeReuse:false $__OfficialBuildIdArg $__SignTypeArg $__SkipRestoreArg"
+if [[ "$__VerboseBuild" == 1 ]]; then
+    VERBOSE=1
+    export VERBOSE
+    __CommonMSBuildArgs="$__CommonMSBuildArgs /v:detailed"
+fi
+if [[ "$__PortableBuild" == 0 ]]; then
+    __CommonMSBuildArgs="$__CommonMSBuildArgs /p:PortableBuild=false"
+fi
+if [[ "$__TargetArch" == wasm ]]; then
+    true
+elif [[ "$__TargetOS" == ios || "$__TargetOS" == iossimulator ]]; then
+    true
+elif [[ "$__TargetOS" == tvos || "$__TargetOS" == tvossimulator ]]; then
+    true
+elif [[ "$__TargetOS" == android ]]; then
+    true
+else
+    __CMakeArgs="-DFEATURE_DISTRO_AGNOSTIC_SSL=$__PortableBuild $__CMakeArgs"
+fi
+if [[ "$__CrossBuild" == 1 ]]; then
+    CROSSCOMPILE=1
+    export CROSSCOMPILE
+    if [[ -z "$ROOTFS_DIR" && "$platform" != "darwin" ]]; then
+        ROOTFS_DIR="$__RepoRootDir/.tools/rootfs/$__TargetArch"
+        export ROOTFS_DIR
+    fi
+fi
+initTargetDistroRid
+if [ -z "$__OutputRid" ]; then
+    if [[ "$__PortableBuild" == 0 ]]; then
+        __OutputRid="$__DistroRid"
+    else
+        __OutputRid="$__PortableTargetOS-$__TargetArch"
+    fi
+fi
+export __OutputRid
+echo "__OutputRid: ${__OutputRid}"
+__HostFallbackOS="${__OutputRid%-*}" # Strip architecture

--- a//dev/null
+++ b/eng/native/gen-buildsys.sh
@@ -0,0 +1,93 @@
+scriptroot="$( cd -P "$( dirname "$0" )" && pwd )"
+if [[ "$#" -lt 4 ]]; then
+  echo "Usage..."
+  echo "gen-buildsys.sh <path to top level CMakeLists.txt> <path to intermediate directory> <Architecture> <Os> <compiler> [build flavor] [ninja] [scan-build] [cmakeargs]"
+  echo "Specify the path to the top level CMake file."
+  echo "Specify the path that the build system files are generated in."
+  echo "Specify the host architecture (the architecture the built tools should run on)."
+  echo "Specify the name of compiler (clang or gcc)."
+  echo "Optionally specify the build configuration (flavor.) Defaults to DEBUG."
+  echo "Optionally specify 'scan-build' to enable build with clang static analyzer."
+  echo "Use the Ninja generator instead of the Unix Makefiles generator."
+  echo "Pass additional arguments to CMake call."
+  exit 1
+fi
+host_arch="$3"
+target_os="$4"
+compiler="$5"
+if [[ "$compiler" != "default" ]]; then
+    nativescriptroot="$( cd -P "$scriptroot/../common/native" && pwd )"
+    build_arch="$host_arch" compiler="$compiler" . "$nativescriptroot/init-compiler.sh"
+    CCC_CC="$CC"
+    CCC_CXX="$CXX"
+fi
+export CCC_CC CCC_CXX
+buildtype=DEBUG
+code_coverage=OFF
+build_tests=OFF
+scan_build=OFF
+generator="Unix Makefiles"
+__UnprocessedCMakeArgs=""
+for i in "${@:6}"; do
+    upperI="$(echo "$i" | tr "[:lower:]" "[:upper:]")"
+    case "$upperI" in
+      DEBUG | CHECKED | RELEASE | RELWITHDEBINFO)
+      buildtype="$upperI"
+      ;;
+      NINJA)
+      generator=Ninja
+      ;;
+      SCAN-BUILD)
+      echo "Static analysis is turned on for this build."
+      scan_build=ON
+      ;;
+      *)
+      __UnprocessedCMakeArgs="${__UnprocessedCMakeArgs}${__UnprocessedCMakeArgs:+ }$i"
+    esac
+done
+cmake_extra_defines=
+if [[ "$CROSSCOMPILE" == "1" ]]; then
+    platform="$(uname -s | tr '[:upper:]' '[:lower:]')"
+    if ! [[ -n "$ROOTFS_DIR" || "$platform" == "darwin" ]]; then
+        echo "ROOTFS_DIR not set for crosscompile"
+        exit 1
+    fi
+    TARGET_BUILD_ARCH="$host_arch"
+    export TARGET_BUILD_ARCH
+    cmake_extra_defines="$cmake_extra_defines -C $scriptroot/tryrun.cmake"
+    if [[ "$platform" == "darwin" ]]; then
+        cmake_extra_defines="$cmake_extra_defines -DCMAKE_SYSTEM_NAME=Darwin"
+    else
+        cmake_extra_defines="$cmake_extra_defines -DCMAKE_TOOLCHAIN_FILE=$scriptroot/../common/cross/toolchain.cmake"
+    fi
+fi
+if [[ "$host_arch" == "armel" ]]; then
+    cmake_extra_defines="$cmake_extra_defines -DARM_SOFTFP=1"
+fi
+if ! cmake_command=$(command -v cmake); then
+    echo "CMake was not found in PATH."
+    exit 1
+fi
+if [[ "$scan_build" == "ON" && -n "$SCAN_BUILD_COMMAND" ]]; then
+    cmake_command="$SCAN_BUILD_COMMAND $cmake_command"
+fi
+if [[ "$host_arch" == "wasm" ]]; then
+    if [[ "$target_os" == "browser" ]]; then
+        cmake_command="emcmake $cmake_command"
+    elif [[ "$target_os" == "wasi" ]]; then
+        true
+    else
+        echo "target_os was not specified"
+        exit 1
+    fi
+fi
+pushd "$2"
+$cmake_command \
+  --no-warn-unused-cli \
+  -G "$generator" \
+  "-DCMAKE_BUILD_TYPE=$buildtype" \
+  "-DCMAKE_INSTALL_PREFIX=$__CMakeBinDir" \
+  $cmake_extra_defines \
+  $__UnprocessedCMakeArgs \
+  "$1"
+popd

--- a//dev/null
+++ b/eng/native/genmoduleindex.sh
@@ -0,0 +1,30 @@
+set -euo pipefail
+if [[ "$#" -lt 2 ]]; then
+  echo "Usage: genmoduleindex.sh ModuleBinaryFile IndexHeaderFile readelfBinaryPath"
+  exit 1
+fi
+function printIdAsBinary() {
+  id="$1"
+  bytesLength="${#id}"
+  printf "0x%02x, " "$((bytesLength/2))"
+  while [[ "$id" ]]; do
+    printf '0x%s, ' "${id:0:2}"
+    id=${id:2}
+  done
+}
+case "$(uname -s)" in
+Darwin)
+  cmd="dwarfdump"
+  arg="-u"
+  pattern='^UUID: ([0-9A-Fa-f\-]+)';;
+*)
+  cmd="$3"
+  arg="-n"
+  pattern='^[[:space:]]*Build ID: ([0-9A-Fa-f\-]+)';;
+esac
+"$cmd" "$arg" "$1" | while read -r line; do
+  if [[ "$line" =~ $pattern ]]; then
+    printIdAsBinary "${BASH_REMATCH[1]//-/}"
+    break
+  fi
+done > "$2"

--- a/src/coreclr/debug/daccess/dacdbiimpl.cpp
+++ b//dev/null
@@ -1,4817 +0,0 @@
-#include "stdafx.h"
-#include "dacdbiinterface.h"
-#include "typestring.h"
-#include "holder.h"
-#include "debuginfostore.h"
-#include "peimagelayout.inl"
-#include "encee.h"
-#include "switches.h"
-#include "generics.h"
-#include "stackwalk.h"
-#include "virtualcallstub.h"
-#include "dacdbiimpl.h"
-#ifdef FEATURE_COMINTEROP
-#include "runtimecallablewrapper.h"
-#include "comcallablewrapper.h"
-#endif // FEATURE_COMINTEROP
-#include "request_common.h"
-IDacDbiInterface::IAllocator * g_pAllocator = NULL;
-forDbiWorker forDbi;
-void * operator new(size_t lenBytes, const forDbiWorker &)
-{
-    _ASSERTE(g_pAllocator != NULL);
-    void *result = g_pAllocator->Alloc(lenBytes);
-    if (result == NULL)
-    {
-        ThrowOutOfMemory();
-    }
-    return result;
-}
-void * operator new[](size_t lenBytes, const forDbiWorker &)
-{
-    _ASSERTE(g_pAllocator != NULL);
-    void *result = g_pAllocator->Alloc(lenBytes);
-    if (result == NULL)
-    {
-        ThrowOutOfMemory();
-    }
-    return result;
-}
-void operator delete(void *p, const forDbiWorker &)
-{
-    if (p == NULL)
-    {
-        return;
-    }
-    _ASSERTE(g_pAllocator != NULL);
-    g_pAllocator->Free((BYTE*) p);
-}
-void operator delete[](void *p, const forDbiWorker &)
-{
-    if (p == NULL)
-    {
-        return;
-    }
-    _ASSERTE(g_pAllocator != NULL);
-    g_pAllocator->Free((BYTE*) p);
-}
-template<class T> void DeleteDbiMemory(T *p)
-{
-    if (p == NULL)
-    {
-        return;
-    }
-    p->~T();
-    _ASSERTE(g_pAllocator != NULL);
-    g_pAllocator->Free((BYTE*) p);
-}
-void* AllocDbiMemory(size_t size)
-{
-    void *result;
-    if (g_pAllocator != nullptr)
-    {
-        result = g_pAllocator->Alloc(size);
-    }
-    else
-    {
-        result = new (nothrow) BYTE[size];
-    }
-    if (result == NULL)
-    {
-        ThrowOutOfMemory();
-    }
-    return result;
-}
-void DeleteDbiMemory(void* p)
-{
-    if (p == NULL)
-    {
-        return;
-    }
-    if (g_pAllocator != nullptr)
-    {
-        g_pAllocator->Free((BYTE*)p);
-    }
-    else
-    {
-        ::delete [] (BYTE*)p;
-    }
-}
-template<class T> void DeleteDbiArrayMemory(T *p, int count)
-{
-    if (p == NULL)
-    {
-        return;
-    }
-    for (T *cur = p; cur < p + count; cur++)
-    {
-        cur->~T();
-    }
-    _ASSERTE(g_pAllocator != NULL);
-    g_pAllocator->Free((BYTE*) p);
-}
-STDAPI
-DLLEXPORT
-DacDbiInterfaceInstance(
-    ICorDebugDataTarget * pTarget,
-    CORDB_ADDRESS baseAddress,
-    IDacDbiInterface::IAllocator * pAllocator,
-    IDacDbiInterface::IMetaDataLookup * pMetaDataLookup,
-    IDacDbiInterface ** ppInterface)
-{
-    SUPPORTS_DAC_HOST_ONLY;
-    if ((ppInterface == NULL) || (pTarget == NULL) || (baseAddress == 0))
-    {
-        return E_INVALIDARG;
-    }
-    *ppInterface = NULL;
-    DacDbiInterfaceImpl * pDac = new (nothrow) DacDbiInterfaceImpl(pTarget, baseAddress, pAllocator, pMetaDataLookup);
-    if (!pDac)
-    {
-        return E_OUTOFMEMORY;
-    }
-    HRESULT hrStatus = pDac->Initialize();
-    if (SUCCEEDED(hrStatus))
-    {
-        *ppInterface = pDac;
-    }
-    else
-    {
-        delete pDac;
-    }
-    return hrStatus;
-}
-DacDbiInterfaceImpl::DacDbiInterfaceImpl(
-    ICorDebugDataTarget* pTarget,
-    CORDB_ADDRESS baseAddress,
-    IAllocator * pAllocator,
-    IMetaDataLookup * pMetaDataLookup
-) : ClrDataAccess(pTarget),
-    m_pAllocator(pAllocator),
-    m_pMetaDataLookup(pMetaDataLookup),
-    m_pCachedPEAssembly(VMPTR_PEAssembly::NullPtr()),
-    m_pCachedImporter(NULL),
-    m_isCachedHijackFunctionValid(FALSE)
-{
-    _ASSERTE(baseAddress != NULL);
-    m_globalBase = CORDB_ADDRESS_TO_TADDR(baseAddress);
-    _ASSERTE(pMetaDataLookup != NULL);
-    _ASSERTE(pAllocator != NULL);
-    _ASSERTE(pTarget != NULL);
-#ifdef _DEBUG
-    m_fEnableDllVerificationAsserts = true;
-#endif
-}
-DacDbiInterfaceImpl::~DacDbiInterfaceImpl()
-{
-    SUPPORTS_DAC_HOST_ONLY;
-}
-interface IMDInternalImport* DacDbiInterfaceImpl::GetMDImport(
-    const PEAssembly* pPEAssembly,
-    const ReflectionModule * pReflectionModule,
-    bool fThrowEx)
-{
-    SUPPORTS_DAC;
-    IDacDbiInterface::IMetaDataLookup * pLookup = m_pMetaDataLookup;
-    _ASSERTE(pLookup != NULL);
-    VMPTR_PEAssembly vmPEAssembly = VMPTR_PEAssembly::NullPtr();
-    if (pPEAssembly != NULL)
-    {
-        vmPEAssembly.SetHostPtr(pPEAssembly);
-    }
-    else if (pReflectionModule != NULL)
-    {
-        vmPEAssembly.SetHostPtr(pReflectionModule->GetPEAssembly());
-    }
-    if (m_pCachedPEAssembly == vmPEAssembly)
-    {
-        return m_pCachedImporter;
-    }
-    IMDInternalImport * pInternal = NULL;
-    bool isILMetaDataForNI = false;
-    EX_TRY
-    {
-        pInternal = pLookup->LookupMetaData(vmPEAssembly, isILMetaDataForNI);
-    }
-    EX_CATCH
-    {
-        if ((GET_EXCEPTION()->GetHR() != HRESULT_FROM_WIN32(ERROR_PARTIAL_COPY)) &&
-            (GET_EXCEPTION()->GetHR() != CORDBG_E_READVIRTUAL_FAILURE) &&
-            (GET_EXCEPTION()->GetHR() != CORDBG_E_SYMBOLS_NOT_AVAILABLE) &&
-            (GET_EXCEPTION()->GetHR() != CORDBG_E_MODULE_LOADED_FROM_DISK))
-        {
-            EX_RETHROW;
-        }
-    }
-    EX_END_CATCH(SwallowAllExceptions)
-    if (pInternal == NULL)
-    {
-        SIMPLIFYING_ASSUMPTION(!"MD lookup failed");
-        if (fThrowEx)
-        {
-            ThrowHR(E_FAIL);
-        }
-        return NULL;
-    }
-    else
-    {
-        m_pCachedPEAssembly   = vmPEAssembly;
-        m_pCachedImporter = pInternal;
-    }
-    return pInternal;
-}
-void DacDbiInterfaceImpl::Destroy()
-{
-    m_pAllocator = NULL;
-    this->Release();
-}
-HRESULT DacDbiInterfaceImpl::CheckDbiVersion(const DbiVersion * pVersion)
-{
-    DD_ENTER_MAY_THROW;
-    if (pVersion->m_dwFormat != kCurrentDbiVersionFormat)
-    {
-        return CORDBG_E_INCOMPATIBLE_PROTOCOL;
-    }
-    if ((pVersion->m_dwProtocolBreakingChangeCounter != kCurrentDacDbiProtocolBreakingChangeCounter) ||
-        (pVersion->m_dwReservedMustBeZero1 != 0))
-    {
-        return CORDBG_E_INCOMPATIBLE_PROTOCOL;
-    }
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::FlushCache()
-{
-    DD_NON_REENTRANT_MAY_THROW;
-    m_pCachedPEAssembly = VMPTR_PEAssembly::NullPtr();
-    m_pCachedImporter = NULL;
-    m_isCachedHijackFunctionValid = FALSE;
-    HRESULT hr  = ClrDataAccess::Flush();
-    _ASSERTE(SUCCEEDED(hr));
-    return hr;
-}
-void DacDbiInterfaceImpl::DacSetTargetConsistencyChecks(bool fEnableAsserts)
-{
-    ClrDataAccess::SetTargetConsistencyChecks(fEnableAsserts);
-}
-BOOL DacDbiInterfaceImpl::IsLeftSideInitialized()
-{
-    DD_ENTER_MAY_THROW;
-    if (g_pDebugger != NULL)
-    {
-        return (g_pDebugger->m_fLeftSideInitialized != 0);
-    }
-    return FALSE;
-}
-BOOL DacDbiInterfaceImpl::IsTransitionStub(CORDB_ADDRESS address)
-{
-    DD_ENTER_MAY_THROW;
-    BOOL fIsStub = FALSE;
-#if defined(TARGET_UNIX)
-    ThrowHR(E_NOTIMPL);
-#else // !TARGET_UNIX
-    TADDR ip = (TADDR)address;
-    if (ip == NULL)
-    {
-        fIsStub = FALSE;
-    }
-    else
-    {
-        fIsStub = StubManager::IsStub(ip);
-    }
-    if (fIsStub == FALSE)
-    {
-        fIsStub = IsIPInModule(m_globalBase, ip);
-    }
-#endif // TARGET_UNIX
-    return fIsStub;
-}
-IDacDbiInterface::AddressType DacDbiInterfaceImpl::GetAddressType(CORDB_ADDRESS address)
-{
-    DD_ENTER_MAY_THROW;
-    TADDR taAddr = CORDB_ADDRESS_TO_TADDR(address);
-    if (IsPossibleCodeAddress(taAddr) == S_OK)
-    {
-        if (ExecutionManager::IsManagedCode(taAddr))
-        {
-            return kAddressManagedMethod;
-        }
-        if (StubManager::IsStub(taAddr))
-        {
-            return kAddressRuntimeUnmanagedStub;
-        }
-    }
-    return kAddressUnrecognized;
-}
-VMPTR_AppDomain DacDbiInterfaceImpl::GetAppDomainFromId(ULONG appdomainId)
-{
-    DD_ENTER_MAY_THROW;
-    VMPTR_AppDomain vmAppDomain;
-    IXCLRDataProcess *   pDAC = this;
-    ReleaseHolder<IXCLRDataAppDomain> pDacAppDomain;
-    HRESULT hrStatus = pDAC->GetAppDomainByUniqueID(appdomainId, &pDacAppDomain);
-    IfFailThrow(hrStatus);
-    IXCLRDataAppDomain * pIAppDomain = pDacAppDomain;
-    AppDomain * pAppDomain = (static_cast<ClrDataAppDomain *> (pIAppDomain))->GetAppDomain();
-    SIMPLIFYING_ASSUMPTION(pAppDomain != NULL);
-    if (pAppDomain == NULL)
-    {
-        ThrowHR(E_FAIL); // corrupted left-side?
-    }
-    TADDR addrAppDomain = PTR_HOST_TO_TADDR(pAppDomain);
-    vmAppDomain.SetDacTargetPtr(addrAppDomain);
-    return vmAppDomain;
-}
-ULONG DacDbiInterfaceImpl::GetAppDomainId(VMPTR_AppDomain   vmAppDomain)
-{
-    DD_ENTER_MAY_THROW;
-    if (vmAppDomain.IsNull())
-    {
-        return 0;
-    }
-    else
-    {
-        AppDomain * pAppDomain = vmAppDomain.GetDacPtr();
-        return DefaultADID;
-    }
-}
-VMPTR_OBJECTHANDLE DacDbiInterfaceImpl::GetAppDomainObject(VMPTR_AppDomain vmAppDomain)
-{
-    DD_ENTER_MAY_THROW;
-    AppDomain* pAppDomain = vmAppDomain.GetDacPtr();
-    OBJECTHANDLE hAppDomainManagedObject = pAppDomain->GetRawExposedObjectHandleForDebugger();
-    VMPTR_OBJECTHANDLE vmObj = VMPTR_OBJECTHANDLE::NullPtr();
-    vmObj.SetDacTargetPtr(hAppDomainManagedObject);
-    return vmObj;
-}
-void DacDbiInterfaceImpl::GetAppDomainFullName(
-    VMPTR_AppDomain   vmAppDomain,
-    IStringHolder *   pStrName )
-{
-    DD_ENTER_MAY_THROW;
-    AppDomain * pAppDomain = vmAppDomain.GetDacPtr();
-    bool fIsUtf8;
-    PVOID pRawName = pAppDomain->GetFriendlyNameNoSet(&fIsUtf8);
-    if (!pRawName)
-    {
-        ThrowHR(E_NOINTERFACE);
-    }
-    HRESULT hrStatus = S_OK;
-    if (fIsUtf8)
-    {
-        ULONG32 dwNameLen = 0;
-        hrStatus = ConvertUtf8((LPCUTF8)pRawName, 0, &dwNameLen, NULL);
-        if (SUCCEEDED( hrStatus ))
-        {
-            NewArrayHolder<WCHAR> pwszName(new WCHAR[dwNameLen]);
-            hrStatus = ConvertUtf8((LPCUTF8)pRawName, dwNameLen, &dwNameLen, pwszName );
-            IfFailThrow(hrStatus);
-            hrStatus =  pStrName->AssignCopy(pwszName);
-        }
-    }
-    else
-    {
-        hrStatus =  pStrName->AssignCopy(static_cast<PCWSTR>(pRawName));
-    }
-    IfFailThrow(hrStatus);
-}
-void DacDbiInterfaceImpl::GetCompilerFlags (
-    VMPTR_DomainAssembly vmDomainAssembly,
-    BOOL *pfAllowJITOpts,
-    BOOL *pfEnableEnC)
-{
-    DD_ENTER_MAY_THROW;
-    DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
-    if (pDomainAssembly == NULL)
-    {
-        ThrowHR(E_FAIL);
-    }
-    Module * pModule = pDomainAssembly->GetModule();
-    DWORD dwBits = pModule->GetDebuggerInfoBits();
-    *pfAllowJITOpts = !CORDisableJITOptimizations(dwBits);
-    *pfEnableEnC = pModule->IsEditAndContinueEnabled();
-} //GetCompilerFlags
-bool DacDbiInterfaceImpl::CanSetEnCBits(Module * pModule)
-{
-    _ASSERTE(pModule != NULL);
-#ifdef EnC_SUPPORTED
-    bool fIgnorePdbs = ((pModule->GetDebuggerInfoBits() & DACF_IGNORE_PDBS) != 0);
-    bool fAllowEnc = pModule->IsEditAndContinueCapable() &&
-#ifdef PROFILING_SUPPORTED_DATA
-        !CORProfilerPresent() && // this queries target
-#endif
-        fIgnorePdbs;
-#else   // ! EnC_SUPPORTED
-    bool fAllowEnc = false;
-#endif
-    return fAllowEnc;
-} // DacDbiInterfaceImpl::SetEnCBits
-HRESULT DacDbiInterfaceImpl::SetCompilerFlags(VMPTR_DomainAssembly vmDomainAssembly,
-                                           BOOL             fAllowJitOpts,
-                                           BOOL             fEnableEnC)
-{
-    DD_ENTER_MAY_THROW;
-    DWORD        dwBits      = 0;
-    DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
-    Module *     pModule     = pDomainAssembly->GetModule();
-    HRESULT      hr          = S_OK;
-    _ASSERTE(pModule != NULL);
-    dwBits = (pModule->GetDebuggerInfoBits() & ~(DACF_ALLOW_JIT_OPTS | DACF_ENC_ENABLED));
-    dwBits &= DACF_CONTROL_FLAGS_MASK;
-    if (fAllowJitOpts)
-    {
-        dwBits |= DACF_ALLOW_JIT_OPTS;
-    }
-    if (fEnableEnC)
-    {
-        if (CanSetEnCBits(pModule))
-        {
-            dwBits |= DACF_ENC_ENABLED;
-        }
-        else
-        {
-            hr = CORDBG_S_NOT_ALL_BITS_SET;
-        }
-    }
-    dwBits |= DACF_USER_OVERRIDE;
-    pModule->SetDebuggerInfoBits((DebuggerAssemblyControlFlags)dwBits);
-    LOG((LF_CORDB, LL_INFO100, "D::HIPCE, Changed Jit-Debug-Info: fOpt=%d, fEnableEnC=%d, new bits=0x%08x\n",
-           (dwBits & DACF_ALLOW_JIT_OPTS) != 0,
-           (dwBits & DACF_ENC_ENABLED) != 0,
-            dwBits));
-    _ASSERTE(SUCCEEDED(hr));
-    return hr;
-} // DacDbiInterfaceImpl::SetCompilerFlags
-void DacDbiInterfaceImpl::GetNativeCodeSequencePointsAndVarInfo(VMPTR_MethodDesc  vmMethodDesc,
-                                                                CORDB_ADDRESS     startAddr,
-                                                                BOOL              fCodeAvailable,
-                                                                NativeVarData *   pNativeVarData,
-                                                                SequencePoints *  pSequencePoints)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(!vmMethodDesc.IsNull());
-    MethodDesc * pMD = vmMethodDesc.GetDacPtr();
-    _ASSERTE(fCodeAvailable != 0);
-    GetNativeVarData(pMD, startAddr, GetArgCount(pMD), pNativeVarData);
-    GetSequencePoints(pMD, startAddr, pSequencePoints);
-} // GetNativeCodeSequencePointsAndVarInfo
-SIZE_T DacDbiInterfaceImpl::GetArgCount(MethodDesc * pMD)
-{
-    PCCOR_SIGNATURE pCallSig;
-    DWORD cbCallSigSize;
-    pMD->GetSig(&pCallSig, &cbCallSigSize);
-    if (pCallSig == NULL)
-    {
-        CONSISTENCY_CHECK_MSGF(false, ("Corrupted image, null sig.(%s::%s)",
-                               pMD->m_pszDebugClassName, pMD->m_pszDebugMethodName));
-        return 0;
-    }
-    MetaSig msig(pCallSig, cbCallSigSize, pMD->GetModule(), NULL, MetaSig::sigMember);
-    UINT32 NumArguments = msig.NumFixedArgs();
-    if (!pMD->IsStatic())
-    {
-        NumArguments++;
-    }
-/*
-    SigParser sigParser(pCallSig, cbCallSigSize);
-    sigParser.SkipMethodHeaderSignature(&m_allArgsCount);
-*/
-    return NumArguments;
-} //GetArgCount
-BYTE* InfoStoreNew(void * pData, size_t cBytes)
-{
-    return new BYTE[cBytes];
-}
-void DacDbiInterfaceImpl::GetNativeVarData(MethodDesc *    pMethodDesc,
-                                           CORDB_ADDRESS   startAddr,
-                                           SIZE_T          fixedArgCount,
-                                           NativeVarData * pVarInfo)
-{
-    if (pVarInfo->IsInitialized())
-    {
-        return;
-    }
-    NewArrayHolder<ICorDebugInfo::NativeVarInfo> nativeVars(NULL);
-    DebugInfoRequest request;
-    request.InitFromStartingAddr(pMethodDesc, CORDB_ADDRESS_TO_TADDR(startAddr));
-    ULONG32 entryCount;
-    BOOL success = DebugInfoManager::GetBoundariesAndVars(request,
-                                                InfoStoreNew, NULL, // allocator
-                                                NULL, NULL,
-                                                &entryCount, &nativeVars);
-    if (!success)
-        ThrowHR(E_FAIL);
-    pVarInfo->InitVarDataList(nativeVars, (int)fixedArgCount, (int)entryCount);
-} // GetNativeVarData
-void DacDbiInterfaceImpl::ComposeMapping(const InstrumentedILOffsetMapping * pProfilerILMap, ICorDebugInfo::OffsetMapping nativeMap[], ULONG32* pEntryCount)
-{
-    ULONG32 entryCount = *pEntryCount;
-    if (pProfilerILMap && !pProfilerILMap->IsNull())
-    {
-        ULONG32 cDuplicate = 0;
-        ULONG32 prevILOffset = (ULONG32)(ICorDebugInfo::MAX_ILNUM);
-        for (ULONG32 i = 0; i < entryCount; i++)
-        {
-            ULONG32 origILOffset = TranslateInstrumentedILOffsetToOriginal(nativeMap[i].ilOffset, pProfilerILMap);
-            if (origILOffset == prevILOffset)
-            {
-                nativeMap[i].ilOffset = (ULONG32)(ICorDebugInfo::MAX_ILNUM);
-                cDuplicate += 1;
-            }
-            else
-            {
-                nativeMap[i].ilOffset = origILOffset;
-                prevILOffset = origILOffset;
-            }
-        }
-        ULONG32 realIndex = 0;
-        for (ULONG32 curIndex = 0; curIndex < entryCount; curIndex++)
-        {
-            if (nativeMap[curIndex].ilOffset != (ULONG32)(ICorDebugInfo::MAX_ILNUM))
-            {
-                nativeMap[realIndex] = nativeMap[curIndex];
-                realIndex += 1;
-            }
-        }
-        _ASSERTE((realIndex + cDuplicate) == entryCount);
-        entryCount -= cDuplicate;
-        *pEntryCount = entryCount;
-    }
-}
-void DacDbiInterfaceImpl::GetSequencePoints(MethodDesc *     pMethodDesc,
-                                            CORDB_ADDRESS    startAddr,
-                                            SequencePoints * pSeqPoints)
-{
-    if (pSeqPoints->IsInitialized())
-    {
-        return;
-    }
-    DebugInfoRequest request;
-    request.InitFromStartingAddr(pMethodDesc, CORDB_ADDRESS_TO_TADDR(startAddr));
-    NewArrayHolder<ICorDebugInfo::OffsetMapping> mapCopy(NULL);
-    ULONG32 entryCount;
-    BOOL success = DebugInfoManager::GetBoundariesAndVars(request,
-                                                      InfoStoreNew, NULL, // allocator
-                                                      &entryCount, &mapCopy,
-                                                      NULL, NULL);
-    if (!success)
-        ThrowHR(E_FAIL);
-#ifdef FEATURE_REJIT
-    CodeVersionManager * pCodeVersionManager = pMethodDesc->GetCodeVersionManager();
-    ILCodeVersion ilVersion;
-    NativeCodeVersion nativeCodeVersion = pCodeVersionManager->GetNativeCodeVersion(dac_cast<PTR_MethodDesc>(pMethodDesc), (PCODE)startAddr);
-    if (!nativeCodeVersion.IsNull())
-    {
-        ilVersion = nativeCodeVersion.GetILCodeVersion();
-    }
-    if (!ilVersion.IsNull() && !ilVersion.IsDefaultVersion())
-    {
-        const InstrumentedILOffsetMapping * pRejitMapping = ilVersion.GetInstrumentedILMap();
-        ComposeMapping(pRejitMapping, mapCopy, &entryCount);
-    }
-    else
-    {
-#endif
-        InstrumentedILOffsetMapping loadTimeMapping =
-            pMethodDesc->GetModule()->GetInstrumentedILOffsetMapping(pMethodDesc->GetMemberDef());
-        ComposeMapping(&loadTimeMapping, mapCopy, &entryCount);
-#ifdef FEATURE_REJIT
-    }
-#endif
-    pSeqPoints->InitSequencePoints(entryCount);
-    pSeqPoints->CopyAndSortSequencePoints(mapCopy);
-} // GetSequencePoints
-ULONG DacDbiInterfaceImpl::TranslateInstrumentedILOffsetToOriginal(ULONG                               ilOffset,
-                                                                   const InstrumentedILOffsetMapping * pMapping)
-{
-    SIZE_T               cMap  = pMapping->GetCount();
-    ARRAY_PTR_COR_IL_MAP rgMap = pMapping->GetOffsets();
-    _ASSERTE((cMap == 0) == (rgMap == NULL));
-    if ((cMap == 0) || ((int)ilOffset < 0))
-    {
-        return ilOffset;
-    }
-    SIZE_T i = 0;
-    for (i = 1; i < cMap; i++)
-    {
-        if (ilOffset < rgMap[i].newOffset)
-        {
-            return rgMap[i - 1].oldOffset;
-        }
-    }
-    return rgMap[i - 1].oldOffset;
-}
-void DacDbiInterfaceImpl::GetILCodeAndSig(VMPTR_DomainAssembly vmDomainAssembly,
-                                          mdToken          functionToken,
-                                          TargetBuffer *   pCodeInfo,
-                                          mdToken *        pLocalSigToken)
-{
-    DD_ENTER_MAY_THROW;
-    DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
-    Module *     pModule     = pDomainAssembly->GetModule();
-    RVA          methodRVA   = 0;
-    DWORD        implFlags;
-    pCodeInfo->Clear();
-    *pLocalSigToken = mdSignatureNil;
-    IfFailThrow(pModule->GetMDImport()->GetMethodImplProps(functionToken,
-                                                           &methodRVA,
-                                                           &implFlags));
-    MethodDesc* pMethodDesc =
-        FindLoadedMethodRefOrDef(pModule, functionToken);
-    if (methodRVA == 0)
-    {
-        LOG((LF_CORDB,LL_INFO100000, "DDI::GICAS: Function is not IL - methodRVA == NULL!\n"));
-        if(!pMethodDesc || !pMethodDesc->IsIL())
-        {
-            LOG((LF_CORDB,LL_INFO100000, "DDI::GICAS: And the MD agrees..\n"));
-            ThrowHR(CORDBG_E_FUNCTION_NOT_IL);
-        }
-        else
-        {
-            LOG((LF_CORDB,LL_INFO100000, "DDI::GICAS: But the MD says it's IL..\n"));
-        }
-        if (pMethodDesc != NULL && pMethodDesc->GetRVA() == 0)
-        {
-            LOG((LF_CORDB,LL_INFO100000, "DDI::GICAS: Actually, MD says RVA is 0 too - keep going...!\n"));
-        }
-    }
-    if (IsMiNative(implFlags))
-    {
-        LOG((LF_CORDB,LL_INFO100000, "DDI::GICAS: Function is not IL - IsMiNative!\n"));
-        ThrowHR(CORDBG_E_FUNCTION_NOT_IL);
-    }
-    *pLocalSigToken = GetILCodeAndSigHelper(pModule, pMethodDesc, functionToken, methodRVA, pCodeInfo);
-} // GetILCodeAndSig
-mdSignature DacDbiInterfaceImpl::GetILCodeAndSigHelper(Module *       pModule,
-                                                       MethodDesc *   pMD,
-                                                       mdMethodDef    mdMethodToken,
-                                                       RVA            methodRVA,
-                                                       TargetBuffer * pIL)
-{
-    _ASSERTE(pModule != NULL);
-    _ASSERTE((pMD == NULL) || ((pMD->GetMemberDef() == mdMethodToken) && (pMD->GetRVA() == methodRVA)));
-    TADDR pTargetIL; // target address of start of IL blob
-    pTargetIL = pModule->GetDynamicIL(mdMethodToken, TRUE);
-    if (pTargetIL == 0 && !pModule->IsReflection())
-    {
-        pTargetIL = (TADDR)pModule->GetIL(methodRVA);
-    }
-    mdSignature mdSig = mdSignatureNil;
-    if (pTargetIL == 0)
-    {
-        _ASSERTE(pMD->IsDynamicMethod());
-        _ASSERTE(pMD->AsDynamicMethodDesc()->IsLCGMethod()||
-                 pMD->AsDynamicMethodDesc()->IsILStub());
-        pIL->Clear();
-    }
-    else
-    {
-        COR_ILMETHOD * pHostIL = DacGetIlMethod(pTargetIL);     // host address of start of IL blob
-        COR_ILMETHOD_DECODER header(pHostIL);                   // host address of header
-        pIL->pAddress = pTargetIL + ((SIZE_T)(header.Code) - (SIZE_T)pHostIL);
-        pIL->cbSize = header.GetCodeSize();
-        if (header.LocalVarSigTok != NULL)
-        {
-            mdSig = header.GetLocalVarSigTok();
-        }
-        else
-        {
-            mdSig = mdSignatureNil;
-        }
-    }
-    return mdSig;
-}
-bool DacDbiInterfaceImpl::GetMetaDataFileInfoFromPEFile(VMPTR_PEAssembly vmPEAssembly,
-                                                        DWORD &dwTimeStamp,
-                                                        DWORD &dwSize,
-                                                        bool  &isNGEN,
-                                                        IStringHolder* pStrFilename)
-{
-    DD_ENTER_MAY_THROW;
-    DWORD dwDataSize;
-    DWORD dwRvaHint;
-    PEAssembly * pPEAssembly = vmPEAssembly.GetDacPtr();
-    _ASSERTE(pPEAssembly != NULL);
-    if (pPEAssembly == NULL)
-        return false;
-    WCHAR wszFilePath[MAX_LONGPATH] = {0};
-    DWORD cchFilePath = MAX_LONGPATH;
-    bool ret = ClrDataAccess::GetMetaDataFileInfoFromPEFile(pPEAssembly,
-                                                            dwTimeStamp,
-                                                            dwSize,
-                                                            dwDataSize,
-                                                            dwRvaHint,
-                                                            isNGEN,
-                                                            wszFilePath,
-                                                            cchFilePath);
-    pStrFilename->AssignCopy(wszFilePath);
-    return ret;
-}
-bool DacDbiInterfaceImpl::GetILImageInfoFromNgenPEFile(VMPTR_PEAssembly vmPEAssembly,
-                                                       DWORD &dwTimeStamp,
-                                                       DWORD &dwSize,
-                                                       IStringHolder* pStrFilename)
-{
-    return false;
-}
-void DacDbiInterfaceImpl::GetMethodRegionInfo(MethodDesc *             pMethodDesc,
-                                              NativeCodeFunctionData * pCodeInfo)
-{
-    CONTRACTL
-    {
-        GC_NOTRIGGER;
-        PRECONDITION(CheckPointer(pCodeInfo));
-    }
-    CONTRACTL_END;
-    IJitManager::MethodRegionInfo methodRegionInfo = {NULL, 0, NULL, 0};
-    PCODE functionAddress = pMethodDesc->GetNativeCode();
-    pCodeInfo->m_rgCodeRegions[kHot].pAddress = CORDB_ADDRESS(PCODEToPINSTR(functionAddress));
-    if (functionAddress != NULL)
-    {
-        EECodeInfo codeInfo(functionAddress);
-        _ASSERTE(codeInfo.IsValid());
-        codeInfo.GetMethodRegionInfo(&methodRegionInfo);
-        pCodeInfo->m_rgCodeRegions[kHot].cbSize = (ULONG)methodRegionInfo.hotSize;
-        pCodeInfo->m_rgCodeRegions[kCold].Init(PCODEToPINSTR(methodRegionInfo.coldStartAddress),
-                                               (ULONG)methodRegionInfo.coldSize);
-        _ASSERTE(pCodeInfo->IsValid());
-    }
-    else
-    {
-        _ASSERTE(!pCodeInfo->IsValid());
-    }
-} // GetMethodRegionInfo
-void DacDbiInterfaceImpl::GetNativeCodeInfo(VMPTR_DomainAssembly         vmDomainAssembly,
-                                            mdToken                  functionToken,
-                                            NativeCodeFunctionData * pCodeInfo)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(pCodeInfo != NULL);
-    pCodeInfo->Clear();
-    DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
-    Module *     pModule     = pDomainAssembly->GetModule();
-    MethodDesc* pMethodDesc = FindLoadedMethodRefOrDef(pModule, functionToken);
-    pCodeInfo->vmNativeCodeMethodDescToken.SetHostPtr(pMethodDesc);
-    if(pMethodDesc != NULL)
-    {
-        GetMethodRegionInfo(pMethodDesc, pCodeInfo);
-        if (pCodeInfo->m_rgCodeRegions[kHot].pAddress != NULL)
-        {
-            pCodeInfo->isInstantiatedGeneric = pMethodDesc->HasClassOrMethodInstantiation();
-            LookupEnCVersions(pModule,
-                              pCodeInfo->vmNativeCodeMethodDescToken,
-                              functionToken,
-                              pCodeInfo->m_rgCodeRegions[kHot].pAddress,
-                              &(pCodeInfo->encVersion));
-        }
-    }
-} // GetNativeCodeInfo
-void DacDbiInterfaceImpl::GetNativeCodeInfoForAddr(VMPTR_MethodDesc         vmMethodDesc,
-                                                   CORDB_ADDRESS            hotCodeStartAddr,
-                                                   NativeCodeFunctionData * pCodeInfo)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(pCodeInfo != NULL);
-    if (hotCodeStartAddr == NULL)
-    {
-        _ASSERTE(!pCodeInfo->IsValid());
-        return;
-    }
-    IJitManager::MethodRegionInfo methodRegionInfo = {NULL, 0, NULL, 0};
-    TADDR codeAddr = CORDB_ADDRESS_TO_TADDR(hotCodeStartAddr);
-#ifdef TARGET_ARM
-    _ASSERTE((codeAddr & THUMB_CODE) == 0);
-    codeAddr &= ~THUMB_CODE;
-#endif
-    EECodeInfo codeInfo(codeAddr);
-    _ASSERTE(codeInfo.IsValid());
-    EX_TRY_ALLOW_DATATARGET_MISSING_MEMORY
-    {
-        codeInfo.GetMethodRegionInfo(&methodRegionInfo);
-    }
-    EX_END_CATCH_ALLOW_DATATARGET_MISSING_MEMORY;
-    _ASSERTE(methodRegionInfo.hotStartAddress == codeAddr);
-    pCodeInfo->m_rgCodeRegions[kHot].Init(PCODEToPINSTR(methodRegionInfo.hotStartAddress),
-                                          (ULONG)methodRegionInfo.hotSize);
-    pCodeInfo->m_rgCodeRegions[kCold].Init(PCODEToPINSTR(methodRegionInfo.coldStartAddress),
-                                               (ULONG)methodRegionInfo.coldSize);
-    _ASSERTE(pCodeInfo->IsValid());
-    MethodDesc* pMethodDesc = vmMethodDesc.GetDacPtr();
-    pCodeInfo->isInstantiatedGeneric = pMethodDesc->HasClassOrMethodInstantiation();
-    pCodeInfo->vmNativeCodeMethodDescToken = vmMethodDesc;
-    SIZE_T unusedLatestEncVersion;
-    Module * pModule = pMethodDesc->GetModule();
-    _ASSERTE(pModule != NULL);
-    LookupEnCVersions(pModule,
-                      vmMethodDesc,
-                      pMethodDesc->GetMemberDef(),
-                      codeAddr,
-                      &unusedLatestEncVersion, //unused by caller
-                      &(pCodeInfo->encVersion));
-} // GetNativeCodeInfo
-void DacDbiInterfaceImpl::GetTypeHandles(VMPTR_TypeHandle  vmThExact,
-                                         VMPTR_TypeHandle  vmThApprox,
-                                         TypeHandle *      pThExact,
-                                         TypeHandle *      pThApprox)
- {
-     _ASSERTE((pThExact != NULL) && (pThApprox != NULL));
-     *pThExact = TypeHandle::FromPtr(vmThExact.GetDacPtr());
-     *pThApprox = TypeHandle::FromPtr(vmThApprox.GetDacPtr());
-    if ((pThApprox->IsNull()) || ((!pThApprox->IsValueType()) && (!pThApprox->IsRestored())))
-    {
-        LOG((LF_CORDB, LL_INFO10000, "D::GASCI: class isn't loaded.\n"));
-        ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
-    }
-    if (!pThExact->IsNull() && !pThExact->IsRestored())
-    {
-        *pThExact = TypeHandle();
-    }
- }  // DacDbiInterfaceImpl::GetTypeHandles
-unsigned int DacDbiInterfaceImpl::GetTotalFieldCount(TypeHandle thApprox)
-{
-    MethodTable *pMT = thApprox.GetMethodTable();
-    unsigned int IFCount = pMT->GetNumIntroducedInstanceFields();
-    unsigned int SFCount = pMT->GetNumStaticFields();
-#ifdef EnC_SUPPORTED
-    PTR_Module pModule = pMT->GetModule();
-    if (pModule->IsEditAndContinueEnabled())
-    {
-        PTR_EnCEEClassData pEncData =
-            (dac_cast<PTR_EditAndContinueModule>(pModule))->GetEnCEEClassData(pMT, TRUE);
-        if (pEncData != NULL)
-        {
-            _ASSERTE(pEncData->GetMethodTable() == pMT);
-            IFCount += pEncData->GetAddedInstanceFields();
-            SFCount += pEncData->GetAddedStaticFields();
-        }
-    }
-#endif
-    return IFCount + SFCount;
-} // DacDbiInterfaceImpl::GetTotalFieldCount
-void DacDbiInterfaceImpl::InitClassData(TypeHandle  thApprox,
-                                        BOOL        fIsInstantiatedType,
-                                        ClassInfo * pData)
-{
-    pData->m_fieldList.Alloc(GetTotalFieldCount(thApprox));
-    pData->m_objectSize = 0;
-    if ((!thApprox.GetNumGenericArgs()) || fIsInstantiatedType)
-    {
-        pData->m_objectSize = thApprox.GetMethodTable()->GetNumInstanceFieldBytes();
-    }
-} // DacDbiInterfaceImpl::InitClassData
-void DacDbiInterfaceImpl::GetStaticsBases(TypeHandle thExact,
-                                         AppDomain * pAppDomain,
-                                         PTR_BYTE *  ppGCStaticsBase,
-                                         PTR_BYTE *  ppNonGCStaticsBase)
- {
-    MethodTable * pMT = thExact.GetMethodTable();
-    Module * pModuleForStatics = pMT->GetModuleForStatics();
-    if (pModuleForStatics != NULL)
-    {
-        PTR_DomainLocalModule pLocalModule = pModuleForStatics->GetDomainLocalModule();
-        if (pLocalModule != NULL)
-        {
-            *ppGCStaticsBase = pLocalModule->GetGCStaticsBasePointer(pMT);
-            *ppNonGCStaticsBase = pLocalModule->GetNonGCStaticsBasePointer(pMT);
-        }
-    }
-} // DacDbiInterfaceImpl::GetStaticsBases
-void DacDbiInterfaceImpl::ComputeFieldData(PTR_FieldDesc pFD,
-                                           PTR_BYTE    pGCStaticsBase,
-                                           PTR_BYTE    pNonGCStaticsBase,
-                                           FieldData * pCurrentFieldData)
-{
-    pCurrentFieldData->Initialize(pFD->IsStatic(), pFD->IsPrimitive(), pFD->GetMemberDef());
-#ifdef EnC_SUPPORTED
-    if (pFD->IsEnCNew())
-    {
-        pCurrentFieldData->m_vmFieldDesc.SetHostPtr(pFD);
-        pCurrentFieldData->m_fFldStorageAvailable = FALSE;
-        pCurrentFieldData->m_fFldIsTLS = FALSE;
-        pCurrentFieldData->m_fFldIsRVA = FALSE;
-        pCurrentFieldData->m_fFldIsCollectibleStatic = FALSE;
-    }
-    else
-#endif // EnC_SUPPORTED
-    {
-        pCurrentFieldData->m_fFldStorageAvailable = TRUE;
-        pCurrentFieldData->m_vmFieldDesc.SetHostPtr(pFD);
-        pCurrentFieldData->m_fFldIsTLS = (pFD->IsThreadStatic() == TRUE);
-        pCurrentFieldData->m_fFldIsRVA = (pFD->IsRVA() == TRUE);
-        pCurrentFieldData->m_fFldIsCollectibleStatic = (pFD->IsStatic() == TRUE &&
-            pFD->GetEnclosingMethodTable()->Collectible());
-        if (pFD->IsStatic())
-        {
-            if (pFD->IsRVA())
-            {
-                DWORD offset = pFD->GetOffset();
-                PTR_VOID addr = pFD->GetModule()->GetRvaField(offset);
-                if (pCurrentFieldData->OkToGetOrSetStaticAddress())
-                {
-                    pCurrentFieldData->SetStaticAddress(PTR_TO_TADDR(addr));
-                }
-            }
-            else if (pFD->IsThreadStatic() ||
-                pCurrentFieldData->m_fFldIsCollectibleStatic)
-            {
-            }
-            else
-            {
-                PTR_BYTE base = pFD->IsPrimitive() ? pNonGCStaticsBase : pGCStaticsBase;
-                if (base == NULL)
-                {
-                    if (pCurrentFieldData->OkToGetOrSetStaticAddress())
-                    {
-                        pCurrentFieldData->SetStaticAddress(NULL);
-                    }
-                }
-                else
-                {
-                    if (pCurrentFieldData->OkToGetOrSetStaticAddress())
-                    {
-                        pCurrentFieldData->SetStaticAddress(PTR_TO_TADDR(base) + pFD->GetOffset());
-                    }
-                }
-            }
-        }
-        else
-        {
-            if (pCurrentFieldData->OkToGetOrSetInstanceOffset())
-            {
-                pCurrentFieldData->SetInstanceOffset(pFD->GetOffset());
-            }
-        }
-    }
-} // DacDbiInterfaceImpl::ComputeFieldData
-void DacDbiInterfaceImpl::CollectFields(TypeHandle                   thExact,
-                                        TypeHandle                   thApprox,
-                                        AppDomain *                  pAppDomain,
-                                        DacDbiArrayList<FieldData> * pFieldList)
-{
-    PTR_BYTE pGCStaticsBase = NULL;
-    PTR_BYTE pNonGCStaticsBase = NULL;
-    if (!thExact.IsNull() && !thExact.GetMethodTable()->Collectible())
-    {
-        GetStaticsBases(thExact, pAppDomain, &pGCStaticsBase, &pNonGCStaticsBase);
-    }
-    unsigned int fieldCount = 0;
-    EncApproxFieldDescIterator fdIterator(thApprox.GetMethodTable(),
-                                          ApproxFieldDescIterator::ALL_FIELDS); // don't fixup EnC (we can't, we're stopped)
-    PTR_FieldDesc pCurrentFD;
-    unsigned int index = 0;
-    while (((pCurrentFD = fdIterator.Next()) != NULL) && (index < pFieldList->Count()))
-    {
-        ComputeFieldData(pCurrentFD, pGCStaticsBase, pNonGCStaticsBase, &((*pFieldList)[index]));
-        fieldCount++;
-        index++;
-    }
-    _ASSERTE(fieldCount == (unsigned int)pFieldList->Count());
-} // DacDbiInterfaceImpl::CollectFields
-BOOL DacDbiInterfaceImpl::IsValueType (VMPTR_TypeHandle vmTypeHandle)
-{
-    DD_ENTER_MAY_THROW;
-    TypeHandle th = TypeHandle::FromPtr(vmTypeHandle.GetDacPtr());
-    return th.IsValueType();
-}
-BOOL DacDbiInterfaceImpl::HasTypeParams (VMPTR_TypeHandle vmTypeHandle)
-{
-    DD_ENTER_MAY_THROW;
-    TypeHandle th = TypeHandle::FromPtr(vmTypeHandle.GetDacPtr());
-    return th.ContainsGenericVariables();
-}
-void DacDbiInterfaceImpl::GetClassInfo(VMPTR_AppDomain  vmAppDomain,
-                                       VMPTR_TypeHandle vmThExact,
-                                       ClassInfo *      pData)
-{
-    DD_ENTER_MAY_THROW;
-    AppDomain * pAppDomain = vmAppDomain.GetDacPtr();
-    TypeHandle  thExact;
-    TypeHandle  thApprox;
-    GetTypeHandles(vmThExact, vmThExact, &thExact, &thApprox);
-    InitClassData(thApprox, false, pData);
-    if (pAppDomain != NULL)
-        CollectFields(thExact, thApprox, pAppDomain, &(pData->m_fieldList));
-} // DacDbiInterfaceImpl::GetClassInfo
-void DacDbiInterfaceImpl::GetInstantiationFieldInfo (VMPTR_DomainAssembly             vmDomainAssembly,
-                                                     VMPTR_TypeHandle             vmThExact,
-                                                     VMPTR_TypeHandle             vmThApprox,
-                                                     DacDbiArrayList<FieldData> * pFieldList,
-                                                     SIZE_T *                     pObjectSize)
-{
-    DD_ENTER_MAY_THROW;
-    DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
-    _ASSERTE(pDomainAssembly != NULL);
-    AppDomain * pAppDomain = pDomainAssembly->GetAppDomain();
-    TypeHandle  thExact;
-    TypeHandle  thApprox;
-    GetTypeHandles(vmThExact, vmThApprox, &thExact, &thApprox);
-    *pObjectSize = thApprox.GetMethodTable()->GetNumInstanceFieldBytes();
-    pFieldList->Alloc(GetTotalFieldCount(thApprox));
-    CollectFields(thExact, thApprox, pAppDomain, pFieldList);
-} // DacDbiInterfaceImpl::GetInstantiationFieldInfo
-DacDbiInterfaceImpl::TypeDataWalk::TypeDataWalk(DebuggerIPCE_TypeArgData * pData, unsigned int nData)
-{
-    m_pCurrentData = pData;
-    m_nRemaining = nData;
-} // DacDbiInterfaceImpl::TypeDataWalk::TypeDataWalk
-DebuggerIPCE_TypeArgData * DacDbiInterfaceImpl::TypeDataWalk::ReadOne()
-{
-    LIMITED_METHOD_CONTRACT;
-    if (m_nRemaining)
-    {
-        m_nRemaining--;
-        return m_pCurrentData++;
-    }
-    else
-    {
-        return NULL;
-    }
-} // DacDbiInterfaceImpl::TypeDataWalk::ReadOne
-void DacDbiInterfaceImpl::TypeDataWalk::Skip()
-{
-    LIMITED_METHOD_CONTRACT;
-    DebuggerIPCE_TypeArgData * pData = ReadOne();
-    if (pData)
-    {
-        for (unsigned int i = 0; i < pData->numTypeArgs; i++)
-        {
-            Skip();
-        }
-    }
-} // DacDbiInterfaceImpl::TypeDataWalk::Skip
-TypeHandle DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedTypeArg(TypeHandleReadType retrieveWhich)
-{
-    CONTRACTL
-    {
-        NOTHROW;
-        GC_NOTRIGGER;
-    }
-    CONTRACTL_END;
-#if !defined(FEATURE_SHARE_GENERIC_CODE)
-    return ReadLoadedTypeHandle(kGetExact);
-#else
-    if (retrieveWhich == kGetExact)
-        return ReadLoadedTypeHandle(kGetExact);
-    DebuggerIPCE_TypeArgData * pData = ReadOne();
-    if (!pData)
-        return TypeHandle();
-    CorElementType elementType = pData->data.elementType;
-    switch (elementType)
-    {
-        case ELEMENT_TYPE_PTR:
-            _ASSERTE(pData->numTypeArgs == 1);
-            return PtrOrByRefTypeArg(pData, retrieveWhich);
-            break;
-        case ELEMENT_TYPE_CLASS:
-        case ELEMENT_TYPE_VALUETYPE:
-            return ClassTypeArg(pData, retrieveWhich);
-            break;
-        case ELEMENT_TYPE_FNPTR:
-            return FnPtrTypeArg(pData, retrieveWhich);
-            break;
-        default:
-            return ObjRefOrPrimitiveTypeArg(pData, elementType);
-            break;
-    }
-#endif // FEATURE_SHARE_GENERIC_CODE
-} // DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedTypeArg
-BOOL DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedTypeHandles(TypeHandleReadType retrieveWhich,
-                                                              unsigned int       nTypeArgs,
-                                                              TypeHandle *       ppResults)
-{
-    WRAPPER_NO_CONTRACT;
-    BOOL allOK = true;
-    for (unsigned int i = 0; i < nTypeArgs; i++)
-    {
-        ppResults[i] = ReadLoadedTypeArg(retrieveWhich);
-        allOK &= !ppResults[i].IsNull();
-    }
-    return allOK;
-} // DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedTypeHandles
-TypeHandle DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedInstantiation(TypeHandleReadType retrieveWhich,
-                                                                      Module *           pModule,
-                                                                      mdTypeDef          mdToken,
-                                                                      unsigned int       nTypeArgs)
-{
-    WRAPPER_NO_CONTRACT;
-    NewArrayHolder<TypeHandle> pInst(new TypeHandle[nTypeArgs]);
-    if (!ReadLoadedTypeHandles(retrieveWhich, nTypeArgs, pInst))
-    {
-        return TypeHandle();
-    }
-    return FindLoadedInstantiation(pModule, mdToken, nTypeArgs, pInst);
-} // DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedInstantiation
-TypeHandle DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedTypeHandle(TypeHandleReadType retrieveWhich)
-{
-    CONTRACTL
-    {
-        NOTHROW;
-        GC_NOTRIGGER;
-    }
-    CONTRACTL_END;
-    DebuggerIPCE_TypeArgData * pData = ReadOne();
-    if (!pData)
-      return TypeHandle();
-    TypeHandle typeHandle;
-    switch (pData->data.elementType)
-    {
-        case ELEMENT_TYPE_ARRAY:
-        case ELEMENT_TYPE_SZARRAY:
-            typeHandle = ArrayTypeArg(pData, retrieveWhich);
-            break;
-        case ELEMENT_TYPE_PTR:
-        case ELEMENT_TYPE_BYREF:
-            typeHandle = PtrOrByRefTypeArg(pData, retrieveWhich);
-            break;
-        case ELEMENT_TYPE_CLASS:
-        case ELEMENT_TYPE_VALUETYPE:
-            {
-                Module *     pModule = pData->data.ClassTypeData.vmModule.GetDacPtr();
-                typeHandle = ReadLoadedInstantiation(retrieveWhich,
-                                                     pModule,
-                                                     pData->data.ClassTypeData.metadataToken,
-                                                     pData->numTypeArgs);
-            }
-            break;
-        case ELEMENT_TYPE_FNPTR:
-            {
-                typeHandle = FnPtrTypeArg(pData, retrieveWhich);
-            }
-            break;
-    default:
-            typeHandle = FindLoadedElementType(pData->data.elementType);
-        break;
-    }
-    return typeHandle;
-} // DacDbiInterfaceImpl::TypeDataWalk::ReadLoadedTypeHandle
-TypeHandle DacDbiInterfaceImpl::TypeDataWalk::ArrayTypeArg(DebuggerIPCE_TypeArgData * pArrayTypeInfo,
-                                                           TypeHandleReadType         retrieveWhich)
-{
-    TypeHandle arrayElementTypeArg = ReadLoadedTypeArg(retrieveWhich);
-    if (!arrayElementTypeArg.IsNull())
-    {
-        return FindLoadedArrayType(pArrayTypeInfo->data.elementType,
-                                   arrayElementTypeArg,
-                                   pArrayTypeInfo->data.ArrayTypeData.arrayRank);
-    }
-    return TypeHandle();
-} // DacDbiInterfaceImpl::TypeDataWalk::ArrayTypeArg
-TypeHandle DacDbiInterfaceImpl::TypeDataWalk::PtrOrByRefTypeArg(DebuggerIPCE_TypeArgData * pPtrOrByRefTypeInfo,
-                                                                TypeHandleReadType         retrieveWhich)
-{
-    TypeHandle referentTypeArg = ReadLoadedTypeArg(retrieveWhich);
-    if (!referentTypeArg.IsNull())
-    {
-        return FindLoadedPointerOrByrefType(pPtrOrByRefTypeInfo->data.elementType, referentTypeArg);
-    }
-    return TypeHandle();
-} // DacDbiInterfaceImpl::TypeDataWalk::PtrOrByRefTypeArg
-TypeHandle DacDbiInterfaceImpl::TypeDataWalk::ClassTypeArg(DebuggerIPCE_TypeArgData * pClassTypeInfo,
-                                                           TypeHandleReadType         retrieveWhich)
-{
-    Module *     pModule = pClassTypeInfo->data.ClassTypeData.vmModule.GetDacPtr();
-    TypeHandle   typeDef = ClassLoader::LookupTypeDefOrRefInModule(pModule,
-                                                                   pClassTypeInfo->data.ClassTypeData.metadataToken);
-    if ((!typeDef.IsNull() && typeDef.IsValueType()) || (pClassTypeInfo->data.elementType == ELEMENT_TYPE_VALUETYPE))
-    {
-        return ReadLoadedInstantiation(retrieveWhich,
-                                       pModule,
-                                       pClassTypeInfo->data.ClassTypeData.metadataToken,
-                                       pClassTypeInfo->numTypeArgs);
-    }
-    else
-    {
-        _ASSERTE(retrieveWhich == kGetCanonical);
-        for (unsigned int i = 0; i < pClassTypeInfo->numTypeArgs; i++)
-        {
-            Skip();
-        }
-        return TypeHandle(g_pCanonMethodTableClass);
-    }
-}// DacDbiInterfaceImpl::TypeDataWalk::ClassTypeArg
-TypeHandle DacDbiInterfaceImpl::TypeDataWalk::FnPtrTypeArg(DebuggerIPCE_TypeArgData * pFnPtrTypeInfo,
-                                                           TypeHandleReadType         retrieveWhich)
-{
-    NewArrayHolder<TypeHandle> pInst(new TypeHandle[sizeof(TypeHandle) * pFnPtrTypeInfo->numTypeArgs]);
-    if (ReadLoadedTypeHandles(retrieveWhich, pFnPtrTypeInfo->numTypeArgs, pInst))
-    {
-        return FindLoadedFnptrType(pFnPtrTypeInfo->numTypeArgs, pInst);
-    }
-    return TypeHandle();
-} // DacDbiInterfaceImpl::TypeDataWalk::FnPtrTypeArg
-TypeHandle DacDbiInterfaceImpl::TypeDataWalk::ObjRefOrPrimitiveTypeArg(DebuggerIPCE_TypeArgData * pArgInfo,
-                                                                       CorElementType             elementType)
-{
-    for (unsigned int i = 0; i < pArgInfo->numTypeArgs; i++)
-    {
-        Skip();
-    }
-    if (CorTypeInfo::IsObjRef_NoThrow(elementType))
-    {
-        return TypeHandle(g_pCanonMethodTableClass);
-    }
-    else
-    {
-        return FindLoadedElementType(elementType);
-    }
-} // DacDbiInterfaceImpl::TypeDataWalk::ObjRefOrPrimitiveTypeArg
-TypeHandle DacDbiInterfaceImpl::FindLoadedArrayType(CorElementType arrayType,
-                                                    TypeHandle     typeArg,
-                                                    unsigned       rank)
-{
-    ENABLE_FORBID_GC_LOADER_USE_IN_THIS_SCOPE();
-    if (typeArg.IsNull())
-    {
-        return TypeHandle();
-    }
-    else
-    {
-        return ClassLoader::LoadArrayTypeThrowing(typeArg,
-                                                  arrayType,
-                                                  rank,
-                                                  ClassLoader::DontLoadTypes );
-    }
-} // DacDbiInterfaceImpl::FindLoadedArrayType;
-TypeHandle DacDbiInterfaceImpl::FindLoadedPointerOrByrefType(CorElementType addressType, TypeHandle typeArg)
-{
-    ENABLE_FORBID_GC_LOADER_USE_IN_THIS_SCOPE();
-    return ClassLoader::LoadPointerOrByrefTypeThrowing(addressType,
-                                                       typeArg,
-                                                       ClassLoader::DontLoadTypes);
-} // DacDbiInterfaceImpl::FindLoadedPointerOrByrefType
-TypeHandle DacDbiInterfaceImpl::FindLoadedFnptrType(DWORD numTypeArgs, TypeHandle * pInst)
-{
-    ENABLE_FORBID_GC_LOADER_USE_IN_THIS_SCOPE();
-    return  ClassLoader::LoadFnptrTypeThrowing(0,
-                                               numTypeArgs - 1,
-                                               pInst,
-                                               ClassLoader::DontLoadTypes);
-} // DacDbiInterfaceImpl::FindLoadedFnptrType
-TypeHandle DacDbiInterfaceImpl::FindLoadedInstantiation(Module *     pModule,
-                                                        mdTypeDef    mdToken,
-                                                        DWORD        nTypeArgs,
-                                                        TypeHandle * pInst)
-{
-    ENABLE_FORBID_GC_LOADER_USE_IN_THIS_SCOPE();
-    return ClassLoader::LoadGenericInstantiationThrowing(pModule,
-                                                         mdToken,
-                                                         Instantiation(pInst,nTypeArgs),
-                                                         ClassLoader::DontLoadTypes);
-} // DacDbiInterfaceImpl::FindLoadedInstantiation
-TypeHandle DacDbiInterfaceImpl::FindLoadedElementType(CorElementType elementType)
-{
-    ENABLE_FORBID_GC_LOADER_USE_IN_THIS_SCOPE();
-    MethodTable * pMethodTable = (&g_CoreLib)->GetElementType(elementType);
-    return TypeHandle(pMethodTable);
-} // DacDbiInterfaceImpl::FindLoadedElementType
-void DacDbiInterfaceImpl::GetArrayTypeInfo(TypeHandle                      typeHandle,
-                                           DebuggerIPCE_ExpandedTypeData * pTypeInfo,
-                                           AppDomain *                     pAppDomain)
-{
-    _ASSERTE(typeHandle.IsArray());
-    pTypeInfo->ArrayTypeData.arrayRank = typeHandle.GetRank();
-    TypeHandleToBasicTypeInfo(typeHandle.GetArrayElementTypeHandle(),
-                              &(pTypeInfo->ArrayTypeData.arrayTypeArg),
-                              pAppDomain);
-} // DacDbiInterfaceImpl::GetArrayTypeInfo
-void DacDbiInterfaceImpl::GetPtrTypeInfo(AreValueTypesBoxed              boxed,
-                                         TypeHandle                      typeHandle,
-                                         DebuggerIPCE_ExpandedTypeData * pTypeInfo,
-                                         AppDomain *                     pAppDomain)
-{
-    if (boxed == AllBoxed)
-    {
-        GetClassTypeInfo(typeHandle, pTypeInfo, pAppDomain);
-    }
-    else
-    {
-        _ASSERTE(typeHandle.IsTypeDesc());
-        TypeHandleToBasicTypeInfo(typeHandle.AsTypeDesc()->GetTypeParam(),
-                                  &(pTypeInfo->UnaryTypeData.unaryTypeArg),
-                                  pAppDomain);
-    }
-} // DacDbiInterfaceImpl::GetPtrTypeInfo
-void DacDbiInterfaceImpl::GetFnPtrTypeInfo(AreValueTypesBoxed              boxed,
-                                           TypeHandle                      typeHandle,
-                                           DebuggerIPCE_ExpandedTypeData * pTypeInfo,
-                                           AppDomain *                     pAppDomain)
-{
-    if (boxed == AllBoxed)
-    {
-        GetClassTypeInfo(typeHandle, pTypeInfo, pAppDomain);
-    }
-    else
-    {
-        pTypeInfo->NaryTypeData.typeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
-    }
-} // DacDbiInterfaceImpl::GetFnPtrTypeInfo
-void DacDbiInterfaceImpl::GetClassTypeInfo(TypeHandle                      typeHandle,
-                                           DebuggerIPCE_ExpandedTypeData * pTypeInfo,
-                                           AppDomain *                     pAppDomain)
-{
-    Module * pModule = typeHandle.GetModule();
-    if (typeHandle.HasInstantiation()) // the type handle represents a generic instantiation
-    {
-        pTypeInfo->ClassTypeData.typeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
-    }
-    else // non-generic
-    {
-        pTypeInfo->ClassTypeData.typeHandle = VMPTR_TypeHandle::NullPtr();
-    }
-    pTypeInfo->ClassTypeData.metadataToken = typeHandle.GetCl();
-    _ASSERTE(pModule);
-    pTypeInfo->ClassTypeData.vmModule.SetDacTargetPtr(PTR_HOST_TO_TADDR(pModule));
-    if (pAppDomain)
-    {
-        pTypeInfo->ClassTypeData.vmDomainAssembly.SetDacTargetPtr(PTR_HOST_TO_TADDR(pModule->GetDomainAssembly()));
-    }
-    else
-    {
-        pTypeInfo->ClassTypeData.vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
-    }
-} // DacDbiInterfaceImpl::GetClassTypeInfo
-CorElementType DacDbiInterfaceImpl::GetElementType (TypeHandle typeHandle)
-{
-    if (typeHandle.IsNull())
-    {
-        return ELEMENT_TYPE_VOID;
-    }
-    else if (typeHandle.GetMethodTable() == g_pObjectClass)
-    {
-       return ELEMENT_TYPE_OBJECT;
-    }
-    else if (typeHandle.GetMethodTable() == g_pStringClass)
-    {
-        return ELEMENT_TYPE_STRING;
-    }
-    else
-    {
-        return typeHandle.GetSignatureCorElementType();
-    }
-} // DacDbiInterfaceImpl::GetElementType
-void DacDbiInterfaceImpl::TypeHandleToBasicTypeInfo(TypeHandle                   typeHandle,
-                                                    DebuggerIPCE_BasicTypeData * pTypeInfo,
-                                                    AppDomain *                  pAppDomain)
-{
-    pTypeInfo->elementType = GetElementType(typeHandle);
-    switch (pTypeInfo->elementType)
-    {
-        case ELEMENT_TYPE_ARRAY:
-        case ELEMENT_TYPE_SZARRAY:
-        case ELEMENT_TYPE_FNPTR:
-        case ELEMENT_TYPE_PTR:
-        case ELEMENT_TYPE_BYREF:
-            pTypeInfo->vmTypeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
-            pTypeInfo->metadataToken = mdTokenNil;
-            pTypeInfo->vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
-            break;
-        case ELEMENT_TYPE_CLASS:
-        case ELEMENT_TYPE_VALUETYPE:
-        {
-            Module * pModule = typeHandle.GetModule();
-            if (typeHandle.HasInstantiation())   // only set if instantiated
-            {
-                pTypeInfo->vmTypeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
-            }
-            else
-            {
-                pTypeInfo->vmTypeHandle = VMPTR_TypeHandle::NullPtr();
-            }
-            pTypeInfo->metadataToken = typeHandle.GetCl();
-            _ASSERTE(pModule);
-            pTypeInfo->vmModule.SetDacTargetPtr(PTR_HOST_TO_TADDR(pModule));
-            if (pAppDomain)
-            {
-                pTypeInfo->vmDomainAssembly.SetDacTargetPtr(PTR_HOST_TO_TADDR(pModule->GetDomainAssembly()));
-            }
-            else
-            {
-                pTypeInfo->vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
-            }
-            break;
-        }
-        default:
-            pTypeInfo->vmTypeHandle = VMPTR_TypeHandle::NullPtr();
-            pTypeInfo->metadataToken = mdTokenNil;
-            pTypeInfo->vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
-            break;
-    }
-    return;
-} // DacDbiInterfaceImpl::TypeHandleToBasicTypeInfo
-void DacDbiInterfaceImpl::GetObjectExpandedTypeInfoFromID(AreValueTypesBoxed boxed,
-                                       VMPTR_AppDomain vmAppDomain,
-                                       COR_TYPEID id,
-                                       DebuggerIPCE_ExpandedTypeData *pTypeInfo)
-{
-    DD_ENTER_MAY_THROW;
-    TypeHandleToExpandedTypeInfoImpl(boxed, vmAppDomain, TypeHandle::FromPtr(TO_TADDR(id.token1)), pTypeInfo);
-}
-void DacDbiInterfaceImpl::GetObjectExpandedTypeInfo(AreValueTypesBoxed boxed,
-                                       VMPTR_AppDomain vmAppDomain,
-                                       CORDB_ADDRESS addr,
-                                       DebuggerIPCE_ExpandedTypeData *pTypeInfo)
-{
-    DD_ENTER_MAY_THROW;
-    PTR_Object obj(TO_TADDR(addr));
-    TypeHandleToExpandedTypeInfoImpl(boxed, vmAppDomain, obj->GetGCSafeTypeHandle(), pTypeInfo);
-}
-void DacDbiInterfaceImpl::TypeHandleToExpandedTypeInfo(AreValueTypesBoxed              boxed,
-                                                       VMPTR_AppDomain                 vmAppDomain,
-                                                       VMPTR_TypeHandle                vmTypeHandle,
-                                                       DebuggerIPCE_ExpandedTypeData * pTypeInfo)
-{
-    DD_ENTER_MAY_THROW;
-    TypeHandle typeHandle = TypeHandle::FromPtr(vmTypeHandle.GetDacPtr());
-    TypeHandleToExpandedTypeInfoImpl(boxed, vmAppDomain, typeHandle, pTypeInfo);
-}
-void DacDbiInterfaceImpl::TypeHandleToExpandedTypeInfoImpl(AreValueTypesBoxed              boxed,
-                                                       VMPTR_AppDomain                 vmAppDomain,
-                                                       TypeHandle                      typeHandle,
-                                                       DebuggerIPCE_ExpandedTypeData * pTypeInfo)
-{
-    AppDomain * pAppDomain = vmAppDomain.GetDacPtr();
-    pTypeInfo->elementType = GetElementType(typeHandle);
-    switch (pTypeInfo->elementType)
-    {
-        case ELEMENT_TYPE_ARRAY:
-        case ELEMENT_TYPE_SZARRAY:
-            GetArrayTypeInfo(typeHandle, pTypeInfo, pAppDomain);
-            break;
-        case ELEMENT_TYPE_PTR:
-        case ELEMENT_TYPE_BYREF:
-            GetPtrTypeInfo(boxed, typeHandle, pTypeInfo, pAppDomain);
-            break;
-        case ELEMENT_TYPE_VALUETYPE:
-            if (boxed == OnlyPrimitivesUnboxed || boxed == AllBoxed)
-            {
-                pTypeInfo->elementType = ELEMENT_TYPE_CLASS;
-            }
-            GetClassTypeInfo(typeHandle, pTypeInfo, pAppDomain);
-           break;
-        case ELEMENT_TYPE_CLASS:
-            GetClassTypeInfo(typeHandle, pTypeInfo, pAppDomain);
-            break;
-        case ELEMENT_TYPE_FNPTR:
-                GetFnPtrTypeInfo(boxed, typeHandle, pTypeInfo, pAppDomain);
-                break;
-        default:
-            if (boxed == AllBoxed)
-            {
-                pTypeInfo->elementType = ELEMENT_TYPE_CLASS;
-                GetClassTypeInfo(typeHandle, pTypeInfo, pAppDomain);
-            }
-            break;
-    }
-    LOG((LF_CORDB, LL_INFO10000, "D::THTETI: converted left-side type handle to expanded right-side type info, pTypeInfo->ClassTypeData.typeHandle = 0x%08x.\n", pTypeInfo->ClassTypeData.typeHandle.GetRawPtr()));
-    return;
-} // DacDbiInterfaceImpl::TypeHandleToExpandedTypeInfo
-VMPTR_TypeHandle DacDbiInterfaceImpl::GetTypeHandle(VMPTR_Module vmModule,
-                                                    mdTypeDef metadataToken)
-{
-    DD_ENTER_MAY_THROW;
-    Module* pModule = vmModule.GetDacPtr();
-    VMPTR_TypeHandle vmTypeHandle = VMPTR_TypeHandle::NullPtr();
-    TypeHandle th = ClassLoader::LookupTypeDefOrRefInModule(pModule, metadataToken);
-    if (th.IsNull())
-    {
-        LOG((LF_CORDB, LL_INFO10000, "D::GTH: class isn't loaded.\n"));
-        ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
-    }
-    vmTypeHandle.SetDacTargetPtr(th.AsTAddr());
-    return vmTypeHandle;
-}
-VMPTR_TypeHandle DacDbiInterfaceImpl::GetApproxTypeHandle(TypeInfoList * pTypeData)
-{
-    DD_ENTER_MAY_THROW;
-    LOG((LF_CORDB, LL_INFO10000, "D::GATH: getting info.\n"));
-    TypeDataWalk walk(&((*pTypeData)[0]), pTypeData->Count());
-    TypeHandle typeHandle = walk.ReadLoadedTypeHandle(TypeDataWalk::kGetCanonical);
-    VMPTR_TypeHandle vmTypeHandle = VMPTR_TypeHandle::NullPtr();
-    vmTypeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
-    if (!typeHandle.IsNull())
-    {
-        vmTypeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
-    }
-    else
-    {
-        ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
-    }
-    LOG((LF_CORDB, LL_INFO10000,
-        "D::GATH: sending result, result = 0x%0x8\n",
-        typeHandle));
-    return vmTypeHandle;
-} // DacDbiInterfaceImpl::GetApproxTypeHandle
-HRESULT DacDbiInterfaceImpl::GetExactTypeHandle(DebuggerIPCE_ExpandedTypeData * pTypeData,
-                                                ArgInfoList *   pArgInfo,
-                                                VMPTR_TypeHandle& vmTypeHandle)
-{
-    DD_ENTER_MAY_THROW;
-    LOG((LF_CORDB, LL_INFO10000, "D::GETH: getting info.\n"));
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        vmTypeHandle = vmTypeHandle.NullPtr();
-        TypeHandle typeHandle = ExpandedTypeInfoToTypeHandle(pTypeData, pArgInfo);
-        _ASSERTE(!typeHandle.IsNull());
-        vmTypeHandle.SetDacTargetPtr(typeHandle.AsTAddr());
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-} // DacDbiInterfaceImpl::GetExactTypeHandle
-void DacDbiInterfaceImpl::GetMethodDescParams(
-    VMPTR_AppDomain     vmAppDomain,
-    VMPTR_MethodDesc    vmMethodDesc,
-    GENERICS_TYPE_TOKEN genericsToken,
-    UINT32 *            pcGenericClassTypeParams,
-    TypeParamsList *    pGenericTypeParams)
-{
-    DD_ENTER_MAY_THROW;
-    if (vmAppDomain.IsNull() || vmMethodDesc.IsNull())
-    {
-        ThrowHR(E_INVALIDARG);
-    }
-    _ASSERTE((pcGenericClassTypeParams != NULL) && (pGenericTypeParams != NULL));
-    MethodDesc * pMD = vmMethodDesc.GetDacPtr();
-    UINT32 cGenericClassTypeParams  = pMD->GetNumGenericClassArgs();
-    UINT32 cGenericMethodTypeParams = pMD->GetNumGenericMethodArgs();
-    UINT32 cTotalGenericTypeParams  = cGenericClassTypeParams + cGenericMethodTypeParams;
-    *pcGenericClassTypeParams = cGenericClassTypeParams;
-    TypeHandle   thSpecificClass;
-    MethodDesc * pSpecificMethod = NULL;
-    BOOL fExact = FALSE;
-    ALLOW_DATATARGET_MISSING_MEMORY(
-        fExact = Generics::GetExactInstantiationsOfMethodAndItsClassFromCallInformation(
-            pMD,
-            PTR_VOID((TADDR)genericsToken),
-            &thSpecificClass,
-            &pSpecificMethod);
-            );
-    if (!fExact ||
-        !thSpecificClass.GetMethodTable()->SanityCheck() ||
-        !pSpecificMethod->GetMethodTable()->SanityCheck())
-    {
-        thSpecificClass = TypeHandle(pMD->GetMethodTable());
-        pSpecificMethod = pMD;
-    }
-    Instantiation classInst  = pSpecificMethod->GetExactClassInstantiation(thSpecificClass);
-    Instantiation methodInst = pSpecificMethod->GetMethodInstantiation();
-    _ASSERTE((classInst.IsEmpty())  == (cGenericClassTypeParams  == 0));
-    _ASSERTE((methodInst.IsEmpty()) == (cGenericMethodTypeParams == 0));
-    pGenericTypeParams->Alloc(cTotalGenericTypeParams);
-    for (UINT32 i = 0; i < cTotalGenericTypeParams; i++)
-    {
-        TypeHandle thCurrent;
-        if (i < cGenericClassTypeParams)
-        {
-            thCurrent = classInst[i];
-        }
-        else
-        {
-            thCurrent = methodInst[i - cGenericClassTypeParams];
-        }
-        EX_TRY_ALLOW_DATATARGET_MISSING_MEMORY_WITH_HANDLER
-        {
-            VMPTR_TypeHandle vmTypeHandle = VMPTR_TypeHandle::NullPtr();
-            vmTypeHandle.SetDacTargetPtr(thCurrent.AsTAddr());
-            TypeHandleToExpandedTypeInfo(NoValueTypeBoxing,
-                                         vmAppDomain,
-                                         vmTypeHandle,
-                                         &((*pGenericTypeParams)[i]));
-        }
-        EX_CATCH_ALLOW_DATATARGET_MISSING_MEMORY_WITH_HANDLER
-        {
-            VMPTR_TypeHandle vmTHCanon = VMPTR_TypeHandle::NullPtr();
-            TypeHandle thCanon = TypeHandle(g_pCanonMethodTableClass);
-            vmTHCanon.SetDacTargetPtr(thCanon.AsTAddr());
-            TypeHandleToExpandedTypeInfo(NoValueTypeBoxing,
-                                         vmAppDomain,
-                                         vmTHCanon,
-                                         &((*pGenericTypeParams)[i]));
-        }
-        EX_END_CATCH_ALLOW_DATATARGET_MISSING_MEMORY_WITH_HANDLER
-    }
-}
-TypeHandle DacDbiInterfaceImpl::GetClassOrValueTypeHandle(DebuggerIPCE_BasicTypeData * pData)
-{
-    TypeHandle typeHandle;
-    if (!pData->vmTypeHandle.IsNull())
-    {
-        typeHandle = TypeHandle::FromPtr(pData->vmTypeHandle.GetDacPtr());
-    }
-    else
-    {
-        DomainAssembly * pDomainAssembly = pData->vmDomainAssembly.GetDacPtr();
-        Module *     pModule = pDomainAssembly->GetModule();
-        typeHandle = ClassLoader::LookupTypeDefOrRefInModule(pModule, pData->metadataToken);
-        if (typeHandle.IsNull())
-        {
-            LOG((LF_CORDB, LL_INFO10000, "D::BTITTH: class isn't loaded.\n"));
-            ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
-        }
-        _ASSERTE(typeHandle.GetNumGenericArgs() == 0);
-    }
-    return typeHandle;
-} // DacDbiInterfaceImpl::GetClassOrValueTypeHandle
-TypeHandle DacDbiInterfaceImpl::GetExactArrayTypeHandle(DebuggerIPCE_ExpandedTypeData * pTopLevelTypeData,
-                                                        ArgInfoList *                   pArgInfo)
-{
-    TypeHandle typeArg;
-    _ASSERTE(pArgInfo->Count() == 1);
-    typeArg = BasicTypeInfoToTypeHandle(&((*pArgInfo)[0]));
-    return FindLoadedArrayType(pTopLevelTypeData->elementType,
-                               typeArg,
-                               pTopLevelTypeData->ArrayTypeData.arrayRank);
-} // DacDbiInterfaceImpl::GetExactArrayTypeHandle
-TypeHandle DacDbiInterfaceImpl::GetExactPtrOrByRefTypeHandle(DebuggerIPCE_ExpandedTypeData * pTopLevelTypeData,
-                                                             ArgInfoList *                   pArgInfo)
-{
-    TypeHandle typeArg;
-    _ASSERTE(pArgInfo->Count() == 1);
-    typeArg = BasicTypeInfoToTypeHandle(&((*pArgInfo)[0]));
-    return FindLoadedPointerOrByrefType(pTopLevelTypeData->elementType, typeArg);
-} // DacDbiInterfaceImpl::GetExactPtrOrByRefTypeHandle
-TypeHandle DacDbiInterfaceImpl::GetExactClassTypeHandle(DebuggerIPCE_ExpandedTypeData * pTopLevelTypeData,
-                                                        ArgInfoList *                   pArgInfo)
-{
-    Module *     pModule = pTopLevelTypeData->ClassTypeData.vmModule.GetDacPtr();
-    int          argCount = pArgInfo->Count();
-    TypeHandle typeConstructor =
-        ClassLoader::LookupTypeDefOrRefInModule(pModule, pTopLevelTypeData->ClassTypeData.metadataToken);
-    if (typeConstructor.IsNull())
-    {
-        LOG((LF_CORDB, LL_INFO10000, "D::ETITTH: class isn't loaded.\n"));
-        ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
-    }
-    if (argCount == 0)
-    {
-        return typeConstructor;
-    }
-    if ((unsigned int)argCount != typeConstructor.GetNumGenericArgs())
-    {
-        LOG((LF_CORDB, LL_INFO10000,
-            "D::ETITTH: wrong number of type parameters, %d given, %d expected\n",
-            argCount, typeConstructor.GetNumGenericArgs()));
-        _ASSERTE((unsigned int)argCount == typeConstructor.GetNumGenericArgs());
-        ThrowHR(E_FAIL);
-    }
-    S_UINT32 allocSize = S_UINT32(argCount) * S_UINT32(sizeof(TypeHandle));
-    if (allocSize.IsOverflow())
-    {
-        ThrowHR(E_OUTOFMEMORY);
-    }
-    NewArrayHolder<TypeHandle> pInst(new TypeHandle[allocSize.Value()]);
-    for (unsigned int i = 0; i < (unsigned int)argCount; i++)
-    {
-        pInst[i] = BasicTypeInfoToTypeHandle(&((*pArgInfo)[i]));
-    }
-    return FindLoadedInstantiation(typeConstructor.GetModule(),
-                                   typeConstructor.GetCl(),
-                                   argCount,
-                                   pInst);
-} // DacDbiInterfaceImpl::GetExactClassTypeHandle
-TypeHandle DacDbiInterfaceImpl::GetExactFnPtrTypeHandle(ArgInfoList * pArgInfo)
-{
-    S_UINT32 allocSize = S_UINT32(pArgInfo->Count()) * S_UINT32(sizeof(TypeHandle));
-    if( allocSize.IsOverflow() )
-    {
-        ThrowHR(E_OUTOFMEMORY);
-    }
-    NewArrayHolder<TypeHandle> pInst(new TypeHandle[allocSize.Value()]);
-    for (unsigned int i = 0; i < pArgInfo->Count(); i++)
-    {
-        pInst[i] = BasicTypeInfoToTypeHandle(&((*pArgInfo)[i]));
-    }
-    return FindLoadedFnptrType(pArgInfo->Count(), pInst);
-} // DacDbiInterfaceImpl::GetExactFnPtrTypeHandle
-TypeHandle DacDbiInterfaceImpl::BasicTypeInfoToTypeHandle(DebuggerIPCE_BasicTypeData * pArgTypeData)
-{
-    LOG((LF_CORDB, LL_INFO10000,
-        "D::BTITTH: expanding basic right-side type to left-side type, ELEMENT_TYPE: %d.\n",
-        pArgTypeData->elementType));
-    TypeHandle typeHandle = TypeHandle();
-    switch (pArgTypeData->elementType)
-    {
-        case ELEMENT_TYPE_ARRAY:
-        case ELEMENT_TYPE_SZARRAY:
-        case ELEMENT_TYPE_PTR:
-        case ELEMENT_TYPE_BYREF:
-        case ELEMENT_TYPE_FNPTR:
-            _ASSERTE(!pArgTypeData->vmTypeHandle.IsNull());
-            typeHandle = TypeHandle::FromPtr(pArgTypeData->vmTypeHandle.GetDacPtr());
-            break;
-        case ELEMENT_TYPE_CLASS:
-        case ELEMENT_TYPE_VALUETYPE:
-            typeHandle = GetClassOrValueTypeHandle(pArgTypeData);
-            break;
-        default:
-            typeHandle = FindLoadedElementType(pArgTypeData->elementType);
-            break;
-    }
-    if (typeHandle.IsNull())
-    {
-        ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
-    }
-    return typeHandle;
-} // DacDbiInterfaceImpl::BasicTypeInfoToTypeHandle
-TypeHandle DacDbiInterfaceImpl::ExpandedTypeInfoToTypeHandle(DebuggerIPCE_ExpandedTypeData * pTopLevelTypeData,
-                                                             ArgInfoList *                   pArgInfo)
-{
-    WRAPPER_NO_CONTRACT;
-    LOG((LF_CORDB, LL_INFO10000,
-        "D::ETITTH: expanding right-side type to left-side type, ELEMENT_TYPE: %d.\n",
-        pData->elementType));
-    TypeHandle typeHandle = TypeHandle();
-    switch (pTopLevelTypeData->elementType)
-    {
-        case ELEMENT_TYPE_ARRAY:
-        case ELEMENT_TYPE_SZARRAY:
-            typeHandle = GetExactArrayTypeHandle(pTopLevelTypeData, pArgInfo);
-            break;
-        case ELEMENT_TYPE_PTR:
-        case ELEMENT_TYPE_BYREF:
-            typeHandle = GetExactPtrOrByRefTypeHandle(pTopLevelTypeData, pArgInfo);
-            break;
-        case ELEMENT_TYPE_CLASS:
-        case ELEMENT_TYPE_VALUETYPE:
-            typeHandle = GetExactClassTypeHandle(pTopLevelTypeData, pArgInfo);
-            break;
-        case ELEMENT_TYPE_FNPTR:
-            typeHandle = GetExactFnPtrTypeHandle(pArgInfo);
-            break;
-        default:
-            typeHandle = FindLoadedElementType(pTopLevelTypeData->elementType);
-            break;
-    } // end switch (pData->elementType)
-    if (typeHandle.IsNull())
-    {
-        LOG((LF_CORDB, LL_INFO10000, "D::ETITTH: type isn't loaded.\n"));
-        ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
-    }
-    return typeHandle;
-} // DacDbiInterfaceImpl::ExpandedTypeInfoToTypeHandle
-CORDB_ADDRESS DacDbiInterfaceImpl::GetThreadStaticAddress(VMPTR_FieldDesc vmField,
-                                                          VMPTR_Thread    vmRuntimeThread)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pRuntimeThread = vmRuntimeThread.GetDacPtr();
-    PTR_FieldDesc pFieldDesc = vmField.GetDacPtr();
-    TADDR fieldAddress = NULL;
-    _ASSERTE(pRuntimeThread != NULL);
-    if (pFieldDesc->IsThreadStatic())
-    {
-        fieldAddress = pRuntimeThread->GetStaticFieldAddrNoCreate(pFieldDesc);
-    }
-    else
-    {
-        ThrowHR(E_NOTIMPL);
-    }
-    return fieldAddress;
-} // DacDbiInterfaceImpl::GetThreadStaticAddress
-CORDB_ADDRESS DacDbiInterfaceImpl::GetCollectibleTypeStaticAddress(VMPTR_FieldDesc vmField,
-                                                                   VMPTR_AppDomain vmAppDomain)
-{
-    DD_ENTER_MAY_THROW;
-    AppDomain * pAppDomain = vmAppDomain.GetDacPtr();
-    PTR_FieldDesc pFieldDesc = vmField.GetDacPtr();
-    _ASSERTE(pAppDomain != NULL);
-    if(!pFieldDesc->IsStatic() ||
-       pFieldDesc->IsSpecialStatic())
-    {
-        _ASSERTE(!"BUG: Unsupported static field type for collectible types");
-    }
-    /* TODO: Ideally we should be checking if the class is allocated first, however
-             we don't appear to be doing this even for non-collectible statics and
-             we have never seen an issue.
-    */
-    PTR_VOID base = pFieldDesc->GetBase();
-    if (base == PTR_NULL)
-    {
-        return PTR_HOST_TO_TADDR(NULL);
-    }
-    PTR_VOID addr = pFieldDesc->GetStaticAddressHandle(base);
-    return PTR_TO_TADDR(addr);
-} // DacDbiInterfaceImpl::GetCollectibleTypeStaticAddress
-void DacDbiInterfaceImpl::GetTypeHandleParams(VMPTR_AppDomain  vmAppDomain,
-                                              VMPTR_TypeHandle vmTypeHandle,
-                                              TypeParamsList * pParams)
-{
-    DD_ENTER_MAY_THROW
-    TypeHandle typeHandle = TypeHandle::FromPtr(vmTypeHandle.GetDacPtr());
-    LOG((LF_CORDB, LL_INFO10000, "D::GTHP: getting type parameters for 0x%08x 0x%0x8.\n",
-         vmAppDomain.GetDacPtr(), typeHandle.AsPtr()));
-    _ASSERTE(pParams->IsEmpty());
-    pParams->Alloc(typeHandle.GetNumGenericArgs());
-    for (unsigned int i = 0; i < pParams->Count(); ++i)
-    {
-        VMPTR_TypeHandle thInst = VMPTR_TypeHandle::NullPtr();
-        thInst.SetDacTargetPtr(typeHandle.GetInstantiation()[i].AsTAddr());
-        TypeHandleToExpandedTypeInfo(NoValueTypeBoxing,
-                                     vmAppDomain,
-                                     thInst,
-                                     &((*pParams)[i]));
-    }
-    LOG((LF_CORDB, LL_INFO10000, "D::GTHP: sending  result"));
-} // DacDbiInterfaceImpl::GetTypeHandleParams
-void DacDbiInterfaceImpl::GetSimpleType(VMPTR_AppDomain    vmAppDomain,
-                                        CorElementType     simpleType,
-                                        mdTypeDef         *pMetadataToken,
-                                        VMPTR_Module      *pVmModule,
-                                        VMPTR_DomainAssembly  *pVmDomainAssembly)
-{
-    DD_ENTER_MAY_THROW;
-    AppDomain *pAppDomain = vmAppDomain.GetDacPtr();
-    _ASSERTE(pVmDomainAssembly != NULL);
-    *pVmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
-    TypeHandle typeHandle =  FindLoadedElementType(simpleType);
-    if (typeHandle.IsNull())
-    {
-        ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
-    }
-    else
-    {
-        _ASSERTE(pMetadataToken != NULL);
-        *pMetadataToken = typeHandle.GetCl();
-        Module * pModule = typeHandle.GetModule();
-        if (pModule == NULL)
-            ThrowHR(CORDBG_E_TARGET_INCONSISTENT);
-        pVmModule->SetHostPtr(pModule);
-        if (pAppDomain)
-        {
-            pVmDomainAssembly->SetHostPtr(pModule->GetDomainAssembly());
-            if (pVmDomainAssembly->IsNull())
-                ThrowHR(CORDBG_E_TARGET_INCONSISTENT);
-        }
-    }
-    LOG((LF_CORDB, LL_INFO10000, "D::STI: sending result.\n"));
-} // DacDbiInterfaceImpl::GetSimpleType
-BOOL DacDbiInterfaceImpl::IsExceptionObject(VMPTR_Object vmObject)
-{
-    DD_ENTER_MAY_THROW;
-    Object* objPtr = vmObject.GetDacPtr();
-    MethodTable* pMT = objPtr->GetMethodTable();
-    return IsExceptionObject(pMT);
-}
-BOOL DacDbiInterfaceImpl::IsExceptionObject(MethodTable* pMT)
-{
-    PTR_MethodTable pExMT = g_pExceptionClass;
-    TADDR targetMT = dac_cast<TADDR>(pMT);
-    TADDR exceptionMT = dac_cast<TADDR>(pExMT);
-    do
-    {
-        if (targetMT == exceptionMT)
-            return TRUE;
-        pMT = pMT->GetParentMethodTable();
-        targetMT = dac_cast<TADDR>(pMT);
-    } while (pMT);
-    return FALSE;
-}
-HRESULT DacDbiInterfaceImpl::GetMethodDescPtrFromIpEx(TADDR funcIp, VMPTR_MethodDesc* ppMD)
-{
-    DD_ENTER_MAY_THROW;
-    CLRDATA_ADDRESS mdAddr;
-    HRESULT hr = g_dacImpl->GetMethodDescPtrFromIP(TO_CDADDR(funcIp), &mdAddr);
-    if (S_OK == hr)
-    {
-        ppMD->SetDacTargetPtr(CLRDATA_ADDRESS_TO_TADDR(mdAddr));
-        return hr;
-    }
-    MethodDesc* pMD = MethodTable::GetMethodDescForSlotAddress(PINSTRToPCODE(funcIp));
-    if (pMD == NULL)
-        return E_INVALIDARG;
-    ppMD->SetDacTargetPtr(PTR_HOST_TO_TADDR(pMD));
-    return S_OK;
-}
-BOOL DacDbiInterfaceImpl::IsDelegate(VMPTR_Object vmObject)
-{
-    DD_ENTER_MAY_THROW;
-    if (vmObject.IsNull())
-        return FALSE;
-    Object *pObj = vmObject.GetDacPtr();
-    return pObj->GetGCSafeMethodTable()->IsDelegate();
-}
-HRESULT DacDbiInterfaceImpl::GetDelegateType(VMPTR_Object delegateObject, DelegateType *delegateType)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(!delegateObject.IsNull());
-    _ASSERTE(delegateType != NULL);
-#ifdef _DEBUG
-    IsDelegate(delegateObject);
-#endif
-    *delegateType = DelegateType::kUnknownDelegateType;
-    PTR_DelegateObject pDelObj = dac_cast<PTR_DelegateObject>(delegateObject.GetDacPtr());
-    INT_PTR invocationCount = pDelObj->GetInvocationCount();
-    if (invocationCount == -1)
-    {
-        *delegateType = kUnmanagedFunctionDelegate;
-        return S_OK;
-    }
-    PTR_Object pInvocationList = OBJECTREFToObject(pDelObj->GetInvocationList());
-    if (invocationCount == NULL)
-    {
-        if (pInvocationList == NULL)
-        {
-            TADDR targetMethodPtr = PCODEToPINSTR(pDelObj->GetMethodPtrAux());
-            if (targetMethodPtr == NULL)
-            {
-                *delegateType = kClosedDelegate;
-            }
-            else {
-                *delegateType = kOpenDelegate;
-            }
-            return S_OK;
-        }
-    }
-    else
-    {
-        if (pInvocationList != NULL)
-        {
-            PTR_MethodTable invocationListMT = pInvocationList->GetGCSafeMethodTable();
-            if (invocationListMT->IsArray())
-                *delegateType = kTrueMulticastDelegate;
-            if (invocationListMT->IsDelegate())
-                *delegateType = kWrapperDelegate;
-            return S_OK;
-        }
-    }
-    _ASSERT(FALSE);
-    *delegateType = kUnknownDelegateType;
-    return CORDBG_E_UNSUPPORTED_DELEGATE;
-}
-HRESULT DacDbiInterfaceImpl::GetDelegateFunctionData(
-    DelegateType delegateType,
-    VMPTR_Object delegateObject,
-    OUT VMPTR_DomainAssembly *ppFunctionDomainAssembly,
-    OUT mdMethodDef *pMethodDef)
-{
-    DD_ENTER_MAY_THROW;
-#ifdef _DEBUG
-    IsDelegate(delegateObject);
-#endif
-    HRESULT hr = S_OK;
-    PTR_DelegateObject pDelObj = dac_cast<PTR_DelegateObject>(delegateObject.GetDacPtr());
-    TADDR targetMethodPtr = NULL;
-    VMPTR_MethodDesc pMD;
-    switch (delegateType)
-    {
-    case kClosedDelegate:
-        targetMethodPtr = PCODEToPINSTR(pDelObj->GetMethodPtr());
-        break;
-    case kOpenDelegate:
-        targetMethodPtr = PCODEToPINSTR(pDelObj->GetMethodPtrAux());
-        break;
-    default:
-        return E_FAIL;
-    }
-    hr = GetMethodDescPtrFromIpEx(targetMethodPtr, &pMD);
-    if (hr != S_OK)
-        return hr;
-    ppFunctionDomainAssembly->SetDacTargetPtr(dac_cast<TADDR>(pMD.GetDacPtr()->GetModule()->GetDomainAssembly()));
-    *pMethodDef = pMD.GetDacPtr()->GetMemberDef();
-    return hr;
-}
-HRESULT DacDbiInterfaceImpl::GetDelegateTargetObject(
-    DelegateType delegateType,
-    VMPTR_Object delegateObject,
-    OUT VMPTR_Object *ppTargetObj,
-    OUT VMPTR_AppDomain *ppTargetAppDomain)
-{
-    DD_ENTER_MAY_THROW;
-#ifdef _DEBUG
-    IsDelegate(delegateObject);
-#endif
-    HRESULT hr = S_OK;
-    PTR_DelegateObject pDelObj = dac_cast<PTR_DelegateObject>(delegateObject.GetDacPtr());
-    switch (delegateType)
-    {
-        case kClosedDelegate:
-        {
-            PTR_Object pRemoteTargetObj = OBJECTREFToObject(pDelObj->GetTarget());
-            ppTargetObj->SetDacTargetPtr(pRemoteTargetObj.GetAddr());
-            ppTargetAppDomain->SetDacTargetPtr(dac_cast<TADDR>(pRemoteTargetObj->GetGCSafeMethodTable()->GetDomain()->AsAppDomain()));
-            break;
-        }
-        default:
-            ppTargetObj->SetDacTargetPtr(NULL);
-            ppTargetAppDomain->SetDacTargetPtr(dac_cast<TADDR>(pDelObj->GetGCSafeMethodTable()->GetDomain()->AsAppDomain()));
-            break;
-    }
-    return hr;
-}
-static bool TrackMemoryRangeHelper(PTR_VOID pvArgs, PTR_VOID pvAllocationBase, SIZE_T cbReserved)
-{
-    CQuickArrayList<COR_MEMORY_RANGE> *rangeCollection =
-                                        (CQuickArrayList<COR_MEMORY_RANGE>*)(dac_cast<TADDR>(pvArgs));
-    TADDR rangeStart = dac_cast<TADDR>(pvAllocationBase);
-    TADDR rangeEnd = rangeStart + cbReserved;
-    rangeCollection->Push({rangeStart, rangeEnd});
-    return false;
-}
-void DacDbiInterfaceImpl::EnumerateMemRangesForLoaderAllocator(PTR_LoaderAllocator pLoaderAllocator, CQuickArrayList<COR_MEMORY_RANGE> *rangeAcummulator)
-{
-    CQuickArrayList<PTR_LoaderHeap> heapsToEnumerate;
-    _ASSERTE(pLoaderAllocator->GetLowFrequencyHeap() != NULL);
-    heapsToEnumerate.Push(pLoaderAllocator->GetLowFrequencyHeap());
-    _ASSERTE(pLoaderAllocator->GetHighFrequencyHeap() != NULL);
-    heapsToEnumerate.Push(pLoaderAllocator->GetHighFrequencyHeap());
-    _ASSERTE(pLoaderAllocator->GetStubHeap() != NULL);
-    heapsToEnumerate.Push(pLoaderAllocator->GetStubHeap());
-    VirtualCallStubManager *pVcsMgr = pLoaderAllocator->GetVirtualCallStubManager();
-    LOG((LF_CORDB, LL_INFO10000, "DDBII::EMRFLA: VirtualCallStubManager 0x%x\n", PTR_HOST_TO_TADDR(pVcsMgr)));
-    if (pVcsMgr)
-    {
-        if (pVcsMgr->indcell_heap != NULL) heapsToEnumerate.Push(pVcsMgr->indcell_heap);
-        if (pVcsMgr->cache_entry_heap != NULL) heapsToEnumerate.Push(pVcsMgr->cache_entry_heap);
-    }
-    TADDR rangeAccumAsTaddr = TO_TADDR(rangeAcummulator);
-    for (uint32_t i = 0; i < (uint32_t)heapsToEnumerate.Size(); i++)
-    {
-        LOG((LF_CORDB, LL_INFO10000, "DDBII::EMRFLA: LoaderHeap 0x%x\n", heapsToEnumerate[i].GetAddr()));
-        heapsToEnumerate[i]->EnumPageRegions(TrackMemoryRangeHelper, rangeAccumAsTaddr);
-    }
-}
-void DacDbiInterfaceImpl::EnumerateMemRangesForJitCodeHeaps(CQuickArrayList<COR_MEMORY_RANGE> *rangeAcummulator)
-{
-    EEJitManager *pEM = ExecutionManager::GetEEJitManager();
-    _ASSERTE(pEM != NULL && pEM->m_pCodeHeap.IsValid());
-    PTR_HeapList pHeapList = pEM->m_pCodeHeap;
-    while (pHeapList != NULL)
-    {
-        CodeHeap *pHeap = pHeapList->pHeap;
-        DacpJitCodeHeapInfo jitCodeHeapInfo = DACGetHeapInfoForCodeHeap(pHeap);
-        switch (jitCodeHeapInfo.codeHeapType)
-        {
-            case CODEHEAP_LOADER:
-            {
-                TADDR targetLoaderHeap = CLRDATA_ADDRESS_TO_TADDR(jitCodeHeapInfo.LoaderHeap);
-                LOG((LF_CORDB, LL_INFO10000,
-                    "DDBII::EMRFJCH: LoaderCodeHeap 0x%x with LoaderHeap at 0x%x\n",
-                    PTR_HOST_TO_TADDR(pHeap), targetLoaderHeap));
-                PTR_ExplicitControlLoaderHeap pLoaderHeap = PTR_ExplicitControlLoaderHeap(targetLoaderHeap);
-                pLoaderHeap->EnumPageRegions(TrackMemoryRangeHelper, TO_TADDR(rangeAcummulator));
-                break;
-            }
-            case CODEHEAP_HOST:
-            {
-                LOG((LF_CORDB, LL_INFO10000,
-                    "DDBII::EMRFJCH: HostCodeHeap 0x%x\n",
-                    PTR_HOST_TO_TADDR(pHeap)));
-                rangeAcummulator->Push({
-                    CLRDATA_ADDRESS_TO_TADDR(jitCodeHeapInfo.HostData.baseAddr),
-                    CLRDATA_ADDRESS_TO_TADDR(jitCodeHeapInfo.HostData.currentAddr)
-                });
-                break;
-            }
-            default:
-            {
-                LOG((LF_CORDB, LL_INFO10000, "DDBII::EMRFJCH: unknown heap type at 0x%x\n\n", pHeap));
-                _ASSERTE("Unknown heap type enumerating code ranges.");
-                break;
-            }
-        }
-        pHeapList = pHeapList->GetNext();
-    }
-}
-HRESULT DacDbiInterfaceImpl::GetLoaderHeapMemoryRanges(DacDbiArrayList<COR_MEMORY_RANGE> *pRanges)
-{
-    LOG((LF_CORDB, LL_INFO10000, "DDBII::GLHMR\n"));
-    DD_ENTER_MAY_THROW;
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        CQuickArrayList<COR_MEMORY_RANGE> memoryRanges;
-        PTR_LoaderAllocator pGlobalAllocator = SystemDomain::System()->GetLoaderAllocator();
-        _ASSERTE(pGlobalAllocator);
-        EnumerateMemRangesForLoaderAllocator(pGlobalAllocator, &memoryRanges);
-        EnumerateMemRangesForJitCodeHeaps(&memoryRanges);
-        _ASSERTE(memoryRanges.Size() < INT_MAX);
-        pRanges->Init(memoryRanges.Ptr(), (UINT) memoryRanges.Size());
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-void DacDbiInterfaceImpl::GetStackFramesFromException(VMPTR_Object vmObject, DacDbiArrayList<DacExceptionCallStackData>& dacStackFrames)
-{
-    DD_ENTER_MAY_THROW;
-    PTR_Object objPtr = vmObject.GetDacPtr();
-#ifdef _DEBUG
-    MethodTable* pMT = objPtr->GetMethodTable();
-    _ASSERTE(IsExceptionObject(pMT));
-#endif
-    OBJECTREF objRef = ObjectToOBJECTREF(objPtr);
-    DebugStackTrace::GetStackFramesData stackFramesData;
-    stackFramesData.pDomain = NULL;
-    stackFramesData.skip = 0;
-    stackFramesData.NumFramesRequested = 0;
-    DebugStackTrace::GetStackFramesFromException(&objRef, &stackFramesData);
-    INT32 dacStackFramesLength = stackFramesData.cElements;
-    if (dacStackFramesLength > 0)
-    {
-        dacStackFrames.Alloc(dacStackFramesLength);
-        for (INT32 index = 0; index < dacStackFramesLength; ++index)
-        {
-            DebugStackTrace::DebugStackTraceElement const& currentElement = stackFramesData.pElements[index];
-            DacExceptionCallStackData& currentFrame = dacStackFrames[index];
-            Module* pModule = currentElement.pFunc->GetModule();
-            BaseDomain* pBaseDomain = currentElement.pFunc->GetAssembly()->GetDomain();
-            AppDomain* pDomain = NULL;
-            DomainAssembly* pDomainAssembly = NULL;
-            pDomain = pBaseDomain->AsAppDomain();
-            _ASSERTE(pDomain != NULL);
-            pDomainAssembly = pModule->GetDomainAssembly();
-            _ASSERTE(pDomainAssembly != NULL);
-            currentFrame.vmAppDomain.SetHostPtr(pDomain);
-            currentFrame.vmDomainAssembly.SetHostPtr(pDomainAssembly);
-            currentFrame.ip = currentElement.ip;
-            currentFrame.methodDef = currentElement.pFunc->GetMemberDef();
-            currentFrame.isLastForeignExceptionFrame = (currentElement.flags & STEF_LAST_FRAME_FROM_FOREIGN_STACK_TRACE) != 0;
-        }
-    }
-}
-#ifdef FEATURE_COMINTEROP
-PTR_RCW GetRcwFromVmptrObject(VMPTR_Object vmObject)
-{
-    PTR_RCW pRCW = NULL;
-    Object* objPtr = vmObject.GetDacPtr();
-    PTR_SyncBlock pSyncBlock = NULL;
-    pSyncBlock = objPtr->PassiveGetSyncBlock();
-    if (pSyncBlock == NULL)
-        return pRCW;
-    PTR_InteropSyncBlockInfo pInfo = NULL;
-    pInfo = pSyncBlock->GetInteropInfoNoCreate();
-    if (pInfo == NULL)
-        return pRCW;
-    pRCW = dac_cast<PTR_RCW>(pInfo->DacGetRawRCW());
-    return pRCW;
-}
-#endif
-BOOL DacDbiInterfaceImpl::IsRcw(VMPTR_Object vmObject)
-{
-#ifdef FEATURE_COMINTEROP
-    DD_ENTER_MAY_THROW;
-    return GetRcwFromVmptrObject(vmObject) != NULL;
-#else
-    return FALSE;
-#endif // FEATURE_COMINTEROP
-}
-void DacDbiInterfaceImpl::GetRcwCachedInterfaceTypes(
-                        VMPTR_Object vmObject,
-                        VMPTR_AppDomain vmAppDomain,
-                        BOOL bIInspectableOnly,
-                        DacDbiArrayList<DebuggerIPCE_ExpandedTypeData> * pDacInterfaces)
-{
-    pDacInterfaces->Alloc(0);
-}
-void DacDbiInterfaceImpl::GetRcwCachedInterfacePointers(
-                    VMPTR_Object vmObject,
-                    BOOL bIInspectableOnly,
-                    DacDbiArrayList<CORDB_ADDRESS> * pDacItfPtrs)
-{
-#ifdef FEATURE_COMINTEROP
-    DD_ENTER_MAY_THROW;
-    Object* objPtr = vmObject.GetDacPtr();
-    InlineSArray<TADDR, INTERFACE_ENTRY_CACHE_SIZE> rgUnks;
-    PTR_RCW pRCW = GetRcwFromVmptrObject(vmObject);
-    if (pRCW != NULL)
-    {
-        pRCW->GetCachedInterfacePointers(bIInspectableOnly, &rgUnks);
-        pDacItfPtrs->Alloc(rgUnks.GetCount());
-        for (COUNT_T i = 0; i < rgUnks.GetCount(); ++i)
-        {
-            (*pDacItfPtrs)[i] = (CORDB_ADDRESS)(rgUnks[i]);
-        }
-    }
-    else
-#endif // FEATURE_COMINTEROP
-    {
-        pDacItfPtrs->Alloc(0);
-    }
-}
-void DacDbiInterfaceImpl::GetCachedWinRTTypesForIIDs(
-                    VMPTR_AppDomain vmAppDomain,
-					DacDbiArrayList<GUID> & iids,
-    				OUT DacDbiArrayList<DebuggerIPCE_ExpandedTypeData> * pTypes)
-{
-    pTypes->Alloc(0);
-}
-void DacDbiInterfaceImpl::GetCachedWinRTTypes(
-                    VMPTR_AppDomain vmAppDomain,
-                    OUT DacDbiArrayList<GUID> * pGuids,
-                    OUT DacDbiArrayList<DebuggerIPCE_ExpandedTypeData> * pTypes)
-{
-    pTypes->Alloc(0);
-}
-PTR_FieldDesc  DacDbiInterfaceImpl::FindField(TypeHandle thApprox, mdFieldDef fldToken)
-{
-    EncApproxFieldDescIterator fdIterator(thApprox.GetMethodTable(),
-                                          ApproxFieldDescIterator::ALL_FIELDS); // don't fixup EnC (we can't, we're stopped)
-    PTR_FieldDesc pCurrentFD;
-    while ((pCurrentFD = fdIterator.Next()) != NULL)
-    {
-        if (pCurrentFD->GetMemberDef() == fldToken)
-        {
-            return pCurrentFD;
-        }
-    }
-    return NULL;
-} // DacDbiInterfaceImpl::FindField
-FieldDesc * DacDbiInterfaceImpl::GetEnCFieldDesc(const EnCHangingFieldInfo * pEnCFieldInfo)
-{
-        FieldDesc * pFD = NULL;
-        DomainAssembly * pDomainAssembly = pEnCFieldInfo->GetObjectTypeData().vmDomainAssembly.GetDacPtr();
-        Module     * pModule     = pDomainAssembly->GetModule();
-        TypeHandle typeHandle = ClassLoader::LookupTypeDefOrRefInModule(pModule,
-                                             pEnCFieldInfo->GetObjectTypeData().metadataToken);
-        if (typeHandle == NULL)
-        {
-            ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
-        }
-        pFD = FindField(typeHandle, pEnCFieldInfo->GetFieldToken());
-        if (pFD == NULL)
-        {
-            ThrowHR(CORDBG_E_ENC_HANGING_FIELD);
-        }
-        return pFD;
-} // DacDbiInterfaceImpl::GetEnCFieldDesc
-PTR_CBYTE DacDbiInterfaceImpl::GetPtrToEnCField(FieldDesc * pFD, const EnCHangingFieldInfo * pEnCFieldInfo)
-{
-#ifndef EnC_SUPPORTED
-    _ASSERTE(!"Trying to get the address of an EnC field where EnC is not supported! ");
-    return NULL;
-#else
-    PTR_EditAndContinueModule pEnCModule;
-    DomainAssembly * pDomainAssembly = pEnCFieldInfo->GetObjectTypeData().vmDomainAssembly.GetDacPtr();
-    Module     * pModule     = pDomainAssembly->GetModule();
-    _ASSERTE(pModule->IsEditAndContinueCapable());
-    pEnCModule = dac_cast<PTR_EditAndContinueModule>(pModule);
-    _ASSERTE(pFD->IsEnCNew());
-    EnCFieldDesc * pEnCFieldDesc;
-    pEnCFieldDesc = dac_cast<PTR_EnCFieldDesc>(pFD);
-    if (pEnCFieldDesc->NeedsFixup())
-    {
-        ThrowHR(CORDBG_E_ENC_HANGING_FIELD);
-    }
-    PTR_CBYTE pORField = NULL;
-    PTR_Object pObject = pEnCFieldInfo->GetVmObject().GetDacPtr();
-    pORField = pEnCModule->ResolveField(ObjectToOBJECTREF(pObject),
-                                        pEnCFieldDesc);
-    if (pORField == NULL)
-    {
-        ThrowHR(CORDBG_E_ENC_HANGING_FIELD);
-    }
-    return pORField;
-#endif // EnC_SUPPORTED
-} // DacDbiInterfaceImpl::GetPtrToEnCField
-void DacDbiInterfaceImpl::InitFieldData(const FieldDesc *           pFD,
-                                        const PTR_CBYTE             pORField,
-                                        const EnCHangingFieldInfo * pEnCFieldData,
-                                        FieldData *           pFieldData)
-{
-    pFieldData->ClearFields();
-    pFieldData->m_fFldIsStatic = (pFD->IsStatic() != 0);
-    pFieldData->m_vmFieldDesc.SetHostPtr(pFD);
-    pFieldData->m_fFldIsTLS = (pFD->IsThreadStatic() == TRUE);
-    pFieldData->m_fldMetadataToken = pFD->GetMemberDef();
-    pFieldData->m_fFldIsRVA = (pFD->IsRVA() == TRUE);
-    pFieldData->m_fFldIsCollectibleStatic = FALSE;
-    pFieldData->m_fFldStorageAvailable = true;
-    if (pFieldData->m_fFldIsStatic)
-    {
-        _ASSERTE(!pFieldData->m_fFldIsTLS);
-        _ASSERTE(!pFieldData->m_fFldIsRVA);
-        pFieldData->SetStaticAddress(PTR_TO_TADDR(pORField));
-    }
-    else
-    {
-        pFieldData->SetInstanceOffset(PTR_TO_TADDR(pORField) -
-                                      (PTR_TO_TADDR(pEnCFieldData->GetVmObject().GetDacPtr()) +
-                                                   pEnCFieldData->GetOffsetToVars()));
-    }
-} // DacDbiInterfaceImpl::InitFieldData
-void DacDbiInterfaceImpl::GetEnCHangingFieldInfo(const EnCHangingFieldInfo * pEnCFieldInfo,
-                                                 FieldData *           pFieldData,
-                                                 BOOL *                pfStatic)
-{
-    DD_ENTER_MAY_THROW;
-    LOG((LF_CORDB, LL_INFO100000, "DDI::IEnCHFI: Obj:0x%x, objType"
-        ":0x%x, offset:0x%x\n", pEnCFieldInfo->m_pObject, pEnCFieldInfo->m_objectTypeData.elementType,
-        pEnCFieldInfo->m_offsetToVars));
-    FieldDesc *  pFD      = NULL;
-    PTR_CBYTE    pORField = NULL;
-    pFD = GetEnCFieldDesc(pEnCFieldInfo);
-    _ASSERTE(pFD->IsEnCNew()); // We shouldn't be here if it wasn't added to an
-#ifdef EnC_SUPPORTED
-    pORField = GetPtrToEnCField(pFD, pEnCFieldInfo);
-#else
-    _ASSERTE(!"We shouldn't be here: EnC not supported");
-#endif // EnC_SUPPORTED
-    InitFieldData(pFD, pORField, pEnCFieldInfo, pFieldData);
-    *pfStatic = (pFD->IsStatic() != 0);
-} // DacDbiInterfaceImpl::GetEnCHangingFieldInfo
-void DacDbiInterfaceImpl::GetAssemblyFromDomainAssembly(VMPTR_DomainAssembly vmDomainAssembly, VMPTR_Assembly *vmAssembly)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(vmAssembly != NULL);
-    DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
-    vmAssembly->SetHostPtr(pDomainAssembly->GetAssembly());
-}
-BOOL DacDbiInterfaceImpl::IsAssemblyFullyTrusted(VMPTR_DomainAssembly vmDomainAssembly)
-{
-    DD_ENTER_MAY_THROW;
-    return TRUE;
-}
-BOOL DacDbiInterfaceImpl::GetAssemblyPath(
-    VMPTR_Assembly  vmAssembly,
-    IStringHolder * pStrFilename)
-{
-    DD_ENTER_MAY_THROW;
-    Assembly * pAssembly = vmAssembly.GetDacPtr();
-    Module * pManifestModule = pAssembly->GetModule();
-    const WCHAR * szPath = pManifestModule->GetPath().DacGetRawUnicode();
-    HRESULT hrStatus = pStrFilename->AssignCopy(szPath);
-    IfFailThrow(hrStatus);
-    if(szPath == NULL || *szPath=='\0')
-    {
-        return FALSE;
-    }
-    return TRUE;
-}
-void DacDbiInterfaceImpl::ResolveTypeReference(const TypeRefData * pTypeRefInfo,
-                                               TypeRefData *       pTargetRefInfo)
-{
-    DD_ENTER_MAY_THROW;
-    DomainAssembly * pDomainAssembly        = pTypeRefInfo->vmDomainAssembly.GetDacPtr();
-    Module *     pReferencingModule = pDomainAssembly->GetModule();
-    BOOL         fSuccess = FALSE;
-    Module * pTargetModule = NULL;
-    mdTypeDef targetTypeDef = mdTokenNil;
-    ENABLE_FORBID_GC_LOADER_USE_IN_THIS_SCOPE();
-    fSuccess = ClassLoader::ResolveTokenToTypeDefThrowing(pReferencingModule,
-                                                          pTypeRefInfo->typeToken,
-                                                          &pTargetModule,
-                                                          &targetTypeDef,
-                                                          Loader::SafeLookup   //don't load, no locks/allocations
-                                                          );
-    if (fSuccess)
-    {
-        _ASSERTE(pTargetModule != NULL);
-        _ASSERTE( TypeFromToken(targetTypeDef) == mdtTypeDef );
-        AppDomain * pAppDomain = pDomainAssembly->GetAppDomain();
-        pTargetRefInfo->vmDomainAssembly.SetDacTargetPtr(PTR_HOST_TO_TADDR(pTargetModule->GetDomainAssembly()));
-        pTargetRefInfo->typeToken = targetTypeDef;
-    }
-    else
-    {
-        ThrowHR(CORDBG_E_CLASS_NOT_LOADED);
-    }
-} // DacDbiInterfaceImpl::ResolveTypeReference
-BOOL DacDbiInterfaceImpl::GetModulePath(VMPTR_Module vmModule,
-                                        IStringHolder *  pStrFilename)
-{
-    DD_ENTER_MAY_THROW;
-    Module * pModule = vmModule.GetDacPtr();
-    PEAssembly * pPEAssembly = pModule->GetPEAssembly();
-    if (pPEAssembly != NULL)
-    {
-        if( !pPEAssembly->GetPath().IsEmpty() )
-        {
-            const WCHAR * szPath = pPEAssembly->GetPath().DacGetRawUnicode();
-            if (szPath == NULL)
-            {
-                szPath = pPEAssembly->GetModuleFileNameHint().DacGetRawUnicode();
-                if (szPath == NULL)
-                {
-                    goto NoFileName;
-                }
-            }
-            IfFailThrow(pStrFilename->AssignCopy(szPath));
-            return TRUE;
-        }
-    }
-NoFileName:
-    IfFailThrow(pStrFilename->AssignCopy(W("")));
-    return FALSE;
-}
-BOOL DacDbiInterfaceImpl::GetModuleNGenPath(VMPTR_Module vmModule,
-                                            IStringHolder *  pStrFilename)
-{
-    DD_ENTER_MAY_THROW;
-    IfFailThrow(pStrFilename->AssignCopy(W("")));
-    return FALSE;
-}
-void DacDbiInterfaceImpl::GetModuleSimpleName(VMPTR_Module vmModule, IStringHolder * pStrFilename)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(pStrFilename != NULL);
-    Module * pModule = vmModule.GetDacPtr();
-    LPCUTF8 szNameUtf8 = pModule->GetSimpleName();
-    SString convert(SString::Utf8, szNameUtf8);
-    IfFailThrow(pStrFilename->AssignCopy(convert.GetUnicode()));
-}
-HRESULT DacDbiInterfaceImpl::IsModuleMapped(VMPTR_Module pModule, OUT BOOL *isModuleMapped)
-{
-    LOG((LF_CORDB, LL_INFO10000, "DDBII::IMM - TADDR 0x%x\n", pModule));
-    DD_ENTER_MAY_THROW;
-    HRESULT hr = S_FALSE;
-    PTR_Module pTargetModule = pModule.GetDacPtr();
-    EX_TRY
-    {
-        PTR_PEAssembly pPEAssembly = pTargetModule->GetPEAssembly();
-        _ASSERTE(pPEAssembly != NULL);
-        if (pPEAssembly->HasLoadedPEImage())
-        {
-            *isModuleMapped = pPEAssembly->GetLoadedLayout()->IsMapped();
-            hr = S_OK;
-        }
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-bool DacDbiInterfaceImpl::MetadataUpdatesApplied()
-{
-    DD_ENTER_MAY_THROW;
-#ifdef EnC_SUPPORTED
-    return g_metadataUpdatesApplied;
-#else
-    return false;
-#endif
-}
-void InitTargetBufferFromMemoryRange(const MemoryRange memoryRange, TargetBuffer * pTargetBuffer)
-{
-    SUPPORTS_DAC;
-    _ASSERTE(pTargetBuffer != NULL);
-    PTR_CVOID p = memoryRange.StartAddress();
-    CORDB_ADDRESS addr = PTR_TO_CORDB_ADDRESS(PTR_TO_TADDR(p));
-    _ASSERTE(memoryRange.Size() <= 0xffffffff);
-    pTargetBuffer->Init(addr, (ULONG)memoryRange.Size());
-}
-void InitTargetBufferFromTargetSBuffer(PTR_SBuffer pBuffer, TargetBuffer * pTargetBuffer)
-{
-    SUPPORTS_DAC;
-    _ASSERTE(pTargetBuffer != NULL);
-    SBuffer * pBufferHost = pBuffer;
-    if (pBufferHost == NULL)
-    {
-        pTargetBuffer->Clear();
-        return;
-    }
-    MemoryRange m = pBufferHost->DacGetRawBuffer();
-    InitTargetBufferFromMemoryRange(m, pTargetBuffer);
-}
-void DacDbiInterfaceImpl::GetMetadata(VMPTR_Module vmModule, TargetBuffer * pTargetBuffer)
-{
-    DD_ENTER_MAY_THROW;
-    pTargetBuffer->Clear();
-    Module     * pModule = vmModule.GetDacPtr();
-    _ASSERTE(pModule->IsVisibleToDebugger());
-    if (pModule->IsReflection())
-    {
-        ReflectionModule * pReflectionModule = pModule->GetReflectionModule();
-        InitTargetBufferFromTargetSBuffer(pReflectionModule->GetDynamicMetadataBuffer(), pTargetBuffer);
-    }
-    else
-    {
-        PEAssembly * pPEAssembly = pModule->GetPEAssembly();
-        COUNT_T size;
-        CORDB_ADDRESS address = PTR_TO_CORDB_ADDRESS(dac_cast<TADDR>(pPEAssembly->GetLoadedMetadata(&size)));
-        pTargetBuffer->Init(address, (ULONG) size);
-    }
-    if (pTargetBuffer->IsEmpty())
-    {
-        ThrowHR(CORDBG_E_MISSING_METADATA);
-    }
-}
-void DacDbiInterfaceImpl::GetSymbolsBuffer(VMPTR_Module vmModule, TargetBuffer * pTargetBuffer, SymbolFormat * pSymbolFormat)
-{
-    DD_ENTER_MAY_THROW;
-    pTargetBuffer->Clear();
-    *pSymbolFormat = kSymbolFormatNone;
-    Module * pModule = vmModule.GetDacPtr();
-    _ASSERTE(pModule->IsVisibleToDebugger());
-    PTR_CGrowableStream pStream = pModule->GetInMemorySymbolStream();
-    if (pStream == NULL)
-    {
-        return;
-    }
-    const MemoryRange m = pStream->GetRawBuffer();
-    if (m.Size() == 0)
-    {
-        return;
-    }
-    InitTargetBufferFromMemoryRange(m, pTargetBuffer);
-    *pSymbolFormat = kSymbolFormatPDB;
-}
-void DacDbiInterfaceImpl::GetModuleForDomainAssembly(VMPTR_DomainAssembly vmDomainAssembly, OUT VMPTR_Module * pModule)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(pModule != NULL);
-    DomainAssembly * pDomainAssembly = vmDomainAssembly.GetDacPtr();
-    pModule->SetHostPtr(pDomainAssembly->GetModule());
-}
-void DacDbiInterfaceImpl::GetDomainAssemblyData(VMPTR_DomainAssembly vmDomainAssembly, DomainAssemblyInfo * pData)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(pData != NULL);
-    ZeroMemory(pData, sizeof(*pData));
-    DomainAssembly * pDomainAssembly  = vmDomainAssembly.GetDacPtr();
-    AppDomain  * pAppDomain   = pDomainAssembly->GetAppDomain();
-    pData->vmDomainAssembly.SetHostPtr(pDomainAssembly);
-    pData->vmAppDomain.SetHostPtr(pAppDomain);
-}
-void DacDbiInterfaceImpl::GetModuleData(VMPTR_Module vmModule, ModuleInfo * pData)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(pData != NULL);
-    ZeroMemory(pData, sizeof(*pData));
-    Module     * pModule      = vmModule.GetDacPtr();
-    PEAssembly * pPEAssembly        = pModule->GetPEAssembly();
-    pData->vmPEAssembly.SetHostPtr(pPEAssembly);
-    pData->vmAssembly.SetHostPtr(pModule->GetAssembly());
-    BOOL fIsDynamic = pModule->IsReflection();
-    pData->fIsDynamic = fIsDynamic;
-    pData->pPEBaseAddress = NULL;
-    pData->nPESize = 0;
-    if (!fIsDynamic)
-    {
-        COUNT_T size = 0;
-        pData->pPEBaseAddress = PTR_TO_TADDR(pPEAssembly->GetDebuggerContents(&size));
-        pData->nPESize = (ULONG) size;
-    }
-    pData->fInMemory = FALSE;
-    if (pPEAssembly != NULL)
-    {
-        pData->fInMemory = pPEAssembly->GetPath().IsEmpty();
-    }
-}
-void DacDbiInterfaceImpl::EnumerateAppDomains(
-    FP_APPDOMAIN_ENUMERATION_CALLBACK fpCallback,
-    void * pUserData)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(fpCallback != NULL);
-    AppDomain * pAppDomain = AppDomain::GetCurrentDomain();
-    VMPTR_AppDomain vmAppDomain = VMPTR_AppDomain::NullPtr();
-    vmAppDomain.SetHostPtr(pAppDomain);
-    fpCallback(vmAppDomain, pUserData);
-}
-void  DacDbiInterfaceImpl::EnumerateAssembliesInAppDomain(
-    VMPTR_AppDomain vmAppDomain,
-    FP_ASSEMBLY_ENUMERATION_CALLBACK fpCallback,
-    void * pUserData
-)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(fpCallback != NULL);
-    AppDomain::AssemblyIterator iterator;
-    AppDomain * pAppDomain = vmAppDomain.GetDacPtr();
-    if (pAppDomain == nullptr)
-    {
-        return;
-    }
-    iterator = pAppDomain->IterateAssembliesEx((AssemblyIterationFlags)(kIncludeLoading | kIncludeLoaded | kIncludeExecution));
-    CollectibleAssemblyHolder<DomainAssembly *> pDomainAssembly;
-    while (iterator.Next(pDomainAssembly.This()))
-    {
-        if (!pDomainAssembly->IsVisibleToDebugger())
-        {
-            continue;
-        }
-        VMPTR_DomainAssembly vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
-        vmDomainAssembly.SetHostPtr(pDomainAssembly);
-        fpCallback(vmDomainAssembly, pUserData);
-    }
-}
-void DacDbiInterfaceImpl::EnumerateModulesInAssembly(
-    VMPTR_DomainAssembly vmAssembly,
-    FP_MODULE_ENUMERATION_CALLBACK fpCallback,
-    void * pUserData)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(fpCallback != NULL);
-    DomainAssembly * pDomainAssembly = vmAssembly.GetDacPtr();
-    if (pDomainAssembly->GetModule()->IsVisibleToDebugger())
-    {
-        if (!pDomainAssembly->IsLoaded())
-            return;
-        VMPTR_DomainAssembly vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
-        vmDomainAssembly.SetHostPtr(pDomainAssembly);
-        fpCallback(vmDomainAssembly, pUserData);
-    }
-}
-VMPTR_DomainAssembly DacDbiInterfaceImpl::ResolveAssembly(
-    VMPTR_DomainAssembly vmScope,
-    mdToken tkAssemblyRef)
-{
-    DD_ENTER_MAY_THROW;
-    DomainAssembly * pDomainAssembly  = vmScope.GetDacPtr();
-    AppDomain  * pAppDomain   = pDomainAssembly->GetAppDomain();
-    Module     * pModule      = pDomainAssembly->GetModule();
-    VMPTR_DomainAssembly vmDomainAssembly = VMPTR_DomainAssembly::NullPtr();
-    Assembly * pAssembly = pModule->LookupAssemblyRef(tkAssemblyRef);
-    if (pAssembly != NULL)
-    {
-        DomainAssembly * pDomainAssembly = pAssembly->GetDomainAssembly();
-        vmDomainAssembly.SetHostPtr(pDomainAssembly);
-    }
-    return vmDomainAssembly;
-}
-void DacDbiInterfaceImpl::RequestSyncAtEvent()
-{
-    DD_ENTER_MAY_THROW;
-    if (g_pDebugger != NULL)
-    {
-        TADDR addr = PTR_HOST_MEMBER_TADDR(Debugger, g_pDebugger, m_RSRequestedSync);
-        BOOL fTrue = TRUE;
-        SafeWriteStructOrThrow<BOOL>(addr, &fTrue);
-    }
-}
-HRESULT DacDbiInterfaceImpl::SetSendExceptionsOutsideOfJMC(BOOL sendExceptionsOutsideOfJMC)
-{
-    DD_ENTER_MAY_THROW
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        if (g_pDebugger != NULL)
-        {
-            TADDR addr = PTR_HOST_MEMBER_TADDR(Debugger, g_pDebugger, m_sendExceptionsOutsideOfJMC);
-            SafeWriteStructOrThrow<BOOL>(addr, &sendExceptionsOutsideOfJMC);
-        }
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-void DacDbiInterfaceImpl::MarkDebuggerAttachPending()
-{
-    DD_ENTER_MAY_THROW;
-    if (g_pDebugger != NULL)
-    {
-        DWORD flags = g_CORDebuggerControlFlags;
-        flags |= DBCF_PENDING_ATTACH;
-        g_CORDebuggerControlFlags = flags;
-    }
-    else
-    {
-        ThrowHR(CORDBG_E_NOTREADY);
-    }
-}
-void DacDbiInterfaceImpl::MarkDebuggerAttached(BOOL fAttached)
-{
-    DD_ENTER_MAY_THROW;
-    if (g_pDebugger != NULL)
-    {
-        DWORD flags = g_CORDebuggerControlFlags;
-        if (fAttached)
-        {
-            flags |= DBCF_ATTACHED;
-        }
-        else
-        {
-            flags &= ~ (DBCF_ATTACHED | DBCF_PENDING_ATTACH);
-        }
-        g_CORDebuggerControlFlags = flags;
-    }
-    else if (fAttached)
-    {
-        ThrowHR(CORDBG_E_NOTREADY);
-    }
-}
-void DacDbiInterfaceImpl::EnumerateThreads(FP_THREAD_ENUMERATION_CALLBACK fpCallback, void * pUserData)
-{
-    DD_ENTER_MAY_THROW;
-    if (ThreadStore::s_pThreadStore == NULL)
-    {
-        return;
-    }
-    Thread *pThread = ThreadStore::GetThreadList(NULL);
-    while (pThread != NULL)
-    {
-        Thread::ThreadState threadState = pThread->GetSnapshotState();
-        if (!((IsThreadMarkedDeadWorker(pThread)) || (threadState & Thread::TS_Unstarted)))
-        {
-            VMPTR_Thread vmThread = VMPTR_Thread::NullPtr();
-            vmThread.SetHostPtr(pThread);
-            fpCallback(vmThread, pUserData);
-        }
-        pThread = ThreadStore::GetThreadList(pThread);
-    }
-}
-bool DacDbiInterfaceImpl::IsThreadMarkedDead(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pThread = vmThread.GetDacPtr();
-    return IsThreadMarkedDeadWorker(pThread);
-}
-bool DacDbiInterfaceImpl::IsThreadMarkedDeadWorker(Thread * pThread)
-{
-    _ASSERTE(pThread != NULL);
-    Thread::ThreadState threadState = pThread->GetSnapshotState();
-    bool fIsDead = (threadState & Thread::TS_Dead) != 0;
-    return fIsDead;
-}
-HANDLE DacDbiInterfaceImpl::GetThreadHandle(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pThread = vmThread.GetDacPtr();
-    return pThread->GetThreadHandle();
-}
-VMPTR_OBJECTHANDLE DacDbiInterfaceImpl::GetThreadObject(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pThread = vmThread.GetDacPtr();
-    Thread::ThreadState threadState = pThread->GetSnapshotState();
-    if ( (threadState & Thread::TS_Dead) ||
-         (threadState & Thread::TS_Unstarted) ||
-         (threadState & Thread::TS_Detached) ||
-         g_fProcessDetach )
-    {
-        ThrowHR(CORDBG_E_BAD_THREAD_STATE);
-    }
-    else
-    {
-        VMPTR_OBJECTHANDLE vmObjHandle = VMPTR_OBJECTHANDLE::NullPtr();
-        vmObjHandle.SetDacTargetPtr(pThread->GetExposedObjectHandleForDebugger());
-        return vmObjHandle;
-    }
-}
-void DacDbiInterfaceImpl::GetThreadAllocInfo(VMPTR_Thread        vmThread,
-                                             DacThreadAllocInfo* threadAllocInfo)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pThread = vmThread.GetDacPtr();
-    gc_alloc_context* allocContext = pThread->GetAllocContext();
-    threadAllocInfo->m_allocBytesSOH = allocContext->alloc_bytes - (allocContext->alloc_limit - allocContext->alloc_ptr);
-    threadAllocInfo->m_allocBytesUOH = allocContext->alloc_bytes_uoh;
-}
-void DacDbiInterfaceImpl::SetDebugState(VMPTR_Thread        vmThread,
-                                        CorDebugThreadState debugState)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pThread = vmThread.GetDacPtr();
-    if (debugState == THREAD_SUSPEND)
-    {
-        pThread->SetThreadStateNC(Thread::TSNC_DebuggerUserSuspend);
-    }
-    else if (debugState == THREAD_RUN)
-    {
-        pThread->ResetThreadStateNC(Thread::TSNC_DebuggerUserSuspend);
-    }
-    else
-    {
-        ThrowHR(E_INVALIDARG);
-    }
-    TADDR taThreadState = PTR_HOST_MEMBER_TADDR(Thread, pThread, m_StateNC);
-    SafeWriteStructOrThrow<Thread::ThreadStateNoConcurrency>(taThreadState, &(pThread->m_StateNC));
-}
-BOOL DacDbiInterfaceImpl::HasUnhandledException(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pThread = vmThread.GetDacPtr();
-    if(pThread->IsLastThrownObjectUnhandled())
-    {
-        return TRUE;
-    }
-    OBJECTHANDLE ohException = pThread->GetThrowableAsHandle();
-    if (ohException != NULL)
-    {
-        return pThread->GetExceptionState()->GetFlags()->IsUnhandled() &&
-            !(pThread->GetExceptionState()->GetFlags()->DebuggerInterceptInfo());
-    }
-    return FALSE;
-}
-CorDebugUserState DacDbiInterfaceImpl::GetUserState(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    UINT result = 0;
-    result = GetPartialUserState(vmThread);
-    if (!IsThreadAtGCSafePlace(vmThread))
-    {
-        result |= USER_UNSAFE_POINT;
-    }
-    return (CorDebugUserState)result;
-}
-CONNID DacDbiInterfaceImpl::GetConnectionID(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    return INVALID_CONNECTION_ID;
-}
-TASKID DacDbiInterfaceImpl::GetTaskID(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    return INVALID_TASK_ID;
-}
-DWORD DacDbiInterfaceImpl::TryGetVolatileOSThreadID(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pThread = vmThread.GetDacPtr();
-    _ASSERTE(pThread != NULL);
-    DWORD dwThreadId = pThread->GetOSThreadIdForDebugger();
-    const DWORD dwSwitchedOutThreadId = SWITCHED_OUT_FIBER_OSID;
-    if (dwThreadId == dwSwitchedOutThreadId)
-    {
-        return 0;
-    }
-    return dwThreadId;
-}
-DWORD DacDbiInterfaceImpl::GetUniqueThreadID(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pThread = vmThread.GetDacPtr();
-    _ASSERTE(pThread != NULL);
-    return pThread->GetOSThreadId();
-}
-VMPTR_OBJECTHANDLE DacDbiInterfaceImpl::GetCurrentException(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pThread = vmThread.GetDacPtr();
-    OBJECTHANDLE ohException = pThread->GetThrowableAsHandle();        // ohException can be NULL
-    if (ohException == NULL)
-    {
-        if (pThread->IsLastThrownObjectUnhandled())
-        {
-            ohException = pThread->LastThrownObjectHandle();
-        }
-    }
-    VMPTR_OBJECTHANDLE vmObjHandle;
-    vmObjHandle.SetDacTargetPtr(ohException);
-    return vmObjHandle;
-}
-VMPTR_OBJECTHANDLE DacDbiInterfaceImpl::GetObjectForCCW(CORDB_ADDRESS ccwPtr)
-{
-    DD_ENTER_MAY_THROW;
-    OBJECTHANDLE ohCCW = NULL;
-#ifdef FEATURE_COMWRAPPERS
-    if (DACTryGetComWrappersHandleFromCCW(ccwPtr, &ohCCW) != S_OK)
-    {
-#endif
-#ifdef FEATURE_COMINTEROP
-    ComCallWrapper *pCCW = DACGetCCWFromAddress(ccwPtr);
-    if (pCCW)
-    {
-        ohCCW = pCCW->GetObjectHandle();
-    }
-#endif
-#ifdef FEATURE_COMWRAPPERS
-    }
-#endif
-    VMPTR_OBJECTHANDLE vmObjHandle;
-    vmObjHandle.SetDacTargetPtr(ohCCW);
-    return vmObjHandle;
-}
-VMPTR_OBJECTHANDLE DacDbiInterfaceImpl::GetCurrentCustomDebuggerNotification(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pThread = vmThread.GetDacPtr();
-    OBJECTHANDLE ohNotification = pThread->GetThreadCurrNotification();        // ohNotification can be NULL
-    VMPTR_OBJECTHANDLE vmObjHandle;
-    vmObjHandle.SetDacTargetPtr(ohNotification);
-    return vmObjHandle;
-}
-VMPTR_AppDomain DacDbiInterfaceImpl::GetCurrentAppDomain(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    Thread *    pThread    = vmThread.GetDacPtr();
-    AppDomain * pAppDomain = pThread->GetDomain();
-    if (pAppDomain == NULL)
-    {
-        ThrowHR(E_FAIL);
-    }
-    VMPTR_AppDomain vmAppDomain = VMPTR_AppDomain::NullPtr();
-    vmAppDomain.SetDacTargetPtr(PTR_HOST_TO_TADDR(pAppDomain));
-    return vmAppDomain;
-}
-CLR_DEBUGGING_PROCESS_FLAGS DacDbiInterfaceImpl::GetAttachStateFlags()
-{
-    DD_ENTER_MAY_THROW;
-    CLR_DEBUGGING_PROCESS_FLAGS res = (CLR_DEBUGGING_PROCESS_FLAGS)0;
-    if (g_pDebugger != NULL)
-    {
-        res = g_pDebugger->GetAttachStateFlags();
-    }
-    else
-    {
-    }
-    return res;
-}
-TADDR DacDbiInterfaceImpl::GetHijackAddress()
-{
-    TADDR addr = NULL;
-    if (g_pDebugger != NULL)
-    {
-        addr = dac_cast<TADDR>(g_pDebugger->m_rgHijackFunction[Debugger::kUnhandledException].StartAddress());
-    }
-    if (addr == NULL)
-    {
-        ThrowHR(CORDBG_E_NOTREADY);
-    }
-    return addr;
-}
-bool DacDbiInterfaceImpl::IsRuntimeUnwindableStub(PCODE targetControlPC)
-{
-    TADDR controlPC = PCODEToPINSTR(targetControlPC);
-    if(!m_isCachedHijackFunctionValid)
-    {
-        Debugger* pDebugger = g_pDebugger;
-        if ((pDebugger == NULL) || (pDebugger->m_rgHijackFunction == NULL))
-        {
-            return false;
-        }
-        for (int i = 0; i < Debugger::kMaxHijackFunctions; i++)
-        {
-            InitTargetBufferFromMemoryRange(pDebugger->m_rgHijackFunction[i], &m_pCachedHijackFunction[i] );
-        }
-        m_isCachedHijackFunctionValid = TRUE;
-    }
-    for (int i = 0; i < Debugger::kMaxHijackFunctions; i++)
-    {
-        CORDB_ADDRESS start = m_pCachedHijackFunction[i].pAddress;
-        CORDB_ADDRESS end = start + m_pCachedHijackFunction[i].cbSize;
-        if ((start <= controlPC) && (controlPC < end))
-        {
-            return true;
-        }
-    }
-    return false;
-}
-void DacDbiInterfaceImpl::AlignStackPointer(CORDB_ADDRESS * pEsp)
-{
-    SUPPORTS_DAC;
-#if defined(HOST_64BIT)
-    *pEsp &= ~((CORDB_ADDRESS) 0xF);
-#endif
-}
-template <class T>
-CORDB_ADDRESS DacDbiInterfaceImpl::PushHelper(CORDB_ADDRESS * pEsp,
-                                              const T * pData,
-                                              BOOL fAlignStack)
-{
-    SUPPORTS_DAC;
-    if (fAlignStack == TRUE)
-    {
-        AlignStackPointer(pEsp);
-    }
-    *pEsp -= sizeof(T);
-    if (fAlignStack == TRUE)
-    {
-        AlignStackPointer(pEsp);
-    }
-    SafeWriteStructOrThrow(*pEsp, pData);
-    return *pEsp;
-}
-void DacDbiInterfaceImpl::WriteExceptionRecordHelper(CORDB_ADDRESS pRemotePtr,
-                                                     const EXCEPTION_RECORD * pExcepRecord)
-{
-    ULONG32 cbSize = offsetof(EXCEPTION_RECORD, ExceptionInformation);
-    cbSize += pExcepRecord->NumberParameters * sizeof(pExcepRecord->ExceptionInformation[0]);
-    HRESULT hr = m_pMutableTarget->WriteVirtual(pRemotePtr,
-                                                reinterpret_cast<const BYTE *>(pExcepRecord),
-                                                cbSize);
-    if (FAILED(hr))
-    {
-        ThrowHR(hr);
-    }
-}
-void DacDbiInterfaceImpl::Hijack(
-    VMPTR_Thread                 vmThread,
-    ULONG32                      dwThreadId,
-    const EXCEPTION_RECORD *     pRecord,
-    T_CONTEXT *                  pOriginalContext,
-    ULONG32                      cbSizeContext,
-    EHijackReason::EHijackReason reason,
-    void *                       pUserData,
-    CORDB_ADDRESS *              pRemoteContextAddr)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE((pOriginalContext == NULL) == (cbSizeContext == 0));
-    _ASSERTE(EHijackReason::IsValid(reason));
-#ifdef TARGET_UNIX
-    _ASSERTE(!"Not supported on this platform");
-#endif
-    Thread* pThread = NULL;
-    if(!vmThread.IsNull())
-    {
-        pThread = vmThread.GetDacPtr();
-        _ASSERTE(pThread->GetOSThreadIdForDebugger() == dwThreadId);
-    }
-    TADDR pfnHijackFunction = GetHijackAddress();
-    T_CONTEXT ctx;
-    HRESULT hr = m_pTarget->GetThreadContext(
-        dwThreadId,
-        CONTEXT_FULL,
-        sizeof(DT_CONTEXT),
-        (BYTE*) &ctx);
-    IfFailThrow(hr);
-    if (pOriginalContext != NULL)
-    {
-        if (cbSizeContext != sizeof(T_CONTEXT))
-        {
-            ThrowHR(E_INVALIDARG);
-        }
-        memcpy(pOriginalContext, &ctx, cbSizeContext);
-    }
-#ifndef FEATURE_EMULATE_SINGLESTEP
-    UnsetSSFlag(reinterpret_cast<DT_CONTEXT *>(&ctx));
-#endif
-    void* espContext = NULL;
-    void* espRecord = NULL;
-    const void* pData = pUserData;
-    CORDB_ADDRESS esp = GetSP(&ctx);
-    CORDB_ADDRESS espOSContext = NULL;
-    CORDB_ADDRESS espOSRecord  = NULL;
-    if (pThread != NULL && pThread->IsExceptionInProgress())
-    {
-        espOSContext = (CORDB_ADDRESS)PTR_TO_TADDR(pThread->GetExceptionState()->GetContextRecord());
-        espOSRecord  = (CORDB_ADDRESS)PTR_TO_TADDR(pThread->GetExceptionState()->GetExceptionRecord());
-        if (espOSContext < esp)
-        {
-            SafeWriteStructOrThrow(espOSContext, &ctx);
-            espContext = CORDB_ADDRESS_TO_PTR(espOSContext);
-            _ASSERTE(pRecord != NULL);
-            WriteExceptionRecordHelper(espOSRecord, pRecord);
-            espRecord  = CORDB_ADDRESS_TO_PTR(espOSRecord);
-            esp = min(espOSContext, espOSRecord);
-        }
-    }
-    if (espContext == NULL)
-    {
-        _ASSERTE(espRecord == NULL);
-        espContext = CORDB_ADDRESS_TO_PTR(PushHelper(&esp, &ctx, TRUE));
-        if (pRecord != NULL)
-        {
-            espRecord  = CORDB_ADDRESS_TO_PTR(PushHelper(&esp, pRecord, TRUE));
-        }
-    }
-    if(pRemoteContextAddr != NULL)
-    {
-        *pRemoteContextAddr = PTR_TO_CORDB_ADDRESS(espContext);
-    }
-#if defined(TARGET_X86)  // TARGET
-    PushHelper(&esp, &pData, TRUE);
-    PushHelper(&esp, &reason, TRUE);
-    PushHelper(&esp, &espRecord, TRUE);
-    PushHelper(&esp, &espContext, TRUE);
-#elif defined (TARGET_AMD64) // TARGET
-    ctx.Rcx = (DWORD64) espContext;
-    ctx.Rdx = (DWORD64) espRecord;
-    ctx.R8  = (DWORD64) reason;
-    ctx.R9  = (DWORD64) pData;
-    PushHelper(&esp, reinterpret_cast<SIZE_T *>(&(ctx.R9)), FALSE);
-    PushHelper(&esp, reinterpret_cast<SIZE_T *>(&(ctx.R8)), FALSE);
-    PushHelper(&esp, reinterpret_cast<SIZE_T *>(&(ctx.Rdx)), FALSE);
-    PushHelper(&esp, reinterpret_cast<SIZE_T *>(&(ctx.Rcx)), FALSE);
-#elif defined(TARGET_ARM)
-    ctx.R0 = (DWORD)espContext;
-    ctx.R1 = (DWORD)espRecord;
-    ctx.R2 = (DWORD)reason;
-    ctx.R3 = (DWORD)pData;
-#elif defined(TARGET_ARM64)
-    ctx.X0 = (DWORD64)espContext;
-    ctx.X1 = (DWORD64)espRecord;
-    ctx.X2 = (DWORD64)reason;
-    ctx.X3 = (DWORD64)pData;
-#else
-    PORTABILITY_ASSERT("CordbThread::HijackForUnhandledException is not implemented on this platform.");
-#endif
-    SetSP(&ctx, CORDB_ADDRESS_TO_TADDR(esp));
-    SetIP(&ctx, pfnHijackFunction);
-    hr = m_pMutableTarget->SetThreadContext(dwThreadId, sizeof(DT_CONTEXT), reinterpret_cast<BYTE*> (&ctx));
-    IfFailThrow(hr);
-}
-VMPTR_CONTEXT DacDbiInterfaceImpl::GetManagedStoppedContext(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    VMPTR_CONTEXT vmContext = VMPTR_CONTEXT::NullPtr();
-    Thread * pThread = vmThread.GetDacPtr();
-    if (pThread->GetInteropDebuggingHijacked())
-    {
-        _ASSERTE(!ISREDIRECTEDTHREAD(pThread));
-        vmContext = VMPTR_CONTEXT::NullPtr();
-    }
-    else
-    {
-        DT_CONTEXT * pLSContext = reinterpret_cast<DT_CONTEXT *>(pThread->GetFilterContext());
-        if (pLSContext != NULL)
-        {
-            _ASSERTE(!ISREDIRECTEDTHREAD(pThread));
-            vmContext.SetHostPtr(pLSContext);
-        }
-        else if (ISREDIRECTEDTHREAD(pThread))
-        {
-            pLSContext = reinterpret_cast<DT_CONTEXT *>(GETREDIRECTEDCONTEXT(pThread));
-            _ASSERTE(pLSContext != NULL);
-            if (pLSContext != NULL)
-            {
-                vmContext.SetHostPtr(pLSContext);
-            }
-        }
-    }
-    return vmContext;
-}
-TargetBuffer DacDbiInterfaceImpl::GetVarArgSig(CORDB_ADDRESS   VASigCookieAddr,
-                                               CORDB_ADDRESS * pArgBase)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(pArgBase != NULL);
-    *pArgBase = NULL;
-    TADDR taVASigCookie = NULL;
-    SafeReadStructOrThrow(VASigCookieAddr, &taVASigCookie);
-    VASigCookie * pVACookie = PTR_VASigCookie(taVASigCookie);
-#if defined(TARGET_X86) // (STACK_GROWS_DOWN_ON_ARGS_WALK)
-    *pArgBase = VASigCookieAddr + pVACookie->sizeOfArgs;
-#else  // !TARGET_X86 (STACK_GROWS_UP_ON_ARGS_WALK)
-    *pArgBase = VASigCookieAddr + sizeof(VASigCookie *);
-#endif // !TARGET_X86 (STACK_GROWS_UP_ON_ARGS_WALK)
-    return TargetBuffer(PTR_TO_CORDB_ADDRESS(pVACookie->signature.GetRawSig()),
-                        pVACookie->signature.GetRawSigLen());
-}
-BOOL DacDbiInterfaceImpl::RequiresAlign8(VMPTR_TypeHandle thExact)
-{
-    DD_ENTER_MAY_THROW;
-#ifdef FEATURE_64BIT_ALIGNMENT
-    TypeHandle th = TypeHandle::FromPtr(thExact.GetDacPtr());
-    PTR_MethodTable mt = th.AsMethodTable();
-    return mt->RequiresAlign8();
-#else
-    ThrowHR(E_NOTIMPL);
-#endif
-}
-GENERICS_TYPE_TOKEN DacDbiInterfaceImpl::ResolveExactGenericArgsToken(DWORD               dwExactGenericArgsTokenIndex,
-                                                                      GENERICS_TYPE_TOKEN rawToken)
-{
-    DD_ENTER_MAY_THROW;
-    if (dwExactGenericArgsTokenIndex == 0)
-    {
-        if (rawToken == 0)
-        {
-            return rawToken;
-        }
-        TADDR addrObjThis = CORDB_ADDRESS_TO_TADDR(rawToken);
-        PTR_Object pObjThis = dac_cast<PTR_Object>(addrObjThis);
-        PTR_MethodTable pMT = pObjThis->GetMethodTable();
-        TADDR addrMT = dac_cast<TADDR>(pMT);
-        GENERICS_TYPE_TOKEN realToken = (GENERICS_TYPE_TOKEN) addrMT;
-        return realToken;
-    }
-    else if (dwExactGenericArgsTokenIndex == (DWORD)ICorDebugInfo::TYPECTXT_ILNUM)
-    {
-        return  rawToken;
-    }
-    _ASSERTE(!"DDII::REGAT - Unexpected generics type token index.");
-    ThrowHR(CORDBG_E_TARGET_INCONSISTENT);
-}
-IDacDbiInterface::DynamicMethodType DacDbiInterfaceImpl::IsILStubOrLCGMethod(VMPTR_MethodDesc vmMethodDesc)
-{
-    DD_ENTER_MAY_THROW;
-    MethodDesc * pMD = vmMethodDesc.GetDacPtr();
-    if (pMD->IsILStub())
-    {
-        return kILStub;
-    }
-    else if (pMD->IsLCGMethod())
-    {
-        return kLCGMethod;
-    }
-    else
-    {
-        return kNone;
-    }
-}
-BOOL DacDbiInterfaceImpl::IsThreadAtGCSafePlace(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    BOOL fIsGCSafe = FALSE;
-    Thread * pThread = vmThread.GetDacPtr();
-    if ((g_fEEShutDown & ShutDown_Finalize2) != 0)
-    {
-        fIsGCSafe = TRUE;
-    }
-    else
-    {
-        T_CONTEXT ctx;
-        REGDISPLAY rd;
-        SetUpRegdisplayForStackWalk(pThread, &ctx, &rd);
-        ULONG32 flags = (QUICKUNWIND | HANDLESKIPPEDFRAMES | DISABLE_MISSING_FRAME_DETECTION);
-        StackFrameIterator iter;
-        iter.Init(pThread, pThread->GetFrame(), &rd, flags);
-        CrawlFrame * pCF = &(iter.m_crawl);
-        if (pCF->IsFrameless() && pCF->IsActiveFunc())
-        {
-            if (pCF->IsGcSafe())
-            {
-                fIsGCSafe = TRUE;
-            }
-        }
-    }
-    return fIsGCSafe;
-}
-CorDebugUserState DacDbiInterfaceImpl::GetPartialUserState(VMPTR_Thread vmThread)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pThread = vmThread.GetDacPtr();
-    Thread::ThreadState ts = pThread->GetSnapshotState();
-    UINT result = 0;
-    if (ts & Thread::TS_Background)
-    {
-        result |= USER_BACKGROUND;
-    }
-    if (ts & Thread::TS_Unstarted)
-    {
-        result |= USER_UNSTARTED;
-    }
-    if (ts & Thread::TS_Dead)
-    {
-        result |= USER_STOPPED;
-    }
-    if (ts & Thread::TS_Interruptible || pThread->HasThreadStateNC(Thread::TSNC_DebuggerSleepWaitJoin))
-    {
-        result |= USER_WAIT_SLEEP_JOIN;
-    }
-    if (pThread->IsThreadPoolThread())
-    {
-        result |= USER_THREADPOOL;
-    }
-    return (CorDebugUserState)result;
-}
-void DacDbiInterfaceImpl::LookupEnCVersions(Module*          pModule,
-                                            VMPTR_MethodDesc vmMethodDesc,
-                                            mdMethodDef      mdMethod,
-                                            CORDB_ADDRESS    pNativeStartAddress,
-                                            SIZE_T *         pLatestEnCVersion,
-                                            SIZE_T *         pJittedInstanceEnCVersion /* = NULL */)
-{
-    MethodDesc * pMD     = vmMethodDesc.GetDacPtr();
-    _ASSERTE(pMD->GetMemberDef() == mdMethod);
-    _ASSERTE(pLatestEnCVersion != NULL);
-    DebuggerMethodInfo * pDMI = NULL;
-    DebuggerJitInfo * pDJI = NULL;
-    EX_TRY_ALLOW_DATATARGET_MISSING_MEMORY
-    {
-        if (g_pDebugger != NULL)
-        {
-            pDMI = g_pDebugger->GetOrCreateMethodInfo(pModule, mdMethod);
-            if (pDMI != NULL)
-            {
-                pDJI = pDMI->FindJitInfo(pMD, CORDB_ADDRESS_TO_TADDR(pNativeStartAddress));
-            }
-        }
-    }
-    EX_END_CATCH_ALLOW_DATATARGET_MISSING_MEMORY;
-    if (pDJI != NULL)
-    {
-        if (pJittedInstanceEnCVersion != NULL)
-        {
-            *pJittedInstanceEnCVersion = pDJI->m_encVersion;
-        }
-        *pLatestEnCVersion = pDMI->GetCurrentEnCVersion();
-    }
-    else
-    {
-        if (pJittedInstanceEnCVersion != NULL)
-        {
-            *pJittedInstanceEnCVersion = CorDB_DEFAULT_ENC_FUNCTION_VERSION;
-        }
-        *pLatestEnCVersion = CorDB_DEFAULT_ENC_FUNCTION_VERSION;
-    }
-}
-CORDB_ADDRESS DacDbiInterfaceImpl::GetDebuggerControlBlockAddress()
-{
-    DD_ENTER_MAY_THROW;
-    if ((g_pDebugger != NULL) &&
-        (g_pDebugger->m_pRCThread != NULL))
-    {
-    return CORDB_ADDRESS(dac_cast<TADDR>(g_pDebugger->m_pRCThread->GetDCB()));
-    }
-    return NULL;
-}
-void DacDbiInterfaceImpl::GetContext(VMPTR_Thread vmThread, DT_CONTEXT * pContextBuffer)
-{
-    DD_ENTER_MAY_THROW
-    _ASSERTE(pContextBuffer != NULL);
-    Thread *  pThread  = vmThread.GetDacPtr();
-    DT_CONTEXT * pFilterContext = reinterpret_cast<DT_CONTEXT *>(pThread->GetFilterContext());
-    if (pFilterContext == NULL)
-    {
-        pContextBuffer->ContextFlags = DT_CONTEXT_ALL;
-        HRESULT hr = m_pTarget->GetThreadContext(pThread->GetOSThreadId(),
-                                                pContextBuffer->ContextFlags,
-                                                sizeof(DT_CONTEXT),
-                                                reinterpret_cast<BYTE *>(pContextBuffer));
-        if (hr == E_NOTIMPL)
-        {
-            REGDISPLAY tmpRd = {};
-            T_CONTEXT tmpContext = {};
-            FillRegDisplay(&tmpRd, &tmpContext);
-            Frame *frame = pThread->GetFrame();
-            while (frame != NULL && frame != FRAME_TOP)
-            {
-                frame->UpdateRegDisplay(&tmpRd);
-                if (GetRegdisplaySP(&tmpRd) != 0 && GetControlPC(&tmpRd) != 0)
-                {
-                    UpdateContextFromRegDisp(&tmpRd, &tmpContext);
-                    CopyMemory(pContextBuffer, &tmpContext, sizeof(*pContextBuffer));
-                    pContextBuffer->ContextFlags = DT_CONTEXT_CONTROL;
-                    return;
-                }
-                frame = frame->Next();
-            }
-            ZeroMemory(pContextBuffer, sizeof(*pContextBuffer));
-        }
-        else
-        {
-            IfFailThrow(hr);
-        }
-    }
-    else
-    {
-        *pContextBuffer = *pFilterContext;
-    }
-} // DacDbiInterfaceImpl::GetContext
-VMPTR_Object DacDbiInterfaceImpl::GetObject(CORDB_ADDRESS ptr)
-{
-    DD_ENTER_MAY_THROW;
-    VMPTR_Object vmObj = VMPTR_Object::NullPtr();
-    vmObj.SetDacTargetPtr(CORDB_ADDRESS_TO_TADDR(ptr));
-    return vmObj;
-}
-HRESULT DacDbiInterfaceImpl::EnableNGENPolicy(CorDebugNGENPolicy ePolicy)
-{
-    return E_NOTIMPL;
-}
-HRESULT DacDbiInterfaceImpl::SetNGENCompilerFlags(DWORD dwFlags)
-{
-    DD_ENTER_MAY_THROW;
-    return CORDBG_E_NGEN_NOT_SUPPORTED;
-}
-HRESULT DacDbiInterfaceImpl::GetNGENCompilerFlags(DWORD *pdwFlags)
-{
-    DD_ENTER_MAY_THROW;
-    return CORDBG_E_NGEN_NOT_SUPPORTED;
-}
-typedef DPTR(OBJECTREF) PTR_ObjectRef;
-VMPTR_Object DacDbiInterfaceImpl::GetObjectFromRefPtr(CORDB_ADDRESS ptr)
-{
-    DD_ENTER_MAY_THROW;
-    VMPTR_Object vmObj = VMPTR_Object::NullPtr();
-    PTR_ObjectRef objRef = PTR_ObjectRef(CORDB_ADDRESS_TO_TADDR(ptr));
-    vmObj.SetDacTargetPtr(PTR_TO_TADDR(*objRef));
-    return vmObj;
-}
-VMPTR_OBJECTHANDLE DacDbiInterfaceImpl::GetVmObjectHandle(CORDB_ADDRESS handleAddress)
-{
-    DD_ENTER_MAY_THROW;
-    VMPTR_OBJECTHANDLE vmObjHandle = VMPTR_OBJECTHANDLE::NullPtr();
-    vmObjHandle.SetDacTargetPtr(CORDB_ADDRESS_TO_TADDR(handleAddress));
-    return vmObjHandle;
-}
-BOOL DacDbiInterfaceImpl::IsVmObjectHandleValid(VMPTR_OBJECTHANDLE vmHandle)
-{
-    DD_ENTER_MAY_THROW;
-    BOOL ret = FALSE;
-    EX_TRY
-    {
-        OBJECTREF objRef = ObjectFromHandle((OBJECTHANDLE)vmHandle.GetDacPtr());
-        if (objRef != NULL)
-        {
-            if (objRef->ValidateObjectWithPossibleAV())
-            {
-                ret = TRUE;
-            }
-        }
-    }
-    EX_CATCH
-    {
-    }
-    EX_END_CATCH(SwallowAllExceptions);
-    return ret;
-}
-HRESULT DacDbiInterfaceImpl::IsWinRTModule(VMPTR_Module vmModule, BOOL& isWinRT)
-{
-    DD_ENTER_MAY_THROW;
-    HRESULT hr = S_OK;
-    isWinRT = FALSE;
-    return hr;
-}
-ULONG DacDbiInterfaceImpl::GetAppDomainIdFromVmObjectHandle(VMPTR_OBJECTHANDLE vmHandle)
-{
-    DD_ENTER_MAY_THROW;
-    return DefaultADID;
-}
-CORDB_ADDRESS DacDbiInterfaceImpl::GetHandleAddressFromVmHandle(VMPTR_OBJECTHANDLE vmHandle)
-{
-    DD_ENTER_MAY_THROW;
-    CORDB_ADDRESS handle = vmHandle.GetDacPtr();
-    return handle;
-}
-TargetBuffer DacDbiInterfaceImpl::GetObjectContents(VMPTR_Object vmObj)
-{
-    DD_ENTER_MAY_THROW;
-    PTR_Object objPtr = vmObj.GetDacPtr();
-    _ASSERTE(objPtr->GetSize() <= 0xffffffff);
-    return TargetBuffer(PTR_TO_TADDR(objPtr), (ULONG)objPtr->GetSize());
-}
-HRESULT DacDbiInterfaceImpl::FastSanityCheckObject(PTR_Object objPtr)
-{
-    CONTRACTL
-    {
-        NOTHROW;
-        GC_NOTRIGGER;
-    }
-    CONTRACTL_END;
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        if (objPtr != NULL)
-        {
-            if (!objPtr->ValidateObjectWithPossibleAV())
-            {
-                LOG((LF_CORDB, LL_INFO10000, "GOI: object methodtable-class invariant doesn't hold.\n"));
-                hr = E_INVALIDARG;
-            }
-        }
-    }
-    EX_CATCH
-    {
-        LOG((LF_CORDB, LL_INFO10000, "GOI: exception indicated ref is bad.\n"));
-        hr = E_INVALIDARG;
-    }
-    EX_END_CATCH(SwallowAllExceptions);
-    return hr;
-}   // DacDbiInterfaceImpl::FastSanityCheckObject
-bool DacDbiInterfaceImpl::CheckRef(PTR_Object objPtr)
-{
-    bool objRefBad = false;
-    if (objPtr == NULL)
-    {
-        LOG((LF_CORDB, LL_INFO10000, "D::GOI: ref is NULL.\n"));
-        objRefBad = true;
-    }
-    else
-    {
-        if (FAILED(FastSanityCheckObject(objPtr)))
-        {
-            LOG((LF_CORDB, LL_INFO10000, "D::GOI: address is not a valid object.\n"));
-            objRefBad = true;
-        }
-    }
-    return objRefBad;
-} // DacDbiInterfaceImpl::CheckRef
-void DacDbiInterfaceImpl::InitObjectData(PTR_Object                objPtr,
-                                         VMPTR_AppDomain           vmAppDomain,
-                                         DebuggerIPCE_ObjectData * pObjectData)
-{
-    _ASSERTE(pObjectData != NULL);
-    VMPTR_TypeHandle vmTypeHandle = VMPTR_TypeHandle::NullPtr();
-    vmTypeHandle.SetDacTargetPtr(objPtr->GetGCSafeTypeHandle().AsTAddr());
-    pObjectData->objSize = objPtr->GetSize();
-    pObjectData->objOffsetToVars = dac_cast<TADDR>((objPtr)->GetData()) - dac_cast<TADDR>(objPtr);
-    TypeHandleToExpandedTypeInfo(AllBoxed, vmAppDomain, vmTypeHandle, &(pObjectData->objTypeData));
-    if (objPtr->GetGCSafeMethodTable() == g_pStringClass)
-    {
-        pObjectData->objTypeData.elementType = ELEMENT_TYPE_STRING;
-        if(pObjectData->objSize < MIN_OBJECT_SIZE)
-        {
-            pObjectData->objSize = PtrAlign(pObjectData->objSize);
-        }
-    }
-} // DacDbiInterfaceImpl::InitObjectData
-void DacDbiInterfaceImpl::GetTypedByRefInfo(CORDB_ADDRESS             pTypedByRef,
-                                            VMPTR_AppDomain           vmAppDomain,
-                                            DebuggerIPCE_ObjectData * pObjectData)
-{
-     DD_ENTER_MAY_THROW;
-    PTR_TypedByRef refAddr = PTR_TypedByRef(TADDR(pTypedByRef));
-    _ASSERTE(refAddr != NULL);
-    _ASSERTE(pObjectData != NULL);
-    TypeHandleToBasicTypeInfo(refAddr->type,
-                              &(pObjectData->typedByrefInfo.typedByrefType),
-                              vmAppDomain.GetDacPtr());
-    CORDB_ADDRESS tempRef = dac_cast<TADDR>(refAddr->data);
-    pObjectData->objRef = CORDB_ADDRESS_TO_PTR(tempRef);
-    LOG((LF_CORDB, LL_INFO10000, "D::GASOI: sending REFANY result: "
-         "ref=0x%08x, cls=0x%08x, mod=0x%p\n",
-         pObjectData->objRef,
-         pObjectData->typedByrefType.metadataToken,
-         pObjectData->typedByrefType.vmDomainAssembly.GetDacPtr()));
-} // DacDbiInterfaceImpl::GetTypedByRefInfo
-void DacDbiInterfaceImpl::GetStringData(CORDB_ADDRESS objectAddress, DebuggerIPCE_ObjectData * pObjectData)
-{
-    DD_ENTER_MAY_THROW;
-    PTR_Object objPtr = PTR_Object(TADDR(objectAddress));
-    LOG((LF_CORDB, LL_INFO10000, "D::GOI: The referent is a string.\n"));
-    if (objPtr->GetGCSafeMethodTable() != g_pStringClass)
-    {
-        ThrowHR(CORDBG_E_TARGET_INCONSISTENT);
-    }
-    PTR_StringObject pStrObj = dac_cast<PTR_StringObject>(objPtr);
-    _ASSERTE(pStrObj != NULL);
-    pObjectData->stringInfo.length = pStrObj->GetStringLength();
-    pObjectData->stringInfo.offsetToStringBase = (UINT_PTR) pStrObj->GetBufferOffset();
-} // DacDbiInterfaceImpl::GetStringData
-void DacDbiInterfaceImpl::GetArrayData(CORDB_ADDRESS objectAddress, DebuggerIPCE_ObjectData * pObjectData)
-{
-    DD_ENTER_MAY_THROW;
-    PTR_Object objPtr = PTR_Object(TADDR(objectAddress));
-    PTR_MethodTable pMT = objPtr->GetGCSafeMethodTable();
-    if (!objPtr->GetGCSafeTypeHandle().IsArray())
-    {
-        LOG((LF_CORDB, LL_INFO10000,
-             "D::GASOI: object should be an array.\n"));
-        pObjectData->objRefBad = true;
-    }
-    else
-    {
-        PTR_ArrayBase arrPtr = dac_cast<PTR_ArrayBase>(objPtr);
-        pObjectData->arrayInfo.rank = arrPtr->GetRank();
-        pObjectData->arrayInfo.componentCount = arrPtr->GetNumComponents();
-        pObjectData->arrayInfo.offsetToArrayBase = arrPtr->GetDataPtrOffset(pMT);
-        if (arrPtr->IsMultiDimArray())
-        {
-            pObjectData->arrayInfo.offsetToUpperBounds = SIZE_T(arrPtr->GetBoundsOffset(pMT));
-            pObjectData->arrayInfo.offsetToLowerBounds = SIZE_T(arrPtr->GetLowerBoundsOffset(pMT));
-        }
-        else
-        {
-            pObjectData->arrayInfo.offsetToUpperBounds = 0;
-            pObjectData->arrayInfo.offsetToLowerBounds = 0;
-        }
-        pObjectData->arrayInfo.elementSize = arrPtr->GetComponentSize();
-        LOG((LF_CORDB, LL_INFO10000, "D::GOI: array info: "
-            "baseOff=%d, lowerOff=%d, upperOff=%d, cnt=%d, rank=%d, rank (2) = %d,"
-             "eleSize=%d, eleType=0x%02x\n",
-             pObjectData->arrayInfo.offsetToArrayBase,
-             pObjectData->arrayInfo.offsetToLowerBounds,
-             pObjectData->arrayInfo.offsetToUpperBounds,
-             pObjectData->arrayInfo.componentCount,
-             pObjectData->arrayInfo.rank,
-             pObjectData->objTypeData.ArrayTypeData.arrayRank,
-             pObjectData->arrayInfo.elementSize,
-             pObjectData->objTypeData.ArrayTypeData.arrayTypeArg.elementType));
-    }
-} // DacDbiInterfaceImpl::GetArrayData
-void DacDbiInterfaceImpl::GetBasicObjectInfo(CORDB_ADDRESS             objectAddress,
-                                             CorElementType            type,
-                                             VMPTR_AppDomain           vmAppDomain,
-                                             DebuggerIPCE_ObjectData * pObjectData)
-{
-    DD_ENTER_MAY_THROW;
-    PTR_Object objPtr = PTR_Object(TADDR(objectAddress));
-    pObjectData->objRefBad = CheckRef(objPtr);
-    if (pObjectData->objRefBad != true)
-    {
-        InitObjectData (objPtr, vmAppDomain, pObjectData);
-    }
-} // DacDbiInterfaceImpl::GetBasicObjectInfo
-struct BlockingObjectUserDataWrapper
-{
-    CALLBACK_DATA pUserData;
-    IDacDbiInterface::FP_BLOCKINGOBJECT_ENUMERATION_CALLBACK fpCallback;
-};
-void EnumerateBlockingObjectsCallback(PTR_DebugBlockingItem obj, VOID* pUserData)
-{
-    BlockingObjectUserDataWrapper* wrapper = (BlockingObjectUserDataWrapper*)pUserData;
-    DacBlockingObject dacObj;
-    dacObj.blockingReason = DacBlockReason_MonitorCriticalSection;
-    dacObj.vmBlockingObject.SetDacTargetPtr(dac_cast<TADDR>(OBJECTREFToObject(obj->pMonitor->GetOwningObject())));
-    dacObj.dwTimeout = obj->dwTimeout;
-    dacObj.vmAppDomain.SetDacTargetPtr(dac_cast<TADDR>(obj->pAppDomain));
-    switch(obj->type)
-    {
-        case DebugBlock_MonitorCriticalSection:
-            dacObj.blockingReason = DacBlockReason_MonitorCriticalSection;
-            break;
-        case DebugBlock_MonitorEvent:
-            dacObj.blockingReason = DacBlockReason_MonitorEvent;
-            break;
-        default:
-            _ASSERTE(!"obj->type has an invalid value");
-            return;
-    }
-    wrapper->fpCallback(dacObj, wrapper->pUserData);
-}
-void DacDbiInterfaceImpl::EnumerateBlockingObjects(VMPTR_Thread                           vmThread,
-                                                   FP_BLOCKINGOBJECT_ENUMERATION_CALLBACK fpCallback,
-                                                   CALLBACK_DATA                          pUserData)
-{
-    DD_ENTER_MAY_THROW;
-    Thread * pThread = vmThread.GetDacPtr();
-    _ASSERTE(pThread != NULL);
-    BlockingObjectUserDataWrapper wrapper;
-    wrapper.fpCallback = fpCallback;
-    wrapper.pUserData = pUserData;
-    pThread->DebugBlockingInfo.VisitBlockingItems((DebugBlockingItemVisitor)EnumerateBlockingObjectsCallback,
-        (VOID*)&wrapper);
-}
-MonitorLockInfo DacDbiInterfaceImpl::GetThreadOwningMonitorLock(VMPTR_Object vmObject)
-{
-    DD_ENTER_MAY_THROW;
-    MonitorLockInfo info;
-    info.lockOwner = VMPTR_Thread::NullPtr();
-    info.acquisitionCount = 0;
-    Object* pObj = vmObject.GetDacPtr();
-    DWORD threadId;
-    DWORD acquisitionCount;
-    if(!pObj->GetHeader()->GetThreadOwningMonitorLock(&threadId, &acquisitionCount))
-    {
-        return info;
-    }
-    Thread *pThread = ThreadStore::GetThreadList(NULL);
-    while (pThread != NULL)
-    {
-        if(pThread->GetThreadId() == threadId)
-        {
-            info.lockOwner.SetDacTargetPtr(PTR_HOST_TO_TADDR(pThread));
-            info.acquisitionCount = acquisitionCount;
-            return info;
-        }
-        pThread = ThreadStore::GetThreadList(pThread);
-    }
-    _ASSERTE(!"A thread should have been found");
-    return info;
-}
-struct ThreadUserDataWrapper
-{
-    CALLBACK_DATA pUserData;
-    IDacDbiInterface::FP_THREAD_ENUMERATION_CALLBACK fpCallback;
-};
-void EnumerateThreadsCallback(PTR_Thread pThread, VOID* pUserData)
-{
-    ThreadUserDataWrapper* wrapper = (ThreadUserDataWrapper*)pUserData;
-    VMPTR_Thread vmThread = VMPTR_Thread::NullPtr();
-    vmThread.SetDacTargetPtr(dac_cast<TADDR>(pThread));
-    wrapper->fpCallback(vmThread, wrapper->pUserData);
-}
-void DacDbiInterfaceImpl::EnumerateMonitorEventWaitList(VMPTR_Object                   vmObject,
-                                                        FP_THREAD_ENUMERATION_CALLBACK fpCallback,
-                                                        CALLBACK_DATA                  pUserData)
-{
-    DD_ENTER_MAY_THROW;
-    Object* pObj = vmObject.GetDacPtr();
-    SyncBlock* psb = pObj->PassiveGetSyncBlock();
-    if(psb == NULL)
-        return;
-    ThreadUserDataWrapper wrapper;
-    wrapper.fpCallback = fpCallback;
-    wrapper.pUserData = pUserData;
-    ThreadQueue::EnumerateThreads(psb, (FP_TQ_THREAD_ENUMERATION_CALLBACK)EnumerateThreadsCallback, (VOID*) &wrapper);
-}
-bool DacDbiInterfaceImpl::AreGCStructuresValid()
-{
-    return true;
-}
-HeapData::HeapData()
-    : YoungestGenPtr(0), YoungestGenLimit(0), Gen0Start(0), Gen0End(0), SegmentCount(0), Segments(0)
-{
-}
-HeapData::~HeapData()
-{
-    if (Segments)
-        delete [] Segments;
-}
-LinearReadCache::LinearReadCache()
-    : mCurrPageStart(0), mPageSize(0), mCurrPageSize(0), mPage(0)
-{
-    SYSTEM_INFO si;
-	GetSystemInfo(&si);
-    mPageSize = si.dwPageSize;
-    mPage = new (nothrow) BYTE[mPageSize];
-}
-LinearReadCache::~LinearReadCache()
-{
-    if (mPage)
-        delete [] mPage;
-}
-bool LinearReadCache::MoveToPage(CORDB_ADDRESS addr)
-{
-    mCurrPageStart = addr - (addr % mPageSize);
-    HRESULT hr = g_dacImpl->m_pTarget->ReadVirtual(mCurrPageStart, mPage, mPageSize, &mCurrPageSize);
-    if (hr != S_OK)
-    {
-        mCurrPageStart = 0;
-        mCurrPageSize = 0;
-        return false;
-    }
-    return true;
-}
-CORDB_ADDRESS DacHeapWalker::HeapStart = 0;
-CORDB_ADDRESS DacHeapWalker::HeapEnd = ~0;
-DacHeapWalker::DacHeapWalker()
-    : mThreadCount(0), mAllocInfo(0), mHeapCount(0), mHeaps(0),
-        mCurrObj(0), mCurrSize(0), mCurrMT(0),
-        mCurrHeap(0), mCurrSeg(0), mStart((TADDR)HeapStart), mEnd((TADDR)HeapEnd)
-{
-}
-DacHeapWalker::~DacHeapWalker()
-{
-    if (mAllocInfo)
-        delete [] mAllocInfo;
-    if (mHeaps)
-        delete [] mHeaps;
-}
-SegmentData *DacHeapWalker::FindSegment(CORDB_ADDRESS obj)
-{
-    for (size_t i = 0; i < mHeapCount; ++i)
-        for (size_t j = 0; j < mHeaps[i].SegmentCount; ++j)
-            if (mHeaps[i].Segments[j].Start <= obj && obj <= mHeaps[i].Segments[j].End)
-                return &mHeaps[i].Segments[j];
-    return NULL;
-}
-HRESULT DacHeapWalker::Next(CORDB_ADDRESS *pValue, CORDB_ADDRESS *pMT, ULONG64 *pSize)
-{
-    if (!HasMoreObjects())
-        return E_FAIL;
-    if (pValue)
-        *pValue = mCurrObj;
-    if (pMT)
-        *pMT = (CORDB_ADDRESS)mCurrMT;
-    if (pSize)
-        *pSize = (ULONG64)mCurrSize;
-    HRESULT hr = MoveToNextObject();
-    return FAILED(hr) ? hr : S_OK;
-}
-HRESULT DacHeapWalker::MoveToNextObject()
-{
-    do
-    {
-        mCurrObj += mCurrSize;
-        bool isGen0 = IsRegionGCEnabled() ? (mHeaps[mCurrHeap].Segments[mCurrSeg].Generation == 0) :
-                                   (mHeaps[mCurrHeap].Gen0Start <= mCurrObj && mHeaps[mCurrHeap].Gen0End > mCurrObj);
-        if (isGen0)
-            CheckAllocAndSegmentRange();
-        if (mCurrObj >= mHeaps[mCurrHeap].Segments[mCurrSeg].End || mCurrObj > mEnd)
-        {
-            HRESULT hr = NextSegment();
-            if (FAILED(hr) || hr == S_FALSE)
-                return hr;
-        }
-        if (!mCache.ReadMT(mCurrObj, &mCurrMT))
-            return E_FAIL;
-        if (!GetSize(mCurrMT, mCurrSize))
-            return E_FAIL;
-    } while (mCurrObj < mStart);
-    _ASSERTE(mStart <= mCurrObj && mCurrObj <= mEnd);
-    return S_OK;
-}
-bool DacHeapWalker::GetSize(TADDR tMT, size_t &size)
-{
-    bool ret = true;
-    EX_TRY
-    {
-        MethodTable *mt = PTR_MethodTable(tMT);
-        size_t cs = mt->GetComponentSize();
-        if (cs)
-        {
-            DWORD tmp = 0;
-            if (mCache.Read(mCurrObj + sizeof(TADDR), &tmp))
-                cs *= tmp;
-            else
-                ret = false;
-        }
-        size = mt->GetBaseSize() + cs;
-        if (mHeaps[mCurrHeap].Segments[mCurrSeg].Generation == 3
-            || mHeaps[mCurrHeap].Segments[mCurrSeg].Generation == 4)
-            size = AlignLarge(size);
-        else
-            size = Align(size);
-        ret &= (0 < size);
-        ret &= ((mCurrObj + size) <= mHeaps[mCurrHeap].Segments[mCurrSeg].End);
-    }
-    EX_CATCH
-    {
-        ret = false;
-    }
-    EX_END_CATCH(SwallowAllExceptions)
-    return ret;
-}
-HRESULT DacHeapWalker::NextSegment()
-{
-    mCurrObj = 0;
-    mCurrMT = 0;
-    mCurrSize = 0;
-    do
-    {
-        do
-        {
-            mCurrSeg++;
-            while (mCurrSeg >= mHeaps[mCurrHeap].SegmentCount)
-            {
-                mCurrSeg = 0;
-                mCurrHeap++;
-                if (mCurrHeap >= mHeapCount)
-                {
-                    return S_FALSE;
-                }
-            }
-        } while (mHeaps[mCurrHeap].Segments[mCurrSeg].Start >= mHeaps[mCurrHeap].Segments[mCurrSeg].End);
-        mCurrObj = mHeaps[mCurrHeap].Segments[mCurrSeg].Start;
-        bool isGen0 = IsRegionGCEnabled() ? (mHeaps[mCurrHeap].Segments[mCurrSeg].Generation == 0) :
-                                   (mHeaps[mCurrHeap].Gen0Start <= mCurrObj && mHeaps[mCurrHeap].Gen0End > mCurrObj);
-        if (isGen0)
-            CheckAllocAndSegmentRange();
-        if (!mCache.ReadMT(mCurrObj, &mCurrMT))
-        {
-            return E_FAIL;
-        }
-        if (!GetSize(mCurrMT, mCurrSize))
-        {
-            return E_FAIL;
-        }
-    } while((mHeaps[mCurrHeap].Segments[mCurrSeg].Start > mEnd) || (mHeaps[mCurrHeap].Segments[mCurrSeg].End < mStart));
-    return S_OK;
-}
-void DacHeapWalker::CheckAllocAndSegmentRange()
-{
-    const size_t MinObjSize = sizeof(TADDR)*3;
-    for (int i = 0; i < mThreadCount; ++i)
-        if (mCurrObj == mAllocInfo[i].Ptr)
-        {
-            mCurrObj = mAllocInfo[i].Limit + Align(MinObjSize);
-            break;
-        }
-    if (mCurrObj == mHeaps[mCurrHeap].YoungestGenPtr)
-    {
-        mCurrObj = mHeaps[mCurrHeap].YoungestGenLimit + Align(MinObjSize);
-    }
-}
-HRESULT DacHeapWalker::Init(CORDB_ADDRESS start, CORDB_ADDRESS end)
-{
-    ThreadStore* threadStore = ThreadStore::s_pThreadStore;
-    if (threadStore != NULL)
-    {
-        int count = (int)threadStore->ThreadCountInEE();
-        mAllocInfo = new (nothrow) AllocInfo[count + 1];
-        if (mAllocInfo == NULL)
-            return E_OUTOFMEMORY;
-        Thread *thread = NULL;
-        int j = 0;
-        for (int i = 0; i < count; ++i)
-        {
-            thread = ThreadStore::GetThreadList(thread);
-            if (thread == NULL)
-                continue;
-            gc_alloc_context *ctx = thread->GetAllocContext();
-            if (ctx == NULL)
-                continue;
-            if ((CORDB_ADDRESS)ctx->alloc_ptr != NULL)
-            {
-                mAllocInfo[j].Ptr = (CORDB_ADDRESS)ctx->alloc_ptr;
-                mAllocInfo[j].Limit = (CORDB_ADDRESS)ctx->alloc_limit;
-                j++;
-            }
-        }
-        if ((&g_global_alloc_context)->alloc_ptr != nullptr)
-        {
-            mAllocInfo[j].Ptr = (CORDB_ADDRESS)(&g_global_alloc_context)->alloc_ptr;
-            mAllocInfo[j].Limit = (CORDB_ADDRESS)(&g_global_alloc_context)->alloc_limit;
-        }
-        mThreadCount = j;
-    }
-#ifdef FEATURE_SVR_GC
-    HRESULT hr = GCHeapUtilities::IsServerHeap() ? InitHeapDataSvr(mHeaps, mHeapCount) : InitHeapDataWks(mHeaps, mHeapCount);
-#else
-    HRESULT hr = InitHeapDataWks(mHeaps, mHeapCount);
-#endif
-    if (SUCCEEDED(hr))
-        hr = Reset(start, end);
-    return hr;
-}
-HRESULT DacHeapWalker::Reset(CORDB_ADDRESS start, CORDB_ADDRESS end)
-{
-    _ASSERTE(mHeaps);
-    _ASSERTE(mHeapCount > 0);
-    _ASSERTE(mHeaps[0].Segments);
-    _ASSERTE(mHeaps[0].SegmentCount > 0);
-    mStart = start;
-    mEnd = end;
-    mCurrObj = mHeaps[0].Segments[0].Start;
-    mCurrMT = 0;
-    mCurrSize = 0;
-    mCurrHeap = 0;
-    mCurrSeg = 0;
-    HRESULT hr = S_OK;
-    if (mCurrObj >= mHeaps[0].Segments[0].End)
-        hr = MoveToNextObject();
-    if (!mCache.ReadMT(mCurrObj, &mCurrMT))
-        return E_FAIL;
-    if (!GetSize(mCurrMT, mCurrSize))
-        return E_FAIL;
-    if (mCurrObj < mStart || mCurrObj > mEnd)
-        hr = MoveToNextObject();
-    return hr;
-}
-HRESULT DacHeapWalker::ListNearObjects(CORDB_ADDRESS obj, CORDB_ADDRESS *pPrev, CORDB_ADDRESS *pContaining, CORDB_ADDRESS *pNext)
-{
-    SegmentData *seg = FindSegment(obj);
-    if (seg == NULL)
-        return E_FAIL;
-    HRESULT hr = Reset(seg->Start, seg->End);
-    if (SUCCEEDED(hr))
-    {
-        CORDB_ADDRESS prev = 0;
-        CORDB_ADDRESS curr = 0;
-        ULONG64 size = 0;
-        bool found = false;
-        while (!found && HasMoreObjects())
-        {
-            prev = curr;
-            hr = Next(&curr, NULL, &size);
-            if (FAILED(hr))
-                break;
-            if (obj >= curr && obj < curr + size)
-                found = true;
-        }
-        if (found)
-        {
-            if (pPrev)
-                *pPrev = prev;
-            if (pContaining)
-                *pContaining = curr;
-            if (pNext)
-            {
-                if (HasMoreObjects())
-                {
-                    hr = Next(&curr, NULL, NULL);
-                    if (SUCCEEDED(hr))
-                        *pNext = curr;
-                }
-                else
-                {
-                    *pNext = 0;
-                }
-            }
-            hr = S_OK;
-        }
-        else if (SUCCEEDED(hr))
-        {
-            hr = E_FAIL;
-        }
-    }
-    return hr;
-}
-HRESULT DacHeapWalker::InitHeapDataWks(HeapData *&pHeaps, size_t &pCount)
-{
-    bool regions = IsRegionGCEnabled();
-    pCount = 1;
-    pHeaps = new (nothrow) HeapData[1];
-    if (pHeaps == NULL)
-        return E_OUTOFMEMORY;
-    dac_generation gen0 = GenerationTableIndex(g_gcDacGlobals->generation_table, 0);
-    dac_generation gen1 = GenerationTableIndex(g_gcDacGlobals->generation_table, 1);
-    dac_generation gen2 = GenerationTableIndex(g_gcDacGlobals->generation_table, 2);
-    dac_generation loh  = GenerationTableIndex(g_gcDacGlobals->generation_table, 3);
-    dac_generation poh  = GenerationTableIndex(g_gcDacGlobals->generation_table, 4);
-    pHeaps[0].YoungestGenPtr = (CORDB_ADDRESS)gen0.allocation_context.alloc_ptr;
-    pHeaps[0].YoungestGenLimit = (CORDB_ADDRESS)gen0.allocation_context.alloc_limit;
-    if (!regions)
-    {
-        pHeaps[0].Gen0Start = (CORDB_ADDRESS)gen0.allocation_start;
-        pHeaps[0].Gen0End = (CORDB_ADDRESS)*g_gcDacGlobals->alloc_allocated;
-        pHeaps[0].Gen1Start = (CORDB_ADDRESS)gen1.allocation_start;
-    }
-    int count = GetSegmentCount(loh.start_segment);
-    count += GetSegmentCount(poh.start_segment);
-    count += GetSegmentCount(gen2.start_segment);
-    if (regions)
-    {
-        count += GetSegmentCount(gen1.start_segment);
-        count += GetSegmentCount(gen0.start_segment);
-    }
-    pHeaps[0].SegmentCount = count;
-    pHeaps[0].Segments = new (nothrow) SegmentData[count];
-    if (pHeaps[0].Segments == NULL)
-        return E_OUTOFMEMORY;
-    DPTR(dac_heap_segment) seg;
-    int i = 0;
-    if (regions)
-    {
-        seg = gen2.start_segment;
-        for (; seg && (i < count); ++i)
-        {
-            pHeaps[0].Segments[i].Generation = seg->flags & HEAP_SEGMENT_FLAGS_READONLY ? CorDebug_NonGC : CorDebug_Gen2;
-            pHeaps[0].Segments[i].Start = (CORDB_ADDRESS)seg->mem;
-            pHeaps[0].Segments[i].End = (CORDB_ADDRESS)seg->allocated;
-            seg = seg->next;
-        }
-        seg = gen1.start_segment;
-        for (; seg && (i < count); ++i)
-        {
-            pHeaps[0].Segments[i].Generation = CorDebug_Gen1;
-            pHeaps[0].Segments[i].Start = (CORDB_ADDRESS)seg->mem;
-            pHeaps[0].Segments[i].End = (CORDB_ADDRESS)seg->allocated;
-            seg = seg->next;
-        }
-        seg = gen0.start_segment;
-        for (; seg && (i < count); ++i)
-        {
-            pHeaps[0].Segments[i].Start = (CORDB_ADDRESS)seg->mem;
-            if (seg.GetAddr() == (TADDR)*g_gcDacGlobals->ephemeral_heap_segment)
-            {
-                pHeaps[0].Segments[i].End = (CORDB_ADDRESS)*g_gcDacGlobals->alloc_allocated;
-                pHeaps[0].EphemeralSegment = i;
-            }
-            else
-            {
-                pHeaps[0].Segments[i].End = (CORDB_ADDRESS)seg->allocated;
-            }
-            pHeaps[0].Segments[i].Generation = CorDebug_Gen0;
-            seg = seg->next;
-        }
-    }
-    else
-    {
-        DPTR(dac_heap_segment) seg = gen2.start_segment;
-        for (; seg && (i < count); ++i)
-        {
-            pHeaps[0].Segments[i].Start = (CORDB_ADDRESS)seg->mem;
-            if (seg.GetAddr() == (TADDR)*g_gcDacGlobals->ephemeral_heap_segment)
-            {
-                pHeaps[0].Segments[i].End = (CORDB_ADDRESS)*g_gcDacGlobals->alloc_allocated;
-                pHeaps[0].Segments[i].Generation = CorDebug_Gen1;
-                pHeaps[0].EphemeralSegment = i;
-            }
-            else
-            {
-                pHeaps[0].Segments[i].End = (CORDB_ADDRESS)seg->allocated;
-                pHeaps[0].Segments[i].Generation = seg->flags & HEAP_SEGMENT_FLAGS_READONLY ? CorDebug_NonGC : CorDebug_Gen2;
-            }
-            seg = seg->next;
-        }
-    }
-    seg = loh.start_segment;
-    for (; seg && (i < count); ++i)
-    {
-        pHeaps[0].Segments[i].Generation = CorDebug_LOH;
-        pHeaps[0].Segments[i].Start = (CORDB_ADDRESS)seg->mem;
-        pHeaps[0].Segments[i].End = (CORDB_ADDRESS)seg->allocated;
-        seg = seg->next;
-    }
-    seg = poh.start_segment;
-    for (; seg && (i < count); ++i)
-    {
-        pHeaps[0].Segments[i].Generation = CorDebug_POH;
-        pHeaps[0].Segments[i].Start = (CORDB_ADDRESS)seg->mem;
-        pHeaps[0].Segments[i].End = (CORDB_ADDRESS)seg->allocated;
-        seg = seg->next;
-    }
-    _ASSERTE(count == i);
-    return S_OK;
-}
- HRESULT DacDbiInterfaceImpl::CreateHeapWalk(IDacDbiInterface::HeapWalkHandle *pHandle)
-{
-    DD_ENTER_MAY_THROW;
-    DacHeapWalker *data = new (nothrow) DacHeapWalker;
-    if (data == NULL)
-        return E_OUTOFMEMORY;
-    HRESULT hr = data->Init();
-    if (SUCCEEDED(hr))
-        *pHandle = reinterpret_cast<HeapWalkHandle>(data);
-    else
-        delete data;
-    return hr;
-}
-void DacDbiInterfaceImpl::DeleteHeapWalk(HeapWalkHandle handle)
-{
-    DD_ENTER_MAY_THROW;
-    DacHeapWalker *data = reinterpret_cast<DacHeapWalker*>(handle);
-    if (data)
-        delete data;
-}
-HRESULT DacDbiInterfaceImpl::WalkHeap(HeapWalkHandle handle,
-                    ULONG count,
-                    OUT COR_HEAPOBJECT * objects,
-                    OUT ULONG *fetched)
-{
-    DD_ENTER_MAY_THROW;
-    if (fetched == NULL)
-        return E_INVALIDARG;
-    DacHeapWalker *walk = reinterpret_cast<DacHeapWalker*>(handle);
-    *fetched = 0;
-    if (!walk->HasMoreObjects())
-        return S_FALSE;
-    CORDB_ADDRESS freeMT = (CORDB_ADDRESS)g_pFreeObjectMethodTable.GetAddr();
-    HRESULT hr = S_OK;
-    CORDB_ADDRESS addr, mt;
-    ULONG64 size;
-    ULONG i = 0;
-    while (i < count && walk->HasMoreObjects())
-    {
-        hr = walk->Next(&addr, &mt, &size);
-        if (FAILED(hr))
-            break;
-        if (mt != freeMT)
-        {
-            objects[i].address = addr;
-            objects[i].type.token1 = mt;
-            objects[i].type.token2 = NULL;
-            objects[i].size = size;
-            i++;
-        }
-    }
-    if (SUCCEEDED(hr))
-        hr = (i < count) ? S_FALSE : S_OK;
-    *fetched = i;
-    return hr;
-}
-HRESULT DacDbiInterfaceImpl::GetHeapSegments(OUT DacDbiArrayList<COR_SEGMENT> *pSegments)
-{
-    DD_ENTER_MAY_THROW;
-    size_t heapCount = 0;
-    HeapData *heaps = 0;
-    bool region = IsRegionGCEnabled();
-#ifdef FEATURE_SVR_GC
-    HRESULT hr = GCHeapUtilities::IsServerHeap() ? DacHeapWalker::InitHeapDataSvr(heaps, heapCount) : DacHeapWalker::InitHeapDataWks(heaps, heapCount);
-#else
-    HRESULT hr = DacHeapWalker::InitHeapDataWks(heaps, heapCount);
-#endif
-    NewArrayHolder<HeapData> _heapHolder = heaps;
-    int total = 0;
-    for (size_t i = 0; i < heapCount; ++i)
-    {
-        total += (int)heaps[i].SegmentCount;
-        if (!region)
-        {
-            total++;
-            const size_t eph = heaps[i].EphemeralSegment;
-            _ASSERTE(eph < heaps[i].SegmentCount);
-            if (heaps[i].Segments[eph].Start != heaps[i].Gen1Start)
-                total++;
-        }
-    }
-    pSegments->Alloc(total);
-    int curr = 0;
-    for (size_t i = 0; i < heapCount; ++i)
-    {
-        _ASSERTE(curr < total);
-        if (!region)
-        {
-            COR_SEGMENT &seg = (*pSegments)[curr++];
-            seg.start = heaps[i].Gen0Start;
-            seg.end = heaps[i].Gen0End;
-            seg.type = CorDebug_Gen0;
-            seg.heap = (ULONG)i;
-        }
-        for (size_t j = 0; j < heaps[i].SegmentCount; ++j)
-        {
-            if (region)
-            {
-                _ASSERTE(curr < total);
-                COR_SEGMENT &seg = (*pSegments)[curr++];
-                seg.start = heaps[i].Segments[j].Start;
-                seg.end = heaps[i].Segments[j].End;
-                seg.type = (CorDebugGenerationTypes)heaps[i].Segments[j].Generation;
-                seg.heap = (ULONG)i;
-            }
-            else if (heaps[i].Segments[j].Generation == 1)
-            {
-                _ASSERTE(heaps[i].Segments[j].Start <= heaps[i].Gen1Start);
-                _ASSERTE(heaps[i].Segments[j].End > heaps[i].Gen1Start);
-                {
-                    _ASSERTE(curr < total);
-                    COR_SEGMENT &seg = (*pSegments)[curr++];
-                    seg.start = heaps[i].Gen1Start;
-                    seg.end = heaps[i].Gen0Start;
-                    seg.type = CorDebug_Gen1;
-                    seg.heap = (ULONG)i;
-                }
-                if (heaps[i].Segments[j].Start != heaps[i].Gen1Start)
-                {
-                    _ASSERTE(curr < total);
-                    COR_SEGMENT &seg = (*pSegments)[curr++];
-                    seg.start = heaps[i].Segments[j].Start;
-                    seg.end = heaps[i].Gen1Start;
-                    seg.type = CorDebug_Gen2;
-                    seg.heap = (ULONG)i;
-                }
-            }
-            else
-            {
-                _ASSERTE(curr < total);
-                COR_SEGMENT &seg = (*pSegments)[curr++];
-                seg.start = heaps[i].Segments[j].Start;
-                seg.end = heaps[i].Segments[j].End;
-                _ASSERTE(heaps[i].Segments[j].Generation <= CorDebug_NonGC);
-                seg.type = (CorDebugGenerationTypes)heaps[i].Segments[j].Generation;
-                seg.heap = (ULONG)i;
-            }
-        }
-    }
-    _ASSERTE(total == curr);
-    return hr;
-}
-bool DacDbiInterfaceImpl::IsValidObject(CORDB_ADDRESS addr)
-{
-    DD_ENTER_MAY_THROW;
-    bool isValid = false;
-    if (addr != 0 && addr != (CORDB_ADDRESS)-1)
-    {
-        EX_TRY
-        {
-            PTR_Object obj(TO_TADDR(addr));
-            PTR_MethodTable mt = obj->GetMethodTable();
-            PTR_EEClass cls = mt->GetClass();
-            if (mt == cls->GetMethodTable())
-                isValid = true;
-            else if (!mt->IsCanonicalMethodTable())
-                isValid = cls->GetMethodTable()->GetClass() == cls;
-        }
-        EX_CATCH
-        {
-            isValid = false;
-        }
-        EX_END_CATCH(SwallowAllExceptions)
-    }
-    return isValid;
-}
-bool DacDbiInterfaceImpl::GetAppDomainForObject(CORDB_ADDRESS addr, OUT VMPTR_AppDomain * pAppDomain,
-                                                OUT VMPTR_Module *pModule, OUT VMPTR_DomainAssembly *pDomainAssembly)
-{
-    DD_ENTER_MAY_THROW;
-    if (addr == 0 || addr == (CORDB_ADDRESS)-1)
-    {
-        return false;
-    }
-    PTR_Object obj(TO_TADDR(addr));
-    MethodTable *mt = obj->GetMethodTable();
-    PTR_Module module = mt->GetModule();
-    PTR_Assembly assembly = module->GetAssembly();
-    BaseDomain *baseDomain = assembly->GetDomain();
-    if (baseDomain->IsAppDomain())
-    {
-        pAppDomain->SetDacTargetPtr(PTR_HOST_TO_TADDR(baseDomain->AsAppDomain()));
-        pModule->SetDacTargetPtr(PTR_HOST_TO_TADDR(module));
-        pDomainAssembly->SetDacTargetPtr(PTR_HOST_TO_TADDR(module->GetDomainAssembly()));
-    }
-    else
-    {
-        return false;
-    }
-    return true;
-}
-HRESULT DacDbiInterfaceImpl::CreateRefWalk(OUT RefWalkHandle * pHandle, BOOL walkStacks, BOOL walkFQ, UINT32 handleWalkMask)
-{
-    DD_ENTER_MAY_THROW;
-    DacRefWalker *walker = new (nothrow) DacRefWalker(this, walkStacks, walkFQ, handleWalkMask, TRUE);
-    if (walker == NULL)
-        return E_OUTOFMEMORY;
-    HRESULT hr = walker->Init();
-    if (FAILED(hr))
-    {
-        delete walker;
-    }
-    else
-    {
-        *pHandle = reinterpret_cast<RefWalkHandle>(walker);
-    }
-    return hr;
-}
-void DacDbiInterfaceImpl::DeleteRefWalk(IN RefWalkHandle handle)
-{
-    DD_ENTER_MAY_THROW;
-    DacRefWalker *walker = reinterpret_cast<DacRefWalker*>(handle);
-    if (walker)
-        delete walker;
-}
-HRESULT DacDbiInterfaceImpl::WalkRefs(RefWalkHandle handle, ULONG count, OUT DacGcReference * objects, OUT ULONG *pFetched)
-{
-    if (objects == NULL || pFetched == NULL)
-        return E_POINTER;
-    DD_ENTER_MAY_THROW;
-    DacRefWalker *walker = reinterpret_cast<DacRefWalker*>(handle);
-    if (!walker)
-        return E_INVALIDARG;
-   return walker->Next(count, objects, pFetched);
-}
-HRESULT DacDbiInterfaceImpl::GetTypeID(CORDB_ADDRESS dbgObj, COR_TYPEID *pID)
-{
-    DD_ENTER_MAY_THROW;
-    TADDR obj[3];
-    ULONG32 read = 0;
-    HRESULT hr = g_dacImpl->m_pTarget->ReadVirtual(dbgObj, (BYTE*)obj, sizeof(obj), &read);
-    if (FAILED(hr))
-        return hr;
-    pID->token1 = (UINT64)(obj[0] & ~1);
-    pID->token2 = 0;
-    return hr;
-}
-HRESULT DacDbiInterfaceImpl::GetTypeIDForType(VMPTR_TypeHandle vmTypeHandle, COR_TYPEID *pID)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(pID != NULL);
-    _ASSERTE(!vmTypeHandle.IsNull());
-    TypeHandle th = TypeHandle::FromPtr(vmTypeHandle.GetDacPtr());
-    PTR_MethodTable pMT = th.GetMethodTable();
-    pID->token1 = pMT.GetAddr();
-    _ASSERTE(pID->token1 != 0);
-    pID->token2 = 0;
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::GetObjectFields(COR_TYPEID id, ULONG32 celt, COR_FIELD *layout, ULONG32 *pceltFetched)
-{
-    if (pceltFetched == NULL)
-        return E_POINTER;
-    if (id.token1 == 0)
-        return CORDBG_E_CLASS_NOT_LOADED;
-    DD_ENTER_MAY_THROW;
-    HRESULT hr = S_OK;
-    TypeHandle typeHandle = TypeHandle::FromPtr(TO_TADDR(id.token1));
-    if (typeHandle.IsTypeDesc())
-        return E_INVALIDARG;
-    ApproxFieldDescIterator fieldDescIterator(typeHandle.AsMethodTable(), ApproxFieldDescIterator::INSTANCE_FIELDS);
-    ULONG32 cFields = fieldDescIterator.Count();
-    if (layout == NULL)
-    {
-        *pceltFetched = cFields;
-        return S_FALSE;
-    }
-    if (celt < cFields)
-    {
-        cFields = celt;
-        hr = S_FALSE;
-    }
-    *pceltFetched = celt;
-    CorElementType componentType = typeHandle.AsMethodTable()->GetInternalCorElementType();
-    BOOL fReferenceType = CorTypeInfo::IsObjRef_NoThrow(componentType);
-    for (ULONG32 i = 0; i < cFields; ++i)
-    {
-        FieldDesc *pField = fieldDescIterator.Next();
-        COR_FIELD* corField = layout + i;
-        corField->token = pField->GetMemberDef();
-        corField->offset = (ULONG32)pField->GetOffset() + (fReferenceType ? Object::GetOffsetOfFirstField() : 0);
-        TypeHandle fieldHandle = pField->LookupFieldTypeHandle();
-        if (fieldHandle.IsNull())
-        {
-            corField->id = {};
-            corField->fieldType = (CorElementType)0;
-        }
-        else if (fieldHandle.IsByRef())
-        {
-            corField->fieldType = ELEMENT_TYPE_BYREF;
-            corField->id.token1 = CoreLibBinder::GetElementType(ELEMENT_TYPE_I).GetAddr();
-            corField->id.token2 = 0;
-        }
-        else
-        {
-            PTR_MethodTable mt = fieldHandle.GetMethodTable();
-            corField->fieldType = mt->GetInternalCorElementType();
-            corField->id.token1 = (ULONG64)mt.GetAddr();
-            corField->id.token2 = 0;
-        }
-    }
-    return hr;
-}
-HRESULT DacDbiInterfaceImpl::GetTypeLayout(COR_TYPEID id, COR_TYPE_LAYOUT *pLayout)
-{
-    if (pLayout == NULL)
-        return E_POINTER;
-    if (id.token1 == 0)
-        return CORDBG_E_CLASS_NOT_LOADED;
-    DD_ENTER_MAY_THROW;
-    PTR_MethodTable mt = PTR_MethodTable(TO_TADDR(id.token1));
-    PTR_MethodTable parentMT = mt->GetParentMethodTable();
-    COR_TYPEID parent = {parentMT.GetAddr(), 0};
-    pLayout->parentID = parent;
-    DWORD size = mt->GetBaseSize();
-    ApproxFieldDescIterator fieldDescIterator(mt, ApproxFieldDescIterator::INSTANCE_FIELDS);
-    pLayout->objectSize = size;
-    pLayout->numFields = fieldDescIterator.Count();
-    CorElementType componentType = mt->IsString() ? ELEMENT_TYPE_STRING : mt->GetInternalCorElementType();
-    pLayout->type = componentType;
-    pLayout->boxOffset = CorTypeInfo::IsObjRef_NoThrow(componentType) ? 0 : sizeof(TADDR);
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::GetArrayLayout(COR_TYPEID id, COR_ARRAY_LAYOUT *pLayout)
-{
-    if (pLayout == NULL)
-        return E_POINTER;
-    if (id.token1 == 0)
-        return CORDBG_E_CLASS_NOT_LOADED;
-    DD_ENTER_MAY_THROW;
-    PTR_MethodTable mt = PTR_MethodTable(TO_TADDR(id.token1));
-    if (!mt->IsStringOrArray())
-        return E_INVALIDARG;
-    if (mt->IsString())
-    {
-        COR_TYPEID token;
-        token.token1 = CoreLibBinder::GetElementType(ELEMENT_TYPE_CHAR).GetAddr();
-        token.token2 = 0;
-        pLayout->componentID = token;
-        pLayout->rankSize = 4;
-        pLayout->numRanks = 1;
-        pLayout->rankOffset = sizeof(TADDR);
-        pLayout->firstElementOffset = sizeof(TADDR) + 4;
-        pLayout->countOffset = sizeof(TADDR);
-        pLayout->componentType = ELEMENT_TYPE_CHAR;
-        pLayout->elementSize = 2;
-    }
-    else
-    {
-        DWORD ranks = mt->GetRank();
-        pLayout->rankSize = 4;
-        pLayout->numRanks = ranks;
-        bool multiDim = (ranks > 1);
-        pLayout->rankOffset = multiDim ? sizeof(TADDR)*2 : sizeof(TADDR);
-        pLayout->countOffset = sizeof(TADDR);
-        pLayout->firstElementOffset = ArrayBase::GetDataPtrOffset(mt);
-        TypeHandle hnd = mt->GetArrayElementTypeHandle();
-        PTR_MethodTable cmt = hnd.GetMethodTable();
-        CorElementType componentType = cmt->GetInternalCorElementType();
-        if ((UINT64)cmt.GetAddr() == (UINT64)g_pStringClass.GetAddr())
-            componentType = ELEMENT_TYPE_STRING;
-        COR_TYPEID token;
-        token.token1 = cmt.GetAddr();  // This could be type handle
-        token.token2 = 0;
-        pLayout->componentID = token;
-        pLayout->componentType = componentType;
-        if (CorTypeInfo::IsObjRef_NoThrow(componentType))
-            pLayout->elementSize = sizeof(TADDR);
-        else if (CorIsPrimitiveType(componentType))
-            pLayout->elementSize = gElementTypeInfo[componentType].m_cbSize;
-        else
-            pLayout->elementSize = cmt->GetNumInstanceFieldBytes();
-    }
-    return S_OK;
-}
-void DacDbiInterfaceImpl::GetGCHeapInformation(COR_HEAPINFO * pHeapInfo)
-{
-    DD_ENTER_MAY_THROW;
-    size_t heapCount = 0;
-    pHeapInfo->areGCStructuresValid = *g_gcDacGlobals->gc_structures_invalid_cnt == 0;
-#ifdef FEATURE_SVR_GC
-    if (GCHeapUtilities::IsServerHeap())
-    {
-        pHeapInfo->gcType = CorDebugServerGC;
-        pHeapInfo->numHeaps = DacGetNumHeaps();
-    }
-    else
-#endif
-    {
-        pHeapInfo->gcType = CorDebugWorkstationGC;
-        pHeapInfo->numHeaps = 1;
-    }
-    pHeapInfo->pointerSize = sizeof(TADDR);
-    pHeapInfo->concurrent = g_pConfig->GetGCconcurrent() ? TRUE : FALSE;
-}
-HRESULT DacDbiInterfaceImpl::GetPEFileMDInternalRW(VMPTR_PEAssembly vmPEAssembly, OUT TADDR* pAddrMDInternalRW)
-{
-    DD_ENTER_MAY_THROW;
-    if (pAddrMDInternalRW == NULL)
-        return E_INVALIDARG;
-    PEAssembly * pPEAssembly = vmPEAssembly.GetDacPtr();
-    *pAddrMDInternalRW = pPEAssembly->GetMDInternalRWAddress();
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::GetReJitInfo(VMPTR_Module vmModule, mdMethodDef methodTk, OUT VMPTR_ReJitInfo* pvmReJitInfo)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(!"You shouldn't be calling this - use GetActiveRejitILCodeVersionNode instead");
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::GetActiveRejitILCodeVersionNode(VMPTR_Module vmModule, mdMethodDef methodTk, OUT VMPTR_ILCodeVersionNode* pVmILCodeVersionNode)
-{
-    DD_ENTER_MAY_THROW;
-    if (pVmILCodeVersionNode == NULL)
-        return E_INVALIDARG;
-#ifdef FEATURE_REJIT
-    PTR_Module pModule = vmModule.GetDacPtr();
-    CodeVersionManager * pCodeVersionManager = pModule->GetCodeVersionManager();
-    ILCodeVersion activeILVersion = pCodeVersionManager->GetActiveILCodeVersion(pModule, methodTk);
-    if (activeILVersion.IsNull() || activeILVersion.IsDefaultVersion() || activeILVersion.GetRejitState() != ILCodeVersion::kStateActive)
-    {
-        pVmILCodeVersionNode->SetDacTargetPtr(0);
-    }
-    else
-    {
-        pVmILCodeVersionNode->SetDacTargetPtr(PTR_TO_TADDR(activeILVersion.AsNode()));
-    }
-#else
-    _ASSERTE(!"You shouldn't be calling this - rejit is not supported in this build");
-    pVmILCodeVersionNode->SetDacTargetPtr(0);
-#endif
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::GetReJitInfo(VMPTR_MethodDesc vmMethod, CORDB_ADDRESS codeStartAddress, OUT VMPTR_ReJitInfo* pvmReJitInfo)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(!"You shouldn't be calling this - use GetNativeCodeVersionNode instead");
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::AreOptimizationsDisabled(VMPTR_Module vmModule, mdMethodDef methodTk, OUT BOOL* pOptimizationsDisabled)
-{
-    DD_ENTER_MAY_THROW;
-#ifdef FEATURE_REJIT
-    PTR_Module pModule = vmModule.GetDacPtr();
-    if (pModule == NULL || pOptimizationsDisabled == NULL || TypeFromToken(methodTk) != mdtMethodDef)
-    {
-        return E_INVALIDARG;
-    }
-    {
-        CodeVersionManager * pCodeVersionManager = pModule->GetCodeVersionManager();
-        ILCodeVersion activeILVersion = pCodeVersionManager->GetActiveILCodeVersion(pModule, methodTk);
-        *pOptimizationsDisabled = activeILVersion.IsDeoptimized();
-    }
-#else
-    pOptimizationsDisabled->SetDacTargetPtr(0);
-#endif
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::GetNativeCodeVersionNode(VMPTR_MethodDesc vmMethod, CORDB_ADDRESS codeStartAddress, OUT VMPTR_NativeCodeVersionNode* pVmNativeCodeVersionNode)
-{
-    DD_ENTER_MAY_THROW;
-    if (pVmNativeCodeVersionNode == NULL)
-        return E_INVALIDARG;
-#ifdef FEATURE_REJIT
-    PTR_MethodDesc pMD = vmMethod.GetDacPtr();
-    CodeVersionManager * pCodeVersionManager = pMD->GetCodeVersionManager();
-    NativeCodeVersion codeVersion = pCodeVersionManager->GetNativeCodeVersion(pMD, (PCODE)codeStartAddress);
-    pVmNativeCodeVersionNode->SetDacTargetPtr(PTR_TO_TADDR(codeVersion.AsNode()));
-#else
-    pVmNativeCodeVersionNode->SetDacTargetPtr(0);
-#endif
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::GetSharedReJitInfo(VMPTR_ReJitInfo vmReJitInfo, OUT VMPTR_SharedReJitInfo* pvmSharedReJitInfo)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(!"You shouldn't be calling this - use GetLCodeVersionNode instead");
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::GetILCodeVersionNode(VMPTR_NativeCodeVersionNode vmNativeCodeVersionNode, VMPTR_ILCodeVersionNode* pVmILCodeVersionNode)
-{
-    DD_ENTER_MAY_THROW;
-    if (pVmILCodeVersionNode == NULL)
-        return E_INVALIDARG;
-#ifdef FEATURE_REJIT
-    NativeCodeVersionNode* pNativeCodeVersionNode = vmNativeCodeVersionNode.GetDacPtr();
-    ILCodeVersion ilCodeVersion = pNativeCodeVersionNode->GetILCodeVersion();
-    if (ilCodeVersion.IsDefaultVersion())
-    {
-        pVmILCodeVersionNode->SetDacTargetPtr(0);
-    }
-    else
-    {
-        pVmILCodeVersionNode->SetDacTargetPtr(PTR_TO_TADDR(ilCodeVersion.AsNode()));
-    }
-#else
-    _ASSERTE(!"You shouldn't be calling this - rejit is not supported in this build");
-    pVmILCodeVersionNode->SetDacTargetPtr(0);
-#endif
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::GetSharedReJitInfoData(VMPTR_SharedReJitInfo vmSharedReJitInfo, DacSharedReJitInfo* pData)
-{
-    DD_ENTER_MAY_THROW;
-    _ASSERTE(!"You shouldn't be calling this - use GetILCodeVersionNodeData instead");
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::GetILCodeVersionNodeData(VMPTR_ILCodeVersionNode vmILCodeVersionNode, DacSharedReJitInfo* pData)
-{
-    DD_ENTER_MAY_THROW;
-#ifdef FEATURE_REJIT
-    ILCodeVersion ilCode(vmILCodeVersionNode.GetDacPtr());
-    pData->m_state = ilCode.GetRejitState();
-    pData->m_pbIL = PTR_TO_CORDB_ADDRESS(dac_cast<ULONG_PTR>(ilCode.GetIL()));
-    pData->m_dwCodegenFlags = ilCode.GetJitFlags();
-    const InstrumentedILOffsetMapping* pMapping = ilCode.GetInstrumentedILMap();
-    if (pMapping)
-    {
-        pData->m_cInstrumentedMapEntries = (ULONG)pMapping->GetCount();
-        pData->m_rgInstrumentedMapEntries = PTR_TO_CORDB_ADDRESS(dac_cast<ULONG_PTR>(pMapping->GetOffsets()));
-    }
-    else
-    {
-        pData->m_cInstrumentedMapEntries = 0;
-        pData->m_rgInstrumentedMapEntries = 0;
-    }
-#else
-    _ASSERTE(!"You shouldn't be calling this - rejit isn't supported in this build");
-#endif
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::GetDefinesBitField(ULONG32 *pDefines)
-{
-    DD_ENTER_MAY_THROW;
-    if (pDefines == NULL)
-        return E_INVALIDARG;
-    if (g_pDebugger == NULL)
-        return CORDBG_E_NOTREADY;
-    *pDefines = g_pDebugger->m_defines;
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::GetMDStructuresVersion(ULONG32* pMDStructuresVersion)
-{
-    DD_ENTER_MAY_THROW;
-    if (pMDStructuresVersion == NULL)
-        return E_INVALIDARG;
-    if (g_pDebugger == NULL)
-        return CORDBG_E_NOTREADY;
-    *pMDStructuresVersion = g_pDebugger->m_mdDataStructureVersion;
-    return S_OK;
-}
-HRESULT DacDbiInterfaceImpl::EnableGCNotificationEvents(BOOL fEnable)
-{
-    DD_ENTER_MAY_THROW
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        if (g_pDebugger != NULL)
-        {
-            TADDR addr = PTR_HOST_MEMBER_TADDR(Debugger, g_pDebugger, m_isGarbageCollectionEventsEnabled);
-            SafeWriteStructOrThrow<BOOL>(addr, &fEnable);
-        }
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-DacRefWalker::DacRefWalker(ClrDataAccess *dac, BOOL walkStacks, BOOL walkFQ, UINT32 handleMask, BOOL resolvePointers)
-    : mDac(dac), mWalkStacks(walkStacks), mWalkFQ(walkFQ), mHandleMask(handleMask), mStackWalker(NULL),
-      mResolvePointers(resolvePointers), mHandleWalker(NULL), mFQStart(PTR_NULL), mFQEnd(PTR_NULL), mFQCurr(PTR_NULL)
-{
-}
-DacRefWalker::~DacRefWalker()
-{
-    Clear();
-}
-HRESULT DacRefWalker::Init()
-{
-    HRESULT hr = S_OK;
-    if (mHandleMask)
-    {
-        mHandleWalker = new DacHandleWalker();
-        hr = mHandleWalker->Init(GetHandleWalkerMask());
-    }
-    if (mWalkStacks && SUCCEEDED(hr))
-    {
-        hr = NextThread();
-    }
-    return hr;
-}
-void DacRefWalker::Clear()
-{
-    if (mHandleWalker)
-    {
-        delete mHandleWalker;
-        mHandleWalker = NULL;
-    }
-    if (mStackWalker)
-    {
-        delete mStackWalker;
-        mStackWalker = NULL;
-    }
-}
-UINT32 DacRefWalker::GetHandleWalkerMask()
-{
-    UINT32 result = 0;
-    if (mHandleMask & CorHandleStrong)
-        result |= (1 << HNDTYPE_STRONG);
-    if (mHandleMask & CorHandleStrongPinning)
-        result |= (1 << HNDTYPE_PINNED);
-    if (mHandleMask & CorHandleWeakShort)
-        result |= (1 << HNDTYPE_WEAK_SHORT);
-    if (mHandleMask & CorHandleWeakLong)
-        result |= (1 << HNDTYPE_WEAK_LONG);
-#if defined(FEATURE_COMINTEROP) || defined(FEATURE_COMWRAPPERS) || defined(FEATURE_OBJCMARSHAL)
-    if ((mHandleMask & CorHandleWeakRefCount) || (mHandleMask & CorHandleStrongRefCount))
-        result |= (1 << HNDTYPE_REFCOUNTED);
-#endif // FEATURE_COMINTEROP || FEATURE_COMWRAPPERS || FEATURE_OBJCMARSHAL
-    if (mHandleMask & CorHandleStrongDependent)
-        result |= (1 << HNDTYPE_DEPENDENT);
-    if (mHandleMask & CorHandleStrongAsyncPinned)
-        result |= (1 << HNDTYPE_ASYNCPINNED);
-    if (mHandleMask & CorHandleStrongSizedByref)
-        result |= (1 << HNDTYPE_SIZEDREF);
-    return result;
-}
-HRESULT DacRefWalker::Next(ULONG celt, DacGcReference roots[], ULONG *pceltFetched)
-{
-    if (roots == NULL || pceltFetched == NULL)
-        return E_POINTER;
-    ULONG total = 0;
-    HRESULT hr = S_OK;
-    if (mHandleWalker)
-    {
-        hr = mHandleWalker->Next(celt, roots, &total);
-        if (total == 0 || FAILED(hr))
-        {
-            delete mHandleWalker;
-            mHandleWalker = NULL;
-            if (FAILED(hr))
-                return hr;
-        }
-    }
-    if (total < celt)
-    {
-        while (total < celt && mFQCurr < mFQEnd)
-        {
-            DacGcReference &ref = roots[total++];
-            ref.vmDomain = VMPTR_AppDomain::NullPtr();
-            ref.objHnd.SetDacTargetPtr(mFQCurr.GetAddr());
-            ref.dwType = (DWORD)CorReferenceFinalizer;
-            ref.i64ExtraData = 0;
-            mFQCurr++;
-        }
-    }
-    while (total < celt && mStackWalker)
-    {
-        ULONG fetched = 0;
-        hr = mStackWalker->Next(celt-total, roots+total, &fetched);
-        if (FAILED(hr))
-            return hr;
-        if (fetched == 0)
-        {
-            hr = NextThread();
-            if (FAILED(hr))
-                return hr;
-        }
-        total += fetched;
-    }
-    *pceltFetched = total;
-    return total < celt ? S_FALSE : S_OK;
-}
-HRESULT DacRefWalker::NextThread()
-{
-    Thread *pThread = NULL;
-    if (mStackWalker)
-    {
-        pThread = mStackWalker->GetThread();
-        delete mStackWalker;
-        mStackWalker = NULL;
-    }
-    pThread = ThreadStore::GetThreadList(pThread);
-    if (!pThread)
-        return S_FALSE;
-    mStackWalker = new DacStackReferenceWalker(mDac, pThread->GetOSThreadId(), mResolvePointers == TRUE);
-    return mStackWalker->Init();
-}
-HRESULT DacHandleWalker::Next(ULONG count, DacGcReference roots[], ULONG *pFetched)
-{
-    SUPPORTS_DAC;
-    if (roots == NULL || pFetched == NULL)
-        return E_POINTER;
-    if (!mEnumerated)
-        WalkHandles();
-    unsigned int i;
-    for (i = 0; i < count && mIteratorIndex < mList.GetCount(); mIteratorIndex++, i++)
-    {
-        const SOSHandleData &handle = mList.Get(mIteratorIndex);
-        roots[i].objHnd.SetDacTargetPtr(TO_TADDR(handle.Handle));
-        roots[i].vmDomain.SetDacTargetPtr(TO_TADDR(handle.AppDomain));
-        roots[i].i64ExtraData = 0;
-        unsigned int refCnt = 0;
-        switch (handle.Type)
-        {
-            case HNDTYPE_STRONG:
-                roots[i].dwType = (DWORD)CorHandleStrong;
-                break;
-            case HNDTYPE_PINNED:
-                roots[i].dwType = (DWORD)CorHandleStrongPinning;
-                break;
-            case HNDTYPE_WEAK_SHORT:
-                roots[i].dwType = (DWORD)CorHandleWeakShort;
-                break;
-            case HNDTYPE_WEAK_LONG:
-                roots[i].dwType = (DWORD)CorHandleWeakLong;
-                break;
-        #if defined(FEATURE_COMINTEROP) || defined(FEATURE_COMWRAPPERS) || defined(FEATURE_OBJCMARSHAL)
-            case HNDTYPE_REFCOUNTED:
-                GetRefCountedHandleInfo((OBJECTREF)CLRDATA_ADDRESS_TO_TADDR(handle.Handle), handle.Type, &refCnt, NULL, NULL, NULL);
-                roots[i].i64ExtraData = refCnt;
-                roots[i].dwType = (DWORD)(roots[i].i64ExtraData ? CorHandleStrongRefCount : CorHandleWeakRefCount);
-                break;
-        #endif // FEATURE_COMINTEROP || FEATURE_COMWRAPPERS || FEATURE_OBJCMARSHAL
-            case HNDTYPE_DEPENDENT:
-                roots[i].dwType = (DWORD)CorHandleStrongDependent;
-                roots[i].i64ExtraData = GetDependentHandleSecondary(CLRDATA_ADDRESS_TO_TADDR(handle.Handle)).GetAddr();
-                break;
-            case HNDTYPE_ASYNCPINNED:
-                roots[i].dwType = (DWORD)CorHandleStrongAsyncPinned;
-                break;
-            case HNDTYPE_SIZEDREF:
-                roots[i].dwType = (DWORD)CorHandleStrongSizedByref;
-                break;
-        }
-    }
-    *pFetched = i;
-    return (unsigned)mIteratorIndex < mList.GetCount() ? S_FALSE : S_OK;
-}
-HRESULT DacStackReferenceWalker::Next(ULONG count, DacGcReference stackRefs[], ULONG *pFetched)
-{
-    if (stackRefs == NULL || pFetched == NULL)
-        return E_POINTER;
-    if (!mEnumerated)
-        WalkStack();
-    TADDR domain = AppDomain::GetCurrentDomain().GetAddr();
-    unsigned int i;
-    for (i = 0; i < count && mIteratorIndex < mList.GetCount(); mIteratorIndex++, i++)
-    {
-        stackRefs[i].dwType = CorReferenceStack;
-        stackRefs[i].vmDomain.SetDacTargetPtr(domain);
-        stackRefs[i].i64ExtraData = 0;
-        const SOSStackRefData &sosStackRef = mList.Get(i);
-        if (sosStackRef.Flags & GC_CALL_INTERIOR || sosStackRef.Address == 0)
-        {
-            stackRefs[i].pObject = CLRDATA_ADDRESS_TO_TADDR(sosStackRef.Object) | 1;
-        }
-        else
-        {
-            stackRefs[i].objHnd.SetDacTargetPtr(CLRDATA_ADDRESS_TO_TADDR(sosStackRef.Address));
-        }
-    }
-    *pFetched = i;
-    return S_OK;
-}

--- a/src/coreclr/debug/di/module.cpp
+++ b//dev/null
@@ -1,3126 +0,0 @@
-#include "stdafx.h"
-#include "winbase.h"
-#include "metadataexports.h"
-#include "winbase.h"
-#include "corpriv.h"
-#include "corsym.h"
-#include "pedecoder.h"
-#include "stgpool.h"
-STDAPI ReOpenMetaDataWithMemoryEx(
-    void        *pUnk,
-    LPCVOID     pData,
-    ULONG       cbData,
-    DWORD       dwReOpenFlags)
-{
-    HRESULT hr = MDReOpenMetaDataWithMemoryEx(pUnk,pData, cbData, dwReOpenFlags);
-    return hr;
-}
-CordbModule::CordbModule(
-    CordbProcess *     pProcess,
-    VMPTR_Module        vmModule,
-    VMPTR_DomainAssembly    vmDomainAssembly)
-: CordbBase(pProcess, vmDomainAssembly.IsNull() ? VmPtrToCookie(vmModule) : VmPtrToCookie(vmDomainAssembly), enumCordbModule),
-    m_pAssembly(0),
-    m_pAppDomain(0),
-    m_classes(11),
-    m_functions(101),
-    m_vmDomainAssembly(vmDomainAssembly),
-    m_vmModule(vmModule),
-    m_EnCCount(0),
-    m_fForceMetaDataSerialize(FALSE),
-    m_nativeCodeTable(101)
-{
-    _ASSERTE(pProcess->GetProcessLock()->HasLock());
-    _ASSERTE(!vmModule.IsNull());
-    m_nLoadEventContinueCounter = 0;
-#ifdef _DEBUG
-    m_classes.DebugSetRSLock(pProcess->GetProcessLock());
-    m_functions.DebugSetRSLock(pProcess->GetProcessLock());
-#endif
-    ModuleInfo modInfo;
-    pProcess->GetDAC()->GetModuleData(vmModule, &modInfo); // throws
-    m_PEBuffer.Init(modInfo.pPEBaseAddress, modInfo.nPESize);
-    m_fDynamic  = modInfo.fIsDynamic;
-    m_fInMemory = modInfo.fInMemory;
-    m_vmPEFile = modInfo.vmPEAssembly;
-    if (!vmDomainAssembly.IsNull())
-    {
-        DomainAssemblyInfo dfInfo;
-        pProcess->GetDAC()->GetDomainAssemblyData(vmDomainAssembly, &dfInfo); // throws
-        m_pAppDomain = pProcess->LookupOrCreateAppDomain(dfInfo.vmAppDomain);
-        m_pAssembly  = m_pAppDomain->LookupOrCreateAssembly(dfInfo.vmDomainAssembly);
-    }
-    else
-    {
-        m_pAppDomain = pProcess->GetSharedAppDomain();
-        m_pAssembly = m_pAppDomain->LookupOrCreateAssembly(modInfo.vmAssembly);
-    }
-#ifdef _DEBUG
-    m_nativeCodeTable.DebugSetRSLock(GetProcess()->GetProcessLock());
-#endif
-}
-#ifdef _DEBUG
-void DbgAssertModuleDeletedCallback(VMPTR_DomainAssembly vmDomainAssembly, void * pUserData)
-{
-    CordbModule * pThis = reinterpret_cast<CordbModule *>(pUserData);
-    INTERNAL_DAC_CALLBACK(pThis->GetProcess());
-    if (!pThis->m_vmDomainAssembly.IsNull())
-    {
-        VMPTR_DomainAssembly vmDomainAssemblyDeleted = pThis->m_vmDomainAssembly;
-        CONSISTENCY_CHECK_MSGF((vmDomainAssemblyDeleted != vmDomainAssembly),
-            ("A Module Unload event was sent for a module, but it still shows up in the enumeration.\n vmDomainAssemblyDeleted=%p\n",
-            VmPtrToCookie(vmDomainAssemblyDeleted)));
-    }
-}
-void CordbModule::DbgAssertModuleDeleted()
-{
-    GetProcess()->GetDAC()->EnumerateModulesInAssembly(
-        m_pAssembly->GetDomainAssemblyPtr(),
-        DbgAssertModuleDeletedCallback,
-        this);
-}
-#endif // _DEBUG
-CordbModule::~CordbModule()
-{
-    _ASSERTE(IsNeutered());
-    _ASSERTE(m_pIMImport == NULL);
-}
-void CordbModule::Neuter()
-{
-    m_classes.NeuterAndClear(GetProcess()->GetProcessLock());
-    m_functions.NeuterAndClear(GetProcess()->GetProcessLock());
-    m_nativeCodeTable.NeuterAndClear(GetProcess()->GetProcessLock());
-    m_pClass.Clear();
-    m_pInternalMetaDataImport.Clear();
-    m_pIMImport.Clear();
-    CordbBase::Neuter();
-}
-void GetStreamFromTargetBuffer(CordbProcess * pProcess, TargetBuffer buffer, IStream ** ppStream)
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    _ASSERTE(ppStream != NULL);
-    _ASSERTE(*ppStream == NULL);
-    int cbSize = buffer.cbSize;
-    NewArrayHolder<BYTE> localBuffer(new BYTE[cbSize]);
-    pProcess->SafeReadBuffer(buffer, localBuffer);
-    HRESULT hr = E_FAIL;
-    hr = CInMemoryStream::CreateStreamOnMemoryCopy(localBuffer, cbSize, ppStream);
-    IfFailThrow(hr);
-    _ASSERTE(*ppStream != NULL);
-}
-IDacDbiInterface::SymbolFormat CordbModule::GetInMemorySymbolStream(IStream ** ppStream)
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    _ASSERTE(ppStream != NULL);
-    _ASSERTE(*ppStream == NULL);
-    *ppStream = NULL;
-    TargetBuffer bufferPdb;
-    IDacDbiInterface::SymbolFormat symFormat;
-    GetProcess()->GetDAC()->GetSymbolsBuffer(m_vmModule, &bufferPdb, &symFormat);
-    if (bufferPdb.IsEmpty())
-    {
-        _ASSERTE(symFormat == IDacDbiInterface::kSymbolFormatNone);
-        return IDacDbiInterface::kSymbolFormatNone;
-    }
-    else
-    {
-        _ASSERTE(symFormat != IDacDbiInterface::kSymbolFormatNone);
-        GetStreamFromTargetBuffer(GetProcess(), bufferPdb, ppStream);
-        return symFormat;
-    }
-}
-VMPTR_PEAssembly CordbModule::GetPEFile()
-{
-    return m_vmPEFile;
-}
-IMetaDataImport * CordbModule::GetMetaDataImporter()
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    if (m_pIMImport != NULL)
-    {
-        return m_pIMImport;
-    }
-    LOG((LF_CORDB,LL_INFO1000, "CM::GMI Lazy init refreshing metadata\n"));
-    ALLOW_DATATARGET_MISSING_MEMORY(
-        RefreshMetaData();
-    );
-    if (m_pIMImport == NULL)
-    {
-        bool isILMetaDataForNGENImage;  // Not currently used for anything.
-        CordbProcess * pProcess = GetProcess();
-        RSLockHolder processLockHolder(pProcess->GetProcessLock());
-        m_pInternalMetaDataImport.Clear();
-        pProcess->LookupMetaDataFromDebugger(m_vmPEFile, isILMetaDataForNGENImage, this);
-    }
-    if (m_pIMImport == NULL)
-    {
-        ThrowHR(CORDBG_E_MISSING_METADATA);
-    }
-    return m_pIMImport;
-}
-void CordbModule::UpdateMetaDataCacheIfNeeded(mdToken token)
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    LOG((LF_CORDB,LL_INFO10000, "CM::UMCIN token=0x%x\n", token));
-    if(GetProcess()->GetWriteableMetadataUpdateMode() != LegacyCompatPolicy)
-    {
-        return;
-    }
-    if(CheckIfTokenInMetaData(token))
-    {
-        LOG((LF_CORDB,LL_INFO10000, "CM::UMCIN token was present\n"));
-        return;
-    }
-    LOG((LF_CORDB,LL_INFO10000, "CM::UMCIN token was not present, refreshing\n"));
-    m_fForceMetaDataSerialize = TRUE;
-    RefreshMetaData();
-}
-BOOL CordbModule::CheckIfTokenInMetaData(mdToken token)
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    LOG((LF_CORDB,LL_INFO10000, "CM::CITIM token=0x%x\n", token));
-    _ASSERTE(TypeFromToken(token) == mdtSignature);
-    RSExtSmartPtr<IMetaDataTables> pTable;
-    HRESULT hr = GetMetaDataImporter()->QueryInterface(IID_IMetaDataTables, (void**) &pTable);
-    _ASSERTE(SUCCEEDED(hr));
-    if (FAILED(hr))
-    {
-        ThrowHR(hr);
-    }
-    ULONG cbRowsAvailable; // number of rows in the table
-    hr = pTable->GetTableInfo(
-        mdtSignature >> 24,                      // [IN] Which table.
-        NULL,                    // [OUT] Size of a row, bytes.
-        &cbRowsAvailable,                    // [OUT] Number of rows.
-        NULL,                    // [OUT] Number of columns in each row.
-        NULL,                     // [OUT] Key column, or -1 if none.
-        NULL);          // [OUT] Name of the table.
-    _ASSERTE(SUCCEEDED(hr));
-    if (FAILED(hr))
-    {
-        ThrowHR(hr);
-    }
-    ULONG rowRequested = RidFromToken(token);
-    LOG((LF_CORDB,LL_INFO10000, "CM::UMCIN requested=0x%x available=0x%x\n", rowRequested, cbRowsAvailable));
-    return (rowRequested <= cbRowsAvailable);
-}
-class CleanupRemoteBuffer
-{
-public:
-    CordbProcess* pProcess;
-    CordbModule* pModule;
-    TargetBuffer bufferMetaData;
-    BOOL fDoCleanup;
-    CleanupRemoteBuffer() :
-    fDoCleanup(FALSE) { }
-    ~CleanupRemoteBuffer()
-    {
-        if(fDoCleanup)
-        {
-            DebuggerIPCEvent event;
-            pProcess->InitIPCEvent(&event,
-                DB_IPCE_RESOLVE_UPDATE_METADATA_2,
-                true,
-                pModule->GetAppDomain()->GetADToken());
-            event.MetadataUpdateRequest.pMetadataStart = CORDB_ADDRESS_TO_PTR(bufferMetaData.pAddress);
-            IfFailThrow(pProcess->SendIPCEvent(&event, sizeof(DebuggerIPCEvent)));
-            _ASSERTE(event.type == DB_IPCE_RESOLVE_UPDATE_METADATA_2_RESULT);
-        }
-    }
-};
-void CordbModule::RefreshMetaData()
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    LOG((LF_CORDB,LL_INFO1000, "CM::RM\n"));
-    CordbProcess * pProcess = GetProcess();
-    TargetBuffer bufferMetaData;
-    CleanupRemoteBuffer cleanup; // this local has a destructor to do some finally work
-    if (GetProcess()->GetShim() == NULL &&
-        GetProcess()->GetWriteableMetadataUpdateMode() == AlwaysShowUpdates &&
-        !m_fDynamic)
-    {
-        TADDR remoteMDInternalRWAddr = NULL;
-        GetProcess()->GetDAC()->GetPEFileMDInternalRW(m_vmPEFile, &remoteMDInternalRWAddr);
-        if (remoteMDInternalRWAddr != NULL)
-        {
-            _ASSERTE(m_pIMImport == NULL);
-            ULONG32 mdStructuresVersion;
-            HRESULT hr = GetProcess()->GetDAC()->GetMDStructuresVersion(&mdStructuresVersion);
-            IfFailThrow(hr);
-            ULONG32 mdStructuresDefines;
-            hr = GetProcess()->GetDAC()->GetDefinesBitField(&mdStructuresDefines);
-            IfFailThrow(hr);
-            IMetaDataDispenserCustom* pDispCustom = NULL;
-            hr = GetProcess()->GetDispenser()->QueryInterface(IID_IMetaDataDispenserCustom, (void**)&pDispCustom);
-            IfFailThrow(hr);
-            IMDCustomDataSource* pDataSource = NULL;
-            hr = CreateRemoteMDInternalRWSource(remoteMDInternalRWAddr, GetProcess()->GetDataTarget(), mdStructuresDefines, mdStructuresVersion, &pDataSource);
-            IfFailThrow(hr);
-            IMetaDataImport* pImport = NULL;
-            hr = pDispCustom->OpenScopeOnCustomDataSource(pDataSource, 0, IID_IMetaDataImport, (IUnknown**)&m_pIMImport);
-            IfFailThrow(hr);
-            UpdateInternalMetaData();
-            return;
-        }
-    }
-    if(!m_fForceMetaDataSerialize) // case 1 and 2
-    {
-        LOG((LF_CORDB,LL_INFO10000, "CM::RM !m_fForceMetaDataSerialize case\n"));
-        GetProcess()->GetDAC()->GetMetadata(m_vmModule, &bufferMetaData); // throws
-    }
-    else if (GetProcess()->GetShim() == NULL) // case 3 won't work on a dump so don't try
-    {
-        return;
-    }
-    else // case 3 on a live process
-    {
-        LOG((LF_CORDB,LL_INFO10000, "CM::RM m_fForceMetaDataSerialize case\n"));
-        DebuggerIPCEvent event;
-        pProcess->InitIPCEvent(&event,
-            DB_IPCE_RESOLVE_UPDATE_METADATA_1,
-            true,
-            GetAppDomain()->GetADToken());
-        event.MetadataUpdateRequest.vmModule = m_vmModule;
-        IfFailThrow(pProcess->SendIPCEvent(&event, sizeof(DebuggerIPCEvent)));
-        _ASSERTE(event.type == DB_IPCE_RESOLVE_UPDATE_METADATA_1_RESULT);
-        bufferMetaData.Init(PTR_TO_CORDB_ADDRESS(event.MetadataUpdateRequest.pMetadataStart), (ULONG) event.MetadataUpdateRequest.nMetadataSize);
-        cleanup.bufferMetaData = bufferMetaData;
-        cleanup.pProcess = pProcess;
-        cleanup.pModule = this;
-        cleanup.fDoCleanup = TRUE;
-    }
-    InitMetaData(bufferMetaData, IsFileMetaDataValid()); // throws
-}
-BOOL CordbModule::IsFileMetaDataValid()
-{
-    bool fOpenFromFile = true;
-    if (m_fDynamic || m_fInMemory || m_fForceMetaDataSerialize)
-    {
-        LOG((LF_CORDB,LL_INFO10000, "CM::IFMV: m_fDynamic=0x%x m_fInMemory=0x%x m_fForceMetaDataSerialize=0x%x\n",
-            m_fDynamic, m_fInMemory, m_fForceMetaDataSerialize));
-        fOpenFromFile = false;
-    }
-#ifdef _DEBUG
-    static DWORD openFromFile = 99;
-    if (openFromFile == 99)
-        openFromFile = CLRConfig::GetConfigValue(CLRConfig::INTERNAL_DbgNoOpenMDByFile);
-    if (openFromFile)
-    {
-        LOG((LF_CORDB,LL_INFO10000, "CM::IFMV: INTERNAL_DbgNoOpenMDByFile is set\n"));
-        fOpenFromFile = false;
-    }
-#endif
-    LOG((LF_CORDB,LL_INFO10000, "CM::IFMV: returns 0x%x\n", fOpenFromFile));
-    return fOpenFromFile;
-}
-IMDInternalImport * CordbModule::GetInternalMD()
-{
-    if (m_pInternalMetaDataImport == NULL)
-    {
-        UpdateInternalMetaData(); // throws
-    }
-    return m_pInternalMetaDataImport;
-}
-void CordbModule::InitMetaData(TargetBuffer buffer, BOOL allowFileMappingOptimization)
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    LOG((LF_CORDB,LL_INFO100000, "CM::IM: initing with remote buffer 0x%p length 0x%x\n",
-        CORDB_ADDRESS_TO_PTR(buffer.pAddress), buffer.cbSize));
-    m_pInternalMetaDataImport.Clear();
-    if (m_pIMImport == NULL)
-    {
-        HRESULT hr = S_OK;
-        if (allowFileMappingOptimization)
-        {
-            hr = InitPublicMetaDataFromFile();
-            if(FAILED(hr))
-            {
-                LOG((LF_CORDB,LL_INFO1000000, "CM::IPM: File mapping failed with hr=0x%x\n", hr));
-            }
-        }
-        if(!allowFileMappingOptimization || FAILED(hr))
-        {
-            InitPublicMetaData(buffer);
-        }
-    }
-    else
-    {
-        UpdatePublicMetaDataFromRemote(buffer);
-    }
-    _ASSERTE(m_pIMImport != NULL);
-    UpdateInternalMetaData();
-}
-void CordbModule::UpdateInternalMetaData()
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    _ASSERTE(m_pInternalMetaDataImport == NULL);
-    IMetaDataImport * pImport = GetMetaDataImporter(); // throws
-    if (m_pInternalMetaDataImport == NULL)
-    {
-        HRESULT hr = GetMDInternalInterfaceFromPublic(
-            pImport,
-            IID_IMDInternalImport,
-            reinterpret_cast<void**> (&m_pInternalMetaDataImport));
-        if (m_pInternalMetaDataImport == NULL)
-        {
-            ThrowHR(hr);
-        }
-    }
-    _ASSERTE(m_pInternalMetaDataImport != NULL);
-}
-HRESULT CordbModule::InitPublicMetaDataFromFile()
-{
-    INTERNAL_API_ENTRY(this->GetProcess());
-    const WCHAR * szFullPathName = NULL;
-    bool fDebuggerLoadingNgen = false;
-    bool fDebuggeeLoadedNgen = false;
-    szFullPathName = GetNGenImagePath();
-    if(szFullPathName != NULL)
-    {
-        fDebuggeeLoadedNgen = true;
-        fDebuggerLoadingNgen = true;
-#ifndef TARGET_UNIX
-        if (NULL == WszGetModuleHandle(szFullPathName))
-#endif
-        {
-            szFullPathName = NULL;
-            fDebuggerLoadingNgen = false;
-        }
-    }
-    if (!fDebuggerLoadingNgen)
-    {
-        szFullPathName = GetModulePath();
-    }
-    if(fDebuggeeLoadedNgen && !fDebuggerLoadingNgen && GetProcess()->GetShim()!=NULL)
-    {
-        return CORDBG_E_MISSING_METADATA;
-    }
-    DWORD dwOpenFlags = 0;
-    if (fDebuggerLoadingNgen)
-    {
-        dwOpenFlags = ofReadOnly | ofTrustedImage;
-    }
-    return InitPublicMetaDataFromFile(szFullPathName, dwOpenFlags, true);
-}
-HRESULT CordbModule::InitPublicMetaDataFromFile(const WCHAR * pszFullPathName,
-                                                DWORD dwOpenFlags,
-                                                bool validateFileInfo)
-{
-#ifdef HOST_UNIX
-    return E_FAIL;
-#else
-    if (validateFileInfo)
-    {
-        DWORD dwImageTimeStamp = 0;
-        DWORD dwImageSize = 0;
-        bool isNGEN = false; // unused
-        StringCopyHolder filePath;
-        _ASSERTE(!m_vmPEFile.IsNull());
-        if (!this->GetProcess()->GetDAC()->GetMetaDataFileInfoFromPEFile(m_vmPEFile,
-                                                                         dwImageTimeStamp,
-                                                                         dwImageSize,
-                                                                         isNGEN,
-                                                                         &filePath))
-        {
-            LOG((LF_CORDB,LL_WARNING, "CM::IM: Couldn't get metadata info for file \"%s\"\n", pszFullPathName));
-            return CORDBG_E_MISSING_METADATA;
-        }
-        HandleHolder hMDFile = WszCreateFile(pszFullPathName,
-                                              GENERIC_READ,
-                                              FILE_SHARE_READ,
-                                              NULL,                 // default security descriptor
-                                              OPEN_EXISTING,
-                                              FILE_ATTRIBUTE_NORMAL,
-                                              NULL);
-        if (hMDFile == INVALID_HANDLE_VALUE)
-        {
-            LOG((LF_CORDB,LL_WARNING, "CM::IM: Couldn't open file \"%s\" (GLE=%x)\n", pszFullPathName, GetLastError()));
-            return CORDBG_E_MISSING_METADATA;
-        }
-        DWORD dwFileHigh = 0;
-        DWORD dwFileLow = GetFileSize(hMDFile, &dwFileHigh);
-        if (dwFileLow == INVALID_FILE_SIZE)
-        {
-            LOG((LF_CORDB,LL_WARNING, "CM::IM: File \"%s\" had invalid size.\n", pszFullPathName));
-            return CORDBG_E_MISSING_METADATA;
-        }
-        _ASSERTE(dwFileHigh == 0);
-        HandleHolder hMap = WszCreateFileMapping(hMDFile, NULL, PAGE_READONLY, dwFileHigh, dwFileLow, NULL);
-        if (hMap == NULL)
-        {
-            LOG((LF_CORDB,LL_WARNING, "CM::IM: Couldn't create mapping of file \"%s\" (GLE=%x)\n", pszFullPathName, GetLastError()));
-            return CORDBG_E_MISSING_METADATA;
-        }
-        MapViewHolder hMapView = MapViewOfFile(hMap, FILE_MAP_READ, 0, 0, 0);
-        if (hMapView == NULL)
-        {
-            LOG((LF_CORDB,LL_WARNING, "CM::IM: Couldn't map view of file \"%s\" (GLE=%x)\n", pszFullPathName, GetLastError()));
-            return CORDBG_E_MISSING_METADATA;
-        }
-        PEDecoder pedecoder(hMapView, (COUNT_T)dwFileLow);
-        if (!pedecoder.HasNTHeaders())
-        {
-            LOG((LF_CORDB,LL_WARNING, "CM::IM: \"%s\" did not have PE headers!\n", pszFullPathName));
-            return CORDBG_E_MISSING_METADATA;
-        }
-        if ((dwImageSize != pedecoder.GetVirtualSize()) ||
-            (dwImageTimeStamp != pedecoder.GetTimeDateStamp()))
-        {
-            LOG((LF_CORDB,LL_WARNING, "CM::IM: Validation of \"%s\" failed.  "
-                "Expected size=%x, Expected timestamp=%x, Actual size=%x, Actual timestamp=%x\n",
-                pszFullPathName,
-                pedecoder.GetVirtualSize(),
-                pedecoder.GetTimeDateStamp(),
-                dwImageSize,
-                dwImageTimeStamp));
-            return CORDBG_E_MISSING_METADATA;
-        }
-    }
-    IMetaDataDispenserEx * pDisp = GetProcess()->GetDispenser();
-    HRESULT hr = pDisp->OpenScope(pszFullPathName, dwOpenFlags, IID_IMetaDataImport, (IUnknown**)&m_pIMImport);
-    _ASSERTE(SUCCEEDED(hr) == (m_pIMImport != NULL));
-    if (FAILED(hr))
-    {
-        LOG((LF_CORDB,LL_WARNING, "CM::IM: Couldn't open metadata in file \"%s\" (hr=%x)\n", pszFullPathName, hr));
-    }
-    return hr;
-#endif // TARGET_UNIX
-}
-void CordbModule::InitPublicMetaData(TargetBuffer buffer)
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    INTERNAL_API_ENTRY(this->GetProcess());
-    LOG((LF_CORDB,LL_INFO100000, "CM::IPM: initing with remote buffer 0x%p length 0x%x\n",
-        CORDB_ADDRESS_TO_PTR(buffer.pAddress), buffer.cbSize));
-    ULONG nMetaDataSize = buffer.cbSize;
-    if (nMetaDataSize == 0)
-    {
-        SIMPLIFYING_ASSUMPTION(!"Error: missing the metadata");
-        return;
-    }
-    HRESULT hr = S_OK;
-    IMetaDataDispenserEx * pDisp = GetProcess()->GetDispenser();
-    CoTaskMemHolder<VOID> pMetaDataCopy;
-    CopyRemoteMetaData(buffer, pMetaDataCopy.GetAddr());
-    VARIANT valueOld;
-    hr = pDisp->GetOption(MetaDataSetUpdate, &valueOld);
-    SIMPLIFYING_ASSUMPTION(!FAILED(hr));
-    VARIANT valueRW;
-    V_VT(&valueRW) = VT_UI4;
-    V_I4(&valueRW) = MDUpdateFull;
-    hr = pDisp->SetOption(MetaDataSetUpdate, &valueRW);
-    SIMPLIFYING_ASSUMPTION(!FAILED(hr));
-    hr = pDisp->OpenScopeOnMemory(pMetaDataCopy,
-                                  nMetaDataSize,
-                                  ofTakeOwnership,
-                                  IID_IMetaDataImport,
-                                  reinterpret_cast<IUnknown**>( &m_pIMImport ));
-    pMetaDataCopy.SuppressRelease();
-    HRESULT hrRestore = pDisp->SetOption(MetaDataSetUpdate, &valueOld);
-    SIMPLIFYING_ASSUMPTION(!FAILED(hrRestore));
-    IfFailThrow(hr);
-    IfFailThrow(hrRestore);
-}
-void CordbModule::UpdatePublicMetaDataFromRemote(TargetBuffer bufferRemoteMetaData)
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    if (bufferRemoteMetaData.IsEmpty())
-    {
-        ThrowHR(E_INVALIDARG);
-    }
-    INTERNAL_API_ENTRY(this->GetProcess()); //
-    LOG((LF_CORDB,LL_INFO100000, "CM::UPMFR: updating with remote buffer 0x%p length 0x%x\n",
-        CORDB_ADDRESS_TO_PTR(bufferRemoteMetaData.pAddress), bufferRemoteMetaData.cbSize));
-    _ASSERTE(m_pIMImport != NULL);
-    HRESULT hr = S_OK;
-    ULONG dwMetaDataSize = bufferRemoteMetaData.cbSize;
-    CoTaskMemHolder<VOID> pLocalMetaDataPtr;
-    CopyRemoteMetaData(bufferRemoteMetaData, pLocalMetaDataPtr.GetAddr());
-    IMetaDataDispenserEx *  pDisp = GetProcess()->GetDispenser();
-    _ASSERTE(pDisp != NULL); // throws on error.
-    LOG((LF_CORDB,LL_INFO100000, "CM::RI: converting to new metadata\n"));
-    {
-        ReleaseHolder<IMetaDataImport> pIMImport;
-        hr = pDisp->OpenScopeOnMemory(pLocalMetaDataPtr,
-                                  dwMetaDataSize,
-                                  0,
-                                  IID_IMetaDataImport,
-                                  (IUnknown**)&pIMImport);
-        IfFailThrow(hr);
-    }
-    _ASSERTE(m_pIMImport != NULL); //
-    hr = ReOpenMetaDataWithMemoryEx(m_pIMImport, pLocalMetaDataPtr, dwMetaDataSize, ofTakeOwnership );
-    IfFailThrow(hr);
-    pLocalMetaDataPtr.SuppressRelease();
-}
-void CordbModule::CopyRemoteMetaData(
-    TargetBuffer buffer,
-    CoTaskMemHolder<VOID> * pLocalBuffer)
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    _ASSERTE(pLocalBuffer != NULL);
-    _ASSERTE(!buffer.IsEmpty());
-    LPVOID pRawBuffer = CoTaskMemAlloc(buffer.cbSize);
-    if (pRawBuffer == NULL)
-    {
-        ThrowOutOfMemory();
-    }
-    pLocalBuffer->Assign(pRawBuffer);
-    GetProcess()->SafeReadBuffer(buffer, (BYTE *)pRawBuffer);
-    return;
-}
-HRESULT CordbModule::QueryInterface(REFIID id, void **pInterface)
-{
-    if (id == IID_ICorDebugModule)
-    {
-        *pInterface = static_cast<ICorDebugModule*>(this);
-    }
-    else if (id == IID_ICorDebugModule2)
-    {
-        *pInterface = static_cast<ICorDebugModule2*>(this);
-    }
-    else if (id == IID_ICorDebugModule3)
-    {
-        *pInterface = static_cast<ICorDebugModule3*>(this);
-    }
-    else if (id == IID_ICorDebugModule4)
-    {
-        *pInterface = static_cast<ICorDebugModule4*>(this);
-    }
-    else if (id == IID_IUnknown)
-    {
-        *pInterface = static_cast<IUnknown*>(static_cast<ICorDebugModule*>(this));
-    }
-    else
-    {
-        *pInterface = NULL;
-        return E_NOINTERFACE;
-    }
-    ExternalAddRef();
-    return S_OK;
-}
-HRESULT CordbModule::GetProcess(ICorDebugProcess **ppProcess)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(ppProcess, ICorDebugProcess **);
-    *ppProcess = static_cast<ICorDebugProcess*> (GetProcess());
-    GetProcess()->ExternalAddRef();
-    return S_OK;
-}
-HRESULT CordbModule::GetBaseAddress(CORDB_ADDRESS *pAddress)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pAddress, CORDB_ADDRESS *);
-    *pAddress = m_PEBuffer.pAddress;
-    return S_OK;
-}
-HRESULT CordbModule::GetAssembly(ICorDebugAssembly **ppAssembly)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(ppAssembly, ICorDebugAssembly **);
-    *ppAssembly = static_cast<ICorDebugAssembly *> (m_pAssembly);
-    if (m_pAssembly != NULL)
-    {
-        m_pAssembly->ExternalAddRef();
-    }
-    return S_OK;
-}
-HRESULT CordbModule::GetName(ULONG32 cchName, ULONG32 *pcchName, _Out_writes_to_opt_(cchName, *pcchName) WCHAR szName[])
-{
-    HRESULT hr = S_OK;
-    PUBLIC_API_BEGIN(this)
-    {
-        EX_TRY
-        {
-            hr = GetNameWorker(cchName, pcchName, szName);
-        }
-        EX_CATCH_HRESULT(hr);
-        if ((hr == CORDBG_E_MISSING_METADATA) ||
-            (hr == CORDBG_E_READVIRTUAL_FAILURE) ||
-            (hr == HRESULT_FROM_WIN32(ERROR_PARTIAL_COPY)))
-        {
-            DWORD dwImageTimeStamp = 0; // unused
-            DWORD dwImageSize = 0;      // unused
-            bool isNGEN = false;
-            StringCopyHolder filePath;
-            _ASSERTE(!m_vmPEFile.IsNull());
-            if (this->GetProcess()->GetDAC()->GetMetaDataFileInfoFromPEFile(m_vmPEFile,
-                                                                             dwImageTimeStamp,
-                                                                             dwImageSize,
-                                                                             isNGEN,
-                                                                             &filePath))
-            {
-                _ASSERTE(filePath.IsSet());
-                if ((isNGEN) &&
-                    (this->GetProcess()->GetDAC()->GetILImageInfoFromNgenPEFile(m_vmPEFile,
-                                                                                dwImageTimeStamp,
-                                                                                dwImageSize,
-                                                                                &filePath)))
-                {
-                    _ASSERTE(filePath.IsSet());
-                }
-                hr = CopyOutString(filePath, cchName, pcchName, szName);
-            }
-        }
-    }
-    PUBLIC_API_END(hr);
-    return hr;
-}
-HRESULT CordbModule::GetNameWorker(ULONG32 cchName, ULONG32 *pcchName, _Out_writes_to_opt_(cchName, *pcchName) WCHAR szName[])
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    HRESULT hr = S_OK;
-    const WCHAR * szTempName = NULL;
-    ALLOW_DATATARGET_MISSING_MEMORY(
-        szTempName = GetModulePath();
-    );
-#if defined(FEATURE_DBGIPC_TRANSPORT_DI)
-    if (szTempName == NULL)
-    {
-        IMetaDataAssemblyImport *pAssemblyImport = NULL;
-        if (SUCCEEDED(hr = GetMetaDataImporter()->QueryInterface(IID_IMetaDataAssemblyImport, (void**)&pAssemblyImport)))
-        {
-            mdAssembly mda = TokenFromRid(1, mdtAssembly);
-            hr = pAssemblyImport->GetAssemblyProps(mda,          // [IN] The Assembly for which to get the properties.
-                                                   NULL,         // [OUT] Pointer to the Originator blob.
-                                                   NULL,         // [OUT] Count of bytes in the Originator Blob.
-                                                   NULL,         // [OUT] Hash Algorithm.
-                                                   szName,       // [OUT] Buffer to fill with name.
-                                                   cchName,      // [IN] Size of buffer in wide chars.
-                                                   (ULONG*)pcchName, // [OUT] Actual # of wide chars in name.
-                                                   NULL,         // [OUT] Assembly MetaData.
-                                                   NULL);        // [OUT] Flags.
-            pAssemblyImport->Release();
-            return hr;
-        }
-        hr = S_OK;
-    }
-#endif // FEATURE_DBGIPC_TRANSPORT_DI
-    EX_TRY_ALLOW_DATATARGET_MISSING_MEMORY
-    {
-        StringCopyHolder buffer;
-        if (!szTempName)
-        {
-            hr = HRESULT_FROM_WIN32(ERROR_PARTIAL_COPY);
-            m_pProcess->GetDAC()->GetModuleSimpleName(m_vmModule, &buffer);
-            _ASSERTE(buffer.IsSet());
-            szTempName = buffer;
-        }
-        hr = CopyOutString(szTempName, cchName, pcchName, szName);
-    }
-    EX_END_CATCH_ALLOW_DATATARGET_MISSING_MEMORY
-    return hr;
-}
-const WCHAR * CordbModule::GetModulePath()
-{
-    if (!m_strModulePath.IsSet())
-    {
-        IDacDbiInterface * pDac = m_pProcess->GetDAC(); // throws
-        pDac->GetModulePath(m_vmModule, &m_strModulePath); // throws
-        _ASSERTE(m_strModulePath.IsSet());
-    }
-    if (m_strModulePath.IsEmpty())
-    {
-        return NULL;    // module has no filename
-    }
-    return m_strModulePath;
-}
-const WCHAR * CordbModule::GetNGenImagePath()
-{
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        if (!m_strNGenImagePath.IsSet())
-        {
-            IDacDbiInterface * pDac = m_pProcess->GetDAC(); // throws
-            BOOL fNonEmpty = pDac->GetModuleNGenPath(m_vmModule, &m_strNGenImagePath); // throws
-            (void)fNonEmpty; //prevent "unused variable" error from GCC
-            _ASSERTE(m_strNGenImagePath.IsSet() && (m_strNGenImagePath.IsEmpty() == !fNonEmpty));
-        }
-    }
-    EX_CATCH_HRESULT(hr);
-    if (FAILED(hr) ||
-        m_strNGenImagePath == NULL ||
-        m_strNGenImagePath.IsEmpty())
-    {
-        return NULL;    // module has no ngen filename
-    }
-    return m_strNGenImagePath;
-}
-HRESULT CordbModule::EnableJITDebugging(BOOL bTrackJITInfo, BOOL bAllowJitOpts)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    DWORD dwFlags = CORDEBUG_JIT_DEFAULT;
-    if (!bAllowJitOpts)
-    {
-        dwFlags |= CORDEBUG_JIT_DISABLE_OPTIMIZATION;
-    }
-    return SetJITCompilerFlags(dwFlags);
-}
-HRESULT CordbModule::EnableClassLoadCallbacks(BOOL bClassLoadCallbacks)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    ATT_ALLOW_LIVE_DO_STOPGO(GetProcess());
-    if (m_fDynamic && !bClassLoadCallbacks)
-        return E_INVALIDARG;
-    if (m_vmDomainAssembly.IsNull())
-        return E_UNEXPECTED;
-    CordbProcess *pProcess = GetProcess();
-    DebuggerIPCEvent event;
-    pProcess->InitIPCEvent(&event,
-                           DB_IPCE_SET_CLASS_LOAD_FLAG,
-                           false,
-                           (GetAppDomain()->GetADToken()));
-    event.SetClassLoad.vmDomainAssembly = this->m_vmDomainAssembly;
-    event.SetClassLoad.flag = (bClassLoadCallbacks == TRUE);
-    HRESULT hr = pProcess->m_cordb->SendIPCEvent(pProcess, &event,
-                                                 sizeof(DebuggerIPCEvent));
-    hr = WORST_HR(hr, event.hr);
-    return hr;
-}
-HRESULT CordbModule::GetFunctionFromToken(mdMethodDef token,
-                                          ICorDebugFunction **ppFunction)
-{
-    PUBLIC_API_ENTRY(this);
-    ATT_ALLOW_LIVE_DO_STOPGO(GetProcess()); // @todo - can this be RequiredStop?
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(ppFunction, ICorDebugFunction **);
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        RSLockHolder lockHolder(GetProcess()->GetProcessLock());
-        if ((token == mdMethodDefNil) ||
-            (TypeFromToken(token) != mdtMethodDef) ||
-            (!GetMetaDataImporter()->IsValidToken(token)))
-        {
-            ThrowHR(E_INVALIDARG);
-        }
-        CordbFunction * pFunction = LookupOrCreateFunctionLatestVersion(token);
-        *ppFunction = static_cast<ICorDebugFunction*> (pFunction);
-        pFunction->ExternalAddRef();
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-HRESULT CordbModule::GetFunctionFromRVA(CORDB_ADDRESS rva,
-                                        ICorDebugFunction **ppFunction)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(ppFunction, ICorDebugFunction **);
-    return E_NOTIMPL;
-}
-HRESULT CordbModule::LookupClassByToken(mdTypeDef token,
-                                        CordbClass **ppClass)
-{
-    INTERNAL_API_ENTRY(this->GetProcess()); //
-    FAIL_IF_NEUTERED(this);
-    HRESULT hr = S_OK;
-    EX_TRY // @dbgtodo  exceptions - push this up
-    {
-        *ppClass = NULL;
-        if ((token == mdTypeDefNil) || (TypeFromToken(token) != mdtTypeDef))
-        {
-            ThrowHR(E_INVALIDARG);
-        }
-        RSLockHolder lockHolder(GetProcess()->GetProcessLock()); // @dbgtodo  synchronization - Push this up
-        CordbClass *pClass = m_classes.GetBase(token);
-        if (pClass == NULL)
-        {
-            if (!GetMetaDataImporter()->IsValidToken(token))
-            {
-                ThrowHR(E_INVALIDARG);
-            }
-            RSInitHolder<CordbClass> pClassInit(new CordbClass(this, token));
-            pClass = pClassInit.TransferOwnershipToHash(&m_classes);
-        }
-        *ppClass = pClass;
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-HRESULT CordbModule::GetClassFromToken(mdTypeDef token,
-                                       ICorDebugClass **ppClass)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    ATT_ALLOW_LIVE_DO_STOPGO(this->GetProcess()); // @todo - could this be RequiredStopped?
-    VALIDATE_POINTER_TO_OBJECT(ppClass, ICorDebugClass **);
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        CordbClass *pClass = NULL;
-        *ppClass = NULL;
-        if (!GetMetaDataImporter()->IsValidToken(token))
-        {
-            ThrowHR(E_INVALIDARG);
-        }
-        hr = LookupClassByToken(token, &pClass);
-        IfFailThrow(hr);
-        *ppClass = static_cast<ICorDebugClass*> (pClass);
-        pClass->ExternalAddRef();
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-HRESULT CordbModule::CreateBreakpoint(ICorDebugModuleBreakpoint **ppBreakpoint)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(ppBreakpoint, ICorDebugModuleBreakpoint **);
-    return E_NOTIMPL;
-}
-HRESULT CordbModule::GetToken(mdModule *pToken)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pToken, mdModule *);
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        hr = GetMetaDataImporter()->GetModuleFromScope(pToken);
-        IfFailThrow(hr);
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-HRESULT CordbModule::GetMetaDataInterface(REFIID riid, IUnknown **ppObj)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(ppObj, IUnknown **);
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        hr = GetMetaDataImporter()->QueryInterface(riid, (void**)ppObj);
-        IfFailThrow(hr);
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-CordbFunction* CordbModule::LookupFunctionLatestVersion(mdMethodDef funcMetaDataToken)
-{
-    INTERNAL_API_ENTRY(this);
-    return m_functions.GetBase(funcMetaDataToken);
-}
-CordbFunction* CordbModule::LookupOrCreateFunctionLatestVersion(mdMethodDef funcMetaDataToken)
-{
-    INTERNAL_API_ENTRY(this);
-    CordbFunction * pFunction = m_functions.GetBase(funcMetaDataToken);
-    if (pFunction != NULL)
-    {
-        return pFunction;
-    }
-    return CreateFunction(funcMetaDataToken, CorDB_DEFAULT_ENC_FUNCTION_VERSION);
-}
-CordbFunction * CordbModule::LookupOrCreateFunction(mdMethodDef funcMetaDataToken, SIZE_T enCVersion)
-{
-    INTERNAL_API_ENTRY(this);
-    _ASSERTE(GetProcess()->ThreadHoldsProcessLock());
-    CordbFunction * pFunction = m_functions.GetBase(funcMetaDataToken);
-    if (pFunction == NULL)
-    {
-        return CreateFunction(funcMetaDataToken, enCVersion);
-    }
-    for (CordbFunction *pf=pFunction; pf != NULL; pf = pf->GetPrevVersion())
-    {
-        if (pf->GetEnCVersionNumber() == enCVersion)
-        {
-            return pf;
-        }
-    }
-    _ASSERTE(!"Couldn't find EnC version of function\n");
-    ThrowHR(E_FAIL);
-}
-HRESULT CordbModule::IsDynamic(BOOL *pDynamic)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pDynamic, BOOL *);
-    (*pDynamic) = m_fDynamic;
-    return S_OK;
-}
-BOOL CordbModule::IsDynamic()
-{
-    return m_fDynamic;
-}
-HRESULT CordbModule::IsInMemory(BOOL *pInMemory)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pInMemory, BOOL *);
-    (*pInMemory) = m_fInMemory;
-    return S_OK;
-}
-HRESULT CordbModule::GetGlobalVariableValue(mdFieldDef fieldDef,
-                                            ICorDebugValue **ppValue)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(ppValue, ICorDebugValue **);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(this->GetProcess());
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        if (m_pClass == NULL)
-        {
-            CordbClass * pGlobalClass = NULL;
-            hr = LookupClassByToken(COR_GLOBAL_PARENT_TOKEN, &pGlobalClass);
-            IfFailThrow(hr);
-            m_pClass.Assign(pGlobalClass);
-            _ASSERTE(m_pClass != NULL);
-        }
-        hr = m_pClass->GetStaticFieldValue(fieldDef, NULL, ppValue);
-        IfFailThrow(hr);
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-CordbFunction * CordbModule::CreateFunction(mdMethodDef funcMetaDataToken, SIZE_T enCVersion)
-{
-    INTERNAL_API_ENTRY(this);
-    RSInitHolder<CordbFunction> pFunction(new CordbFunction(this, funcMetaDataToken, enCVersion)); // throws
-    CordbFunction * pCopy = pFunction.TransferOwnershipToHash(&m_functions);
-    return pCopy;
-}
-#ifdef EnC_SUPPORTED
-HRESULT CordbModule::UpdateFunction(mdMethodDef funcMetaDataToken,
-                                    SIZE_T enCVersion,
-                                    CordbFunction** ppFunction)
-{
-    INTERNAL_API_ENTRY(this);
-    if (ppFunction)
-        *ppFunction = NULL;
-    _ASSERTE(funcMetaDataToken);
-    RSLockHolder lockHolder(GetProcess()->GetProcessLock());
-    CordbFunction* pOldVersion = LookupFunctionLatestVersion(funcMetaDataToken);
-    if (!pOldVersion)
-    {
-        LOG((LF_ENC, LL_INFO10000, "CM::UF: adding %8.8x with version %d\n", funcMetaDataToken, enCVersion));
-        HRESULT hr = S_OK;
-        EX_TRY
-        {
-            pOldVersion = CreateFunction(funcMetaDataToken, CorDB_DEFAULT_ENC_FUNCTION_VERSION);
-        }
-        EX_CATCH_HRESULT(hr);
-        if (FAILED(hr))
-        {
-            return hr;
-        }
-    }
-    _ASSERTE( enCVersion > pOldVersion->GetEnCVersionNumber());
-    LOG((LF_ENC, LL_INFO10000, "CM::UF: updating %8.8x with version %d\n", funcMetaDataToken, enCVersion));
-    CordbFunction * pNewVersion = new (nothrow) CordbFunction(this, funcMetaDataToken, enCVersion);
-    if (pNewVersion == NULL)
-        return E_OUTOFMEMORY;
-    pNewVersion->SetPrevVersion(pOldVersion);
-    HRESULT hr = m_functions.SwapBase(pOldVersion, pNewVersion);
-    if (FAILED(hr))
-    {
-        delete pNewVersion;
-        return hr;
-    }
-    pNewVersion->GetPrevVersion()->MakeOld();
-    if (ppFunction)
-        *ppFunction = pNewVersion;
-    return hr;
-}
-#endif // EnC_SUPPORTED
-HRESULT CordbModule::LookupOrCreateClass(mdTypeDef classMetaDataToken,CordbClass** ppClass)
-{
-    INTERNAL_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    RSLockHolder lockHolder(GetProcess()->GetProcessLock()); // @dbgtodo  exceptions synchronization-
-    HRESULT hr = S_OK;
-    *ppClass = LookupClass(classMetaDataToken);
-    if (*ppClass == NULL)
-    {
-        hr = CreateClass(classMetaDataToken,ppClass);
-        if (!SUCCEEDED(hr))
-        {
-            return hr;
-        }
-        _ASSERTE(*ppClass != NULL);
-    }
-    return hr;
-}
-CordbClass* CordbModule::LookupClass(mdTypeDef classMetaDataToken)
-{
-    INTERNAL_API_ENTRY(this);
-    _ASSERTE(GetProcess()->ThreadHoldsProcessLock());
-    return m_classes.GetBase(classMetaDataToken);
-}
-HRESULT CordbModule::CreateClass(mdTypeDef classMetaDataToken,
-                                 CordbClass** ppClass)
-{
-    INTERNAL_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    _ASSERTE(GetProcess()->ThreadHoldsProcessLock());
-    CordbClass* pClass = new (nothrow) CordbClass(this, classMetaDataToken);
-    if (pClass == NULL)
-        return E_OUTOFMEMORY;
-    HRESULT hr = m_classes.AddBase(pClass);
-    if (SUCCEEDED(hr))
-    {
-        *ppClass = pClass;
-        if (classMetaDataToken == COR_GLOBAL_PARENT_TOKEN)
-        {
-            _ASSERTE( m_pClass == NULL ); //redundant create
-            m_pClass.Assign(pClass);
-        }
-    }
-    else
-    {
-        delete pClass;
-    }
-    return hr;
-}
-HRESULT CordbModule::ResolveTypeRef(mdTypeRef token, CordbClass **ppClass)
-{
-    FAIL_IF_NEUTERED(this);
-    INTERNAL_SYNC_API_ENTRY(GetProcess()); //
-    CordbProcess * pProcess = GetProcess();
-    _ASSERTE((pProcess->GetShim() == NULL) || pProcess->GetSynchronized());
-    if ((token == mdTypeRefNil) || (TypeFromToken(token) != mdtTypeRef))
-    {
-        return E_INVALIDARG;
-    }
-    if (m_vmDomainAssembly.IsNull() || m_pAppDomain == NULL)
-    {
-        return E_UNEXPECTED;
-    }
-    HRESULT         hr = S_OK;
-    *ppClass = NULL;
-    EX_TRY
-    {
-        TypeRefData inData = {m_vmDomainAssembly, token};
-        TypeRefData outData;
-        {
-            RSLockHolder lockHolder(pProcess->GetProcessLock());
-            pProcess->GetDAC()->ResolveTypeReference(&inData, &outData);
-        }
-        CordbModule * pModule = m_pAppDomain->LookupOrCreateModule(outData.vmDomainAssembly);
-        IfFailThrow(pModule->LookupClassByToken(outData.typeToken, ppClass));
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-} // CordbModule::ResolveTypeRef
-HRESULT CordbModule::ResolveTypeRefOrDef(mdToken token, CordbClass **ppClass)
-{
-    FAIL_IF_NEUTERED(this);
-    INTERNAL_SYNC_API_ENTRY(this->GetProcess()); //
-    if ((token == mdTypeRefNil) ||
-        (TypeFromToken(token) != mdtTypeRef && TypeFromToken(token) != mdtTypeDef))
-        return E_INVALIDARG;
-    if (TypeFromToken(token)==mdtTypeRef)
-    {
-        return ( ResolveTypeRef(token, ppClass) );
-    }
-    else
-    {
-        return ( LookupClassByToken(token, ppClass) );
-    }
-}
-HRESULT CordbModule::GetSize(ULONG32 *pcBytes)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pcBytes, ULONG32 *);
-    *pcBytes = m_PEBuffer.cbSize;
-    return S_OK;
-}
-CordbAssembly *CordbModule::GetCordbAssembly()
-{
-    INTERNAL_API_ENTRY(this);
-    return m_pAssembly;
-}
-HRESULT CordbModule::GetEditAndContinueSnapshot(
-    ICorDebugEditAndContinueSnapshot **ppEditAndContinueSnapshot)
-{
-    return E_NOTIMPL;
-}
-HRESULT CordbModule::ApplyChanges(ULONG  cbMetaData,
-                                  BYTE   pbMetaData[],
-                                  ULONG  cbIL,
-                                  BYTE   pbIL[])
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
-#ifdef FEATURE_ENC_SUPPORTED
-    LOG((LF_CORDB,LL_INFO10000, "CP::AC: applying changes"));
-    VALIDATE_POINTER_TO_OBJECT_ARRAY(pbMetaData,
-                                   BYTE,
-                                   cbMetaData,
-                                   true,
-                                   true);
-    VALIDATE_POINTER_TO_OBJECT_ARRAY(pbIL,
-                                   BYTE,
-                                   cbIL,
-                                   true,
-                                   true);
-    HRESULT hr;
-    RSExtSmartPtr<IUnknown> pUnk;
-    RSExtSmartPtr<IMDInternalImport> pMDImport;
-    RSExtSmartPtr<IMDInternalImport> pMDImport2;
-    ++m_EnCCount;
-    _ASSERTE(m_pIMImport != NULL); // must have metadata at this point in EnC
-    IfFailGo(m_pIMImport->QueryInterface(IID_IUnknown, (void**)&pUnk));
-    IfFailGo(GetMDInternalInterfaceFromPublic(pUnk, IID_IMDInternalImport,
-                                                    (void **)&pMDImport));
-    hr = pMDImport->ApplyEditAndContinue(pbMetaData, cbMetaData, &pMDImport2);
-    if (pMDImport2 != NULL)
-    {
-        pMDImport2->AddRef();
-    }
-    IfFailGo(hr);
-    m_pIMImport.Clear();
-    IfFailGo(GetMDPublicInterfaceFromInternal(pMDImport2, IID_IMetaDataImport, (void **)&m_pIMImport));
-    IfFailGo( ApplyChangesInternal(cbMetaData, pbMetaData, cbIL, pbIL) );
-    EX_TRY
-    {
-        m_pInternalMetaDataImport.Clear();
-        UpdateInternalMetaData();
-    }
-    EX_CATCH_HRESULT(hr);
-    _ASSERTE(SUCCEEDED(hr));
-ErrExit:
-    return hr;
-#else
-    return E_NOTIMPL;
-#endif
-}
-HRESULT CordbModule::ApplyChangesInternal(ULONG  cbMetaData,
-                                          BYTE   pbMetaData[],
-                                          ULONG  cbIL,
-                                          BYTE   pbIL[])
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    LOG((LF_ENC,LL_INFO100, "CordbProcess::ApplyChangesInternal\n"));
-    FAIL_IF_NEUTERED(this);
-    INTERNAL_SYNC_API_ENTRY(this->GetProcess()); //
-    if (m_vmDomainAssembly.IsNull())
-        return E_UNEXPECTED;
-#ifdef FEATURE_ENC_SUPPORTED
-    HRESULT hr;
-    void * pRemoteBuf = NULL;
-    EX_TRY
-    {
-        DebuggerIPCEvent event;
-        GetProcess()->InitIPCEvent(&event, DB_IPCE_APPLY_CHANGES, false, VMPTR_AppDomain::NullPtr());
-        event.ApplyChanges.vmDomainAssembly = this->m_vmDomainAssembly;
-        ULONG cbSize = cbMetaData+cbIL;
-        TargetBuffer tbFull = GetProcess()->GetRemoteBuffer(cbSize);
-        pRemoteBuf = CORDB_ADDRESS_TO_PTR(tbFull.pAddress);
-        TargetBuffer tbMetaData = tbFull.SubBuffer(0, cbMetaData); // 1st half
-        TargetBuffer tbIL = tbFull.SubBuffer(cbMetaData); // 2nd half
-        GetProcess()->SafeWriteBuffer(tbMetaData, pbMetaData); // throws
-        GetProcess()->SafeWriteBuffer(tbIL, pbIL); // throws
-        event.ApplyChanges.pDeltaMetadata = tbMetaData.pAddress;
-        event.ApplyChanges.cbDeltaMetadata = tbMetaData.cbSize;
-        event.ApplyChanges.pDeltaIL = tbIL.pAddress;
-        event.ApplyChanges.cbDeltaIL = tbIL.cbSize;
-        LOG((LF_ENC,LL_INFO100, "CordbProcess::ApplyChangesInternal sending event\n"));
-        hr = GetProcess()->SendIPCEvent(&event, sizeof(event));
-        hr = WORST_HR(hr, event.hr);
-        IfFailThrow(hr);
-        DebuggerIPCEvent *retEvent = (DebuggerIPCEvent *) _alloca(CorDBIPC_BUFFER_SIZE);
-        {
-            while (TRUE)
-            {
-                hr = GetProcess()->m_cordb->WaitForIPCEventFromProcess(GetProcess(),
-                                                                       GetAppDomain(),
-                                                                       retEvent);
-                IfFailThrow(hr);
-                if (retEvent->type == DB_IPCE_APPLY_CHANGES_RESULT)
-                {
-                    hr = retEvent->ApplyChangesResult.hr;
-                    LOG((LF_CORDB, LL_INFO1000, "[%x] RCET::DRCE: EnC apply changes result %8.8x.\n", hr));
-                    break;
-                }
-                _ASSERTE(retEvent->type == DB_IPCE_ENC_UPDATE_FUNCTION ||
-                                  retEvent->type == DB_IPCE_ENC_ADD_FUNCTION ||
-                                  retEvent->type == DB_IPCE_ENC_ADD_FIELD);
-                LOG((LF_CORDB, LL_INFO1000, "[%x] RCET::DRCE: EnC %s %8.8x to version %d.\n",
-                        GetCurrentThreadId(),
-                        retEvent->type == DB_IPCE_ENC_UPDATE_FUNCTION ? "Update function" :
-                        retEvent->type == DB_IPCE_ENC_ADD_FUNCTION ? "Add function" : "Add field",
-                        retEvent->EnCUpdate.memberMetadataToken, retEvent->EnCUpdate.newVersionNumber));
-                CordbAppDomain *pAppDomain = GetAppDomain();
-                _ASSERTE(NULL != pAppDomain);
-                CordbModule* pModule = NULL;
-                pModule = pAppDomain->LookupOrCreateModule(retEvent->EnCUpdate.vmDomainAssembly); // throws
-                _ASSERTE(pModule != NULL);
-                if (retEvent->type == DB_IPCE_ENC_UPDATE_FUNCTION ||
-                     retEvent->type == DB_IPCE_ENC_ADD_FUNCTION)
-                {
-                    hr = pModule->UpdateFunction(retEvent->EnCUpdate.memberMetadataToken, retEvent->EnCUpdate.newVersionNumber, NULL);
-                }
-                if (retEvent->type == DB_IPCE_ENC_ADD_FUNCTION ||
-                     retEvent->type == DB_IPCE_ENC_ADD_FIELD)
-                {
-                    RSLockHolder lockHolder(GetProcess()->GetProcessLock()); // @dbgtodo  synchronization -  push this up
-                    CordbClass* pClass = pModule->LookupClass(retEvent->EnCUpdate.classMetadataToken);
-                    if (pClass)
-                    {
-                        pClass->MakeOld();
-                    }
-                }
-            }
-        }
-        LOG((LF_ENC,LL_INFO100, "CordbProcess::ApplyChangesInternal complete.\n"));
-    }
-    EX_CATCH_HRESULT(hr);
-    CordbProcess *pProcess = GetProcess();
-    if (pProcess)
-    {
-        HRESULT hr2 = pProcess->ReleaseRemoteBuffer(&pRemoteBuf);
-        TESTANDRETURNHR(hr2);
-    }
-    return hr;
-#else // FEATURE_ENC_SUPPORTED
-    return E_NOTIMPL;
-#endif // FEATURE_ENC_SUPPORTED
-}
-HRESULT CordbModule::SetJMCStatus(
-        BOOL fIsUserCode,
-        ULONG32 cOthers,
-        mdToken others[])
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
-    if (m_vmDomainAssembly.IsNull())
-        return E_UNEXPECTED;
-    if (cOthers != 0)
-    {
-        _ASSERTE(!"not yet impl for cOthers != 0");
-        return E_NOTIMPL;
-    }
-    CordbProcess* pProcess = this->GetProcess();
-    _ASSERTE(pProcess != NULL);
-    DebuggerIPCEvent event;
-    pProcess->InitIPCEvent(&event, DB_IPCE_SET_MODULE_JMC_STATUS, true, this->GetAppDomain()->GetADToken());
-    event.SetJMCFunctionStatus.vmDomainAssembly = m_vmDomainAssembly;
-    event.SetJMCFunctionStatus.dwStatus = fIsUserCode;
-    HRESULT hr = pProcess->m_cordb->SendIPCEvent(pProcess, &event, sizeof(DebuggerIPCEvent));
-    if (!SUCCEEDED(hr))
-    {
-        LOG((LF_CORDB, LL_INFO10, "CordbModule::SetJMCStatus failed  0x%08x...\n", hr));
-        return hr;
-    }
-    _ASSERTE(event.type == DB_IPCE_SET_MODULE_JMC_STATUS_RESULT);
-    LOG((LF_CORDB, LL_INFO10, "returning from CordbModule::SetJMCStatus 0x%08x...\n", hr));
-    return event.hr;
-}
-HRESULT CordbModule::ResolveAssembly(mdToken tkAssemblyRef,
-                                    ICorDebugAssembly **ppAssembly)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(this->GetProcess());
-    if(ppAssembly)
-    {
-        *ppAssembly = NULL;
-    }
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        CordbAssembly *pCordbAsm = ResolveAssemblyInternal(tkAssemblyRef);
-        if (pCordbAsm == NULL)
-        {
-            hr = CORDBG_E_CANNOT_RESOLVE_ASSEMBLY;
-        }
-        else if(ppAssembly)
-        {
-            _ASSERTE(pCordbAsm != NULL);
-            *ppAssembly = pCordbAsm;
-            pCordbAsm->ExternalAddRef();
-        }
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-CordbAssembly * CordbModule::ResolveAssemblyInternal(mdToken tkAssemblyRef)
-{
-    INTERNAL_SYNC_API_ENTRY(GetProcess()); //
-    if (TypeFromToken(tkAssemblyRef) != mdtAssemblyRef || tkAssemblyRef == mdAssemblyRefNil)
-    {
-        ThrowHR(E_INVALIDARG);
-    }
-    CordbAssembly *    pAssembly = NULL;
-    if (!m_vmDomainAssembly.IsNull())
-    {
-        VMPTR_DomainAssembly vmDomainAssembly = GetProcess()->GetDAC()->ResolveAssembly(m_vmDomainAssembly, tkAssemblyRef);
-        if (!vmDomainAssembly.IsNull() && m_pAppDomain != NULL)
-        {
-            RSLockHolder lockHolder(GetProcess()->GetProcessLock());
-            pAssembly = m_pAppDomain->LookupOrCreateAssembly(vmDomainAssembly);
-        }
-    }
-    return pAssembly;
-}
-HRESULT CordbModule::CreateReaderForInMemorySymbols(REFIID riid, void** ppObj)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    CordbProcess *pProcess = GetProcess();
-    ATT_REQUIRE_STOPPED_MAY_FAIL(pProcess);
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        ReleaseHolder<IStream> pStream;
-        IDacDbiInterface::SymbolFormat symFormat = GetInMemorySymbolStream(&pStream);
-        ReleaseHolder<ISymUnmanagedBinder> pBinder;
-        if (symFormat == IDacDbiInterface::kSymbolFormatPDB)
-        {
-#ifndef TARGET_UNIX
-            InlineSString<_MAX_PATH> ssBuf;
-            IfFailThrow(GetClrModuleDirectory(ssBuf));
-            IfFailThrow(FakeCoCreateInstanceEx(CLSID_CorSymBinder_SxS,
-                                               ssBuf.GetUnicode(),
-                                               IID_ISymUnmanagedBinder,
-                                               (void**)&pBinder,
-                                               NULL));
-#else
-            IfFailThrow(FakeCoCreateInstance(CLSID_CorSymBinder_SxS,
-                                             IID_ISymUnmanagedBinder,
-                                             (void**)&pBinder));
-#endif
-        }
-        else
-        {
-            _ASSERTE(symFormat == IDacDbiInterface::kSymbolFormatNone);
-            if (m_fDynamic || m_fInMemory)
-            {
-                ThrowHR(CORDBG_E_SYMBOLS_NOT_AVAILABLE);
-            }
-            ThrowHR(CORDBG_E_MODULE_LOADED_FROM_DISK);
-        }
-        if (m_pIMImport == NULL)
-        {
-            ThrowHR(CORDBG_E_SYMBOLS_NOT_AVAILABLE);
-        }
-        ReleaseHolder<ISymUnmanagedReader> pReader;
-        IfFailThrow(pBinder->GetReaderFromStream(m_pIMImport, pStream, &pReader));
-        IfFailThrow(pReader->QueryInterface(riid, ppObj));
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-/* ------------------------------------------------------------------------- *
- * Class class
- * ------------------------------------------------------------------------- */
-void CordbModule::SetLoadEventContinueMarker()
-{
-    GetProcess()->TargetConsistencyCheck(m_nLoadEventContinueCounter == 0);
-    m_nLoadEventContinueCounter = GetProcess()->m_continueCounter;
-}
-HRESULT CordbModule::EnsureModuleIsInLoadCallback()
-{
-    if (this->m_nLoadEventContinueCounter < GetProcess()->m_continueCounter)
-    {
-        return CORDBG_E_MUST_BE_IN_LOAD_MODULE;
-    }
-    else
-    {
-        return S_OK;
-    }
-}
-HRESULT CordbModule::SetJITCompilerFlags(DWORD dwFlags)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    CordbProcess *pProcess = GetProcess();
-    ATT_REQUIRE_STOPPED_MAY_FAIL(pProcess);
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        if ((dwFlags != CORDEBUG_JIT_DEFAULT) &&
-            (dwFlags != CORDEBUG_JIT_DISABLE_OPTIMIZATION) &&
-            (dwFlags != CORDEBUG_JIT_ENABLE_ENC))
-        {
-            hr = E_INVALIDARG;
-        }
-        else
-        {
-            BOOL fAllowJitOpts = ((dwFlags & CORDEBUG_JIT_DISABLE_OPTIMIZATION) != CORDEBUG_JIT_DISABLE_OPTIMIZATION);
-            BOOL fEnableEnC = ((dwFlags & CORDEBUG_JIT_ENABLE_ENC) == CORDEBUG_JIT_ENABLE_ENC);
-            hr = EnsureModuleIsInLoadCallback();
-            if (SUCCEEDED(hr))
-            {
-                hr = pProcess->GetDAC()->SetCompilerFlags(GetRuntimeDomainAssembly(), fAllowJitOpts, fEnableEnC);
-            }
-        }
-    }
-    EX_CATCH_HRESULT(hr);
-    if (GetProcess()->GetShim() != NULL)
-    {
-        hr = GetProcess()->GetShim()->FilterSetJitFlagsHresult(hr);
-    }
-    return hr;
-}
-HRESULT CordbModule::GetJITCompilerFlags(DWORD *pdwFlags )
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pdwFlags, DWORD*);
-    *pdwFlags = CORDEBUG_JIT_DEFAULT;;
-    CordbProcess *pProcess = GetProcess();
-    ATT_REQUIRE_STOPPED_MAY_FAIL(pProcess);
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        BOOL fAllowJitOpts;
-        BOOL fEnableEnC;
-        pProcess->GetDAC()->GetCompilerFlags (
-            GetRuntimeDomainAssembly(),
-            &fAllowJitOpts,
-            &fEnableEnC);
-        if (fEnableEnC)
-        {
-            *pdwFlags = CORDEBUG_JIT_ENABLE_ENC;
-        }
-        else if (! fAllowJitOpts)
-        {
-            *pdwFlags = CORDEBUG_JIT_DISABLE_OPTIMIZATION;
-        }
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-HRESULT CordbModule::IsMappedLayout(BOOL *isMapped)
-{
-    PUBLIC_API_ENTRY(this);
-    VALIDATE_POINTER_TO_OBJECT(isMapped, BOOL*);
-    FAIL_IF_NEUTERED(this);
-    HRESULT hr = S_OK;
-    *isMapped = FALSE;
-    CordbProcess *pProcess = GetProcess();
-    ATT_REQUIRE_STOPPED_MAY_FAIL(pProcess);
-    EX_TRY
-    {
-        hr = pProcess->GetDAC()->IsModuleMapped(m_vmModule, isMapped);
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-/* ------------------------------------------------------------------------- *
- * CordbCode class
- * ------------------------------------------------------------------------- */
-CordbCode::CordbCode(CordbFunction * pFunction, UINT_PTR id, SIZE_T encVersion, BOOL fIsIL)
-  : CordbBase(pFunction->GetProcess(), id, enumCordbCode),
-    m_fIsIL(fIsIL),
-    m_nVersion(encVersion),
-    m_rgbCode(NULL),
-    m_continueCounterLastSync(0),
-    m_pFunction(pFunction)
-{
-    _ASSERTE(pFunction != NULL);
-    _ASSERTE(m_nVersion >= CorDB_DEFAULT_ENC_FUNCTION_VERSION);
-} // CordbCode::CordbCode
-CordbCode::~CordbCode()
-{
-    _ASSERTE(IsNeutered());
-}
-void CordbCode::Neuter()
-{
-    m_pFunction = NULL;
-    delete [] m_rgbCode;
-    m_rgbCode = NULL;
-    CordbBase::Neuter();
-}
-HRESULT CordbCode::QueryInterface(REFIID id, void ** pInterface)
-{
-    if (id == IID_ICorDebugCode)
-    {
-        *pInterface = static_cast<ICorDebugCode*>(this);
-    }
-    else if (id == IID_IUnknown)
-    {
-        *pInterface = static_cast<IUnknown *>(static_cast<ICorDebugCode *>(this));
-    }
-    else
-    {
-        *pInterface = NULL;
-        return E_NOINTERFACE;
-    }
-    ExternalAddRef();
-    return S_OK;
-}
-HRESULT CordbCode::GetEnCRemapSequencePoints(ULONG32 cMap, ULONG32 * pcMap, ULONG32 offsets[])
-{
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT_OR_NULL(pcMap, ULONG32*);
-    VALIDATE_POINTER_TO_OBJECT_ARRAY_OR_NULL(offsets, ULONG32*, cMap, true, true);
-    return E_NOTIMPL;
-} // CordbCode::GetEnCRemapSequencePoints
-HRESULT CordbCode::IsIL(BOOL *pbIL)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pbIL, BOOL *);
-    *pbIL = IsIL();
-    return S_OK;
-}
-HRESULT CordbCode::GetFunction(ICorDebugFunction **ppFunction)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(ppFunction, ICorDebugFunction **);
-    *ppFunction = static_cast<ICorDebugFunction*> (m_pFunction);
-    m_pFunction->ExternalAddRef();
-    return S_OK;
-}
-HRESULT CordbCode::GetSize(ULONG32 *pcBytes)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pcBytes, ULONG32 *);
-    *pcBytes = GetSize();
-    return S_OK;
-}
-HRESULT CordbCode::CreateBreakpoint(ULONG32 offset,
-                                    ICorDebugFunctionBreakpoint **ppBreakpoint)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(ppBreakpoint, ICorDebugFunctionBreakpoint **);
-    HRESULT hr;
-    ULONG32 size = GetSize();
-    BOOL offsetIsIl = IsIL();
-    LOG((LF_CORDB, LL_INFO10000, "CCode::CreateBreakpoint, offset=%d, size=%d, IsIl=%d, this=0x%p\n",
-        offset, size, offsetIsIl, this));
-    if (offset >= size)
-    {
-        return CORDBG_E_UNABLE_TO_SET_BREAKPOINT;
-    }
-    CordbFunctionBreakpoint *bp = new (nothrow) CordbFunctionBreakpoint(this, offset, offsetIsIl);
-    if (bp == NULL)
-        return E_OUTOFMEMORY;
-    hr = bp->Activate(TRUE);
-    if (SUCCEEDED(hr))
-    {
-        *ppBreakpoint = static_cast<ICorDebugFunctionBreakpoint*> (bp);
-        bp->ExternalAddRef();
-        return S_OK;
-    }
-    else
-    {
-        delete bp;
-        return hr;
-    }
-}
-HRESULT CordbCode::GetCode(ULONG32 startOffset,
-                           ULONG32 endOffset,
-                           ULONG32 cBufferAlloc,
-                           BYTE buffer[],
-                           ULONG32 *pcBufferSize)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT_ARRAY(buffer, BYTE, cBufferAlloc, true, true);
-    VALIDATE_POINTER_TO_OBJECT(pcBufferSize, ULONG32 *);
-    LOG((LF_CORDB,LL_EVERYTHING, "CC::GC: for token:0x%x\n", m_pFunction->GetMetadataToken()));
-    ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
-    HRESULT hr = S_OK;
-    *pcBufferSize = 0;
-    ULONG32 totalSize = GetSize();
-    if (cBufferAlloc < endOffset - startOffset)
-        endOffset = startOffset + cBufferAlloc;
-    if (endOffset > totalSize)
-        endOffset = totalSize;
-    if (startOffset > totalSize)
-        startOffset = totalSize;
-    if ((m_rgbCode == NULL) ||
-        (m_continueCounterLastSync < GetProcess()->m_continueCounter))
-    {
-        ReadCodeBytes();
-        m_continueCounterLastSync = GetProcess()->m_continueCounter;
-    }
-    if (*pcBufferSize == 0 && m_rgbCode != NULL)
-    {
-        memcpy(buffer,
-               m_rgbCode+startOffset,
-               endOffset - startOffset);
-        *pcBufferSize = endOffset - startOffset;
-    }
-    return hr;
-} // CordbCode::GetCode
-#include "dbgipcevents.h"
-HRESULT CordbCode::GetVersionNumber( ULONG32 *nVersion)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(nVersion, ULONG32 *);
-    LOG((LF_CORDB,LL_INFO10000,"R:CC:GVN:Returning 0x%x "
-        "as version\n",m_nVersion));
-    *nVersion = (ULONG32)m_nVersion;
-#ifndef EnC_SUPPORTED
-    _ASSERTE(*nVersion == 1);
-#endif // EnC_SUPPORTED
-    return S_OK;
-}
-CordbFunction * CordbCode::GetFunction()
-{
-    _ASSERTE(m_pFunction != NULL);
-    return m_pFunction;
-}
-/* ------------------------------------------------------------------------- *
- * CordbILCode class
- * ------------------------------------------------------------------------- */
-CordbILCode::CordbILCode(CordbFunction * pFunction,
-                         TargetBuffer    codeRegionInfo,
-                         SIZE_T          nVersion,
-                         mdSignature     localVarSigToken,
-                         UINT_PTR        id)
-  : CordbCode(pFunction, id, nVersion, TRUE),
-#ifdef EnC_SUPPORTED
-    m_fIsOld(FALSE),
-#endif
-    m_codeRegionInfo(codeRegionInfo),
-    m_localVarSigToken(localVarSigToken)
-{
-} // CordbILCode::CordbILCode
-#ifdef EnC_SUPPORTED
-void CordbILCode::MakeOld()
-{
-    m_fIsOld = TRUE;
-}
-#endif
-HRESULT CordbILCode::GetAddress(CORDB_ADDRESS * pStart)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pStart, CORDB_ADDRESS *);
-    _ASSERTE(this != NULL);
-    _ASSERTE(this->GetFunction() != NULL);
-    _ASSERTE(this->GetFunction()->GetModule() != NULL);
-    _ASSERTE(this->GetFunction()->GetModule()->GetProcess() == GetProcess());
-    *pStart = (m_codeRegionInfo.pAddress);
-    return S_OK;
-} // CordbILCode::GetAddress
-HRESULT CordbILCode::ReadCodeBytes()
-{
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        CORDB_ADDRESS pStart = m_codeRegionInfo.pAddress;
-        ULONG32 cbSize = (ULONG32) m_codeRegionInfo.cbSize;
-        delete [] m_rgbCode;
-        m_rgbCode = new BYTE[cbSize];    // throws
-        SIZE_T cbRead;
-        hr = GetProcess()->ReadMemory(pStart, cbSize, m_rgbCode, &cbRead);
-        IfFailThrow(hr);
-        SIMPLIFYING_ASSUMPTION(cbRead == cbSize);
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-} // CordbILCode::ReadCodeBytes
-HRESULT CordbILCode::GetILToNativeMapping(ULONG32                    cMap,
-                                          ULONG32 *                  pcMap,
-                                          COR_DEBUG_IL_TO_NATIVE_MAP map[])
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT_OR_NULL(pcMap, ULONG32 *);
-    VALIDATE_POINTER_TO_OBJECT_ARRAY_OR_NULL(map, COR_DEBUG_IL_TO_NATIVE_MAP *, cMap, true, true);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
-    return CORDBG_E_NON_NATIVE_FRAME;
-} // CordbILCode::GetILToNativeMapping
-/*
-* CordbILCode::GetLocalVarSig
-*
-* Get the method's local variable metadata signature. This may be cached, but for dynamic modules we'll always
-* read it from the metadata. This function also returns the count of local variables in the method.
-*
-* Parameters:
-*    pLocalSigParser - OUT: the local variable signature for the method.
-*    pLocalCount - OUT: the number of locals the method has.
-*
-* Returns:
-*    HRESULT for success or failure.
-*
-*/
-HRESULT CordbILCode::GetLocalVarSig(SigParser *pLocalSigParser,
-    ULONG *pLocalVarCount)
-{
-    INTERNAL_SYNC_API_ENTRY(GetProcess());
-    CONTRACTL  // @dbgtodo  exceptions - convert to throws...
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    FAIL_IF_NEUTERED(this);
-    HRESULT hr = S_OK;
-    if (m_localVarSigToken != mdSignatureNil)
-    {
-        PCCOR_SIGNATURE localSignature = NULL;
-        ULONG size = 0;
-        EX_TRY // // @dbgtodo  exceptions  - push this up
-        {
-            GetFunction()->GetModule()->UpdateMetaDataCacheIfNeeded(m_localVarSigToken);
-            hr = GetFunction()->GetModule()->GetMetaDataImporter()->GetSigFromToken(m_localVarSigToken,
-                &localSignature,
-                &size);
-        }
-        EX_CATCH_HRESULT(hr);
-        if (FAILED(hr))
-        {
-            LOG((LF_CORDB, LL_WARNING, "CICF::GLVS caught hr=0x%x\n", hr));
-        }
-        IfFailRet(hr);
-        LOG((LF_CORDB, LL_INFO100000, "CIC::GLVS creating sig parser sig=0x%x size=0x%x\n", localSignature, size));
-        SigParser sigParser = SigParser(localSignature, size);
-        uint32_t data;
-        IfFailRet(sigParser.GetCallingConvInfo(&data));
-        _ASSERTE(data == IMAGE_CEE_CS_CALLCONV_LOCAL_SIG);
-        uint32_t localCount;
-        IfFailRet(sigParser.GetData(&localCount));
-        LOG((LF_CORDB, LL_INFO100000, "CIC::GLVS localCount=0x%x\n", localCount));
-        if (pLocalSigParser != NULL)
-        {
-            *pLocalSigParser = sigParser;
-        }
-        if (pLocalVarCount != NULL)
-        {
-            *pLocalVarCount = localCount;
-        }
-    }
-    else
-    {
-        if (pLocalSigParser != NULL)
-        {
-            *pLocalSigParser = SigParser(NULL, 0);
-        }
-        if (pLocalVarCount != NULL)
-        {
-            *pLocalVarCount = 0;
-        }
-    }
-    LOG((LF_CORDB, LL_INFO100000, "CIC::GLVS returning hr=0x%x\n", hr));
-    return hr;
-}
-HRESULT CordbILCode::GetLocalVariableType(DWORD dwIndex,
-    const Instantiation * pInst,
-    CordbType ** ppResultType)
-{
-    ATT_ALLOW_LIVE_DO_STOPGO(GetProcess());
-    LOG((LF_CORDB, LL_INFO10000, "CIC::GLVT dwIndex=0x%x pInst=0x%p\n", dwIndex, pInst));
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        SigParser sigParser;
-        ULONG cLocals;
-        IfFailThrow(GetLocalVarSig(&sigParser, &cLocals));
-        if (dwIndex >= cLocals)
-        {
-            ThrowHR(E_INVALIDARG);
-        }
-        for (unsigned int i = 0; i < dwIndex; i++)
-        {
-            LOG((LF_CORDB, LL_INFO10000, "CIC::GLVT scanning index 0x%x\n", dwIndex));
-            IfFailThrow(sigParser.SkipExactlyOne());
-        }
-        hr = CordbType::SigToType(GetFunction()->GetModule(), &sigParser, pInst, ppResultType);
-        LOG((LF_CORDB, LL_INFO10000, "CIC::GLVT CT::SigToType returned hr=0x%x\n", hr));
-        IfFailThrow(hr);
-    } EX_CATCH_HRESULT(hr);
-    return hr;
-}
-mdSignature CordbILCode::GetLocalVarSigToken()
-{
-    return m_localVarSigToken;
-}
-HRESULT CordbILCode::CreateNativeBreakpoint(ICorDebugFunctionBreakpoint **ppBreakpoint)
-{
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(ppBreakpoint, ICorDebugFunctionBreakpoint **);
-    HRESULT hr;
-    ULONG32 size = GetSize();
-    LOG((LF_CORDB, LL_INFO10000, "CordbILCode::CreateNativeBreakpoint, size=%d, this=0x%p\n",
-        size, this));
-    ULONG32 offset = 0;
-    CordbFunctionBreakpoint *bp = new (nothrow) CordbFunctionBreakpoint(this, offset, FALSE);
-    if (bp == NULL)
-    {
-        return E_OUTOFMEMORY;
-    }
-    hr = bp->Activate(TRUE);
-    if (SUCCEEDED(hr))
-    {
-        *ppBreakpoint = static_cast<ICorDebugFunctionBreakpoint*> (bp);
-        bp->ExternalAddRef();
-        return S_OK;
-    }
-    else
-    {
-        delete bp;
-        return hr;
-    }
-}
-CordbReJitILCode::CordbReJitILCode(CordbFunction *pFunction, SIZE_T encVersion, VMPTR_ILCodeVersionNode vmILCodeVersionNode) :
-CordbILCode(pFunction, TargetBuffer(), encVersion, mdSignatureNil, VmPtrToCookie(vmILCodeVersionNode)),
-m_cClauses(0),
-m_cbLocalIL(0),
-m_cILMap(0)
-{
-    _ASSERTE(!vmILCodeVersionNode.IsNull());
-    DacSharedReJitInfo data = { 0 };
-    IfFailThrow(GetProcess()->GetDAC()->GetILCodeVersionNodeData(vmILCodeVersionNode, &data));
-    IfFailThrow(Init(&data));
-}
-HRESULT CordbReJitILCode::Init(DacSharedReJitInfo* pSharedReJitInfo)
-{
-    HRESULT hr = S_OK;
-    if (pSharedReJitInfo->m_cInstrumentedMapEntries)
-    {
-        if (pSharedReJitInfo->m_cInstrumentedMapEntries > 100000)
-            return CORDBG_E_TARGET_INCONSISTENT;
-        m_cILMap = pSharedReJitInfo->m_cInstrumentedMapEntries;
-        m_pILMap = new (nothrow)COR_IL_MAP[m_cILMap];
-        TargetBuffer mapBuffer(pSharedReJitInfo->m_rgInstrumentedMapEntries, m_cILMap*sizeof(COR_IL_MAP));
-        IfFailRet(GetProcess()->SafeReadBuffer(mapBuffer, (BYTE*)m_pILMap.GetValue(), FALSE /* bThrowOnError */));
-    }
-    CORDB_ADDRESS pIlHeader = pSharedReJitInfo->m_pbIL;
-    IMAGE_COR_ILMETHOD_FAT header = { 0 };
-    bool headerMustBeTiny = false;
-    ULONG32 headerSize = 0;
-    hr = GetProcess()->SafeReadStruct(pIlHeader, &header);
-    if (hr != S_OK)
-    {
-        headerMustBeTiny = true;
-        IfFailRet(GetProcess()->SafeReadStruct(pIlHeader, (IMAGE_COR_ILMETHOD_TINY *)&header));
-    }
-    ULONG32 ilCodeSize = 0;
-    IMAGE_COR_ILMETHOD_TINY *pMethodTinyHeader = (IMAGE_COR_ILMETHOD_TINY *)&header;
-    bool isTinyHeader = ((pMethodTinyHeader->Flags_CodeSize & (CorILMethod_FormatMask >> 1)) == CorILMethod_TinyFormat);
-    if (isTinyHeader)
-    {
-        ilCodeSize = (((unsigned)pMethodTinyHeader->Flags_CodeSize) >> (CorILMethod_FormatShift - 1));
-        headerSize = sizeof(IMAGE_COR_ILMETHOD_TINY);
-        m_localVarSigToken = mdSignatureNil;
-    }
-    else if (headerMustBeTiny)
-    {
-        return CORDBG_E_READVIRTUAL_FAILURE;
-    }
-    else
-    {
-        ilCodeSize = header.CodeSize;
-        headerSize = header.Size * 4;
-        m_localVarSigToken = header.LocalVarSigTok;
-    }
-    if (ilCodeSize == 0 || ilCodeSize > 100000)
-    {
-        return CORDBG_E_TARGET_INCONSISTENT;
-    }
-    m_codeRegionInfo.Init(pIlHeader + headerSize, ilCodeSize);
-    m_pLocalIL = new (nothrow) BYTE[ilCodeSize];
-    if (m_pLocalIL == NULL)
-        return E_OUTOFMEMORY;
-    m_cbLocalIL = ilCodeSize;
-    IfFailRet(GetProcess()->SafeReadBuffer(m_codeRegionInfo, m_pLocalIL, FALSE /*throwOnError*/));
-    if ((pMethodTinyHeader->Flags_CodeSize & CorILMethod_MoreSects) == 0)
-    {
-        return S_OK; // no EH, done initing
-    }
-    CORDB_ADDRESS ehClauseHeader = ((pIlHeader + headerSize + ilCodeSize - 1) & ~3) + 4;
-    BYTE kind = 0;
-    IfFailRet(GetProcess()->SafeReadStruct(ehClauseHeader, &kind));
-    if ((kind & CorILMethod_Sect_KindMask) != CorILMethod_Sect_EHTable)
-    {
-        return S_OK;
-    }
-    if (kind & CorILMethod_Sect_FatFormat)
-    {
-        IMAGE_COR_ILMETHOD_SECT_FAT sectionHeader = { 0 };
-        IfFailRet(GetProcess()->SafeReadStruct(ehClauseHeader, &sectionHeader));
-        m_cClauses = (sectionHeader.DataSize - 4) / sizeof(IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT);
-        if (m_cClauses > 10000) // sanity check the data before allocating
-        {
-            return CORDBG_E_TARGET_INCONSISTENT;
-        }
-        TargetBuffer buffer(ehClauseHeader + sizeof(IMAGE_COR_ILMETHOD_SECT_FAT), m_cClauses*sizeof(IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT));
-        NewArrayHolder<IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT> pClauses = new (nothrow)IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT[m_cClauses];
-        if (pClauses == NULL)
-            return E_OUTOFMEMORY;
-        IfFailRet(GetProcess()->SafeReadBuffer(buffer, (BYTE*)pClauses.GetValue(), FALSE /*throwOnError*/));
-        m_pClauses = new (nothrow)CorDebugEHClause[m_cClauses];
-        if (m_pClauses == NULL)
-            return E_OUTOFMEMORY;
-        for (ULONG32 i = 0; i < m_cClauses; i++)
-        {
-            BOOL isFilter = ((pClauses[i].Flags & COR_ILEXCEPTION_CLAUSE_FILTER) != 0);
-            m_pClauses[i].Flags = pClauses[i].Flags;
-            m_pClauses[i].TryOffset = pClauses[i].TryOffset;
-            m_pClauses[i].TryLength = pClauses[i].TryLength;
-            m_pClauses[i].HandlerOffset = pClauses[i].HandlerOffset;
-            m_pClauses[i].HandlerLength = pClauses[i].HandlerLength;
-            m_pClauses[i].ClassToken = isFilter ? 0 : pClauses[i].ClassToken;
-            m_pClauses[i].FilterOffset = isFilter ? pClauses[i].FilterOffset : 0;
-        }
-    }
-    else
-    {
-        IMAGE_COR_ILMETHOD_SECT_SMALL sectionHeader = { 0 };
-        IfFailRet(GetProcess()->SafeReadStruct(ehClauseHeader, &sectionHeader));
-        ULONG32 m_cClauses = (sectionHeader.DataSize - 4) / sizeof(IMAGE_COR_ILMETHOD_SECT_SMALL);
-        if (m_cClauses > 10000) // sanity check the data before allocating
-        {
-            return CORDBG_E_TARGET_INCONSISTENT;
-        }
-        TargetBuffer buffer(ehClauseHeader + sizeof(IMAGE_COR_ILMETHOD_SECT_SMALL), m_cClauses*sizeof(IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_SMALL));
-        NewArrayHolder<IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_SMALL> pClauses = new (nothrow)IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_SMALL[m_cClauses];
-        if (pClauses == NULL)
-            return E_OUTOFMEMORY;
-        IfFailRet(GetProcess()->SafeReadBuffer(buffer, (BYTE*)pClauses.GetValue(), FALSE /*throwOnError*/));
-        m_pClauses = new (nothrow)CorDebugEHClause[m_cClauses];
-        if (m_pClauses == NULL)
-            return E_OUTOFMEMORY;
-        for (ULONG32 i = 0; i < m_cClauses; i++)
-        {
-            BOOL isFilter = ((pClauses[i].Flags & COR_ILEXCEPTION_CLAUSE_FILTER) != 0);
-            m_pClauses[i].Flags = pClauses[i].Flags;
-            m_pClauses[i].TryOffset = pClauses[i].TryOffset;
-            m_pClauses[i].TryLength = pClauses[i].TryLength;
-            m_pClauses[i].HandlerOffset = pClauses[i].HandlerOffset;
-            m_pClauses[i].HandlerLength = pClauses[i].HandlerLength;
-            m_pClauses[i].ClassToken = isFilter ? 0 : pClauses[i].ClassToken;
-            m_pClauses[i].FilterOffset = isFilter ? pClauses[i].FilterOffset : 0;
-        }
-    }
-    return S_OK;
-}
-#ifndef MIN
-#define MIN(a,b) ((a) < (b) ? (a) : (b))
-#endif
-HRESULT CordbReJitILCode::GetEHClauses(ULONG32 cClauses, ULONG32 * pcClauses, CorDebugEHClause clauses[])
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT_OR_NULL(pcClauses, ULONG32 *);
-    VALIDATE_POINTER_TO_OBJECT_ARRAY_OR_NULL(clauses, CorDebugEHClause *, cClauses, true, true);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
-    if (cClauses != 0 && clauses == NULL)
-    {
-        return E_INVALIDARG;
-    }
-    if (pcClauses != NULL)
-    {
-        if (cClauses == 0)
-        {
-            *pcClauses = m_cClauses;
-        }
-        else
-        {
-            *pcClauses = MIN(cClauses, m_cClauses);
-        }
-    }
-    if (clauses != NULL)
-    {
-        memcpy_s(clauses, sizeof(CorDebugEHClause)*cClauses, m_pClauses, sizeof(CorDebugEHClause)*MIN(cClauses, m_cClauses));
-    }
-    return S_OK;
-}
-ULONG CordbReJitILCode::AddRef()
-{
-    return CordbCode::AddRef();
-}
-ULONG CordbReJitILCode::Release()
-{
-    return CordbCode::Release();
-}
-HRESULT CordbReJitILCode::QueryInterface(REFIID riid, void** ppInterface)
-{
-    if (riid == IID_ICorDebugILCode)
-    {
-        *ppInterface = static_cast<ICorDebugILCode*>(this);
-    }
-    else if (riid == IID_ICorDebugILCode2)
-    {
-        *ppInterface = static_cast<ICorDebugILCode2*>(this);
-    }
-    else
-    {
-        return CordbILCode::QueryInterface(riid, ppInterface);
-    }
-    AddRef();
-    return S_OK;
-}
-HRESULT CordbReJitILCode::GetLocalVarSigToken(mdSignature *pmdSig)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pmdSig, mdSignature *);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
-    *pmdSig = m_localVarSigToken;
-    return S_OK;
-}
-HRESULT CordbReJitILCode::GetInstrumentedILMap(ULONG32 cMap, ULONG32 *pcMap, COR_IL_MAP map[])
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT_OR_NULL(pcClauses, ULONG32 *);
-    VALIDATE_POINTER_TO_OBJECT_ARRAY_OR_NULL(map, COR_IL_MAP *, cMap, true, true);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
-    if (cMap != 0 && map == NULL)
-    {
-        return E_INVALIDARG;
-    }
-    if (pcMap != NULL)
-    {
-        if (cMap == 0)
-        {
-            *pcMap = m_cILMap;
-        }
-        else
-        {
-            *pcMap = MIN(cMap, m_cILMap);
-        }
-    }
-    if (map != NULL)
-    {
-        memcpy_s(map, sizeof(COR_IL_MAP)*cMap, m_pILMap, sizeof(COR_IL_MAP)*MIN(cMap, m_cILMap));
-    }
-    return S_OK;
-}
-HRESULT FindNativeInfoInILVariableArray(DWORD                                               dwIndex,
-                                        SIZE_T                                              ip,
-                                        const DacDbiArrayList<ICorDebugInfo::NativeVarInfo> * nativeInfoList,
-                                        const ICorDebugInfo::NativeVarInfo **                 ppNativeInfo)
-{
-    _ASSERTE(ppNativeInfo != NULL);
-    *ppNativeInfo = NULL;
-    int lastGoodOne = -1;
-    for (unsigned int i = 0; i < (unsigned)nativeInfoList->Count(); i++)
-    {
-        if ((*nativeInfoList)[i].varNumber == dwIndex)
-        {
-            if ( (lastGoodOne == -1) ||
-                 ((*nativeInfoList)[lastGoodOne].startOffset < (*nativeInfoList)[i].startOffset) )
-            {
-                lastGoodOne = i;
-            }
-            if (((*nativeInfoList)[i].startOffset <= ip) &&
-                ((*nativeInfoList)[i].endOffset > ip))
-            {
-                *ppNativeInfo = &((*nativeInfoList)[i]);
-                return S_OK;
-            }
-        }
-    }
-    if ((lastGoodOne > -1) && ((*nativeInfoList)[lastGoodOne].endOffset == ip))
-    {
-        *ppNativeInfo = &((*nativeInfoList)[lastGoodOne]);
-        return S_OK;
-    }
-    return CORDBG_E_IL_VAR_NOT_AVAILABLE;
-} // FindNativeInfoInILVariableArray
-CordbVariableHome::CordbVariableHome(CordbNativeCode *pCode,
-                                     const ICorDebugInfo::NativeVarInfo nativeVarInfo,
-                                     BOOL isLocal,
-                                     ULONG index) :
-    CordbBase(pCode->GetModule()->GetProcess(), 0)
-{
-    _ASSERTE(pCode != NULL);
-    m_pCode.Assign(pCode);
-    m_nativeVarInfo = nativeVarInfo;
-    m_isLocal = isLocal;
-    m_index = index;
-}
-CordbVariableHome::~CordbVariableHome()
-{
-    _ASSERTE(this->IsNeutered());
-}
-void CordbVariableHome::Neuter()
-{
-    m_pCode.Clear();
-    CordbBase::Neuter();
-}
-HRESULT CordbVariableHome::QueryInterface(REFIID id, void **pInterface)
-{
-    if (id == IID_ICorDebugVariableHome)
-    {
-        *pInterface = static_cast<ICorDebugVariableHome *>(this);
-    }
-    else if (id == IID_IUnknown)
-    {
-        *pInterface = static_cast<IUnknown *>(static_cast<ICorDebugVariableHome *>(this));
-    }
-    else
-    {
-        *pInterface = NULL;
-        return E_NOINTERFACE;
-    }
-    ExternalAddRef();
-    return S_OK;
-}
-HRESULT CordbVariableHome::GetCode(ICorDebugCode **ppCode)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(ppCode, ICorDebugCode **);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
-    HRESULT hr = m_pCode->QueryInterface(IID_ICorDebugCode, (LPVOID*)ppCode);
-    return hr;
-}
-HRESULT CordbVariableHome::GetSlotIndex(ULONG32 *pSlotIndex)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pSlotIndex, ULONG32 *);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
-    if (!m_isLocal)
-    {
-        return E_FAIL;
-    }
-    *pSlotIndex = m_index;
-    return S_OK;
-}
-HRESULT CordbVariableHome::GetArgumentIndex(ULONG32 *pArgumentIndex)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pArgumentIndex, ULONG32 *);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
-    if (m_isLocal)
-    {
-        return E_FAIL;
-    }
-    *pArgumentIndex = m_index;
-    return S_OK;
-}
-HRESULT CordbVariableHome::GetLiveRange(ULONG32 *pStartOffset,
-                                        ULONG32 *pEndOffset)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pStartOffset, ULONG32 *);
-    VALIDATE_POINTER_TO_OBJECT(pEndOffset, ULONG32 *);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
-    *pStartOffset = m_nativeVarInfo.startOffset;
-    *pEndOffset = m_nativeVarInfo.endOffset;
-    return S_OK;
-}
-HRESULT CordbVariableHome::GetLocationType(VariableLocationType *pLocationType)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pLocationType, VariableLocationType *);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
-    switch (m_nativeVarInfo.loc.vlType)
-    {
-    case ICorDebugInfo::VLT_REG:
-        *pLocationType = VLT_REGISTER;
-        break;
-    case ICorDebugInfo::VLT_STK:
-        *pLocationType = VLT_REGISTER_RELATIVE;
-        break;
-    default:
-        *pLocationType = VLT_INVALID;
-    }
-    return S_OK;
-}
-HRESULT CordbVariableHome::GetRegister(CorDebugRegister *pRegister)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pRegister, CorDebugRegister *);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
-    switch (m_nativeVarInfo.loc.vlType)
-    {
-    case ICorDebugInfo::VLT_REG:
-        *pRegister = ConvertRegNumToCorDebugRegister(m_nativeVarInfo.loc.vlReg.vlrReg);
-        break;
-    case ICorDebugInfo::VLT_STK:
-        *pRegister = ConvertRegNumToCorDebugRegister(m_nativeVarInfo.loc.vlStk.vlsBaseReg);
-        break;
-    default:
-        return E_FAIL;
-    }
-    return S_OK;
-}
-HRESULT CordbVariableHome::GetOffset(LONG *pOffset)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pOffset, LONG *);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(m_pCode->GetProcess());
-    switch (m_nativeVarInfo.loc.vlType)
-    {
-    case ICorDebugInfo::VLT_STK:
-        *pOffset = m_nativeVarInfo.loc.vlStk.vlsOffset;
-        break;
-    default:
-        return E_FAIL;
-    }
-    return S_OK;
-}
-CordbNativeCode::CordbNativeCode(CordbFunction *                pFunction,
-                                 const NativeCodeFunctionData * pJitData,
-                                 BOOL                           fIsInstantiatedGeneric)
-  : CordbCode(pFunction, (UINT_PTR)pJitData->m_rgCodeRegions[kHot].pAddress, pJitData->encVersion, FALSE),
-    m_vmNativeCodeMethodDescToken(pJitData->vmNativeCodeMethodDescToken),
-    m_fCodeAvailable(TRUE),
-    m_fIsInstantiatedGeneric(fIsInstantiatedGeneric != FALSE)
-{
-    _ASSERTE(GetVersion() >= CorDB_DEFAULT_ENC_FUNCTION_VERSION);
-    for (CodeBlobRegion region = kHot; region < MAX_REGIONS; ++region)
-    {
-        m_rgCodeRegions[region] = pJitData->m_rgCodeRegions[region];
-    }
-} //CordbNativeCode::CordbNativeCode
-HRESULT CordbNativeCode::QueryInterface(REFIID id, void ** pInterface)
-{
-    if (id == IID_ICorDebugCode)
-    {
-        *pInterface = static_cast<ICorDebugCode *>(this);
-    }
-    else if (id == IID_ICorDebugCode2)
-    {
-        *pInterface = static_cast<ICorDebugCode2 *>(this);
-    }
-    else if (id == IID_ICorDebugCode3)
-    {
-        *pInterface = static_cast<ICorDebugCode3 *>(this);
-    }
-    else if (id == IID_ICorDebugCode4)
-    {
-        *pInterface = static_cast<ICorDebugCode4 *>(this);
-    }
-    else if (id == IID_IUnknown)
-    {
-        *pInterface = static_cast<IUnknown *>(static_cast<ICorDebugCode *>(this));
-    }
-    else
-    {
-        *pInterface = NULL;
-        return E_NOINTERFACE;
-    }
-    ExternalAddRef();
-    return S_OK;
-}
-HRESULT CordbNativeCode::GetAddress(CORDB_ADDRESS * pStart)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pStart, CORDB_ADDRESS *);
-    _ASSERTE(this != NULL);
-    _ASSERTE(this->GetFunction() != NULL);
-    _ASSERTE(this->GetFunction()->GetModule() != NULL);
-    _ASSERTE(this->GetFunction()->GetModule()->GetProcess() == GetProcess());
-    *pStart = (m_rgCodeRegions[kHot].pAddress);
-    if (*pStart == NULL)
-    {
-        return CORDBG_E_CODE_NOT_AVAILABLE;
-    }
-    return S_OK;
-} // CordbNativeCode::GetAddress
-HRESULT CordbNativeCode::ReadCodeBytes()
-{
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        CORDB_ADDRESS pHotStart = m_rgCodeRegions[kHot].pAddress;
-        CORDB_ADDRESS pColdStart = m_rgCodeRegions[kCold].pAddress;
-        ULONG32 cbHotSize = (ULONG32) m_rgCodeRegions[kHot].cbSize;
-        ULONG32 cbColdSize = GetColdSize();
-        delete [] m_rgbCode;
-        m_rgbCode = new BYTE[cbHotSize + cbColdSize];
-        SIZE_T cbRead;
-        hr = GetProcess()->ReadMemory(pHotStart, cbHotSize, m_rgbCode, &cbRead);
-        IfFailThrow(hr);
-        SIMPLIFYING_ASSUMPTION(cbRead == cbHotSize);
-        if (HasColdRegion())
-        {
-            hr = GetProcess()->ReadMemory(pColdStart, cbColdSize, (BYTE *) m_rgbCode + cbHotSize, &cbRead);
-            IfFailThrow(hr);
-            SIMPLIFYING_ASSUMPTION(cbRead == cbColdSize);
-        }
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-} // CordbNativeCode::ReadCodeBytes
-ULONG32 CordbNativeCode::GetColdSize()
-{
-    ULONG32 pcBytes = 0;
-    for (CodeBlobRegion index = kCold; index < MAX_REGIONS; ++index)
-    {
-        pcBytes += m_rgCodeRegions[index].cbSize;
-    }
-    return pcBytes;
-} // CordbNativeCode::GetColdSize
-ULONG32 CordbNativeCode::GetSize()
-{
-    ULONG32 pcBytes = 0;
-    for (CodeBlobRegion index = kHot; index < MAX_REGIONS; ++index)
-    {
-        pcBytes += m_rgCodeRegions[index].cbSize;
-    }
-    return pcBytes;
-} // CordbNativeCode::GetSize
-HRESULT CordbNativeCode::GetILToNativeMapping(ULONG32                    cMap,
-                                              ULONG32 *                  pcMap,
-                                              COR_DEBUG_IL_TO_NATIVE_MAP map[])
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT_OR_NULL(pcMap, ULONG32 *);
-    VALIDATE_POINTER_TO_OBJECT_ARRAY_OR_NULL(map, COR_DEBUG_IL_TO_NATIVE_MAP *,cMap,true,true);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
-    HRESULT hr = S_OK;
-    EX_TRY
-    {
-        LoadNativeInfo();
-        SequencePoints * pSeqPts = GetSequencePoints();
-        ULONG32 cMapIntCount = pSeqPts->GetEntryCount();
-        if (map != NULL && cMapIntCount != 0)
-        {
-            DebuggerILToNativeMap * rgMapInt = pSeqPts->GetMapAddr();
-            ULONG32 cMapToCopy = min(cMap, cMapIntCount);
-            ULONG32 size = GetSize();
-            ExportILToNativeMap(cMapToCopy, map, rgMapInt, size);
-        }
-        if (pcMap)
-        {
-            *pcMap = cMapIntCount;
-        }
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-} // CordbNativeCode::GetILToNativeMapping
-HRESULT CordbNativeCode::GetCodeChunks(
-    ULONG32 cbufSize,
-    ULONG32 * pcnumChunks,
-    CodeChunkInfo chunks[]
-)
-{
-    PUBLIC_API_ENTRY(this);
-    if (pcnumChunks == NULL)
-    {
-        return E_INVALIDARG;
-    }
-    if ((chunks == NULL) != (cbufSize == 0))
-    {
-        return E_INVALIDARG;
-    }
-    ULONG32 cActualChunks = HasColdRegion() ? 2 : 1;
-    if (cbufSize == 0)
-    {
-        *pcnumChunks = cActualChunks;
-        return S_OK;
-    }
-    for (CodeBlobRegion index = kHot; (index < MAX_REGIONS) && ((int)cbufSize > index); ++index)
-    {
-        chunks[index].startAddr = m_rgCodeRegions[index].pAddress;
-        chunks[index].length = (ULONG32) (m_rgCodeRegions[index].cbSize);
-        *pcnumChunks = cbufSize;
-    }
-    return S_OK;
-} // CordbNativeCode::GetCodeChunks
-HRESULT CordbNativeCode::GetCompilerFlags(DWORD * pdwFlags)
-{
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pdwFlags, DWORD *);
-    *pdwFlags = 0;
-    ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
-    return GetFunction()->GetModule()->GetJITCompilerFlags(pdwFlags);
-} // CordbNativeCode::GetCompilerFlags
-HRESULT CordbNativeCode::ILVariableToNative(DWORD dwIndex,
-                                            SIZE_T ip,
-                                            const ICorDebugInfo::NativeVarInfo ** ppNativeInfo)
-{
-    _ASSERTE(m_nativeVarData.IsInitialized());
-    return FindNativeInfoInILVariableArray(dwIndex,
-                                           ip,
-                                           m_nativeVarData.GetOffsetInfoList(),
-                                           ppNativeInfo);
-} // CordbNativeCode::ILVariableToNative
-HRESULT CordbNativeCode::GetReturnValueLiveOffset(ULONG32 ILoffset, ULONG32 bufferSize, ULONG32 *pFetched, ULONG32 *pOffsets)
-{
-    HRESULT hr = S_OK;
-    PUBLIC_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(pFetched, ULONG32 *);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
-    EX_TRY
-    {
-        hr = GetReturnValueLiveOffsetImpl(NULL, ILoffset, bufferSize, pFetched, pOffsets);
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-HRESULT CordbNativeCode::EnumerateVariableHomes(ICorDebugVariableHomeEnum **ppEnum)
-{
-    PUBLIC_REENTRANT_API_ENTRY(this);
-    FAIL_IF_NEUTERED(this);
-    VALIDATE_POINTER_TO_OBJECT(ppEnum, ICorDebugVariableHomeEnum **);
-    ATT_REQUIRE_STOPPED_MAY_FAIL(GetProcess());
-    HRESULT hr = S_OK;
-    ULONG argCount = 0;
-    CordbFunction *func = GetFunction();
-    _ASSERTE(func != NULL);
-    IfFailRet(func->GetSig(NULL, &argCount, NULL));
-#ifdef _DEBUG
-    ULONG localCount = 0;
-    EX_TRY
-    {
-        GetFunction()->GetILCode()->GetLocalVarSig(NULL, &localCount);
-    }
-    EX_CATCH_HRESULT(hr);
-    IfFailRet(hr);
-#endif
-    RSSmartPtr<CordbVariableHome> *rsHomes = NULL;
-    EX_TRY
-    {
-        CordbProcess *pProcess = GetProcess();
-        _ASSERTE(pProcess != NULL);
-        const DacDbiArrayList<ICorDebugInfo::NativeVarInfo> *pOffsetInfoList = m_nativeVarData.GetOffsetInfoList();
-        _ASSERTE(pOffsetInfoList != NULL);
-        DWORD countHomes = 0;
-        for (unsigned int i = 0; i < pOffsetInfoList->Count(); i++)
-        {
-            const ICorDebugInfo::NativeVarInfo *pNativeVarInfo = &((*pOffsetInfoList)[i]);
-            _ASSERTE(pNativeVarInfo != NULL);
-            if (pNativeVarInfo->varNumber < (DWORD)ICorDebugInfo::MAX_ILNUM)
-            {
-                countHomes++;
-            }
-        }
-        rsHomes = new RSSmartPtr<CordbVariableHome>[countHomes];
-        DWORD varHomeInd = 0;
-        for (unsigned int i = 0; i < pOffsetInfoList->Count(); i++)
-        {
-            const ICorDebugInfo::NativeVarInfo *pNativeVarInfo = &((*pOffsetInfoList)[i]);
-            if (pNativeVarInfo->varNumber < (DWORD)ICorDebugInfo::MAX_ILNUM)
-            {
-                BOOL isLocal = ((ULONG)pNativeVarInfo->varNumber >= argCount);
-                ULONG argOrSlotIndex;
-                if (isLocal) {
-                    argOrSlotIndex = pNativeVarInfo->varNumber - argCount;
-                    _ASSERTE(argOrSlotIndex < localCount);
-                } else {
-                    argOrSlotIndex = pNativeVarInfo->varNumber;
-                }
-                RSInitHolder<CordbVariableHome> pCVH(new CordbVariableHome(this,
-                                                                           (*pOffsetInfoList)[i],
-                                                                           isLocal,
-                                                                           argOrSlotIndex));
-                pProcess->GetContinueNeuterList()->Add(pProcess, pCVH);
-                _ASSERTE(varHomeInd < countHomes);
-                rsHomes[varHomeInd].Assign(pCVH);
-                pCVH.ClearAndMarkDontNeuter();
-                varHomeInd++;
-            }
-        }
-        RSInitHolder<CordbVariableHomeEnumerator> pCDVHE(
-            new CordbVariableHomeEnumerator(GetProcess(), &rsHomes, countHomes));
-        pProcess->GetContinueNeuterList()->Add(pProcess, pCDVHE);
-        pCDVHE.TransferOwnershipExternal(ppEnum);
-    }
-    EX_CATCH_HRESULT(hr);
-    return hr;
-}
-int CordbNativeCode::GetCallInstructionLength(BYTE *ip, ULONG32 count)
-{
-#if defined(TARGET_ARM)
-    if (Is32BitInstruction(*(WORD*)ip))
-        return 4;
-    else
-        return 2;
-#elif defined(TARGET_ARM64)
-    return MAX_INSTRUCTION_LENGTH;
-#elif defined(TARGET_LOONGARCH64)
-    return MAX_INSTRUCTION_LENGTH;
-#elif defined(TARGET_X86)
-    if (count < 2)
-        return -1;
-    do
-    {
-        switch (*ip)
-        {
-        case 0x26: // ES
-        case 0x2E: // CS
-        case 0x36: // SS
-        case 0x3E: // DS
-        case 0x64: // FS
-        case 0x65: // GS
-        case 0x66: // Operand-Size
-        case 0x67: // Address-Size
-        case 0xf0:
-        case 0xf1:
-        case 0xf2: // REPNE/REPNZ
-        case 0xf3:
-            ip++;
-            count--;
-            continue;
-        default:
-            break;
-        }
-    } while (0);
-    BYTE opcode = *ip++;
-    if (opcode == 0xcc)
-    {
-        _ASSERTE(!"Hit break opcode!");
-        return -1;
-    }
-    switch (opcode)
-    {
-    case 0xff:
-    {
-                 if (count < 2)
-                     return -1;
-                 BYTE modrm = *ip++;
-                 BYTE mod = (modrm & 0xC0) >> 6;
-                 BYTE reg = (modrm & 0x38) >> 3;
-                 BYTE rm  = (modrm & 0x07);
-                 int displace = -1;
-                 if ((reg != 2) && (reg != 3) && (reg != 4) && (reg != 5))
-                 {
-                     _ASSERTE(!"Unhandled opcode!");
-                     return -1;
-                 }
-                 switch (mod)
-                 {
-                 case 0:
-                 case 1:
-                 case 2:
-                     if (rm == 4)
-                     {
-                         if (count < 3)
-                             return -1;
-                         BYTE ss    = (*ip & 0xC0) >> 6;
-                         BYTE index = (*ip & 0x38) >> 3;
-                         BYTE base  = (*ip & 0x7);
-                         if (mod == 0)
-                         {
-                             if (base == 5)
-                                 displace = 7;
-                             else
-                                 displace = 3;
-                         }
-                         else if (mod == 1)
-                         {
-                             displace = 4;
-                         }
-                         else
-                         {
-                             displace = 7;
-                         }
-                     }
-                     else
-                     {
-                         if (mod == 0)
-                         {
-                             if (rm == 5)
-                                 displace = 6;
-                             else
-                                 displace = 2;
-                         }
-                         else if (mod == 1)
-                         {
-                             displace = 3;
-                         }
-                         else
-                         {
-                             displace = 6;
-                         }
-                     }
-                     break;
-                 case 3:
-                 default:
-                     displace = 2;
-                     break;
-                 }
-                 return displace;
-    }  // end of 0xFF case
-    case 0xe8:
-        return 5;
-    default:
-        break;
-    }
-    _ASSERTE(!"Unhandled opcode!");
-    return -1;
-#elif defined(TARGET_AMD64)
-    BYTE rex = NULL;
-    BYTE prefix = *ip;
-    BOOL fContainsPrefix = FALSE;
-    if (prefix == 0xcc)
-        return -1;
-    do
-    {
-        switch (prefix)
-        {
-        case 0x26: // ES
-        case 0x2E: // CS
-        case 0x36: // SS
-        case 0x3E: // DS
-        case 0x64: // FS
-        case 0x65: // GS
-        case 0x66: // Operand-Size
-        case 0x67: // Address-Size
-        case 0xf0:
-        case 0xf2: // REPNE/REPNZ
-        case 0xf3:
-            ip++;
-            fContainsPrefix = TRUE;
-            continue;
-        case 0x40:
-        case 0x41:
-        case 0x42:
-        case 0x43:
-        case 0x44:
-        case 0x45:
-        case 0x46:
-        case 0x47:
-        case 0x48:
-        case 0x49:
-        case 0x4a:
-        case 0x4b:
-        case 0x4c:
-        case 0x4d:
-        case 0x4e:
-        case 0x4f:
-            rex = prefix;
-            ip++;
-            fContainsPrefix = TRUE;
-            continue;
-        default:
-            break;
-        }
-    } while (0);
-    BYTE opcode = *ip++;
-    if (opcode == 0xcc)
-        return -1;
-    BYTE rex_b = 0;
-    BYTE rex_x = 0;
-    BYTE rex_r = 0;
-    if (rex != NULL)
-    {
-        rex_b = (rex & 0x1);       // high bit to modrm r/m field or SIB base field or OPCODE reg field    -- Hmm, when which?
-        rex_x = (rex & 0x2) >> 1;  // high bit to sib index field
-        rex_r = (rex & 0x4) >> 2;  // high bit to modrm reg field
-    }
-    switch (opcode)
-    {
-    case 0xff:
-    {
-                 BYTE modrm = *ip++;
-                 _ASSERT(modrm != NULL);
-                 BYTE mod = (modrm & 0xC0) >> 6;
-                 BYTE reg = (modrm & 0x38) >> 3;
-                 BYTE rm  = (modrm & 0x07);
-                 reg   |= (rex_r << 3);
-                 rm    |= (rex_b << 3);
-                 if ((reg < 2) || (reg > 5 && reg < 8) || (reg > 15)) {
-                     _ASSERTE(!"Invalid opcode!");
-                     return -1;
-                 }
-                 SHORT displace = -1;
-                 switch (mod)
-                 {
-                 case 0:
-                 case 1:
-                 case 2:
-                     if ((rm & 0x07) == 4) // we have an SIB byte following
-                     {
-                         BYTE sib   = *ip;
-                         _ASSERT(sib != NULL);
-                         BYTE base  = (sib & 0x07);
-                         base  |= (rex_b << 3);
-                         ip++;
-                         if (mod == 0)
-                         {
-                             if ((base & 0x07) == 5)
-                                 displace = 7;
-                             else
-                                 displace = 3;
-                         }
-                         else if (mod == 1)
-                         {
-                             displace = 4;
-                         }
-                         else // mod == 2
-                         {
-                             displace = 7;
-                         }
-                     }
-                     else
-                     {
-                         if ((mod == 0) && ((rm & 0x07) == 5))
-                         {
-                             displace = 6;   // 1 byte opcode + 1 byte modrm + 4 byte displacement (signed)
-                         }
-                         else
-                         {
-                             if (mod == 0)
-                                 displace = 2;
-                             else if (mod == 1)
-                                 displace = 3;
-                             else // mod == 2
-                                 displace = 6;
-                         }
-                     }
-                     break;
-                 case 3:
-                 default:
-                     displace = 2;
-                 }
-                 if (displace == -1)
-                 {
-                     _ASSERTE(!"GetCallInstructionLength() encountered unexpected call instruction");
-                     return -1;
-                 }
-                 if (fContainsPrefix)
-                     displace++;
-                 if ((reg != 4) && (reg != 5))
-                     return displace;
-                 break;
-    }
-    case 0xe8:
-    {
-                 return 5 + (fContainsPrefix ? 1 : 0);
-    }
-    default:
-        break;
-    }
-    _ASSERTE(!"Invalid opcode!");
-    return -1;
-#elif defined(TARGET_RISCV64)
-    return MAX_INSTRUCTION_LENGTH;
-#else
-#error Platform not implemented
-#endif
-}
-HRESULT CordbNativeCode::GetSigParserFromFunction(mdToken mdFunction, mdToken *pClass, SigParser &parser, SigParser &methodGenerics)
-{
-    HRESULT hr = S_OK;
-    IMetaDataImport* pImport = m_pFunction->GetModule()->GetMetaDataImporter();
-    RSExtSmartPtr<IMetaDataImport2> pImport2;
-    IfFailRet(pImport->QueryInterface(IID_IMetaDataImport2, (void**)&pImport2));
-    if (TypeFromToken(mdFunction) == mdtMemberRef)
-    {
-        PCCOR_SIGNATURE sig = 0;
-        ULONG sigSize = 0;
-        IfFailRet(pImport->GetMemberRefProps(mdFunction, pClass, NULL, 0, 0, &sig, &sigSize));
-        parser = SigParser(sig, sigSize);
-    }
-    else if (TypeFromToken(mdFunction) == mdtMethodDef)
-    {
-        PCCOR_SIGNATURE sig = 0;
-        ULONG sigSize = 0;
-        IfFailRet(pImport->GetMethodProps(mdFunction, pClass, NULL, 0, NULL, NULL, &sig, &sigSize, NULL, NULL));
-        parser = SigParser(sig, sigSize);
-    }
-    else if (TypeFromToken(mdFunction) == mdtMethodSpec)
-    {
-        PCCOR_SIGNATURE sig = 0;
-        ULONG sigSize = 0;
-        mdToken parentToken = 0;
-        IfFailRet(pImport2->GetMethodSpecProps(mdFunction, &parentToken, &sig, &sigSize));
-        methodGenerics = SigParser(sig, sigSize);
-        if (pClass)
-            *pClass = parentToken;
-        return GetSigParserFromFunction(parentToken, pClass, parser, methodGenerics);
-    }
-    else
-    {
-        return E_UNEXPECTED;
-    }
-    return S_OK;
-}
-HRESULT CordbNativeCode::EnsureReturnValueAllowed(Instantiation *currentInstantiation, mdToken targetClass, SigParser &parser, SigParser &methodGenerics)
-{
-    HRESULT hr = S_OK;
-    uint32_t genCount = 0;
-    IfFailRet(SkipToReturn(parser, &genCount));
-    return EnsureReturnValueAllowedWorker(currentInstantiation, targetClass, parser, methodGenerics, genCount);
-}
-HRESULT CordbNativeCode::EnsureReturnValueAllowedWorker(Instantiation *currentInstantiation, mdToken targetClass, SigParser &parser, SigParser &methodGenerics, ULONG genCount)
-{
-    SigParser original(parser);
-    HRESULT hr = S_OK;
-    CorElementType returnType;
-    IfFailRet(parser.GetElemType(&returnType));
-    if (returnType == ELEMENT_TYPE_GENERICINST)
-    {
-        IfFailRet(parser.GetElemType(&returnType));
-        if (returnType == ELEMENT_TYPE_CLASS)
-            return S_OK;
-        if (returnType != ELEMENT_TYPE_VALUETYPE)
-            return META_E_BAD_SIGNATURE;
-        if (currentInstantiation == NULL)
-            return S_OK;  // We will check again when we have the instantiation.
-        NewArrayHolder<CordbType*> types;
-        Instantiation inst;
-        IfFailRet(CordbJITILFrame::BuildInstantiationForCallsite(GetModule(), types, inst, currentInstantiation, targetClass, SigParser(methodGenerics)));
-        CordbType *pType = 0;
-        IfFailRet(CordbType::SigToType(GetModule(), &original, &inst, &pType));
-        IfFailRet(pType->ReturnedByValue());
-        if (hr == S_OK) // not S_FALSE
-            return S_OK;
-        return CORDBG_E_UNSUPPORTED;
-    }
-    if (returnType == ELEMENT_TYPE_VALUETYPE)
-    {
-        Instantiation inst;
-        CordbType *pType = 0;
-        IfFailRet(CordbType::SigToType(GetModule(), &original, &inst, &pType));
-        IfFailRet(pType->ReturnedByValue());
-        if (hr == S_OK) // not S_FALSE
-            return S_OK;
-        return CORDBG_E_UNSUPPORTED;
-    }
-    if (returnType == ELEMENT_TYPE_TYPEDBYREF)
-        return CORDBG_E_UNSUPPORTED;
-    if (returnType == ELEMENT_TYPE_VOID)
-        return E_UNEXPECTED;
-    if (returnType == ELEMENT_TYPE_MVAR)
-    {
-        uint32_t genParam = 0;
-        IfFailRet(parser.GetData(&genParam));
-        uint32_t callingConv = 0;
-        IfFailRet(methodGenerics.GetCallingConvInfo(&callingConv));
-        if (callingConv != IMAGE_CEE_CS_CALLCONV_GENERICINST)
-            return META_E_BAD_SIGNATURE;
-        SigParser generics(methodGenerics);     // Make a copy since operations are destructive.
-        uint32_t maxCount = 0;
-        IfFailRet(generics.GetData(&maxCount));
-        if (maxCount <= genParam || genParam > 1024)
-            return META_E_BAD_SIGNATURE;
-        while (genParam--)
-            IfFailRet(generics.SkipExactlyOne());
-        return EnsureReturnValueAllowedWorker(currentInstantiation, targetClass, generics, methodGenerics, genCount);
-    }
-    if (returnType == ELEMENT_TYPE_VAR)
-    {
-        uint32_t typeParam = 0;
-        parser.GetData(&typeParam);
-        if (typeParam > 1024)
-            return META_E_BAD_SIGNATURE;
-        IMetaDataImport *pImport = m_pFunction->GetModule()->GetMetaDataImporter();
-        PCCOR_SIGNATURE sig;
-        ULONG countSig;
-        IfFailRet(pImport->GetTypeSpecFromToken(targetClass, &sig, &countSig));
-        SigParser typeParser(sig, countSig);
-        CorElementType et;
-        IfFailRet(typeParser.GetElemType(&et));
-        if (et != ELEMENT_TYPE_GENERICINST)
-            return META_E_BAD_SIGNATURE;
-        IfFailRet(typeParser.GetElemType(&et));
-        if (et != ELEMENT_TYPE_VALUETYPE && et != ELEMENT_TYPE_CLASS)
-            return META_E_BAD_SIGNATURE;
-        IfFailRet(typeParser.GetToken(NULL));
-        uint32_t totalTypeCount = 0;
-        IfFailRet(typeParser.GetData(&totalTypeCount));
-        if (totalTypeCount < typeParam)
-            return META_E_BAD_SIGNATURE;
-        while (typeParam--)
-            IfFailRet(typeParser.SkipExactlyOne());
-        IfFailRet(typeParser.PeekElemType(&et));
-        if (et == ELEMENT_TYPE_VAR)
-            return E_FAIL;
-        return EnsureReturnValueAllowedWorker(currentInstantiation, targetClass, typeParser, methodGenerics, genCount);
-    }
-    return S_OK;
-}
-HRESULT CordbNativeCode::SkipToReturn(SigParser &parser, uint32_t *genCount)
-{
-    HRESULT hr = S_OK;
-    uint32_t uCallConv;
-    IfFailRet(parser.GetCallingConvInfo(&uCallConv));
-    if ((uCallConv == IMAGE_CEE_CS_CALLCONV_FIELD) || (uCallConv == IMAGE_CEE_CS_CALLCONV_LOCAL_SIG))
-        return META_E_BAD_SIGNATURE;
-    if (uCallConv & IMAGE_CEE_CS_CALLCONV_GENERIC)
-        IfFailRet(parser.GetData(genCount));
-    IfFailRet(parser.GetData(NULL));
-    return S_OK;
-}
-HRESULT CordbNativeCode::GetCallSignature(ULONG32 ILoffset, mdToken *pClass, mdToken *pFunction, SigParser &parser, SigParser &generics)
-{
-    CordbILCode *pCode = this->m_pFunction->GetILCode();
-    BYTE buffer[3];
-    ULONG32 fetched = 0;
-    HRESULT hr = pCode->GetCode(ILoffset, ILoffset+ARRAY_SIZE(buffer), ARRAY_SIZE(buffer), buffer, &fetched);
-    if (FAILED(hr))
-        return hr;
-    else if (fetched != ARRAY_SIZE(buffer))
-        return CORDBG_E_INVALID_OPCODE;
-    BYTE instruction = buffer[0];
-    if (buffer[0] == 0xfe && buffer[1] == 0x14)
-    {
-        return CORDBG_E_INVALID_OPCODE;
-    }
-    if (instruction != 0x28 && instruction != 0x6f)
-        return CORDBG_E_INVALID_OPCODE;
-    mdToken mdFunction = 0;
-    const ULONG32 offset = ILoffset + 1;
-    hr = pCode->GetCode(offset, offset+sizeof(mdToken), sizeof(mdToken), (BYTE*)&mdFunction, &fetched);
-    if (FAILED(hr) || fetched != sizeof(mdToken))
-        return CORDBG_E_INVALID_OPCODE;
-    if (pFunction)
-        *pFunction = mdFunction;
-    return GetSigParserFromFunction(mdFunction, pClass, parser, generics);
-}
-HRESULT CordbNativeCode::GetReturnValueLiveOffsetImpl(Instantiation *currentInstantiation, ULONG32 ILoffset, ULONG32 bufferSize, ULONG32 *pFetched, ULONG32 *pOffsets)
-{
-    if (pFetched == NULL)
-        return E_INVALIDARG;
-    HRESULT hr = S_OK;
-    ULONG32 found = 0;
-    SigParser signature, generics;
-    mdToken mdClass = 0;
-    IfFailRet(GetCallSignature(ILoffset, &mdClass, NULL, signature, generics));
-    IfFailRet(EnsureReturnValueAllowed(currentInstantiation, mdClass, signature, generics));
-    SequencePoints *pSP = GetSequencePoints();
-    DebuggerILToNativeMap *pMap = pSP->GetCallsiteMapAddr();
-    for (ULONG32 i = 0; i < pSP->GetCallsiteEntryCount() && pMap; ++i, pMap++)
-    {
-        if (pMap->ilOffset == ILoffset && (pMap->source & ICorDebugInfo::CALL_INSTRUCTION) == ICorDebugInfo::CALL_INSTRUCTION)
-        {
-            if (pOffsets && found < bufferSize)
-            {
-                BYTE nativeBuffer[8];
-                ULONG32 fetched = 0;
-                IfFailRet(GetCode(pMap->nativeStartOffset, pMap->nativeStartOffset+ARRAY_SIZE(nativeBuffer), ARRAY_SIZE(nativeBuffer), nativeBuffer, &fetched));
-                int skipBytes = 0;
-#if defined(PSEUDORANDOM_NOP_INSERTION)
-                const BYTE nop_opcode = 0x90;
-                while (fetched && nativeBuffer[0] == nop_opcode)
-                {
-                    skipBytes++;
-                    for (int j = 1; j < ARRAY_SIZE(nativeBuffer) && nativeBuffer[j] == nop_opcode; ++j)
-                        skipBytes++;
-                    IfFailRet(GetCode(pMap->nativeStartOffset+skipBytes, pMap->nativeStartOffset+skipBytes+ARRAY_SIZE(nativeBuffer), ARRAY_SIZE(nativeBuffer), nativeBuffer, &fetched));
-                }
-#endif
-                int offset = GetCallInstructionLength(nativeBuffer, fetched);
-                if (offset == -1)
-                    return E_UNEXPECTED; // Could not decode instruction, this should never happen.
-                pOffsets[found] = pMap->nativeStartOffset + offset + skipBytes;
-            }
-            found++;
-        }
-    }
-    if (pOffsets)
-        *pFetched = found < bufferSize ? found : bufferSize;
-    else
-        *pFetched = found;
-    if (found == 0)
-        return E_FAIL;
-    if (pOffsets && found > bufferSize)
-        return S_FALSE;
-    return S_OK;
-}
-CordbNativeCode * CordbModule::LookupOrCreateNativeCode(mdMethodDef methodToken,
-                                                        VMPTR_MethodDesc methodDesc,
-                                                        CORDB_ADDRESS startAddress)
-{
-    INTERNAL_SYNC_API_ENTRY(GetProcess());
-    _ASSERTE(startAddress != NULL);
-    _ASSERTE(methodDesc != VMPTR_MethodDesc::NullPtr());
-    CordbNativeCode * pNativeCode = NULL;
-    NativeCodeFunctionData codeInfo;
-    RSLockHolder lockHolder(GetProcess()->GetProcessLock());
-    pNativeCode = m_nativeCodeTable.GetBase((UINT_PTR) startAddress);
-    if (pNativeCode == NULL)
-    {
-        GetProcess()->GetDAC()->GetNativeCodeInfoForAddr(methodDesc, startAddress, &codeInfo);
-        LOG((LF_CORDB,
-             LL_INFO10000,
-             "R:CT::RSCreating code w/ ver:0x%x, md:0x%x, nativeStart=0x%08x, nativeSize=0x%08x\n",
-             codeInfo.encVersion,
-             VmPtrToCookie(codeInfo.vmNativeCodeMethodDescToken),
-             codeInfo.m_rgCodeRegions[kHot].pAddress,
-             codeInfo.m_rgCodeRegions[kHot].cbSize));
-        CordbFunction* pFunction = CordbModule::LookupOrCreateFunction(methodToken, codeInfo.encVersion);
-        _ASSERTE(pFunction != NULL);
-        pFunction->InitParentClassOfFunction();
-        pNativeCode = new (nothrow)CordbNativeCode(pFunction, &codeInfo, codeInfo.isInstantiatedGeneric != 0);
-        _ASSERTE(pNativeCode != NULL);
-        m_nativeCodeTable.AddBaseOrThrow(pNativeCode);
-    }
-    return pNativeCode;
-} // CordbNativeCode::LookupOrCreateFromJITData
-void CordbNativeCode::LoadNativeInfo()
-{
-    THROW_IF_NEUTERED(this);
-    INTERNAL_API_ENTRY(this->GetProcess());
-    if(m_nativeVarData.IsInitialized())
-    {
-        return;
-    }
-    if (GetFunction()->IsNativeImpl() == CordbFunction::kNativeOnly)
-    {
-        ThrowHR(CORDBG_E_FUNCTION_NOT_IL);
-    }
-     CordbProcess *pProcess = GetProcess();
-    if (m_fCodeAvailable)
-    {
-        RSLockHolder lockHolder(pProcess->GetProcessLock());
-        pProcess->GetDAC()->GetNativeCodeSequencePointsAndVarInfo(GetVMNativeCodeMethodDescToken(),
-                                                                  GetAddress(),
-                                                                  m_fCodeAvailable,
-                                                                  &m_nativeVarData,
-                                                                  &m_sequencePoints);
-    }
-} // CordbNativeCode::LoadNativeInfo

--- a/src/coreclr/gc/gc.cpp
+++ b//dev/null
@@ -1,42652 +0,0 @@
-#include "gcpriv.h"
-#if defined(TARGET_AMD64) && defined(TARGET_WINDOWS)
-#define USE_VXSORT
-#else
-#define USE_INTROSORT
-#endif
-#ifdef DACCESS_COMPILE
-#error this source file should not be compiled with DACCESS_COMPILE!
-#endif //DACCESS_COMPILE
-class gc_rand
-{
-public:
-    static uint64_t x;
-    static uint64_t get_rand()
-    {
-        x = (314159269*x+278281) & 0x7FFFFFFF;
-        return x;
-    }
-    static uint64_t get_rand(uint64_t r) {
-        uint64_t x = (uint64_t)((get_rand() * r) >> 31);
-        return x;
-    }
-};
-uint64_t gc_rand::x = 0;
-#if defined(BACKGROUND_GC) && defined(FEATURE_EVENT_TRACE)
-BOOL bgc_heap_walk_for_etw_p = FALSE;
-#endif //BACKGROUND_GC && FEATURE_EVENT_TRACE
-#define MAX_PTR ((uint8_t*)(~(ptrdiff_t)0))
-#define commit_min_th (16*OS_PAGE_SIZE)
-#define MIN_SOH_CROSS_GEN_REFS (400)
-#define MIN_LOH_CROSS_GEN_REFS (800)
-#ifdef SERVER_GC
-#define partial_size_th 100
-#define num_partial_refs 64
-#else //SERVER_GC
-#define partial_size_th 100
-#define num_partial_refs 32
-#endif //SERVER_GC
-#ifdef USE_REGIONS
-#define demotion_pinned_ratio_th (1)
-#define sip_surv_ratio_th (90)
-#define sip_old_card_surv_ratio_th (90)
-#else
-#define demotion_plug_len_th (6*1024*1024)
-#endif //USE_REGIONS
-#ifdef HOST_64BIT
-#define MARK_STACK_INITIAL_LENGTH 1024
-#else
-#define MARK_STACK_INITIAL_LENGTH 128
-#endif // HOST_64BIT
-#define LOH_PIN_QUEUE_LENGTH 100
-#define LOH_PIN_DECAY 10
-#define UOH_ALLOCATION_RETRY_MAX_COUNT 2
-#define MAX_YP_SPIN_COUNT_UNIT 32768
-uint32_t yp_spin_count_unit = 0;
-uint32_t original_spin_count_unit = 0;
-size_t loh_size_threshold = LARGE_OBJECT_SIZE;
-#ifdef GC_CONFIG_DRIVEN
-int compact_ratio = 0;
-#endif //GC_CONFIG_DRIVEN
-#ifdef FEATURE_SVR_GC
-bool g_built_with_svr_gc = true;
-#else
-bool g_built_with_svr_gc = false;
-#endif // FEATURE_SVR_GC
-#if defined(BUILDENV_DEBUG)
-uint8_t g_build_variant = 0;
-#elif defined(BUILDENV_CHECKED)
-uint8_t g_build_variant = 1;
-#else
-uint8_t g_build_variant = 2;
-#endif //BUILDENV_DEBUG
-VOLATILE(int32_t) g_no_gc_lock = -1;
-#ifdef TRACE_GC
-const char * const allocation_state_str[] = {
-    "start",
-    "can_allocate",
-    "cant_allocate",
-    "retry_allocate",
-    "try_fit",
-    "try_fit_new_seg",
-    "try_fit_after_cg",
-    "try_fit_after_bgc",
-    "try_free_full_seg_in_bgc",
-    "try_free_after_bgc",
-    "try_seg_end",
-    "acquire_seg",
-    "acquire_seg_after_cg",
-    "acquire_seg_after_bgc",
-    "check_and_wait_for_bgc",
-    "trigger_full_compact_gc",
-    "trigger_ephemeral_gc",
-    "trigger_2nd_ephemeral_gc",
-    "check_retry_seg"
-};
-const char * const msl_take_state_str[] = {
-    "get_large_seg",
-    "bgc_loh_sweep",
-    "wait_bgc",
-    "block_gc",
-    "clr_mem",
-    "clr_large_mem",
-    "t_eph_gc",
-    "t_full_gc",
-    "alloc_small",
-    "alloc_large",
-    "alloc_small_cant",
-    "alloc_large_cant",
-    "try_alloc",
-    "try_budget"
-};
-#endif //TRACE_GC
-#if (defined(DT_LOG) || defined(TRACE_GC))
-static const char* const str_gc_reasons[] =
-{
-    "alloc_soh",
-    "induced",
-    "lowmem",
-    "empty",
-    "alloc_loh",
-    "oos_soh",
-    "oos_loh",
-    "induced_noforce",
-    "gcstress",
-    "induced_lowmem",
-    "induced_compacting",
-    "lowmemory_host",
-    "pm_full_gc",
-    "lowmemory_host_blocking"
-};
-static const char* const str_gc_pause_modes[] =
-{
-    "batch",
-    "interactive",
-    "low_latency",
-    "sustained_low_latency",
-    "no_gc"
-};
-static const char* const str_root_kinds[] = {
-    "Stack",
-    "FinalizeQueue",
-    "Handles",
-    "OlderGen",
-    "SizedRef",
-    "Overflow",
-    "DependentHandles",
-    "NewFQ",
-    "Steal",
-    "BGC"
-};
-#endif //DT_LOG || TRACE_GC
-inline
-BOOL is_induced (gc_reason reason)
-{
-    return ((reason == reason_induced) ||
-            (reason == reason_induced_noforce) ||
-            (reason == reason_lowmemory) ||
-            (reason == reason_lowmemory_blocking) ||
-            (reason == reason_induced_compacting) ||
-            (reason == reason_induced_aggressive) ||
-            (reason == reason_lowmemory_host) ||
-            (reason == reason_lowmemory_host_blocking));
-}
-inline
-BOOL is_induced_blocking (gc_reason reason)
-{
-    return ((reason == reason_induced) ||
-            (reason == reason_lowmemory_blocking) ||
-            (reason == reason_induced_compacting) ||
-            (reason == reason_induced_aggressive) ||
-            (reason == reason_lowmemory_host_blocking));
-}
-gc_oh_num gen_to_oh(int gen)
-{
-    switch (gen)
-    {
-        case soh_gen0:
-            return gc_oh_num::soh;
-        case soh_gen1:
-            return gc_oh_num::soh;
-        case soh_gen2:
-            return gc_oh_num::soh;
-        case loh_generation:
-            return gc_oh_num::loh;
-        case poh_generation:
-            return gc_oh_num::poh;
-        default:
-            assert(false);
-            return gc_oh_num::unknown;
-    }
-}
-uint64_t qpf;
-double qpf_ms;
-double qpf_us;
-uint64_t GetHighPrecisionTimeStamp()
-{
-    int64_t ts = GCToOSInterface::QueryPerformanceCounter();
-    return (uint64_t)((double)ts * qpf_us);
-}
-uint64_t RawGetHighPrecisionTimeStamp()
-{
-    return (uint64_t)GCToOSInterface::QueryPerformanceCounter();
-}
-#ifdef BGC_SERVO_TUNING
-bool gc_heap::bgc_tuning::enable_fl_tuning = false;
-uint32_t gc_heap::bgc_tuning::memory_load_goal = 0;
-uint32_t gc_heap::bgc_tuning::memory_load_goal_slack = 0;
-uint64_t gc_heap::bgc_tuning::available_memory_goal = 0;
-bool gc_heap::bgc_tuning::panic_activated_p = false;
-double gc_heap::bgc_tuning::accu_error_panic = 0.0;
-double gc_heap::bgc_tuning::above_goal_kp = 0.0;
-double gc_heap::bgc_tuning::above_goal_ki = 0.0;
-bool gc_heap::bgc_tuning::enable_kd = false;
-bool gc_heap::bgc_tuning::enable_ki = false;
-bool gc_heap::bgc_tuning::enable_smooth = false;
-bool gc_heap::bgc_tuning::enable_tbh = false;
-bool gc_heap::bgc_tuning::enable_ff = false;
-bool gc_heap::bgc_tuning::enable_gradual_d = false;
-double gc_heap::bgc_tuning::above_goal_kd = 0.0;
-double gc_heap::bgc_tuning::above_goal_ff = 0.0;
-double gc_heap::bgc_tuning::num_gen1s_smooth_factor = 0.0;
-double gc_heap::bgc_tuning::ml_kp = 0.0;
-double gc_heap::bgc_tuning::ml_ki = 0.0;
-double gc_heap::bgc_tuning::accu_error = 0.0;
-bool gc_heap::bgc_tuning::fl_tuning_triggered = false;
-size_t gc_heap::bgc_tuning::num_bgcs_since_tuning_trigger = 0;
-bool gc_heap::bgc_tuning::next_bgc_p = false;
-size_t gc_heap::bgc_tuning::gen1_index_last_bgc_end;
-size_t gc_heap::bgc_tuning::gen1_index_last_bgc_start;
-size_t gc_heap::bgc_tuning::gen1_index_last_bgc_sweep;
-size_t gc_heap::bgc_tuning::actual_num_gen1s_to_trigger;
-gc_heap::bgc_tuning::tuning_calculation gc_heap::bgc_tuning::gen_calc[2];
-gc_heap::bgc_tuning::tuning_stats gc_heap::bgc_tuning::gen_stats[2];
-gc_heap::bgc_tuning::bgc_size_data gc_heap::bgc_tuning::current_bgc_end_data[2];
-size_t gc_heap::bgc_tuning::last_stepping_bgc_count = 0;
-uint32_t gc_heap::bgc_tuning::last_stepping_mem_load = 0;
-uint32_t gc_heap::bgc_tuning::stepping_interval = 0;
-bool gc_heap::bgc_tuning::use_stepping_trigger_p = true;
-double gc_heap::bgc_tuning::gen2_ratio_correction = 0.0;
-double gc_heap::bgc_tuning::ratio_correction_step = 0.0;
-int gc_heap::saved_bgc_tuning_reason = -1;
-#endif //BGC_SERVO_TUNING
-inline
-size_t round_up_power2 (size_t size)
-{
-    DWORD highest_set_bit_index;
-    if (0 ==
-#ifdef HOST_64BIT
-        BitScanReverse64(
-#else
-        BitScanReverse(
-#endif
-            &highest_set_bit_index, size - 1)) { return 1; }
-    return static_cast<size_t>(2) << highest_set_bit_index;
-}
-inline
-size_t round_down_power2 (size_t size)
-{
-    DWORD highest_set_bit_index;
-    if (0 ==
-#ifdef HOST_64BIT
-        BitScanReverse64(
-#else
-        BitScanReverse(
-#endif
-            &highest_set_bit_index, size)) { return 0; }
-    return static_cast<size_t>(1) << highest_set_bit_index;
-}
-inline
-int index_of_highest_set_bit (size_t value)
-{
-    DWORD highest_set_bit_index;
-    return (0 ==
-#ifdef HOST_64BIT
-        BitScanReverse64(
-#else
-        BitScanReverse(
-#endif
-            &highest_set_bit_index, value)) ? -1 : static_cast<int>(highest_set_bit_index);
-}
-inline
-int relative_index_power2_plug (size_t power2)
-{
-    int index = index_of_highest_set_bit (power2);
-    assert (index <= MAX_INDEX_POWER2);
-    return ((index < MIN_INDEX_POWER2) ? 0 : (index - MIN_INDEX_POWER2));
-}
-inline
-int relative_index_power2_free_space (size_t power2)
-{
-    int index = index_of_highest_set_bit (power2);
-    assert (index <= MAX_INDEX_POWER2);
-    return ((index < MIN_INDEX_POWER2) ? -1 : (index - MIN_INDEX_POWER2));
-}
-#ifdef BACKGROUND_GC
-uint32_t bgc_alloc_spin_count = 140;
-uint32_t bgc_alloc_spin_count_loh = 16;
-uint32_t bgc_alloc_spin = 2;
-inline
-void c_write (uint32_t& place, uint32_t value)
-{
-    Interlocked::Exchange (&place, value);
-}
-const size_t bgc_min_per_heap = 4*1024*1024;
-int gc_heap::gchist_index = 0;
-gc_mechanisms_store gc_heap::gchist[max_history_count];
-#ifndef MULTIPLE_HEAPS
-VOLATILE(bgc_state) gc_heap::current_bgc_state = bgc_not_in_process;
-int gc_heap::gchist_index_per_heap = 0;
-gc_heap::gc_history gc_heap::gchist_per_heap[max_history_count];
-#endif //MULTIPLE_HEAPS
-#endif //BACKGROUND_GC
-void gc_heap::add_to_history_per_heap()
-{
-#if defined(GC_HISTORY) && defined(BACKGROUND_GC)
-    gc_history* current_hist = &gchist_per_heap[gchist_index_per_heap];
-    current_hist->gc_index = settings.gc_index;
-    current_hist->current_bgc_state = current_bgc_state;
-    size_t elapsed = dd_gc_elapsed_time (dynamic_data_of (0));
-    current_hist->gc_time_ms = (uint32_t)(elapsed / 1000);
-    current_hist->gc_efficiency = (elapsed ? (total_promoted_bytes / elapsed) : total_promoted_bytes);
-#ifndef USE_REGIONS
-    current_hist->eph_low = generation_allocation_start (generation_of (max_generation - 1));
-    current_hist->gen0_start = generation_allocation_start (generation_of (0));
-    current_hist->eph_high = heap_segment_allocated (ephemeral_heap_segment);
-#endif //!USE_REGIONS
-#ifdef BACKGROUND_GC
-    current_hist->bgc_lowest = background_saved_lowest_address;
-    current_hist->bgc_highest = background_saved_highest_address;
-#endif //BACKGROUND_GC
-    current_hist->fgc_lowest = lowest_address;
-    current_hist->fgc_highest = highest_address;
-    current_hist->g_lowest = g_gc_lowest_address;
-    current_hist->g_highest = g_gc_highest_address;
-    gchist_index_per_heap++;
-    if (gchist_index_per_heap == max_history_count)
-    {
-        gchist_index_per_heap = 0;
-    }
-#endif //GC_HISTORY && BACKGROUND_GC
-}
-void gc_heap::add_to_history()
-{
-#if defined(GC_HISTORY) && defined(BACKGROUND_GC)
-    gc_mechanisms_store* current_settings = &gchist[gchist_index];
-    current_settings->store (&settings);
-    gchist_index++;
-    if (gchist_index == max_history_count)
-    {
-        gchist_index = 0;
-    }
-#endif //GC_HISTORY && BACKGROUND_GC
-}
-#if defined(TRACE_GC) && defined(SIMPLE_DPRINTF)
-BOOL   gc_log_on = TRUE;
-FILE* gc_log = NULL;
-size_t gc_log_file_size = 0;
-size_t gc_buffer_index = 0;
-size_t max_gc_buffers = 0;
-static CLRCriticalSection gc_log_lock;
-#define gc_log_buffer_size (1024*1024)
-uint8_t* gc_log_buffer = 0;
-size_t gc_log_buffer_offset = 0;
-void flush_gc_log (bool close)
-{
-    if (gc_log_on && (gc_log != NULL))
-    {
-        fwrite(gc_log_buffer, gc_log_buffer_offset, 1, gc_log);
-        fflush(gc_log);
-        if (close)
-        {
-            fclose(gc_log);
-            gc_log_on = false;
-            gc_log = NULL;
-        }
-        gc_log_buffer_offset = 0;
-    }
-}
-void log_va_msg(const char *fmt, va_list args)
-{
-    gc_log_lock.Enter();
-    const int BUFFERSIZE = 4096;
-    static char rgchBuffer[BUFFERSIZE];
-    char *  pBuffer  = &rgchBuffer[0];
-    pBuffer[0] = '\n';
-    int buffer_start = 1;
-    int pid_len = sprintf_s (&pBuffer[buffer_start], BUFFERSIZE - buffer_start,
-        "[%5d]", (uint32_t)GCToOSInterface::GetCurrentThreadIdForLogging());
-    buffer_start += pid_len;
-    memset(&pBuffer[buffer_start], '-', BUFFERSIZE - buffer_start);
-    int msg_len = _vsnprintf_s (&pBuffer[buffer_start], BUFFERSIZE - buffer_start, _TRUNCATE, fmt, args);
-    if (msg_len == -1)
-    {
-        msg_len = BUFFERSIZE - buffer_start;
-    }
-    msg_len += buffer_start;
-    if ((gc_log_buffer_offset + msg_len) > (gc_log_buffer_size - 12))
-    {
-        char index_str[8];
-        memset (index_str, '-', 8);
-        sprintf_s (index_str, ARRAY_SIZE(index_str), "%d", (int)gc_buffer_index);
-        gc_log_buffer[gc_log_buffer_offset] = '\n';
-        memcpy (gc_log_buffer + (gc_log_buffer_offset + 1), index_str, 8);
-        gc_buffer_index++;
-        if (gc_buffer_index > max_gc_buffers)
-        {
-            fseek (gc_log, 0, SEEK_SET);
-            gc_buffer_index = 0;
-        }
-        fwrite(gc_log_buffer, gc_log_buffer_size, 1, gc_log);
-        fflush(gc_log);
-        memset (gc_log_buffer, '*', gc_log_buffer_size);
-        gc_log_buffer_offset = 0;
-    }
-    memcpy (gc_log_buffer + gc_log_buffer_offset, pBuffer, msg_len);
-    gc_log_buffer_offset += msg_len;
-    gc_log_lock.Leave();
-}
-void GCLog (const char *fmt, ... )
-{
-    if (gc_log_on && (gc_log != NULL))
-    {
-        va_list     args;
-        va_start(args, fmt);
-        log_va_msg (fmt, args);
-        va_end(args);
-    }
-}
-#endif //TRACE_GC && SIMPLE_DPRINTF
-#ifdef GC_CONFIG_DRIVEN
-BOOL   gc_config_log_on = FALSE;
-FILE* gc_config_log = NULL;
-#define gc_config_log_buffer_size (1*1024) // TEMP
-uint8_t* gc_config_log_buffer = 0;
-size_t gc_config_log_buffer_offset = 0;
-void log_va_msg_config(const char *fmt, va_list args)
-{
-    const int BUFFERSIZE = 256;
-    static char rgchBuffer[BUFFERSIZE];
-    char *  pBuffer  = &rgchBuffer[0];
-    pBuffer[0] = '\n';
-    int buffer_start = 1;
-    int msg_len = _vsnprintf_s (&pBuffer[buffer_start], BUFFERSIZE - buffer_start, _TRUNCATE, fmt, args );
-    assert (msg_len != -1);
-    msg_len += buffer_start;
-    if ((gc_config_log_buffer_offset + msg_len) > gc_config_log_buffer_size)
-    {
-        fwrite(gc_config_log_buffer, gc_config_log_buffer_offset, 1, gc_config_log);
-        fflush(gc_config_log);
-        gc_config_log_buffer_offset = 0;
-    }
-    memcpy (gc_config_log_buffer + gc_config_log_buffer_offset, pBuffer, msg_len);
-    gc_config_log_buffer_offset += msg_len;
-}
-void GCLogConfig (const char *fmt, ... )
-{
-    if (gc_config_log_on && (gc_config_log != NULL))
-    {
-        va_list     args;
-        va_start( args, fmt );
-        log_va_msg_config (fmt, args);
-    }
-}
-#endif // GC_CONFIG_DRIVEN
-void GCHeap::Shutdown()
-{
-#if defined(TRACE_GC) && defined(SIMPLE_DPRINTF) && !defined(BUILD_AS_STANDALONE)
-    flush_gc_log (true);
-#endif //TRACE_GC && SIMPLE_DPRINTF && !BUILD_AS_STANDALONE
-}
-#ifdef SYNCHRONIZATION_STATS
-static unsigned int         gc_count_during_log;
-static const unsigned int   log_interval = 5000;
-static uint64_t             log_start_tick;
-static unsigned int         gc_lock_contended;
-static int64_t              log_start_hires;
-static uint64_t             suspend_ee_during_log;
-static uint64_t             restart_ee_during_log;
-static uint64_t             gc_during_log;
-#endif //SYNCHRONIZATION_STATS
-void
-init_sync_log_stats()
-{
-#ifdef SYNCHRONIZATION_STATS
-    if (gc_count_during_log == 0)
-    {
-        gc_heap::init_sync_stats();
-        suspend_ee_during_log = 0;
-        restart_ee_during_log = 0;
-        gc_during_log = 0;
-        gc_lock_contended = 0;
-        log_start_tick = GCToOSInterface::GetLowPrecisionTimeStamp();
-        log_start_hires = GCToOSInterface::QueryPerformanceCounter();
-    }
-    gc_count_during_log++;
-#endif //SYNCHRONIZATION_STATS
-}
-void
-process_sync_log_stats()
-{
-#ifdef SYNCHRONIZATION_STATS
-    uint64_t log_elapsed = GCToOSInterface::GetLowPrecisionTimeStamp() - log_start_tick;
-    if (log_elapsed > log_interval)
-    {
-        uint64_t total = GCToOSInterface::QueryPerformanceCounter() - log_start_hires;
-        printf("\n_________________________________________________________________________________\n"
-            "Past %d(s): #%3d GCs; Total gc_lock contended: %8u; GC: %12u\n"
-            "SuspendEE: %8u; RestartEE: %8u GC %.3f%%\n",
-            log_interval / 1000,
-            gc_count_during_log,
-            gc_lock_contended,
-            (unsigned int)(gc_during_log / gc_count_during_log),
-            (unsigned int)(suspend_ee_during_log / gc_count_during_log),
-            (unsigned int)(restart_ee_during_log / gc_count_during_log),
-            (double)(100.0f * gc_during_log / total));
-        gc_heap::print_sync_stats(gc_count_during_log);
-        gc_count_during_log = 0;
-    }
-#endif //SYNCHRONIZATION_STATS
-}
-#ifdef MULTIPLE_HEAPS
-uint32_t g_num_active_processors = 0;
-enum gc_join_stage
-{
-    gc_join_init_cpu_mapping = 0,
-    gc_join_done = 1,
-    gc_join_generation_determined = 2,
-    gc_join_begin_mark_phase = 3,
-    gc_join_scan_dependent_handles = 4,
-    gc_join_rescan_dependent_handles = 5,
-    gc_join_scan_sizedref_done = 6,
-    gc_join_null_dead_short_weak = 7,
-    gc_join_scan_finalization = 8,
-    gc_join_null_dead_long_weak = 9,
-    gc_join_null_dead_syncblk = 10,
-    gc_join_decide_on_compaction = 11,
-    gc_join_rearrange_segs_compaction = 12,
-    gc_join_adjust_handle_age_compact = 13,
-    gc_join_adjust_handle_age_sweep = 14,
-    gc_join_begin_relocate_phase = 15,
-    gc_join_relocate_phase_done = 16,
-    gc_join_verify_objects_done = 17,
-    gc_join_start_bgc = 18,
-    gc_join_restart_ee = 19,
-    gc_join_concurrent_overflow = 20,
-    gc_join_suspend_ee = 21,
-    gc_join_bgc_after_ephemeral = 22,
-    gc_join_allow_fgc = 23,
-    gc_join_bgc_sweep = 24,
-    gc_join_suspend_ee_verify = 25,
-    gc_join_restart_ee_verify = 26,
-    gc_join_set_state_free = 27,
-    gc_r_join_update_card_bundle = 28,
-    gc_join_after_absorb = 29,
-    gc_join_verify_copy_table = 30,
-    gc_join_after_reset = 31,
-    gc_join_after_ephemeral_sweep = 32,
-    gc_join_after_profiler_heap_walk = 33,
-    gc_join_minimal_gc = 34,
-    gc_join_after_commit_soh_no_gc = 35,
-    gc_join_expand_loh_no_gc = 36,
-    gc_join_final_no_gc = 37,
-    gc_join_disable_software_write_watch = 38,
-    gc_join_merge_temp_fl = 39,
-    gc_join_max = 40
-};
-enum gc_join_flavor
-{
-    join_flavor_server_gc = 0,
-    join_flavor_bgc = 1
-};
-#define first_thread_arrived 2
-#pragma warning(push)
-#pragma warning(disable:4324) // don't complain if DECLSPEC_ALIGN actually pads
-struct DECLSPEC_ALIGN(HS_CACHE_LINE_SIZE) join_structure
-{
-    int n_threads;
-    DECLSPEC_ALIGN(HS_CACHE_LINE_SIZE)
-    GCEvent joined_event[3]; // the last event in the array is only used for first_thread_arrived.
-    Volatile<int> lock_color;
-    VOLATILE(BOOL) wait_done;
-    VOLATILE(BOOL) joined_p;
-    DECLSPEC_ALIGN(HS_CACHE_LINE_SIZE)
-    VOLATILE(int) join_lock;
-    VOLATILE(int) r_join_lock;
-};
-#pragma warning(pop)
-enum join_type
-{
-    type_last_join = 0,
-    type_join = 1,
-    type_restart = 2,
-    type_first_r_join = 3,
-    type_r_join = 4
-};
-enum join_time
-{
-    time_start = 0,
-    time_end = 1
-};
-enum join_heap_index
-{
-    join_heap_restart = 100,
-    join_heap_r_restart = 200
-};
-class t_join
-{
-    join_structure join_struct;
-    int id;
-    gc_join_flavor flavor;
-#ifdef JOIN_STATS
-    uint64_t start[MAX_SUPPORTED_CPUS], end[MAX_SUPPORTED_CPUS], start_seq;
-    int thd;
-    uint64_t start_tick;
-    uint64_t elapsed_total[gc_join_max], wake_total[gc_join_max], seq_loss_total[gc_join_max], par_loss_total[gc_join_max], in_join_total[gc_join_max];
-#endif //JOIN_STATS
-public:
-    BOOL init (int n_th, gc_join_flavor f)
-    {
-        dprintf (JOIN_LOG, ("Initializing join structure"));
-        join_struct.n_threads = n_th;
-        join_struct.lock_color = 0;
-        for (int i = 0; i < 3; i++)
-        {
-            if (!join_struct.joined_event[i].IsValid())
-            {
-                join_struct.joined_p = FALSE;
-                dprintf (JOIN_LOG, ("Creating join event %d", i));
-                if (!join_struct.joined_event[i].CreateManualEventNoThrow(FALSE))
-                    return FALSE;
-            }
-        }
-        join_struct.join_lock = join_struct.n_threads;
-        join_struct.r_join_lock = join_struct.n_threads;
-        join_struct.wait_done = FALSE;
-        flavor = f;
-#ifdef JOIN_STATS
-        start_tick = GCToOSInterface::GetLowPrecisionTimeStamp();
-#endif //JOIN_STATS
-        return TRUE;
-    }
-    void update_n_threads(int n_th)
-    {
-        join_struct.n_threads = n_th;
-        join_struct.join_lock = n_th;
-        join_struct.r_join_lock = n_th;
-    }
-    int get_num_threads()
-    {
-        return join_struct.n_threads;
-    }
-    void destroy ()
-    {
-        dprintf (JOIN_LOG, ("Destroying join structure"));
-        for (int i = 0; i < 3; i++)
-        {
-            if (join_struct.joined_event[i].IsValid())
-                join_struct.joined_event[i].CloseEvent();
-        }
-    }
-    inline void fire_event (int heap, join_time time, join_type type, int join_id)
-    {
-        FIRE_EVENT(GCJoin_V2, heap, time, type, join_id);
-    }
-    void join (gc_heap* gch, int join_id)
-    {
-#ifdef JOIN_STATS
-        end[gch->heap_number] = get_ts();
-#endif //JOIN_STATS
-        assert (!join_struct.joined_p);
-        int color = join_struct.lock_color.LoadWithoutBarrier();
-        if (Interlocked::Decrement(&join_struct.join_lock) != 0)
-        {
-            dprintf (JOIN_LOG, ("join%d(%d): Join() Waiting...join_lock is now %d",
-                flavor, join_id, (int32_t)(join_struct.join_lock)));
-            fire_event (gch->heap_number, time_start, type_join, join_id);
-            if (color == join_struct.lock_color.LoadWithoutBarrier())
-            {
-respin:
-                int spin_count = 128 * yp_spin_count_unit;
-                for (int j = 0; j < spin_count; j++)
-                {
-                    if (color != join_struct.lock_color.LoadWithoutBarrier())
-                    {
-                        break;
-                    }
-                    YieldProcessor();           // indicate to the processor that we are spinning
-                }
-                if (color == join_struct.lock_color.LoadWithoutBarrier())
-                {
-                    dprintf (JOIN_LOG, ("join%d(%d): Join() hard wait on reset event %d, join_lock is now %d",
-                        flavor, join_id, color, (int32_t)(join_struct.join_lock)));
-                    uint32_t dwJoinWait = join_struct.joined_event[color].Wait(INFINITE, FALSE);
-                    if (dwJoinWait != WAIT_OBJECT_0)
-                    {
-                        STRESS_LOG1 (LF_GC, LL_FATALERROR, "joined event wait failed with code: %zx", dwJoinWait);
-                        FATAL_GC_ERROR ();
-                    }
-                }
-                if (color == join_struct.lock_color.LoadWithoutBarrier())
-                {
-                    dprintf (9999, ("---h%d %d j%d %d - respin!!! (c:%d-%d)",
-                        gch->heap_number, join_id, join_struct.n_threads, color, join_struct.lock_color.LoadWithoutBarrier()));
-                    goto respin;
-                }
-                dprintf (JOIN_LOG, ("join%d(%d): Join() done, join_lock is %d",
-                    flavor, join_id, (int32_t)(join_struct.join_lock)));
-            }
-            fire_event (gch->heap_number, time_end, type_join, join_id);
-#ifdef JOIN_STATS
-            start[gch->heap_number] = get_ts();
-            Interlocked::ExchangeAdd(&in_join_total[join_id], (start[gch->heap_number] - end[gch->heap_number]));
-#endif //JOIN_STATS
-        }
-        else
-        {
-            fire_event (gch->heap_number, time_start, type_last_join, join_id);
-            join_struct.joined_p = TRUE;
-            dprintf (JOIN_LOG, ("join%d(%d): Last thread to complete the join, setting id", flavor, join_id));
-            join_struct.joined_event[!color].Reset();
-            id = join_id;
-#ifdef JOIN_STATS
-            thd = gch->heap_number;
-            start_seq = get_ts();
-            Interlocked::ExchangeAdd(&in_join_total[join_id], (start_seq - end[gch->heap_number]));
-#endif //JOIN_STATS
-        }
-    }
-    BOOL r_join (gc_heap* gch, int join_id)
-    {
-        if (join_struct.n_threads == 1)
-        {
-            return TRUE;
-        }
-        if (Interlocked::CompareExchange(&join_struct.r_join_lock, 0, join_struct.n_threads) == 0)
-        {
-            fire_event (gch->heap_number, time_start, type_join, join_id);
-            dprintf (JOIN_LOG, ("r_join() Waiting..."));
-respin:
-            int spin_count = 256 * yp_spin_count_unit;
-            for (int j = 0; j < spin_count; j++)
-            {
-                if (join_struct.wait_done)
-                {
-                    break;
-                }
-                YieldProcessor();           // indicate to the processor that we are spinning
-            }
-            if (!join_struct.wait_done)
-            {
-                dprintf (JOIN_LOG, ("Join() hard wait on reset event %d", first_thread_arrived));
-                uint32_t dwJoinWait = join_struct.joined_event[first_thread_arrived].Wait(INFINITE, FALSE);
-                if (dwJoinWait != WAIT_OBJECT_0)
-                {
-                    STRESS_LOG1 (LF_GC, LL_FATALERROR, "joined event wait failed with code: %zx", dwJoinWait);
-                    FATAL_GC_ERROR ();
-                }
-            }
-            if (!join_struct.wait_done)
-            {
-                goto respin;
-            }
-            dprintf (JOIN_LOG, ("r_join() done"));
-            fire_event (gch->heap_number, time_end, type_join, join_id);
-            return FALSE;
-        }
-        else
-        {
-            fire_event (gch->heap_number, time_start, type_first_r_join, join_id);
-            return TRUE;
-        }
-    }
-#ifdef JOIN_STATS
-    uint64_t get_ts()
-    {
-        return GCToOSInterface::QueryPerformanceCounter();
-    }
-    void start_ts (gc_heap* gch)
-    {
-        start[gch->heap_number] = get_ts();
-    }
-#endif //JOIN_STATS
-    void restart()
-    {
-#ifdef JOIN_STATS
-        uint64_t elapsed_seq = get_ts() - start_seq;
-        uint64_t max = 0, sum = 0, wake = 0;
-        uint64_t min_ts = start[0];
-        for (int i = 1; i < join_struct.n_threads; i++)
-        {
-            if(min_ts > start[i]) min_ts = start[i];
-        }
-        for (int i = 0; i < join_struct.n_threads; i++)
-        {
-            uint64_t wake_delay = start[i] - min_ts;
-            uint64_t elapsed = end[i] - start[i];
-            if (max < elapsed)
-                max = elapsed;
-            sum += elapsed;
-            wake += wake_delay;
-        }
-        uint64_t seq_loss = (join_struct.n_threads - 1)*elapsed_seq;
-        uint64_t par_loss = join_struct.n_threads*max - sum;
-        double efficiency = 0.0;
-        if (max > 0)
-            efficiency = sum*100.0/(join_struct.n_threads*max);
-        const double ts_scale = 1e-6;
-        elapsed_total[id] += sum;
-        wake_total[id] += wake;
-        seq_loss_total[id] += seq_loss;
-        par_loss_total[id] += par_loss;
-        if (GCToOSInterface::GetLowPrecisionTimeStamp() - start_tick > 10*1000)
-        {
-            printf("**** summary *****\n");
-            for (int i = 0; i < 16; i++)
-            {
-                printf("join #%3d  elapsed_total = %8g wake_loss = %8g seq_loss = %8g  par_loss = %8g  in_join_total = %8g\n",
-                   i,
-                   ts_scale*elapsed_total[i],
-                   ts_scale*wake_total[i],
-                   ts_scale*seq_loss_total[i],
-                   ts_scale*par_loss_total[i],
-                   ts_scale*in_join_total[i]);
-                elapsed_total[i] = wake_total[i] = seq_loss_total[i] = par_loss_total[i] = in_join_total[i] = 0;
-            }
-            start_tick = GCToOSInterface::GetLowPrecisionTimeStamp();
-        }
-#endif //JOIN_STATS
-        fire_event (join_heap_restart, time_start, type_restart, -1);
-        assert (join_struct.joined_p);
-        join_struct.joined_p = FALSE;
-        join_struct.join_lock = join_struct.n_threads;
-        dprintf (JOIN_LOG, ("join%d(%d): Restarting from join: join_lock is %d", flavor, id, (int32_t)(join_struct.join_lock)));
-        int color = join_struct.lock_color.LoadWithoutBarrier();
-        join_struct.lock_color = !color;
-        join_struct.joined_event[color].Set();
-        fire_event (join_heap_restart, time_end, type_restart, -1);
-#ifdef JOIN_STATS
-        start[thd] = get_ts();
-#endif //JOIN_STATS
-    }
-    BOOL joined()
-    {
-        dprintf (JOIN_LOG, ("join%d(%d): joined, join_lock is %d", flavor, id, (int32_t)(join_struct.join_lock)));
-        return join_struct.joined_p;
-    }
-    void r_restart()
-    {
-        if (join_struct.n_threads != 1)
-        {
-            fire_event (join_heap_r_restart, time_start, type_restart, -1);
-            join_struct.wait_done = TRUE;
-            join_struct.joined_event[first_thread_arrived].Set();
-            fire_event (join_heap_r_restart, time_end, type_restart, -1);
-        }
-    }
-    void r_init()
-    {
-        if (join_struct.n_threads != 1)
-        {
-            join_struct.r_join_lock = join_struct.n_threads;
-            join_struct.wait_done = FALSE;
-            join_struct.joined_event[first_thread_arrived].Reset();
-        }
-    }
-};
-t_join gc_t_join;
-#ifdef BACKGROUND_GC
-t_join bgc_t_join;
-#endif //BACKGROUND_GC
-#endif //MULTIPLE_HEAPS
-#define spin_and_switch(count_to_spin, expr) \
-{ \
-    for (int j = 0; j < count_to_spin; j++) \
-    { \
-        if (expr) \
-        { \
-            break;\
-        } \
-        YieldProcessor(); \
-    } \
-    if (!(expr)) \
-    { \
-        GCToOSInterface::YieldThread(0); \
-    } \
-}
-#define spin_and_wait(count_to_spin, expr) \
-{ \
-    while (!expr) \
-    { \
-        for (int j = 0; j < count_to_spin; j++) \
-        { \
-            if (expr) \
-            { \
-                break; \
-            } \
-                YieldProcessor (); \
-        } \
-        if (!(expr)) \
-        { \
-            GCToOSInterface::YieldThread (0); \
-        } \
-    } \
-}
-#ifdef BACKGROUND_GC
-#define max_pending_allocs 64
-class exclusive_sync
-{
-    VOLATILE(uint8_t*) rwp_object;
-    VOLATILE(int32_t) needs_checking;
-    int spin_count;
-    uint8_t cache_separator[HS_CACHE_LINE_SIZE - (sizeof (spin_count) + sizeof (needs_checking) + sizeof (rwp_object))];
-    VOLATILE(uint8_t*) alloc_objects[max_pending_allocs];
-    int find_free_index ()
-    {
-        for (int i = 0; i < max_pending_allocs; i++)
-        {
-            if (alloc_objects [i] == (uint8_t*)0)
-            {
-                return i;
-            }
-        }
-        return -1;
-    }
-public:
-    void init()
-    {
-        spin_count = 32 * (g_num_processors - 1);
-        rwp_object = 0;
-        needs_checking = 0;
-        for (int i = 0; i < max_pending_allocs; i++)
-        {
-            alloc_objects [i] = (uint8_t*)0;
-        }
-    }
-    void check()
-    {
-        for (int i = 0; i < max_pending_allocs; i++)
-        {
-            if (alloc_objects [i] != (uint8_t*)0)
-            {
-                FATAL_GC_ERROR();
-            }
-        }
-    }
-    void bgc_mark_set (uint8_t* obj)
-    {
-        dprintf (3, ("cm: probing %p", obj));
-retry:
-        if (Interlocked::CompareExchange(&needs_checking, 1, 0) == 0)
-        {
-            for (int i = 0; i < max_pending_allocs; i++)
-            {
-                if (obj == alloc_objects[i])
-                {
-                    needs_checking = 0;
-                    dprintf (3, ("cm: will spin"));
-                    spin_and_switch (spin_count, (obj != alloc_objects[i]));
-                    goto retry;
-                }
-            }
-            rwp_object = obj;
-            needs_checking = 0;
-            dprintf (3, ("cm: set %p", obj));
-            return;
-        }
-        else
-        {
-            spin_and_switch (spin_count, (needs_checking == 0));
-            goto retry;
-        }
-    }
-    int uoh_alloc_set (uint8_t* obj)
-    {
-        if (!gc_heap::cm_in_progress)
-        {
-            return -1;
-        }
-retry:
-        dprintf (3, ("uoh alloc: probing %p", obj));
-        if (Interlocked::CompareExchange(&needs_checking, 1, 0) == 0)
-        {
-            if (obj == rwp_object)
-            {
-                needs_checking = 0;
-                spin_and_switch (spin_count, (obj != rwp_object));
-                goto retry;
-            }
-            else
-            {
-                int cookie = find_free_index();
-                if (cookie != -1)
-                {
-                    alloc_objects[cookie] = obj;
-                    needs_checking = 0;
-                    dprintf (3, ("uoh alloc: set %p at %d", obj, cookie));
-                    return cookie;
-                }
-                else
-                {
-                    needs_checking = 0;
-                    dprintf (3, ("uoh alloc: setting %p will spin to acquire a free index", obj));
-                    spin_and_switch (spin_count, (find_free_index () != -1));
-                    goto retry;
-                }
-            }
-        }
-        else
-        {
-            dprintf (3, ("uoh alloc: will spin on checking %p", obj));
-            spin_and_switch (spin_count, (needs_checking == 0));
-            goto retry;
-        }
-    }
-    void bgc_mark_done ()
-    {
-        dprintf (3, ("cm: release lock on %p", (uint8_t *)rwp_object));
-        rwp_object = 0;
-    }
-    void uoh_alloc_done_with_index (int index)
-    {
-        dprintf (3, ("uoh alloc: release lock on %p based on %d", (uint8_t *)alloc_objects[index], index));
-        assert ((index >= 0) && (index < max_pending_allocs));
-        alloc_objects[index] = (uint8_t*)0;
-    }
-    void uoh_alloc_done (uint8_t* obj)
-    {
-        if (!gc_heap::cm_in_progress)
-        {
-            return;
-        }
-        for (int i = 0; i < max_pending_allocs; i++)
-        {
-            if (alloc_objects [i] == obj)
-            {
-                uoh_alloc_done_with_index(i);
-                return;
-            }
-        }
-        dprintf (3, ("uoh alloc: could not release lock on %p", obj));
-    }
-};
-#endif //BACKGROUND_GC
-void reset_memory (uint8_t* o, size_t sizeo);
-#ifdef WRITE_WATCH
-#ifndef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-static bool virtual_alloc_hardware_write_watch = false;
-#endif // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-static bool hardware_write_watch_capability = false;
-void hardware_write_watch_api_supported()
-{
-    if (GCToOSInterface::SupportsWriteWatch())
-    {
-        hardware_write_watch_capability = true;
-        dprintf (2, ("WriteWatch supported"));
-    }
-    else
-    {
-        dprintf (2,("WriteWatch not supported"));
-    }
-}
-inline bool can_use_hardware_write_watch()
-{
-    return hardware_write_watch_capability;
-}
-inline bool can_use_write_watch_for_gc_heap()
-{
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    return true;
-#else // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    return can_use_hardware_write_watch();
-#endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-}
-inline bool can_use_write_watch_for_card_table()
-{
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-    return true;
-#else
-    return can_use_hardware_write_watch();
-#endif
-}
-#else
-#define mem_reserve (MEM_RESERVE)
-#endif //WRITE_WATCH
-void WaitLongerNoInstru (int i)
-{
-    bool bToggleGC = GCToEEInterface::EnablePreemptiveGC();
-    if (g_fSuspensionPending == 0)
-    {
-        if  (g_num_processors > 1)
-        {
-            YieldProcessor();           // indicate to the processor that we are spinning
-            if  (i & 0x01f)
-                GCToOSInterface::YieldThread (0);
-            else
-                GCToOSInterface::Sleep (5);
-        }
-        else
-            GCToOSInterface::Sleep (5);
-    }
-    if (bToggleGC)
-    {
-#ifdef _DEBUG
-        if (gc_heap::gc_started)
-        {
-            gc_heap::wait_for_gc_done();
-        }
-#endif // _DEBUG
-        GCToEEInterface::DisablePreemptiveGC();
-    }
-    else if (g_fSuspensionPending > 0)
-    {
-        g_theGCHeap->WaitUntilGCComplete();
-    }
-}
-inline
-static void safe_switch_to_thread()
-{
-    bool cooperative_mode = gc_heap::enable_preemptive();
-    GCToOSInterface::YieldThread(0);
-    gc_heap::disable_preemptive(cooperative_mode);
-}
-#define check_msl_status(msg, size) if (msl_status == msl_retry_different_heap) \
-    { \
-        dprintf (5555, ("h%d RETRY %s(%Id)", heap_number, msg, size)); \
-        return a_state_retry_allocate; \
-    }
-static const int32_t lock_free = -1;
-static const int32_t lock_taken = 0;
-static const int32_t lock_decommissioned = 1;
-bool gc_heap::should_move_heap (GCSpinLock* msl)
-{
-#ifdef MULTIPLE_HEAPS
-    if (msl->lock == lock_decommissioned)
-    {
-        dprintf (5555, ("heap#%d got decommissioned! need to retry", heap_number));
-    }
-    return (msl->lock == lock_decommissioned);
-#else //MULTIPLE_HEAPS
-    return false;
-#endif //MULTIPLE_HEAPS
-}
-enter_msl_status gc_heap::enter_spin_lock_msl_helper (GCSpinLock* msl)
-{
-    do
-    {
-#ifdef DYNAMIC_HEAP_COUNT
-        uint64_t start = GetHighPrecisionTimeStamp();
-#endif //DYNAMIC_HEAP_COUNT
-        unsigned int i = 0;
-        while (VolatileLoad (&msl->lock) != lock_free)
-        {
-            if (should_move_heap (msl))
-            {
-                return msl_retry_different_heap;
-            }
-            if ((++i & 7) && !IsGCInProgress ())
-            {
-                if (g_num_processors > 1)
-                {
-#ifndef MULTIPLE_HEAPS
-                    int spin_count = 32 * yp_spin_count_unit;
-#else //!MULTIPLE_HEAPS
-                    int spin_count = yp_spin_count_unit;
-#endif //!MULTIPLE_HEAPS
-                    for (int j = 0; j < spin_count; j++)
-                    {
-                        if (VolatileLoad (&msl->lock) == lock_free || IsGCInProgress ())
-                            break;
-                        YieldProcessor ();           // indicate to the processor that we are spinning
-                    }
-                    if (VolatileLoad (&msl->lock) != lock_free && !IsGCInProgress ())
-                    {
-#ifdef DYNAMIC_HEAP_COUNT
-                        start -= GetHighPrecisionTimeStamp();
-#endif //DYNAMIC_HEAP_COUNT
-                        safe_switch_to_thread ();
-#ifdef DYNAMIC_HEAP_COUNT
-                        start += GetHighPrecisionTimeStamp();
-#endif //DYNAMIC_HEAP_COUNT
-                    }
-                }
-                else
-                {
-                    safe_switch_to_thread ();
-                }
-            }
-            else
-            {
-#ifdef DYNAMIC_HEAP_COUNT
-                start -= GetHighPrecisionTimeStamp();
-#endif //DYNAMIC_HEAP_COUNT
-                WaitLongerNoInstru (i);
-#ifdef DYNAMIC_HEAP_COUNT
-                start += GetHighPrecisionTimeStamp();
-#endif //DYNAMIC_HEAP_COUNT
-            }
-        }
-#ifdef DYNAMIC_HEAP_COUNT
-        uint64_t end = GetHighPrecisionTimeStamp();
-        Interlocked::ExchangeAdd64 (&msl->msl_wait_time, end - start);
-        dprintf (3, ("h%d wait for msl lock wait time %zd, total wait time: %zd", heap_number, (end - start), msl->msl_wait_time));
-#endif //DYNAMIC_HEAP_COUNT
-    }
-    while (Interlocked::CompareExchange (&msl->lock, lock_taken, lock_free) != lock_free);
-    return msl_entered;
-}
-inline
-enter_msl_status gc_heap::enter_spin_lock_msl (GCSpinLock* msl)
-{
-    if (Interlocked::CompareExchange (&msl->lock, lock_taken, lock_free) == lock_free)
-        return msl_entered;
-    return enter_spin_lock_msl_helper (msl);
-}
-inline
-static void enter_spin_lock_noinstru (RAW_KEYWORD(volatile) int32_t* lock)
-{
-retry:
-    if (Interlocked::CompareExchange(lock, lock_taken, lock_free) != lock_free)
-    {
-        unsigned int i = 0;
-        while (VolatileLoad(lock) != lock_free)
-        {
-            assert (VolatileLoad(lock) != lock_decommissioned);
-            if ((++i & 7) && !IsGCInProgress())
-            {
-                if  (g_num_processors > 1)
-                {
-#ifndef MULTIPLE_HEAPS
-                    int spin_count = 32 * yp_spin_count_unit;
-#else //!MULTIPLE_HEAPS
-                    int spin_count = yp_spin_count_unit;
-#endif //!MULTIPLE_HEAPS
-                    for (int j = 0; j < spin_count; j++)
-                    {
-                        if  (VolatileLoad(lock) == lock_free || IsGCInProgress())
-                            break;
-                        YieldProcessor();           // indicate to the processor that we are spinning
-                    }
-                    if  (VolatileLoad(lock) != lock_free && !IsGCInProgress())
-                    {
-                        safe_switch_to_thread();
-                    }
-                }
-                else
-                {
-                    safe_switch_to_thread();
-                }
-            }
-            else
-            {
-                WaitLongerNoInstru(i);
-            }
-        }
-        goto retry;
-    }
-}
-inline
-static BOOL try_enter_spin_lock_noinstru(RAW_KEYWORD(volatile) int32_t* lock)
-{
-    return (Interlocked::CompareExchange(&*lock, lock_taken, lock_free) == lock_free);
-}
-inline
-static void leave_spin_lock_noinstru (RAW_KEYWORD(volatile) int32_t* lock)
-{
-    VolatileStore<int32_t>((int32_t*)lock, lock_free);
-}
-#ifdef _DEBUG
-inline
-static void enter_spin_lock (GCSpinLock *pSpinLock)
-{
-    enter_spin_lock_noinstru (&pSpinLock->lock);
-    assert (pSpinLock->holding_thread == (Thread*)-1);
-    pSpinLock->holding_thread = GCToEEInterface::GetThread();
-}
-inline
-static BOOL try_enter_spin_lock(GCSpinLock *pSpinLock)
-{
-    BOOL ret = try_enter_spin_lock_noinstru(&pSpinLock->lock);
-    if (ret)
-        pSpinLock->holding_thread = GCToEEInterface::GetThread();
-    return ret;
-}
-inline
-static void leave_spin_lock(GCSpinLock *pSpinLock)
-{
-    bool gc_thread_p = GCToEEInterface::WasCurrentThreadCreatedByGC();
-    pSpinLock->released_by_gc_p = gc_thread_p;
-    pSpinLock->holding_thread = (Thread*) -1;
-    if (pSpinLock->lock != lock_free)
-        leave_spin_lock_noinstru(&pSpinLock->lock);
-}
-#define ASSERT_HOLDING_SPIN_LOCK(pSpinLock) \
-    _ASSERTE((pSpinLock)->holding_thread == GCToEEInterface::GetThread());
-#define ASSERT_NOT_HOLDING_SPIN_LOCK(pSpinLock) \
-    _ASSERTE((pSpinLock)->holding_thread != GCToEEInterface::GetThread());
-#else //_DEBUG
-void WaitLonger (int i
-#ifdef SYNCHRONIZATION_STATS
-    , GCSpinLock* spin_lock
-#endif //SYNCHRONIZATION_STATS
-    )
-{
-#ifdef SYNCHRONIZATION_STATS
-    (spin_lock->num_wait_longer)++;
-#endif //SYNCHRONIZATION_STATS
-    bool bToggleGC = GCToEEInterface::EnablePreemptiveGC();
-    assert (bToggleGC);
-    if (!gc_heap::gc_started)
-    {
-#ifdef SYNCHRONIZATION_STATS
-        (spin_lock->num_switch_thread_w)++;
-#endif //SYNCHRONIZATION_STATS
-        if  (g_num_processors > 1)
-        {
-            YieldProcessor();           // indicate to the processor that we are spinning
-            if  (i & 0x01f)
-                GCToOSInterface::YieldThread (0);
-            else
-                GCToOSInterface::Sleep (5);
-        }
-        else
-            GCToOSInterface::Sleep (5);
-    }
-    if (gc_heap::gc_started)
-    {
-        gc_heap::wait_for_gc_done();
-    }
-    if (bToggleGC)
-    {
-#ifdef SYNCHRONIZATION_STATS
-        (spin_lock->num_disable_preemptive_w)++;
-#endif //SYNCHRONIZATION_STATS
-        GCToEEInterface::DisablePreemptiveGC();
-    }
-}
-inline
-static void enter_spin_lock (GCSpinLock* spin_lock)
-{
-retry:
-    if (Interlocked::CompareExchange(&spin_lock->lock, lock_taken, lock_free) != lock_free)
-    {
-        unsigned int i = 0;
-        while (spin_lock->lock != lock_free)
-        {
-            assert (spin_lock->lock != lock_decommissioned);
-            if ((++i & 7) && !gc_heap::gc_started)
-            {
-                if  (g_num_processors > 1)
-                {
-#ifndef MULTIPLE_HEAPS
-                    int spin_count = 32 * yp_spin_count_unit;
-#else //!MULTIPLE_HEAPS
-                    int spin_count = yp_spin_count_unit;
-#endif //!MULTIPLE_HEAPS
-                    for (int j = 0; j < spin_count; j++)
-                    {
-                        if  (spin_lock->lock == lock_free || gc_heap::gc_started)
-                            break;
-                        YieldProcessor();           // indicate to the processor that we are spinning
-                    }
-                    if  (spin_lock->lock != lock_free && !gc_heap::gc_started)
-                    {
-#ifdef SYNCHRONIZATION_STATS
-                        (spin_lock->num_switch_thread)++;
-#endif //SYNCHRONIZATION_STATS
-                        bool cooperative_mode = gc_heap::enable_preemptive ();
-                        GCToOSInterface::YieldThread(0);
-                        gc_heap::disable_preemptive (cooperative_mode);
-                    }
-                }
-                else
-                    GCToOSInterface::YieldThread(0);
-            }
-            else
-            {
-                WaitLonger(i
-#ifdef SYNCHRONIZATION_STATS
-                        , spin_lock
-#endif //SYNCHRONIZATION_STATS
-                    );
-            }
-        }
-        goto retry;
-    }
-}
-inline
-static BOOL try_enter_spin_lock(GCSpinLock* spin_lock)
-{
-    return (Interlocked::CompareExchange(&spin_lock->lock, lock_taken, lock_free) == lock_free);
-}
-inline
-static void leave_spin_lock (GCSpinLock * spin_lock)
-{
-    spin_lock->lock = lock_free;
-}
-#define ASSERT_HOLDING_SPIN_LOCK(pSpinLock)
-#endif //_DEBUG
-bool gc_heap::enable_preemptive ()
-{
-    return GCToEEInterface::EnablePreemptiveGC();
-}
-void gc_heap::disable_preemptive (bool restore_cooperative)
-{
-    if (restore_cooperative)
-    {
-        GCToEEInterface::DisablePreemptiveGC();
-    }
-}
-typedef void **  PTR_PTR;
-inline
-void memclr ( uint8_t* mem, size_t size)
-{
-    dprintf (3, ("MEMCLR: %p, %zd", mem, size));
-    assert ((size & (sizeof(PTR_PTR)-1)) == 0);
-    assert (sizeof(PTR_PTR) == DATA_ALIGNMENT);
-    memset (mem, 0, size);
-}
-void memcopy (uint8_t* dmem, uint8_t* smem, size_t size)
-{
-    const size_t sz4ptr = sizeof(PTR_PTR)*4;
-    const size_t sz2ptr = sizeof(PTR_PTR)*2;
-    const size_t sz1ptr = sizeof(PTR_PTR)*1;
-    assert ((size & (sizeof (PTR_PTR)-1)) == 0);
-    assert (sizeof(PTR_PTR) == DATA_ALIGNMENT);
-    if (size >= sz4ptr)
-    {
-        do
-        {
-            ((PTR_PTR)dmem)[0] = ((PTR_PTR)smem)[0];
-            ((PTR_PTR)dmem)[1] = ((PTR_PTR)smem)[1];
-            ((PTR_PTR)dmem)[2] = ((PTR_PTR)smem)[2];
-            ((PTR_PTR)dmem)[3] = ((PTR_PTR)smem)[3];
-            dmem += sz4ptr;
-            smem += sz4ptr;
-        }
-        while ((size -= sz4ptr) >= sz4ptr);
-    }
-    if (size & sz2ptr)
-    {
-        ((PTR_PTR)dmem)[0] = ((PTR_PTR)smem)[0];
-        ((PTR_PTR)dmem)[1] = ((PTR_PTR)smem)[1];
-        dmem += sz2ptr;
-        smem += sz2ptr;
-    }
-    if (size & sz1ptr)
-    {
-        ((PTR_PTR)dmem)[0] = ((PTR_PTR)smem)[0];
-    }
-}
-inline
-ptrdiff_t round_down (ptrdiff_t add, int pitch)
-{
-    return ((add / pitch) * pitch);
-}
-#if defined(FEATURE_STRUCTALIGN) && defined(RESPECT_LARGE_ALIGNMENT)
-#error FEATURE_STRUCTALIGN should imply !RESPECT_LARGE_ALIGNMENT
-#endif
-#if defined(FEATURE_STRUCTALIGN) && defined(FEATURE_LOH_COMPACTION)
-#error FEATURE_STRUCTALIGN and FEATURE_LOH_COMPACTION are mutually exclusive
-#endif
-inline
-BOOL same_large_alignment_p (uint8_t* p1, uint8_t* p2)
-{
-#ifdef RESPECT_LARGE_ALIGNMENT
-    const size_t LARGE_ALIGNMENT_MASK = 2 * DATA_ALIGNMENT - 1;
-    return ((((size_t)p1 ^ (size_t)p2) & LARGE_ALIGNMENT_MASK) == 0);
-#else
-    UNREFERENCED_PARAMETER(p1);
-    UNREFERENCED_PARAMETER(p2);
-    return TRUE;
-#endif // RESPECT_LARGE_ALIGNMENT
-}
-inline
-size_t switch_alignment_size (BOOL already_padded_p)
-{
-#ifndef RESPECT_LARGE_ALIGNMENT
-    assert (!"Should not be called");
-#endif // RESPECT_LARGE_ALIGNMENT
-    if (already_padded_p)
-        return DATA_ALIGNMENT;
-    else
-        return Align (min_obj_size) | DATA_ALIGNMENT;
-}
-#ifdef FEATURE_STRUCTALIGN
-void set_node_aligninfo (uint8_t *node, int requiredAlignment, ptrdiff_t pad);
-void clear_node_aligninfo (uint8_t *node);
-#else // FEATURE_STRUCTALIGN
-#define node_realigned(node)    (((plug_and_reloc*)(node))[-1].reloc & 1)
-void set_node_realigned (uint8_t* node);
-void clear_node_realigned(uint8_t* node);
-#endif // FEATURE_STRUCTALIGN
-inline
-size_t AlignQword (size_t nbytes)
-{
-#ifdef FEATURE_STRUCTALIGN
-    return Align (nbytes);
-#else // FEATURE_STRUCTALIGN
-    return (nbytes + 7) & ~7;
-#endif // FEATURE_STRUCTALIGN
-}
-inline
-BOOL Aligned (size_t n)
-{
-    return (n & ALIGNCONST) == 0;
-}
-#define OBJECT_ALIGNMENT_OFFSET (sizeof(MethodTable *))
-#ifdef FEATURE_STRUCTALIGN
-#define MAX_STRUCTALIGN OS_PAGE_SIZE
-#else // FEATURE_STRUCTALIGN
-#define MAX_STRUCTALIGN 0
-#endif // FEATURE_STRUCTALIGN
-#ifdef FEATURE_STRUCTALIGN
-inline
-ptrdiff_t AdjustmentForMinPadSize(ptrdiff_t pad, int requiredAlignment)
-{
-    if ((size_t)(pad - DATA_ALIGNMENT) < Align (min_obj_size) - DATA_ALIGNMENT)
-    {
-        return requiredAlignment;
-    }
-    return 0;
-}
-inline
-uint8_t* StructAlign (uint8_t* origPtr, int requiredAlignment, ptrdiff_t alignmentOffset=OBJECT_ALIGNMENT_OFFSET)
-{
-    _ASSERTE(((size_t)origPtr & ALIGNCONST) == 0);
-    _ASSERTE(((requiredAlignment - 1) & requiredAlignment) == 0);
-    _ASSERTE(requiredAlignment >= sizeof(void *));
-    _ASSERTE(requiredAlignment <= MAX_STRUCTALIGN);
-    uint8_t* result = (uint8_t*)Align ((size_t)origPtr + alignmentOffset, requiredAlignment-1) - alignmentOffset;
-    ptrdiff_t alignpad = result - origPtr;
-    return result + AdjustmentForMinPadSize (alignpad, requiredAlignment);
-}
-inline
-ptrdiff_t ComputeStructAlignPad (uint8_t* plug, int requiredAlignment, size_t alignmentOffset=OBJECT_ALIGNMENT_OFFSET)
-{
-    return StructAlign (plug, requiredAlignment, alignmentOffset) - plug;
-}
-BOOL IsStructAligned (uint8_t *ptr, int requiredAlignment)
-{
-    return StructAlign (ptr, requiredAlignment) == ptr;
-}
-inline
-ptrdiff_t ComputeMaxStructAlignPad (int requiredAlignment)
-{
-    if (requiredAlignment == DATA_ALIGNMENT)
-        return 0;
-    return requiredAlignment + Align (min_obj_size) - DATA_ALIGNMENT;
-}
-inline
-ptrdiff_t ComputeMaxStructAlignPadLarge (int requiredAlignment)
-{
-    if (requiredAlignment <= get_alignment_constant (TRUE)+1)
-        return 0;
-    return requiredAlignment + Align (min_obj_size) * 2 - DATA_ALIGNMENT;
-}
-uint8_t* gc_heap::pad_for_alignment (uint8_t* newAlloc, int requiredAlignment, size_t size, alloc_context* acontext)
-{
-    uint8_t* alignedPtr = StructAlign (newAlloc, requiredAlignment);
-    if (alignedPtr != newAlloc) {
-        make_unused_array (newAlloc, alignedPtr - newAlloc);
-    }
-    acontext->alloc_ptr = alignedPtr + Align (size);
-    return alignedPtr;
-}
-uint8_t* gc_heap::pad_for_alignment_large (uint8_t* newAlloc, int requiredAlignment, size_t size)
-{
-    uint8_t* alignedPtr = StructAlign (newAlloc, requiredAlignment);
-    if (alignedPtr != newAlloc) {
-        make_unused_array (newAlloc, alignedPtr - newAlloc);
-    }
-    if (alignedPtr < newAlloc + ComputeMaxStructAlignPadLarge (requiredAlignment)) {
-        make_unused_array (alignedPtr + AlignQword (size), newAlloc + ComputeMaxStructAlignPadLarge (requiredAlignment) - alignedPtr);
-    }
-    return alignedPtr;
-}
-#else // FEATURE_STRUCTALIGN
-#define ComputeMaxStructAlignPad(requiredAlignment) 0
-#define ComputeMaxStructAlignPadLarge(requiredAlignment) 0
-#endif // FEATURE_STRUCTALIGN
-#ifdef SERVER_GC
-#define CLR_SIZE ((size_t)(8*1024+32))
-#else //SERVER_GC
-#define CLR_SIZE ((size_t)(8*1024+32))
-#endif //SERVER_GC
-#define END_SPACE_AFTER_GC (loh_size_threshold + MAX_STRUCTALIGN)
-#define END_SPACE_AFTER_GC_FL (END_SPACE_AFTER_GC + Align (min_obj_size))
-#if defined(BACKGROUND_GC) && !defined(USE_REGIONS)
-#define SEGMENT_INITIAL_COMMIT (2*OS_PAGE_SIZE)
-#else
-#define SEGMENT_INITIAL_COMMIT (OS_PAGE_SIZE)
-#endif //BACKGROUND_GC && !USE_REGIONS
-const size_t min_segment_size_hard_limit = 1024*1024*16;
-inline
-size_t align_on_segment_hard_limit (size_t add)
-{
-    return ((size_t)(add + (min_segment_size_hard_limit - 1)) & ~(min_segment_size_hard_limit - 1));
-}
-#ifdef SERVER_GC
-#ifdef HOST_64BIT
-#define INITIAL_ALLOC ((size_t)((size_t)4*1024*1024*1024))
-#define LHEAP_ALLOC   ((size_t)(1024*1024*256))
-#else
-#define INITIAL_ALLOC ((size_t)(1024*1024*64))
-#define LHEAP_ALLOC   ((size_t)(1024*1024*32))
-#endif  // HOST_64BIT
-#else //SERVER_GC
-#ifdef HOST_64BIT
-#define INITIAL_ALLOC ((size_t)(1024*1024*256))
-#define LHEAP_ALLOC   ((size_t)(1024*1024*128))
-#else
-#define INITIAL_ALLOC ((size_t)(1024*1024*16))
-#define LHEAP_ALLOC   ((size_t)(1024*1024*16))
-#endif  // HOST_64BIT
-#endif //SERVER_GC
-const size_t etw_allocation_tick = 100*1024;
-const size_t low_latency_alloc = 256*1024;
-const size_t fgn_check_quantum = 2*1024*1024;
-#ifdef MH_SC_MARK
-const int max_snoop_level = 128;
-#endif //MH_SC_MARK
-#ifdef CARD_BUNDLE
-#define SH_TH_CARD_BUNDLE  (40*1024*1024)
-#define MH_TH_CARD_BUNDLE  (180*1024*1024)
-#endif //CARD_BUNDLE
-#define MIN_DECOMMIT_SIZE  (100*OS_PAGE_SIZE)
-#define DECOMMIT_SIZE_PER_MILLISECOND (160*1024)
-#define DECOMMIT_TIME_STEP_MILLISECONDS (100)
-inline
-size_t align_on_page (size_t add)
-{
-    return ((add + OS_PAGE_SIZE - 1) & ~((size_t)OS_PAGE_SIZE - 1));
-}
-inline
-uint8_t* align_on_page (uint8_t* add)
-{
-    return (uint8_t*)align_on_page ((size_t) add);
-}
-inline
-size_t align_lower_page (size_t add)
-{
-    return (add & ~((size_t)OS_PAGE_SIZE - 1));
-}
-inline
-uint8_t* align_lower_page (uint8_t* add)
-{
-    return (uint8_t*)align_lower_page ((size_t)add);
-}
-inline
-size_t align_write_watch_lower_page (size_t add)
-{
-    return (add & ~(WRITE_WATCH_UNIT_SIZE - 1));
-}
-inline
-uint8_t* align_write_watch_lower_page (uint8_t* add)
-{
-    return (uint8_t*)align_lower_page ((size_t)add);
-}
-inline
-BOOL power_of_two_p (size_t integer)
-{
-    return !(integer & (integer-1));
-}
-inline
-BOOL oddp (size_t integer)
-{
-    return (integer & 1) != 0;
-}
-size_t logcount (size_t word)
-{
-    assert (word < 0x10000);
-    size_t count;
-    count = (word & 0x5555) + ( (word >> 1 ) & 0x5555);
-    count = (count & 0x3333) + ( (count >> 2) & 0x3333);
-    count = (count & 0x0F0F) + ( (count >> 4) & 0x0F0F);
-    count = (count & 0x00FF) + ( (count >> 8) & 0x00FF);
-    return count;
-}
-void stomp_write_barrier_resize(bool is_runtime_suspended, bool requires_upper_bounds_check)
-{
-    WriteBarrierParameters args = {};
-    args.operation = WriteBarrierOp::StompResize;
-    args.is_runtime_suspended = is_runtime_suspended;
-    args.requires_upper_bounds_check = requires_upper_bounds_check;
-    args.card_table = g_gc_card_table;
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-    args.card_bundle_table = g_gc_card_bundle_table;
-#endif
-    args.lowest_address = g_gc_lowest_address;
-    args.highest_address = g_gc_highest_address;
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    if (SoftwareWriteWatch::IsEnabledForGCHeap())
-    {
-        args.write_watch_table = g_gc_sw_ww_table;
-    }
-#endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    GCToEEInterface::StompWriteBarrier(&args);
-}
-#ifdef USE_REGIONS
-void region_write_barrier_settings (WriteBarrierParameters* args,
-                                    gc_heap::region_info* map_region_to_generation_skewed,
-                                    uint8_t region_shr)
-{
-    switch (GCConfig::GetGCWriteBarrier())
-    {
-    default:
-    case GCConfig::WRITE_BARRIER_DEFAULT:
-    case GCConfig::WRITE_BARRIER_REGION_BIT:
-        args->region_to_generation_table = (uint8_t*)map_region_to_generation_skewed;
-        args->region_shr = region_shr;
-        args->region_use_bitwise_write_barrier = true;
-        break;
-    case GCConfig::WRITE_BARRIER_REGION_BYTE:
-        args->region_to_generation_table = (uint8_t*)map_region_to_generation_skewed;
-        args->region_shr = region_shr;
-        assert (args->region_use_bitwise_write_barrier == false);
-        break;
-    case GCConfig::WRITE_BARRIER_SERVER:
-        assert (args->region_use_bitwise_write_barrier == false);
-        assert (args->region_to_generation_table == nullptr);
-        assert (args->region_shr == 0);
-        break;
-    }
-}
-#endif //USE_REGIONS
-void stomp_write_barrier_ephemeral (uint8_t* ephemeral_low, uint8_t* ephemeral_high
-#ifdef USE_REGIONS
-                                   , gc_heap::region_info* map_region_to_generation_skewed
-                                   , uint8_t region_shr
-#endif //USE_REGIONS
-                                   )
-{
-#ifndef USE_REGIONS
-    initGCShadow();
-#endif
-    WriteBarrierParameters args = {};
-    args.operation = WriteBarrierOp::StompEphemeral;
-    args.is_runtime_suspended = true;
-    args.ephemeral_low = ephemeral_low;
-    args.ephemeral_high = ephemeral_high;
-#ifdef USE_REGIONS
-    region_write_barrier_settings (&args, map_region_to_generation_skewed, region_shr);
-#endif //USE_REGIONS
-    GCToEEInterface::StompWriteBarrier(&args);
-}
-void stomp_write_barrier_initialize(uint8_t* ephemeral_low, uint8_t* ephemeral_high
-#ifdef USE_REGIONS
-                                   , gc_heap::region_info* map_region_to_generation_skewed
-                                   , uint8_t region_shr
-#endif //USE_REGIONS
-                                   )
-{
-    WriteBarrierParameters args = {};
-    args.operation = WriteBarrierOp::Initialize;
-    args.is_runtime_suspended = true;
-    args.requires_upper_bounds_check = false;
-    args.card_table = g_gc_card_table;
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-    args.card_bundle_table = g_gc_card_bundle_table;
-#endif
-    args.lowest_address = g_gc_lowest_address;
-    args.highest_address = g_gc_highest_address;
-    args.ephemeral_low = ephemeral_low;
-    args.ephemeral_high = ephemeral_high;
-#ifdef USE_REGIONS
-    region_write_barrier_settings (&args, map_region_to_generation_skewed, region_shr);
-#endif //USE_REGIONS
-    GCToEEInterface::StompWriteBarrier(&args);
-}
-#define lowbits(wrd, bits) ((wrd) & ((1 << (bits))-1))
-#define highbits(wrd, bits) ((wrd) & ~((1 << (bits))-1))
-static static_data static_data_table[latency_level_last - latency_level_first + 1][total_generation_count] =
-{
-    {
-        {0, 0, 40000, 0.5f, 9.0f, 20.0f, (1000 * 1000), 1},
-        {160*1024, 0, 80000, 0.5f, 2.0f, 7.0f, (10 * 1000 * 1000), 10},
-        {256*1024, SSIZE_T_MAX, 200000, 0.25f, 1.2f, 1.8f, (100 * 1000 * 1000), 100},
-        {3*1024*1024, SSIZE_T_MAX, 0, 0.0f, 1.25f, 4.5f, 0, 0},
-        {3*1024*1024, SSIZE_T_MAX, 0, 0.0f, 1.25f, 4.5f, 0, 0},
-    },
-    {
-        {0, 0, 40000, 0.5f,
-#ifdef MULTIPLE_HEAPS
-            20.0f, 40.0f,
-#else
-            9.0f, 20.0f,
-#endif //MULTIPLE_HEAPS
-            (1000 * 1000), 1},
-        {256*1024, 0, 80000, 0.5f, 2.0f, 7.0f, (10 * 1000 * 1000), 10},
-        {256*1024, SSIZE_T_MAX, 200000, 0.25f, 1.2f, 1.8f, (100 * 1000 * 1000), 100},
-        {3*1024*1024, SSIZE_T_MAX, 0, 0.0f, 1.25f, 4.5f, 0, 0},
-        {3*1024*1024, SSIZE_T_MAX, 0, 0.0f, 1.25f, 4.5f, 0, 0}
-    },
-};
-class mark;
-class generation;
-class heap_segment;
-class CObjectHeader;
-class dynamic_data;
-class l_heap;
-class sorted_table;
-class c_synchronize;
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-static
-HRESULT AllocateCFinalize(CFinalize **pCFinalize);
-#endif // FEATURE_PREMORTEM_FINALIZATION
-uint8_t* tree_search (uint8_t* tree, uint8_t* old_address);
-#ifdef USE_INTROSORT
-#define _sort introsort::sort
-#elif defined(USE_VXSORT)
-#else //USE_INTROSORT
-#define _sort qsort1
-void qsort1(uint8_t** low, uint8_t** high, unsigned int depth);
-#endif //USE_INTROSORT
-void* virtual_alloc (size_t size);
-void* virtual_alloc (size_t size, bool use_large_pages_p, uint16_t numa_node = NUMA_NODE_UNDEFINED);
-/* per heap static initialization */
-#if defined(BACKGROUND_GC) && !defined(MULTIPLE_HEAPS)
-uint32_t*   gc_heap::mark_array;
-#endif //BACKGROUND_GC && !MULTIPLE_HEAPS
-uint8_t**   gc_heap::g_mark_list;
-uint8_t**   gc_heap::g_mark_list_copy;
-size_t      gc_heap::mark_list_size;
-size_t      gc_heap::g_mark_list_total_size;
-bool        gc_heap::mark_list_overflow;
-#ifdef USE_REGIONS
-uint8_t***  gc_heap::g_mark_list_piece;
-size_t      gc_heap::g_mark_list_piece_size;
-size_t      gc_heap::g_mark_list_piece_total_size;
-#endif //USE_REGIONS
-seg_mapping* seg_mapping_table;
-#ifdef FEATURE_BASICFREEZE
-sorted_table* gc_heap::seg_table;
-#endif //FEATURE_BASICFREEZE
-#ifdef MULTIPLE_HEAPS
-GCEvent     gc_heap::ee_suspend_event;
-size_t      gc_heap::min_gen0_balance_delta = 0;
-size_t      gc_heap::min_balance_threshold = 0;
-#endif //MULTIPLE_HEAPS
-VOLATILE(BOOL) gc_heap::gc_started;
-#ifdef MULTIPLE_HEAPS
-GCEvent     gc_heap::gc_start_event;
-bool        gc_heap::gc_thread_no_affinitize_p = false;
-uintptr_t   process_mask = 0;
-int         gc_heap::n_heaps;       // current number of heaps
-int         gc_heap::n_max_heaps;   // maximum number of heaps
-gc_heap**   gc_heap::g_heaps;
-#if !defined(USE_REGIONS) || defined(_DEBUG)
-size_t*     gc_heap::g_promoted;
-#endif //!USE_REGIONS || _DEBUG
-#ifdef MH_SC_MARK
-int*        gc_heap::g_mark_stack_busy;
-#endif //MH_SC_MARK
-#ifdef BACKGROUND_GC
-size_t*     gc_heap::g_bpromoted;
-#endif //BACKGROUND_GC
-BOOL        gc_heap::gradual_decommit_in_progress_p = FALSE;
-size_t      gc_heap::max_decommit_step_size = 0;
-#else  //MULTIPLE_HEAPS
-#if !defined(USE_REGIONS) || defined(_DEBUG)
-size_t      gc_heap::g_promoted;
-#endif //!USE_REGIONS || _DEBUG
-#ifdef BACKGROUND_GC
-size_t      gc_heap::g_bpromoted;
-#endif //BACKGROUND_GC
-const int n_heaps = 1;
-#endif //MULTIPLE_HEAPS
-size_t      gc_heap::card_table_element_layout[total_bookkeeping_elements + 1];
-uint8_t*    gc_heap::bookkeeping_start = nullptr;
-#ifdef USE_REGIONS
-uint8_t*    gc_heap::bookkeeping_covered_committed = nullptr;
-size_t      gc_heap::bookkeeping_sizes[total_bookkeeping_elements];
-#endif //USE_REGIONS
-size_t      gc_heap::reserved_memory = 0;
-size_t      gc_heap::reserved_memory_limit = 0;
-BOOL        gc_heap::g_low_memory_status;
-static gc_reason gc_trigger_reason = reason_empty;
-gc_latency_level gc_heap::latency_level = latency_level_default;
-gc_mechanisms  gc_heap::settings;
-gc_history_global gc_heap::gc_data_global;
-uint64_t    gc_heap::gc_last_ephemeral_decommit_time = 0;
-CLRCriticalSection gc_heap::check_commit_cs;
-CLRCriticalSection gc_heap::decommit_lock;
-size_t      gc_heap::current_total_committed = 0;
-size_t      gc_heap::committed_by_oh[recorded_committed_bucket_counts];
-size_t      gc_heap::current_total_committed_bookkeeping = 0;
-BOOL        gc_heap::reset_mm_p = TRUE;
-#ifdef FEATURE_EVENT_TRACE
-bool gc_heap::informational_event_enabled_p = false;
-uint64_t*   gc_heap::gc_time_info = 0;
-#ifdef BACKGROUND_GC
-uint64_t*   gc_heap::bgc_time_info = 0;
-#endif //BACKGROUND_GC
-size_t      gc_heap::physical_memory_from_config = 0;
-size_t      gc_heap::gen0_min_budget_from_config = 0;
-size_t      gc_heap::gen0_max_budget_from_config = 0;
-int         gc_heap::high_mem_percent_from_config = 0;
-bool        gc_heap::use_frozen_segments_p = false;
-#ifdef FEATURE_LOH_COMPACTION
-gc_heap::etw_loh_compact_info* gc_heap::loh_compact_info;
-#endif //FEATURE_LOH_COMPACTION
-#endif //FEATURE_EVENT_TRACE
-bool        gc_heap::hard_limit_config_p = false;
-#if defined(SHORT_PLUGS) && !defined(USE_REGIONS)
-double      gc_heap::short_plugs_pad_ratio = 0;
-#endif //SHORT_PLUGS && !USE_REGIONS
-int         gc_heap::generation_skip_ratio_threshold = 0;
-int         gc_heap::conserve_mem_setting = 0;
-bool        gc_heap::spin_count_unit_config_p = false;
-uint64_t    gc_heap::suspended_start_time = 0;
-uint64_t    gc_heap::end_gc_time = 0;
-uint64_t    gc_heap::total_suspended_time = 0;
-uint64_t    gc_heap::process_start_time = 0;
-last_recorded_gc_info gc_heap::last_ephemeral_gc_info;
-last_recorded_gc_info gc_heap::last_full_blocking_gc_info;
-#ifdef BACKGROUND_GC
-last_recorded_gc_info gc_heap::last_bgc_info[2];
-VOLATILE(bool)        gc_heap::is_last_recorded_bgc = false;
-VOLATILE(int)         gc_heap::last_bgc_info_index = 0;
-#endif //BACKGROUND_GC
-#if defined(HOST_64BIT)
-#define MAX_ALLOWED_MEM_LOAD 85
-#define MIN_YOUNGEST_GEN_DESIRED (16*1024*1024)
-size_t      gc_heap::youngest_gen_desired_th;
-#endif //HOST_64BIT
-uint64_t    gc_heap::mem_one_percent = 0;
-uint32_t    gc_heap::high_memory_load_th = 0;
-uint32_t    gc_heap::m_high_memory_load_th;
-uint32_t    gc_heap::v_high_memory_load_th;
-bool        gc_heap::is_restricted_physical_mem;
-uint64_t    gc_heap::total_physical_mem = 0;
-uint64_t    gc_heap::entry_available_physical_mem = 0;
-size_t      gc_heap::heap_hard_limit = 0;
-size_t      gc_heap::heap_hard_limit_oh[total_oh_count];
-#ifdef USE_REGIONS
-size_t      gc_heap::regions_range = 0;
-#endif //USE_REGIONS
-bool        affinity_config_specified_p = false;
-#ifdef USE_REGIONS
-region_allocator global_region_allocator;
-uint8_t*(*initial_regions)[total_generation_count][2] = nullptr;
-size_t      gc_heap::region_count = 0;
-gc_heap::region_info* gc_heap::map_region_to_generation = nullptr;
-gc_heap::region_info* gc_heap::map_region_to_generation_skewed = nullptr;
-#endif //USE_REGIONS
-#ifdef BACKGROUND_GC
-GCEvent     gc_heap::bgc_start_event;
-gc_mechanisms gc_heap::saved_bgc_settings;
-gc_history_global gc_heap::bgc_data_global;
-GCEvent     gc_heap::background_gc_done_event;
-GCEvent     gc_heap::ee_proceed_event;
-bool        gc_heap::gc_can_use_concurrent = false;
-bool        gc_heap::temp_disable_concurrent_p = false;
-uint32_t    gc_heap::cm_in_progress = FALSE;
-BOOL        gc_heap::dont_restart_ee_p = FALSE;
-BOOL        gc_heap::keep_bgc_threads_p = FALSE;
-GCEvent     gc_heap::bgc_threads_sync_event;
-BOOL        gc_heap::do_ephemeral_gc_p = FALSE;
-BOOL        gc_heap::do_concurrent_p = FALSE;
-size_t      gc_heap::ephemeral_fgc_counts[max_generation];
-VOLATILE(c_gc_state) gc_heap::current_c_gc_state = c_gc_state_free;
-VOLATILE(BOOL) gc_heap::gc_background_running = FALSE;
-#endif //BACKGROUND_GC
-#ifdef USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-uint8_t*    gc_heap::gc_low;
-uint8_t*    gc_heap::gc_high;
-#endif //MULTIPLE_HEAPS
-VOLATILE(uint8_t*)    gc_heap::ephemeral_low;
-VOLATILE(uint8_t*)    gc_heap::ephemeral_high;
-#endif //USE_REGIONS
-#ifndef MULTIPLE_HEAPS
-#ifdef SPINLOCK_HISTORY
-int         gc_heap::spinlock_info_index = 0;
-spinlock_info gc_heap::last_spinlock_info[max_saved_spinlock_info];
-allocation_state gc_heap::current_uoh_alloc_state = (allocation_state)-1;
-#endif //SPINLOCK_HISTORY
-uint32_t    gc_heap::fgn_maxgen_percent = 0;
-size_t      gc_heap::fgn_last_alloc = 0;
-int         gc_heap::generation_skip_ratio = 100;
-#ifdef FEATURE_CARD_MARKING_STEALING
-VOLATILE(size_t) gc_heap::n_eph_soh = 0;
-VOLATILE(size_t) gc_heap::n_gen_soh = 0;
-VOLATILE(size_t) gc_heap::n_eph_loh = 0;
-VOLATILE(size_t) gc_heap::n_gen_loh = 0;
-#endif //FEATURE_CARD_MARKING_STEALING
-uint64_t    gc_heap::loh_alloc_since_cg = 0;
-BOOL        gc_heap::elevation_requested = FALSE;
-BOOL        gc_heap::last_gc_before_oom = FALSE;
-BOOL        gc_heap::sufficient_gen0_space_p = FALSE;
-#ifdef BACKGROUND_GC
-uint8_t*    gc_heap::background_saved_lowest_address = 0;
-uint8_t*    gc_heap::background_saved_highest_address = 0;
-uint8_t*    gc_heap::next_sweep_obj = 0;
-uint8_t*    gc_heap::current_sweep_pos = 0;
-#ifdef DOUBLY_LINKED_FL
-heap_segment* gc_heap::current_sweep_seg = 0;
-#endif //DOUBLY_LINKED_FL
-exclusive_sync* gc_heap::bgc_alloc_lock;
-#endif //BACKGROUND_GC
-oom_history gc_heap::oom_info;
-int         gc_heap::oomhist_index_per_heap = 0;
-oom_history gc_heap::oomhist_per_heap[max_oom_history_count];
-fgm_history gc_heap::fgm_result;
-size_t      gc_heap::allocated_since_last_gc[total_oh_count];
-BOOL        gc_heap::ro_segments_in_range;
-#ifndef USE_REGIONS
-uint8_t*    gc_heap::ephemeral_low;
-uint8_t*    gc_heap::ephemeral_high;
-BOOL        gc_heap::ephemeral_promotion;
-uint8_t*    gc_heap::saved_ephemeral_plan_start[ephemeral_generation_count];
-size_t      gc_heap::saved_ephemeral_plan_start_size[ephemeral_generation_count];
-#endif //!USE_REGIONS
-uint8_t*    gc_heap::lowest_address;
-uint8_t*    gc_heap::highest_address;
-short*      gc_heap::brick_table;
-uint32_t*   gc_heap::card_table;
-#ifdef CARD_BUNDLE
-uint32_t*   gc_heap::card_bundle_table;
-#endif //CARD_BUNDLE
-uint8_t*    gc_heap::gc_low = 0;
-uint8_t*    gc_heap::gc_high = 0;
-#ifndef USE_REGIONS
-uint8_t*    gc_heap::demotion_low;
-uint8_t*    gc_heap::demotion_high;
-BOOL        gc_heap::demote_gen1_p = TRUE;
-uint8_t*    gc_heap::last_gen1_pin_end;
-#endif //!USE_REGIONS
-gen_to_condemn_tuning gc_heap::gen_to_condemn_reasons;
-size_t      gc_heap::etw_allocation_running_amount[total_oh_count];
-uint64_t    gc_heap::total_alloc_bytes_soh = 0;
-uint64_t    gc_heap::total_alloc_bytes_uoh = 0;
-int         gc_heap::gc_policy = 0;
-uint64_t    gc_heap::allocation_running_time;
-size_t      gc_heap::allocation_running_amount;
-heap_segment* gc_heap::ephemeral_heap_segment = 0;
-#ifdef USE_REGIONS
-#ifdef STRESS_REGIONS
-OBJECTHANDLE* gc_heap::pinning_handles_for_alloc = 0;
-int         gc_heap::ph_index_per_heap = 0;
-int         gc_heap::pinning_seg_interval = 2;
-size_t      gc_heap::num_gen0_regions = 0;
-int         gc_heap::sip_seg_interval = 0;
-int         gc_heap::sip_seg_maxgen_interval = 0;
-size_t      gc_heap::num_condemned_regions = 0;
-#endif //STRESS_REGIONS
-region_free_list gc_heap::free_regions[count_free_region_kinds];
-int         gc_heap::num_regions_freed_in_sweep = 0;
-int         gc_heap::regions_per_gen[max_generation + 1];
-int         gc_heap::planned_regions_per_gen[max_generation + 1];
-int         gc_heap::sip_maxgen_regions_per_gen[max_generation + 1];
-heap_segment* gc_heap::reserved_free_regions_sip[max_generation];
-int         gc_heap::new_gen0_regions_in_plns = 0;
-int         gc_heap::new_regions_in_prr = 0;
-int         gc_heap::new_regions_in_threading = 0;
-size_t      gc_heap::end_gen0_region_space = 0;
-size_t      gc_heap::end_gen0_region_committed_space = 0;
-size_t      gc_heap::gen0_pinned_free_space = 0;
-bool        gc_heap::gen0_large_chunk_found = false;
-size_t*     gc_heap::survived_per_region = nullptr;
-size_t*     gc_heap::old_card_survived_per_region = nullptr;
-#endif //USE_REGIONS
-BOOL        gc_heap::blocking_collection = FALSE;
-heap_segment* gc_heap::freeable_uoh_segment = 0;
-uint64_t    gc_heap::time_bgc_last = 0;
-size_t      gc_heap::mark_stack_tos = 0;
-size_t      gc_heap::mark_stack_bos = 0;
-size_t      gc_heap::mark_stack_array_length = 0;
-mark*       gc_heap::mark_stack_array = 0;
-#if defined (_DEBUG) && defined (VERIFY_HEAP)
-BOOL        gc_heap::verify_pinned_queue_p = FALSE;
-#endif //_DEBUG && VERIFY_HEAP
-uint8_t*    gc_heap::oldest_pinned_plug = 0;
-size_t      gc_heap::num_pinned_objects = 0;
-#ifdef FEATURE_LOH_COMPACTION
-size_t      gc_heap::loh_pinned_queue_tos = 0;
-size_t      gc_heap::loh_pinned_queue_bos = 0;
-size_t      gc_heap::loh_pinned_queue_length = 0;
-mark*       gc_heap::loh_pinned_queue = 0;
-BOOL        gc_heap::loh_compacted_p = FALSE;
-#endif //FEATURE_LOH_COMPACTION
-#ifdef BACKGROUND_GC
-EEThreadId  gc_heap::bgc_thread_id;
-uint8_t*    gc_heap::background_written_addresses [array_size+2];
-heap_segment* gc_heap::freeable_soh_segment = 0;
-size_t      gc_heap::bgc_overflow_count = 0;
-size_t      gc_heap::bgc_begin_loh_size = 0;
-size_t      gc_heap::end_loh_size = 0;
-size_t      gc_heap::bgc_begin_poh_size = 0;
-size_t      gc_heap::end_poh_size = 0;
-#ifdef BGC_SERVO_TUNING
-uint64_t    gc_heap::loh_a_no_bgc = 0;
-uint64_t    gc_heap::loh_a_bgc_marking = 0;
-uint64_t    gc_heap::loh_a_bgc_planning = 0;
-size_t      gc_heap::bgc_maxgen_end_fl_size = 0;
-#endif //BGC_SERVO_TUNING
-size_t      gc_heap::bgc_loh_size_increased = 0;
-size_t      gc_heap::bgc_poh_size_increased = 0;
-size_t      gc_heap::background_soh_size_end_mark = 0;
-size_t      gc_heap::background_soh_alloc_count = 0;
-size_t      gc_heap::background_uoh_alloc_count = 0;
-uint8_t**   gc_heap::background_mark_stack_tos = 0;
-uint8_t**   gc_heap::background_mark_stack_array = 0;
-size_t      gc_heap::background_mark_stack_array_length = 0;
-BOOL        gc_heap::processed_eph_overflow_p = FALSE;
-#ifdef USE_REGIONS
-BOOL        gc_heap::background_overflow_p = FALSE;
-#else //USE_REGIONS
-uint8_t*    gc_heap::background_min_overflow_address =0;
-uint8_t*    gc_heap::background_max_overflow_address =0;
-uint8_t*    gc_heap::background_min_soh_overflow_address =0;
-uint8_t*    gc_heap::background_max_soh_overflow_address =0;
-heap_segment* gc_heap::saved_overflow_ephemeral_seg = 0;
-heap_segment* gc_heap::saved_sweep_ephemeral_seg = 0;
-uint8_t*    gc_heap::saved_sweep_ephemeral_start = 0;
-#endif //USE_REGIONS
-Thread*     gc_heap::bgc_thread = 0;
-uint8_t**   gc_heap::c_mark_list = 0;
-size_t      gc_heap::c_mark_list_length = 0;
-size_t      gc_heap::c_mark_list_index = 0;
-gc_history_per_heap gc_heap::bgc_data_per_heap;
-BOOL    gc_heap::bgc_thread_running;
-CLRCriticalSection gc_heap::bgc_threads_timeout_cs;
-#endif //BACKGROUND_GC
-uint8_t**   gc_heap::mark_list;
-uint8_t**   gc_heap::mark_list_index;
-uint8_t**   gc_heap::mark_list_end;
-#ifdef SNOOP_STATS
-snoop_stats_data gc_heap::snoop_stat;
-#endif //SNOOP_STATS
-uint8_t*    gc_heap::min_overflow_address = MAX_PTR;
-uint8_t*    gc_heap::max_overflow_address = 0;
-uint8_t*    gc_heap::shigh = 0;
-uint8_t*    gc_heap::slow = MAX_PTR;
-#ifndef USE_REGIONS
-size_t      gc_heap::ordered_free_space_indices[MAX_NUM_BUCKETS];
-size_t      gc_heap::saved_ordered_free_space_indices[MAX_NUM_BUCKETS];
-size_t      gc_heap::ordered_plug_indices[MAX_NUM_BUCKETS];
-size_t      gc_heap::saved_ordered_plug_indices[MAX_NUM_BUCKETS];
-BOOL        gc_heap::ordered_plug_indices_init = FALSE;
-BOOL        gc_heap::use_bestfit = FALSE;
-uint8_t*    gc_heap::bestfit_first_pin = 0;
-BOOL        gc_heap::commit_end_of_seg = FALSE;
-size_t      gc_heap::max_free_space_items = 0;
-size_t      gc_heap::free_space_buckets = 0;
-size_t      gc_heap::free_space_items = 0;
-int         gc_heap::trimmed_free_space_index = 0;
-size_t      gc_heap::total_ephemeral_plugs = 0;
-seg_free_spaces* gc_heap::bestfit_seg = 0;
-size_t      gc_heap::total_ephemeral_size = 0;
-#endif //!USE_REGIONS
-#ifdef HEAP_ANALYZE
-size_t      gc_heap::internal_root_array_length = initial_internal_roots;
-uint8_t**   gc_heap::internal_root_array = 0;
-size_t      gc_heap::internal_root_array_index = 0;
-BOOL        gc_heap::heap_analyze_success = TRUE;
-uint8_t*    gc_heap::current_obj = 0;
-size_t      gc_heap::current_obj_size = 0;
-#endif //HEAP_ANALYZE
-#ifdef GC_CONFIG_DRIVEN
-size_t gc_heap::interesting_data_per_gc[max_idp_count];
-#endif //GC_CONFIG_DRIVEN
-#endif //MULTIPLE_HEAPS
-no_gc_region_info gc_heap::current_no_gc_region_info;
-FinalizerWorkItem* gc_heap::finalizer_work;
-BOOL gc_heap::proceed_with_gc_p = FALSE;
-GCSpinLock gc_heap::gc_lock;
-#ifdef BGC_SERVO_TUNING
-uint64_t gc_heap::total_loh_a_last_bgc = 0;
-#endif //BGC_SERVO_TUNING
-#ifdef USE_REGIONS
-region_free_list gc_heap::global_regions_to_decommit[count_free_region_kinds];
-region_free_list gc_heap::global_free_huge_regions;
-#else //USE_REGIONS
-size_t gc_heap::eph_gen_starts_size = 0;
-heap_segment* gc_heap::segment_standby_list;
-#endif //USE_REGIONS
-bool          gc_heap::use_large_pages_p = 0;
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-size_t        gc_heap::last_gc_end_time_us = 0;
-#endif //HEAP_BALANCE_INSTRUMENTATION
-#ifdef USE_REGIONS
-bool          gc_heap::enable_special_regions_p = false;
-#else //USE_REGIONS
-size_t        gc_heap::min_segment_size = 0;
-size_t        gc_heap::min_uoh_segment_size = 0;
-#endif //!USE_REGIONS
-size_t        gc_heap::min_segment_size_shr = 0;
-size_t        gc_heap::soh_segment_size = 0;
-size_t        gc_heap::segment_info_size = 0;
-#ifdef GC_CONFIG_DRIVEN
-size_t gc_heap::compact_or_sweep_gcs[2];
-#endif //GC_CONFIG_DRIVEN
-#ifdef FEATURE_LOH_COMPACTION
-BOOL                   gc_heap::loh_compaction_always_p = FALSE;
-gc_loh_compaction_mode gc_heap::loh_compaction_mode = loh_compaction_default;
-#endif //FEATURE_LOH_COMPACTION
-GCEvent gc_heap::full_gc_approach_event;
-GCEvent gc_heap::full_gc_end_event;
-uint32_t gc_heap::fgn_loh_percent = 0;
-#ifdef BACKGROUND_GC
-BOOL gc_heap::fgn_last_gc_was_concurrent = FALSE;
-#endif //BACKGROUND_GC
-VOLATILE(bool) gc_heap::full_gc_approach_event_set;
-size_t gc_heap::full_gc_counts[gc_type_max];
-bool gc_heap::maxgen_size_inc_p = false;
-#ifndef USE_REGIONS
-BOOL gc_heap::should_expand_in_full_gc = FALSE;
-#endif //!USE_REGIONS
-#ifdef DYNAMIC_HEAP_COUNT
-int gc_heap::dynamic_adaptation_mode = dynamic_adaptation_default;
-gc_heap::dynamic_heap_count_data_t SVR::gc_heap::dynamic_heap_count_data;
-uint64_t gc_heap::last_suspended_end_time = 0;
-size_t gc_heap::gc_index_full_gc_end = 0;
-#ifdef STRESS_DYNAMIC_HEAP_COUNT
-int gc_heap::heaps_in_this_gc = 0;
-#endif //STRESS_DYNAMIC_HEAP_COUNT
-#endif // DYNAMIC_HEAP_COUNT
-bool gc_heap::provisional_mode_triggered = false;
-bool gc_heap::pm_trigger_full_gc = false;
-size_t gc_heap::provisional_triggered_gc_count = 0;
-size_t gc_heap::provisional_off_gc_count = 0;
-size_t gc_heap::num_provisional_triggered = 0;
-bool   gc_heap::pm_stress_on = false;
-#ifdef HEAP_ANALYZE
-BOOL        gc_heap::heap_analyze_enabled = FALSE;
-#endif //HEAP_ANALYZE
-#ifndef MULTIPLE_HEAPS
-alloc_list gc_heap::loh_alloc_list [NUM_LOH_ALIST-1];
-alloc_list gc_heap::gen2_alloc_list[NUM_GEN2_ALIST-1];
-alloc_list gc_heap::poh_alloc_list [NUM_POH_ALIST-1];
-#ifdef DOUBLY_LINKED_FL
-size_t gc_heap::gen2_removed_no_undo = 0;
-size_t gc_heap::saved_pinned_plug_index = INVALID_SAVED_PINNED_PLUG_INDEX;
-#endif //DOUBLY_LINKED_FL
-#ifdef FEATURE_EVENT_TRACE
-etw_bucket_info gc_heap::bucket_info[NUM_GEN2_ALIST];
-#endif //FEATURE_EVENT_TRACE
-dynamic_data gc_heap::dynamic_data_table [total_generation_count];
-gc_history_per_heap gc_heap::gc_data_per_heap;
-size_t gc_heap::total_promoted_bytes = 0;
-size_t gc_heap::finalization_promoted_bytes = 0;
-size_t gc_heap::maxgen_pinned_compact_before_advance = 0;
-uint8_t* gc_heap::alloc_allocated = 0;
-size_t gc_heap::allocation_quantum = CLR_SIZE;
-GCSpinLock gc_heap::more_space_lock_soh;
-GCSpinLock gc_heap::more_space_lock_uoh;
-#ifdef BACKGROUND_GC
-VOLATILE(int32_t) gc_heap::uoh_alloc_thread_count = 0;
-#endif //BACKGROUND_GC
-#ifdef SYNCHRONIZATION_STATS
-unsigned int gc_heap::good_suspension = 0;
-unsigned int gc_heap::bad_suspension = 0;
-uint64_t     gc_heap::total_msl_acquire = 0;
-unsigned int gc_heap::num_msl_acquired = 0;
-unsigned int gc_heap::num_high_msl_acquire = 0;
-unsigned int gc_heap::num_low_msl_acquire = 0;
-#endif //SYNCHRONIZATION_STATS
-size_t   gc_heap::alloc_contexts_used = 0;
-size_t   gc_heap::soh_allocation_no_gc = 0;
-size_t   gc_heap::loh_allocation_no_gc = 0;
-bool     gc_heap::no_gc_oom_p = false;
-heap_segment* gc_heap::saved_loh_segment_no_gc = 0;
-#endif //MULTIPLE_HEAPS
-#ifndef MULTIPLE_HEAPS
-BOOL        gc_heap::gen0_bricks_cleared = FALSE;
-int         gc_heap::gen0_must_clear_bricks = 0;
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-CFinalize*  gc_heap::finalize_queue = 0;
-#endif // FEATURE_PREMORTEM_FINALIZATION
-#ifdef FEATURE_CARD_MARKING_STEALING
-VOLATILE(uint32_t) gc_heap::card_mark_chunk_index_soh;
-VOLATILE(bool) gc_heap::card_mark_done_soh;
-VOLATILE(uint32_t) gc_heap::card_mark_chunk_index_loh;
-VOLATILE(uint32_t) gc_heap::card_mark_chunk_index_poh;
-VOLATILE(bool) gc_heap::card_mark_done_uoh;
-#endif // FEATURE_CARD_MARKING_STEALING
-generation gc_heap::generation_table [total_generation_count];
-size_t     gc_heap::interesting_data_per_heap[max_idp_count];
-size_t     gc_heap::compact_reasons_per_heap[max_compact_reasons_count];
-size_t     gc_heap::expand_mechanisms_per_heap[max_expand_mechanisms_count];
-size_t     gc_heap::interesting_mechanism_bits_per_heap[max_gc_mechanism_bits_count];
-mark_queue_t gc_heap::mark_queue;
-#ifdef USE_REGIONS
-bool gc_heap::special_sweep_p = false;
-#endif //USE_REGIONS
-int gc_heap::loh_pinned_queue_decay = LOH_PIN_DECAY;
-#endif // MULTIPLE_HEAPS
-/* end of per heap static initialization */
-#ifdef USE_REGIONS
-const size_t uninitialized_end_gen0_region_space = (size_t)(-1);
-#endif //USE_REGIONS
-size_t     gc_heap::smoothed_desired_total[total_generation_count];
-/* end of static initialization */
-inline
-int get_start_generation_index()
-{
-#ifdef USE_REGIONS
-    return 0;
-#else
-    return max_generation;
-#endif //USE_REGIONS
-}
-inline
-int get_stop_generation_index (int condemned_gen_number)
-{
-#ifdef USE_REGIONS
-    return 0;
-#else
-    return condemned_gen_number;
-#endif //USE_REGIONS
-}
-void gen_to_condemn_tuning::print (int heap_num)
-{
-#ifdef DT_LOG
-    dprintf (DT_LOG_0, ("condemned reasons (%d %d)", condemn_reasons_gen, condemn_reasons_condition));
-    dprintf (DT_LOG_0, ("%s", record_condemn_reasons_gen_header));
-    gc_condemn_reason_gen r_gen;
-    for (int i = 0; i < gcrg_max; i++)
-    {
-        r_gen = (gc_condemn_reason_gen)(i);
-        str_reasons_gen[i * 2] = get_gen_char (get_gen (r_gen));
-    }
-    dprintf (DT_LOG_0, ("[%2d]%s", heap_num, str_reasons_gen));
-    dprintf (DT_LOG_0, ("%s", record_condemn_reasons_condition_header));
-    gc_condemn_reason_condition r_condition;
-    for (int i = 0; i < gcrc_max; i++)
-    {
-        r_condition = (gc_condemn_reason_condition)(i);
-        str_reasons_condition[i * 2] = get_condition_char (get_condition (r_condition));
-    }
-    dprintf (DT_LOG_0, ("[%2d]%s", heap_num, str_reasons_condition));
-#else
-    UNREFERENCED_PARAMETER(heap_num);
-#endif //DT_LOG
-}
-void gc_generation_data::print (int heap_num, int gen_num)
-{
-#if defined(SIMPLE_DPRINTF) && defined(DT_LOG)
-    dprintf (DT_LOG_0, ("[%2d]gen%d beg %zd fl %zd fo %zd end %zd fl %zd fo %zd in %zd p %zd np %zd alloc %zd",
-                heap_num, gen_num,
-                size_before,
-                free_list_space_before, free_obj_space_before,
-                size_after,
-                free_list_space_after, free_obj_space_after,
-                in, pinned_surv, npinned_surv,
-                new_allocation));
-#else
-    UNREFERENCED_PARAMETER(heap_num);
-    UNREFERENCED_PARAMETER(gen_num);
-#endif //SIMPLE_DPRINTF && DT_LOG
-}
-void gc_history_per_heap::set_mechanism (gc_mechanism_per_heap mechanism_per_heap, uint32_t value)
-{
-    uint32_t* mechanism = &mechanisms[mechanism_per_heap];
-    *mechanism = 0;
-    *mechanism |= mechanism_mask;
-    *mechanism |= (1 << value);
-#ifdef DT_LOG
-    gc_mechanism_descr* descr = &gc_mechanisms_descr[mechanism_per_heap];
-    dprintf (DT_LOG_0, ("setting %s: %s",
-            descr->name,
-            (descr->descr)[value]));
-#endif //DT_LOG
-}
-void gc_history_per_heap::print()
-{
-#if defined(SIMPLE_DPRINTF) && defined(DT_LOG)
-    for (int i = 0; i < (sizeof (gen_data)/sizeof (gc_generation_data)); i++)
-    {
-        gen_data[i].print (heap_index, i);
-    }
-    dprintf (DT_LOG_0, ("fla %zd flr %zd esa %zd ca %zd pa %zd paa %zd, rfle %d, ec %zd",
-                    maxgen_size_info.free_list_allocated,
-                    maxgen_size_info.free_list_rejected,
-                    maxgen_size_info.end_seg_allocated,
-                    maxgen_size_info.condemned_allocated,
-                    maxgen_size_info.pinned_allocated,
-                    maxgen_size_info.pinned_allocated_advance,
-                    maxgen_size_info.running_free_list_efficiency,
-                    extra_gen0_committed));
-    int mechanism = 0;
-    gc_mechanism_descr* descr = 0;
-    for (int i = 0; i < max_mechanism_per_heap; i++)
-    {
-        mechanism = get_mechanism ((gc_mechanism_per_heap)i);
-        if (mechanism >= 0)
-        {
-            descr = &gc_mechanisms_descr[(gc_mechanism_per_heap)i];
-            dprintf (DT_LOG_0, ("[%2d]%s%s",
-                        heap_index,
-                        descr->name,
-                        (descr->descr)[mechanism]));
-        }
-    }
-#endif //SIMPLE_DPRINTF && DT_LOG
-}
-void gc_history_global::print()
-{
-#ifdef DT_LOG
-    char str_settings[64];
-    memset (str_settings, '|', sizeof (char) * 64);
-    str_settings[max_global_mechanisms_count*2] = 0;
-    for (int i = 0; i < max_global_mechanisms_count; i++)
-    {
-        str_settings[i * 2] = (get_mechanism_p ((gc_global_mechanism_p)i) ? 'Y' : 'N');
-    }
-    dprintf (DT_LOG_0, ("[hp]|c|p|o|d|b|e|"));
-    dprintf (DT_LOG_0, ("%4d|%s", num_heaps, str_settings));
-    dprintf (DT_LOG_0, ("Condemned gen%d(reason: %s; mode: %s), youngest budget %zd(%d), memload %d",
-                        condemned_generation,
-                        str_gc_reasons[reason],
-                        str_gc_pause_modes[pause_mode],
-                        final_youngest_desired,
-                        gen0_reduction_count,
-                        mem_pressure));
-#endif //DT_LOG
-}
-uint32_t limit_time_to_uint32 (uint64_t time)
-{
-    time = min (time, UINT32_MAX);
-    return (uint32_t)time;
-}
-void gc_heap::fire_per_heap_hist_event (gc_history_per_heap* current_gc_data_per_heap, int heap_num)
-{
-    maxgen_size_increase* maxgen_size_info = &(current_gc_data_per_heap->maxgen_size_info);
-    FIRE_EVENT(GCPerHeapHistory_V3,
-               (void *)(maxgen_size_info->free_list_allocated),
-               (void *)(maxgen_size_info->free_list_rejected),
-               (void *)(maxgen_size_info->end_seg_allocated),
-               (void *)(maxgen_size_info->condemned_allocated),
-               (void *)(maxgen_size_info->pinned_allocated),
-               (void *)(maxgen_size_info->pinned_allocated_advance),
-               maxgen_size_info->running_free_list_efficiency,
-               current_gc_data_per_heap->gen_to_condemn_reasons.get_reasons0(),
-               current_gc_data_per_heap->gen_to_condemn_reasons.get_reasons1(),
-               current_gc_data_per_heap->mechanisms[gc_heap_compact],
-               current_gc_data_per_heap->mechanisms[gc_heap_expand],
-               current_gc_data_per_heap->heap_index,
-               (void *)(current_gc_data_per_heap->extra_gen0_committed),
-               total_generation_count,
-               (uint32_t)(sizeof (gc_generation_data)),
-               (void *)&(current_gc_data_per_heap->gen_data[0]));
-    current_gc_data_per_heap->print();
-    current_gc_data_per_heap->gen_to_condemn_reasons.print (heap_num);
-}
-void gc_heap::fire_pevents()
-{
-    gc_history_global* current_gc_data_global = get_gc_data_global();
-    settings.record (current_gc_data_global);
-    current_gc_data_global->print();
-#ifdef FEATURE_EVENT_TRACE
-    if (!informational_event_enabled_p) return;
-    uint32_t count_time_info = (settings.concurrent ? max_bgc_time_type :
-                                (settings.compaction ? max_compact_time_type : max_sweep_time_type));
-#ifdef BACKGROUND_GC
-    uint64_t* time_info = (settings.concurrent ? bgc_time_info : gc_time_info);
-#else
-    uint64_t* time_info = gc_time_info;
-#endif //BACKGROUND_GC
-    uint32_t* time_info_32 = (uint32_t*)time_info;
-    for (uint32_t i = 0; i < count_time_info; i++)
-    {
-        time_info_32[i] = limit_time_to_uint32 (time_info[i]);
-    }
-    FIRE_EVENT(GCGlobalHeapHistory_V4,
-               current_gc_data_global->final_youngest_desired,
-               current_gc_data_global->num_heaps,
-               current_gc_data_global->condemned_generation,
-               current_gc_data_global->gen0_reduction_count,
-               current_gc_data_global->reason,
-               current_gc_data_global->global_mechanisms_p,
-               current_gc_data_global->pause_mode,
-               current_gc_data_global->mem_pressure,
-               current_gc_data_global->gen_to_condemn_reasons.get_reasons0(),
-               current_gc_data_global->gen_to_condemn_reasons.get_reasons1(),
-               count_time_info,
-               (uint32_t)(sizeof (uint32_t)),
-               (void*)time_info_32);
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-        gc_history_per_heap* current_gc_data_per_heap = hp->get_gc_data_per_heap();
-        fire_per_heap_hist_event (current_gc_data_per_heap, hp->heap_number);
-    }
-#else
-    gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
-    fire_per_heap_hist_event (current_gc_data_per_heap, heap_number);
-#endif //MULTIPLE_HEAPS
-#ifdef FEATURE_LOH_COMPACTION
-    if (!settings.concurrent && settings.loh_compaction)
-    {
-        FIRE_EVENT(GCLOHCompact,
-                   (uint16_t)get_num_heaps(),
-                   (uint32_t)(sizeof (etw_loh_compact_info)),
-                   (void *)loh_compact_info);
-    }
-#endif //FEATURE_LOH_COMPACTION
-#endif //FEATURE_EVENT_TRACE
-}
-void gc_heap::fire_committed_usage_event()
-{
-#if defined(FEATURE_EVENT_TRACE) && defined(USE_REGIONS)
-    if (!EVENT_ENABLED (GCMarkWithType)) return;
-    size_t total_committed = 0;
-    size_t committed_decommit = 0;
-    size_t committed_free = 0;
-    size_t committed_bookkeeping = 0;
-    size_t new_current_total_committed;
-    size_t new_current_total_committed_bookkeeping;
-    size_t new_committed_by_oh[recorded_committed_bucket_counts];
-    compute_committed_bytes(total_committed, committed_decommit, committed_free,
-                            committed_bookkeeping, new_current_total_committed, new_current_total_committed_bookkeeping,
-                            new_committed_by_oh);
-    size_t total_committed_in_use = new_committed_by_oh[soh] + new_committed_by_oh[loh] + new_committed_by_oh[poh];
-    size_t total_committed_in_global_decommit = committed_decommit;
-    size_t total_committed_in_free = committed_free;
-    size_t total_committed_in_global_free = new_committed_by_oh[recorded_committed_free_bucket] - total_committed_in_free - total_committed_in_global_decommit;
-    size_t total_bookkeeping_committed = committed_bookkeeping;
-    GCEventFireCommittedUsage_V1 (
-        (uint64_t)total_committed_in_use,
-        (uint64_t)total_committed_in_global_decommit,
-        (uint64_t)total_committed_in_free,
-        (uint64_t)total_committed_in_global_free,
-        (uint64_t)total_bookkeeping_committed
-    );
-#endif //FEATURE_EVENT_TRACE && USE_REGIONS
-}
-inline BOOL
-gc_heap::dt_low_ephemeral_space_p (gc_tuning_point tp)
-{
-    BOOL ret = FALSE;
-    switch (tp)
-    {
-        case tuning_deciding_condemned_gen:
-#ifndef USE_REGIONS
-        case tuning_deciding_compaction:
-        case tuning_deciding_expansion:
-#endif //USE_REGIONS
-        case tuning_deciding_full_gc:
-        {
-            ret = (!ephemeral_gen_fit_p (tp));
-            break;
-        }
-#ifndef USE_REGIONS
-        case tuning_deciding_promote_ephemeral:
-        {
-            size_t new_gen0size = approximate_new_allocation();
-            ptrdiff_t plan_ephemeral_size = total_ephemeral_size;
-            dprintf (GTC_LOG, ("h%d: plan eph size is %zd, new gen0 is %zd",
-                heap_number, plan_ephemeral_size, new_gen0size));
-            ret = ((soh_segment_size - segment_info_size) < (plan_ephemeral_size + new_gen0size));
-            break;
-        }
-#endif //USE_REGIONS
-        default:
-        {
-            assert (!"invalid tuning reason");
-            break;
-        }
-    }
-    return ret;
-}
-BOOL
-gc_heap::dt_high_frag_p (gc_tuning_point tp,
-                         int gen_number,
-                         BOOL elevate_p)
-{
-    BOOL ret = FALSE;
-    switch (tp)
-    {
-        case tuning_deciding_condemned_gen:
-        {
-            dynamic_data* dd = dynamic_data_of (gen_number);
-            float fragmentation_burden = 0;
-            if (elevate_p)
-            {
-                ret = (dd_fragmentation (dynamic_data_of (max_generation)) >= dd_max_size(dd));
-                dprintf (GTC_LOG, ("h%d: frag is %zd, max size is %zd",
-                    heap_number, dd_fragmentation (dd), dd_max_size(dd)));
-            }
-            else
-            {
-#ifndef MULTIPLE_HEAPS
-                if (gen_number == max_generation)
-                {
-                    size_t maxgen_size = generation_size (max_generation);
-                    float frag_ratio = (maxgen_size ? ((float)dd_fragmentation (dynamic_data_of (max_generation)) / (float)maxgen_size) : 0.0f);
-                    if (frag_ratio > 0.65)
-                    {
-                        dprintf (GTC_LOG, ("g2 FR: %d%%", (int)(frag_ratio*100)));
-                        return TRUE;
-                    }
-                }
-#endif //!MULTIPLE_HEAPS
-                size_t fr = generation_unusable_fragmentation (generation_of (gen_number));
-                ret = (fr > dd_fragmentation_limit(dd));
-                if (ret)
-                {
-                    size_t gen_size = generation_size (gen_number);
-                    fragmentation_burden = (gen_size ? ((float)fr / (float)gen_size) : 0.0f);
-                    ret = (fragmentation_burden > dd_v_fragmentation_burden_limit (dd));
-                }
-                dprintf (GTC_LOG, ("h%d: gen%d, frag is %zd, alloc effi: %d%%, unusable frag is %zd, ratio is %d",
-                    heap_number, gen_number, dd_fragmentation (dd),
-                    (int)(100*generation_allocator_efficiency (generation_of (gen_number))),
-                    fr, (int)(fragmentation_burden*100)));
-            }
-            break;
-        }
-        default:
-            break;
-    }
-    return ret;
-}
-inline BOOL
-gc_heap::dt_estimate_reclaim_space_p (gc_tuning_point tp, int gen_number)
-{
-    BOOL ret = FALSE;
-    switch (tp)
-    {
-        case tuning_deciding_condemned_gen:
-        {
-            if (gen_number == max_generation)
-            {
-                size_t est_maxgen_free = estimated_reclaim (gen_number);
-                uint32_t num_heaps = 1;
-#ifdef MULTIPLE_HEAPS
-                num_heaps = gc_heap::n_heaps;
-#endif //MULTIPLE_HEAPS
-                size_t min_frag_th = min_reclaim_fragmentation_threshold (num_heaps);
-                dprintf (GTC_LOG, ("h%d, min frag is %zd", heap_number, min_frag_th));
-                ret = (est_maxgen_free >= min_frag_th);
-            }
-            else
-            {
-                assert (0);
-            }
-            break;
-        }
-        default:
-            break;
-    }
-    return ret;
-}
-inline BOOL
-gc_heap::dt_estimate_high_frag_p (gc_tuning_point tp, int gen_number, uint64_t available_mem)
-{
-    BOOL ret = FALSE;
-    switch (tp)
-    {
-        case tuning_deciding_condemned_gen:
-        {
-            if (gen_number == max_generation)
-            {
-                dynamic_data* dd = dynamic_data_of (gen_number);
-                float est_frag_ratio = 0;
-                if (dd_current_size (dd) == 0)
-                {
-                    est_frag_ratio = 1;
-                }
-                else if ((dd_fragmentation (dd) == 0) || (dd_fragmentation (dd) + dd_current_size (dd) == 0))
-                {
-                    est_frag_ratio = 0;
-                }
-                else
-                {
-                    est_frag_ratio = (float)dd_fragmentation (dd) / (float)(dd_fragmentation (dd) + dd_current_size (dd));
-                }
-                size_t est_frag = (dd_fragmentation (dd) + (size_t)((dd_desired_allocation (dd) - dd_new_allocation (dd)) * est_frag_ratio));
-                dprintf (GTC_LOG, ("h%d: gen%d: current_size is %zd, frag is %zd, est_frag_ratio is %d%%, estimated frag is %zd",
-                    heap_number,
-                    gen_number,
-                    dd_current_size (dd),
-                    dd_fragmentation (dd),
-                    (int)(est_frag_ratio*100),
-                    est_frag));
-                uint32_t num_heaps = 1;
-#ifdef MULTIPLE_HEAPS
-                num_heaps = gc_heap::n_heaps;
-#endif //MULTIPLE_HEAPS
-                uint64_t min_frag_th = min_high_fragmentation_threshold(available_mem, num_heaps);
-                ret = (est_frag >= min_frag_th);
-            }
-            else
-            {
-                assert (0);
-            }
-            break;
-        }
-        default:
-            break;
-    }
-    return ret;
-}
-inline BOOL
-gc_heap::dt_low_card_table_efficiency_p (gc_tuning_point tp)
-{
-    BOOL ret = FALSE;
-    switch (tp)
-    {
-    case tuning_deciding_condemned_gen:
-    {
-        /* promote into max-generation if the card table has too many
-        * generation faults besides the n -> 0
-        */
-        ret = (generation_skip_ratio < generation_skip_ratio_threshold);
-        break;
-    }
-    default:
-        break;
-    }
-    return ret;
-}
-inline BOOL
-gc_heap::dt_high_memory_load_p()
-{
-    return ((settings.entry_memory_load >= high_memory_load_th) || g_low_memory_status);
-}
-inline BOOL
-in_range_for_segment(uint8_t* add, heap_segment* seg)
-{
-    return ((add >= heap_segment_mem (seg)) && (add < heap_segment_reserved (seg)));
-}
-#ifdef FEATURE_BASICFREEZE
-struct bk
-{
-    uint8_t* add;
-    size_t val;
-};
-class sorted_table
-{
-private:
-    ptrdiff_t size;
-    ptrdiff_t count;
-    bk* slots;
-    bk* buckets() { return (slots + 1); }
-    uint8_t*& last_slot (bk* arr) { return arr[0].add; }
-    bk* old_slots;
-public:
-    static  sorted_table* make_sorted_table ();
-    BOOL    insert (uint8_t* add, size_t val);;
-    size_t  lookup (uint8_t*& add);
-    void    remove (uint8_t* add);
-    void    clear ();
-    void    delete_sorted_table();
-    void    delete_old_slots();
-    void    enqueue_old_slot(bk* sl);
-    BOOL    ensure_space_for_insert();
-};
-sorted_table*
-sorted_table::make_sorted_table ()
-{
-    size_t size = 400;
-    sorted_table* res = (sorted_table*)new (nothrow) char [sizeof (sorted_table) + (size + 1) * sizeof (bk)];
-    if (!res)
-        return 0;
-    res->size = size;
-    res->slots = (bk*)(res + 1);
-    res->old_slots = 0;
-    res->clear();
-    return res;
-}
-void
-sorted_table::delete_sorted_table()
-{
-    if (slots != (bk*)(this+1))
-    {
-        delete slots;
-    }
-    delete_old_slots();
-    delete this;
-}
-void
-sorted_table::delete_old_slots()
-{
-    uint8_t* sl = (uint8_t*)old_slots;
-    while (sl)
-    {
-        uint8_t* dsl = sl;
-        sl = last_slot ((bk*)sl);
-        delete dsl;
-    }
-    old_slots = 0;
-}
-void
-sorted_table::enqueue_old_slot(bk* sl)
-{
-    last_slot (sl) = (uint8_t*)old_slots;
-    old_slots = sl;
-}
-inline
-size_t
-sorted_table::lookup (uint8_t*& add)
-{
-    ptrdiff_t high = (count-1);
-    ptrdiff_t low = 0;
-    ptrdiff_t ti;
-    ptrdiff_t mid;
-    bk* buck = buckets();
-    while (low <= high)
-    {
-        mid = ((low + high)/2);
-        ti = mid;
-        if (buck[ti].add > add)
-        {
-            if ((ti > 0) && (buck[ti-1].add <= add))
-            {
-                add = buck[ti-1].add;
-                return buck[ti - 1].val;
-            }
-            high = mid - 1;
-        }
-        else
-        {
-            if (buck[ti+1].add > add)
-            {
-                add = buck[ti].add;
-                return buck[ti].val;
-            }
-            low = mid + 1;
-        }
-    }
-    add = 0;
-    return 0;
-}
-BOOL
-sorted_table::ensure_space_for_insert()
-{
-    if (count == size)
-    {
-        size = (size * 3)/2;
-        assert((size * sizeof (bk)) > 0);
-        bk* res = (bk*)new (nothrow) char [(size + 1) * sizeof (bk)];
-        assert (res);
-        if (!res)
-            return FALSE;
-        last_slot (res) = 0;
-        memcpy (((bk*)res + 1), buckets(), count * sizeof (bk));
-        bk* last_old_slots = slots;
-        slots = res;
-        if (last_old_slots != (bk*)(this + 1))
-            enqueue_old_slot (last_old_slots);
-    }
-    return TRUE;
-}
-BOOL
-sorted_table::insert (uint8_t* add, size_t val)
-{
-    assert (count < size);
-    ptrdiff_t high = (count-1);
-    ptrdiff_t low = 0;
-    ptrdiff_t ti;
-    ptrdiff_t mid;
-    bk* buck = buckets();
-    while (low <= high)
-    {
-        mid = ((low + high)/2);
-        ti = mid;
-        if (buck[ti].add > add)
-        {
-            if ((ti == 0) || (buck[ti-1].add <= add))
-            {
-                for (ptrdiff_t k = count; k > ti;k--)
-                {
-                    buck [k] = buck [k-1];
-                }
-                buck[ti].add = add;
-                buck[ti].val = val;
-                count++;
-                return TRUE;
-            }
-            high = mid - 1;
-        }
-        else
-        {
-            if (buck[ti+1].add > add)
-            {
-                for (ptrdiff_t k = count; k > ti+1;k--)
-                {
-                    buck [k] = buck [k-1];
-                }
-                buck[ti+1].add = add;
-                buck[ti+1].val = val;
-                count++;
-                return TRUE;
-            }
-            low = mid + 1;
-        }
-    }
-    assert (0);
-    return TRUE;
-}
-void
-sorted_table::remove (uint8_t* add)
-{
-    ptrdiff_t high = (count-1);
-    ptrdiff_t low = 0;
-    ptrdiff_t ti;
-    ptrdiff_t mid;
-    bk* buck = buckets();
-    while (low <= high)
-    {
-        mid = ((low + high)/2);
-        ti = mid;
-        if (buck[ti].add > add)
-        {
-            if (buck[ti-1].add <= add)
-            {
-                for (ptrdiff_t k = ti; k < count; k++)
-                    buck[k-1] = buck[k];
-                count--;
-                return;
-            }
-            high = mid - 1;
-        }
-        else
-        {
-            if (buck[ti+1].add > add)
-            {
-                for (ptrdiff_t k = ti+1; k < count; k++)
-                    buck[k-1] = buck[k];
-                count--;
-                return;
-            }
-            low = mid + 1;
-        }
-    }
-    assert (0);
-}
-void
-sorted_table::clear()
-{
-    count = 1;
-    buckets()[0].add = MAX_PTR;
-}
-#endif //FEATURE_BASICFREEZE
-#ifdef USE_REGIONS
-inline
-size_t get_skewed_basic_region_index_for_address (uint8_t* address)
-{
-    assert ((g_gc_lowest_address <= address) && (address <= g_gc_highest_address));
-    size_t skewed_basic_region_index = (size_t)address >> gc_heap::min_segment_size_shr;
-    return skewed_basic_region_index;
-}
-inline
-size_t get_basic_region_index_for_address (uint8_t* address)
-{
-    size_t skewed_basic_region_index = get_skewed_basic_region_index_for_address (address);
-    return (skewed_basic_region_index - get_skewed_basic_region_index_for_address (g_gc_lowest_address));
-}
-inline
-heap_segment* get_region_info_for_address (uint8_t* address)
-{
-    size_t basic_region_index = (size_t)address >> gc_heap::min_segment_size_shr;
-    heap_segment* basic_region_info_entry = (heap_segment*)&seg_mapping_table[basic_region_index];
-    ptrdiff_t first_field = (ptrdiff_t)heap_segment_allocated (basic_region_info_entry);
-    if (first_field < 0)
-    {
-        basic_region_index += first_field;
-    }
-    return ((heap_segment*)(&seg_mapping_table[basic_region_index]));
-}
-inline
-heap_segment* get_region_info (uint8_t* region_start)
-{
-    size_t region_index = (size_t)region_start >> gc_heap::min_segment_size_shr;
-    heap_segment* region_info_entry = (heap_segment*)&seg_mapping_table[region_index];
-    dprintf (REGIONS_LOG, ("region info for region %p is at %zd, %zx (alloc: %p)",
-        region_start, region_index, (size_t)region_info_entry, heap_segment_allocated (region_info_entry)));
-    return (heap_segment*)&seg_mapping_table[region_index];
-}
-inline
-uint8_t* get_region_start (heap_segment* region_info)
-{
-    uint8_t* obj_start = heap_segment_mem (region_info);
-    return (obj_start - sizeof (aligned_plug_and_gap));
-}
-inline
-size_t get_region_size (heap_segment* region_info)
-{
-    return (size_t)(heap_segment_reserved (region_info) - get_region_start (region_info));
-}
-inline
-size_t get_region_committed_size (heap_segment* region)
-{
-    uint8_t* start = get_region_start (region);
-    uint8_t* committed = heap_segment_committed (region);
-    return committed - start;
-}
-inline bool is_free_region (heap_segment* region)
-{
-    return (heap_segment_allocated (region) == nullptr);
-}
-bool region_allocator::init (uint8_t* start, uint8_t* end, size_t alignment, uint8_t** lowest, uint8_t** highest)
-{
-    uint8_t* actual_start = start;
-    region_alignment = alignment;
-    large_region_alignment = LARGE_REGION_FACTOR * alignment;
-    global_region_start = (uint8_t*)align_region_up ((size_t)actual_start);
-    uint8_t* actual_end = end;
-    global_region_end = (uint8_t*)align_region_down ((size_t)actual_end);
-    global_region_left_used = global_region_start;
-    global_region_right_used = global_region_end;
-    num_left_used_free_units = 0;
-    num_right_used_free_units = 0;
-    size_t total_num_units = (global_region_end - global_region_start) / region_alignment;
-    total_free_units = (uint32_t)total_num_units;
-    uint32_t* unit_map = new (nothrow) uint32_t[total_num_units];
-    if (unit_map)
-    {
-        memset (unit_map, 0, sizeof (uint32_t) * total_num_units);
-        region_map_left_start = unit_map;
-        region_map_left_end = region_map_left_start;
-        region_map_right_start = unit_map + total_num_units;
-        region_map_right_end = region_map_right_start;
-        dprintf (REGIONS_LOG, ("start: %zx, end: %zx, total %zdmb(alignment: %zdmb), map units %zd",
-            (size_t)start, (size_t)end,
-            (size_t)((end - start) / 1024 / 1024),
-            (alignment / 1024 / 1024),
-            total_num_units));
-        *lowest = global_region_start;
-        *highest = global_region_end;
-    }
-    return (unit_map != 0);
-}
-inline
-uint8_t* region_allocator::region_address_of (uint32_t* map_index)
-{
-    return (global_region_start + ((map_index - region_map_left_start) * region_alignment));
-}
-inline
-uint32_t* region_allocator::region_map_index_of (uint8_t* address)
-{
-    return (region_map_left_start + ((address - global_region_start) / region_alignment));
-}
-void region_allocator::make_busy_block (uint32_t* index_start, uint32_t num_units)
-{
-#ifdef _DEBUG
-    dprintf (REGIONS_LOG, ("MBB[B: %zd] %d->%d", (size_t)num_units, (int)(index_start - region_map_left_start), (int)(index_start - region_map_left_start + num_units)));
-#endif //_DEBUG
-    ASSERT_HOLDING_SPIN_LOCK (&region_allocator_lock);
-    uint32_t* index_end = index_start + (num_units - 1);
-    *index_start = *index_end = num_units;
-}
-void region_allocator::make_free_block (uint32_t* index_start, uint32_t num_units)
-{
-#ifdef _DEBUG
-    dprintf (REGIONS_LOG, ("MFB[F: %zd] %d->%d", (size_t)num_units, (int)(index_start - region_map_left_start), (int)(index_start - region_map_left_start + num_units)));
-#endif //_DEBUG
-    ASSERT_HOLDING_SPIN_LOCK (&region_allocator_lock);
-    uint32_t* index_end = index_start + (num_units - 1);
-    *index_start = *index_end = region_alloc_free_bit | num_units;
-}
-void region_allocator::print_map (const char* msg)
-{
-    ASSERT_HOLDING_SPIN_LOCK (&region_allocator_lock);
-#ifdef _DEBUG
-    const char* heap_type = "UH";
-    dprintf (REGIONS_LOG, ("[%s]-----printing----%s", heap_type, msg));
-    uint32_t* current_index = region_map_left_start;
-    uint32_t* end_index = region_map_left_end;
-    uint32_t  count_free_units = 0;
-    for (int i = 0; i < 2; i++)
-    {
-        while (current_index < end_index)
-        {
-            uint32_t current_val = *current_index;
-            uint32_t current_num_units = get_num_units (current_val);
-            bool free_p = is_unit_memory_free (current_val);
-            dprintf (REGIONS_LOG, ("[%s][%s: %zd]%d->%d", heap_type, (free_p ? "F" : "B"), (size_t)current_num_units,
-                (int)(current_index - region_map_left_start),
-                (int)(current_index - region_map_left_start + current_num_units)));
-            if (free_p)
-            {
-                count_free_units += current_num_units;
-            }
-            current_index += current_num_units;
-        }
-        current_index = region_map_right_start;
-        end_index = region_map_right_end;
-        if (i == 0)
-        {
-            assert (count_free_units == num_left_used_free_units);
-        }
-        else
-        {
-            assert (count_free_units == num_left_used_free_units + num_right_used_free_units);
-        }
-    }
-    count_free_units += (uint32_t)(region_map_right_start - region_map_left_end);
-    assert(count_free_units == total_free_units);
-    uint32_t total_regions = (uint32_t)((global_region_end - global_region_start) / region_alignment);
-    dprintf (REGIONS_LOG, ("[%s]-----end printing----[%d total, left used %zd (free: %d), right used %zd (free: %d)]\n", heap_type, total_regions,
-        (region_map_left_end - region_map_left_start), num_left_used_free_units, (region_map_right_end - region_map_right_start), num_right_used_free_units));
-#endif //_DEBUG
-}
-uint8_t* region_allocator::allocate_end (uint32_t num_units, allocate_direction direction)
-{
-    uint8_t* alloc = NULL;
-    ASSERT_HOLDING_SPIN_LOCK (&region_allocator_lock);
-    if (global_region_left_used < global_region_right_used)
-    {
-        size_t end_remaining = global_region_right_used - global_region_left_used;
-        if ((end_remaining / region_alignment) >= num_units)
-        {
-            if (direction == allocate_forward)
-            {
-                make_busy_block (region_map_left_end, num_units);
-                region_map_left_end += num_units;
-                alloc = global_region_left_used;
-                global_region_left_used += num_units * region_alignment;
-            }
-            else
-            {
-                assert(direction == allocate_backward);
-                region_map_right_start -= num_units;
-                make_busy_block (region_map_right_start, num_units);
-                global_region_right_used -= num_units * region_alignment;
-                alloc = global_region_right_used;
-            }
-        }
-    }
-    return alloc;
-}
-void region_allocator::enter_spin_lock()
-{
-    while (true)
-    {
-        if (Interlocked::CompareExchange(&region_allocator_lock.lock, 0, -1) < 0)
-            break;
-        while (region_allocator_lock.lock >= 0)
-        {
-            YieldProcessor();           // indicate to the processor that we are spinning
-        }
-    }
-#ifdef _DEBUG
-    region_allocator_lock.holding_thread = GCToEEInterface::GetThread();
-#endif //_DEBUG
-}
-void region_allocator::leave_spin_lock()
-{
-#ifdef _DEBUG
-    region_allocator_lock.holding_thread = (Thread*)-1;
-#endif //_DEBUG
-    region_allocator_lock.lock = -1;
-}
-uint8_t* region_allocator::allocate (uint32_t num_units, allocate_direction direction, region_allocator_callback_fn fn)
-{
-    enter_spin_lock();
-    uint32_t* current_index;
-    uint32_t* end_index;
-    if (direction == allocate_forward)
-    {
-        current_index = region_map_left_start;
-        end_index = region_map_left_end;
-    }
-    else
-    {
-        assert(direction == allocate_backward);
-        current_index = region_map_right_end;
-        end_index = region_map_right_start;
-    }
-    dprintf (REGIONS_LOG, ("searching %d->%d", (int)(current_index - region_map_left_start), (int)(end_index - region_map_left_start)));
-    print_map ("before alloc");
-    if (((direction == allocate_forward) && (num_left_used_free_units >= num_units)) ||
-        ((direction == allocate_backward) && (num_right_used_free_units >= num_units)))
-    {
-        while (((direction == allocate_forward) && (current_index < end_index)) ||
-            ((direction == allocate_backward) && (current_index > end_index)))
-        {
-            uint32_t current_val = *(current_index - ((direction == allocate_backward) ? 1 : 0));
-            uint32_t current_num_units = get_num_units (current_val);
-            bool free_p = is_unit_memory_free (current_val);
-            dprintf (REGIONS_LOG, ("ALLOC[%s: %zd]%d->%d", (free_p ? "F" : "B"), (size_t)current_num_units,
-                (int)(current_index - region_map_left_start), (int)(current_index + current_num_units - region_map_left_start)));
-            if (free_p)
-            {
-                if (current_num_units >= num_units)
-                {
-                    dprintf (REGIONS_LOG, ("found %zd contiguous free units(%d->%d), sufficient",
-                        (size_t)current_num_units,
-                        (int)(current_index - region_map_left_start),
-                        (int)(current_index - region_map_left_start + current_num_units)));
-                    if (direction == allocate_forward)
-                    {
-                        assert (num_left_used_free_units >= num_units);
-                        num_left_used_free_units -= num_units;
-                    }
-                    else
-                    {
-                        assert (direction == allocate_backward);
-                        assert (num_right_used_free_units >= num_units);
-                        num_right_used_free_units -= num_units;
-                    }
-                    uint32_t* busy_block;
-                    uint32_t* free_block;
-                    if (direction == 1)
-                    {
-                        busy_block = current_index;
-                        free_block = current_index + num_units;
-                    }
-                    else
-                    {
-                        busy_block = current_index - num_units;
-                        free_block = current_index - current_num_units;
-                    }
-                    make_busy_block (busy_block, num_units);
-                    if ((current_num_units - num_units) > 0)
-                    {
-                        make_free_block (free_block, (current_num_units - num_units));
-                    }
-                    total_free_units -= num_units;
-                    print_map ("alloc: found in free");
-                    leave_spin_lock();
-                    return region_address_of (busy_block);
-                }
-            }
-            if (direction == allocate_forward)
-            {
-                current_index += current_num_units;
-            }
-            else
-            {
-                current_index -= current_num_units;
-            }
-        }
-    }
-    uint8_t* alloc = allocate_end (num_units, direction);
-    if (alloc)
-    {
-        total_free_units -= num_units;
-        if (fn != nullptr)
-        {
-            if (!fn (global_region_left_used))
-            {
-                delete_region_impl (alloc);
-                alloc = nullptr;
-            }
-        }
-        if (alloc)
-        {
-            print_map ("alloc: found at the end");
-        }
-    }
-    else
-    {
-        dprintf (REGIONS_LOG, ("couldn't find memory at the end! only %zd bytes left", (global_region_right_used - global_region_left_used)));
-    }
-    leave_spin_lock();
-    return alloc;
-}
-bool region_allocator::allocate_region (int gen_num, size_t size, uint8_t** start, uint8_t** end, allocate_direction direction, region_allocator_callback_fn fn)
-{
-    size_t alignment = region_alignment;
-    size_t alloc_size = align_region_up (size);
-    uint32_t num_units = (uint32_t)(alloc_size / alignment);
-    bool ret = false;
-    uint8_t* alloc = NULL;
-    dprintf (REGIONS_LOG, ("----GET %u-----", num_units));
-    alloc = allocate (num_units, direction, fn);
-    *start = alloc;
-    *end = alloc + alloc_size;
-    ret = (alloc != NULL);
-    gc_etw_segment_type segment_type;
-    if (gen_num == loh_generation)
-    {
-        segment_type = gc_etw_segment_large_object_heap;
-    }
-    else if (gen_num == poh_generation)
-    {
-        segment_type = gc_etw_segment_pinned_object_heap;
-    }
-    else
-    {
-        segment_type = gc_etw_segment_small_object_heap;
-    }
-    FIRE_EVENT(GCCreateSegment_V1, (alloc + sizeof (aligned_plug_and_gap)),
-                                  size - sizeof (aligned_plug_and_gap),
-                                  segment_type);
-    return ret;
-}
-bool region_allocator::allocate_basic_region (int gen_num, uint8_t** start, uint8_t** end, region_allocator_callback_fn fn)
-{
-    return allocate_region (gen_num, region_alignment, start, end, allocate_forward, fn);
-}
-bool region_allocator::allocate_large_region (int gen_num, uint8_t** start, uint8_t** end, allocate_direction direction, size_t size, region_allocator_callback_fn fn)
-{
-    if (size == 0)
-        size = large_region_alignment;
-    else
-    {
-        assert (round_up_power2(large_region_alignment) == large_region_alignment);
-        size = (size + (large_region_alignment - 1)) & ~(large_region_alignment - 1);
-    }
-    return allocate_region (gen_num, size, start, end, direction, fn);
-}
-void region_allocator::delete_region (uint8_t* region_start)
-{
-    enter_spin_lock();
-    delete_region_impl (region_start);
-    leave_spin_lock();
-}
-void region_allocator::delete_region_impl (uint8_t* region_start)
-{
-    ASSERT_HOLDING_SPIN_LOCK (&region_allocator_lock);
-    assert (is_region_aligned (region_start));
-    print_map ("before delete");
-    uint32_t* current_index = region_map_index_of (region_start);
-    uint32_t current_val = *current_index;
-    assert (!is_unit_memory_free (current_val));
-    dprintf (REGIONS_LOG, ("----DEL %d (%u units)-----", (*current_index - *region_map_left_start), current_val));
-    uint32_t* region_end_index = current_index + current_val;
-    uint8_t* region_end = region_address_of (region_end_index);
-    int free_block_size = current_val;
-    uint32_t* free_index = current_index;
-    if (free_index <= region_map_left_end)
-    {
-        num_left_used_free_units += free_block_size;
-    }
-    else
-    {
-        assert (free_index >= region_map_right_start);
-        num_right_used_free_units += free_block_size;
-    }
-    if ((current_index != region_map_left_start) && (current_index != region_map_right_start))
-    {
-        uint32_t previous_val = *(current_index - 1);
-        if (is_unit_memory_free(previous_val))
-        {
-            uint32_t previous_size = get_num_units (previous_val);
-            free_index -= previous_size;
-            free_block_size += previous_size;
-        }
-    }
-    if ((region_end != global_region_left_used) && (region_end != global_region_end))
-    {
-        uint32_t next_val = *region_end_index;
-        if (is_unit_memory_free(next_val))
-        {
-            uint32_t next_size = get_num_units (next_val);
-            free_block_size += next_size;
-            region_end += next_size;
-        }
-    }
-    if (region_end == global_region_left_used)
-    {
-        num_left_used_free_units -= free_block_size;
-        region_map_left_end = free_index;
-        dprintf (REGIONS_LOG, ("adjust global left used from %p to %p",
-            global_region_left_used, region_address_of (free_index)));
-        global_region_left_used = region_address_of (free_index);
-    }
-    else if (region_start == global_region_right_used)
-    {
-        num_right_used_free_units -= free_block_size;
-        region_map_right_start = free_index + free_block_size;
-        dprintf (REGIONS_LOG, ("adjust global right used from %p to %p",
-            global_region_right_used, region_address_of (free_index + free_block_size)));
-        global_region_right_used = region_address_of (free_index + free_block_size);
-    }
-    else
-    {
-        make_free_block (free_index, free_block_size);
-    }
-    total_free_units += current_val;
-    print_map ("after delete");
-}
-void region_allocator::move_highest_free_regions (int64_t n, bool small_region_p, region_free_list to_free_list[count_free_region_kinds])
-{
-    assert (n > 0);
-    uint32_t* current_index = region_map_left_end - 1;
-    uint32_t* lowest_index = region_map_left_start;
-    while (current_index >= lowest_index)
-    {
-        uint32_t current_val = *current_index;
-        uint32_t current_num_units = get_num_units (current_val);
-        bool free_p = is_unit_memory_free (current_val);
-        if (!free_p && ((current_num_units == 1) == small_region_p))
-        {
-            uint32_t* index = current_index - (current_num_units - 1);
-            heap_segment* region = get_region_info (region_address_of (index));
-            if (is_free_region (region) && !region_free_list::is_on_free_list (region, to_free_list))
-            {
-                if (n >= current_num_units)
-                {
-                    n -= current_num_units;
-                    region_free_list::unlink_region (region);
-                    region_free_list::add_region (region, to_free_list);
-                }
-                else
-                {
-                    break;
-                }
-            }
-        }
-        current_index -= current_num_units;
-    }
-}
-#endif //USE_REGIONS
-inline
-uint8_t* align_on_segment (uint8_t* add)
-{
-    return (uint8_t*)((size_t)(add + (((size_t)1 << gc_heap::min_segment_size_shr) - 1)) & ~(((size_t)1 << gc_heap::min_segment_size_shr) - 1));
-}
-inline
-uint8_t* align_lower_segment (uint8_t* add)
-{
-    return (uint8_t*)((size_t)(add) & ~(((size_t)1 << gc_heap::min_segment_size_shr) - 1));
-}
-size_t size_seg_mapping_table_of (uint8_t* from, uint8_t* end)
-{
-    from = align_lower_segment (from);
-    end = align_on_segment (end);
-    dprintf (1, ("from: %p, end: %p, size: %zx", from, end,
-        sizeof (seg_mapping)*(((size_t)(end - from) >> gc_heap::min_segment_size_shr))));
-    return sizeof (seg_mapping)*((size_t)(end - from) >> gc_heap::min_segment_size_shr);
-}
-size_t size_region_to_generation_table_of (uint8_t* from, uint8_t* end)
-{
-    dprintf (1, ("from: %p, end: %p, size: %zx", from, end,
-        sizeof (uint8_t)*(((size_t)(end - from) >> gc_heap::min_segment_size_shr))));
-    return sizeof (uint8_t)*((size_t)(end - from) >> gc_heap::min_segment_size_shr);
-}
-inline
-size_t seg_mapping_word_of (uint8_t* add)
-{
-    return (size_t)add >> gc_heap::min_segment_size_shr;
-}
-#ifdef FEATURE_BASICFREEZE
-inline
-size_t ro_seg_begin_index (heap_segment* seg)
-{
-#ifdef USE_REGIONS
-    size_t begin_index = (size_t)heap_segment_mem (seg) >> gc_heap::min_segment_size_shr;
-#else
-    size_t begin_index = (size_t)seg >> gc_heap::min_segment_size_shr;
-#endif //USE_REGIONS
-    begin_index = max (begin_index, (size_t)g_gc_lowest_address >> gc_heap::min_segment_size_shr);
-    return begin_index;
-}
-inline
-size_t ro_seg_end_index (heap_segment* seg)
-{
-    size_t end_index = (size_t)(heap_segment_reserved (seg) - 1) >> gc_heap::min_segment_size_shr;
-    end_index = min (end_index, (size_t)g_gc_highest_address >> gc_heap::min_segment_size_shr);
-    return end_index;
-}
-void seg_mapping_table_add_ro_segment (heap_segment* seg)
-{
-    if ((heap_segment_reserved (seg) <= g_gc_lowest_address) || (heap_segment_mem (seg) >= g_gc_highest_address))
-        return;
-    for (size_t entry_index = ro_seg_begin_index (seg); entry_index <= ro_seg_end_index (seg); entry_index++)
-    {
-#ifdef USE_REGIONS
-        heap_segment* region = (heap_segment*)&seg_mapping_table[entry_index];
-        heap_segment_allocated (region) = (uint8_t*)ro_in_entry;
-#else
-        seg_mapping_table[entry_index].seg1 = (heap_segment*)((size_t)seg_mapping_table[entry_index].seg1 | ro_in_entry);
-#endif //USE_REGIONS
-    }
-}
-void seg_mapping_table_remove_ro_segment (heap_segment* seg)
-{
-    UNREFERENCED_PARAMETER(seg);
-#if 0
-#endif //0
-}
-heap_segment* ro_segment_lookup (uint8_t* o)
-{
-    uint8_t* ro_seg_start = o;
-    heap_segment* seg = (heap_segment*)gc_heap::seg_table->lookup (ro_seg_start);
-    if (ro_seg_start && in_range_for_segment (o, seg))
-        return seg;
-    else
-        return 0;
-}
-#endif //FEATURE_BASICFREEZE
-void gc_heap::seg_mapping_table_add_segment (heap_segment* seg, gc_heap* hp)
-{
-#ifndef USE_REGIONS
-    size_t seg_end = (size_t)(heap_segment_reserved (seg) - 1);
-    size_t begin_index = (size_t)seg >> gc_heap::min_segment_size_shr;
-    seg_mapping* begin_entry = &seg_mapping_table[begin_index];
-    size_t end_index = seg_end >> gc_heap::min_segment_size_shr;
-    seg_mapping* end_entry = &seg_mapping_table[end_index];
-    dprintf (2, ("adding seg %p(%zd)-%p(%zd)",
-        seg, begin_index, heap_segment_reserved (seg), end_index));
-    dprintf (2, ("before add: begin entry%zd: boundary: %p; end entry: %zd: boundary: %p",
-        begin_index, (seg_mapping_table[begin_index].boundary + 1),
-        end_index, (seg_mapping_table[end_index].boundary + 1)));
-#ifdef MULTIPLE_HEAPS
-#ifdef SIMPLE_DPRINTF
-    dprintf (2, ("begin %zd: h0: %p(%d), h1: %p(%d); end %zd: h0: %p(%d), h1: %p(%d)",
-        begin_index, (uint8_t*)(begin_entry->h0), (begin_entry->h0 ? begin_entry->h0->heap_number : -1),
-        (uint8_t*)(begin_entry->h1), (begin_entry->h1 ? begin_entry->h1->heap_number : -1),
-        end_index, (uint8_t*)(end_entry->h0), (end_entry->h0 ? end_entry->h0->heap_number : -1),
-        (uint8_t*)(end_entry->h1), (end_entry->h1 ? end_entry->h1->heap_number : -1)));
-#endif //SIMPLE_DPRINTF
-    assert (end_entry->boundary == 0);
-    assert (end_entry->h0 == 0);
-    end_entry->h0 = hp;
-    assert (begin_entry->h1 == 0);
-    begin_entry->h1 = hp;
-#else
-    UNREFERENCED_PARAMETER(hp);
-#endif //MULTIPLE_HEAPS
-    end_entry->boundary = (uint8_t*)seg_end;
-    dprintf (2, ("set entry %zd seg1 and %zd seg0 to %p", begin_index, end_index, seg));
-    assert ((begin_entry->seg1 == 0) || ((size_t)(begin_entry->seg1) == ro_in_entry));
-    begin_entry->seg1 = (heap_segment*)((size_t)(begin_entry->seg1) | (size_t)seg);
-    end_entry->seg0 = seg;
-    for (size_t entry_index = (begin_index + 1); entry_index <= (end_index - 1); entry_index++)
-    {
-        assert (seg_mapping_table[entry_index].boundary == 0);
-#ifdef MULTIPLE_HEAPS
-        assert (seg_mapping_table[entry_index].h0 == 0);
-        seg_mapping_table[entry_index].h1 = hp;
-#endif //MULTIPLE_HEAPS
-        seg_mapping_table[entry_index].seg1 = seg;
-    }
-    dprintf (2, ("after add: begin entry%zd: boundary: %p; end entry: %zd: boundary: %p",
-        begin_index, (seg_mapping_table[begin_index].boundary + 1),
-        end_index, (seg_mapping_table[end_index].boundary + 1)));
-#if defined(MULTIPLE_HEAPS) && defined(SIMPLE_DPRINTF)
-    dprintf (2, ("begin %zd: h0: %p(%d), h1: %p(%d); end: %zd h0: %p(%d), h1: %p(%d)",
-        begin_index, (uint8_t*)(begin_entry->h0), (begin_entry->h0 ? begin_entry->h0->heap_number : -1),
-        (uint8_t*)(begin_entry->h1), (begin_entry->h1 ? begin_entry->h1->heap_number : -1),
-        end_index, (uint8_t*)(end_entry->h0), (end_entry->h0 ? end_entry->h0->heap_number : -1),
-        (uint8_t*)(end_entry->h1), (end_entry->h1 ? end_entry->h1->heap_number : -1)));
-#endif //MULTIPLE_HEAPS && SIMPLE_DPRINTF
-#endif //!USE_REGIONS
-}
-void gc_heap::seg_mapping_table_remove_segment (heap_segment* seg)
-{
-#ifndef USE_REGIONS
-    size_t seg_end = (size_t)(heap_segment_reserved (seg) - 1);
-    size_t begin_index = (size_t)seg >> gc_heap::min_segment_size_shr;
-    seg_mapping* begin_entry = &seg_mapping_table[begin_index];
-    size_t end_index = seg_end >> gc_heap::min_segment_size_shr;
-    seg_mapping* end_entry = &seg_mapping_table[end_index];
-    dprintf (2, ("removing seg %p(%zd)-%p(%zd)",
-        seg, begin_index, heap_segment_reserved (seg), end_index));
-    assert (end_entry->boundary == (uint8_t*)seg_end);
-    end_entry->boundary = 0;
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hp = heap_segment_heap (seg);
-    assert (end_entry->h0 == hp);
-    end_entry->h0 = 0;
-    assert (begin_entry->h1 == hp);
-    begin_entry->h1 = 0;
-#endif //MULTIPLE_HEAPS
-    assert (begin_entry->seg1 != 0);
-    begin_entry->seg1 = (heap_segment*)((size_t)(begin_entry->seg1) & ro_in_entry);
-    end_entry->seg0 = 0;
-    for (size_t entry_index = (begin_index + 1); entry_index <= (end_index - 1); entry_index++)
-    {
-        assert (seg_mapping_table[entry_index].boundary == 0);
-#ifdef MULTIPLE_HEAPS
-        assert (seg_mapping_table[entry_index].h0 == 0);
-        assert (seg_mapping_table[entry_index].h1 == hp);
-        seg_mapping_table[entry_index].h1 = 0;
-#endif //MULTIPLE_HEAPS
-        seg_mapping_table[entry_index].seg1 = 0;
-    }
-    dprintf (2, ("after remove: begin entry%zd: boundary: %p; end entry: %zd: boundary: %p",
-        begin_index, (seg_mapping_table[begin_index].boundary + 1),
-        end_index, (seg_mapping_table[end_index].boundary + 1)));
-#ifdef MULTIPLE_HEAPS
-    dprintf (2, ("begin %zd: h0: %p, h1: %p; end: %zd h0: %p, h1: %p",
-        begin_index, (uint8_t*)(begin_entry->h0), (uint8_t*)(begin_entry->h1),
-        end_index, (uint8_t*)(end_entry->h0), (uint8_t*)(end_entry->h1)));
-#endif //MULTIPLE_HEAPS
-#endif //!USE_REGIONS
-}
-#ifdef MULTIPLE_HEAPS
-inline
-gc_heap* seg_mapping_table_heap_of_worker (uint8_t* o)
-{
-    size_t index = (size_t)o >> gc_heap::min_segment_size_shr;
-    seg_mapping* entry = &seg_mapping_table[index];
-#ifdef USE_REGIONS
-    gc_heap* hp = heap_segment_heap ((heap_segment*)entry);
-#else
-    gc_heap* hp = ((o > entry->boundary) ? entry->h1 : entry->h0);
-    dprintf (2, ("checking obj %p, index is %zd, entry: boundary: %p, h0: %p, seg0: %p, h1: %p, seg1: %p",
-        o, index, (entry->boundary + 1),
-        (uint8_t*)(entry->h0), (uint8_t*)(entry->seg0),
-        (uint8_t*)(entry->h1), (uint8_t*)(entry->seg1)));
-#ifdef _DEBUG
-    heap_segment* seg = ((o > entry->boundary) ? entry->seg1 : entry->seg0);
-#ifdef FEATURE_BASICFREEZE
-    if ((size_t)seg & ro_in_entry)
-        seg = (heap_segment*)((size_t)seg & ~ro_in_entry);
-#endif //FEATURE_BASICFREEZE
-#ifdef TRACE_GC
-    if (seg)
-    {
-        if (in_range_for_segment (o, seg))
-        {
-            dprintf (2, ("obj %p belongs to segment %p(-%p)", o, seg, (uint8_t*)heap_segment_allocated (seg)));
-        }
-        else
-        {
-            dprintf (2, ("found seg %p(-%p) for obj %p, but it's not on the seg",
-                seg, (uint8_t*)heap_segment_allocated (seg), o));
-        }
-    }
-    else
-    {
-        dprintf (2, ("could not find obj %p in any existing segments", o));
-    }
-#endif //TRACE_GC
-#endif //_DEBUG
-#endif //USE_REGIONS
-    return hp;
-}
-gc_heap* seg_mapping_table_heap_of (uint8_t* o)
-{
-    if ((o < g_gc_lowest_address) || (o >= g_gc_highest_address))
-        return 0;
-    return seg_mapping_table_heap_of_worker (o);
-}
-gc_heap* seg_mapping_table_heap_of_gc (uint8_t* o)
-{
-#ifdef FEATURE_BASICFREEZE
-    if ((o < g_gc_lowest_address) || (o >= g_gc_highest_address))
-        return 0;
-#endif //FEATURE_BASICFREEZE
-    return seg_mapping_table_heap_of_worker (o);
-}
-#endif //MULTIPLE_HEAPS
-heap_segment* seg_mapping_table_segment_of (uint8_t* o)
-{
-#ifdef FEATURE_BASICFREEZE
-    if ((o < g_gc_lowest_address) || (o >= g_gc_highest_address))
-        return ro_segment_lookup (o);
-#endif //FEATURE_BASICFREEZE
-    size_t index = (size_t)o >> gc_heap::min_segment_size_shr;
-    seg_mapping* entry = &seg_mapping_table[index];
-#ifdef USE_REGIONS
-    ptrdiff_t first_field = (ptrdiff_t)heap_segment_allocated ((heap_segment*)entry);
-    if (first_field == 0)
-    {
-        dprintf (REGIONS_LOG, ("asked for seg for %p, in a freed region mem: %p, committed %p",
-            o, heap_segment_mem ((heap_segment*)entry),
-            heap_segment_committed ((heap_segment*)entry)));
-        return 0;
-    }
-    assert (first_field != 0);
-    assert (first_field != ro_in_entry);
-    if (first_field < 0)
-    {
-        index += first_field;
-    }
-    heap_segment* seg = (heap_segment*)&seg_mapping_table[index];
-#else //USE_REGIONS
-    dprintf (2, ("checking obj %p, index is %zd, entry: boundary: %p, seg0: %p, seg1: %p",
-        o, index, (entry->boundary + 1),
-        (uint8_t*)(entry->seg0), (uint8_t*)(entry->seg1)));
-    heap_segment* seg = ((o > entry->boundary) ? entry->seg1 : entry->seg0);
-#ifdef FEATURE_BASICFREEZE
-    if ((size_t)seg & ro_in_entry)
-        seg = (heap_segment*)((size_t)seg & ~ro_in_entry);
-#endif //FEATURE_BASICFREEZE
-#endif //USE_REGIONS
-    if (seg)
-    {
-        if (in_range_for_segment (o, seg))
-        {
-            dprintf (2, ("obj %p belongs to segment %p(-%p)", o, (uint8_t*)heap_segment_mem(seg), (uint8_t*)heap_segment_reserved(seg)));
-        }
-        else
-        {
-            dprintf (2, ("found seg %p(-%p) for obj %p, but it's not on the seg, setting it to 0",
-                (uint8_t*)heap_segment_mem(seg), (uint8_t*)heap_segment_reserved(seg), o));
-            seg = 0;
-        }
-    }
-    else
-    {
-        dprintf (2, ("could not find obj %p in any existing segments", o));
-    }
-#ifdef FEATURE_BASICFREEZE
-    if (!seg)
-    {
-        seg = ro_segment_lookup (o);
-        if (seg && !in_range_for_segment (o, seg))
-            seg = 0;
-    }
-#endif //FEATURE_BASICFREEZE
-    return seg;
-}
-size_t gcard_of ( uint8_t*);
-#define GC_MARKED       (size_t)0x1
-#ifdef DOUBLY_LINKED_FL
-#define BGC_MARKED_BY_FGC (size_t)0x2
-#define MAKE_FREE_OBJ_IN_COMPACT (size_t)0x4
-#define ALLOWED_SPECIAL_HEADER_BITS (GC_MARKED|BGC_MARKED_BY_FGC|MAKE_FREE_OBJ_IN_COMPACT)
-#else //DOUBLY_LINKED_FL
-#define ALLOWED_SPECIAL_HEADER_BITS (GC_MARKED)
-#endif //!DOUBLY_LINKED_FL
-#ifdef HOST_64BIT
-#define SPECIAL_HEADER_BITS (0x7)
-#else
-#define SPECIAL_HEADER_BITS (0x3)
-#endif
-#define slot(i, j) ((uint8_t**)(i))[(j)+1]
-#define free_object_base_size (plug_skew + sizeof(ArrayBase))
-#define free_list_slot(x) ((uint8_t**)(x))[2]
-#define free_list_undo(x) ((uint8_t**)(x))[-1]
-#define UNDO_EMPTY ((uint8_t*)1)
-#ifdef DOUBLY_LINKED_FL
-#define free_list_prev(x) ((uint8_t**)(x))[3]
-#define PREV_EMPTY ((uint8_t*)1)
-void check_and_clear_in_free_list (uint8_t* o, size_t size)
-{
-    if (size >= min_free_list)
-    {
-        free_list_prev (o) = PREV_EMPTY;
-    }
-}
-void clear_prev_bit (uint8_t* o, size_t size)
-{
-    if (size >= min_free_list)
-    {
-        free_list_prev (o) = 0;
-    }
-}
-#endif //DOUBLY_LINKED_FL
-class CObjectHeader : public Object
-{
-public:
-#if defined(FEATURE_NATIVEAOT) || defined(BUILD_AS_STANDALONE)
-    uint32_t GetNumComponents()
-    {
-        return ((ArrayBase *)this)->GetNumComponents();
-    }
-    void Validate(BOOL bDeep=TRUE, BOOL bVerifyNextHeader = FALSE, BOOL bVerifySyncBlock = FALSE)
-    {
-        UNREFERENCED_PARAMETER(bVerifyNextHeader);
-        UNREFERENCED_PARAMETER(bVerifySyncBlock);
-        MethodTable * pMT = GetMethodTable();
-        _ASSERTE(pMT->SanityCheck());
-        bool noRangeChecks =
-            (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_NO_RANGE_CHECKS) == GCConfig::HEAPVERIFY_NO_RANGE_CHECKS;
-        BOOL fSmallObjectHeapPtr = FALSE, fLargeObjectHeapPtr = FALSE;
-        if (!noRangeChecks)
-        {
-            fSmallObjectHeapPtr = g_theGCHeap->IsHeapPointer(this, TRUE);
-            if (!fSmallObjectHeapPtr)
-                fLargeObjectHeapPtr = g_theGCHeap->IsHeapPointer(this);
-            _ASSERTE(fSmallObjectHeapPtr || fLargeObjectHeapPtr);
-        }
-#ifdef FEATURE_STRUCTALIGN
-        _ASSERTE(IsStructAligned((uint8_t *)this, GetMethodTable()->GetBaseAlignment()));
-#endif // FEATURE_STRUCTALIGN
-#if defined(FEATURE_64BIT_ALIGNMENT) && !defined(FEATURE_NATIVEAOT)
-        if (pMT->RequiresAlign8())
-        {
-            _ASSERTE((((size_t)this) & 0x7) == (pMT->IsValueType() ? 4U : 0U));
-        }
-#endif // FEATURE_64BIT_ALIGNMENT
-#ifdef VERIFY_HEAP
-        if (bDeep && (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC))
-            g_theGCHeap->ValidateObjectMember(this);
-#endif
-        if (fSmallObjectHeapPtr)
-        {
-#ifdef FEATURE_BASICFREEZE
-            _ASSERTE(!g_theGCHeap->IsLargeObject(this) || g_theGCHeap->IsInFrozenSegment(this));
-#else
-            _ASSERTE(!g_theGCHeap->IsLargeObject(this));
-#endif
-        }
-    }
-    void ValidateHeap(BOOL bDeep)
-    {
-        Validate(bDeep);
-    }
-#endif //FEATURE_NATIVEAOT || BUILD_AS_STANDALONE
-    MethodTable    *GetMethodTable() const
-    {
-        return( (MethodTable *) (((size_t) RawGetMethodTable()) & (~SPECIAL_HEADER_BITS)));
-    }
-    void SetMarked()
-    {
-        _ASSERTE(RawGetMethodTable());
-        RawSetMethodTable((MethodTable *) (((size_t) RawGetMethodTable()) | GC_MARKED));
-    }
-    BOOL IsMarked() const
-    {
-        return !!(((size_t)RawGetMethodTable()) & GC_MARKED);
-    }
-    void SetPinned()
-    {
-        assert (!(gc_heap::settings.concurrent));
-        GetHeader()->SetGCBit();
-    }
-    BOOL IsPinned() const
-    {
-        return !!((((CObjectHeader*)this)->GetHeader()->GetBits()) & BIT_SBLK_GC_RESERVE);
-    }
-    void ClearMarked()
-    {
-#ifdef DOUBLY_LINKED_FL
-        RawSetMethodTable ((MethodTable *)(((size_t) RawGetMethodTable()) & (~GC_MARKED)));
-#else
-        RawSetMethodTable (GetMethodTable());
-#endif //DOUBLY_LINKED_FL
-    }
-#ifdef DOUBLY_LINKED_FL
-    void SetBGCMarkBit()
-    {
-        RawSetMethodTable((MethodTable *) (((size_t) RawGetMethodTable()) | BGC_MARKED_BY_FGC));
-    }
-    BOOL IsBGCMarkBitSet() const
-    {
-        return !!(((size_t)RawGetMethodTable()) & BGC_MARKED_BY_FGC);
-    }
-    void ClearBGCMarkBit()
-    {
-        RawSetMethodTable((MethodTable *)(((size_t) RawGetMethodTable()) & (~BGC_MARKED_BY_FGC)));
-    }
-    void SetFreeObjInCompactBit()
-    {
-        RawSetMethodTable((MethodTable *) (((size_t) RawGetMethodTable()) | MAKE_FREE_OBJ_IN_COMPACT));
-    }
-    BOOL IsFreeObjInCompactBitSet() const
-    {
-        return !!(((size_t)RawGetMethodTable()) & MAKE_FREE_OBJ_IN_COMPACT);
-    }
-    void ClearFreeObjInCompactBit()
-    {
-#ifdef _DEBUG
-        Validate(FALSE);
-#endif //_DEBUG
-        RawSetMethodTable((MethodTable *)(((size_t) RawGetMethodTable()) & (~MAKE_FREE_OBJ_IN_COMPACT)));
-    }
-#endif //DOUBLY_LINKED_FL
-    size_t ClearSpecialBits()
-    {
-        size_t special_bits = ((size_t)RawGetMethodTable()) & SPECIAL_HEADER_BITS;
-        if (special_bits != 0)
-        {
-            assert ((special_bits & (~ALLOWED_SPECIAL_HEADER_BITS)) == 0);
-            RawSetMethodTable ((MethodTable*)(((size_t)RawGetMethodTable()) & ~(SPECIAL_HEADER_BITS)));
-        }
-        return special_bits;
-    }
-    void SetSpecialBits (size_t special_bits)
-    {
-        assert ((special_bits & (~ALLOWED_SPECIAL_HEADER_BITS)) == 0);
-        if (special_bits != 0)
-        {
-            RawSetMethodTable ((MethodTable*)(((size_t)RawGetMethodTable()) | special_bits));
-        }
-    }
-    CGCDesc *GetSlotMap ()
-    {
-        assert (GetMethodTable()->ContainsPointers());
-        return CGCDesc::GetCGCDescFromMT(GetMethodTable());
-    }
-    void SetFree(size_t size)
-    {
-        assert (size >= free_object_base_size);
-        assert (g_gc_pFreeObjectMethodTable->GetBaseSize() == free_object_base_size);
-        assert (g_gc_pFreeObjectMethodTable->RawGetComponentSize() == 1);
-        RawSetMethodTable( g_gc_pFreeObjectMethodTable );
-        size_t* numComponentsPtr = (size_t*) &((uint8_t*) this)[ArrayBase::GetOffsetOfNumComponents()];
-        *numComponentsPtr = size - free_object_base_size;
-#ifdef VERIFY_HEAP
-        assert (*numComponentsPtr >= 0);
-        if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
-        {
-            memset (((uint8_t*)this)+sizeof(ArrayBase), 0xcc, *numComponentsPtr);
-#ifdef DOUBLY_LINKED_FL
-            if (*numComponentsPtr > 0)
-            {
-                free_list_slot (this) = 0;
-            }
-#endif //DOUBLY_LINKED_FL
-        }
-#endif //VERIFY_HEAP
-#ifdef DOUBLY_LINKED_FL
-        check_and_clear_in_free_list ((uint8_t*)this, size);
-#endif //DOUBLY_LINKED_FL
-    }
-    void UnsetFree()
-    {
-        size_t size = free_object_base_size - plug_skew;
-        PTR_PTR m = (PTR_PTR) this;
-        for (size_t i = 0; i < size / sizeof(PTR_PTR); i++)
-            *(m++) = 0;
-    }
-    BOOL IsFree () const
-    {
-        return (GetMethodTable() == g_gc_pFreeObjectMethodTable);
-    }
-#ifdef FEATURE_STRUCTALIGN
-    int GetRequiredAlignment () const
-    {
-        return GetMethodTable()->GetRequiredAlignment();
-    }
-#endif // FEATURE_STRUCTALIGN
-    BOOL ContainsPointers() const
-    {
-        return GetMethodTable()->ContainsPointers();
-    }
-#ifdef COLLECTIBLE_CLASS
-    BOOL Collectible() const
-    {
-        return GetMethodTable()->Collectible();
-    }
-    FORCEINLINE BOOL ContainsPointersOrCollectible() const
-    {
-        MethodTable *pMethodTable = GetMethodTable();
-        return (pMethodTable->ContainsPointers() || pMethodTable->Collectible());
-    }
-#endif //COLLECTIBLE_CLASS
-    Object* GetObjectBase() const
-    {
-        return (Object*) this;
-    }
-};
-#define header(i) ((CObjectHeader*)(i))
-#define method_table(o) ((CObjectHeader*)(o))->GetMethodTable()
-#ifdef DOUBLY_LINKED_FL
-inline
-BOOL is_on_free_list (uint8_t* o, size_t size)
-{
-    if (size >= min_free_list)
-    {
-        if (header(o)->GetMethodTable() == g_gc_pFreeObjectMethodTable)
-        {
-            return (free_list_prev (o) != PREV_EMPTY);
-        }
-    }
-    return FALSE;
-}
-inline
-void set_plug_bgc_mark_bit (uint8_t* node)
-{
-    header(node)->SetBGCMarkBit();
-}
-inline
-BOOL is_plug_bgc_mark_bit_set (uint8_t* node)
-{
-    return header(node)->IsBGCMarkBitSet();
-}
-inline
-void clear_plug_bgc_mark_bit (uint8_t* node)
-{
-    header(node)->ClearBGCMarkBit();
-}
-inline
-void set_free_obj_in_compact_bit (uint8_t* node)
-{
-    header(node)->SetFreeObjInCompactBit();
-}
-inline
-BOOL is_free_obj_in_compact_bit_set (uint8_t* node)
-{
-    return header(node)->IsFreeObjInCompactBitSet();
-}
-inline
-void clear_free_obj_in_compact_bit (uint8_t* node)
-{
-    header(node)->ClearFreeObjInCompactBit();
-}
-#endif //DOUBLY_LINKED_FL
-#ifdef SHORT_PLUGS
-inline
-void set_plug_padded (uint8_t* node)
-{
-    header(node)->SetMarked();
-}
-inline
-void clear_plug_padded (uint8_t* node)
-{
-    header(node)->ClearMarked();
-}
-inline
-BOOL is_plug_padded (uint8_t* node)
-{
-    return header(node)->IsMarked();
-}
-#else //SHORT_PLUGS
-inline void set_plug_padded (uint8_t* node){}
-inline void clear_plug_padded (uint8_t* node){}
-inline
-BOOL is_plug_padded (uint8_t* node){return FALSE;}
-#endif //SHORT_PLUGS
-inline
-size_t clear_special_bits (uint8_t* node)
-{
-    return header(node)->ClearSpecialBits();
-}
-inline
-void set_special_bits (uint8_t* node, size_t special_bits)
-{
-    header(node)->SetSpecialBits (special_bits);
-}
-inline size_t unused_array_size(uint8_t * p)
-{
-    assert(((CObjectHeader*)p)->IsFree());
-    size_t* numComponentsPtr = (size_t*)(p + ArrayBase::GetOffsetOfNumComponents());
-    return free_object_base_size + *numComponentsPtr;
-}
-inline
-heap_segment* heap_segment_non_sip (heap_segment* ns)
-{
-#ifdef USE_REGIONS
-    if ((ns == 0) || !heap_segment_swept_in_plan (ns))
-    {
-        return ns;
-    }
-    else
-    {
-        do
-        {
-            if (heap_segment_swept_in_plan (ns))
-            {
-                dprintf (REGIONS_LOG, ("region %p->%p SIP",
-                    heap_segment_mem (ns), heap_segment_allocated (ns)));
-            }
-            ns = heap_segment_next (ns);
-        } while ((ns != 0) && heap_segment_swept_in_plan (ns));
-        return ns;
-    }
-#else //USE_REGIONS
-    return ns;
-#endif //USE_REGIONS
-}
-inline
-heap_segment* heap_segment_next_non_sip (heap_segment* seg)
-{
-    heap_segment* ns = heap_segment_next (seg);
-#ifdef USE_REGIONS
-    return heap_segment_non_sip (ns);
-#else
-    return ns;
-#endif //USE_REGIONS
-}
-heap_segment* heap_segment_rw (heap_segment* ns)
-{
-    if ((ns == 0) || !heap_segment_read_only_p (ns))
-    {
-        return ns;
-    }
-    else
-    {
-        do
-        {
-            ns = heap_segment_next (ns);
-        } while ((ns != 0) && heap_segment_read_only_p (ns));
-        return ns;
-    }
-}
-heap_segment* heap_segment_next_rw (heap_segment* seg)
-{
-    heap_segment* ns = heap_segment_next (seg);
-    return heap_segment_rw (ns);
-}
-heap_segment* heap_segment_prev_rw (heap_segment* begin, heap_segment* seg)
-{
-    assert (begin != 0);
-    heap_segment* prev = begin;
-    heap_segment* current = heap_segment_next_rw (begin);
-    while (current && current != seg)
-    {
-        prev = current;
-        current = heap_segment_next_rw (current);
-    }
-    if (current == seg)
-    {
-        return prev;
-    }
-    else
-    {
-        return 0;
-    }
-}
-heap_segment* heap_segment_prev (heap_segment* begin, heap_segment* seg)
-{
-    assert (begin != 0);
-    heap_segment* prev = begin;
-    heap_segment* current = heap_segment_next (begin);
-    while (current && current != seg)
-    {
-        prev = current;
-        current = heap_segment_next (current);
-    }
-    if (current == seg)
-    {
-        return prev;
-    }
-    else
-    {
-        return 0;
-    }
-}
-heap_segment* heap_segment_in_range (heap_segment* ns)
-{
-    if ((ns == 0) || heap_segment_in_range_p (ns))
-    {
-        return ns;
-    }
-    else
-    {
-        do
-        {
-            ns = heap_segment_next (ns);
-        } while ((ns != 0) && !heap_segment_in_range_p (ns));
-        return ns;
-    }
-}
-heap_segment* heap_segment_next_in_range (heap_segment* seg)
-{
-    heap_segment* ns = heap_segment_next (seg);
-    return heap_segment_in_range (ns);
-}
-struct imemory_data
-{
-    uint8_t* memory_base;
-};
-struct numa_reserved_block
-{
-    uint8_t*        memory_base;
-    size_t          block_size;
-    numa_reserved_block() : memory_base(nullptr), block_size(0) { }
-};
-struct initial_memory_details
-{
-    imemory_data *initial_memory;
-    imemory_data *initial_normal_heap; // points into initial_memory_array
-    imemory_data *initial_large_heap;  // points into initial_memory_array
-    imemory_data *initial_pinned_heap; // points into initial_memory_array
-    size_t block_size_normal;
-    size_t block_size_large;
-    size_t block_size_pinned;
-    int block_count;                // # of blocks in each
-    int current_block_normal;
-    int current_block_large;
-    int current_block_pinned;
-    enum
-    {
-        ALLATONCE = 1,
-        EACH_GENERATION,
-        EACH_BLOCK,
-        ALLATONCE_SEPARATED_POH,
-        EACH_NUMA_NODE
-    };
-    size_t allocation_pattern;
-    size_t block_size(int i)
-    {
-        switch (i / block_count)
-        {
-            case 0: return block_size_normal;
-            case 1: return block_size_large;
-            case 2: return block_size_pinned;
-            default: __UNREACHABLE();
-        }
-    };
-    void* get_initial_memory (int gen, int h_number)
-    {
-        switch (gen)
-        {
-            case soh_gen0:
-            case soh_gen1:
-            case soh_gen2: return initial_normal_heap[h_number].memory_base;
-            case loh_generation: return initial_large_heap[h_number].memory_base;
-            case poh_generation: return initial_pinned_heap[h_number].memory_base;
-            default: __UNREACHABLE();
-        }
-    };
-    size_t get_initial_size (int gen)
-    {
-        switch (gen)
-        {
-            case soh_gen0:
-            case soh_gen1:
-            case soh_gen2: return block_size_normal;
-            case loh_generation: return block_size_large;
-            case poh_generation: return block_size_pinned;
-            default: __UNREACHABLE();
-        }
-    };
-    int numa_reserved_block_count;
-    numa_reserved_block* numa_reserved_block_table;
-};
-initial_memory_details memory_details;
-BOOL gc_heap::reserve_initial_memory (size_t normal_size, size_t large_size, size_t pinned_size, int num_heaps, bool use_large_pages_p, bool separated_poh_p, uint16_t* heap_no_to_numa_node)
-{
-    BOOL reserve_success = FALSE;
-    assert (memory_details.initial_memory == 0);
-    memory_details.initial_memory = new (nothrow) imemory_data[num_heaps * (total_generation_count - ephemeral_generation_count)];
-    if (memory_details.initial_memory == 0)
-    {
-        dprintf (2, ("failed to reserve %zd bytes for imemory_data",
-            num_heaps * (total_generation_count - ephemeral_generation_count) * sizeof (imemory_data)));
-        return FALSE;
-    }
-    memory_details.initial_normal_heap = memory_details.initial_memory;
-    memory_details.initial_large_heap = memory_details.initial_normal_heap + num_heaps;
-    memory_details.initial_pinned_heap = memory_details.initial_large_heap + num_heaps;
-    memory_details.block_size_normal = normal_size;
-    memory_details.block_size_large = large_size;
-    memory_details.block_size_pinned = pinned_size;
-    memory_details.block_count = num_heaps;
-    memory_details.current_block_normal = 0;
-    memory_details.current_block_large = 0;
-    memory_details.current_block_pinned = 0;
-    g_gc_lowest_address = MAX_PTR;
-    g_gc_highest_address = 0;
-    if (((size_t)MAX_PTR - large_size) < normal_size)
-    {
-        dprintf (2, ("0x%zx + 0x%zx already overflow", normal_size, large_size));
-        return FALSE;
-    }
-    if (((size_t)MAX_PTR / memory_details.block_count) < (normal_size + large_size + pinned_size))
-    {
-        dprintf (2, ("(0x%zx + 0x%zx)*0x%x overflow", normal_size, large_size, memory_details.block_count));
-        return FALSE;
-    }
-    memory_details.numa_reserved_block_count = 0;
-    memory_details.numa_reserved_block_table = nullptr;
-    int numa_node_count = 0;
-    if (heap_no_to_numa_node != nullptr)
-    {
-        uint16_t highest_numa_node = 0;
-        for (int heap_no = 0; heap_no < num_heaps; heap_no++)
-        {
-            uint16_t heap_numa_node = heap_no_to_numa_node[heap_no];
-            highest_numa_node = max (highest_numa_node, heap_numa_node);
-        }
-        assert (highest_numa_node < MAX_SUPPORTED_CPUS);
-        numa_node_count = highest_numa_node + 1;
-        memory_details.numa_reserved_block_count = numa_node_count * (1 + separated_poh_p);
-        memory_details.numa_reserved_block_table = new (nothrow) numa_reserved_block[memory_details.numa_reserved_block_count];
-        if (memory_details.numa_reserved_block_table == nullptr)
-        {
-            dprintf(2, ("failed to reserve %zd bytes for numa_reserved_block data", memory_details.numa_reserved_block_count * sizeof(numa_reserved_block)));
-            memory_details.numa_reserved_block_count = 0;
-        }
-    }
-    if (memory_details.numa_reserved_block_table != nullptr)
-    {
-        size_t merged_pinned_size = separated_poh_p ? 0 : pinned_size;
-        for (int heap_no = 0; heap_no < num_heaps; heap_no++)
-        {
-            uint16_t heap_numa_node = heap_no_to_numa_node[heap_no];
-            numa_reserved_block * block = &memory_details.numa_reserved_block_table[heap_numa_node];
-            block->block_size += normal_size + large_size + merged_pinned_size;
-            if (separated_poh_p)
-            {
-                numa_reserved_block* pinned_block = &memory_details.numa_reserved_block_table[numa_node_count + heap_numa_node];
-                pinned_block->block_size += pinned_size;
-            }
-        }
-        bool failure = false;
-        for (int block_index = 0; block_index < memory_details.numa_reserved_block_count; block_index++)
-        {
-            numa_reserved_block * block = &memory_details.numa_reserved_block_table[block_index];
-            if (block->block_size == 0)
-                continue;
-            int numa_node = block_index % numa_node_count;
-            bool pinned_block = block_index >= numa_node_count;
-            block->memory_base = (uint8_t*)virtual_alloc (block->block_size, use_large_pages_p && !pinned_block, (uint16_t)numa_node);
-            if (block->memory_base == nullptr)
-            {
-                dprintf(2, ("failed to reserve %zd bytes for on NUMA node %u", block->block_size, numa_node));
-                failure = true;
-                break;
-            }
-            else
-            {
-                g_gc_lowest_address = min(g_gc_lowest_address, block->memory_base);
-                g_gc_highest_address = max(g_gc_highest_address, block->memory_base + block->block_size);
-            }
-        }
-        if (failure)
-        {
-            for (int block_index = 0; block_index < memory_details.numa_reserved_block_count; block_index++)
-            {
-                numa_reserved_block * block = &memory_details.numa_reserved_block_table[block_index];
-                if (block->memory_base != nullptr)
-                {
-                    virtual_free(block->memory_base, block->block_size);
-                    block->memory_base = nullptr;
-                }
-            }
-            delete [] memory_details.numa_reserved_block_table;
-            memory_details.numa_reserved_block_table = nullptr;
-            memory_details.numa_reserved_block_count = 0;
-        }
-        else
-        {
-            for (uint16_t numa_node = 0; numa_node < numa_node_count; numa_node++)
-            {
-                numa_reserved_block * block = &memory_details.numa_reserved_block_table[numa_node];
-                numa_reserved_block* pinned_block = separated_poh_p ?
-                    &memory_details.numa_reserved_block_table[numa_node_count + numa_node] : nullptr;
-                if (block->block_size == 0)
-                {
-                    assert((pinned_block == nullptr) || (pinned_block->block_size == 0));
-                    continue;
-                }
-                uint8_t* memory_base = block->memory_base;
-                uint8_t* pinned_memory_base = ((pinned_block == nullptr) ? nullptr : pinned_block->memory_base);
-                for (int heap_no = 0; heap_no < num_heaps; heap_no++)
-                {
-                    uint16_t heap_numa_node = heap_no_to_numa_node[heap_no];
-                    if (heap_numa_node != numa_node)
-                    {
-                        continue;
-                    }
-                    memory_details.initial_normal_heap[heap_no].memory_base = memory_base;
-                    memory_base += normal_size;
-                    memory_details.initial_large_heap[heap_no].memory_base = memory_base;
-                    memory_base += large_size;
-                    if (separated_poh_p)
-                    {
-                        memory_details.initial_pinned_heap[heap_no].memory_base = pinned_memory_base;
-                        pinned_memory_base += pinned_size;
-                    }
-                    else
-                    {
-                        memory_details.initial_pinned_heap[heap_no].memory_base = memory_base;
-                        memory_base += pinned_size;
-                    }
-                }
-                assert (memory_base == block->memory_base + block->block_size);
-                assert ((pinned_block == nullptr) || (pinned_memory_base == pinned_block->memory_base + pinned_block->block_size));
-            }
-            memory_details.allocation_pattern = initial_memory_details::EACH_NUMA_NODE;
-            reserve_success = TRUE;
-        }
-    }
-    if (!reserve_success)
-    {
-        size_t temp_pinned_size = (separated_poh_p ? 0 : pinned_size);
-        size_t separate_pinned_size = memory_details.block_count * pinned_size;
-        size_t requestedMemory = memory_details.block_count * (normal_size + large_size + temp_pinned_size);
-        uint8_t* allatonce_block = (uint8_t*)virtual_alloc(requestedMemory, use_large_pages_p);
-        uint8_t* separated_poh_block = nullptr;
-        if (allatonce_block && separated_poh_p)
-        {
-            separated_poh_block = (uint8_t*)virtual_alloc(separate_pinned_size, false);
-            if (!separated_poh_block)
-            {
-                virtual_free(allatonce_block, requestedMemory);
-                allatonce_block = nullptr;
-            }
-        }
-        if (allatonce_block)
-        {
-            if (separated_poh_p)
-            {
-                g_gc_lowest_address = min(allatonce_block, separated_poh_block);
-                g_gc_highest_address = max((allatonce_block + requestedMemory),
-                    (separated_poh_block + separate_pinned_size));
-                memory_details.allocation_pattern = initial_memory_details::ALLATONCE_SEPARATED_POH;
-            }
-            else
-            {
-                g_gc_lowest_address = allatonce_block;
-                g_gc_highest_address = allatonce_block + requestedMemory;
-                memory_details.allocation_pattern = initial_memory_details::ALLATONCE;
-            }
-            for (int i = 0; i < memory_details.block_count; i++)
-            {
-                memory_details.initial_normal_heap[i].memory_base = allatonce_block +
-                    (i * normal_size);
-                memory_details.initial_large_heap[i].memory_base = allatonce_block +
-                    (memory_details.block_count * normal_size) + (i * large_size);
-                if (separated_poh_p)
-                {
-                    memory_details.initial_pinned_heap[i].memory_base = separated_poh_block +
-                        (i * pinned_size);
-                }
-                else
-                {
-                    memory_details.initial_pinned_heap[i].memory_base = allatonce_block +
-                        (memory_details.block_count * (normal_size + large_size)) + (i * pinned_size);
-                }
-            }
-            reserve_success = TRUE;
-        }
-        else
-        {
-            uint8_t* b1 = (uint8_t*)virtual_alloc(memory_details.block_count * normal_size, use_large_pages_p);
-            uint8_t* b2 = (uint8_t*)virtual_alloc(memory_details.block_count * large_size, use_large_pages_p);
-            uint8_t* b3 = (uint8_t*)virtual_alloc(memory_details.block_count * pinned_size, use_large_pages_p && !separated_poh_p);
-            if (b1 && b2 && b3)
-            {
-                memory_details.allocation_pattern = initial_memory_details::EACH_GENERATION;
-                g_gc_lowest_address = min(b1, min(b2, b3));
-                g_gc_highest_address = max(b1 + memory_details.block_count * normal_size,
-                    max(b2 + memory_details.block_count * large_size,
-                        b3 + memory_details.block_count * pinned_size));
-                for (int i = 0; i < memory_details.block_count; i++)
-                {
-                    memory_details.initial_normal_heap[i].memory_base = b1 + (i * normal_size);
-                    memory_details.initial_large_heap[i].memory_base = b2 + (i * large_size);
-                    memory_details.initial_pinned_heap[i].memory_base = b3 + (i * pinned_size);
-                }
-                reserve_success = TRUE;
-            }
-            else
-            {
-                if (b1)
-                    virtual_free(b1, memory_details.block_count * normal_size);
-                if (b2)
-                    virtual_free(b2, memory_details.block_count * large_size);
-                if (b3)
-                    virtual_free(b3, memory_details.block_count * pinned_size);
-            }
-            if ((b2 == NULL) && (memory_details.block_count > 1))
-            {
-                memory_details.allocation_pattern = initial_memory_details::EACH_BLOCK;
-                imemory_data* current_block = memory_details.initial_memory;
-                for (int i = 0; i < (memory_details.block_count * (total_generation_count - ephemeral_generation_count)); i++, current_block++)
-                {
-                    size_t block_size = memory_details.block_size(i);
-                    uint16_t numa_node = NUMA_NODE_UNDEFINED;
-                    if (heap_no_to_numa_node != nullptr)
-                    {
-                        int heap_no = i % memory_details.block_count;
-                        numa_node = heap_no_to_numa_node[heap_no];
-                    }
-                    current_block->memory_base =
-                        (uint8_t*)virtual_alloc(block_size, use_large_pages_p, numa_node);
-                    if (current_block->memory_base == 0)
-                    {
-                        current_block = memory_details.initial_memory;
-                        for (int j = 0; j < i; j++, current_block++) {
-                            if (current_block->memory_base != 0) {
-                                block_size = memory_details.block_size(i);
-                                virtual_free(current_block->memory_base, block_size);
-                            }
-                        }
-                        reserve_success = FALSE;
-                        break;
-                    }
-                    else
-                    {
-                        if (current_block->memory_base < g_gc_lowest_address)
-                            g_gc_lowest_address = current_block->memory_base;
-                        if (((uint8_t*)current_block->memory_base + block_size) > g_gc_highest_address)
-                            g_gc_highest_address = (current_block->memory_base + block_size);
-                    }
-                    reserve_success = TRUE;
-                }
-            }
-        }
-    }
-    if (reserve_success && separated_poh_p)
-    {
-        for (int heap_no = 0; (reserve_success && (heap_no < num_heaps)); heap_no++)
-        {
-            if (!GCToOSInterface::VirtualCommit(memory_details.initial_pinned_heap[heap_no].memory_base, pinned_size))
-            {
-                reserve_success = FALSE;
-            }
-        }
-    }
-    return reserve_success;
-}
-void gc_heap::destroy_initial_memory()
-{
-    if (memory_details.initial_memory != NULL)
-    {
-        switch (memory_details.allocation_pattern)
-        {
-            case initial_memory_details::ALLATONCE:
-                virtual_free (memory_details.initial_memory[0].memory_base,
-                    memory_details.block_count*(memory_details.block_size_normal +
-                    memory_details.block_size_large + memory_details.block_size_pinned));
-                break;
-            case initial_memory_details::ALLATONCE_SEPARATED_POH:
-                virtual_free(memory_details.initial_memory[0].memory_base,
-                    memory_details.block_count * (memory_details.block_size_normal +
-                        memory_details.block_size_large));
-                virtual_free(memory_details.initial_pinned_heap[0].memory_base,
-                    memory_details.block_count * (memory_details.block_size_pinned));
-                break;
-            case initial_memory_details::EACH_GENERATION:
-                virtual_free (memory_details.initial_normal_heap[0].memory_base,
-                    memory_details.block_count*memory_details.block_size_normal);
-                virtual_free (memory_details.initial_large_heap[0].memory_base,
-                    memory_details.block_count*memory_details.block_size_large);
-                virtual_free (memory_details.initial_pinned_heap[0].memory_base,
-                    memory_details.block_count*memory_details.block_size_pinned);
-                break;
-            case initial_memory_details::EACH_BLOCK:
-            {
-                imemory_data* current_block = memory_details.initial_memory;
-                int total_block_count = memory_details.block_count *
-                    (total_generation_count - ephemeral_generation_count);
-                for (int i = 0; i < total_block_count; i++, current_block++)
-                {
-                    size_t block_size = memory_details.block_size (i);
-                    if (current_block->memory_base != NULL)
-                    {
-                        virtual_free (current_block->memory_base, block_size);
-                    }
-                }
-                break;
-            }
-            case initial_memory_details::EACH_NUMA_NODE:
-                for (int block_index = 0; block_index < memory_details.numa_reserved_block_count; block_index++)
-                {
-                    numa_reserved_block * block = &memory_details.numa_reserved_block_table[block_index];
-                    if (block->memory_base != nullptr)
-                    {
-                        virtual_free (block->memory_base, block->block_size);
-                    }
-                }
-                delete [] memory_details.numa_reserved_block_table;
-                break;
-            default:
-                assert (!"unexpected allocation_pattern");
-                break;
-        }
-        delete [] memory_details.initial_memory;
-        memory_details.initial_memory = NULL;
-        memory_details.initial_normal_heap = NULL;
-        memory_details.initial_large_heap = NULL;
-        memory_details.initial_pinned_heap = NULL;
-    }
-}
-heap_segment* make_initial_segment (int gen, int h_number, gc_heap* hp)
-{
-    void* mem = memory_details.get_initial_memory (gen, h_number);
-    size_t size = memory_details.get_initial_size (gen);
-    heap_segment* res = gc_heap::make_heap_segment ((uint8_t*)mem, size, hp, gen);
-    return res;
-}
-void* virtual_alloc (size_t size)
-{
-    return virtual_alloc(size, false);
-}
-void* virtual_alloc (size_t size, bool use_large_pages_p, uint16_t numa_node)
-{
-    size_t requested_size = size;
-    if ((gc_heap::reserved_memory_limit - gc_heap::reserved_memory) < requested_size)
-    {
-        gc_heap::reserved_memory_limit =
-            GCScan::AskForMoreReservedMemory (gc_heap::reserved_memory_limit, requested_size);
-        if ((gc_heap::reserved_memory_limit - gc_heap::reserved_memory) < requested_size)
-        {
-            return 0;
-        }
-    }
-    uint32_t flags = VirtualReserveFlags::None;
-#ifndef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    if (virtual_alloc_hardware_write_watch)
-    {
-        flags = VirtualReserveFlags::WriteWatch;
-    }
-#endif // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    void* prgmem = use_large_pages_p ?
-        GCToOSInterface::VirtualReserveAndCommitLargePages(requested_size, numa_node) :
-        GCToOSInterface::VirtualReserve(requested_size, card_size * card_word_width, flags, numa_node);
-    void *aligned_mem = prgmem;
-    if (prgmem)
-    {
-        uint8_t* end_mem = (uint8_t*)prgmem + requested_size;
-        if ((end_mem == 0) || ((size_t)(MAX_PTR - end_mem) <= END_SPACE_AFTER_GC))
-        {
-            GCToOSInterface::VirtualRelease (prgmem, requested_size);
-            dprintf (2, ("Virtual Alloc size %zd returned memory right against 4GB [%zx, %zx[ - discarding",
-                        requested_size, (size_t)prgmem, (size_t)((uint8_t*)prgmem+requested_size)));
-            prgmem = 0;
-            aligned_mem = 0;
-        }
-    }
-    if (prgmem)
-    {
-        gc_heap::reserved_memory += requested_size;
-    }
-    dprintf (2, ("Virtual Alloc size %zd: [%zx, %zx[",
-                 requested_size, (size_t)prgmem, (size_t)((uint8_t*)prgmem+requested_size)));
-    return aligned_mem;
-}
-static size_t get_valid_segment_size (BOOL large_seg=FALSE)
-{
-    size_t seg_size, initial_seg_size;
-    if (!large_seg)
-    {
-        initial_seg_size = INITIAL_ALLOC;
-        seg_size = static_cast<size_t>(GCConfig::GetSegmentSize());
-    }
-    else
-    {
-        initial_seg_size = LHEAP_ALLOC;
-        seg_size = static_cast<size_t>(GCConfig::GetSegmentSize()) / 2;
-    }
-#ifdef MULTIPLE_HEAPS
-#ifdef HOST_64BIT
-    if (!large_seg)
-#endif // HOST_64BIT
-    {
-        if (g_num_processors > 4)
-            initial_seg_size /= 2;
-        if (g_num_processors > 8)
-            initial_seg_size /= 2;
-    }
-#endif //MULTIPLE_HEAPS
-    if (!g_theGCHeap->IsValidSegmentSize(seg_size))
-    {
-        if ((seg_size >> 1) && !(seg_size >> 22))
-            seg_size = 1024*1024*4;
-        else
-            seg_size = initial_seg_size;
-    }
-#ifdef HOST_64BIT
-    seg_size = round_up_power2 (seg_size);
-#else
-    seg_size = round_down_power2 (seg_size);
-#endif // HOST_64BIT
-    return (seg_size);
-}
-#ifndef USE_REGIONS
-void
-gc_heap::compute_new_ephemeral_size()
-{
-    int eph_gen_max = max_generation - 1 - (settings.promotion ? 1 : 0);
-    size_t padding_size = 0;
-    for (int i = 0; i <= eph_gen_max; i++)
-    {
-        dynamic_data* dd = dynamic_data_of (i);
-        total_ephemeral_size += (dd_survived_size (dd) - dd_pinned_survived_size (dd));
-#ifdef RESPECT_LARGE_ALIGNMENT
-        total_ephemeral_size += dd_num_npinned_plugs (dd) * switch_alignment_size (FALSE);
-#endif //RESPECT_LARGE_ALIGNMENT
-#ifdef FEATURE_STRUCTALIGN
-        total_ephemeral_size += dd_num_npinned_plugs (dd) * MAX_STRUCTALIGN;
-#endif //FEATURE_STRUCTALIGN
-#ifdef SHORT_PLUGS
-        padding_size += dd_padding_size (dd);
-#endif //SHORT_PLUGS
-    }
-    total_ephemeral_size += eph_gen_starts_size;
-#ifdef RESPECT_LARGE_ALIGNMENT
-    size_t planned_ephemeral_size = heap_segment_plan_allocated (ephemeral_heap_segment) -
-                                       generation_plan_allocation_start (generation_of (max_generation-1));
-    total_ephemeral_size = min (total_ephemeral_size, planned_ephemeral_size);
-#endif //RESPECT_LARGE_ALIGNMENT
-#ifdef SHORT_PLUGS
-    total_ephemeral_size = Align ((size_t)((double)total_ephemeral_size * short_plugs_pad_ratio) + 1);
-    total_ephemeral_size += Align (DESIRED_PLUG_LENGTH);
-#endif //SHORT_PLUGS
-    dprintf (3, ("total ephemeral size is %zx, padding %zx(%zx)",
-        total_ephemeral_size,
-        padding_size, (total_ephemeral_size - padding_size)));
-}
-heap_segment*
-gc_heap::soh_get_segment_to_expand()
-{
-    size_t size = soh_segment_size;
-    ordered_plug_indices_init = FALSE;
-    use_bestfit = FALSE;
-    compute_new_ephemeral_size();
-    if ((settings.pause_mode != pause_low_latency) &&
-        (settings.pause_mode != pause_no_gc)
-#ifdef BACKGROUND_GC
-        && (!gc_heap::background_running_p())
-#endif //BACKGROUND_GC
-        )
-    {
-        assert (settings.condemned_generation <= max_generation);
-        allocator*  gen_alloc = ((settings.condemned_generation == max_generation) ? nullptr :
-                              generation_allocator (generation_of (max_generation)));
-        dprintf (2, ("(gen%d)soh_get_segment_to_expand", settings.condemned_generation));
-        heap_segment* fseg = heap_segment_rw (generation_start_segment (generation_of (max_generation)));
-        PREFIX_ASSUME(fseg != NULL);
-#ifdef SEG_REUSE_STATS
-        int try_reuse = 0;
-#endif //SEG_REUSE_STATS
-        heap_segment* seg = ephemeral_heap_segment;
-        while ((seg = heap_segment_prev_rw (fseg, seg)) && (seg != fseg))
-        {
-#ifdef SEG_REUSE_STATS
-        try_reuse++;
-#endif //SEG_REUSE_STATS
-            if (can_expand_into_p (seg, size/3, total_ephemeral_size, gen_alloc))
-            {
-                get_gc_data_per_heap()->set_mechanism (gc_heap_expand,
-                    (use_bestfit ? expand_reuse_bestfit : expand_reuse_normal));
-                if (settings.condemned_generation == max_generation)
-                {
-                    if (use_bestfit)
-                    {
-                        build_ordered_free_spaces (seg);
-                        dprintf (GTC_LOG, ("can use best fit"));
-                    }
-#ifdef SEG_REUSE_STATS
-                    dprintf (SEG_REUSE_LOG_0, ("(gen%d)soh_get_segment_to_expand: found seg #%d to reuse",
-                        settings.condemned_generation, try_reuse));
-#endif //SEG_REUSE_STATS
-                    dprintf (GTC_LOG, ("max_gen: Found existing segment to expand into %zx", (size_t)seg));
-                    return seg;
-                }
-                else
-                {
-#ifdef SEG_REUSE_STATS
-                    dprintf (SEG_REUSE_LOG_0, ("(gen%d)soh_get_segment_to_expand: found seg #%d to reuse - returning",
-                        settings.condemned_generation, try_reuse));
-#endif //SEG_REUSE_STATS
-                    dprintf (GTC_LOG, ("max_gen-1: Found existing segment to expand into %zx", (size_t)seg));
-                    if (settings.pause_mode != pause_sustained_low_latency)
-                    {
-                        dprintf (GTC_LOG, ("max_gen-1: SustainedLowLatency is set, acquire a new seg"));
-                        get_gc_data_per_heap()->set_mechanism (gc_heap_expand, expand_next_full_gc);
-                        return 0;
-                    }
-                }
-            }
-        }
-    }
-    heap_segment* result = get_segment (size, gc_oh_num::soh);
-    if(result)
-    {
-#ifdef BACKGROUND_GC
-        if (current_c_gc_state == c_gc_state_planning)
-        {
-            result->flags |= heap_segment_flags_swept;
-        }
-#endif //BACKGROUND_GC
-        FIRE_EVENT(GCCreateSegment_V1, heap_segment_mem(result),
-                                  (size_t)(heap_segment_reserved (result) - heap_segment_mem(result)),
-                                  gc_etw_segment_small_object_heap);
-    }
-    get_gc_data_per_heap()->set_mechanism (gc_heap_expand, (result ? expand_new_seg : expand_no_memory));
-    if (result == 0)
-    {
-        dprintf (2, ("h%d: failed to allocate a new segment!", heap_number));
-    }
-    else
-    {
-#ifdef MULTIPLE_HEAPS
-        heap_segment_heap (result) = this;
-#endif //MULTIPLE_HEAPS
-    }
-    dprintf (GTC_LOG, ("(gen%d)creating new segment %p", settings.condemned_generation, result));
-    return result;
-}
-heap_segment*
-gc_heap::get_segment (size_t size, gc_oh_num oh)
-{
-    assert(oh != gc_oh_num::unknown);
-    BOOL uoh_p = (oh == gc_oh_num::loh) || (oh == gc_oh_num::poh);
-    if (heap_hard_limit)
-        return NULL;
-    heap_segment* result = 0;
-    if (segment_standby_list != 0)
-    {
-        result = segment_standby_list;
-        heap_segment* last = 0;
-        while (result)
-        {
-            size_t hs = (size_t)(heap_segment_reserved (result) - (uint8_t*)result);
-            if ((hs >= size) && ((hs / 2) < size))
-            {
-                dprintf (2, ("Hoarded segment %zx found", (size_t) result));
-                if (last)
-                {
-                    heap_segment_next (last) = heap_segment_next (result);
-                }
-                else
-                {
-                    segment_standby_list = heap_segment_next (result);
-                }
-                break;
-            }
-            else
-            {
-                last = result;
-                result = heap_segment_next (result);
-            }
-        }
-    }
-    if (result)
-    {
-        init_heap_segment (result, __this);
-#ifdef BACKGROUND_GC
-        if (is_bgc_in_progress())
-        {
-            dprintf (GC_TABLE_LOG, ("hoarded seg %p, mark_array is %p", result, mark_array));
-            if (!commit_mark_array_new_seg (__this, result))
-            {
-                dprintf (GC_TABLE_LOG, ("failed to commit mark array for hoarded seg"));
-                if (segment_standby_list != 0)
-                {
-                    heap_segment_next (result) = segment_standby_list;
-                    segment_standby_list = result;
-                }
-                else
-                {
-                    segment_standby_list = result;
-                }
-                result = 0;
-            }
-        }
-#endif //BACKGROUND_GC
-        if (result)
-            seg_mapping_table_add_segment (result, __this);
-    }
-    if (!result)
-    {
-        void* mem = virtual_alloc (size);
-        if (!mem)
-        {
-            fgm_result.set_fgm (fgm_reserve_segment, size, uoh_p);
-            return 0;
-        }
-        result = make_heap_segment ((uint8_t*)mem, size, __this, (uoh_p ? max_generation : 0));
-        if (result)
-        {
-            uint8_t* start;
-            uint8_t* end;
-            if (mem < g_gc_lowest_address)
-            {
-                start =  (uint8_t*)mem;
-            }
-            else
-            {
-                start = (uint8_t*)g_gc_lowest_address;
-            }
-            if (((uint8_t*)mem + size) > g_gc_highest_address)
-            {
-                end = (uint8_t*)mem + size;
-            }
-            else
-            {
-                end = (uint8_t*)g_gc_highest_address;
-            }
-            if (gc_heap::grow_brick_card_tables (start, end, size, result, __this, uoh_p) != 0)
-            {
-                virtual_free (mem, size);
-                return 0;
-            }
-        }
-        else
-        {
-            fgm_result.set_fgm (fgm_commit_segment_beg, SEGMENT_INITIAL_COMMIT, uoh_p);
-            virtual_free (mem, size);
-        }
-        if (result)
-        {
-            seg_mapping_table_add_segment (result, __this);
-        }
-    }
-#ifdef BACKGROUND_GC
-    if (result)
-    {
-        ::record_changed_seg ((uint8_t*)result, heap_segment_reserved (result),
-                            settings.gc_index, current_bgc_state,
-                            seg_added);
-        bgc_verify_mark_array_cleared (result);
-    }
-#endif //BACKGROUND_GC
-    dprintf (GC_TABLE_LOG, ("h%d: new seg: %p-%p (%zd)", heap_number, result, ((uint8_t*)result + size), size));
-    return result;
-}
-void gc_heap::release_segment (heap_segment* sg)
-{
-    ptrdiff_t delta = 0;
-    FIRE_EVENT(GCFreeSegment_V1, heap_segment_mem(sg));
-    virtual_free (sg, (uint8_t*)heap_segment_reserved (sg)-(uint8_t*)sg, sg);
-}
-#endif //!USE_REGIONS
-heap_segment* gc_heap::get_segment_for_uoh (int gen_number, size_t size
-#ifdef MULTIPLE_HEAPS
-                                           , gc_heap* hp
-#endif //MULTIPLE_HEAPS
-                                           )
-{
-#ifndef MULTIPLE_HEAPS
-    gc_heap* hp = 0;
-#endif //MULTIPLE_HEAPS
-#ifdef USE_REGIONS
-    heap_segment* res = hp->get_new_region (gen_number, size);
-#else //USE_REGIONS
-    gc_oh_num oh = gen_to_oh (gen_number);
-    heap_segment* res = hp->get_segment (size, oh);
-#endif //USE_REGIONS
-    if (res != 0)
-    {
-#ifdef MULTIPLE_HEAPS
-        heap_segment_heap (res) = hp;
-#endif //MULTIPLE_HEAPS
-        size_t flags = (gen_number == poh_generation) ?
-            heap_segment_flags_poh :
-            heap_segment_flags_loh;
-#ifdef USE_REGIONS
-        assert ((res->flags & (heap_segment_flags_loh | heap_segment_flags_poh)) == flags);
-#else //USE_REGIONS
-        res->flags |= flags;
-        FIRE_EVENT(GCCreateSegment_V1,
-            heap_segment_mem(res),
-            (size_t)(heap_segment_reserved (res) - heap_segment_mem(res)),
-            (gen_number == poh_generation) ?
-                gc_etw_segment_pinned_object_heap :
-                gc_etw_segment_large_object_heap);
-#ifdef MULTIPLE_HEAPS
-        hp->thread_uoh_segment (gen_number, res);
-#else
-        thread_uoh_segment (gen_number, res);
-#endif //MULTIPLE_HEAPS
-#endif //USE_REGIONS
-        GCToEEInterface::DiagAddNewRegion(
-                            gen_number,
-                            heap_segment_mem (res),
-                            heap_segment_allocated (res),
-                            heap_segment_reserved (res)
-                        );
-    }
-    return res;
-}
-void gc_heap::thread_uoh_segment (int gen_number, heap_segment* new_seg)
-{
-    heap_segment* seg = generation_allocation_segment (generation_of (gen_number));
-    while (heap_segment_next_rw (seg))
-        seg = heap_segment_next_rw (seg);
-    heap_segment_next (seg) = new_seg;
-}
-heap_segment*
-gc_heap::get_uoh_segment (int gen_number, size_t size, BOOL* did_full_compact_gc, enter_msl_status* msl_status)
-{
-    *did_full_compact_gc = FALSE;
-    size_t last_full_compact_gc_count = get_full_compact_gc_count();
-    add_saved_spinlock_info (true, me_release, mt_get_large_seg, msl_entered);
-    leave_spin_lock (&more_space_lock_uoh);
-    enter_spin_lock (&gc_heap::gc_lock);
-    dprintf (SPINLOCK_LOG, ("[%d]Seg: Egc", heap_number));
-    size_t current_full_compact_gc_count = get_full_compact_gc_count();
-    if (current_full_compact_gc_count > last_full_compact_gc_count)
-    {
-        *did_full_compact_gc = TRUE;
-    }
-    if (should_move_heap (&more_space_lock_uoh))
-    {
-        *msl_status = msl_retry_different_heap;
-        leave_spin_lock (&gc_heap::gc_lock);
-        return NULL;
-    }
-    heap_segment* res = get_segment_for_uoh (gen_number, size
-#ifdef MULTIPLE_HEAPS
-                                            , this
-#endif //MULTIPLE_HEAPS
-                                            );
-    dprintf (SPINLOCK_LOG, ("[%d]Seg: A Lgc", heap_number));
-    leave_spin_lock (&gc_heap::gc_lock);
-    *msl_status = enter_spin_lock_msl (&more_space_lock_uoh);
-    if (*msl_status == msl_retry_different_heap)
-        return NULL;
-    add_saved_spinlock_info (true, me_acquire, mt_get_large_seg, *msl_status);
-    return res;
-}
-#ifdef MULTIPLE_HEAPS
-#ifdef HOST_X86
-#ifdef _MSC_VER
-#pragma warning(disable:4035)
-    static ptrdiff_t  get_cycle_count()
-    {
-        __asm   rdtsc
-    }
-#pragma warning(default:4035)
-#elif defined(__GNUC__)
-    static ptrdiff_t  get_cycle_count()
-    {
-        ptrdiff_t cycles;
-        ptrdiff_t cyclesHi;
-        __asm__ __volatile__
-        ("rdtsc":"=a" (cycles), "=d" (cyclesHi));
-        return cycles;
-    }
-#else //_MSC_VER
-#error Unknown compiler
-#endif //_MSC_VER
-#elif defined(TARGET_AMD64)
-#ifdef _MSC_VER
-extern "C" uint64_t __rdtsc();
-#pragma intrinsic(__rdtsc)
-    static ptrdiff_t get_cycle_count()
-    {
-        return (ptrdiff_t)__rdtsc();
-    }
-#elif defined(__GNUC__)
-    static ptrdiff_t get_cycle_count()
-    {
-        ptrdiff_t cycles;
-        ptrdiff_t cyclesHi;
-        __asm__ __volatile__
-        ("rdtsc":"=a" (cycles), "=d" (cyclesHi));
-        return (cyclesHi << 32) | cycles;
-    }
-#else // _MSC_VER
-    extern "C" ptrdiff_t get_cycle_count(void);
-#endif // _MSC_VER
-#elif defined(TARGET_LOONGARCH64)
-    static ptrdiff_t get_cycle_count()
-    {
-        __asm__ volatile ("break 0 \n");
-        return 0;
-    }
-#else
-    static ptrdiff_t get_cycle_count()
-    {
-        return 0;
-    }
-#endif //TARGET_X86
-struct node_heap_count
-{
-    int node_no;
-    int heap_count;
-};
-class heap_select
-{
-    heap_select() {}
-public:
-    static uint8_t* sniff_buffer;
-    static unsigned n_sniff_buffers;
-    static unsigned cur_sniff_index;
-    static uint16_t proc_no_to_heap_no[MAX_SUPPORTED_CPUS];
-    static uint16_t heap_no_to_proc_no[MAX_SUPPORTED_CPUS];
-    static uint16_t heap_no_to_numa_node[MAX_SUPPORTED_CPUS];
-    static uint16_t proc_no_to_numa_node[MAX_SUPPORTED_CPUS];
-    static uint16_t numa_node_to_heap_map[MAX_SUPPORTED_CPUS+4];
-    static uint16_t total_numa_nodes;
-    static node_heap_count heaps_on_node[MAX_SUPPORTED_NODES];
-    static int access_time(uint8_t *sniff_buffer, int heap_number, unsigned sniff_index, unsigned n_sniff_buffers)
-    {
-        ptrdiff_t start_cycles = get_cycle_count();
-        uint8_t sniff = sniff_buffer[(1 + heap_number*n_sniff_buffers + sniff_index)*HS_CACHE_LINE_SIZE];
-        assert (sniff == 0);
-        ptrdiff_t elapsed_cycles = get_cycle_count() - start_cycles;
-        elapsed_cycles += sniff;
-        return (int) elapsed_cycles;
-    }
-public:
-    static BOOL init(int n_heaps)
-    {
-        assert (sniff_buffer == NULL && n_sniff_buffers == 0);
-        if (!GCToOSInterface::CanGetCurrentProcessorNumber())
-        {
-            n_sniff_buffers = n_heaps*2+1;
-            size_t n_cache_lines = 1 + n_heaps * n_sniff_buffers + 1;
-            size_t sniff_buf_size = n_cache_lines * HS_CACHE_LINE_SIZE;
-            if (sniff_buf_size / HS_CACHE_LINE_SIZE != n_cache_lines) // check for overlow
-            {
-                return FALSE;
-            }
-            sniff_buffer = new (nothrow) uint8_t[sniff_buf_size];
-            if (sniff_buffer == 0)
-                return FALSE;
-            memset(sniff_buffer, 0, sniff_buf_size*sizeof(uint8_t));
-        }
-        bool do_numa = GCToOSInterface::CanEnableGCNumaAware();
-        uint16_t proc_no[MAX_SUPPORTED_CPUS];
-        uint16_t node_no[MAX_SUPPORTED_CPUS];
-        uint16_t max_node_no = 0;
-        uint16_t heap_num;
-        for (heap_num = 0; heap_num < n_heaps; heap_num++)
-        {
-            if (!GCToOSInterface::GetProcessorForHeap (heap_num, &proc_no[heap_num], &node_no[heap_num]))
-                break;
-            assert(proc_no[heap_num] < MAX_SUPPORTED_CPUS);
-            if (!do_numa || node_no[heap_num] == NUMA_NODE_UNDEFINED)
-                node_no[heap_num] = 0;
-            max_node_no = max(max_node_no, node_no[heap_num]);
-        }
-        int cur_heap_no = 0;
-        for (uint16_t cur_node_no = 0; cur_node_no <= max_node_no; cur_node_no++)
-        {
-            for (int i = 0; i < heap_num; i++)
-            {
-                if (node_no[i] != cur_node_no)
-                    continue;
-                heap_no_to_proc_no[cur_heap_no] = proc_no[i];
-                heap_no_to_numa_node[cur_heap_no] = cur_node_no;
-                proc_no_to_numa_node[proc_no[i]] = cur_node_no;
-                cur_heap_no++;
-            }
-        }
-        return TRUE;
-    }
-    static void init_cpu_mapping(int heap_number)
-    {
-        if (GCToOSInterface::CanGetCurrentProcessorNumber())
-        {
-            uint32_t proc_no = GCToOSInterface::GetCurrentProcessorNumber();
-            proc_no_to_heap_no[proc_no % MAX_SUPPORTED_CPUS] = (uint16_t)heap_number;
-        }
-    }
-    static void mark_heap(int heap_number)
-    {
-        if (GCToOSInterface::CanGetCurrentProcessorNumber())
-            return;
-        for (unsigned sniff_index = 0; sniff_index < n_sniff_buffers; sniff_index++)
-            sniff_buffer[(1 + heap_number*n_sniff_buffers + sniff_index)*HS_CACHE_LINE_SIZE] &= 1;
-    }
-    static int select_heap(alloc_context* acontext)
-    {
-#ifndef TRACE_GC
-        UNREFERENCED_PARAMETER(acontext); // only referenced by dprintf
-#endif //TRACE_GC
-        if (GCToOSInterface::CanGetCurrentProcessorNumber())
-        {
-            uint32_t proc_no = GCToOSInterface::GetCurrentProcessorNumber();
-            int adjusted_heap = proc_no_to_heap_no[proc_no % MAX_SUPPORTED_CPUS];
-            if (adjusted_heap >= gc_heap::n_heaps)
-            {
-                adjusted_heap %= gc_heap::n_heaps;
-            }
-            return adjusted_heap;
-        }
-        unsigned sniff_index = Interlocked::Increment(&cur_sniff_index);
-        sniff_index %= n_sniff_buffers;
-        int best_heap = 0;
-        int best_access_time = 1000*1000*1000;
-        int second_best_access_time = best_access_time;
-        uint8_t *l_sniff_buffer = sniff_buffer;
-        unsigned l_n_sniff_buffers = n_sniff_buffers;
-        for (int heap_number = 0; heap_number < gc_heap::n_heaps; heap_number++)
-        {
-            int this_access_time = access_time(l_sniff_buffer, heap_number, sniff_index, l_n_sniff_buffers);
-            if (this_access_time < best_access_time)
-            {
-                second_best_access_time = best_access_time;
-                best_access_time = this_access_time;
-                best_heap = heap_number;
-            }
-            else if (this_access_time < second_best_access_time)
-            {
-                second_best_access_time = this_access_time;
-            }
-        }
-        if (best_access_time*2 < second_best_access_time)
-        {
-            sniff_buffer[(1 + best_heap*n_sniff_buffers + sniff_index)*HS_CACHE_LINE_SIZE] &= 1;
-            dprintf (3, ("select_heap yields crisp %d for context %p\n", best_heap, (void *)acontext));
-        }
-        else
-        {
-            dprintf (3, ("select_heap yields vague %d for context %p\n", best_heap, (void *)acontext ));
-        }
-        return best_heap;
-    }
-    static bool can_find_heap_fast()
-    {
-        return GCToOSInterface::CanGetCurrentProcessorNumber();
-    }
-    static uint16_t find_heap_no_from_proc_no(uint16_t proc_no)
-    {
-        return proc_no_to_heap_no[proc_no];
-    }
-    static uint16_t find_proc_no_from_heap_no(int heap_number)
-    {
-        return heap_no_to_proc_no[heap_number];
-    }
-    static void set_proc_no_for_heap(int heap_number, uint16_t proc_no)
-    {
-        heap_no_to_proc_no[heap_number] = proc_no;
-    }
-    static uint16_t find_numa_node_from_heap_no(int heap_number)
-    {
-        return heap_no_to_numa_node[heap_number];
-    }
-    static uint16_t find_numa_node_from_proc_no (uint16_t proc_no)
-    {
-        return proc_no_to_numa_node[proc_no];
-    }
-    static void set_numa_node_for_heap_and_proc(int heap_number, uint16_t proc_no, uint16_t numa_node)
-    {
-        heap_no_to_numa_node[heap_number] = numa_node;
-        proc_no_to_numa_node[proc_no] = numa_node;
-    }
-    static void init_numa_node_to_heap_map(int nheaps)
-    {
-        numa_node_to_heap_map[heap_no_to_numa_node[0]] = 0;
-        total_numa_nodes = 0;
-        memset (heaps_on_node, 0, sizeof (heaps_on_node));
-        heaps_on_node[0].node_no = heap_no_to_numa_node[0];
-        heaps_on_node[0].heap_count = 1;
-        for (int i=1; i < nheaps; i++)
-        {
-            if (heap_no_to_numa_node[i] != heap_no_to_numa_node[i-1])
-            {
-                total_numa_nodes++;
-                heaps_on_node[total_numa_nodes].node_no = heap_no_to_numa_node[i];
-                numa_node_to_heap_map[heap_no_to_numa_node[i-1] + 1] =
-                numa_node_to_heap_map[heap_no_to_numa_node[i]] = (uint16_t)i;
-            }
-            (heaps_on_node[total_numa_nodes].heap_count)++;
-        }
-        numa_node_to_heap_map[heap_no_to_numa_node[nheaps-1] + 1] = (uint16_t)nheaps; //mark the end with nheaps
-        total_numa_nodes++;
-    }
-    static void distribute_other_procs()
-    {
-        if (affinity_config_specified_p)
-            return;
-        uint16_t proc_no = 0;
-        uint16_t node_no = 0;
-        bool res = false;
-        int start_heap = -1;
-        int end_heap = -1;
-        int current_node_no = -1;
-        int current_heap_on_node = -1;
-        for (int i = gc_heap::n_heaps; i < (int)g_num_active_processors; i++)
-        {
-            if (!GCToOSInterface::GetProcessorForHeap ((uint16_t)i, &proc_no, &node_no))
-                break;
-            if (node_no == NUMA_NODE_UNDEFINED)
-                node_no = 0;
-            int start_heap = (int)numa_node_to_heap_map[node_no];
-            int end_heap = (int)(numa_node_to_heap_map[node_no + 1]);
-            if ((end_heap - start_heap) > 0)
-            {
-                if (node_no == current_node_no)
-                {
-                    if (current_heap_on_node >= end_heap)
-                    {
-                        continue;
-                    }
-                }
-                else
-                {
-                    current_node_no = node_no;
-                    current_heap_on_node = start_heap;
-                }
-                proc_no_to_heap_no[proc_no] = (uint16_t)current_heap_on_node;
-                proc_no_to_numa_node[proc_no] = (uint16_t)node_no;
-                current_heap_on_node++;
-            }
-        }
-    }
-    static void get_heap_range_for_heap(int hn, int* start, int* end)
-    {
-        uint16_t numa_node = heap_no_to_numa_node[hn];
-        *start = (int)numa_node_to_heap_map[numa_node];
-        *end   = (int)(numa_node_to_heap_map[numa_node+1]);
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-        dprintf(HEAP_BALANCE_TEMP_LOG, ("TEMPget_heap_range: %d is in numa node %d, start = %d, end = %d", hn, numa_node, *start, *end));
-#endif //HEAP_BALANCE_INSTRUMENTATION
-    }
-    static uint16_t get_next_numa_node (uint16_t current_index, int* start, int* end)
-    {
-        int start_index = current_index + 1;
-        int nheaps = gc_heap::n_heaps;
-        bool found_node_with_heaps_p = false;
-        do
-        {
-            int start_heap = (int)numa_node_to_heap_map[start_index];
-            int end_heap = (int)numa_node_to_heap_map[start_index + 1];
-            if (start_heap == nheaps)
-            {
-                start_index = 0;
-                continue;
-            }
-            if ((end_heap - start_heap) == 0)
-            {
-                start_index++;
-            }
-            else
-            {
-                found_node_with_heaps_p = true;
-                *start = start_heap;
-                *end = end_heap;
-            }
-        } while (!found_node_with_heaps_p);
-        return (uint16_t)start_index;
-    }
-};
-uint8_t* heap_select::sniff_buffer;
-unsigned heap_select::n_sniff_buffers;
-unsigned heap_select::cur_sniff_index;
-uint16_t heap_select::proc_no_to_heap_no[MAX_SUPPORTED_CPUS];
-uint16_t heap_select::heap_no_to_proc_no[MAX_SUPPORTED_CPUS];
-uint16_t heap_select::heap_no_to_numa_node[MAX_SUPPORTED_CPUS];
-uint16_t heap_select::proc_no_to_numa_node[MAX_SUPPORTED_CPUS];
-uint16_t heap_select::numa_node_to_heap_map[MAX_SUPPORTED_CPUS+4];
-uint16_t  heap_select::total_numa_nodes;
-node_heap_count heap_select::heaps_on_node[MAX_SUPPORTED_NODES];
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-struct heap_balance_info
-{
-    uint64_t timestamp;
-    int tid;
-    int alloc_heap;
-    int ideal_proc_no;
-};
-#define default_max_hb_heap_balance_info 4096
-struct heap_balance_info_proc
-{
-    int count;
-    int index;
-    heap_balance_info hb_info[default_max_hb_heap_balance_info];
-};
-struct heap_balance_info_numa
-{
-    heap_balance_info_proc* hb_info_procs;
-};
-uint64_t start_raw_ts = 0;
-bool cpu_group_enabled_p = false;
-uint32_t procs_per_numa_node = 0;
-uint16_t total_numa_nodes_on_machine = 0;
-uint32_t procs_per_cpu_group = 0;
-uint16_t total_cpu_groups_on_machine = 0;
-heap_balance_info_numa* hb_info_numa_nodes = NULL;
-int get_proc_index_numa (int proc_no, int* numa_no)
-{
-    if (total_numa_nodes_on_machine == 1)
-    {
-        *numa_no = 0;
-        return proc_no;
-    }
-    else
-    {
-        if (cpu_group_enabled_p)
-        {
-            *numa_no = proc_no >> 6;
-            return (proc_no % 64);
-        }
-        else
-        {
-            *numa_no = proc_no / procs_per_numa_node;
-            return (proc_no % procs_per_numa_node);
-        }
-    }
-}
-void add_to_hb_numa (
-    int proc_no,
-    int ideal_proc_no,
-    int alloc_heap,
-    bool multiple_procs_p,
-    bool alloc_count_p,
-    bool set_ideal_p)
-{
-    int tid = (int)GCToOSInterface::GetCurrentThreadIdForLogging ();
-    uint64_t timestamp = RawGetHighPrecisionTimeStamp ();
-    int saved_proc_no = proc_no;
-    int numa_no = -1;
-    proc_no = get_proc_index_numa (proc_no, &numa_no);
-    heap_balance_info_numa* hb_info_numa_node = &hb_info_numa_nodes[numa_no];
-    heap_balance_info_proc* hb_info_proc = &(hb_info_numa_node->hb_info_procs[proc_no]);
-    int index = hb_info_proc->index;
-    int count = hb_info_proc->count;
-    if (index == count)
-    {
-        dprintf (HEAP_BALANCE_LOG, ("too much info between GCs, already logged %d entries", index));
-        GCToOSInterface::DebugBreak ();
-    }
-    heap_balance_info* hb_info = &(hb_info_proc->hb_info[index]);
-    dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMP[p%3d->%3d(i:%3d), N%d] #%4d: %zd, tid %d, ah: %d, m: %d, p: %d, i: %d",
-        saved_proc_no, proc_no, ideal_proc_no, numa_no, index,
-        (timestamp - start_raw_ts) / 1000, tid, alloc_heap, (int)multiple_procs_p, (int)(!alloc_count_p), (int)set_ideal_p));
-    if (multiple_procs_p)
-    {
-        tid |= (1 << (sizeof (tid) * 8 - 1));
-    }
-    if (!alloc_count_p)
-    {
-        alloc_heap |= (1 << (sizeof (alloc_heap) * 8 - 1));
-    }
-    if (set_ideal_p)
-    {
-        alloc_heap |= (1 << (sizeof (alloc_heap) * 8 - 2));
-    }
-    hb_info->timestamp = timestamp;
-    hb_info->tid = tid;
-    hb_info->alloc_heap = alloc_heap;
-    hb_info->ideal_proc_no = ideal_proc_no;
-    (hb_info_proc->index)++;
-}
-const int hb_log_buffer_size = 4096;
-static char hb_log_buffer[hb_log_buffer_size];
-int last_hb_recorded_gc_index = -1;
-#endif //HEAP_BALANCE_INSTRUMENTATION
-void gc_heap::hb_log_balance_activities()
-{
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-    char* log_buffer = hb_log_buffer;
-    uint64_t now = GetHighPrecisionTimeStamp();
-    size_t time_since_last_gc_ms = (size_t)((now - last_gc_end_time_us) / 1000);
-    dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMP%zd - %zd = %zd", now, last_gc_end_time_ms, time_since_last_gc_ms));
-    uint64_t min_timestamp = 0xffffffffffffffff;
-    uint64_t max_timestamp = 0;
-    for (int numa_node_index = 0; numa_node_index < total_numa_nodes_on_machine; numa_node_index++)
-    {
-        heap_balance_info_proc* hb_info_procs = hb_info_numa_nodes[numa_node_index].hb_info_procs;
-        for (int proc_index = 0; proc_index < (int)procs_per_numa_node; proc_index++)
-        {
-            heap_balance_info_proc* hb_info_proc = &hb_info_procs[proc_index];
-            int total_entries_on_proc = hb_info_proc->index;
-            if (total_entries_on_proc > 0)
-            {
-                min_timestamp = min (min_timestamp, hb_info_proc->hb_info[0].timestamp);
-                max_timestamp = max (max_timestamp, hb_info_proc->hb_info[total_entries_on_proc - 1].timestamp);
-            }
-        }
-    }
-    dprintf (HEAP_BALANCE_LOG, ("[GCA#%zd %zd-%zd-%zd]",
-        settings.gc_index, time_since_last_gc_ms, (min_timestamp - start_raw_ts), (max_timestamp - start_raw_ts)));
-    if (last_hb_recorded_gc_index == (int)settings.gc_index)
-    {
-        GCToOSInterface::DebugBreak ();
-    }
-    last_hb_recorded_gc_index = (int)settings.gc_index;
-    for (int numa_node_index = 0; numa_node_index < total_numa_nodes_on_machine; numa_node_index++)
-    {
-        heap_balance_info_proc* hb_info_procs = hb_info_numa_nodes[numa_node_index].hb_info_procs;
-        for (int proc_index = 0; proc_index < (int)procs_per_numa_node; proc_index++)
-        {
-            heap_balance_info_proc* hb_info_proc = &hb_info_procs[proc_index];
-            int total_entries_on_proc = hb_info_proc->index;
-            if (total_entries_on_proc > 0)
-            {
-                int total_exec_time_ms =
-                    (int)((double)(hb_info_proc->hb_info[total_entries_on_proc - 1].timestamp -
-                                   hb_info_proc->hb_info[0].timestamp) * qpf_ms);
-                dprintf (HEAP_BALANCE_LOG, ("[p%d]-%d-%dms",
-                    (proc_index + numa_node_index * procs_per_numa_node),
-                    total_entries_on_proc, total_exec_time_ms));
-            }
-            for (int i = 0; i < hb_info_proc->index; i++)
-            {
-                heap_balance_info* hb_info = &hb_info_proc->hb_info[i];
-                bool multiple_procs_p = false;
-                bool alloc_count_p = true;
-                bool set_ideal_p = false;
-                int tid = hb_info->tid;
-                int alloc_heap = hb_info->alloc_heap;
-                if (tid & (1 << (sizeof (tid) * 8 - 1)))
-                {
-                    multiple_procs_p = true;
-                    tid &= ~(1 << (sizeof (tid) * 8 - 1));
-                }
-                if (alloc_heap & (1 << (sizeof (alloc_heap) * 8 - 1)))
-                {
-                    alloc_count_p = false;
-                    alloc_heap &= ~(1 << (sizeof (alloc_heap) * 8 - 1));
-                }
-                if (alloc_heap & (1 << (sizeof (alloc_heap) * 8 - 2)))
-                {
-                    set_ideal_p = true;
-                    alloc_heap &= ~(1 << (sizeof (alloc_heap) * 8 - 2));
-                }
-                int ideal_proc_no = hb_info->ideal_proc_no;
-                int ideal_node_no = -1;
-                ideal_proc_no = get_proc_index_numa (ideal_proc_no, &ideal_node_no);
-                ideal_proc_no = ideal_proc_no + ideal_node_no * procs_per_numa_node;
-                dprintf (HEAP_BALANCE_LOG, ("%zd,%d,%d,%d%s%s%s",
-                    (hb_info->timestamp - start_raw_ts),
-                    tid,
-                    ideal_proc_no,
-                    (int)alloc_heap,
-                    (multiple_procs_p ? "|m" : ""), (!alloc_count_p ? "|p" : ""), (set_ideal_p ? "|i" : "")));
-            }
-        }
-    }
-    for (int numa_node_index = 0; numa_node_index < total_numa_nodes_on_machine; numa_node_index++)
-    {
-        heap_balance_info_proc* hb_info_procs = hb_info_numa_nodes[numa_node_index].hb_info_procs;
-        for (int proc_index = 0; proc_index < (int)procs_per_numa_node; proc_index++)
-        {
-            heap_balance_info_proc* hb_info_proc = &hb_info_procs[proc_index];
-            hb_info_proc->index = 0;
-        }
-    }
-#endif //HEAP_BALANCE_INSTRUMENTATION
-}
-void gc_heap::hb_log_new_allocation()
-{
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-    char* log_buffer = hb_log_buffer;
-    int desired_alloc_mb = (int)(dd_desired_allocation (g_heaps[0]->dynamic_data_of (0)) / 1024 / 1024);
-    int buffer_pos = sprintf_s (hb_log_buffer, hb_log_buffer_size, "[GC_alloc_mb]\n");
-    for (int numa_node_index = 0; numa_node_index < heap_select::total_numa_nodes; numa_node_index++)
-    {
-        int node_allocated_mb = 0;
-        buffer_pos += sprintf_s (hb_log_buffer + buffer_pos, hb_log_buffer_size - buffer_pos, "[N#%3d]",
-            desired_alloc_mb);
-        int heaps_on_node = heap_select::heaps_on_node[numa_node_index].heap_count;
-        for (int heap_index = 0; heap_index < heaps_on_node; heap_index++)
-        {
-            int actual_heap_index = heap_index + numa_node_index * heaps_on_node;
-            gc_heap* hp = g_heaps[actual_heap_index];
-            dynamic_data* dd0 = hp->dynamic_data_of (0);
-            int allocated_mb = (int)((dd_desired_allocation (dd0) - dd_new_allocation (dd0)) / 1024 / 1024);
-            node_allocated_mb += allocated_mb;
-            buffer_pos += sprintf_s (hb_log_buffer + buffer_pos, hb_log_buffer_size - buffer_pos, "%d,",
-                allocated_mb);
-        }
-        dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMPN#%d a %dmb(%dmb)",
-            numa_node_index, node_allocated_mb, desired_alloc_mb));
-        buffer_pos += sprintf_s (hb_log_buffer + buffer_pos, hb_log_buffer_size - buffer_pos, "\n");
-    }
-    dprintf (HEAP_BALANCE_LOG, ("%s", hb_log_buffer));
-#endif //HEAP_BALANCE_INSTRUMENTATION
-}
-BOOL gc_heap::create_thread_support (int number_of_heaps)
-{
-    BOOL ret = FALSE;
-    if (!gc_start_event.CreateOSManualEventNoThrow (FALSE))
-    {
-        goto cleanup;
-    }
-    if (!ee_suspend_event.CreateOSAutoEventNoThrow (FALSE))
-    {
-        goto cleanup;
-    }
-    if (!gc_t_join.init (number_of_heaps, join_flavor_server_gc))
-    {
-        goto cleanup;
-    }
-    ret = TRUE;
-cleanup:
-    if (!ret)
-    {
-        destroy_thread_support();
-    }
-    return ret;
-}
-void gc_heap::destroy_thread_support ()
-{
-    if (ee_suspend_event.IsValid())
-    {
-        ee_suspend_event.CloseEvent();
-    }
-    if (gc_start_event.IsValid())
-    {
-        gc_start_event.CloseEvent();
-    }
-}
-void set_thread_affinity_for_heap (int heap_number, uint16_t proc_no)
-{
-    if (!GCToOSInterface::SetThreadAffinity (proc_no))
-    {
-        dprintf (1, ("Failed to set thread affinity for GC thread %d on proc #%d", heap_number, proc_no));
-    }
-}
-bool gc_heap::create_gc_thread ()
-{
-    dprintf (3, ("Creating gc thread\n"));
-    return GCToEEInterface::CreateThread(gc_thread_stub, this, false, ".NET Server GC");
-}
-#ifdef _MSC_VER
-#pragma warning(disable:4715) //IA64 xcompiler recognizes that without the 'break;' the while(1) will never end and therefore not return a value for that code path
-#endif //_MSC_VER
-void gc_heap::gc_thread_function ()
-{
-    assert (gc_done_event.IsValid());
-    assert (gc_start_event.IsValid());
-    dprintf (3, ("gc thread started"));
-    heap_select::init_cpu_mapping(heap_number);
-    while (1)
-    {
-        assert ((n_heaps <= heap_number) || !gc_t_join.joined());
-        if (heap_number == 0)
-        {
-            bool wait_on_time_out_p = gradual_decommit_in_progress_p;
-            uint32_t wait_time = DECOMMIT_TIME_STEP_MILLISECONDS;
-#ifdef DYNAMIC_HEAP_COUNT
-            if (!gc_heap::background_running_p () && dynamic_heap_count_data.should_change_heap_count)
-            {
-                assert (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes);
-                dynamic_heap_count_data_t::sample& sample = dynamic_heap_count_data.samples[dynamic_heap_count_data.sample_index];
-                wait_time = min (wait_time, (uint32_t)(sample.elapsed_between_gcs / 1000 / 3));
-                wait_time = max (wait_time, 1);
-                dprintf (6666, ("gc#0 thread waiting for %d ms (betwen GCs %I64d)", wait_time, sample.elapsed_between_gcs));
-            }
-#endif //DYNAMIC_HEAP_COUNT
-            uint32_t wait_result = gc_heap::ee_suspend_event.Wait(wait_on_time_out_p ? wait_time : INFINITE, FALSE);
-            dprintf (9999, ("waiting for ee done res %d (timeout %d, %I64d ms since last suspend end)(should_change_heap_count is %d) (gradual_decommit_in_progress_p %d)",
-                wait_result, wait_time, ((GetHighPrecisionTimeStamp() - last_suspended_end_time) / 1000),
-                dynamic_heap_count_data.should_change_heap_count, gradual_decommit_in_progress_p));
-            if (wait_result == WAIT_TIMEOUT)
-            {
-#ifdef DYNAMIC_HEAP_COUNT
-                if (dynamic_heap_count_data.should_change_heap_count)
-                {
-#ifdef BACKGROUND_GC
-                    if (!gc_heap::background_running_p ())
-#endif //BACKGROUND_GC
-                    {
-                        dprintf (6666, ("changing heap count due to timeout"));
-                        check_heap_count();
-                    }
-                }
-#endif //DYNAMIC_HEAP_COUNT
-                if (gradual_decommit_in_progress_p)
-                {
-                    decommit_lock.Enter ();
-                    gradual_decommit_in_progress_p = decommit_step (DECOMMIT_TIME_STEP_MILLISECONDS);
-                    decommit_lock.Leave ();
-                }
-                continue;
-            }
-#ifdef DYNAMIC_HEAP_COUNT
-            if (dynamic_heap_count_data.should_change_heap_count)
-            {
-#ifdef BACKGROUND_GC
-                if (!gc_heap::background_running_p ())
-#endif //BACKGROUND_GC
-                {
-                    dprintf (6666, ("changing heap count at a GC start"));
-                    check_heap_count ();
-                }
-            }
-            if ((gc_heap::dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes) && 
-                (n_heaps != dynamic_heap_count_data.last_n_heaps))
-            {
-                int spin_count = 1024;
-                int idle_thread_count = n_max_heaps - n_heaps;
-                dprintf (9999, ("heap count changed %d->%d, idle should be %d and is %d", dynamic_heap_count_data.last_n_heaps, n_heaps,
-                    idle_thread_count, VolatileLoadWithoutBarrier (&dynamic_heap_count_data.idle_thread_count)));
-                if (idle_thread_count != dynamic_heap_count_data.idle_thread_count)
-                {
-                    spin_and_wait (spin_count, (idle_thread_count == dynamic_heap_count_data.idle_thread_count));
-                    dprintf (9999, ("heap count changed %d->%d, now idle is %d", dynamic_heap_count_data.last_n_heaps, n_heaps,
-                        VolatileLoadWithoutBarrier (&dynamic_heap_count_data.idle_thread_count)));
-                }
-                dynamic_heap_count_data.last_n_heaps = n_heaps;
-            }
-#endif //DYNAMIC_HEAP_COUNT
-            suspended_start_time = GetHighPrecisionTimeStamp();
-            BEGIN_TIMING(suspend_ee_during_log);
-            dprintf (9999, ("h0 suspending EE in GC!"));
-            GCToEEInterface::SuspendEE(SUSPEND_FOR_GC);
-            dprintf (9999, ("h0 suspended EE in GC!"));
-            END_TIMING(suspend_ee_during_log);
-            proceed_with_gc_p = TRUE;
-            if (!should_proceed_with_gc())
-            {
-                update_collection_counts_for_no_gc();
-                proceed_with_gc_p = FALSE;
-            }
-            else
-            {
-                settings.init_mechanisms();
-#ifdef DYNAMIC_HEAP_COUNT
-                if (gc_heap::dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes)
-                {
-                    assert (dynamic_heap_count_data.new_n_heaps == n_heaps);
-                }
-#endif //DYNAMIC_HEAP_COUNT
-                dprintf (9999, ("GC thread %d setting_gc_start_in_gc(h%d)", heap_number, n_heaps));
-                gc_start_event.Set();
-            }
-            dprintf (3, (ThreadStressLog::gcServerThread0StartMsg(), heap_number));
-        }
-        else
-        {
-            dprintf (9999, ("GC thread %d waiting_for_gc_start(%d)(gc%Id)", heap_number, n_heaps, VolatileLoadWithoutBarrier(&settings.gc_index)));
-            gc_start_event.Wait(INFINITE, FALSE);
-#ifdef DYNAMIC_HEAP_COUNT
-            dprintf (9999, ("GC thread %d waiting_done_gc_start(%d-%d)(i: %d)(gc%Id)",
-                heap_number, n_heaps, dynamic_heap_count_data.new_n_heaps, dynamic_heap_count_data.init_only_p, VolatileLoadWithoutBarrier (&settings.gc_index)));
-            if ((gc_heap::dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes) &&
-                (dynamic_heap_count_data.new_n_heaps != n_heaps))
-            {
-                int old_n_heaps = n_heaps;
-                int new_n_heaps = dynamic_heap_count_data.new_n_heaps;
-                int num_threads_to_wake = max (new_n_heaps, old_n_heaps);
-                if (heap_number < num_threads_to_wake)
-                {
-                    dprintf (9999, ("h%d < %d, calling change", heap_number, num_threads_to_wake));
-                    change_heap_count (dynamic_heap_count_data.new_n_heaps);
-                    if (new_n_heaps < old_n_heaps)
-                    {
-                        dprintf (9999, ("h%d after change", heap_number));
-                        if (heap_number < new_n_heaps)
-                        {
-                            dprintf (9999, ("h%d < %d participating (dec)", heap_number, new_n_heaps));
-                        }
-                        else
-                        {
-                            Interlocked::Increment (&dynamic_heap_count_data.idle_thread_count);
-                            dprintf (9999, ("GC thread %d wait_on_idle(%d < %d)(gc%Id), total idle %d", heap_number, old_n_heaps, new_n_heaps,
-                                VolatileLoadWithoutBarrier (&settings.gc_index), VolatileLoadWithoutBarrier (&dynamic_heap_count_data.idle_thread_count)));
-                            gc_idle_thread_event.Wait (INFINITE, FALSE);
-                            dprintf (9999, ("GC thread %d waking_from_idle(%d)(gc%Id) after doing change", heap_number, n_heaps, VolatileLoadWithoutBarrier (&settings.gc_index)));
-                        }
-                    }
-                    else
-                    {
-                        dprintf (9999, ("h%d < %d participating (inc)", heap_number, new_n_heaps));
-                    }
-                }
-                else
-                {
-                    Interlocked::Increment (&dynamic_heap_count_data.idle_thread_count);
-                    dprintf (9999, ("GC thread %d wait_on_idle(< max %d)(gc%Id), total  idle %d", heap_number, num_threads_to_wake,
-                        VolatileLoadWithoutBarrier (&settings.gc_index), VolatileLoadWithoutBarrier (&dynamic_heap_count_data.idle_thread_count)));
-                    gc_idle_thread_event.Wait (INFINITE, FALSE);
-                    dprintf (9999, ("GC thread %d waking_from_idle(%d)(gc%Id)", heap_number, n_heaps, VolatileLoadWithoutBarrier (&settings.gc_index)));
-                }
-                continue;
-            }
-#endif //DYNAMIC_HEAP_COUNT
-            dprintf (3, (ThreadStressLog::gcServerThreadNStartMsg(), heap_number));
-        }
-        assert ((heap_number == 0) || proceed_with_gc_p);
-        if (proceed_with_gc_p)
-        {
-            garbage_collect (GCHeap::GcCondemnedGeneration);
-            if (pm_trigger_full_gc)
-            {
-                garbage_collect_pm_full_gc();
-            }
-        }
-        if (heap_number == 0)
-        {
-            if (proceed_with_gc_p && (!settings.concurrent))
-            {
-                do_post_gc();
-            }
-#ifdef BACKGROUND_GC
-            recover_bgc_settings();
-#endif //BACKGROUND_GC
-#ifdef MULTIPLE_HEAPS
-#ifdef STRESS_DYNAMIC_HEAP_COUNT
-            dynamic_heap_count_data.lowest_heap_with_msl_uoh = -1;
-#endif //STRESS_DYNAMIC_HEAP_COUNT
-            for (int i = 0; i < gc_heap::n_heaps; i++)
-            {
-                gc_heap* hp = gc_heap::g_heaps[i];
-                leave_spin_lock(&hp->more_space_lock_soh);
-#ifdef STRESS_DYNAMIC_HEAP_COUNT
-                if ((dynamic_heap_count_data.lowest_heap_with_msl_uoh == -1) && (hp->uoh_msl_before_gc_p))
-                {
-                    dynamic_heap_count_data.lowest_heap_with_msl_uoh = i;
-                }
-                if (hp->uoh_msl_before_gc_p)
-                {
-                    dprintf (5555, ("h%d uoh msl was taken before GC", i));
-                    hp->uoh_msl_before_gc_p = false;
-                }
-#endif //STRESS_DYNAMIC_HEAP_COUNT
-            }
-#endif //MULTIPLE_HEAPS
-            gc_heap::gc_started = FALSE;
-#ifdef BACKGROUND_GC
-            gc_heap::add_bgc_pause_duration_0();
-#endif //BACKGROUND_GC
-            BEGIN_TIMING(restart_ee_during_log);
-            GCToEEInterface::RestartEE(TRUE);
-            END_TIMING(restart_ee_during_log);
-            process_sync_log_stats();
-            dprintf (SPINLOCK_LOG, ("GC Lgc"));
-            leave_spin_lock (&gc_heap::gc_lock);
-            gc_heap::internal_gc_done = true;
-            if (proceed_with_gc_p)
-                set_gc_done();
-            else
-            {
-                for (int i = 0; i < gc_heap::n_heaps; i++)
-                {
-                    gc_heap* hp = gc_heap::g_heaps[i];
-                    hp->set_gc_done();
-                }
-            }
-            if (gradual_decommit_in_progress_p)
-            {
-                gradual_decommit_in_progress_p = decommit_step (DECOMMIT_TIME_STEP_MILLISECONDS);
-            }
-        }
-        else
-        {
-            int spin_count = 32 * (gc_heap::n_heaps - 1);
-            while (!gc_heap::internal_gc_done && !GCHeap::SafeToRestartManagedThreads())
-            {
-                spin_and_switch (spin_count, (gc_heap::internal_gc_done || GCHeap::SafeToRestartManagedThreads()));
-            }
-            set_gc_done();
-        }
-    }
-}
-#ifdef _MSC_VER
-#pragma warning(default:4715) //IA64 xcompiler recognizes that without the 'break;' the while(1) will never end and therefore not return a value for that code path
-#endif //_MSC_VER
-#endif //MULTIPLE_HEAPS
-bool gc_heap::virtual_alloc_commit_for_heap (void* addr, size_t size, int h_number)
-{
-#if defined(MULTIPLE_HEAPS) && !defined(FEATURE_NATIVEAOT)
-#if !defined(FEATURE_CORECLR) && !defined(BUILD_AS_STANDALONE)
-    if (!CLRMemoryHosted())
-#endif
-    {
-        if (GCToOSInterface::CanEnableGCNumaAware())
-        {
-            uint16_t numa_node = heap_select::find_numa_node_from_heap_no(h_number);
-            if (GCToOSInterface::VirtualCommit (addr, size, numa_node))
-                return true;
-        }
-    }
-#else //MULTIPLE_HEAPS && !FEATURE_NATIVEAOT
-    UNREFERENCED_PARAMETER(h_number);
-#endif //MULTIPLE_HEAPS && !FEATURE_NATIVEAOT
-    return GCToOSInterface::VirtualCommit(addr, size);
-}
-bool gc_heap::virtual_commit (void* address, size_t size, int bucket, int h_number, bool* hard_limit_exceeded_p)
-{
-    /**
-     * Here are all the possible cases for the commits:
-     *
-     * Case 1: This is for a particular generation - the bucket will be one of the gc_oh_num != unknown, and the h_number will be the right heap
-     * Case 2: This is for bookkeeping - the bucket will be recorded_committed_bookkeeping_bucket, and the h_number will be -1
-     *
-     * Note  : We never commit into free directly, so bucket != recorded_committed_free_bucket
-     */
-#ifndef HOST_64BIT
-    assert (heap_hard_limit == 0);
-#endif //!HOST_64BIT
-    assert(0 <= bucket && bucket < recorded_committed_bucket_counts);
-    assert(bucket < total_oh_count || h_number == -1);
-    assert(bucket != recorded_committed_free_bucket);
-    dprintf(3, ("commit-accounting:  commit in %d [%p, %p) for heap %d", bucket, address, ((uint8_t*)address + size), h_number));
-#ifndef COMMITTED_BYTES_SHADOW
-    if (heap_hard_limit)
-#endif //!COMMITTED_BYTES_SHADOW
-    {
-        check_commit_cs.Enter();
-        bool exceeded_p = false;
-        if (heap_hard_limit_oh[soh] != 0)
-        {
-            if ((bucket < total_oh_count) && (committed_by_oh[bucket] + size) > heap_hard_limit_oh[bucket])
-            {
-                exceeded_p = true;
-            }
-        }
-        else
-        {
-            size_t base = current_total_committed;
-            size_t limit = heap_hard_limit;
-            if ((base + size) > limit)
-            {
-                dprintf (1, ("%zd + %zd = %zd > limit %zd ", base, size, (base + size), limit));
-                exceeded_p = true;
-            }
-        }
-#ifdef COMMITTED_BYTES_SHADOW
-        if (!heap_hard_limit) {
-            exceeded_p = false;
-        }
-#endif //COMMITTED_BYTES_SHADOW
-        if (!exceeded_p)
-        {
-#if defined(_DEBUG) && defined(MULTIPLE_HEAPS)
-            if ((h_number != -1) && (bucket < total_oh_count))
-            {
-                g_heaps[h_number]->committed_by_oh_per_heap[bucket] += size;
-            }
-#endif // _DEBUG && MULTIPLE_HEAPS
-            committed_by_oh[bucket] += size;
-            current_total_committed += size;
-            if (h_number < 0)
-                current_total_committed_bookkeeping += size;
-        }
-        check_commit_cs.Leave();
-        if (hard_limit_exceeded_p)
-            *hard_limit_exceeded_p = exceeded_p;
-        if (exceeded_p)
-        {
-            dprintf (1, ("can't commit %zx for %zd bytes > HARD LIMIT %zd", (size_t)address, size, heap_hard_limit));
-            return false;
-        }
-    }
-    bool commit_succeeded_p = ((h_number >= 0) ? (use_large_pages_p ? true :
-                              virtual_alloc_commit_for_heap (address, size, h_number)) :
-                              GCToOSInterface::VirtualCommit(address, size));
-    if (!commit_succeeded_p && heap_hard_limit)
-    {
-        check_commit_cs.Enter();
-        committed_by_oh[bucket] -= size;
-#if defined(_DEBUG) && defined(MULTIPLE_HEAPS)
-        if ((h_number != -1) && (bucket < total_oh_count))
-        {
-            assert (g_heaps[h_number]->committed_by_oh_per_heap[bucket] >= size);
-            g_heaps[h_number]->committed_by_oh_per_heap[bucket] -= size;
-        }
-#endif // _DEBUG && MULTIPLE_HEAPS
-        dprintf (1, ("commit failed, updating %zd to %zd",
-                current_total_committed, (current_total_committed - size)));
-        current_total_committed -= size;
-        if (h_number < 0)
-        {
-            assert (current_total_committed_bookkeeping >= size);
-            current_total_committed_bookkeeping -= size;
-        }
-        check_commit_cs.Leave();
-    }
-    return commit_succeeded_p;
-}
-bool gc_heap::virtual_decommit (void* address, size_t size, int bucket, int h_number)
-{
-    /**
-     * Here are all possible cases for the decommits:
-     *
-     * Case 1: This is for a particular generation - the bucket will be one of the gc_oh_num != unknown, and the h_number will be the right heap
-     * Case 2: This is for bookkeeping - the bucket will be recorded_committed_bookkeeping_bucket, and the h_number will be -1
-     * Case 3: This is for free - the bucket will be recorded_committed_free_bucket, and the h_number will be -1
-     */
-#ifndef HOST_64BIT
-    assert (heap_hard_limit == 0);
-#endif //!HOST_64BIT
-    assert(0 <= bucket && bucket < recorded_committed_bucket_counts);
-    assert(bucket < total_oh_count || h_number == -1);
-    bool decommit_succeeded_p = ((bucket != recorded_committed_bookkeeping_bucket) && use_large_pages_p) ? true : GCToOSInterface::VirtualDecommit (address, size);
-    dprintf(3, ("commit-accounting:  decommit in %d [%p, %p) for heap %d", bucket, address, ((uint8_t*)address + size), h_number));
-    if (decommit_succeeded_p)
-#ifndef COMMITTED_BYTES_SHADOW
-    if (heap_hard_limit)
-#endif //!COMMITTED_BYTES_SHADOW
-    {
-        check_commit_cs.Enter();
-        assert (committed_by_oh[bucket] >= size);
-        committed_by_oh[bucket] -= size;
-#if defined(_DEBUG) && defined(MULTIPLE_HEAPS)
-        if ((h_number != -1) && (bucket < total_oh_count))
-        {
-            assert (g_heaps[h_number]->committed_by_oh_per_heap[bucket] >= size);
-            g_heaps[h_number]->committed_by_oh_per_heap[bucket] -= size;
-        }
-#endif // _DEBUG && MULTIPLE_HEAPS
-        assert (current_total_committed >= size);
-        current_total_committed -= size;
-        if (bucket == recorded_committed_bookkeeping_bucket)
-        {
-            assert (current_total_committed_bookkeeping >= size);
-            current_total_committed_bookkeeping -= size;
-        }
-        check_commit_cs.Leave();
-    }
-    return decommit_succeeded_p;
-}
-void gc_heap::virtual_free (void* add, size_t allocated_size, heap_segment* sg)
-{
-    bool release_succeeded_p = GCToOSInterface::VirtualRelease (add, allocated_size);
-    if (release_succeeded_p)
-    {
-        reserved_memory -= allocated_size;
-        dprintf (2, ("Virtual Free size %zd: [%zx, %zx[",
-                    allocated_size, (size_t)add, (size_t)((uint8_t*)add + allocated_size)));
-    }
-}
-class mark
-{
-public:
-    uint8_t* first;
-    size_t len;
-    gap_reloc_pair saved_pre_plug;
-    gap_reloc_pair saved_pre_plug_reloc;
-    gap_reloc_pair saved_post_plug;
-    gap_reloc_pair saved_post_plug_reloc;
-    uint8_t* saved_pre_plug_info_reloc_start;
-    uint8_t* saved_post_plug_info_start;
-#ifdef SHORT_PLUGS
-    uint8_t* allocation_context_start_region;
-#endif //SHORT_PLUGS
-    BOOL saved_pre_p;
-    BOOL saved_post_p;
-#ifdef _DEBUG
-    gap_reloc_pair saved_post_plug_debug;
-#endif //_DEBUG
-    size_t get_max_short_bits()
-    {
-        return (sizeof (gap_reloc_pair) / sizeof (uint8_t*));
-    }
-    size_t get_pre_short_start_bit ()
-    {
-        return (sizeof (saved_pre_p) * 8 - 1 - (sizeof (gap_reloc_pair) / sizeof (uint8_t*)));
-    }
-    BOOL pre_short_p()
-    {
-        return (saved_pre_p & (1 << (sizeof (saved_pre_p) * 8 - 1)));
-    }
-    void set_pre_short()
-    {
-        saved_pre_p |= (1 << (sizeof (saved_pre_p) * 8 - 1));
-    }
-    void set_pre_short_bit (size_t bit)
-    {
-        saved_pre_p |= 1 << (get_pre_short_start_bit() + bit);
-    }
-    BOOL pre_short_bit_p (size_t bit)
-    {
-        return (saved_pre_p & (1 << (get_pre_short_start_bit() + bit)));
-    }
-#ifdef COLLECTIBLE_CLASS
-    void set_pre_short_collectible()
-    {
-        saved_pre_p |= 2;
-    }
-    BOOL pre_short_collectible_p()
-    {
-        return (saved_pre_p & 2);
-    }
-#endif //COLLECTIBLE_CLASS
-    size_t get_post_short_start_bit ()
-    {
-        return (sizeof (saved_post_p) * 8 - 1 - (sizeof (gap_reloc_pair) / sizeof (uint8_t*)));
-    }
-    BOOL post_short_p()
-    {
-        return (saved_post_p & (1 << (sizeof (saved_post_p) * 8 - 1)));
-    }
-    void set_post_short()
-    {
-        saved_post_p |= (1 << (sizeof (saved_post_p) * 8 - 1));
-    }
-    void set_post_short_bit (size_t bit)
-    {
-        saved_post_p |= 1 << (get_post_short_start_bit() + bit);
-    }
-    BOOL post_short_bit_p (size_t bit)
-    {
-        return (saved_post_p & (1 << (get_post_short_start_bit() + bit)));
-    }
-#ifdef COLLECTIBLE_CLASS
-    void set_post_short_collectible()
-    {
-        saved_post_p |= 2;
-    }
-    BOOL post_short_collectible_p()
-    {
-        return (saved_post_p & 2);
-    }
-#endif //COLLECTIBLE_CLASS
-    uint8_t* get_plug_address() { return first; }
-    BOOL has_pre_plug_info() { return saved_pre_p; }
-    BOOL has_post_plug_info() { return saved_post_p; }
-    gap_reloc_pair* get_pre_plug_reloc_info() { return &saved_pre_plug_reloc; }
-    gap_reloc_pair* get_post_plug_reloc_info() { return &saved_post_plug_reloc; }
-    void set_pre_plug_info_reloc_start (uint8_t* reloc) { saved_pre_plug_info_reloc_start = reloc; }
-    uint8_t* get_post_plug_info_start() { return saved_post_plug_info_start; }
-    void swap_pre_plug_and_saved()
-    {
-        gap_reloc_pair temp;
-        memcpy (&temp, (first - sizeof (plug_and_gap)), sizeof (temp));
-        memcpy ((first - sizeof (plug_and_gap)), &saved_pre_plug_reloc, sizeof (saved_pre_plug_reloc));
-        saved_pre_plug_reloc = temp;
-    }
-    void swap_post_plug_and_saved()
-    {
-        gap_reloc_pair temp;
-        memcpy (&temp, saved_post_plug_info_start, sizeof (temp));
-        memcpy (saved_post_plug_info_start, &saved_post_plug_reloc, sizeof (saved_post_plug_reloc));
-        saved_post_plug_reloc = temp;
-    }
-    void swap_pre_plug_and_saved_for_profiler()
-    {
-        gap_reloc_pair temp;
-        memcpy (&temp, (first - sizeof (plug_and_gap)), sizeof (temp));
-        memcpy ((first - sizeof (plug_and_gap)), &saved_pre_plug, sizeof (saved_pre_plug));
-        saved_pre_plug = temp;
-    }
-    void swap_post_plug_and_saved_for_profiler()
-    {
-        gap_reloc_pair temp;
-        memcpy (&temp, saved_post_plug_info_start, sizeof (temp));
-        memcpy (saved_post_plug_info_start, &saved_post_plug, sizeof (saved_post_plug));
-        saved_post_plug = temp;
-    }
-    size_t recover_plug_info()
-    {
-        size_t recovered_sweep_size = 0;
-        if (saved_pre_p)
-        {
-            if (gc_heap::settings.compaction)
-            {
-                dprintf (3, ("%p: REC Pre: %p-%p",
-                    first,
-                    &saved_pre_plug_reloc,
-                    saved_pre_plug_info_reloc_start));
-                memcpy (saved_pre_plug_info_reloc_start, &saved_pre_plug_reloc, sizeof (saved_pre_plug_reloc));
-            }
-            else
-            {
-                dprintf (3, ("%p: REC Pre: %p-%p",
-                    first,
-                    &saved_pre_plug,
-                    (first - sizeof (plug_and_gap))));
-                memcpy ((first - sizeof (plug_and_gap)), &saved_pre_plug, sizeof (saved_pre_plug));
-                recovered_sweep_size += sizeof (saved_pre_plug);
-            }
-        }
-        if (saved_post_p)
-        {
-            if (gc_heap::settings.compaction)
-            {
-                dprintf (3, ("%p: REC Post: %p-%p",
-                    first,
-                    &saved_post_plug_reloc,
-                    saved_post_plug_info_start));
-                memcpy (saved_post_plug_info_start, &saved_post_plug_reloc, sizeof (saved_post_plug_reloc));
-            }
-            else
-            {
-                dprintf (3, ("%p: REC Post: %p-%p",
-                    first,
-                    &saved_post_plug,
-                    saved_post_plug_info_start));
-                memcpy (saved_post_plug_info_start, &saved_post_plug, sizeof (saved_post_plug));
-                recovered_sweep_size += sizeof (saved_post_plug);
-            }
-        }
-        return recovered_sweep_size;
-    }
-};
-void gc_mechanisms::init_mechanisms()
-{
-    condemned_generation = 0;
-    promotion = FALSE;//TRUE;
-    compaction = TRUE;
-#ifdef FEATURE_LOH_COMPACTION
-    loh_compaction = gc_heap::loh_compaction_requested();
-#else
-    loh_compaction = FALSE;
-#endif //FEATURE_LOH_COMPACTION
-    heap_expansion = FALSE;
-    concurrent = FALSE;
-    demotion = FALSE;
-    elevation_reduced = FALSE;
-    found_finalizers = FALSE;
-#ifdef BACKGROUND_GC
-    background_p = gc_heap::background_running_p() != FALSE;
-#endif //BACKGROUND_GC
-    entry_memory_load = 0;
-    entry_available_physical_mem = 0;
-    exit_memory_load = 0;
-#ifdef STRESS_HEAP
-    stress_induced = FALSE;
-#endif // STRESS_HEAP
-}
-void gc_mechanisms::first_init()
-{
-    gc_index = 0;
-    gen0_reduction_count = 0;
-    should_lock_elevation = FALSE;
-    elevation_locked_count = 0;
-    reason = reason_empty;
-#ifdef BACKGROUND_GC
-    pause_mode = gc_heap::gc_can_use_concurrent ? pause_interactive : pause_batch;
-#ifdef _DEBUG
-    int debug_pause_mode = static_cast<int>(GCConfig::GetLatencyMode());
-    if (debug_pause_mode >= 0)
-    {
-        assert (debug_pause_mode <= pause_sustained_low_latency);
-        pause_mode = (gc_pause_mode)debug_pause_mode;
-    }
-#endif //_DEBUG
-#else //BACKGROUND_GC
-    pause_mode = pause_batch;
-#endif //BACKGROUND_GC
-    init_mechanisms();
-}
-void gc_mechanisms::record (gc_history_global* history)
-{
-#ifdef MULTIPLE_HEAPS
-    history->num_heaps = gc_heap::n_heaps;
-#else
-    history->num_heaps = 1;
-#endif //MULTIPLE_HEAPS
-    history->condemned_generation = condemned_generation;
-    history->gen0_reduction_count = gen0_reduction_count;
-    history->reason = reason;
-    history->pause_mode = (int)pause_mode;
-    history->mem_pressure = entry_memory_load;
-    history->global_mechanisms_p = 0;
-    if (concurrent)
-        history->set_mechanism_p (global_concurrent);
-    if (compaction)
-        history->set_mechanism_p (global_compaction);
-    if (promotion)
-        history->set_mechanism_p (global_promotion);
-    if (demotion)
-        history->set_mechanism_p (global_demotion);
-    if (card_bundles)
-        history->set_mechanism_p (global_card_bundles);
-    if (elevation_reduced)
-        history->set_mechanism_p (global_elevation);
-}
-/**********************************
-   called at the beginning of GC to fix the allocated size to
-   what is really allocated, or to turn the free area into an unused object
-   It needs to be called after all of the other allocation contexts have been
-   fixed since it relies on alloc_allocated.
- ********************************/
-void gc_heap::fix_youngest_allocation_area()
-{
-    assert (generation_allocation_pointer (youngest_generation) == nullptr);
-    assert (generation_allocation_limit (youngest_generation) == nullptr);
-    heap_segment_allocated (ephemeral_heap_segment) = alloc_allocated;
-    assert (heap_segment_mem (ephemeral_heap_segment) <= heap_segment_allocated (ephemeral_heap_segment));
-    assert (heap_segment_allocated (ephemeral_heap_segment) <= heap_segment_reserved (ephemeral_heap_segment));
-}
-void gc_heap::fix_allocation_context (alloc_context* acontext, BOOL for_gc_p,
-                                      BOOL record_ac_p)
-{
-    dprintf (3, ("Fixing allocation context %zx: ptr: %zx, limit: %zx",
-                 (size_t)acontext,
-                 (size_t)acontext->alloc_ptr, (size_t)acontext->alloc_limit));
-    if (acontext->alloc_ptr == 0)
-    {
-        return;
-    }
-    int align_const = get_alignment_constant (TRUE);
-#ifdef USE_REGIONS
-    bool is_ephemeral_heap_segment = in_range_for_segment (acontext->alloc_limit, ephemeral_heap_segment);
-#else // USE_REGIONS
-    bool is_ephemeral_heap_segment = true;
-#endif // USE_REGIONS
-    if ((!is_ephemeral_heap_segment) || ((size_t)(alloc_allocated - acontext->alloc_limit) > Align (min_obj_size, align_const)) ||
-        !for_gc_p)
-    {
-        uint8_t*  point = acontext->alloc_ptr;
-        size_t  size = (acontext->alloc_limit - acontext->alloc_ptr);
-        size += Align (min_obj_size, align_const);
-        assert ((size >= Align (min_obj_size)));
-        dprintf(3,("Making unused area [%zx, %zx[", (size_t)point,
-                    (size_t)point + size ));
-        make_unused_array (point, size);
-        if (for_gc_p)
-        {
-            generation_free_obj_space (generation_of (0)) += size;
-            if (record_ac_p)
-                alloc_contexts_used ++;
-        }
-    }
-    else if (for_gc_p)
-    {
-        assert (is_ephemeral_heap_segment);
-        alloc_allocated = acontext->alloc_ptr;
-        assert (heap_segment_allocated (ephemeral_heap_segment) <=
-                heap_segment_committed (ephemeral_heap_segment));
-        if (record_ac_p)
-            alloc_contexts_used ++;
-    }
-    if (for_gc_p)
-    {
-        acontext->alloc_bytes -= (acontext->alloc_limit - acontext->alloc_ptr);
-        total_alloc_bytes_soh -= (acontext->alloc_limit - acontext->alloc_ptr);
-        acontext->alloc_ptr = 0;
-        acontext->alloc_limit = acontext->alloc_ptr;
-    }
-}
-void repair_allocation (gc_alloc_context* acontext, void*)
-{
-    uint8_t*  point = acontext->alloc_ptr;
-    if (point != 0)
-    {
-        dprintf (3, ("Clearing [%zx, %zx[", (size_t)acontext->alloc_ptr,
-                     (size_t)acontext->alloc_limit+Align(min_obj_size)));
-        memclr (acontext->alloc_ptr - plug_skew,
-                (acontext->alloc_limit - acontext->alloc_ptr)+Align (min_obj_size));
-    }
-}
-void void_allocation (gc_alloc_context* acontext, void*)
-{
-    uint8_t*  point = acontext->alloc_ptr;
-    if (point != 0)
-    {
-        dprintf (3, ("Void [%zx, %zx[", (size_t)acontext->alloc_ptr,
-                     (size_t)acontext->alloc_limit+Align(min_obj_size)));
-        acontext->alloc_ptr = 0;
-        acontext->alloc_limit = acontext->alloc_ptr;
-    }
-}
-void gc_heap::repair_allocation_contexts (BOOL repair_p)
-{
-    GCToEEInterface::GcEnumAllocContexts (repair_p ? repair_allocation : void_allocation, NULL);
-}
-struct fix_alloc_context_args
-{
-    BOOL for_gc_p;
-    void* heap;
-};
-void fix_alloc_context (gc_alloc_context* acontext, void* param)
-{
-    fix_alloc_context_args* args = (fix_alloc_context_args*)param;
-    g_theGCHeap->FixAllocContext(acontext, (void*)(size_t)(args->for_gc_p), args->heap);
-}
-void gc_heap::fix_allocation_contexts (BOOL for_gc_p)
-{
-    fix_alloc_context_args args;
-    args.for_gc_p = for_gc_p;
-    args.heap = __this;
-    GCToEEInterface::GcEnumAllocContexts(fix_alloc_context, &args);
-    fix_youngest_allocation_area();
-}
-void gc_heap::fix_older_allocation_area (generation* older_gen)
-{
-    heap_segment* older_gen_seg = generation_allocation_segment (older_gen);
-    if (generation_allocation_limit (older_gen) !=
-        heap_segment_plan_allocated (older_gen_seg))
-    {
-        uint8_t*  point = generation_allocation_pointer (older_gen);
-        size_t  size = (generation_allocation_limit (older_gen) -
-                               generation_allocation_pointer (older_gen));
-        if (size != 0)
-        {
-            assert ((size >= Align (min_obj_size)));
-            dprintf(3,("Making unused area [%zx, %zx[", (size_t)point, (size_t)point+size));
-            make_unused_array (point, size);
-            if (size >= min_free_list)
-            {
-                generation_allocator (older_gen)->thread_item_front (point, size);
-                add_gen_free (older_gen->gen_num, size);
-                generation_free_list_space (older_gen) += size;
-            }
-            else
-            {
-                generation_free_obj_space (older_gen) += size;
-            }
-        }
-    }
-    else
-    {
-        assert (older_gen_seg != ephemeral_heap_segment);
-        heap_segment_plan_allocated (older_gen_seg) =
-            generation_allocation_pointer (older_gen);
-        generation_allocation_limit (older_gen) =
-            generation_allocation_pointer (older_gen);
-    }
-    generation_allocation_pointer (older_gen) = 0;
-    generation_allocation_limit (older_gen) = 0;
-}
-#ifdef MULTIPLE_HEAPS
-void gc_heap::fix_allocation_context_heaps (gc_alloc_context* gc_context, void*)
-{
-    alloc_context* acontext = (alloc_context*)gc_context;
-    GCHeap* pHomeHeap = acontext->get_home_heap ();
-    int home_hp_num = pHomeHeap ? pHomeHeap->pGenGCHeap->heap_number : 0;
-    if (home_hp_num >= gc_heap::n_heaps)
-    {
-        home_hp_num %= gc_heap::n_heaps;
-        acontext->set_home_heap (GCHeap::GetHeap (home_hp_num));
-    }
-    GCHeap* pAllocHeap = acontext->get_alloc_heap ();
-    int alloc_hp_num = pAllocHeap ? pAllocHeap->pGenGCHeap->heap_number : 0;
-    if (alloc_hp_num >= gc_heap::n_heaps)
-    {
-        alloc_hp_num %= gc_heap::n_heaps;
-        acontext->set_alloc_heap (GCHeap::GetHeap (alloc_hp_num));
-        gc_heap* hp = acontext->get_alloc_heap ()->pGenGCHeap;
-        hp->alloc_context_count++;
-    }
-}
-void gc_heap::fix_allocation_contexts_heaps()
-{
-    GCToEEInterface::GcEnumAllocContexts (fix_allocation_context_heaps, nullptr);
-}
-#endif //MULTIPLE_HEAPS
-void gc_heap::set_allocation_heap_segment (generation* gen)
-{
-#ifdef USE_REGIONS
-    heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-    dprintf (REGIONS_LOG, ("set gen%d alloc seg to start seg %p", gen->gen_num, heap_segment_mem (seg)));
-#else
-    uint8_t* p = generation_allocation_start (gen);
-    assert (p);
-    heap_segment* seg = generation_allocation_segment (gen);
-    if (in_range_for_segment (p, seg))
-        return;
-    seg = ephemeral_heap_segment;
-    if (!in_range_for_segment (p, seg))
-    {
-        seg = heap_segment_rw (generation_start_segment (gen));
-        PREFIX_ASSUME(seg != NULL);
-        while (!in_range_for_segment (p, seg))
-        {
-            seg = heap_segment_next_rw (seg);
-            PREFIX_ASSUME(seg != NULL);
-        }
-    }
-#endif //USE_REGIONS
-    generation_allocation_segment (gen) = seg;
-}
-void gc_heap::reset_allocation_pointers (generation* gen, uint8_t* start)
-{
-    assert (start);
-    assert (Align ((size_t)start) == (size_t)start);
-#ifndef USE_REGIONS
-    generation_allocation_start (gen) = start;
-#endif //!USE_REGIONS
-    generation_allocation_pointer (gen) =  0;//start + Align (min_obj_size);
-    generation_allocation_limit (gen) = 0;//generation_allocation_pointer (gen);
-    set_allocation_heap_segment (gen);
-}
-bool gc_heap::new_allocation_allowed (int gen_number)
-{
-    if (dd_new_allocation (dynamic_data_of (gen_number)) < 0)
-    {
-        if (gen_number != 0)
-        {
-            if (settings.concurrent)
-            {
-                dynamic_data* dd2 = dynamic_data_of (gen_number);
-                if (dd_new_allocation (dd2) <= (ptrdiff_t)(-2 * dd_desired_allocation (dd2)))
-                {
-                    return TRUE;
-                }
-            }
-        }
-        return FALSE;
-    }
-#ifndef MULTIPLE_HEAPS
-    else if ((settings.pause_mode != pause_no_gc) && (gen_number == 0))
-    {
-        dynamic_data* dd0 = dynamic_data_of (0);
-        dprintf (3, ("evaluating, running amount %zd - new %zd = %zd",
-            allocation_running_amount, dd_new_allocation (dd0),
-            (allocation_running_amount - dd_new_allocation (dd0))));
-        if ((allocation_running_amount - dd_new_allocation (dd0)) >
-            dd_min_size (dd0))
-        {
-            uint64_t ctime = GCToOSInterface::GetLowPrecisionTimeStamp();
-            if ((ctime - allocation_running_time) > 1000)
-            {
-                dprintf (2, (">1s since last gen0 gc"));
-                return FALSE;
-            }
-            else
-            {
-                allocation_running_amount = dd_new_allocation (dd0);
-            }
-        }
-    }
-#endif //MULTIPLE_HEAPS
-    return TRUE;
-}
-inline
-ptrdiff_t gc_heap::get_desired_allocation (int gen_number)
-{
-    return dd_desired_allocation (dynamic_data_of (gen_number));
-}
-inline
-ptrdiff_t  gc_heap::get_new_allocation (int gen_number)
-{
-    return dd_new_allocation (dynamic_data_of (gen_number));
-}
-inline
-ptrdiff_t  gc_heap::get_allocation (int gen_number)
-{
-    dynamic_data* dd = dynamic_data_of (gen_number);
-    return dd_desired_allocation (dd) - dd_new_allocation (dd);
-}
-inline
-BOOL grow_mark_stack (mark*& m, size_t& len, size_t init_len)
-{
-    size_t new_size = max (init_len, 2*len);
-    mark* tmp = new (nothrow) mark [new_size];
-    if (tmp)
-    {
-        memcpy (tmp, m, len * sizeof (mark));
-        delete[] m;
-        m = tmp;
-        len = new_size;
-        return TRUE;
-    }
-    else
-    {
-        dprintf (1, ("Failed to allocate %zd bytes for mark stack", (len * sizeof (mark))));
-        return FALSE;
-    }
-}
-inline
-uint8_t* pinned_plug (mark* m)
-{
-   return m->first;
-}
-inline
-size_t& pinned_len (mark* m)
-{
-    return m->len;
-}
-inline
-void set_new_pin_info (mark* m, uint8_t* pin_free_space_start)
-{
-    m->len = pinned_plug (m) - pin_free_space_start;
-#ifdef SHORT_PLUGS
-    m->allocation_context_start_region = pin_free_space_start;
-#endif //SHORT_PLUGS
-}
-#ifdef SHORT_PLUGS
-inline
-uint8_t*& pin_allocation_context_start_region (mark* m)
-{
-    return m->allocation_context_start_region;
-}
-uint8_t* get_plug_start_in_saved (uint8_t* old_loc, mark* pinned_plug_entry)
-{
-    uint8_t* saved_pre_plug_info = (uint8_t*)(pinned_plug_entry->get_pre_plug_reloc_info());
-    uint8_t* plug_start_in_saved = saved_pre_plug_info + (old_loc - (pinned_plug (pinned_plug_entry) - sizeof (plug_and_gap)));
-    dprintf (1, ("EP: %p(%p), %p", old_loc, pinned_plug (pinned_plug_entry), plug_start_in_saved));
-    return plug_start_in_saved;
-}
-inline
-void set_padding_in_expand (uint8_t* old_loc,
-                            BOOL set_padding_on_saved_p,
-                            mark* pinned_plug_entry)
-{
-    if (set_padding_on_saved_p)
-    {
-        set_plug_padded (get_plug_start_in_saved (old_loc, pinned_plug_entry));
-    }
-    else
-    {
-        set_plug_padded (old_loc);
-    }
-}
-inline
-void clear_padding_in_expand (uint8_t* old_loc,
-                              BOOL set_padding_on_saved_p,
-                              mark* pinned_plug_entry)
-{
-    if (set_padding_on_saved_p)
-    {
-        clear_plug_padded (get_plug_start_in_saved (old_loc, pinned_plug_entry));
-    }
-    else
-    {
-        clear_plug_padded (old_loc);
-    }
-}
-#endif //SHORT_PLUGS
-void gc_heap::reset_pinned_queue()
-{
-    mark_stack_tos = 0;
-    mark_stack_bos = 0;
-}
-void gc_heap::reset_pinned_queue_bos()
-{
-    mark_stack_bos = 0;
-}
-void gc_heap::merge_with_last_pinned_plug (uint8_t* last_pinned_plug, size_t plug_size)
-{
-    if (last_pinned_plug)
-    {
-        mark& last_m = mark_stack_array[mark_stack_tos - 1];
-        assert (last_pinned_plug == last_m.first);
-        if (last_m.saved_post_p)
-        {
-            last_m.saved_post_p = FALSE;
-            dprintf (3, ("setting last plug %p post to false", last_m.first));
-            memcpy ((last_m.first + last_m.len - sizeof (plug_and_gap)), &(last_m.saved_post_plug), sizeof (gap_reloc_pair));
-        }
-        last_m.len += plug_size;
-        dprintf (3, ("recovered the last part of plug %p, setting its plug size to %zx", last_m.first, last_m.len));
-    }
-}
-void gc_heap::set_allocator_next_pin (generation* gen)
-{
-    dprintf (3, ("SANP: gen%d, ptr; %p, limit: %p", gen->gen_num, generation_allocation_pointer (gen), generation_allocation_limit (gen)));
-    if (!(pinned_plug_que_empty_p()))
-    {
-        mark*  oldest_entry = oldest_pin();
-        uint8_t* plug = pinned_plug (oldest_entry);
-        if ((plug >= generation_allocation_pointer (gen)) &&
-            (plug <  generation_allocation_limit (gen)))
-        {
-#ifdef USE_REGIONS
-            assert (region_of (generation_allocation_pointer (gen)) ==
-                    region_of (generation_allocation_limit (gen) - 1));
-#endif //USE_REGIONS
-            generation_allocation_limit (gen) = pinned_plug (oldest_entry);
-            dprintf (3, ("SANP: get next pin free space in gen%d for alloc: %p->%p(%zd)",
-                gen->gen_num,
-                generation_allocation_pointer (gen), generation_allocation_limit (gen),
-                (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
-        }
-        else
-            assert (!((plug < generation_allocation_pointer (gen)) &&
-                      (plug >= heap_segment_mem (generation_allocation_segment (gen)))));
-    }
-}
-void gc_heap::set_pinned_info (uint8_t* last_pinned_plug, size_t plug_len, generation* gen)
-{
-#ifndef _DEBUG
-    UNREFERENCED_PARAMETER(last_pinned_plug);
-#endif //_DEBUG
-    mark& m = mark_stack_array[mark_stack_tos];
-    assert (m.first == last_pinned_plug);
-    m.len = plug_len;
-    mark_stack_tos++;
-    assert (gen != 0);
-    if (gen != 0)
-    {
-        set_allocator_next_pin (gen);
-    }
-}
-size_t gc_heap::deque_pinned_plug ()
-{
-    size_t m = mark_stack_bos;
-    dprintf (3, ("deque: %zd->%p", mark_stack_bos, pinned_plug (pinned_plug_of (m))));
-    mark_stack_bos++;
-    return m;
-}
-inline
-mark* gc_heap::pinned_plug_of (size_t bos)
-{
-    return &mark_stack_array [ bos ];
-}
-inline
-mark* gc_heap::oldest_pin ()
-{
-    return pinned_plug_of (mark_stack_bos);
-}
-inline
-BOOL gc_heap::pinned_plug_que_empty_p ()
-{
-    return (mark_stack_bos == mark_stack_tos);
-}
-inline
-mark* gc_heap::before_oldest_pin()
-{
-    if (mark_stack_bos >= 1)
-        return pinned_plug_of (mark_stack_bos-1);
-    else
-        return 0;
-}
-inline
-BOOL gc_heap::ephemeral_pointer_p (uint8_t* o)
-{
-#ifdef USE_REGIONS
-    int gen_num = object_gennum ((uint8_t*)o);
-    assert (gen_num >= 0);
-    return (gen_num < max_generation);
-#else
-    return ((o >= ephemeral_low) && (o < ephemeral_high));
-#endif //USE_REGIONS
-}
-inline
-bool gc_heap::is_in_find_object_range (uint8_t* o)
-{
-    if (o == nullptr)
-    {
-        return false;
-    }
-#if defined(USE_REGIONS) && defined(FEATURE_CONSERVATIVE_GC)
-    return ((o >= g_gc_lowest_address) && (o < bookkeeping_covered_committed));
-#else //USE_REGIONS && FEATURE_CONSERVATIVE_GC
-    if ((o >= g_gc_lowest_address) && (o < g_gc_highest_address))
-    {
-#ifdef USE_REGIONS
-        assert ((o >= g_gc_lowest_address) && (o < bookkeeping_covered_committed));
-#endif //USE_REGIONS
-        return true;
-    }
-    else
-    {
-        return false;
-    }
-#endif //USE_REGIONS && FEATURE_CONSERVATIVE_GC
-}
-#ifdef USE_REGIONS
-inline
-bool gc_heap::is_in_condemned_gc (uint8_t* o)
-{
-    assert ((o >= g_gc_lowest_address) && (o < g_gc_highest_address));
-    int condemned_gen = settings.condemned_generation;
-    if (condemned_gen < max_generation)
-    {
-        int gen = get_region_gen_num (o);
-        if (gen > condemned_gen)
-        {
-            return false;
-        }
-    }
-    return true;
-}
-inline
-bool gc_heap::should_check_brick_for_reloc (uint8_t* o)
-{
-    assert ((o >= g_gc_lowest_address) && (o < g_gc_highest_address));
-    size_t skewed_basic_region_index = get_skewed_basic_region_index_for_address (o);
-    return (map_region_to_generation_skewed[skewed_basic_region_index] & (RI_SIP|RI_GEN_MASK)) <= settings.condemned_generation;
-}
-#endif //USE_REGIONS
-#ifdef MH_SC_MARK
-inline
-int& gc_heap::mark_stack_busy()
-{
-    return  g_mark_stack_busy [(heap_number+2)*HS_CACHE_LINE_SIZE/sizeof(int)];
-}
-#endif //MH_SC_MARK
-void gc_heap::make_mark_stack (mark* arr)
-{
-    reset_pinned_queue();
-    mark_stack_array = arr;
-    mark_stack_array_length = MARK_STACK_INITIAL_LENGTH;
-#ifdef MH_SC_MARK
-    mark_stack_busy() = 0;
-#endif //MH_SC_MARK
-}
-#ifdef BACKGROUND_GC
-inline
-size_t& gc_heap::bpromoted_bytes(int thread)
-{
-#ifdef MULTIPLE_HEAPS
-    return g_bpromoted [thread*16];
-#else //MULTIPLE_HEAPS
-    UNREFERENCED_PARAMETER(thread);
-    return g_bpromoted;
-#endif //MULTIPLE_HEAPS
-}
-void gc_heap::make_background_mark_stack (uint8_t** arr)
-{
-    background_mark_stack_array = arr;
-    background_mark_stack_array_length = MARK_STACK_INITIAL_LENGTH;
-    background_mark_stack_tos = arr;
-}
-void gc_heap::make_c_mark_list (uint8_t** arr)
-{
-    c_mark_list = arr;
-    c_mark_list_index = 0;
-    c_mark_list_length = 1 + (OS_PAGE_SIZE / MIN_OBJECT_SIZE);
-}
-#endif //BACKGROUND_GC
-#ifdef CARD_BUNDLE
-static const size_t card_bundle_word_width = 32;
-static const size_t card_bundle_size = (size_t)(GC_PAGE_SIZE / (sizeof(uint32_t)*card_bundle_word_width));
-inline
-size_t card_bundle_word (size_t cardb)
-{
-    return cardb / card_bundle_word_width;
-}
-inline
-uint32_t card_bundle_bit (size_t cardb)
-{
-    return (uint32_t)(cardb % card_bundle_word_width);
-}
-size_t align_cardw_on_bundle (size_t cardw)
-{
-    return ((size_t)(cardw + card_bundle_size - 1) & ~(card_bundle_size - 1 ));
-}
-size_t cardw_card_bundle (size_t cardw)
-{
-    return cardw / card_bundle_size;
-}
-size_t card_bundle_cardw (size_t cardb)
-{
-    return cardb * card_bundle_size;
-}
-void gc_heap::card_bundle_clear (size_t cardb)
-{
-    uint32_t bit = (uint32_t)(1 << card_bundle_bit (cardb));
-    uint32_t* bundle = &card_bundle_table[card_bundle_word (cardb)];
-#ifdef MULTIPLE_HEAPS
-    if ((*bundle & bit) != 0)
-    {
-        Interlocked::And (bundle, ~bit);
-    }
-#else
-    *bundle &= ~bit;
-#endif
-    assert ((*bundle & bit) == 0);
-    dprintf (2, ("Cleared card bundle %zx [%zx, %zx[", cardb, (size_t)card_bundle_cardw (cardb),
-              (size_t)card_bundle_cardw (cardb+1)));
-}
-inline void set_bundle_bits (uint32_t* bundle, uint32_t bits)
-{
-#ifdef MULTIPLE_HEAPS
-    if ((*bundle & bits) != bits)
-    {
-        Interlocked::Or (bundle, bits);
-    }
-#else
-    *bundle |= bits;
-#endif
-    assert ((*bundle & bits) == bits);
-}
-void gc_heap::card_bundle_set (size_t cardb)
-{
-    uint32_t bits = (1 << card_bundle_bit (cardb));
-    set_bundle_bits (&card_bundle_table [card_bundle_word (cardb)], bits);
-}
-void gc_heap::card_bundles_set (size_t start_cardb, size_t end_cardb)
-{
-    if (start_cardb == end_cardb)
-    {
-        card_bundle_set(start_cardb);
-        return;
-    }
-    size_t start_word = card_bundle_word (start_cardb);
-    size_t end_word = card_bundle_word (end_cardb);
-    if (start_word < end_word)
-    {
-        uint32_t bits = highbits (~0u, card_bundle_bit (start_cardb));
-        set_bundle_bits (&card_bundle_table [start_word], bits);
-        if (card_bundle_bit (end_cardb))
-        {
-            bits = lowbits (~0u, card_bundle_bit (end_cardb));
-            set_bundle_bits (&card_bundle_table [end_word], bits);
-        }
-        for (size_t i = start_word + 1; i < end_word; i++)
-        {
-            card_bundle_table [i] = ~0u;
-        }
-    }
-    else
-    {
-        uint32_t bits = (highbits (~0u, card_bundle_bit (start_cardb)) &
-                          lowbits (~0u, card_bundle_bit (end_cardb)));
-        set_bundle_bits (&card_bundle_table [start_word], bits);
-    }
-}
-BOOL gc_heap::card_bundle_set_p (size_t cardb)
-{
-    return (card_bundle_table[card_bundle_word(cardb)] & (1 << card_bundle_bit (cardb)));
-}
-size_t size_card_bundle_of (uint8_t* from, uint8_t* end)
-{
-    size_t cbw_span = card_size * card_word_width * card_bundle_size * card_bundle_word_width;
-    from = (uint8_t*)((size_t)from & ~(cbw_span - 1));
-    end = (uint8_t*)((size_t)(end + (cbw_span - 1)) & ~(cbw_span - 1));
-    assert (((size_t)from & (cbw_span - 1)) == 0);
-    assert (((size_t)end  & (cbw_span - 1)) == 0);
-    return ((end - from) / cbw_span) * sizeof (uint32_t);
-}
-uint32_t* translate_card_bundle_table (uint32_t* cb, uint8_t* lowest_address)
-{
-    const size_t heap_bytes_for_bundle_word = card_size * card_word_width * card_bundle_size * card_bundle_word_width;
-    return (uint32_t*)((uint8_t*)cb - (((size_t)lowest_address / heap_bytes_for_bundle_word) * sizeof (uint32_t)));
-}
-void gc_heap::enable_card_bundles ()
-{
-    if (can_use_write_watch_for_card_table() && (!card_bundles_enabled()))
-    {
-        dprintf (1, ("Enabling card bundles"));
-        card_bundles_set (cardw_card_bundle (card_word (card_of (lowest_address))),
-                          cardw_card_bundle (align_cardw_on_bundle (card_word (card_of (highest_address)))));
-        settings.card_bundles = TRUE;
-    }
-}
-BOOL gc_heap::card_bundles_enabled ()
-{
-    return settings.card_bundles;
-}
-#endif // CARD_BUNDLE
-#if defined (HOST_64BIT)
-#define brick_size ((size_t)4096)
-#else
-#define brick_size ((size_t)2048)
-#endif //HOST_64BIT
-inline
-size_t gc_heap::brick_of (uint8_t* add)
-{
-    return (size_t)(add - lowest_address) / brick_size;
-}
-inline
-uint8_t* gc_heap::brick_address (size_t brick)
-{
-    return lowest_address + (brick_size * brick);
-}
-void gc_heap::clear_brick_table (uint8_t* from, uint8_t* end)
-{
-    size_t from_brick = brick_of (from);
-    size_t end_brick = brick_of (end);
-    memset (&brick_table[from_brick], 0, sizeof(brick_table[from_brick])*(end_brick-from_brick));
-}
-inline
-void gc_heap::set_brick (size_t index, ptrdiff_t val)
-{
-    if (val < -32767)
-    {
-        val = -32767;
-    }
-    assert (val < 32767);
-    if (val >= 0)
-        brick_table [index] = (short)val+1;
-    else
-        brick_table [index] = (short)val;
-    dprintf (3, ("set brick[%zx] to %d\n", index, (short)val));
-}
-inline
-int gc_heap::get_brick_entry (size_t index)
-{
-#ifdef MULTIPLE_HEAPS
-    return VolatileLoadWithoutBarrier(&brick_table [index]);
-#else
-    return brick_table[index];
-#endif
-}
-inline
-uint8_t* align_on_brick (uint8_t* add)
-{
-    return (uint8_t*)((size_t)(add + brick_size - 1) & ~(brick_size - 1));
-}
-inline
-uint8_t* align_lower_brick (uint8_t* add)
-{
-    return (uint8_t*)(((size_t)add) & ~(brick_size - 1));
-}
-size_t size_brick_of (uint8_t* from, uint8_t* end)
-{
-    assert (((size_t)from & (brick_size-1)) == 0);
-    assert (((size_t)end  & (brick_size-1)) == 0);
-    return ((end - from) / brick_size) * sizeof (short);
-}
-inline
-uint8_t* gc_heap::card_address (size_t card)
-{
-    return  (uint8_t*) (card_size * card);
-}
-inline
-size_t gc_heap::card_of ( uint8_t* object)
-{
-    return (size_t)(object) / card_size;
-}
-inline
-uint8_t* align_on_card (uint8_t* add)
-{
-    return (uint8_t*)((size_t)(add + card_size - 1) & ~(card_size - 1 ));
-}
-inline
-uint8_t* align_on_card_word (uint8_t* add)
-{
-    return (uint8_t*) ((size_t)(add + (card_size*card_word_width)-1) & ~(card_size*card_word_width - 1));
-}
-inline
-uint8_t* align_lower_card (uint8_t* add)
-{
-    return (uint8_t*)((size_t)add & ~(card_size-1));
-}
-inline
-void gc_heap::clear_card (size_t card)
-{
-    card_table [card_word (card)] =
-        (card_table [card_word (card)] & ~(1 << card_bit (card)));
-    dprintf (3,("Cleared card %zx [%zx, %zx[", card, (size_t)card_address (card),
-              (size_t)card_address (card+1)));
-}
-inline
-void gc_heap::set_card (size_t card)
-{
-    size_t word = card_word (card);
-    card_table[word] = (card_table [word] | (1 << card_bit (card)));
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-    size_t bundle_to_set = cardw_card_bundle(word);
-    card_bundle_set(bundle_to_set);
-    dprintf (3,("Set card %zx [%zx, %zx[ and bundle %zx", card, (size_t)card_address (card), (size_t)card_address (card+1), bundle_to_set));
-#endif
-}
-inline
-BOOL  gc_heap::card_set_p (size_t card)
-{
-    return ( card_table [ card_word (card) ] & (1 << card_bit (card)));
-}
-size_t count_card_of (uint8_t* from, uint8_t* end)
-{
-    return card_word (gcard_of (end - 1)) - card_word (gcard_of (from)) + 1;
-}
-size_t size_card_of (uint8_t* from, uint8_t* end)
-{
-    return count_card_of (from, end) * sizeof(uint32_t);
-}
-class card_table_info
-{
-public:
-    unsigned    recount;
-    size_t      size;
-    uint32_t*   next_card_table;
-    uint8_t*    lowest_address;
-    uint8_t*    highest_address;
-    short*      brick_table;
-#ifdef CARD_BUNDLE
-    uint32_t*   card_bundle_table;
-#endif //CARD_BUNDLE
-#ifdef BACKGROUND_GC
-    uint32_t*   mark_array;
-#endif //BACKGROUND_GC
-};
-static_assert(offsetof(dac_card_table_info, size) == offsetof(card_table_info, size), "DAC card_table_info layout mismatch");
-static_assert(offsetof(dac_card_table_info, next_card_table) == offsetof(card_table_info, next_card_table), "DAC card_table_info layout mismatch");
-inline
-unsigned& card_table_refcount (uint32_t* c_table)
-{
-    return *(unsigned*)((char*)c_table - sizeof (card_table_info));
-}
-inline
-uint8_t*& card_table_lowest_address (uint32_t* c_table)
-{
-    return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->lowest_address;
-}
-uint32_t* translate_card_table (uint32_t* ct)
-{
-    return (uint32_t*)((uint8_t*)ct - card_word (gcard_of (card_table_lowest_address (ct))) * sizeof(uint32_t));
-}
-inline
-uint8_t*& card_table_highest_address (uint32_t* c_table)
-{
-    return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->highest_address;
-}
-inline
-short*& card_table_brick_table (uint32_t* c_table)
-{
-    return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->brick_table;
-}
-#ifdef CARD_BUNDLE
-inline
-uint32_t*& card_table_card_bundle_table (uint32_t* c_table)
-{
-    return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->card_bundle_table;
-}
-#endif //CARD_BUNDLE
-#ifdef BACKGROUND_GC
-inline
-uint32_t*& card_table_mark_array (uint32_t* c_table)
-{
-    return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->mark_array;
-}
-#ifdef HOST_64BIT
-#define mark_bit_pitch ((size_t)16)
-#else
-#define mark_bit_pitch ((size_t)8)
-#endif // HOST_64BIT
-#define mark_word_width ((size_t)32)
-#define mark_word_size (mark_word_width * mark_bit_pitch)
-inline
-uint8_t* align_on_mark_bit (uint8_t* add)
-{
-    return (uint8_t*)((size_t)(add + (mark_bit_pitch - 1)) & ~(mark_bit_pitch - 1));
-}
-inline
-uint8_t* align_lower_mark_bit (uint8_t* add)
-{
-    return (uint8_t*)((size_t)(add) & ~(mark_bit_pitch - 1));
-}
-inline
-BOOL is_aligned_on_mark_word (uint8_t* add)
-{
-    return ((size_t)add == ((size_t)(add) & ~(mark_word_size - 1)));
-}
-inline
-uint8_t* align_on_mark_word (uint8_t* add)
-{
-    return (uint8_t*)((size_t)(add + mark_word_size - 1) & ~(mark_word_size - 1));
-}
-inline
-uint8_t* align_lower_mark_word (uint8_t* add)
-{
-    return (uint8_t*)((size_t)(add) & ~(mark_word_size - 1));
-}
-inline
-size_t mark_bit_of (uint8_t* add)
-{
-    return ((size_t)add / mark_bit_pitch);
-}
-inline
-unsigned int mark_bit_bit (size_t mark_bit)
-{
-    return (unsigned int)(mark_bit % mark_word_width);
-}
-inline
-size_t mark_bit_word (size_t mark_bit)
-{
-    return (mark_bit / mark_word_width);
-}
-inline
-size_t mark_word_of (uint8_t* add)
-{
-    return ((size_t)add) / mark_word_size;
-}
-uint8_t* mark_word_address (size_t wd)
-{
-    return (uint8_t*)(wd*mark_word_size);
-}
-uint8_t* mark_bit_address (size_t mark_bit)
-{
-    return (uint8_t*)(mark_bit*mark_bit_pitch);
-}
-inline
-size_t mark_bit_bit_of (uint8_t* add)
-{
-    return  (((size_t)add / mark_bit_pitch) % mark_word_width);
-}
-inline
-unsigned int gc_heap::mark_array_marked(uint8_t* add)
-{
-    return mark_array [mark_word_of (add)] & (1 << mark_bit_bit_of (add));
-}
-inline
-BOOL gc_heap::is_mark_bit_set (uint8_t* add)
-{
-    return (mark_array [mark_word_of (add)] & (1 << mark_bit_bit_of (add)));
-}
-inline
-void gc_heap::mark_array_set_marked (uint8_t* add)
-{
-    size_t index = mark_word_of (add);
-    uint32_t val = (1 << mark_bit_bit_of (add));
-#ifdef MULTIPLE_HEAPS
-    Interlocked::Or (&(mark_array [index]), val);
-#else
-    mark_array [index] |= val;
-#endif
-}
-inline
-void gc_heap::mark_array_clear_marked (uint8_t* add)
-{
-    mark_array [mark_word_of (add)] &= ~(1 << mark_bit_bit_of (add));
-}
-size_t size_mark_array_of (uint8_t* from, uint8_t* end)
-{
-    assert (((size_t)from & ((mark_word_size)-1)) == 0);
-    assert (((size_t)end  & ((mark_word_size)-1)) == 0);
-    return sizeof (uint32_t)*(((end - from) / mark_word_size));
-}
-uint32_t* translate_mark_array (uint32_t* ma)
-{
-    return (uint32_t*)((uint8_t*)ma - size_mark_array_of (0, g_gc_lowest_address));
-}
-#ifdef FEATURE_BASICFREEZE
-void gc_heap::clear_mark_array (uint8_t* from, uint8_t* end)
-{
-    assert (gc_can_use_concurrent);
-    assert (end == align_on_mark_word (end));
-    uint8_t* current_lowest_address = background_saved_lowest_address;
-    uint8_t* current_highest_address = background_saved_highest_address;
-    if ((end <= current_highest_address) && (from >= current_lowest_address))
-    {
-        size_t beg_word = mark_word_of (align_on_mark_word (from));
-        size_t end_word = mark_word_of (align_on_mark_word (end));
-        dprintf (3, ("Calling clearing mark array [%zx, %zx[ for addresses [%zx, %zx[",
-                     (size_t)mark_word_address (beg_word),
-                     (size_t)mark_word_address (end_word),
-                     (size_t)from, (size_t)end));
-        uint8_t* op = from;
-        while (op < mark_word_address (beg_word))
-        {
-            mark_array_clear_marked (op);
-            op += mark_bit_pitch;
-        }
-        memset (&mark_array[beg_word], 0, (end_word - beg_word)*sizeof (uint32_t));
-#ifdef _DEBUG
-        size_t  markw = mark_word_of (align_on_mark_word (from));
-        size_t  markw_end = mark_word_of (align_on_mark_word (end));
-        while (markw < markw_end)
-        {
-            assert (!(mark_array [markw]));
-            markw++;
-        }
-        uint8_t* p = mark_word_address (markw_end);
-        while (p < end)
-        {
-            assert (!(mark_array_marked (p)));
-            p++;
-        }
-#endif //_DEBUG
-    }
-}
-#endif // FEATURE_BASICFREEZE
-#endif //BACKGROUND_GC
-inline
-uint32_t*& card_table_next (uint32_t* c_table)
-{
-    return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->next_card_table;
-}
-inline
-size_t& card_table_size (uint32_t* c_table)
-{
-    return ((card_table_info*)((uint8_t*)c_table - sizeof (card_table_info)))->size;
-}
-void own_card_table (uint32_t* c_table)
-{
-    card_table_refcount (c_table) += 1;
-}
-void destroy_card_table (uint32_t* c_table);
-void delete_next_card_table (uint32_t* c_table)
-{
-    uint32_t* n_table = card_table_next (c_table);
-    if (n_table)
-    {
-        if (card_table_next (n_table))
-        {
-            delete_next_card_table (n_table);
-        }
-        if (card_table_refcount (n_table) == 0)
-        {
-            destroy_card_table (n_table);
-            card_table_next (c_table) = 0;
-        }
-    }
-}
-void release_card_table (uint32_t* c_table)
-{
-    assert (card_table_refcount (c_table) >0);
-    card_table_refcount (c_table) -= 1;
-    if (card_table_refcount (c_table) == 0)
-    {
-        delete_next_card_table (c_table);
-        if (card_table_next (c_table) == 0)
-        {
-            destroy_card_table (c_table);
-            if (&g_gc_card_table[card_word (gcard_of(g_gc_lowest_address))] == c_table)
-            {
-                g_gc_card_table = 0;
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-                g_gc_card_bundle_table = 0;
-#endif
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-                SoftwareWriteWatch::StaticClose();
-#endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-            }
-            else
-            {
-                uint32_t* p_table = &g_gc_card_table[card_word (gcard_of(g_gc_lowest_address))];
-                if (p_table)
-                {
-                    while (p_table && (card_table_next (p_table) != c_table))
-                        p_table = card_table_next (p_table);
-                    card_table_next (p_table) = 0;
-                }
-            }
-        }
-    }
-}
-void destroy_card_table (uint32_t* c_table)
-{
-    GCToOSInterface::VirtualRelease (&card_table_refcount(c_table), card_table_size(c_table));
-    dprintf (2, ("Table Virtual Free : %zx", (size_t)&card_table_refcount(c_table)));
-}
-void gc_heap::get_card_table_element_sizes (uint8_t* start, uint8_t* end, size_t sizes[total_bookkeeping_elements])
-{
-    memset (sizes, 0, sizeof(size_t) * total_bookkeeping_elements);
-    sizes[card_table_element] = size_card_of (start, end);
-    sizes[brick_table_element] = size_brick_of (start, end);
-#ifdef CARD_BUNDLE
-    if (can_use_write_watch_for_card_table())
-    {
-        sizes[card_bundle_table_element] = size_card_bundle_of (start, end);
-    }
-#endif //CARD_BUNDLE
-#if defined(FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP) && defined (BACKGROUND_GC)
-    if (gc_can_use_concurrent)
-    {
-        sizes[software_write_watch_table_element] = SoftwareWriteWatch::GetTableByteSize(start, end);
-    }
-#endif //FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP && BACKGROUND_GC
-#ifdef USE_REGIONS
-    sizes[region_to_generation_table_element] = size_region_to_generation_table_of (start, end);
-#endif //USE_REGIONS
-    sizes[seg_mapping_table_element] = size_seg_mapping_table_of (start, end);
-#ifdef BACKGROUND_GC
-    if (gc_can_use_concurrent)
-    {
-        sizes[mark_array_element] = size_mark_array_of (start, end);
-    }
-#endif //BACKGROUND_GC
-}
-void gc_heap::get_card_table_element_layout (uint8_t* start, uint8_t* end, size_t layout[total_bookkeeping_elements + 1])
-{
-    size_t sizes[total_bookkeeping_elements];
-    get_card_table_element_sizes(start, end, sizes);
-    const size_t alignment[total_bookkeeping_elements + 1] =
-    {
-        sizeof (uint32_t), // card_table_element
-        sizeof (short),    // brick_table_element
-#ifdef CARD_BUNDLE
-        sizeof (uint32_t), // card_bundle_table_element
-#endif //CARD_BUNDLE
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-        sizeof(size_t),    // software_write_watch_table_element
-#endif //FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-#ifdef USE_REGIONS
-        sizeof (uint8_t),  // region_to_generation_table_element
-#endif //USE_REGIONS
-        sizeof (uint8_t*), // seg_mapping_table_element
-#ifdef BACKGROUND_GC
-        OS_PAGE_SIZE,      // mark_array_element
-#endif //BACKGROUND_GC
-        OS_PAGE_SIZE       // total_bookkeeping_elements
-    };
-    layout[card_table_element] = ALIGN_UP(sizeof(card_table_info), alignment[card_table_element]);
-    for (int element = brick_table_element; element <= total_bookkeeping_elements; element++)
-    {
-        layout[element] = layout[element - 1] + sizes[element - 1];
-        if ((element != total_bookkeeping_elements) && (sizes[element] != 0))
-        {
-            layout[element] = ALIGN_UP(layout[element], alignment[element]);
-        }
-    }
-}
-#ifdef USE_REGIONS
-bool gc_heap::on_used_changed (uint8_t* new_used)
-{
-#if defined(WRITE_BARRIER_CHECK) && !defined (SERVER_GC)
-    if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_BARRIERCHECK)
-    {
-        size_t shadow_covered = g_GCShadowEnd - g_GCShadow;
-        size_t used_heap_range = new_used - g_gc_lowest_address;
-        if (used_heap_range > shadow_covered)
-        {
-            size_t extra = used_heap_range - shadow_covered;
-            if (!GCToOSInterface::VirtualCommit (g_GCShadowEnd, extra))
-            {
-                _ASSERTE(!"Not enough memory to run HeapVerify level 2");
-                deleteGCShadow();
-            }
-            else
-            {
-                g_GCShadowEnd += extra;
-            }
-        }
-    }
-#endif //WRITE_BARRIER_CHECK && !SERVER_GC
-    if (new_used > bookkeeping_covered_committed)
-    {
-        bool speculative_commit_tried = false;
-#ifdef STRESS_REGIONS
-        if (gc_rand::get_rand(10) > 3)
-        {
-            dprintf (REGIONS_LOG, ("skipping speculative commit under stress regions"));
-            speculative_commit_tried = true;
-        }
-#endif
-        while (true)
-        {
-            uint8_t* new_bookkeeping_covered_committed = nullptr;
-            if (speculative_commit_tried)
-            {
-                new_bookkeeping_covered_committed = new_used;
-            }
-            else
-            {
-                uint64_t committed_size = (uint64_t)(bookkeeping_covered_committed - g_gc_lowest_address);
-                uint64_t total_size = (uint64_t)(g_gc_highest_address - g_gc_lowest_address);
-                assert (committed_size <= total_size);
-                assert (committed_size < (UINT64_MAX / 2));
-                uint64_t new_committed_size = min(committed_size * 2, total_size);
-                assert ((UINT64_MAX - new_committed_size) > (uint64_t)g_gc_lowest_address);
-                uint8_t* double_commit = g_gc_lowest_address + new_committed_size;
-                new_bookkeeping_covered_committed = max(double_commit, new_used);
-                dprintf (REGIONS_LOG, ("committed_size                           = %zd", committed_size));
-                dprintf (REGIONS_LOG, ("total_size                               = %zd", total_size));
-                dprintf (REGIONS_LOG, ("new_committed_size                       = %zd", new_committed_size));
-                dprintf (REGIONS_LOG, ("double_commit                            = %p", double_commit));
-            }
-            dprintf (REGIONS_LOG, ("bookkeeping_covered_committed     = %p", bookkeeping_covered_committed));
-            dprintf (REGIONS_LOG, ("new_bookkeeping_covered_committed = %p", new_bookkeeping_covered_committed));
-            if (inplace_commit_card_table (bookkeeping_covered_committed, new_bookkeeping_covered_committed))
-            {
-                bookkeeping_covered_committed = new_bookkeeping_covered_committed;
-                break;
-            }
-            else
-            {
-                if (new_bookkeeping_covered_committed == new_used)
-                {
-                    dprintf (REGIONS_LOG, ("The minimal commit for the GC bookkeeping data structure failed, giving up"));
-                    return false;
-                }
-                dprintf (REGIONS_LOG, ("The speculative commit for the GC bookkeeping data structure failed, retry for minimal commit"));
-                speculative_commit_tried = true;
-            }
-        }
-    }
-    return true;
-}
-bool gc_heap::get_card_table_commit_layout (uint8_t* from, uint8_t* to,
-                    uint8_t* commit_begins[total_bookkeeping_elements],
-                    size_t commit_sizes[total_bookkeeping_elements],
-                    size_t new_sizes[total_bookkeeping_elements])
-{
-    uint8_t* start = g_gc_lowest_address;
-    uint8_t* end = g_gc_highest_address;
-    bool initial_commit = (from == start);
-    bool additional_commit = !initial_commit && (to > from);
-    if (!initial_commit && !additional_commit)
-    {
-        return false;
-    }
-#ifdef DEBUG
-    size_t offsets[total_bookkeeping_elements + 1];
-    get_card_table_element_layout(start, end, offsets);
-    dprintf (REGIONS_LOG, ("layout"));
-    for (int i = card_table_element; i <= total_bookkeeping_elements; i++)
-    {
-        assert (offsets[i] == card_table_element_layout[i]);
-        dprintf (REGIONS_LOG, ("%zd", card_table_element_layout[i]));
-    }
-#endif //DEBUG
-    get_card_table_element_sizes (start, to, new_sizes);
-#ifdef DEBUG
-    dprintf (REGIONS_LOG, ("new_sizes"));
-    for (int i = card_table_element; i < total_bookkeeping_elements; i++)
-    {
-        dprintf (REGIONS_LOG, ("%zd", new_sizes[i]));
-    }
-    if (additional_commit)
-    {
-        size_t current_sizes[total_bookkeeping_elements];
-        get_card_table_element_sizes (start, from, current_sizes);
-        dprintf (REGIONS_LOG, ("old_sizes"));
-        for (int i = card_table_element; i < total_bookkeeping_elements; i++)
-        {
-            assert (current_sizes[i] == bookkeeping_sizes[i]);
-            dprintf (REGIONS_LOG, ("%zd", bookkeeping_sizes[i]));
-        }
-    }
-#endif //DEBUG
-    for (int i = card_table_element; i <= seg_mapping_table_element; i++)
-    {
-        uint8_t* required_begin = nullptr;
-        uint8_t* required_end = nullptr;
-        uint8_t* commit_begin = nullptr;
-        uint8_t* commit_end = nullptr;
-        if (initial_commit)
-        {
-            required_begin = bookkeeping_start + ((i == card_table_element) ? 0 : card_table_element_layout[i]);
-            required_end = bookkeeping_start + card_table_element_layout[i] + new_sizes[i];
-            commit_begin = align_lower_page(required_begin);
-        }
-        else
-        {
-            assert (additional_commit);
-            required_begin = bookkeeping_start + card_table_element_layout[i] + bookkeeping_sizes[i];
-            required_end = required_begin + new_sizes[i] - bookkeeping_sizes[i];
-            commit_begin = align_on_page(required_begin);
-        }
-        assert (required_begin <= required_end);
-        commit_end = align_on_page(required_end);
-        commit_end = min (commit_end, align_lower_page(bookkeeping_start + card_table_element_layout[i + 1]));
-        commit_begin = min (commit_begin, commit_end);
-        assert (commit_begin <= commit_end);
-        dprintf (REGIONS_LOG, ("required = [%p, %p), size = %zd", required_begin, required_end, required_end - required_begin));
-        dprintf (REGIONS_LOG, ("commit   = [%p, %p), size = %zd", commit_begin, commit_end, commit_end - commit_begin));
-        commit_begins[i] = commit_begin;
-        commit_sizes[i] = (size_t)(commit_end - commit_begin);
-    }
-    dprintf (REGIONS_LOG, ("---------------------------------------"));
-    return true;
-}
-bool gc_heap::inplace_commit_card_table (uint8_t* from, uint8_t* to)
-{
-    dprintf (REGIONS_LOG, ("inplace_commit_card_table(%p, %p), size = %zd", from, to, to - from));
-    uint8_t* start = g_gc_lowest_address;
-    uint8_t* end = g_gc_highest_address;
-    uint8_t* commit_begins[total_bookkeeping_elements];
-    size_t commit_sizes[total_bookkeeping_elements];
-    size_t new_sizes[total_bookkeeping_elements];
-    if (!get_card_table_commit_layout(from, to, commit_begins, commit_sizes, new_sizes))
-    {
-        return true;
-    }
-    int failed_commit = -1;
-    for (int i = card_table_element; i <= seg_mapping_table_element; i++)
-    {
-        bool succeed;
-        if (commit_sizes[i] > 0)
-        {
-            succeed = virtual_commit (commit_begins[i], commit_sizes[i], recorded_committed_bookkeeping_bucket);
-            if (!succeed)
-            {
-                failed_commit = i;
-                break;
-            }
-        }
-    }
-    if (failed_commit == -1)
-    {
-        for (int i = card_table_element; i < total_bookkeeping_elements; i++)
-        {
-            bookkeeping_sizes[i] = new_sizes[i];
-        }
-    }
-    else
-    {
-        for (int i = card_table_element; i < failed_commit; i++)
-        {
-            bool succeed;
-            if (commit_sizes[i] > 0)
-            {
-                succeed = virtual_decommit (commit_begins[i], commit_sizes[i], recorded_committed_bookkeeping_bucket);
-                assert (succeed);
-            }
-        }
-        return false;
-    }
-    return true;
-}
-#endif //USE_REGIONS
-uint32_t* gc_heap::make_card_table (uint8_t* start, uint8_t* end)
-{
-    assert (g_gc_lowest_address == start);
-    assert (g_gc_highest_address == end);
-    uint32_t virtual_reserve_flags = VirtualReserveFlags::None;
-#ifdef CARD_BUNDLE
-    if (can_use_write_watch_for_card_table())
-    {
-#ifndef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-        virtual_reserve_flags |= VirtualReserveFlags::WriteWatch;
-#endif
-    }
-#endif //CARD_BUNDLE
-    get_card_table_element_layout(start, end, card_table_element_layout);
-    size_t alloc_size = card_table_element_layout[total_bookkeeping_elements];
-    uint8_t* mem = (uint8_t*)GCToOSInterface::VirtualReserve (alloc_size, 0, virtual_reserve_flags);
-    bookkeeping_start = mem;
-    if (!mem)
-        return 0;
-    dprintf (2, ("Init - Card table alloc for %zd bytes: [%zx, %zx[",
-                 alloc_size, (size_t)mem, (size_t)(mem+alloc_size)));
-#ifdef USE_REGIONS
-    if (!inplace_commit_card_table (g_gc_lowest_address, global_region_allocator.get_left_used_unsafe()))
-    {
-        dprintf (1, ("Card table commit failed"));
-        GCToOSInterface::VirtualRelease (mem, alloc_size);
-        return 0;
-    }
-    bookkeeping_covered_committed = global_region_allocator.get_left_used_unsafe();
-#else
-    size_t commit_size = card_table_element_layout[seg_mapping_table_element + 1];
-    if (!virtual_commit (mem, commit_size, recorded_committed_bookkeeping_bucket))
-    {
-        dprintf (1, ("Card table commit failed"));
-        GCToOSInterface::VirtualRelease (mem, alloc_size);
-        return 0;
-    }
-#endif //USE_REGIONS
-    uint32_t* ct = (uint32_t*)(mem + card_table_element_layout[card_table_element]);
-    card_table_refcount (ct) = 0;
-    card_table_lowest_address (ct) = start;
-    card_table_highest_address (ct) = end;
-    card_table_brick_table (ct) = (short*)(mem + card_table_element_layout[brick_table_element]);
-    card_table_size (ct) = alloc_size;
-    card_table_next (ct) = 0;
-#ifdef CARD_BUNDLE
-    card_table_card_bundle_table (ct) = (uint32_t*)(mem + card_table_element_layout[card_bundle_table_element]);
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-    g_gc_card_bundle_table = translate_card_bundle_table(card_table_card_bundle_table(ct), g_gc_lowest_address);
-#endif
-#endif //CARD_BUNDLE
-#if defined(FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP) && defined (BACKGROUND_GC)
-    if (gc_can_use_concurrent)
-    {
-        SoftwareWriteWatch::InitializeUntranslatedTable(mem + card_table_element_layout[software_write_watch_table_element], start);
-    }
-#endif //FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP && BACKGROUND_GC
-#ifdef USE_REGIONS
-    map_region_to_generation = (region_info*)(mem + card_table_element_layout[region_to_generation_table_element]);
-    map_region_to_generation_skewed = map_region_to_generation - size_region_to_generation_table_of (0, g_gc_lowest_address);
-#endif //USE_REGIONS
-    seg_mapping_table = (seg_mapping*)(mem + card_table_element_layout[seg_mapping_table_element]);
-    seg_mapping_table = (seg_mapping*)((uint8_t*)seg_mapping_table -
-                                        size_seg_mapping_table_of (0, (align_lower_segment (g_gc_lowest_address))));
-#ifdef BACKGROUND_GC
-    if (gc_can_use_concurrent)
-        card_table_mark_array (ct) = (uint32_t*)(mem + card_table_element_layout[mark_array_element]);
-    else
-        card_table_mark_array (ct) = NULL;
-#endif //BACKGROUND_GC
-    return translate_card_table(ct);
-}
-void gc_heap::set_fgm_result (failure_get_memory f, size_t s, BOOL loh_p)
-{
-#ifdef MULTIPLE_HEAPS
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps [hn];
-        hp->fgm_result.set_fgm (f, s, loh_p);
-    }
-#else //MULTIPLE_HEAPS
-    fgm_result.set_fgm (f, s, loh_p);
-#endif //MULTIPLE_HEAPS
-}
-#ifndef USE_REGIONS
-int gc_heap::grow_brick_card_tables (uint8_t* start,
-                                     uint8_t* end,
-                                     size_t size,
-                                     heap_segment* new_seg,
-                                     gc_heap* hp,
-                                     BOOL uoh_p)
-{
-    uint8_t* la = g_gc_lowest_address;
-    uint8_t* ha = g_gc_highest_address;
-    uint8_t* saved_g_lowest_address = min (start, g_gc_lowest_address);
-    uint8_t* saved_g_highest_address = max (end, g_gc_highest_address);
-    seg_mapping* new_seg_mapping_table = nullptr;
-#ifdef BACKGROUND_GC
-    size_t logging_ma_commit_size = size_mark_array_of (0, (uint8_t*)size);
-#endif //BACKGROUND_GC
-    if ((la != saved_g_lowest_address ) || (ha != saved_g_highest_address))
-    {
-        {
-            uint8_t* top = (uint8_t*)0 + Align (GCToOSInterface::GetVirtualMemoryLimit());
-            if (top < saved_g_highest_address)
-            {
-                top = saved_g_highest_address;
-            }
-            size_t ps = ha-la;
-#ifdef HOST_64BIT
-            if (ps > (uint64_t)200*1024*1024*1024)
-                ps += (uint64_t)100*1024*1024*1024;
-            else
-#endif // HOST_64BIT
-                ps *= 2;
-            if (saved_g_lowest_address < g_gc_lowest_address)
-            {
-                if (ps > (size_t)g_gc_lowest_address)
-                    saved_g_lowest_address = (uint8_t*)(size_t)OS_PAGE_SIZE;
-                else
-                {
-                    assert (((size_t)g_gc_lowest_address - ps) >= OS_PAGE_SIZE);
-                    saved_g_lowest_address = min (saved_g_lowest_address, (g_gc_lowest_address - ps));
-                }
-            }
-            if (saved_g_highest_address > g_gc_highest_address)
-            {
-                saved_g_highest_address = max ((saved_g_lowest_address + ps), saved_g_highest_address);
-                if (saved_g_highest_address > top)
-                    saved_g_highest_address = top;
-            }
-        }
-        dprintf (GC_TABLE_LOG, ("Growing card table [%zx, %zx[",
-                                (size_t)saved_g_lowest_address,
-                                (size_t)saved_g_highest_address));
-        bool write_barrier_updated = false;
-        uint32_t virtual_reserve_flags = VirtualReserveFlags::None;
-        uint32_t* saved_g_card_table = g_gc_card_table;
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-        uint32_t* saved_g_card_bundle_table = g_gc_card_bundle_table;
-#endif
-        get_card_table_element_layout(saved_g_lowest_address, saved_g_highest_address, card_table_element_layout);
-        size_t cb = 0;
-        uint32_t* ct = 0;
-        uint32_t* translated_ct = 0;
-#ifdef CARD_BUNDLE
-        if (can_use_write_watch_for_card_table())
-        {
-            cb = size_card_bundle_of (saved_g_lowest_address, saved_g_highest_address);
-#ifndef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-            virtual_reserve_flags |= VirtualReserveFlags::WriteWatch;
-#endif
-        }
-#endif //CARD_BUNDLE
-        size_t alloc_size = card_table_element_layout[total_bookkeeping_elements];
-        uint8_t* mem = (uint8_t*)GCToOSInterface::VirtualReserve (alloc_size, 0, virtual_reserve_flags);
-        if (!mem)
-        {
-            set_fgm_result (fgm_grow_table, alloc_size, uoh_p);
-            goto fail;
-        }
-        dprintf (GC_TABLE_LOG, ("Table alloc for %zd bytes: [%zx, %zx[",
-                                 alloc_size, (size_t)mem, (size_t)((uint8_t*)mem+alloc_size)));
-        {
-            size_t commit_size = card_table_element_layout[seg_mapping_table_element + 1];
-            if (!virtual_commit (mem, commit_size, recorded_committed_bookkeeping_bucket))
-            {
-                dprintf (GC_TABLE_LOG, ("Table commit failed"));
-                set_fgm_result (fgm_commit_table, commit_size, uoh_p);
-                goto fail;
-            }
-        }
-        ct = (uint32_t*)(mem + card_table_element_layout[card_table_element]);
-        card_table_refcount (ct) = 0;
-        card_table_lowest_address (ct) = saved_g_lowest_address;
-        card_table_highest_address (ct) = saved_g_highest_address;
-        card_table_next (ct) = &g_gc_card_table[card_word (gcard_of (la))];
-/*
-        memclr ((uint8_t*)ct,
-                (((saved_g_highest_address - saved_g_lowest_address)*sizeof (uint32_t) /
-                  (card_size * card_word_width))
-                 + sizeof (uint32_t)));
-*/
-        card_table_brick_table (ct) = (short*)(mem + card_table_element_layout[brick_table_element]);
-#ifdef CARD_BUNDLE
-        card_table_card_bundle_table (ct) = (uint32_t*)(mem + card_table_element_layout[card_bundle_table_element]);
-        memset(card_table_card_bundle_table (ct), 0xFF, cb);
-#endif //CARD_BUNDLE
-        new_seg_mapping_table = (seg_mapping*)(mem + card_table_element_layout[seg_mapping_table_element]);
-        new_seg_mapping_table = (seg_mapping*)((uint8_t*)new_seg_mapping_table -
-                                            size_seg_mapping_table_of (0, (align_lower_segment (saved_g_lowest_address))));
-        memcpy(&new_seg_mapping_table[seg_mapping_word_of(g_gc_lowest_address)],
-            &seg_mapping_table[seg_mapping_word_of(g_gc_lowest_address)],
-            size_seg_mapping_table_of(g_gc_lowest_address, g_gc_highest_address));
-#ifdef BACKGROUND_GC
-        if(gc_can_use_concurrent)
-            card_table_mark_array (ct) = (uint32_t*)(mem + card_table_element_layout[mark_array_element]);
-        else
-            card_table_mark_array (ct) = NULL;
-#endif //BACKGROUND_GC
-        translated_ct = translate_card_table (ct);
-        dprintf (GC_TABLE_LOG, ("card table: %zx(translated: %zx), seg map: %zx, mark array: %zx",
-            (size_t)ct, (size_t)translated_ct, (size_t)new_seg_mapping_table, (size_t)card_table_mark_array (ct)));
-#ifdef BACKGROUND_GC
-        if (hp->is_bgc_in_progress())
-        {
-            dprintf (GC_TABLE_LOG, ("new low: %p, new high: %p, latest mark array is %p(translate: %p)",
-                                    saved_g_lowest_address, saved_g_highest_address,
-                                    card_table_mark_array (ct),
-                                    translate_mark_array (card_table_mark_array (ct))));
-            uint32_t* new_mark_array = (uint32_t*)((uint8_t*)card_table_mark_array (ct) - size_mark_array_of (0, saved_g_lowest_address));
-            if (!commit_new_mark_array_global (new_mark_array))
-            {
-                dprintf (GC_TABLE_LOG, ("failed to commit portions in the mark array for existing segments"));
-                set_fgm_result (fgm_commit_table, logging_ma_commit_size, uoh_p);
-                goto fail;
-            }
-            if (!commit_mark_array_new_seg (hp, new_seg, translated_ct, saved_g_lowest_address))
-            {
-                dprintf (GC_TABLE_LOG, ("failed to commit mark array for the new seg"));
-                set_fgm_result (fgm_commit_table, logging_ma_commit_size, uoh_p);
-                goto fail;
-            }
-        }
-        else
-        {
-            clear_commit_flag_global();
-        }
-#endif //BACKGROUND_GC
-#if defined(FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP) && defined(BACKGROUND_GC)
-        if (gc_can_use_concurrent)
-        {
-            bool is_runtime_suspended = GCToEEInterface::IsGCThread();
-            if (!is_runtime_suspended)
-            {
-                suspend_EE();
-            }
-            g_gc_card_table = translated_ct;
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-            g_gc_card_bundle_table = translate_card_bundle_table(card_table_card_bundle_table(ct), saved_g_lowest_address);
-#endif
-            SoftwareWriteWatch::SetResizedUntranslatedTable(
-                mem + card_table_element_layout[software_write_watch_table_element],
-                saved_g_lowest_address,
-                saved_g_highest_address);
-            seg_mapping_table = new_seg_mapping_table;
-            g_gc_lowest_address = saved_g_lowest_address;
-            g_gc_highest_address = saved_g_highest_address;
-            stomp_write_barrier_resize(true, la != saved_g_lowest_address);
-            write_barrier_updated = true;
-            if (!is_runtime_suspended)
-            {
-                restart_EE();
-            }
-        }
-        else
-#endif //FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP && BACKGROUND_GC
-        {
-            g_gc_card_table = translated_ct;
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-            g_gc_card_bundle_table = translate_card_bundle_table(card_table_card_bundle_table(ct), saved_g_lowest_address);
-#endif
-        }
-        if (!write_barrier_updated)
-        {
-            seg_mapping_table = new_seg_mapping_table;
-            GCToOSInterface::FlushProcessWriteBuffers();
-            g_gc_lowest_address = saved_g_lowest_address;
-            g_gc_highest_address = saved_g_highest_address;
-            stomp_write_barrier_resize(GCToEEInterface::IsGCThread(), la != saved_g_lowest_address);
-        }
-        return 0;
-fail:
-        if (mem)
-        {
-            assert(g_gc_card_table == saved_g_card_table);
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-            assert(g_gc_card_bundle_table  == saved_g_card_bundle_table);
-#endif
-            if (!GCToOSInterface::VirtualRelease (mem, alloc_size))
-            {
-                dprintf (GC_TABLE_LOG, ("GCToOSInterface::VirtualRelease failed"));
-                assert (!"release failed");
-            }
-        }
-        return -1;
-    }
-    else
-    {
-#ifdef BACKGROUND_GC
-        if (hp->is_bgc_in_progress())
-        {
-            dprintf (GC_TABLE_LOG, ("in range new seg %p, mark_array is %p", new_seg, hp->mark_array));
-            if (!commit_mark_array_new_seg (hp, new_seg))
-            {
-                dprintf (GC_TABLE_LOG, ("failed to commit mark array for the new seg in range"));
-                set_fgm_result (fgm_commit_table, logging_ma_commit_size, uoh_p);
-                return -1;
-            }
-        }
-#endif //BACKGROUND_GC
-    }
-    return 0;
-}
-#endif //!USE_REGIONS
-void gc_heap::copy_brick_card_range (uint8_t* la, uint32_t* old_card_table,
-                                     short* old_brick_table,
-                                     uint8_t* start, uint8_t* end)
-{
-    ptrdiff_t brick_offset = brick_of (start) - brick_of (la);
-    dprintf (2, ("copying tables for range [%zx %zx[", (size_t)start, (size_t)end));
-    short* brick_start = &brick_table [brick_of (start)];
-    if (old_brick_table)
-    {
-        memcpy (brick_start, &old_brick_table[brick_offset],
-                size_brick_of (start, end));
-    }
-    uint32_t* old_ct = &old_card_table[card_word (card_of (la))];
-#ifdef BACKGROUND_GC
-    if (gc_heap::background_running_p())
-    {
-        uint32_t* old_mark_array = card_table_mark_array (old_ct);
-        if ((card_table_highest_address (old_ct) >= start) &&
-            (card_table_lowest_address (old_ct) <= end))
-        {
-            if ((background_saved_highest_address >= start) &&
-                (background_saved_lowest_address <= end))
-            {
-                uint8_t* m_start = max (background_saved_lowest_address, start);
-                uint8_t* m_end = min (background_saved_highest_address, end);
-                memcpy (&mark_array[mark_word_of (m_start)],
-                        &old_mark_array[mark_word_of (m_start) - mark_word_of (la)],
-                        size_mark_array_of (m_start, m_end));
-            }
-        }
-        else
-        {
-            assert (old_brick_table == 0);
-        }
-    }
-#endif //BACKGROUND_GC
-    uint32_t* ct = card_table_next (&card_table[card_word (card_of(lowest_address))]);
-    assert (ct);
-    while (card_table_next (old_ct) != ct)
-    {
-        if ((card_table_highest_address (ct) >= end) &&
-            (card_table_lowest_address (ct) <= start))
-        {
-            size_t start_word = card_word (card_of (start));
-            uint32_t* dest = &card_table[start_word];
-            uint32_t* src = &((translate_card_table (ct))[start_word]);
-            ptrdiff_t count = count_card_of (start, end);
-            for (int x = 0; x < count; x++)
-            {
-                *dest |= *src;
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-                if (*src != 0)
-                {
-                    card_bundle_set(cardw_card_bundle(start_word+x));
-                }
-#endif
-                dest++;
-                src++;
-            }
-        }
-        ct = card_table_next (ct);
-    }
-}
-void gc_heap::copy_brick_card_table()
-{
-    uint32_t* old_card_table = card_table;
-    short* old_brick_table = brick_table;
-    uint8_t* la = lowest_address;
-#ifdef _DEBUG
-    uint8_t* ha = highest_address;
-    assert (la == card_table_lowest_address (&old_card_table[card_word (card_of (la))]));
-    assert (ha == card_table_highest_address (&old_card_table[card_word (card_of (la))]));
-#endif //_DEBUG
-    /* todo: Need a global lock for this */
-    uint32_t* ct = &g_gc_card_table[card_word (gcard_of (g_gc_lowest_address))];
-    own_card_table (ct);
-    card_table = translate_card_table (ct);
-    bookkeeping_start = (uint8_t*)ct - sizeof(card_table_info);
-    card_table_size(ct) = card_table_element_layout[total_bookkeeping_elements];
-    /* End of global lock */
-    highest_address = card_table_highest_address (ct);
-    lowest_address = card_table_lowest_address (ct);
-    brick_table = card_table_brick_table (ct);
-#ifdef BACKGROUND_GC
-    if (gc_can_use_concurrent)
-    {
-        mark_array = translate_mark_array (card_table_mark_array (ct));
-        assert (mark_word_of (g_gc_highest_address) ==
-            mark_word_of (align_on_mark_word (g_gc_highest_address)));
-    }
-    else
-        mark_array = NULL;
-#endif //BACKGROUND_GC
-#ifdef CARD_BUNDLE
-    card_bundle_table = translate_card_bundle_table (card_table_card_bundle_table (ct), g_gc_lowest_address);
-    assert (&card_bundle_table [card_bundle_word (cardw_card_bundle (card_word (card_of (g_gc_lowest_address))))] ==
-            card_table_card_bundle_table (ct));
-    if (card_bundles_enabled())
-    {
-        card_bundles_set (cardw_card_bundle (card_word (card_of (lowest_address))),
-                          cardw_card_bundle (align_cardw_on_bundle (card_word (card_of (highest_address)))));
-    }
-#ifdef MULTIPLE_HEAPS
-    uint64_t th = (uint64_t)MH_TH_CARD_BUNDLE*gc_heap::n_heaps;
-#else
-    uint64_t th = (uint64_t)SH_TH_CARD_BUNDLE;
-#endif //MULTIPLE_HEAPS
-    if (reserved_memory >= th)
-    {
-        enable_card_bundles();
-    }
-#endif //CARD_BUNDLE
-    for (int i = get_start_generation_index(); i < total_generation_count; i++)
-    {
-        heap_segment* seg = generation_start_segment (generation_of (i));
-        while (seg)
-        {
-            if (heap_segment_read_only_p (seg) && !heap_segment_in_range_p (seg))
-            {
-                if ((heap_segment_reserved (seg) > lowest_address) &&
-                    (heap_segment_mem (seg) < highest_address))
-                {
-                    set_ro_segment_in_range (seg);
-                }
-            }
-            else
-            {
-                uint8_t* end = align_on_page (heap_segment_allocated (seg));
-                copy_brick_card_range (la, old_card_table,
-                    (i < uoh_start_generation) ? old_brick_table : NULL,
-                    align_lower_page (heap_segment_mem (seg)),
-                    end);
-            }
-            seg = heap_segment_next (seg);
-        }
-    }
-    release_card_table (&old_card_table[card_word (card_of(la))]);
-}
-#ifdef FEATURE_BASICFREEZE
-BOOL gc_heap::insert_ro_segment (heap_segment* seg)
-{
-#ifdef FEATURE_EVENT_TRACE
-    if (!use_frozen_segments_p)
-        use_frozen_segments_p = true;
-#endif //FEATURE_EVENT_TRACE
-    enter_spin_lock (&gc_heap::gc_lock);
-    if (!gc_heap::seg_table->ensure_space_for_insert ()
-#ifdef BACKGROUND_GC
-        || (is_bgc_in_progress() && !commit_mark_array_new_seg(__this, seg))
-#endif //BACKGROUND_GC
-        )
-    {
-        leave_spin_lock(&gc_heap::gc_lock);
-        return FALSE;
-    }
-    generation* gen2 = generation_of (max_generation);
-    heap_segment* oldhead = generation_start_segment (gen2);
-    heap_segment_next (seg) = oldhead;
-    generation_start_segment (gen2) = seg;
-#ifdef USE_REGIONS
-    dprintf (REGIONS_LOG, ("setting gen2 start seg to %zx(%p)->%p",
-        (size_t)seg, heap_segment_mem (seg), heap_segment_mem (oldhead)));
-    if (generation_tail_ro_region (gen2) == 0)
-    {
-        dprintf (REGIONS_LOG, ("setting gen2 tail ro -> %p", heap_segment_mem (seg)));
-        generation_tail_ro_region (gen2) = seg;
-    }
-#endif //USE_REGIONS
-    seg_table->insert (heap_segment_mem(seg), (size_t)seg);
-    seg_mapping_table_add_ro_segment (seg);
-    if ((heap_segment_reserved (seg) > lowest_address) &&
-        (heap_segment_mem (seg) < highest_address))
-    {
-        set_ro_segment_in_range (seg);
-    }
-    FIRE_EVENT(GCCreateSegment_V1, heap_segment_mem(seg), (size_t)(heap_segment_reserved (seg) - heap_segment_mem(seg)), gc_etw_segment_read_only_heap);
-    leave_spin_lock (&gc_heap::gc_lock);
-    return TRUE;
-}
-void gc_heap::update_ro_segment (heap_segment* seg, uint8_t* allocated, uint8_t* committed)
-{
-    enter_spin_lock (&gc_heap::gc_lock);
-    assert (heap_segment_read_only_p (seg));
-    assert (allocated <= committed);
-    assert (committed <= heap_segment_reserved (seg));
-    heap_segment_allocated (seg) = allocated;
-    heap_segment_committed (seg) = committed;
-    leave_spin_lock (&gc_heap::gc_lock);
-}
-void gc_heap::remove_ro_segment (heap_segment* seg)
-{
-#ifdef BACKGROUND_GC
-    if (gc_can_use_concurrent)
-    {
-        if ((seg->flags & heap_segment_flags_ma_committed) || (seg->flags & heap_segment_flags_ma_pcommitted))
-        {
-            seg_clear_mark_array_bits_soh (seg);
-        }
-    }
-#endif //BACKGROUND_GC
-    enter_spin_lock (&gc_heap::gc_lock);
-    seg_table->remove (heap_segment_mem (seg));
-    seg_mapping_table_remove_ro_segment (seg);
-    generation* gen2 = generation_of (max_generation);
-#ifdef USE_REGIONS
-    if (generation_tail_ro_region (gen2) == seg)
-    {
-        generation_tail_ro_region (gen2) = 0;
-    }
-#endif //USE_REGIONS
-    heap_segment* curr_seg = generation_start_segment (gen2);
-    heap_segment* prev_seg = NULL;
-    while (curr_seg && curr_seg != seg)
-    {
-        prev_seg = curr_seg;
-        curr_seg = heap_segment_next (curr_seg);
-    }
-    assert (curr_seg == seg);
-    if (prev_seg)
-        heap_segment_next (prev_seg) = heap_segment_next (curr_seg);
-    else
-        generation_start_segment (gen2) = heap_segment_next (curr_seg);
-    leave_spin_lock (&gc_heap::gc_lock);
-}
-#endif //FEATURE_BASICFREEZE
-BOOL gc_heap::set_ro_segment_in_range (heap_segment* seg)
-{
-    seg->flags |= heap_segment_flags_inrange;
-    ro_segments_in_range = TRUE;
-    return TRUE;
-}
-uint8_t** make_mark_list (size_t size)
-{
-    uint8_t** mark_list = new (nothrow) uint8_t* [size];
-    return mark_list;
-}
-#define swap(a,b){uint8_t* t; t = a; a = b; b = t;}
-void verify_qsort_array (uint8_t* *low, uint8_t* *high)
-{
-    uint8_t **i = 0;
-    for (i = low+1; i <= high; i++)
-    {
-        if (*i < *(i-1))
-        {
-            FATAL_GC_ERROR();
-        }
-    }
-}
-#ifndef USE_INTROSORT
-void qsort1( uint8_t* *low, uint8_t* *high, unsigned int depth)
-{
-    if (((low + 16) >= high) || (depth > 100))
-    {
-        uint8_t **i, **j;
-        for (i = low+1; i <= high; i++)
-        {
-            uint8_t* val = *i;
-            for (j=i;j >low && val<*(j-1);j--)
-            {
-                *j=*(j-1);
-            }
-            *j=val;
-        }
-    }
-    else
-    {
-        uint8_t *pivot, **left, **right;
-        if (*(low+((high-low)/2)) < *low)
-            swap (*(low+((high-low)/2)), *low);
-        if (*high < *low)
-            swap (*low, *high);
-        if (*high < *(low+((high-low)/2)))
-            swap (*(low+((high-low)/2)), *high);
-        swap (*(low+((high-low)/2)), *(high-1));
-        pivot =  *(high-1);
-        left = low; right = high-1;
-        while (1) {
-            while (*(--right) > pivot);
-            while (*(++left)  < pivot);
-            if (left < right)
-            {
-                swap(*left, *right);
-            }
-            else
-                break;
-        }
-        swap (*left, *(high-1));
-        qsort1(low, left-1, depth+1);
-        qsort1(left+1, high, depth+1);
-    }
-}
-#endif //USE_INTROSORT
-void rqsort1( uint8_t* *low, uint8_t* *high)
-{
-    if ((low + 16) >= high)
-    {
-        uint8_t **i, **j;
-        for (i = low+1; i <= high; i++)
-        {
-            uint8_t* val = *i;
-            for (j=i;j >low && val>*(j-1);j--)
-            {
-                *j=*(j-1);
-            }
-            *j=val;
-        }
-    }
-    else
-    {
-        uint8_t *pivot, **left, **right;
-        if (*(low+((high-low)/2)) > *low)
-            swap (*(low+((high-low)/2)), *low);
-        if (*high > *low)
-            swap (*low, *high);
-        if (*high > *(low+((high-low)/2)))
-            swap (*(low+((high-low)/2)), *high);
-        swap (*(low+((high-low)/2)), *(high-1));
-        pivot =  *(high-1);
-        left = low; right = high-1;
-        while (1) {
-            while (*(--right) < pivot);
-            while (*(++left)  > pivot);
-            if (left < right)
-            {
-                swap(*left, *right);
-            }
-            else
-                break;
-        }
-        swap (*left, *(high-1));
-        rqsort1(low, left-1);
-        rqsort1(left+1, high);
-    }
-}
-#if defined(USE_INTROSORT) || defined(USE_VXSORT)
-class introsort
-{
-private:
-    static const int size_threshold = 64;
-    static const int max_depth = 100;
-inline static void swap_elements(uint8_t** i,uint8_t** j)
-    {
-        uint8_t* t=*i;
-        *i=*j;
-        *j=t;
-    }
-public:
-    static void sort (uint8_t** begin, uint8_t** end, int ignored)
-    {
-        ignored = 0;
-        introsort_loop (begin, end, max_depth);
-        insertionsort (begin, end);
-    }
-private:
-    static void introsort_loop (uint8_t** lo, uint8_t** hi, int depth_limit)
-    {
-        while (hi-lo >= size_threshold)
-        {
-            if (depth_limit == 0)
-            {
-                heapsort (lo, hi);
-                return;
-            }
-            uint8_t** p=median_partition (lo, hi);
-            depth_limit=depth_limit-1;
-            introsort_loop (p, hi, depth_limit);
-            hi=p-1;
-        }
-    }
-    static uint8_t** median_partition (uint8_t** low, uint8_t** high)
-    {
-        uint8_t *pivot, **left, **right;
-        if (*(low+((high-low)/2)) < *low)
-            swap_elements ((low+((high-low)/2)), low);
-        if (*high < *low)
-            swap_elements (low, high);
-        if (*high < *(low+((high-low)/2)))
-            swap_elements ((low+((high-low)/2)), high);
-        swap_elements ((low+((high-low)/2)), (high-1));
-        pivot =  *(high-1);
-        left = low; right = high-1;
-        while (1) {
-            while (*(--right) > pivot);
-            while (*(++left)  < pivot);
-            if (left < right)
-            {
-                swap_elements(left, right);
-            }
-            else
-                break;
-        }
-        swap_elements (left, (high-1));
-        return left;
-    }
-    static void insertionsort (uint8_t** lo, uint8_t** hi)
-    {
-        for (uint8_t** i=lo+1; i <= hi; i++)
-        {
-            uint8_t** j = i;
-            uint8_t* t = *i;
-            while((j > lo) && (t <*(j-1)))
-            {
-                *j = *(j-1);
-                j--;
-            }
-            *j = t;
-        }
-    }
-    static void heapsort (uint8_t** lo, uint8_t** hi)
-    {
-        size_t n = hi - lo + 1;
-        for (size_t i=n / 2; i >= 1; i--)
-        {
-            downheap (i,n,lo);
-        }
-        for (size_t i = n; i > 1; i--)
-        {
-            swap_elements (lo, lo + i - 1);
-            downheap(1, i - 1,  lo);
-        }
-    }
-    static void downheap (size_t i, size_t n, uint8_t** lo)
-    {
-        uint8_t* d = *(lo + i - 1);
-        size_t child;
-        while (i <= n / 2)
-        {
-            child = 2*i;
-            if (child < n && *(lo + child - 1)<(*(lo + child)))
-            {
-                child++;
-            }
-            if (!(d<*(lo + child - 1)))
-            {
-                break;
-            }
-            *(lo + i - 1) = *(lo + child - 1);
-            i = child;
-        }
-        *(lo + i - 1) = d;
-    }
-};
-#endif //defined(USE_INTROSORT) || defined(USE_VXSORT)
-#ifdef USE_VXSORT
-static void do_vxsort (uint8_t** item_array, ptrdiff_t item_count, uint8_t* range_low, uint8_t* range_high)
-{
-    const size_t AVX2_THRESHOLD_SIZE = 8 * 1024;
-    const size_t AVX512F_THRESHOLD_SIZE = 128 * 1024;
-    if (item_count <= 1)
-        return;
-    if (IsSupportedInstructionSet (InstructionSet::AVX2) && (item_count > AVX2_THRESHOLD_SIZE))
-    {
-        dprintf(3, ("Sorting mark lists"));
-        if (IsSupportedInstructionSet (InstructionSet::AVX512F) && (item_count > AVX512F_THRESHOLD_SIZE))
-        {
-            do_vxsort_avx512 (item_array, &item_array[item_count - 1], range_low, range_high);
-        }
-        else
-        {
-            do_vxsort_avx2 (item_array, &item_array[item_count - 1], range_low, range_high);
-        }
-    }
-    else
-    {
-        dprintf (3, ("Sorting mark lists"));
-        introsort::sort (item_array, &item_array[item_count - 1], 0);
-    }
-#ifdef _DEBUG
-    for (ptrdiff_t i = 0; i < item_count - 1; i++)
-    {
-        assert (item_array[i] <= item_array[i + 1]);
-    }
-    assert ((range_low <= item_array[0]) && (item_array[item_count - 1] <= range_high));
-#endif
-}
-#endif //USE_VXSORT
-#ifdef MULTIPLE_HEAPS
-static size_t target_mark_count_for_heap (size_t total_mark_count, int heap_count, int heap_number)
-{
-    size_t average_mark_count = total_mark_count / heap_count;
-    size_t remaining_mark_count = total_mark_count - (average_mark_count * heap_count);
-    if (heap_number == (heap_count - 1))
-        return (average_mark_count + remaining_mark_count);
-    else
-        return average_mark_count;
-}
-NOINLINE
-uint8_t** gc_heap::equalize_mark_lists (size_t total_mark_list_size)
-{
-    size_t local_mark_count[MAX_SUPPORTED_CPUS];
-    size_t total_mark_count = 0;
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap* hp = g_heaps[i];
-        size_t mark_count = hp->mark_list_index - hp->mark_list;
-        local_mark_count[i] = mark_count;
-        total_mark_count += mark_count;
-    }
-    assert(total_mark_count == total_mark_list_size);
-    size_t this_target_mark_count = target_mark_count_for_heap (total_mark_count, n_heaps, heap_number);
-    if (local_mark_count[heap_number] >= this_target_mark_count)
-        return (mark_list + this_target_mark_count);
-    int surplus_heap_index = 0;
-    for (int deficit_heap_index = 0; deficit_heap_index <= heap_number; deficit_heap_index++)
-    {
-        size_t deficit_target_mark_count = target_mark_count_for_heap (total_mark_count, n_heaps, deficit_heap_index);
-        if (local_mark_count[deficit_heap_index] >= deficit_target_mark_count)
-            continue;
-        while ((surplus_heap_index < n_heaps) && (local_mark_count[deficit_heap_index] < deficit_target_mark_count))
-        {
-            size_t deficit = deficit_target_mark_count - local_mark_count[deficit_heap_index];
-            size_t surplus_target_mark_count = target_mark_count_for_heap(total_mark_count, n_heaps, surplus_heap_index);
-            if (local_mark_count[surplus_heap_index] > surplus_target_mark_count)
-            {
-                size_t surplus = local_mark_count[surplus_heap_index] - surplus_target_mark_count;
-                size_t amount_to_transfer = min(deficit, surplus);
-                local_mark_count[surplus_heap_index] -= amount_to_transfer;
-                if (deficit_heap_index == heap_number)
-                {
-                    memcpy(&g_heaps[deficit_heap_index]->mark_list[local_mark_count[deficit_heap_index]],
-                           &g_heaps[surplus_heap_index]->mark_list[local_mark_count[surplus_heap_index]],
-                           (amount_to_transfer*sizeof(mark_list[0])));
-                }
-                local_mark_count[deficit_heap_index] += amount_to_transfer;
-            }
-            else
-            {
-                surplus_heap_index++;
-            }
-        }
-    }
-    return (mark_list + local_mark_count[heap_number]);
-}
-NOINLINE
-size_t gc_heap::sort_mark_list()
-{
-    if ((settings.condemned_generation >= max_generation)
-#ifdef USE_REGIONS
-      || (g_mark_list_piece == nullptr)
-#endif //USE_REGIONS
-        )
-    {
-        mark_list_index = mark_list_end + 1;
-        return 0;
-    }
-    if (mark_list_index > mark_list_end)
-    {
-        dprintf (2, ("h%d sort_mark_list overflow", heap_number));
-        mark_list_overflow = true;
-        return 0;
-    }
-    for (int i = 0; i < n_heaps; i++)
-    {
-        if (g_heaps[i]->mark_list_index > g_heaps[i]->mark_list_end)
-        {
-            mark_list_index = mark_list_end + 1;
-            dprintf (2, ("h%d sort_mark_list: detected overflow on heap %d", heap_number, i));
-            return 0;
-        }
-    }
-    size_t total_mark_list_size = 0;
-    size_t total_ephemeral_size = 0;
-    uint8_t* low = (uint8_t*)~0;
-    uint8_t* high = 0;
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap* hp = g_heaps[i];
-        total_mark_list_size += (hp->mark_list_index - hp->mark_list);
-#ifdef USE_REGIONS
-        for (int gen_num = settings.condemned_generation; gen_num >= 0; gen_num--)
-        {
-            generation* gen = hp->generation_of (gen_num);
-            for (heap_segment* seg = generation_start_segment (gen); seg != nullptr; seg = heap_segment_next (seg))
-            {
-                size_t ephemeral_size = heap_segment_allocated (seg) - heap_segment_mem (seg);
-                total_ephemeral_size += ephemeral_size;
-                low = min (low, heap_segment_mem (seg));
-                high = max (high, heap_segment_allocated (seg));
-            }
-        }
-#else //USE_REGIONS
-        size_t ephemeral_size = heap_segment_allocated (hp->ephemeral_heap_segment) - hp->gc_low;
-        total_ephemeral_size += ephemeral_size;
-        low = min (low, hp->gc_low);
-        high = max (high, heap_segment_allocated (hp->ephemeral_heap_segment));
-#endif //USE_REGIONS
-    }
-    if (total_mark_list_size > (total_ephemeral_size / 256))
-    {
-        mark_list_index = mark_list_end + 1;
-        dprintf (2, ("h%d total mark list %zd is too large > (%zd / 256), don't use",
-            heap_number, total_mark_list_size, total_ephemeral_size));
-        mark_list_overflow = false;
-        return 0;
-    }
-    uint8_t **local_mark_list_index = equalize_mark_lists (total_mark_list_size);
-#ifdef USE_VXSORT
-    ptrdiff_t item_count = local_mark_list_index - mark_list;
-#if defined(_DEBUG) || defined(WRITE_SORT_DATA)
-    uint8_t** mark_list_copy = &g_mark_list_copy[heap_number * mark_list_size];
-    uint8_t** mark_list_copy_index = &mark_list_copy[item_count];
-    for (ptrdiff_t i = 0; i < item_count; i++)
-    {
-        uint8_t* item = mark_list[i];
-        assert ((low <= item) && (item < high));
-        mark_list_copy[i] = item;
-    }
-#endif // _DEBUG || WRITE_SORT_DATA
-    do_vxsort (mark_list, item_count, low, high);
-#ifdef WRITE_SORT_DATA
-    char file_name[256];
-    sprintf_s (file_name, ARRAY_SIZE(file_name), "sort_data_gc%d_heap%d", settings.gc_index, heap_number);
-    FILE* f;
-    errno_t err = fopen_s (&f, file_name, "wb");
-    if (err == 0)
-    {
-        size_t magic = 'SDAT';
-        if (fwrite (&magic, sizeof(magic), 1, f) != 1)
-            dprintf (3, ("fwrite failed\n"));
-        if (fwrite (&elapsed_cycles, sizeof(elapsed_cycles), 1, f) != 1)
-            dprintf (3, ("fwrite failed\n"));
-        if (fwrite (&low, sizeof(low), 1, f) != 1)
-            dprintf (3, ("fwrite failed\n"));
-        if (fwrite (&item_count, sizeof(item_count), 1, f) != 1)
-            dprintf (3, ("fwrite failed\n"));
-        if (fwrite (mark_list_copy, sizeof(mark_list_copy[0]), item_count, f) != item_count)
-            dprintf (3, ("fwrite failed\n"));
-        if (fwrite (&magic, sizeof(magic), 1, f) != 1)
-            dprintf (3, ("fwrite failed\n"));
-        if (fclose (f) != 0)
-            dprintf (3, ("fclose failed\n"));
-    }
-#endif
-#ifdef _DEBUG
-    if (mark_list_copy_index > mark_list_copy)
-    {
-        introsort::sort (mark_list_copy, mark_list_copy_index - 1, 0);
-    }
-    for (ptrdiff_t i = 0; i < item_count; i++)
-    {
-        uint8_t* item = mark_list[i];
-        assert (mark_list_copy[i] == item);
-    }
-#endif //_DEBUG
-#else //USE_VXSORT
-    dprintf (3, ("Sorting mark lists"));
-    if (local_mark_list_index > mark_list)
-    {
-        introsort::sort (mark_list, local_mark_list_index - 1, 0);
-    }
-#endif //USE_VXSORT
-    uint8_t** x = mark_list;
-#ifdef USE_REGIONS
-    assert (g_mark_list_piece_size >= region_count);
-    assert (g_mark_list_piece_total_size >= region_count*n_heaps);
-    for (size_t region_index = 0; region_index < region_count; region_index++)
-    {
-        mark_list_piece_start[region_index] = NULL;
-        mark_list_piece_end[region_index] = NULL;
-    }
-#define predicate(x) (((x) < local_mark_list_index) && (*(x) < region_limit))
-    while (x < local_mark_list_index)
-    {
-        heap_segment* region = get_region_info_for_address (*x);
-        assert ((heap_segment_mem (region) <= *x) && (*x < heap_segment_allocated (region)));
-        size_t region_index = get_basic_region_index_for_address (heap_segment_mem (region));
-        uint8_t* region_limit = heap_segment_allocated (region);
-        uint8_t*** mark_list_piece_start_ptr = &mark_list_piece_start[region_index];
-        uint8_t*** mark_list_piece_end_ptr = &mark_list_piece_end[region_index];
-#else // USE_REGIONS
-#define predicate(x) (((x) < local_mark_list_index) && (*(x) < heap->ephemeral_high))
-    int heap_num;
-    for (heap_num = 0; heap_num < n_heaps; heap_num++)
-    {
-        mark_list_piece_start[heap_num] = NULL;
-        mark_list_piece_end[heap_num] = NULL;
-    }
-    heap_num = -1;
-    while (x < local_mark_list_index)
-    {
-        gc_heap* heap;
-#ifdef _DEBUG
-        int last_heap_num = heap_num;
-#endif //_DEBUG
-        do
-        {
-            heap_num++;
-            if (heap_num >= n_heaps)
-                heap_num = 0;
-            assert(heap_num != last_heap_num); // we should always find the heap - infinite loop if not!
-            heap = g_heaps[heap_num];
-        }
-        while (!(*x >= heap->ephemeral_low && *x < heap->ephemeral_high));
-        uint8_t*** mark_list_piece_start_ptr = &mark_list_piece_start[heap_num];
-        uint8_t*** mark_list_piece_end_ptr = &mark_list_piece_end[heap_num];
-#endif // USE_REGIONS
-        *mark_list_piece_start_ptr = x;
-        if (predicate(x))
-        {
-            if (predicate(local_mark_list_index -1))
-            {
-                x = local_mark_list_index;
-                *mark_list_piece_end_ptr = x;
-                break;
-            }
-            unsigned inc = 1;
-            do
-            {
-                inc *= 2;
-                uint8_t** temp_x = x;
-                x += inc;
-                if (temp_x > x)
-                {
-                    break;
-                }
-            }
-            while (predicate(x));
-            x -= inc;
-            do
-            {
-                assert (predicate(x) && !(((x + inc) > x) && predicate(x + inc)));
-                inc /= 2;
-                if (((x + inc) > x) && predicate(x + inc))
-                {
-                    x += inc;
-                }
-            }
-            while (inc > 1);
-            assert(predicate(x) && !predicate(x + inc) && (inc == 1));
-            x += 1;
-        }
-        *mark_list_piece_end_ptr = x;
-    }
-#undef predicate
-    return total_mark_list_size;
-}
-void gc_heap::append_to_mark_list (uint8_t **start, uint8_t **end)
-{
-    size_t slots_needed = end - start;
-    size_t slots_available = mark_list_end + 1 - mark_list_index;
-    size_t slots_to_copy = min(slots_needed, slots_available);
-    memcpy(mark_list_index, start, slots_to_copy*sizeof(*start));
-    mark_list_index += slots_to_copy;
-    dprintf (3, ("h%d: appended %zd slots to mark_list\n", heap_number, slots_to_copy));
-}
-#ifdef _DEBUG
-#if !defined(_MSC_VER)
-#if !defined(__cdecl)
-#if defined(__i386__)
-#define __cdecl __attribute__((cdecl))
-#else
-#define __cdecl
-#endif
-#endif
-#endif
-static int __cdecl cmp_mark_list_item (const void* vkey, const void* vdatum)
-{
-    uint8_t** key = (uint8_t**)vkey;
-    uint8_t** datum = (uint8_t**)vdatum;
-    if (*key < *datum)
-        return -1;
-    else if (*key > *datum)
-        return 1;
-    else
-        return 0;
-}
-#endif // _DEBUG
-#ifdef USE_REGIONS
-uint8_t** gc_heap::get_region_mark_list (BOOL& use_mark_list, uint8_t* start, uint8_t* end, uint8_t*** mark_list_end_ptr)
-{
-    size_t region_number = get_basic_region_index_for_address (start);
-    size_t source_number = region_number;
-#else //USE_REGIONS
-void gc_heap::merge_mark_lists (size_t total_mark_list_size)
-{
-    if (total_mark_list_size == 0)
-    {
-        return;
-    }
-#ifdef _DEBUG
-    size_t this_mark_list_size = target_mark_count_for_heap (total_mark_list_size, n_heaps, heap_number);
-    for (uint8_t** p = mark_list + this_mark_list_size; p < mark_list_index; p++)
-    {
-        uint8_t* item = *p;
-        uint8_t** found_slot = nullptr;
-        for (int i = 0; i < n_heaps; i++)
-        {
-            uint8_t** heap_mark_list = &g_mark_list[i * mark_list_size];
-            size_t heap_mark_list_size = target_mark_count_for_heap (total_mark_list_size, n_heaps, i);
-            found_slot = (uint8_t**)bsearch (&item, heap_mark_list, heap_mark_list_size, sizeof(item), cmp_mark_list_item);
-            if (found_slot != nullptr)
-                break;
-        }
-        assert ((found_slot != nullptr) && (*found_slot == item));
-    }
-#endif
-    dprintf(3, ("merge_mark_lists: heap_number = %d  starts out with %zd entries",
-        heap_number, (mark_list_index - mark_list)));
-    int source_number = (size_t)heap_number;
-#endif //USE_REGIONS
-    uint8_t** source[MAX_SUPPORTED_CPUS];
-    uint8_t** source_end[MAX_SUPPORTED_CPUS];
-    int source_heap[MAX_SUPPORTED_CPUS];
-    int source_count = 0;
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap* heap = g_heaps[i];
-        if (heap->mark_list_piece_start[source_number] < heap->mark_list_piece_end[source_number])
-        {
-            source[source_count] = heap->mark_list_piece_start[source_number];
-            source_end[source_count] = heap->mark_list_piece_end[source_number];
-            source_heap[source_count] = i;
-            if (source_count < MAX_SUPPORTED_CPUS)
-                source_count++;
-        }
-    }
-    dprintf(3, ("source_number = %zd  has %d sources\n", (size_t)source_number, source_count));
-#if defined(_DEBUG) || defined(TRACE_GC)
-    for (int j = 0; j < source_count; j++)
-    {
-        dprintf(3, ("source_number = %zd  ", (size_t)source_number));
-        dprintf(3, (" source from heap %zd = %zx .. %zx (%zd entries)",
-            (size_t)(source_heap[j]), (size_t)(source[j][0]),
-            (size_t)(source_end[j][-1]), (size_t)(source_end[j] - source[j])));
-        for (uint8_t **x = source[j]; x < source_end[j] - 1; x++)
-        {
-            if (x[0] > x[1])
-            {
-                dprintf(3, ("oops, mark_list from source %d for heap %zd isn't sorted\n", j,  (size_t)source_number));
-                assert (0);
-            }
-        }
-    }
-#endif //_DEBUG || TRACE_GC
-    mark_list = &g_mark_list_copy [heap_number*mark_list_size];
-    mark_list_index = mark_list;
-    mark_list_end = &mark_list [mark_list_size-1];
-    int piece_count = 0;
-    if (source_count == 0)
-    {
-        ; // nothing to do
-    }
-    else if (source_count == 1)
-    {
-        mark_list = source[0];
-        mark_list_index = source_end[0];
-        mark_list_end = mark_list_index;
-        piece_count++;
-    }
-    else
-    {
-        while (source_count > 1)
-        {
-            int lowest_source = 0;
-            uint8_t *lowest = *source[0];
-            uint8_t *second_lowest = *source[1];
-            for (int i = 1; i < source_count; i++)
-            {
-                if (lowest > *source[i])
-                {
-                    second_lowest = lowest;
-                    lowest = *source[i];
-                    lowest_source = i;
-                }
-                else if (second_lowest > *source[i])
-                {
-                    second_lowest = *source[i];
-                }
-            }
-            uint8_t **x;
-            if (source_end[lowest_source][-1] <= second_lowest)
-                x = source_end[lowest_source];
-            else
-            {
-                for (x = source[lowest_source]; x < source_end[lowest_source] && *x <= second_lowest; x++)
-                    ;
-            }
-            append_to_mark_list(source[lowest_source], x);
-#ifdef USE_REGIONS
-            if (mark_list_index > mark_list_end)
-            {
-                use_mark_list = false;
-                return nullptr;
-            }
-#endif //USE_REGIONS
-            piece_count++;
-            source[lowest_source] = x;
-            if (x >= source_end[lowest_source])
-            {
-                if (lowest_source < source_count-1)
-                {
-                    source[lowest_source] = source[source_count-1];
-                    source_end[lowest_source] = source_end[source_count-1];
-                }
-                source_count--;
-            }
-        }
-        append_to_mark_list(source[0], source_end[0]);
-#ifdef USE_REGIONS
-        if (mark_list_index > mark_list_end)
-        {
-            use_mark_list = false;
-            return nullptr;
-        }
-#endif //USE_REGIONS
-        piece_count++;
-    }
-#if defined(_DEBUG) || defined(TRACE_GC)
-    for (uint8_t **x = mark_list; x < mark_list_index - 1; x++)
-    {
-        if (x[0] > x[1])
-        {
-            dprintf(3, ("oops, mark_list for heap %d isn't sorted at the end of merge_mark_lists", heap_number));
-            assert (0);
-        }
-    }
-#endif //_DEBUG || TRACE_GC
-#ifdef USE_REGIONS
-    *mark_list_end_ptr = mark_list_index;
-    return mark_list;
-#endif // USE_REGIONS
-}
-#else
-#ifdef USE_REGIONS
-static uint8_t** binary_search (uint8_t** left, uint8_t** right, uint8_t* e)
-{
-    if (left == right)
-        return left;
-    assert (left < right);
-    uint8_t** a = left;
-    size_t l = 0;
-    size_t r = (size_t)(right - left);
-    while ((r - l) >= 2)
-    {
-        size_t m = l + (r - l) / 2;
-        assert ((l < m) && (m < r));
-        if (a[m] < e)
-        {
-            l = m;
-        }
-        else
-        {
-            r = m;
-        }
-    }
-    if (a[l] < e)
-        return a + l + 1;
-    else
-        return a + l;
-}
-uint8_t** gc_heap::get_region_mark_list (BOOL& use_mark_list, uint8_t* start, uint8_t* end, uint8_t*** mark_list_end_ptr)
-{
-    *mark_list_end_ptr = binary_search (mark_list, mark_list_index, end);
-    return binary_search (mark_list, *mark_list_end_ptr, start);
-}
-#endif //USE_REGIONS
-#endif //MULTIPLE_HEAPS
-void gc_heap::grow_mark_list ()
-{
-#ifdef USE_VXSORT
-#ifdef MULTIPLE_HEAPS
-    const size_t MAX_MARK_LIST_SIZE = IsSupportedInstructionSet (InstructionSet::AVX2) ?
-        (1000 * 1024) : (200 * 1024);
-#else //MULTIPLE_HEAPS
-    const size_t MAX_MARK_LIST_SIZE = IsSupportedInstructionSet (InstructionSet::AVX2) ?
-        (32 * 1024) : (16 * 1024);
-#endif //MULTIPLE_HEAPS
-#else //USE_VXSORT
-#ifdef MULTIPLE_HEAPS
-    const size_t MAX_MARK_LIST_SIZE = 200 * 1024;
-#else //MULTIPLE_HEAPS
-    const size_t MAX_MARK_LIST_SIZE = 16 * 1024;
-#endif //MULTIPLE_HEAPS
-#endif //USE_VXSORT
-    size_t new_mark_list_size = min (mark_list_size * 2, MAX_MARK_LIST_SIZE);
-    size_t new_mark_list_total_size = new_mark_list_size*n_heaps;
-    if (new_mark_list_total_size == g_mark_list_total_size)
-        return;
-#ifdef MULTIPLE_HEAPS
-    uint8_t** new_mark_list = make_mark_list (new_mark_list_total_size);
-    uint8_t** new_mark_list_copy = make_mark_list (new_mark_list_total_size);
-    if ((new_mark_list != nullptr) && (new_mark_list_copy != nullptr))
-    {
-        delete[] g_mark_list;
-        g_mark_list = new_mark_list;
-        delete[] g_mark_list_copy;
-        g_mark_list_copy = new_mark_list_copy;
-        mark_list_size = new_mark_list_size;
-        g_mark_list_total_size = new_mark_list_total_size;
-    }
-    else
-    {
-        delete[] new_mark_list;
-        delete[] new_mark_list_copy;
-    }
-#else //MULTIPLE_HEAPS
-    uint8_t** new_mark_list = make_mark_list (new_mark_list_size);
-    if (new_mark_list != nullptr)
-    {
-        delete[] mark_list;
-        g_mark_list = new_mark_list;
-        mark_list_size = new_mark_list_size;
-        g_mark_list_total_size = new_mark_list_size;
-    }
-#endif //MULTIPLE_HEAPS
-}
-#ifndef USE_REGIONS
-class seg_free_spaces
-{
-    struct seg_free_space
-    {
-        BOOL is_plug;
-        void* start;
-    };
-    struct free_space_bucket
-    {
-        seg_free_space* free_space;
-        ptrdiff_t count_add; // Assigned when we first construct the array.
-        ptrdiff_t count_fit; // How many items left when we are fitting plugs.
-    };
-    void move_bucket (int old_power2, int new_power2)
-    {
-        assert (old_power2 >= 0);
-        assert (old_power2 >= new_power2);
-        if (old_power2 == new_power2)
-        {
-            return;
-        }
-        seg_free_space* src_index = free_space_buckets[old_power2].free_space;
-        for (int i = old_power2; i > new_power2; i--)
-        {
-            seg_free_space** dest = &(free_space_buckets[i].free_space);
-            (*dest)++;
-            seg_free_space* dest_index = free_space_buckets[i - 1].free_space;
-            if (i > (new_power2 + 1))
-            {
-                seg_free_space temp = *src_index;
-                *src_index = *dest_index;
-                *dest_index = temp;
-            }
-            src_index = dest_index;
-        }
-        free_space_buckets[old_power2].count_fit--;
-        free_space_buckets[new_power2].count_fit++;
-    }
-#ifdef _DEBUG
-    void dump_free_space (seg_free_space* item)
-    {
-        uint8_t* addr = 0;
-        size_t len = 0;
-        if (item->is_plug)
-        {
-            mark* m = (mark*)(item->start);
-            len = pinned_len (m);
-            addr = pinned_plug (m) - len;
-        }
-        else
-        {
-            heap_segment* seg = (heap_segment*)(item->start);
-            addr = heap_segment_plan_allocated (seg);
-            len = heap_segment_committed (seg) - addr;
-        }
-        dprintf (SEG_REUSE_LOG_1, ("[%d]0x%p %zd", heap_num, addr, len));
-    }
-    void dump()
-    {
-        seg_free_space* item = NULL;
-        int i = 0;
-        dprintf (SEG_REUSE_LOG_1, ("[%d]----------------------------------\nnow the free spaces look like:", heap_num));
-        for (i = 0; i < (free_space_bucket_count - 1); i++)
-        {
-            dprintf (SEG_REUSE_LOG_1, ("[%d]Free spaces for 2^%d bucket:", heap_num, (base_power2 + i)));
-            dprintf (SEG_REUSE_LOG_1, ("[%d]%s %s", heap_num, "start", "len"));
-            item = free_space_buckets[i].free_space;
-            while (item < free_space_buckets[i + 1].free_space)
-            {
-                dump_free_space (item);
-                item++;
-            }
-            dprintf (SEG_REUSE_LOG_1, ("[%d]----------------------------------", heap_num));
-        }
-        dprintf (SEG_REUSE_LOG_1, ("[%d]Free spaces for 2^%d bucket:", heap_num, (base_power2 + i)));
-        dprintf (SEG_REUSE_LOG_1, ("[%d]%s %s", heap_num, "start", "len"));
-        item = free_space_buckets[i].free_space;
-        while (item <= &seg_free_space_array[free_space_item_count - 1])
-        {
-            dump_free_space (item);
-            item++;
-        }
-        dprintf (SEG_REUSE_LOG_1, ("[%d]----------------------------------", heap_num));
-    }
-#endif //_DEBUG
-    free_space_bucket* free_space_buckets;
-    seg_free_space* seg_free_space_array;
-    ptrdiff_t free_space_bucket_count;
-    ptrdiff_t free_space_item_count;
-    int base_power2;
-    int heap_num;
-#ifdef _DEBUG
-    BOOL has_end_of_seg;
-#endif //_DEBUG
-public:
-    seg_free_spaces (int h_number)
-    {
-        heap_num = h_number;
-    }
-    BOOL alloc ()
-    {
-        size_t total_prealloc_size =
-            MAX_NUM_BUCKETS * sizeof (free_space_bucket) +
-            MAX_NUM_FREE_SPACES * sizeof (seg_free_space);
-        free_space_buckets = (free_space_bucket*) new (nothrow) uint8_t[total_prealloc_size];
-        return (!!free_space_buckets);
-    }
-    void add_buckets (int base, size_t* ordered_free_spaces, int bucket_count, size_t item_count)
-    {
-        assert (free_space_buckets);
-        assert (item_count <= (size_t)MAX_PTR);
-        free_space_bucket_count = bucket_count;
-        free_space_item_count = item_count;
-        base_power2 = base;
-#ifdef _DEBUG
-        has_end_of_seg = FALSE;
-#endif //_DEBUG
-        ptrdiff_t total_item_count = 0;
-        ptrdiff_t i = 0;
-        seg_free_space_array = (seg_free_space*)(free_space_buckets + free_space_bucket_count);
-        for (i = 0; i < (ptrdiff_t)item_count; i++)
-        {
-            seg_free_space_array[i].start = 0;
-            seg_free_space_array[i].is_plug = FALSE;
-        }
-        for (i = 0; i < bucket_count; i++)
-        {
-            free_space_buckets[i].count_add = ordered_free_spaces[i];
-            free_space_buckets[i].count_fit = ordered_free_spaces[i];
-            free_space_buckets[i].free_space = &seg_free_space_array[total_item_count];
-            total_item_count += free_space_buckets[i].count_add;
-        }
-        assert (total_item_count == (ptrdiff_t)item_count);
-    }
-    void add (void* start, BOOL plug_p, BOOL first_p)
-    {
-        size_t size = (plug_p ?
-                       pinned_len ((mark*)start) :
-                       (heap_segment_committed ((heap_segment*)start) -
-                           heap_segment_plan_allocated ((heap_segment*)start)));
-        if (plug_p)
-        {
-            dprintf (SEG_REUSE_LOG_1, ("[%d]Adding a free space before plug: %zd", heap_num, size));
-        }
-        else
-        {
-            dprintf (SEG_REUSE_LOG_1, ("[%d]Adding a free space at end of seg: %zd", heap_num, size));
-#ifdef _DEBUG
-            has_end_of_seg = TRUE;
-#endif //_DEBUG
-        }
-        if (first_p)
-        {
-            size_t eph_gen_starts = gc_heap::eph_gen_starts_size;
-            size -= eph_gen_starts;
-            if (plug_p)
-            {
-                mark* m = (mark*)(start);
-                pinned_len (m) -= eph_gen_starts;
-            }
-            else
-            {
-                heap_segment* seg = (heap_segment*)start;
-                heap_segment_plan_allocated (seg) += eph_gen_starts;
-            }
-        }
-        int bucket_power2 = index_of_highest_set_bit (size);
-        if (bucket_power2 < base_power2)
-        {
-            return;
-        }
-        free_space_bucket* bucket = &free_space_buckets[bucket_power2 - base_power2];
-        seg_free_space* bucket_free_space = bucket->free_space;
-        assert (plug_p || (!plug_p && bucket->count_add));
-        if (bucket->count_add == 0)
-        {
-            dprintf (SEG_REUSE_LOG_1, ("[%d]Already have enough of 2^%d", heap_num, bucket_power2));
-            return;
-        }
-        ptrdiff_t index = bucket->count_add - 1;
-        dprintf (SEG_REUSE_LOG_1, ("[%d]Building free spaces: adding %p; len: %zd (2^%d)",
-                    heap_num,
-                    (plug_p ?
-                        (pinned_plug ((mark*)start) - pinned_len ((mark*)start)) :
-                        heap_segment_plan_allocated ((heap_segment*)start)),
-                    size,
-                    bucket_power2));
-        if (plug_p)
-        {
-            bucket_free_space[index].is_plug = TRUE;
-        }
-        bucket_free_space[index].start = start;
-        bucket->count_add--;
-    }
-#ifdef _DEBUG
-    void check()
-    {
-        ptrdiff_t i = 0;
-        int end_of_seg_count = 0;
-        for (i = 0; i < free_space_item_count; i++)
-        {
-            assert (seg_free_space_array[i].start);
-            if (!(seg_free_space_array[i].is_plug))
-            {
-                end_of_seg_count++;
-            }
-        }
-        if (has_end_of_seg)
-        {
-            assert (end_of_seg_count == 1);
-        }
-        else
-        {
-            assert (end_of_seg_count == 0);
-        }
-        for (i = 0; i < free_space_bucket_count; i++)
-        {
-            assert (free_space_buckets[i].count_add == 0);
-        }
-    }
-#endif //_DEBUG
-    uint8_t* fit (uint8_t* old_loc,
-               size_t plug_size
-               REQD_ALIGN_AND_OFFSET_DCL)
-    {
-        if (old_loc)
-        {
-#ifdef SHORT_PLUGS
-            assert (!is_plug_padded (old_loc));
-#endif //SHORT_PLUGS
-            assert (!node_realigned (old_loc));
-        }
-        size_t saved_plug_size = plug_size;
-#ifdef FEATURE_STRUCTALIGN
-        _ASSERTE(requiredAlignment == DATA_ALIGNMENT && false);
-#endif // FEATURE_STRUCTALIGN
-        size_t plug_size_to_fit = plug_size;
-#ifdef RESPECT_LARGE_ALIGNMENT
-        plug_size_to_fit += switch_alignment_size(FALSE);
-#endif //RESPECT_LARGE_ALIGNMENT
-        int plug_power2 = index_of_highest_set_bit (round_up_power2 (plug_size_to_fit + Align(min_obj_size)));
-        ptrdiff_t i;
-        uint8_t* new_address = 0;
-        if (plug_power2 < base_power2)
-        {
-            plug_power2 = base_power2;
-        }
-        int chosen_power2 = plug_power2 - base_power2;
-retry:
-        for (i = chosen_power2; i < free_space_bucket_count; i++)
-        {
-            if (free_space_buckets[i].count_fit != 0)
-            {
-                break;
-            }
-            chosen_power2++;
-        }
-        dprintf (SEG_REUSE_LOG_1, ("[%d]Fitting plug len %zd (2^%d) using 2^%d free space",
-            heap_num,
-            plug_size,
-            plug_power2,
-            (chosen_power2 + base_power2)));
-        assert (i < free_space_bucket_count);
-        seg_free_space* bucket_free_space = free_space_buckets[chosen_power2].free_space;
-        ptrdiff_t free_space_count = free_space_buckets[chosen_power2].count_fit;
-        size_t new_free_space_size = 0;
-        BOOL can_fit = FALSE;
-        size_t pad = 0;
-        for (i = 0; i < free_space_count; i++)
-        {
-            size_t free_space_size = 0;
-            pad = 0;
-            if (bucket_free_space[i].is_plug)
-            {
-                mark* m = (mark*)(bucket_free_space[i].start);
-                uint8_t* plug_free_space_start = pinned_plug (m) - pinned_len (m);
-                if (!((old_loc == 0) || same_large_alignment_p (old_loc, plug_free_space_start)))
-                {
-                    pad = switch_alignment_size (FALSE);
-                }
-                plug_size = saved_plug_size + pad;
-                free_space_size = pinned_len (m);
-                new_address = pinned_plug (m) - pinned_len (m);
-                if (free_space_size >= (plug_size + Align (min_obj_size)) ||
-                    free_space_size == plug_size)
-                {
-                    new_free_space_size = free_space_size - plug_size;
-                    pinned_len (m) = new_free_space_size;
-#ifdef SIMPLE_DPRINTF
-                    dprintf (SEG_REUSE_LOG_0, ("[%d]FP: 0x%p->0x%p(%zx)(%zx), [0x%p (2^%d) -> [0x%p (2^%d)",
-                                heap_num,
-                                old_loc,
-                                new_address,
-                                (plug_size - pad),
-                                pad,
-                                pinned_plug (m),
-                                index_of_highest_set_bit (free_space_size),
-                                (pinned_plug (m) - pinned_len (m)),
-                                index_of_highest_set_bit (new_free_space_size)));
-#endif //SIMPLE_DPRINTF
-                    if (pad != 0)
-                    {
-                        set_node_realigned (old_loc);
-                    }
-                    can_fit = TRUE;
-                }
-            }
-            else
-            {
-                heap_segment* seg = (heap_segment*)(bucket_free_space[i].start);
-                free_space_size = heap_segment_committed (seg) - heap_segment_plan_allocated (seg);
-                if (!((old_loc == 0) || same_large_alignment_p (old_loc, heap_segment_plan_allocated (seg))))
-                {
-                    pad = switch_alignment_size (FALSE);
-                }
-                plug_size = saved_plug_size + pad;
-                if (free_space_size >= (plug_size + Align (min_obj_size)) ||
-                    free_space_size == plug_size)
-                {
-                    new_address = heap_segment_plan_allocated (seg);
-                    new_free_space_size = free_space_size - plug_size;
-                    heap_segment_plan_allocated (seg) = new_address + plug_size;
-#ifdef SIMPLE_DPRINTF
-                    dprintf (SEG_REUSE_LOG_0, ("[%d]FS: 0x%p-> 0x%p(%zd) (2^%d) -> 0x%p (2^%d)",
-                                heap_num,
-                                old_loc,
-                                new_address,
-                                (plug_size - pad),
-                                index_of_highest_set_bit (free_space_size),
-                                heap_segment_plan_allocated (seg),
-                                index_of_highest_set_bit (new_free_space_size)));
-#endif //SIMPLE_DPRINTF
-                    if (pad != 0)
-                        set_node_realigned (old_loc);
-                    can_fit = TRUE;
-                }
-            }
-            if (can_fit)
-            {
-                break;
-            }
-        }
-        if (!can_fit)
-        {
-            assert (chosen_power2 == 0);
-            chosen_power2 = 1;
-            goto retry;
-        }
-        new_address += pad;
-        assert ((chosen_power2 && (i == 0)) ||
-                ((!chosen_power2) && (i < free_space_count)));
-        int new_bucket_power2 = index_of_highest_set_bit (new_free_space_size);
-        if (new_bucket_power2 < base_power2)
-        {
-            new_bucket_power2 = base_power2;
-        }
-        move_bucket (chosen_power2, new_bucket_power2 - base_power2);
-        return new_address;
-    }
-    void cleanup ()
-    {
-        if (free_space_buckets)
-        {
-            delete [] free_space_buckets;
-        }
-        if (seg_free_space_array)
-        {
-            delete [] seg_free_space_array;
-        }
-    }
-};
-#endif //!USE_REGIONS
-#define marked(i) header(i)->IsMarked()
-#define set_marked(i) header(i)->SetMarked()
-#define clear_marked(i) header(i)->ClearMarked()
-#define pinned(i) header(i)->IsPinned()
-#define set_pinned(i) header(i)->SetPinned()
-#define clear_pinned(i) header(i)->GetHeader()->ClrGCBit();
-inline size_t my_get_size (Object* ob)
-{
-    MethodTable* mT = header(ob)->GetMethodTable();
-    return (mT->GetBaseSize() +
-            (mT->HasComponentSize() ?
-             ((size_t)((CObjectHeader*)ob)->GetNumComponents() * mT->RawGetComponentSize()) : 0));
-}
-#define size(i) my_get_size (header(i))
-#define contain_pointers(i) header(i)->ContainsPointers()
-#ifdef COLLECTIBLE_CLASS
-#define contain_pointers_or_collectible(i) header(i)->ContainsPointersOrCollectible()
-#define get_class_object(i) GCToEEInterface::GetLoaderAllocatorObjectForGC((Object *)i)
-#define is_collectible(i) method_table(i)->Collectible()
-#else //COLLECTIBLE_CLASS
-#define contain_pointers_or_collectible(i) header(i)->ContainsPointers()
-#endif //COLLECTIBLE_CLASS
-#ifdef BACKGROUND_GC
-#ifdef FEATURE_BASICFREEZE
-inline
-void gc_heap::seg_clear_mark_array_bits_soh (heap_segment* seg)
-{
-    uint8_t* range_beg = 0;
-    uint8_t* range_end = 0;
-    if (bgc_mark_array_range (seg, FALSE, &range_beg, &range_end))
-    {
-        clear_mark_array (range_beg, align_on_mark_word (range_end));
-    }
-}
-inline
-void gc_heap::seg_set_mark_array_bits_soh (heap_segment* seg)
-{
-    uint8_t* range_beg = 0;
-    uint8_t* range_end = 0;
-    if (bgc_mark_array_range (seg, FALSE, &range_beg, &range_end))
-    {
-        size_t beg_word = mark_word_of (align_on_mark_word (range_beg));
-        size_t end_word = mark_word_of (align_on_mark_word (range_end));
-        uint8_t* op = range_beg;
-        while (op < mark_word_address (beg_word))
-        {
-            mark_array_set_marked (op);
-            op += mark_bit_pitch;
-        }
-        memset (&mark_array[beg_word], 0xFF, (end_word - beg_word)*sizeof (uint32_t));
-    }
-}
-#endif //FEATURE_BASICFREEZE
-void gc_heap::bgc_clear_batch_mark_array_bits (uint8_t* start, uint8_t* end)
-{
-    if ((start < background_saved_highest_address) &&
-        (end > background_saved_lowest_address))
-    {
-        start = max (start, background_saved_lowest_address);
-        end = min (end, background_saved_highest_address);
-        size_t start_mark_bit = mark_bit_of (start);
-        size_t end_mark_bit = mark_bit_of (end);
-        unsigned int startbit = mark_bit_bit (start_mark_bit);
-        unsigned int endbit = mark_bit_bit (end_mark_bit);
-        size_t startwrd = mark_bit_word (start_mark_bit);
-        size_t endwrd = mark_bit_word (end_mark_bit);
-        dprintf (3, ("Clearing all mark array bits between [%zx:%zx-[%zx:%zx",
-            (size_t)start, (size_t)start_mark_bit,
-            (size_t)end, (size_t)end_mark_bit));
-        unsigned int firstwrd = lowbits (~0, startbit);
-        unsigned int lastwrd = highbits (~0, endbit);
-        if (startwrd == endwrd)
-        {
-            if (startbit != endbit)
-            {
-                unsigned int wrd = firstwrd | lastwrd;
-                mark_array[startwrd] &= wrd;
-            }
-            else
-            {
-                assert (start == end);
-            }
-            return;
-        }
-        if (startbit)
-        {
-            mark_array[startwrd] &= firstwrd;
-            startwrd++;
-        }
-        for (size_t wrdtmp = startwrd; wrdtmp < endwrd; wrdtmp++)
-        {
-            mark_array[wrdtmp] = 0;
-        }
-        if (endbit)
-        {
-            mark_array[endwrd] &= lastwrd;
-        }
-    }
-}
-#endif //BACKGROUND_GC
-inline
-BOOL gc_heap::is_mark_set (uint8_t* o)
-{
-    return marked (o);
-}
-#if defined (_MSC_VER) && defined (TARGET_X86)
-#pragma optimize("y", on)        // Small critical routines, don't put in EBP frame
-#endif //_MSC_VER && TARGET_X86
-int gc_heap::object_gennum (uint8_t* o)
-{
-#ifdef USE_REGIONS
-    return get_region_gen_num (o);
-#else
-    if (in_range_for_segment (o, ephemeral_heap_segment) &&
-        (o >= generation_allocation_start (generation_of (max_generation - 1))))
-    {
-        for ( int i = 0; i < max_generation-1; i++)
-        {
-            if ((o >= generation_allocation_start (generation_of (i))))
-                return i;
-        }
-        return max_generation-1;
-    }
-    else
-    {
-        return max_generation;
-    }
-#endif //USE_REGIONS
-}
-int gc_heap::object_gennum_plan (uint8_t* o)
-{
-#ifdef USE_REGIONS
-    return get_region_plan_gen_num (o);
-#else
-    if (in_range_for_segment (o, ephemeral_heap_segment))
-    {
-        for (int i = 0; i < ephemeral_generation_count; i++)
-        {
-            uint8_t* plan_start = generation_plan_allocation_start (generation_of (i));
-            if (plan_start && (o >= plan_start))
-            {
-                return i;
-            }
-        }
-    }
-    return max_generation;
-#endif //USE_REGIONS
-}
-#if defined(_MSC_VER) && defined(TARGET_X86)
-#pragma optimize("", on)        // Go back to command line default optimizations
-#endif //_MSC_VER && TARGET_X86
-#ifdef USE_REGIONS
-void get_initial_region(int gen, int hn, uint8_t** region_start, uint8_t** region_end)
-{
-    *region_start = initial_regions[hn][gen][0];
-    *region_end = initial_regions[hn][gen][1];
-}
-bool gc_heap::initial_make_soh_regions (gc_heap* hp)
-{
-    uint8_t* region_start;
-    uint8_t* region_end;
-    uint32_t hn = 0;
-#ifdef MULTIPLE_HEAPS
-    hn = hp->heap_number;
-#endif //MULTIPLE_HEAPS
-    for (int i = max_generation; i >= 0; i--)
-    {
-        get_initial_region(i, hn, &region_start, &region_end);
-        size_t region_size = region_end - region_start;
-        heap_segment* current_region = make_heap_segment (region_start, region_size, hp, i);
-        if (current_region == nullptr)
-        {
-            return false;
-        }
-        uint8_t* gen_start = heap_segment_mem (current_region);
-        make_generation (i, current_region, gen_start);
-        if (i == 0)
-        {
-            ephemeral_heap_segment = current_region;
-            alloc_allocated = heap_segment_allocated (current_region);
-        }
-    }
-    for (int i = max_generation; i >= 0; i--)
-    {
-        dprintf (REGIONS_LOG, ("h%d gen%d alloc seg is %p, start seg is %p (%p-%p)",
-            heap_number, i, generation_allocation_segment (generation_of (i)),
-            generation_start_segment (generation_of (i)),
-            heap_segment_mem (generation_start_segment (generation_of (i))),
-            heap_segment_allocated (generation_start_segment (generation_of (i)))));
-    }
-    return true;
-}
-bool gc_heap::initial_make_uoh_regions (int gen, gc_heap* hp)
-{
-    uint8_t* region_start;
-    uint8_t* region_end;
-    uint32_t hn = 0;
-#ifdef MULTIPLE_HEAPS
-    hn = hp->heap_number;
-#endif //MULTIPLE_HEAPS
-    get_initial_region(gen, hn, &region_start, &region_end);
-    size_t region_size = region_end - region_start;
-    heap_segment* uoh_region = make_heap_segment (region_start, region_size, hp, gen);
-    if (uoh_region == nullptr)
-    {
-        return false;
-    }
-    uoh_region->flags |=
-        (gen == loh_generation) ? heap_segment_flags_loh : heap_segment_flags_poh;
-    uint8_t* gen_start = heap_segment_mem (uoh_region);
-    make_generation (gen, uoh_region, gen_start);
-    return true;
-}
-void gc_heap::clear_region_info (heap_segment* region)
-{
-    if (!heap_segment_uoh_p (region))
-    {
-        clear_brick_table (heap_segment_mem (region), heap_segment_reserved (region));
-    }
-    clear_card_for_addresses (get_region_start (region), heap_segment_reserved (region));
-#ifdef BACKGROUND_GC
-    ::record_changed_seg ((uint8_t*)region, heap_segment_reserved (region),
-                        settings.gc_index, current_bgc_state,
-                        seg_deleted);
-    bgc_verify_mark_array_cleared (region);
-#endif //BACKGROUND_GC
-}
-void gc_heap::return_free_region (heap_segment* region)
-{
-    gc_oh_num oh = heap_segment_oh (region);
-    dprintf(3, ("commit-accounting:  from %d to free [%p, %p) for heap %d", oh, get_region_start (region), heap_segment_committed (region), heap_number));
-#ifndef COMMITTED_BYTES_SHADOW
-    if (heap_hard_limit)
-#endif //!COMMITTED_BYTES_SHADOW
-    {
-        size_t committed = heap_segment_committed (region) - get_region_start (region);
-        if (committed > 0)
-        {
-            check_commit_cs.Enter();
-            assert (committed_by_oh[oh] >= committed);
-            committed_by_oh[oh] -= committed;
-            committed_by_oh[recorded_committed_free_bucket] += committed;
-#if defined(_DEBUG) && defined(MULTIPLE_HEAPS)
-            assert (committed_by_oh_per_heap[oh] >= committed);
-            committed_by_oh_per_heap[oh] -= committed;
-#endif // _DEBUG && MULTIPLE_HEAPS
-            check_commit_cs.Leave();
-        }
-    }
-    clear_region_info (region);
-    region_free_list::add_region_descending (region, free_regions);
-    uint8_t* region_start = get_region_start (region);
-    uint8_t* region_end = heap_segment_reserved (region);
-    int num_basic_regions = (int)((region_end - region_start) >> min_segment_size_shr);
-    dprintf (REGIONS_LOG, ("RETURNING region %p (%d basic regions) to free",
-        heap_segment_mem (region), num_basic_regions));
-    for (int i = 0; i < num_basic_regions; i++)
-    {
-        uint8_t* basic_region_start = region_start + ((size_t)i << min_segment_size_shr);
-        heap_segment* basic_region = get_region_info (basic_region_start);
-        heap_segment_allocated (basic_region) = 0;
-#ifdef MULTIPLE_HEAPS
-        heap_segment_heap (basic_region) = 0;
-#endif //MULTIPLE_HEAPS
-    }
-}
-heap_segment* gc_heap::get_free_region (int gen_number, size_t size)
-{
-    heap_segment* region = 0;
-    if (gen_number <= max_generation)
-    {
-        assert (size == 0);
-        region = free_regions[basic_free_region].unlink_region_front();
-    }
-    else
-    {
-        const size_t LARGE_REGION_SIZE = global_region_allocator.get_large_region_alignment();
-        assert (size >= LARGE_REGION_SIZE);
-        if (size == LARGE_REGION_SIZE)
-        {
-            region = free_regions[large_free_region].unlink_region_front();
-        }
-        else
-        {
-            region = free_regions[huge_free_region].unlink_smallest_region (size);
-            if (region == nullptr)
-            {
-                if (settings.pause_mode == pause_no_gc)
-                {
-                    assert (gc_lock.holding_thread != (Thread*)-1);
-                }
-                else
-                {
-                    ASSERT_HOLDING_SPIN_LOCK(&gc_lock);
-                }
-                region = global_free_huge_regions.unlink_smallest_region (size);
-            }
-        }
-    }
-    if (region)
-    {
-        uint8_t* region_start = get_region_start (region);
-        uint8_t* region_end = heap_segment_reserved (region);
-        init_heap_segment (region, __this, region_start,
-                           (region_end - region_start),
-                           gen_number, true);
-        gc_oh_num oh = gen_to_oh (gen_number);
-        dprintf(3, ("commit-accounting:  from free to %d [%p, %p) for heap %d", oh, get_region_start (region), heap_segment_committed (region), heap_number));
-#ifndef COMMITTED_BYTES_SHADOW
-        if (heap_hard_limit)
-#endif //!COMMITTED_BYTES_SHADOW
-        {
-            size_t committed = heap_segment_committed (region) - get_region_start (region);
-            if (committed > 0)
-            {
-                check_commit_cs.Enter();
-                committed_by_oh[oh] += committed;
-                assert (committed_by_oh[recorded_committed_free_bucket] >= committed);
-                committed_by_oh[recorded_committed_free_bucket] -= committed;
-#if defined(_DEBUG) && defined(MULTIPLE_HEAPS)
-                committed_by_oh_per_heap[oh] += committed;
-#endif // _DEBUG && MULTIPLE_HEAPS
-                check_commit_cs.Leave();
-            }
-        }
-        dprintf (REGIONS_LOG, ("h%d GFR get region %zx (%p-%p) for gen%d",
-            heap_number, (size_t)region,
-            region_start, region_end,
-            gen_number));
-        assert (heap_segment_allocated(region) == heap_segment_mem (region));
-    }
-    else
-    {
-        region = allocate_new_region (__this, gen_number, (gen_number > max_generation), size);
-    }
-    if (region)
-    {
-        if (!init_table_for_region (gen_number, region))
-        {
-            region = 0;
-        }
-    }
-    return region;
-}
-heap_segment* gc_heap::region_of (uint8_t* obj)
-{
-    size_t index = (size_t)obj >> gc_heap::min_segment_size_shr;
-    seg_mapping* entry = &seg_mapping_table[index];
-    return (heap_segment*)entry;
-}
-heap_segment* gc_heap::get_region_at_index (size_t index)
-{
-    index += (size_t)g_gc_lowest_address >> gc_heap::min_segment_size_shr;
-    return (heap_segment*)(&seg_mapping_table[index]);
-}
-void gc_heap::check_seg_gen_num (heap_segment* seg)
-{
-#ifdef _DEBUG
-    uint8_t* mem = heap_segment_mem (seg);
-    if ((mem < g_gc_lowest_address) || (mem >= g_gc_highest_address))
-    {
-        GCToOSInterface::DebugBreak();
-    }
-    int alloc_seg_gen_num = get_region_gen_num (mem);
-    int alloc_seg_plan_gen_num = get_region_plan_gen_num (mem);
-    dprintf (3, ("seg %p->%p, num %d, %d",
-        seg, mem, alloc_seg_gen_num, alloc_seg_plan_gen_num));
-#endif //_DEBUG
-}
-int gc_heap::get_region_gen_num (heap_segment* region)
-{
-    return heap_segment_gen_num (region);
-}
-int gc_heap::get_region_gen_num (uint8_t* obj)
-{
-    size_t skewed_basic_region_index = get_skewed_basic_region_index_for_address (obj);
-    int gen_num = map_region_to_generation_skewed[skewed_basic_region_index] & gc_heap::RI_GEN_MASK;
-    assert ((soh_gen0 <= gen_num) && (gen_num <= soh_gen2));
-    assert (gen_num == heap_segment_gen_num (region_of (obj)));
-    return gen_num;
-}
-int gc_heap::get_region_plan_gen_num (uint8_t* obj)
-{
-    size_t skewed_basic_region_index = get_skewed_basic_region_index_for_address (obj);
-    int plan_gen_num = map_region_to_generation_skewed[skewed_basic_region_index] >> gc_heap::RI_PLAN_GEN_SHR;
-    assert ((soh_gen0 <= plan_gen_num) && (plan_gen_num <= soh_gen2));
-    assert (plan_gen_num == heap_segment_plan_gen_num (region_of (obj)));
-    return plan_gen_num;
-}
-bool gc_heap::is_region_demoted (uint8_t* obj)
-{
-    size_t skewed_basic_region_index = get_skewed_basic_region_index_for_address (obj);
-    bool demoted_p = (map_region_to_generation_skewed[skewed_basic_region_index] & gc_heap::RI_DEMOTED) != 0;
-    assert (demoted_p == heap_segment_demoted_p (region_of (obj)));
-    return demoted_p;
-}
-static GCSpinLock write_barrier_spin_lock;
-inline
-void gc_heap::set_region_gen_num (heap_segment* region, int gen_num)
-{
-    assert (gen_num < (1 << (sizeof (uint8_t) * 8)));
-    assert (gen_num >= 0);
-    heap_segment_gen_num (region) = (uint8_t)gen_num;
-    uint8_t* region_start = get_region_start (region);
-    uint8_t* region_end = heap_segment_reserved (region);
-    size_t region_index_start = get_basic_region_index_for_address (region_start);
-    size_t region_index_end = get_basic_region_index_for_address (region_end);
-    region_info entry = (region_info)((gen_num << RI_PLAN_GEN_SHR) | gen_num);
-    for (size_t region_index = region_index_start; region_index < region_index_end; region_index++)
-    {
-        assert (gen_num <= max_generation);
-        map_region_to_generation[region_index] = entry;
-    }
-    if (gen_num <= soh_gen1)
-    {
-        if ((region_start < ephemeral_low) || (ephemeral_high < region_end))
-        {
-            while (true)
-            {
-                if (Interlocked::CompareExchange(&write_barrier_spin_lock.lock, 0, -1) < 0)
-                    break;
-                if ((ephemeral_low <= region_start) && (region_end <= ephemeral_high))
-                    return;
-                while (write_barrier_spin_lock.lock >= 0)
-                {
-                    YieldProcessor();           // indicate to the processor that we are spinning
-                }
-            }
-#ifdef _DEBUG
-            write_barrier_spin_lock.holding_thread = GCToEEInterface::GetThread();
-#endif //_DEBUG
-            if ((region_start < ephemeral_low) || (ephemeral_high < region_end))
-            {
-                uint8_t* new_ephemeral_low = min (region_start, (uint8_t*)ephemeral_low);
-                uint8_t* new_ephemeral_high = max (region_end, (uint8_t*)ephemeral_high);
-                dprintf (REGIONS_LOG, ("about to set ephemeral_low = %p ephemeral_high = %p", new_ephemeral_low, new_ephemeral_high));
-                stomp_write_barrier_ephemeral (new_ephemeral_low, new_ephemeral_high,
-                                               map_region_to_generation_skewed, (uint8_t)min_segment_size_shr);
-                if (ephemeral_low < new_ephemeral_low)
-                    GCToOSInterface::DebugBreak ();
-                if (new_ephemeral_high < ephemeral_high)
-                    GCToOSInterface::DebugBreak ();
-                ephemeral_low = new_ephemeral_low;
-                ephemeral_high = new_ephemeral_high;
-                dprintf (REGIONS_LOG, ("set ephemeral_low = %p ephemeral_high = %p", new_ephemeral_low, new_ephemeral_high));
-            }
-            else
-            {
-                dprintf (REGIONS_LOG, ("leaving lock - no need to update ephemeral range [%p,%p[ for region [%p,%p]", (uint8_t*)ephemeral_low, (uint8_t*)ephemeral_high, region_start, region_end));
-            }
-#ifdef _DEBUG
-            write_barrier_spin_lock.holding_thread = (Thread*)-1;
-#endif //_DEBUG
-            write_barrier_spin_lock.lock = -1;
-        }
-        else
-        {
-            dprintf (REGIONS_LOG, ("no need to update ephemeral range [%p,%p[ for region [%p,%p]", (uint8_t*)ephemeral_low, (uint8_t*)ephemeral_high, region_start, region_end));
-        }
-    }
-}
-inline
-void gc_heap::set_region_plan_gen_num (heap_segment* region, int plan_gen_num, bool replace_p)
-{
-    int gen_num = heap_segment_gen_num (region);
-    int supposed_plan_gen_num = get_plan_gen_num (gen_num);
-    dprintf (REGIONS_LOG, ("h%d setting plan gen on %p->%p(was gen%d) to %d(should be: %d) %s",
-        heap_number, region,
-        heap_segment_mem (region),
-        gen_num, plan_gen_num,
-        supposed_plan_gen_num,
-        ((plan_gen_num < supposed_plan_gen_num) ? "DEMOTED" : "ND")));
-    region_info region_info_bits_to_set = (region_info)(plan_gen_num << RI_PLAN_GEN_SHR);
-    if ((plan_gen_num < supposed_plan_gen_num) && (heap_segment_pinned_survived (region) != 0))
-    {
-        if (!settings.demotion)
-        {
-            settings.demotion = TRUE;
-        }
-        get_gc_data_per_heap()->set_mechanism_bit (gc_demotion_bit);
-        region->flags |= heap_segment_flags_demoted;
-        region_info_bits_to_set = (region_info)(region_info_bits_to_set | RI_DEMOTED);
-    }
-    else
-    {
-        region->flags &= ~heap_segment_flags_demoted;
-    }
-    if (replace_p)
-    {
-        int original_plan_gen_num = heap_segment_plan_gen_num (region);
-        planned_regions_per_gen[original_plan_gen_num]--;
-    }
-    planned_regions_per_gen[plan_gen_num]++;
-    dprintf (REGIONS_LOG, ("h%d g%d %zx(%zx) -> g%d (total %d region planned in g%d)",
-        heap_number, heap_segment_gen_num (region), (size_t)region, heap_segment_mem (region), plan_gen_num, planned_regions_per_gen[plan_gen_num], plan_gen_num));
-    heap_segment_plan_gen_num (region) = plan_gen_num;
-    uint8_t* region_start = get_region_start (region);
-    uint8_t* region_end = heap_segment_reserved (region);
-    size_t region_index_start = get_basic_region_index_for_address (region_start);
-    size_t region_index_end = get_basic_region_index_for_address (region_end);
-    for (size_t region_index = region_index_start; region_index < region_index_end; region_index++)
-    {
-        assert (plan_gen_num <= max_generation);
-        map_region_to_generation[region_index] = (region_info)(region_info_bits_to_set | (map_region_to_generation[region_index] & ~(RI_PLAN_GEN_MASK|RI_DEMOTED)));
-    }
-}
-inline
-void gc_heap::set_region_plan_gen_num_sip (heap_segment* region, int plan_gen_num)
-{
-    if (!heap_segment_swept_in_plan (region))
-    {
-        set_region_plan_gen_num (region, plan_gen_num);
-    }
-}
-void gc_heap::set_region_sweep_in_plan (heap_segment*region)
-{
-    heap_segment_swept_in_plan (region) = true;
-    assert (get_region_size (region) == global_region_allocator.get_region_alignment());
-    uint8_t* region_start = get_region_start (region);
-    size_t region_index = get_basic_region_index_for_address (region_start);
-    map_region_to_generation[region_index] = (region_info)(map_region_to_generation[region_index] | RI_SIP);
-}
-void gc_heap::clear_region_sweep_in_plan (heap_segment*region)
-{
-    heap_segment_swept_in_plan (region) = false;
-    assert (get_region_size (region) == global_region_allocator.get_region_alignment());
-    uint8_t* region_start = get_region_start (region);
-    size_t region_index = get_basic_region_index_for_address (region_start);
-    map_region_to_generation[region_index] = (region_info)(map_region_to_generation[region_index] & ~RI_SIP);
-}
-void gc_heap::clear_region_demoted (heap_segment* region)
-{
-    region->flags &= ~heap_segment_flags_demoted;
-    assert (get_region_size (region) == global_region_allocator.get_region_alignment());
-    uint8_t* region_start = get_region_start (region);
-    size_t region_index = get_basic_region_index_for_address (region_start);
-    map_region_to_generation[region_index] = (region_info)(map_region_to_generation[region_index] & ~RI_DEMOTED);
-}
-#endif //USE_REGIONS
-int gc_heap::get_plan_gen_num (int gen_number)
-{
-    return ((settings.promotion) ? min ((gen_number + 1), max_generation) : gen_number);
-}
-uint8_t* gc_heap::get_uoh_start_object (heap_segment* region, generation* gen)
-{
-#ifdef USE_REGIONS
-    uint8_t* o = heap_segment_mem (region);
-#else
-    uint8_t* o = generation_allocation_start (gen);
-    assert(((CObjectHeader*)o)->IsFree());
-    size_t s = Align (size (o), get_alignment_constant (FALSE));
-    assert (s == AlignQword (min_obj_size));
-    o += s;
-#endif //USE_REGIONS
-    return o;
-}
-uint8_t* gc_heap::get_soh_start_object (heap_segment* region, generation* gen)
-{
-#ifdef USE_REGIONS
-    uint8_t* o             = heap_segment_mem (region);
-#else
-    uint8_t* o             = generation_allocation_start (gen);
-#endif //USE_REGIONS
-    return o;
-}
-size_t gc_heap::get_soh_start_obj_len (uint8_t* start_obj)
-{
-#ifdef USE_REGIONS
-    return 0;
-#else
-    return Align (size (start_obj));
-#endif //USE_REGIONS
-}
-void gc_heap::clear_gen1_cards()
-{
-#if defined(_DEBUG) && !defined(USE_REGIONS)
-    for (int x = 0; x <= max_generation; x++)
-    {
-        assert (generation_allocation_start (generation_of (x)));
-    }
-#endif //_DEBUG && !USE_REGIONS
-    if (!settings.demotion && settings.promotion)
-    {
-#ifdef USE_REGIONS
-        heap_segment* region = generation_start_segment (generation_of (1));
-        while (region)
-        {
-            clear_card_for_addresses (get_region_start (region), heap_segment_reserved (region));
-            region = heap_segment_next (region);
-        }
-#else //USE_REGIONS
-        clear_card_for_addresses (
-            generation_allocation_start (generation_of (1)),
-            generation_allocation_start (generation_of (0)));
-#endif //USE_REGIONS
-#ifdef _DEBUG
-        uint8_t* start = get_soh_start_object (ephemeral_heap_segment, youngest_generation);
-        assert (heap_segment_allocated (ephemeral_heap_segment) ==
-                (start + get_soh_start_obj_len (start)));
-#endif //_DEBUG
-    }
-}
-heap_segment* gc_heap::make_heap_segment (uint8_t* new_pages, size_t size, gc_heap* hp, int gen_num)
-{
-    gc_oh_num oh = gen_to_oh (gen_num);
-    size_t initial_commit = use_large_pages_p ? size : SEGMENT_INITIAL_COMMIT;
-    int h_number =
-#ifdef MULTIPLE_HEAPS
-        hp->heap_number;
-#else
-        0;
-#endif //MULTIPLE_HEAPS
-    if (!virtual_commit (new_pages, initial_commit, oh, h_number))
-    {
-        return 0;
-    }
-#ifdef USE_REGIONS
-    dprintf (REGIONS_LOG, ("Making region %p->%p(%zdmb)",
-        new_pages, (new_pages + size), (size / 1024 / 1024)));
-    heap_segment* new_segment = get_region_info (new_pages);
-    uint8_t* start = new_pages + sizeof (aligned_plug_and_gap);
-#else
-    heap_segment* new_segment = (heap_segment*)new_pages;
-    uint8_t* start = new_pages + segment_info_size;
-#endif //USE_REGIONS
-    heap_segment_mem (new_segment) = start;
-    heap_segment_used (new_segment) = start;
-    heap_segment_reserved (new_segment) = new_pages + size;
-    heap_segment_committed (new_segment) = new_pages + initial_commit;
-    init_heap_segment (new_segment, hp
-#ifdef USE_REGIONS
-                       , new_pages, size, gen_num
-#endif //USE_REGIONS
-                       );
-    dprintf (2, ("Creating heap segment %zx", (size_t)new_segment));
-    return new_segment;
-}
-void gc_heap::init_heap_segment (heap_segment* seg, gc_heap* hp
-#ifdef USE_REGIONS
-                                 , uint8_t* start, size_t size, int gen_num, bool existing_region_p
-#endif //USE_REGIONS
-    )
-{
-#ifndef USE_REGIONS
-    bool existing_region_p = false;
-#endif //!USE_REGIONS
-#ifdef BACKGROUND_GC
-    seg->flags = existing_region_p ? (seg->flags & heap_segment_flags_ma_committed) : 0;
-#else
-    seg->flags = 0;
-#endif
-    heap_segment_next (seg) = 0;
-    heap_segment_plan_allocated (seg) = heap_segment_mem (seg);
-    heap_segment_allocated (seg) = heap_segment_mem (seg);
-    heap_segment_saved_allocated (seg) = heap_segment_mem (seg);
-    heap_segment_decommit_target (seg) = heap_segment_reserved (seg);
-#ifdef BACKGROUND_GC
-    heap_segment_background_allocated (seg) = 0;
-    heap_segment_saved_bg_allocated (seg) = 0;
-#endif //BACKGROUND_GC
-#ifdef MULTIPLE_HEAPS
-    heap_segment_heap (seg) = hp;
-#endif //MULTIPLE_HEAPS
-#ifdef USE_REGIONS
-    int gen_num_for_region = min (gen_num, max_generation);
-    set_region_gen_num (seg, gen_num_for_region);
-    heap_segment_plan_gen_num (seg) = gen_num_for_region;
-    heap_segment_swept_in_plan (seg) = false;
-#endif //USE_REGIONS
-#ifdef USE_REGIONS
-    int num_basic_regions = (int)(size >> min_segment_size_shr);
-    size_t basic_region_size = (size_t)1 << min_segment_size_shr;
-    dprintf (REGIONS_LOG, ("this region contains %d basic regions", num_basic_regions));
-    if (num_basic_regions > 1)
-    {
-        for (int i = 1; i < num_basic_regions; i++)
-        {
-            uint8_t* basic_region_start = start + (i * basic_region_size);
-            heap_segment* basic_region = get_region_info (basic_region_start);
-            heap_segment_allocated (basic_region) = (uint8_t*)(ptrdiff_t)-i;
-            dprintf (REGIONS_LOG, ("Initing basic region %p->%p(%zdmb) alloc to %p",
-                basic_region_start, (basic_region_start + basic_region_size),
-                (size_t)(basic_region_size / 1024 / 1024),
-                heap_segment_allocated (basic_region)));
-            heap_segment_gen_num (basic_region) = (uint8_t)gen_num_for_region;
-            heap_segment_plan_gen_num (basic_region) = gen_num_for_region;
-#ifdef MULTIPLE_HEAPS
-            heap_segment_heap (basic_region) = hp;
-#endif //MULTIPLE_HEAPS
-        }
-    }
-#endif //USE_REGIONS
-}
-void gc_heap::delete_heap_segment (heap_segment* seg, BOOL consider_hoarding)
-{
-    if (!heap_segment_uoh_p (seg))
-    {
-        clear_brick_table (heap_segment_mem (seg), heap_segment_reserved (seg));
-    }
-#ifdef USE_REGIONS
-    return_free_region (seg);
-#else // USE_REGIONS
-    if (consider_hoarding)
-    {
-        assert ((heap_segment_mem (seg) - (uint8_t*)seg) <= ptrdiff_t(2*OS_PAGE_SIZE));
-        size_t ss = (size_t) (heap_segment_reserved (seg) - (uint8_t*)seg);
-        if (ss <= INITIAL_ALLOC)
-        {
-            dprintf (2, ("Hoarding segment %zx", (size_t)seg));
-#ifdef BACKGROUND_GC
-            if (!heap_segment_decommitted_p (seg))
-#endif //BACKGROUND_GC
-            {
-                decommit_heap_segment (seg);
-            }
-            seg_mapping_table_remove_segment (seg);
-            heap_segment_next (seg) = segment_standby_list;
-            segment_standby_list = seg;
-            seg = 0;
-        }
-    }
-    if (seg != 0)
-    {
-        dprintf (2, ("h%d: del seg: [%zx, %zx[",
-                     heap_number, (size_t)seg,
-                     (size_t)(heap_segment_reserved (seg))));
-#ifdef BACKGROUND_GC
-        ::record_changed_seg ((uint8_t*)seg, heap_segment_reserved (seg),
-                            settings.gc_index, current_bgc_state,
-                            seg_deleted);
-        bgc_verify_mark_array_cleared (seg);
-        decommit_mark_array_by_seg (seg);
-#endif //BACKGROUND_GC
-        seg_mapping_table_remove_segment (seg);
-        release_segment (seg);
-    }
-#endif //USE_REGIONS
-}
-void gc_heap::reset_heap_segment_pages (heap_segment* seg)
-{
-    size_t page_start = align_on_page ((size_t)heap_segment_allocated (seg));
-    size_t size = (size_t)heap_segment_committed (seg) - page_start;
-    if (size != 0)
-        GCToOSInterface::VirtualReset((void*)page_start, size, false /* unlock */);
-}
-void gc_heap::decommit_heap_segment_pages (heap_segment* seg,
-                                           size_t extra_space)
-{
-    if (use_large_pages_p)
-        return;
-    uint8_t*  page_start = align_on_page (heap_segment_allocated(seg));
-    assert (heap_segment_committed (seg) >= page_start);
-    size_t size = heap_segment_committed (seg) - page_start;
-    extra_space = align_on_page (extra_space);
-    if (size >= max ((extra_space + 2*OS_PAGE_SIZE), MIN_DECOMMIT_SIZE))
-    {
-        page_start += max(extra_space, 32*OS_PAGE_SIZE);
-        decommit_heap_segment_pages_worker (seg, page_start);
-    }
-}
-size_t gc_heap::decommit_heap_segment_pages_worker (heap_segment* seg,
-                                                    uint8_t* new_committed)
-{
-    assert (!use_large_pages_p);
-    uint8_t* page_start = align_on_page (new_committed);
-    ptrdiff_t size = heap_segment_committed (seg) - page_start;
-    if (size > 0)
-    {
-        bool decommit_succeeded_p = virtual_decommit (page_start, (size_t)size, heap_segment_oh (seg), heap_number);
-        if (decommit_succeeded_p)
-        {
-            dprintf (3, ("Decommitting heap segment [%zx, %zx[(%zd)",
-                (size_t)page_start,
-                (size_t)(page_start + size),
-                size));
-            heap_segment_committed (seg) = page_start;
-            if (heap_segment_used (seg) > heap_segment_committed (seg))
-            {
-                heap_segment_used (seg) = heap_segment_committed (seg);
-            }
-        }
-        else
-        {
-            dprintf (3, ("Decommitting heap segment failed"));
-        }
-    }
-    return size;
-}
-void gc_heap::decommit_heap_segment (heap_segment* seg)
-{
-#ifdef USE_REGIONS
-    if (!dt_high_memory_load_p())
-    {
-        return;
-    }
-#endif
-    uint8_t*  page_start = align_on_page (heap_segment_mem (seg));
-    dprintf (3, ("Decommitting heap segment %zx(%p)", (size_t)seg, heap_segment_mem (seg)));
-#if defined(BACKGROUND_GC) && !defined(USE_REGIONS)
-    page_start += OS_PAGE_SIZE;
-#endif //BACKGROUND_GC && !USE_REGIONS
-    assert (heap_segment_committed (seg) >= page_start);
-    size_t size = heap_segment_committed (seg) - page_start;
-    bool decommit_succeeded_p = virtual_decommit (page_start, size, heap_segment_oh (seg), heap_number);
-    if (decommit_succeeded_p)
-    {
-        heap_segment_committed (seg) = page_start;
-        if (heap_segment_used (seg) > heap_segment_committed (seg))
-        {
-            heap_segment_used (seg) = heap_segment_committed (seg);
-        }
-    }
-}
-void gc_heap::clear_gen0_bricks()
-{
-    if (!gen0_bricks_cleared)
-    {
-        gen0_bricks_cleared = TRUE;
-#ifdef USE_REGIONS
-        heap_segment* gen0_region = generation_start_segment (generation_of (0));
-        while (gen0_region)
-        {
-            uint8_t* clear_start = heap_segment_mem (gen0_region);
-#else
-        heap_segment* gen0_region = ephemeral_heap_segment;
-        uint8_t* clear_start = generation_allocation_start (generation_of (0));
-        {
-#endif //USE_REGIONS
-            for (size_t b = brick_of (clear_start);
-                    b < brick_of (align_on_brick
-                                (heap_segment_allocated (gen0_region)));
-                    b++)
-            {
-                set_brick (b, -1);
-            }
-#ifdef USE_REGIONS
-            gen0_region = heap_segment_next (gen0_region);
-#endif //USE_REGIONS
-        }
-    }
-}
-void gc_heap::check_gen0_bricks()
-{
-    if (gen0_bricks_cleared)
-    {
-#ifdef USE_REGIONS
-        heap_segment* gen0_region = generation_start_segment (generation_of (0));
-        while (gen0_region)
-        {
-            uint8_t* start = heap_segment_mem (gen0_region);
-#else
-        heap_segment* gen0_region = ephemeral_heap_segment;
-        uint8_t* start = generation_allocation_start (generation_of (0));
-        {
-#endif //USE_REGIONS
-            size_t end_b = brick_of (heap_segment_allocated (gen0_region));
-            for (size_t b = brick_of (start); b < end_b; b++)
-            {
-                assert (brick_table[b] != 0);
-                if (brick_table[b] == 0)
-                {
-                    GCToOSInterface::DebugBreak();
-                }
-            }
-#ifdef USE_REGIONS
-            gen0_region = heap_segment_next (gen0_region);
-#endif //USE_REGIONS
-        }
-    }
-}
-#ifdef BACKGROUND_GC
-void gc_heap::rearrange_small_heap_segments()
-{
-    heap_segment* seg = freeable_soh_segment;
-    while (seg)
-    {
-        heap_segment* next_seg = heap_segment_next (seg);
-        delete_heap_segment (seg, FALSE);
-        seg = next_seg;
-    }
-    freeable_soh_segment = 0;
-}
-#endif //BACKGROUND_GC
-void gc_heap::rearrange_uoh_segments()
-{
-    dprintf (2, ("deleting empty large segments"));
-    heap_segment* seg = freeable_uoh_segment;
-    while (seg)
-    {
-        heap_segment* next_seg = heap_segment_next (seg);
-        delete_heap_segment (seg, GCConfig::GetRetainVM());
-        seg = next_seg;
-    }
-    freeable_uoh_segment = 0;
-}
-void gc_heap::delay_free_segments()
-{
-    rearrange_uoh_segments();
-#ifdef BACKGROUND_GC
-    background_delay_delete_uoh_segments();
-    if (!gc_heap::background_running_p())
-        rearrange_small_heap_segments();
-#endif //BACKGROUND_GC
-}
-#ifndef USE_REGIONS
-void gc_heap::rearrange_heap_segments(BOOL compacting)
-{
-    heap_segment* seg =
-        generation_start_segment (generation_of (max_generation));
-    heap_segment* prev_seg = 0;
-    heap_segment* next_seg = 0;
-    while (seg)
-    {
-        next_seg = heap_segment_next (seg);
-        if ((next_seg == 0) && (seg != ephemeral_heap_segment))
-        {
-            seg->next = ephemeral_heap_segment;
-            next_seg = heap_segment_next (seg);
-        }
-        if ((seg == ephemeral_heap_segment) && next_seg)
-        {
-            heap_segment_next (prev_seg) = next_seg;
-            heap_segment_next (seg) = 0;
-        }
-        else
-        {
-            uint8_t* end_segment = (compacting ?
-                                 heap_segment_plan_allocated (seg) :
-                                 heap_segment_allocated (seg));
-            if ((end_segment == heap_segment_mem (seg))&&
-                !heap_segment_read_only_p (seg))
-            {
-                assert (prev_seg);
-                assert (seg != ephemeral_heap_segment);
-                heap_segment_next (prev_seg) = next_seg;
-                delete_heap_segment (seg, GCConfig::GetRetainVM());
-                dprintf (2, ("Deleting heap segment %zx", (size_t)seg));
-            }
-            else
-            {
-                if (!heap_segment_read_only_p (seg))
-                {
-                    if (compacting)
-                    {
-                        heap_segment_allocated (seg) =
-                            heap_segment_plan_allocated (seg);
-                    }
-                    if (seg != ephemeral_heap_segment)
-                    {
-                        decommit_heap_segment_pages (seg, 0);
-                    }
-                }
-                prev_seg = seg;
-            }
-        }
-        seg = next_seg;
-    }
-}
-#endif //!USE_REGIONS
-#if defined(USE_REGIONS)
-static void remove_surplus_regions (region_free_list* free_list, region_free_list* surplus_list, size_t target_count)
-{
-    while (free_list->get_num_free_regions() > target_count)
-    {
-        heap_segment* region = free_list->unlink_region_front();
-        surplus_list->add_region_front (region);
-    }
-}
-static int64_t add_regions (region_free_list* free_list, region_free_list* surplus_list, size_t target_count)
-{
-    int64_t added_count = 0;
-    while (free_list->get_num_free_regions() < target_count)
-    {
-        if (surplus_list->get_num_free_regions() == 0)
-            break;
-        added_count++;
-        heap_segment* region = surplus_list->unlink_region_front();
-        free_list->add_region_front (region);
-    }
-    return added_count;
-}
-region_free_list::region_free_list() : num_free_regions (0),
-                                       size_free_regions (0),
-                                       size_committed_in_free_regions (0),
-                                       num_free_regions_added (0),
-                                       num_free_regions_removed (0),
-                                       head_free_region (nullptr),
-                                       tail_free_region (nullptr)
-{
-}
-void region_free_list::verify (bool empty_p)
-{
-#ifdef _DEBUG
-    assert ((num_free_regions == 0) == empty_p);
-    assert ((size_free_regions == 0) == empty_p);
-    assert ((size_committed_in_free_regions == 0) == empty_p);
-    assert ((head_free_region == nullptr) == empty_p);
-    assert ((tail_free_region == nullptr) == empty_p);
-    assert (num_free_regions == (num_free_regions_added - num_free_regions_removed));
-    if (!empty_p)
-    {
-        assert (heap_segment_next (tail_free_region) == nullptr);
-        assert (heap_segment_prev_free_region (head_free_region) == nullptr);
-        size_t actual_count = 0;
-        heap_segment* last_region = nullptr;
-        for (heap_segment* region = head_free_region; region != nullptr; region = heap_segment_next(region))
-        {
-            last_region = region;
-            actual_count++;
-        }
-        assert (num_free_regions == actual_count);
-        assert (last_region == tail_free_region);
-        heap_segment* first_region = nullptr;
-        for (heap_segment* region = tail_free_region; region != nullptr; region = heap_segment_prev_free_region(region))
-        {
-            first_region = region;
-            actual_count--;
-        }
-        assert (actual_count == 0);
-        assert (head_free_region == first_region);
-    }
-#endif
-}
-void region_free_list::reset()
-{
-    num_free_regions = 0;
-    size_free_regions = 0;
-    size_committed_in_free_regions = 0;
-    head_free_region = nullptr;
-    tail_free_region = nullptr;
-}
-inline
-void region_free_list::update_added_region_info (heap_segment* region)
-{
-    num_free_regions++;
-    num_free_regions_added++;
-    size_t region_size = get_region_size (region);
-    size_free_regions += region_size;
-    size_t region_committed_size = get_region_committed_size (region);
-    size_committed_in_free_regions += region_committed_size;
-    verify (false);
-}
-void region_free_list::add_region_front (heap_segment* region)
-{
-    assert (heap_segment_containing_free_list (region) == nullptr);
-    heap_segment_containing_free_list(region) = this;
-    if (head_free_region != nullptr)
-    {
-        heap_segment_prev_free_region(head_free_region) = region;
-        assert (tail_free_region != nullptr);
-    }
-    else
-    {
-        tail_free_region = region;
-    }
-    heap_segment_next (region) = head_free_region;
-    head_free_region = region;
-    heap_segment_prev_free_region (region) = nullptr;
-    update_added_region_info (region);
-}
-void region_free_list::add_region_in_descending_order (heap_segment* region_to_add)
-{
-    assert (heap_segment_containing_free_list (region_to_add) == nullptr);
-    heap_segment_containing_free_list (region_to_add) = this;
-    heap_segment_age_in_free (region_to_add) = 0;
-    heap_segment* prev_region = nullptr;
-    heap_segment* region = nullptr;
-    if (heap_segment_committed (region_to_add) == heap_segment_reserved (region_to_add))
-    {
-        region = head_free_region;
-    }
-    else
-    {
-        size_t region_to_add_committed = get_region_committed_size (region_to_add);
-        for (prev_region = tail_free_region; prev_region != nullptr; prev_region = heap_segment_prev_free_region (prev_region))
-        {
-            size_t prev_region_committed = get_region_committed_size (prev_region);
-            if (prev_region_committed >= region_to_add_committed)
-            {
-                break;
-            }
-            region = prev_region;
-        }
-    }
-    if (prev_region != nullptr)
-    {
-        heap_segment_next (prev_region) = region_to_add;
-    }
-    else
-    {
-        assert (region == head_free_region);
-        head_free_region = region_to_add;
-    }
-    heap_segment_prev_free_region (region_to_add) = prev_region;
-    heap_segment_next (region_to_add) = region;
-    if (region != nullptr)
-    {
-        heap_segment_prev_free_region (region) = region_to_add;
-    }
-    else
-    {
-        assert (prev_region == tail_free_region);
-        tail_free_region = region_to_add;
-    }
-    update_added_region_info (region_to_add);
-}
-heap_segment* region_free_list::unlink_region_front()
-{
-    heap_segment* region = head_free_region;
-    if (region != nullptr)
-    {
-        assert (heap_segment_containing_free_list (region) == this);
-        unlink_region (region);
-    }
-    return region;
-}
-void region_free_list::unlink_region (heap_segment* region)
-{
-    region_free_list* rfl = heap_segment_containing_free_list (region);
-    rfl->verify (false);
-    heap_segment* prev = heap_segment_prev_free_region (region);
-    heap_segment* next = heap_segment_next (region);
-    if (prev != nullptr)
-    {
-        assert (region != rfl->head_free_region);
-        assert (heap_segment_next (prev) == region);
-        heap_segment_next (prev) = next;
-    }
-    else
-    {
-        assert (region == rfl->head_free_region);
-        rfl->head_free_region = next;
-    }
-    if (next != nullptr)
-    {
-        assert (region != rfl->tail_free_region);
-        assert (heap_segment_prev_free_region (next) == region);
-        heap_segment_prev_free_region (next) = prev;
-    }
-    else
-    {
-        assert (region == rfl->tail_free_region);
-        rfl->tail_free_region = prev;
-    }
-    heap_segment_containing_free_list (region) = nullptr;
-    rfl->num_free_regions--;
-    rfl->num_free_regions_removed++;
-    size_t region_size = get_region_size (region);
-    assert (rfl->size_free_regions >= region_size);
-    rfl->size_free_regions -= region_size;
-    size_t region_committed_size = get_region_committed_size (region);
-    assert (rfl->size_committed_in_free_regions >= region_committed_size);
-    rfl->size_committed_in_free_regions -= region_committed_size;
-}
-free_region_kind region_free_list::get_region_kind (heap_segment* region)
-{
-    const size_t BASIC_REGION_SIZE = global_region_allocator.get_region_alignment();
-    const size_t LARGE_REGION_SIZE = global_region_allocator.get_large_region_alignment();
-    size_t region_size = get_region_size (region);
-    if (region_size == BASIC_REGION_SIZE)
-        return basic_free_region;
-    else if (region_size == LARGE_REGION_SIZE)
-        return large_free_region;
-    else
-    {
-        assert(region_size > LARGE_REGION_SIZE);
-        return huge_free_region;
-    }
-}
-heap_segment* region_free_list::unlink_smallest_region (size_t minimum_size)
-{
-    verify (num_free_regions == 0);
-    heap_segment* smallest_region = nullptr;
-    size_t smallest_size = (size_t)-1;
-    for (heap_segment* region = head_free_region; region != nullptr; region = heap_segment_next (region))
-    {
-        uint8_t* region_start = get_region_start(region);
-        uint8_t* region_end = heap_segment_reserved(region);
-        size_t region_size = get_region_size (region);
-        const size_t LARGE_REGION_SIZE = global_region_allocator.get_large_region_alignment();
-        assert (region_size >= LARGE_REGION_SIZE * 2);
-        if (region_size >= minimum_size)
-        {
-            if (smallest_size > region_size)
-            {
-                smallest_size = region_size;
-                smallest_region = region;
-            }
-            if (region_size == LARGE_REGION_SIZE * 2)
-            {
-                assert (region == smallest_region);
-                break;
-            }
-        }
-    }
-    if (smallest_region != nullptr)
-    {
-        unlink_region (smallest_region);
-        dprintf(REGIONS_LOG, ("get %p-%p-%p",
-            heap_segment_mem(smallest_region), heap_segment_committed(smallest_region), heap_segment_used(smallest_region)));
-    }
-    return smallest_region;
-}
-void region_free_list::transfer_regions (region_free_list* from)
-{
-    this->verify (this->num_free_regions == 0);
-    from->verify (from->num_free_regions == 0);
-    if (from->num_free_regions == 0)
-    {
-        return;
-    }
-    if (num_free_regions == 0)
-    {
-        head_free_region = from->head_free_region;
-        tail_free_region = from->tail_free_region;
-    }
-    else
-    {
-        heap_segment* this_tail = tail_free_region;
-        heap_segment* from_head = from->head_free_region;
-        heap_segment_next (this_tail) = from_head;
-        heap_segment_prev_free_region (from_head) = this_tail;
-        tail_free_region = from->tail_free_region;
-    }
-    for (heap_segment* region = from->head_free_region; region != nullptr; region = heap_segment_next (region))
-    {
-        heap_segment_containing_free_list (region) = this;
-    }
-    num_free_regions += from->num_free_regions;
-    num_free_regions_added += from->num_free_regions;
-    size_free_regions += from->size_free_regions;
-    size_committed_in_free_regions += from->size_committed_in_free_regions;
-    from->num_free_regions_removed += from->num_free_regions;
-    from->reset();
-    verify (false);
-}
-size_t region_free_list::get_num_free_regions()
-{
-#ifdef _DEBUG
-    verify (num_free_regions == 0);
-#endif //_DEBUG
-    return num_free_regions;
-}
-void region_free_list::add_region (heap_segment* region, region_free_list to_free_list[count_free_region_kinds])
-{
-    free_region_kind kind = get_region_kind (region);
-    to_free_list[kind].add_region_front (region);
-}
-void region_free_list::add_region_descending (heap_segment* region, region_free_list to_free_list[count_free_region_kinds])
-{
-    free_region_kind kind = get_region_kind (region);
-    to_free_list[kind].add_region_in_descending_order (region);
-}
-bool region_free_list::is_on_free_list (heap_segment* region, region_free_list free_list[count_free_region_kinds])
-{
-    region_free_list* rfl = heap_segment_containing_free_list (region);
-    free_region_kind kind = get_region_kind (region);
-    return rfl == &free_list[kind];
-}
-void region_free_list::age_free_regions()
-{
-    for (heap_segment* region = head_free_region; region != nullptr; region = heap_segment_next (region))
-    {
-        if (heap_segment_age_in_free (region) < MAX_AGE_IN_FREE)
-            heap_segment_age_in_free (region)++;
-    }
-}
-void region_free_list::age_free_regions (region_free_list free_lists[count_free_region_kinds])
-{
-    for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
-    {
-        free_lists[kind].age_free_regions();
-    }
-}
-void region_free_list::print (int hn, const char* msg, int* ages)
-{
-    dprintf (3, ("h%2d PRINTING-------------------------------", hn));
-    for (heap_segment* region = head_free_region; region != nullptr; region = heap_segment_next (region))
-    {
-        if (ages)
-        {
-            ages[heap_segment_age_in_free (region)]++;
-        }
-        dprintf (3, ("[%s] h%2d age %d region %p (%zd)%s",
-            msg, hn, (int)heap_segment_age_in_free (region),
-            heap_segment_mem (region), get_region_committed_size (region),
-            ((heap_segment_committed (region) == heap_segment_reserved (region)) ? "(FC)" : "")));
-    }
-    dprintf (3, ("h%2d PRINTING END-------------------------------", hn));
-}
-void region_free_list::print (region_free_list free_lists[count_free_region_kinds], int hn, const char* msg, int* ages)
-{
-    for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
-    {
-        free_lists[kind].print (hn, msg, ages);
-    }
-}
-static int compare_by_committed_and_age (heap_segment* l, heap_segment* r)
-{
-    size_t l_committed = get_region_committed_size (l);
-    size_t r_committed = get_region_committed_size (r);
-    if (l_committed > r_committed)
-        return -1;
-    else if (l_committed < r_committed)
-        return 1;
-    int l_age = heap_segment_age_in_free (l);
-    int r_age = heap_segment_age_in_free (r);
-    return (l_age - r_age);
-}
-static heap_segment* merge_sort_by_committed_and_age (heap_segment *head, size_t count)
-{
-    if (count <= 1)
-        return head;
-    size_t half = count / 2;
-    heap_segment* mid = nullptr;
-    size_t i = 0;
-    for (heap_segment *region = head; region != nullptr; region = heap_segment_next (region))
-    {
-        i++;
-        if (i == half)
-        {
-            mid = heap_segment_next (region);
-            heap_segment_next (region) = nullptr;
-            break;
-        }
-    }
-    head = merge_sort_by_committed_and_age (head, half);
-    mid = merge_sort_by_committed_and_age (mid, count - half);
-    heap_segment* new_head;
-    if (compare_by_committed_and_age (head, mid) <= 0)
-    {
-        new_head = head;
-        head = heap_segment_next (head);
-    }
-    else
-    {
-        new_head = mid;
-        mid = heap_segment_next (mid);
-    }
-    heap_segment* new_tail = new_head;
-    while ((head != nullptr) && (mid != nullptr))
-    {
-        heap_segment* region = nullptr;
-        if (compare_by_committed_and_age (head, mid) <= 0)
-        {
-            region = head;
-            head = heap_segment_next (head);
-        }
-        else
-        {
-            region = mid;
-            mid = heap_segment_next (mid);
-        }
-        heap_segment_next (new_tail) = region;
-        new_tail = region;
-    }
-    if (head != nullptr)
-    {
-        assert (mid == nullptr);
-        heap_segment_next (new_tail) = head;
-    }
-    else
-    {
-        heap_segment_next (new_tail) = mid;
-    }
-    return new_head;
-}
-void region_free_list::sort_by_committed_and_age()
-{
-    if (num_free_regions <= 1)
-        return;
-    heap_segment* new_head = merge_sort_by_committed_and_age (head_free_region, num_free_regions);
-    head_free_region = new_head;
-    heap_segment* prev = nullptr;
-    for (heap_segment* region = new_head; region != nullptr; region = heap_segment_next (region))
-    {
-        heap_segment_prev_free_region (region) = prev;
-        assert ((prev == nullptr) || (compare_by_committed_and_age (prev, region) <= 0));
-        prev = region;
-    }
-    tail_free_region = prev;
-}
-#endif //USE_REGIONS
-void gc_heap::distribute_free_regions()
-{
-#ifdef USE_REGIONS
-    const int kind_count = large_free_region + 1;
-#ifdef MULTIPLE_HEAPS
-    BOOL joined_last_gc_before_oom = FALSE;
-    for (int i = 0; i < n_heaps; i++)
-    {
-        if (g_heaps[i]->last_gc_before_oom)
-        {
-            joined_last_gc_before_oom = TRUE;
-            break;
-        }
-    }
-#else
-    BOOL joined_last_gc_before_oom = last_gc_before_oom;
-#endif //MULTIPLE_HEAPS
-    if (settings.reason == reason_induced_aggressive)
-    {
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-#else //MULTIPLE_HEAPS
-        {
-            gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-            for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
-            {
-                global_regions_to_decommit[kind].transfer_regions (&hp->free_regions[kind]);
-            }
-        }
-        while (decommit_step(DECOMMIT_TIME_STEP_MILLISECONDS))
-        {
-        }
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-            int hn = i;
-#else //MULTIPLE_HEAPS
-        {
-            gc_heap* hp = pGenGCHeap;
-            int hn  = 0;
-#endif //MULTIPLE_HEAPS
-            for (int i = 0; i < total_generation_count; i++)
-            {
-                generation* generation = hp->generation_of (i);
-                heap_segment* region = heap_segment_rw (generation_start_segment (generation));
-                while (region != nullptr)
-                {
-                    uint8_t* aligned_allocated = align_on_page (heap_segment_allocated (region));
-                    size_t end_space = heap_segment_committed (region) - aligned_allocated;
-                    if (end_space > 0)
-                    {
-                        virtual_decommit (aligned_allocated, end_space, gen_to_oh (i), hn);
-                        heap_segment_committed (region) = aligned_allocated;
-                        heap_segment_used (region) = min (heap_segment_used (region), heap_segment_committed (region));
-                        assert (heap_segment_committed (region) > heap_segment_mem (region));
-                    }
-                    region = heap_segment_next_rw (region);
-                }
-            }
-        }
-        return;
-    }
-    size_t total_num_free_regions[kind_count] = { 0, 0 };
-    size_t total_budget_in_region_units[kind_count] = { 0,  0 };
-    size_t num_decommit_regions_by_time = 0;
-    size_t size_decommit_regions_by_time = 0;
-    size_t heap_budget_in_region_units[MAX_SUPPORTED_CPUS][kind_count];
-    size_t min_heap_budget_in_region_units[MAX_SUPPORTED_CPUS];
-    size_t region_size[kind_count] = { global_region_allocator.get_region_alignment(), global_region_allocator.get_large_region_alignment() };
-    region_free_list surplus_regions[kind_count];
-    for (int kind = basic_free_region; kind < kind_count; kind++)
-    {
-        surplus_regions[kind].transfer_regions (&global_regions_to_decommit[kind]);
-    }
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap* hp = g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-        const int i = 0;
-        const int n_heaps = 1;
-#endif //MULTIPLE_HEAPS
-        for (int kind = basic_free_region; kind < kind_count; kind++)
-        {
-            region_free_list& region_list = hp->free_regions[kind];
-            heap_segment* next_region = nullptr;
-            for (heap_segment* region = region_list.get_first_free_region(); region != nullptr; region = next_region)
-            {
-                next_region = heap_segment_next (region);
-                int age_in_free_to_decommit = min (max (AGE_IN_FREE_TO_DECOMMIT, n_heaps), MAX_AGE_IN_FREE);
-                if ((heap_segment_age_in_free (region) >= age_in_free_to_decommit) ||
-                    ((get_region_committed_size (region) == GC_PAGE_SIZE) && joined_last_gc_before_oom))
-                {
-                    num_decommit_regions_by_time++;
-                    size_decommit_regions_by_time += get_region_committed_size (region);
-                    dprintf (REGIONS_LOG, ("h%2d region %p age %2d, decommit",
-                        i, heap_segment_mem (region), heap_segment_age_in_free (region)));
-                    region_free_list::unlink_region (region);
-                    region_free_list::add_region (region, global_regions_to_decommit);
-                }
-            }
-            total_num_free_regions[kind] += region_list.get_num_free_regions();
-        }
-        global_free_huge_regions.transfer_regions (&hp->free_regions[huge_free_region]);
-        heap_budget_in_region_units[i][basic_free_region] = 0;
-        min_heap_budget_in_region_units[i] = 0;
-        heap_budget_in_region_units[i][large_free_region] = 0;
-    }
-    for (int gen = soh_gen0; gen < total_generation_count; gen++)
-    {
-        if ((gen <= soh_gen2) &&
-            total_budget_in_region_units[basic_free_region] >= (total_num_free_regions[basic_free_region] +
-                                                                surplus_regions[basic_free_region].get_num_free_regions()))
-        {
-            dprintf (REGIONS_LOG, ("out of free regions - skipping gen %d budget = %zd >= avail %zd",
-                gen,
-                total_budget_in_region_units[basic_free_region],
-                total_num_free_regions[basic_free_region] + surplus_regions[basic_free_region].get_num_free_regions()));
-            continue;
-        }
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-#else //MULTIPLE_HEAPS
-        {
-            gc_heap* hp = pGenGCHeap;
-            const int i = 0;
-            const int n_heaps = 1;
-#endif //MULTIPLE_HEAPS
-            ptrdiff_t budget_gen = max (hp->estimate_gen_growth (gen), 0);
-            int kind = gen >= loh_generation;
-            size_t budget_gen_in_region_units = (budget_gen + (region_size[kind] - 1)) / region_size[kind];
-            dprintf (REGIONS_LOG, ("h%2d gen %d has an estimated growth of %zd bytes (%zd regions)", i, gen, budget_gen, budget_gen_in_region_units));
-            if (gen <= soh_gen2)
-            {
-                min_heap_budget_in_region_units[i] = heap_budget_in_region_units[i][kind];
-            }
-            heap_budget_in_region_units[i][kind] += budget_gen_in_region_units;
-            total_budget_in_region_units[kind] += budget_gen_in_region_units;
-        }
-    }
-    dprintf (1, ("moved %2zd regions (%8zd) to decommit based on time", num_decommit_regions_by_time, size_decommit_regions_by_time));
-    global_free_huge_regions.transfer_regions (&global_regions_to_decommit[huge_free_region]);
-    size_t free_space_in_huge_regions = global_free_huge_regions.get_size_free_regions();
-    ptrdiff_t num_regions_to_decommit[kind_count];
-    int region_factor[kind_count] = { 1, LARGE_REGION_FACTOR };
-#ifdef TRACE_GC
-    const char* kind_name[count_free_region_kinds] = { "basic", "large", "huge"};
-#endif // TRACE_GC
-#ifndef MULTIPLE_HEAPS
-    const int n_heaps = 1;
-#endif //!MULTIPLE_HEAPS
-    size_t num_huge_region_units_to_consider[kind_count] = { 0, free_space_in_huge_regions / region_size[large_free_region] };
-    for (int kind = basic_free_region; kind < kind_count; kind++)
-    {
-        num_regions_to_decommit[kind] = surplus_regions[kind].get_num_free_regions();
-        dprintf(REGIONS_LOG, ("%zd %s free regions, %zd regions budget, %zd regions on decommit list, %zd huge regions to consider",
-            total_num_free_regions[kind],
-            kind_name[kind],
-            total_budget_in_region_units[kind],
-            num_regions_to_decommit[kind],
-            num_huge_region_units_to_consider[kind]));
-        total_num_free_regions[kind] += num_regions_to_decommit[kind];
-        ptrdiff_t balance = total_num_free_regions[kind] + num_huge_region_units_to_consider[kind] - total_budget_in_region_units[kind];
-        if (
-#ifdef BACKGROUND_GC
-            background_running_p() ||
-#endif
-            (balance < 0))
-        {
-            dprintf (REGIONS_LOG, ("distributing the %zd %s regions deficit", -balance, kind_name[kind]));
-#ifdef MULTIPLE_HEAPS
-            if (balance != 0)
-            {
-                ptrdiff_t curr_balance = 0;
-                ptrdiff_t rem_balance = 0;
-                for (int i = 0; i < n_heaps; i++)
-                {
-                    curr_balance += balance;
-                    ptrdiff_t adjustment_per_heap = curr_balance / n_heaps;
-                    curr_balance -= adjustment_per_heap * n_heaps;
-                    ptrdiff_t new_budget = (ptrdiff_t)heap_budget_in_region_units[i][kind] + adjustment_per_heap;
-                    ptrdiff_t min_budget = (kind == basic_free_region) ? (ptrdiff_t)min_heap_budget_in_region_units[i] : 0;
-                    dprintf (REGIONS_LOG, ("adjusting the budget for heap %d from %zd %s regions by %zd to %zd",
-                        i,
-                        heap_budget_in_region_units[i][kind],
-                        kind_name[kind],
-                        adjustment_per_heap,
-                        max (min_budget, new_budget)));
-                    heap_budget_in_region_units[i][kind] = max (min_budget, new_budget);
-                    rem_balance += new_budget - heap_budget_in_region_units[i][kind];
-                }
-                assert (rem_balance <= 0);
-                dprintf (REGIONS_LOG, ("remaining balance: %zd %s regions", rem_balance, kind_name[kind]));
-                while (rem_balance < 0)
-                {
-                    for (int i = 0; i < n_heaps; i++)
-                    {
-                        size_t min_budget = (kind == basic_free_region) ? min_heap_budget_in_region_units[i] : 0;
-                        if (heap_budget_in_region_units[i][kind] > min_budget)
-                        {
-                            dprintf (REGIONS_LOG, ("adjusting the budget for heap %d from %zd %s regions by %d to %zd",
-                                i,
-                                heap_budget_in_region_units[i][kind],
-                                kind_name[kind],
-                                -1,
-                                heap_budget_in_region_units[i][kind] - 1));
-                            heap_budget_in_region_units[i][kind] -= 1;
-                            rem_balance += 1;
-                            if (rem_balance == 0)
-                                break;
-                        }
-                    }
-                }
-            }
-#endif //MULTIPLE_HEAPS
-        }
-        else
-        {
-            num_regions_to_decommit[kind] = balance;
-            dprintf(REGIONS_LOG, ("distributing the %zd %s regions, removing %zd regions",
-                total_budget_in_region_units[kind],
-                kind_name[kind],
-                num_regions_to_decommit[kind]));
-            if (num_regions_to_decommit[kind] > 0)
-            {
-                size_t num_regions_to_decommit_before = global_regions_to_decommit[kind].get_num_free_regions();
-                global_region_allocator.move_highest_free_regions (num_regions_to_decommit[kind]*region_factor[kind],
-                                                                   kind == basic_free_region,
-                                                                   global_regions_to_decommit);
-                dprintf (REGIONS_LOG, ("Moved %zd %s regions to decommit list",
-                         global_regions_to_decommit[kind].get_num_free_regions(), kind_name[kind]));
-                if (kind == basic_free_region)
-                {
-                    assert (global_regions_to_decommit[kind].get_num_free_regions() ==
-                            num_regions_to_decommit_before + (size_t)num_regions_to_decommit[kind]);
-                }
-                else
-                {
-                    dprintf (REGIONS_LOG, ("Moved %zd %s regions to decommit list",
-                        global_regions_to_decommit[huge_free_region].get_num_free_regions(), kind_name[huge_free_region]));
-                }
-            }
-        }
-    }
-    for (int kind = basic_free_region; kind < kind_count; kind++)
-    {
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-            if (hp->free_regions[kind].get_num_free_regions() > heap_budget_in_region_units[i][kind])
-            {
-                dprintf (REGIONS_LOG, ("removing %zd %s regions from heap %d with %zd regions, budget is %zd",
-                    hp->free_regions[kind].get_num_free_regions() - heap_budget_in_region_units[i][kind],
-                    kind_name[kind],
-                    i,
-                    hp->free_regions[kind].get_num_free_regions(),
-                    heap_budget_in_region_units[i][kind]));
-                remove_surplus_regions (&hp->free_regions[kind], &surplus_regions[kind], heap_budget_in_region_units[i][kind]);
-            }
-        }
-        for (int i = 0; i < n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-#else //MULTIPLE_HEAPS
-        {
-            gc_heap* hp = pGenGCHeap;
-            const int i = 0;
-#endif //MULTIPLE_HEAPS
-            if (hp->free_regions[kind].get_num_free_regions() < heap_budget_in_region_units[i][kind])
-            {
-                int64_t num_added_regions = add_regions (&hp->free_regions[kind], &surplus_regions[kind], heap_budget_in_region_units[i][kind]);
-                dprintf (REGIONS_LOG, ("added %zd %s regions to heap %d - now has %zd, budget is %zd",
-                    (size_t)num_added_regions,
-                    kind_name[kind],
-                    i,
-                    hp->free_regions[kind].get_num_free_regions(),
-                    heap_budget_in_region_units[i][kind]));
-            }
-            hp->free_regions[kind].sort_by_committed_and_age();
-        }
-        if (surplus_regions[kind].get_num_free_regions() > 0)
-        {
-            assert (!"should have exhausted the surplus_regions");
-            global_regions_to_decommit[kind].transfer_regions (&surplus_regions[kind]);
-        }
-    }
-#ifdef MULTIPLE_HEAPS
-    for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
-    {
-        if (global_regions_to_decommit[kind].get_num_free_regions() != 0)
-        {
-            gradual_decommit_in_progress_p = TRUE;
-            break;
-        }
-    }
-#else //MULTIPLE_HEAPS
-    dynamic_data* dd0 = dynamic_data_of (0);
-    size_t ephemeral_elapsed = (size_t)((dd_time_clock (dd0) - gc_last_ephemeral_decommit_time) / 1000);
-    if (ephemeral_elapsed >= DECOMMIT_TIME_STEP_MILLISECONDS)
-    {
-        gc_last_ephemeral_decommit_time = dd_time_clock (dd0);
-        size_t decommit_step_milliseconds = min (ephemeral_elapsed, (10*1000));
-        decommit_step (decommit_step_milliseconds);
-    }
-    for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
-    {
-        if (global_regions_to_decommit[kind].get_num_free_regions() != 0)
-        {
-            free_regions[kind].transfer_regions (&global_regions_to_decommit[kind]);
-        }
-    }
-#endif //MULTIPLE_HEAPS
-#endif //USE_REGIONS
-}
-#ifdef WRITE_WATCH
-uint8_t* g_addresses [array_size+2]; // to get around the bug in GetWriteWatch
-#ifdef CARD_BUNDLE
-inline void gc_heap::verify_card_bundle_bits_set(size_t first_card_word, size_t last_card_word)
-{
-#ifdef _DEBUG
-    for (size_t x = cardw_card_bundle (first_card_word); x < cardw_card_bundle (last_card_word); x++)
-    {
-        if (!card_bundle_set_p (x))
-        {
-            assert (!"Card bundle not set");
-            dprintf (3, ("Card bundle %zx not set", x));
-        }
-    }
-#else
-    UNREFERENCED_PARAMETER(first_card_word);
-    UNREFERENCED_PARAMETER(last_card_word);
-#endif
-}
-inline void gc_heap::verify_card_bundles()
-{
-#ifdef _DEBUG
-    size_t lowest_card = card_word (card_of (lowest_address));
-#ifdef USE_REGIONS
-    size_t highest_card = card_word (card_of (global_region_allocator.get_left_used_unsafe()));
-#else
-    size_t highest_card = card_word (card_of (highest_address));
-#endif
-    size_t cardb = cardw_card_bundle (lowest_card);
-    size_t end_cardb = cardw_card_bundle (align_cardw_on_bundle (highest_card));
-    while (cardb < end_cardb)
-    {
-        uint32_t* card_word = &card_table[max(card_bundle_cardw (cardb), lowest_card)];
-        uint32_t* card_word_end = &card_table[min(card_bundle_cardw (cardb+1), highest_card)];
-        if (card_bundle_set_p (cardb) == 0)
-        {
-            while (card_word < card_word_end)
-            {
-                if (*card_word != 0)
-                {
-                    dprintf  (3, ("gc: %zd, Card word %zx for address %zx set, card_bundle %zx clear",
-                            dd_collection_count (dynamic_data_of (0)),
-                            (size_t)(card_word-&card_table[0]),
-                            (size_t)(card_address ((size_t)(card_word-&card_table[0]) * card_word_width)),
-                            cardb));
-                }
-                assert((*card_word)==0);
-                card_word++;
-            }
-        }
-        cardb++;
-    }
-#endif
-}
-void gc_heap::update_card_table_bundle()
-{
-    if (card_bundles_enabled())
-    {
-        uint8_t* base_address = (uint8_t*)(&card_table[card_word (card_of (lowest_address))]);
-#ifdef USE_REGIONS
-        uint8_t* high_address = (uint8_t*)(&card_table[card_word (card_of (global_region_allocator.get_left_used_unsafe()))]);
-#else
-        uint8_t* high_address = (uint8_t*)(&card_table[card_word (card_of (highest_address))]);
-#endif //USE_REGIONS
-        uint8_t* saved_base_address = base_address;
-        uintptr_t bcount = array_size;
-        size_t saved_region_size = align_on_page (high_address) - saved_base_address;
-        do
-        {
-            size_t region_size = align_on_page (high_address) - base_address;
-            dprintf (3,("Probing card table pages [%zx, %zx[",
-                (size_t)base_address, (size_t)(base_address + region_size)));
-            bool success = GCToOSInterface::GetWriteWatch(false /* resetState */,
-                                                          base_address,
-                                                          region_size,
-                                                          (void**)g_addresses,
-                                                          &bcount);
-            assert (success && "GetWriteWatch failed!");
-            dprintf (3,("Found %zd pages written", bcount));
-            for (unsigned i = 0; i < bcount; i++)
-            {
-                size_t bcardw = (uint32_t*)(max(g_addresses[i],base_address)) - &card_table[0];
-                size_t ecardw = (uint32_t*)(min(g_addresses[i]+OS_PAGE_SIZE, high_address)) - &card_table[0];
-                assert (bcardw >= card_word (card_of (g_gc_lowest_address)));
-                card_bundles_set (cardw_card_bundle (bcardw),
-                                  cardw_card_bundle (align_cardw_on_bundle (ecardw)));
-                dprintf (3,("Set Card bundle [%zx, %zx[",
-                    cardw_card_bundle (bcardw), cardw_card_bundle (align_cardw_on_bundle (ecardw))));
-                verify_card_bundle_bits_set(bcardw, ecardw);
-            }
-            if (bcount >= array_size)
-            {
-                base_address = g_addresses [array_size-1] + OS_PAGE_SIZE;
-                bcount = array_size;
-            }
-        } while ((bcount >= array_size) && (base_address < high_address));
-        GCToOSInterface::ResetWriteWatch (saved_base_address, saved_region_size);
-    }
-}
-#endif //CARD_BUNDLE
-#ifdef BACKGROUND_GC
-void gc_heap::reset_write_watch_for_gc_heap(void* base_address, size_t region_size)
-{
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    SoftwareWriteWatch::ClearDirty(base_address, region_size);
-#else // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    GCToOSInterface::ResetWriteWatch(base_address, region_size);
-#endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-}
-void gc_heap::get_write_watch_for_gc_heap(bool reset, void *base_address, size_t region_size,
-                                          void** dirty_pages, uintptr_t* dirty_page_count_ref,
-                                          bool is_runtime_suspended)
-{
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    SoftwareWriteWatch::GetDirty(base_address, region_size, dirty_pages, dirty_page_count_ref,
-                                 reset, is_runtime_suspended);
-#else // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    UNREFERENCED_PARAMETER(is_runtime_suspended);
-    bool success = GCToOSInterface::GetWriteWatch(reset, base_address, region_size, dirty_pages,
-                                                  dirty_page_count_ref);
-    assert(success);
-#endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-}
-const size_t ww_reset_quantum = 128*1024*1024;
-inline
-void gc_heap::switch_one_quantum()
-{
-    enable_preemptive ();
-    GCToOSInterface::Sleep (1);
-    disable_preemptive (true);
-}
-void gc_heap::reset_ww_by_chunk (uint8_t* start_address, size_t total_reset_size)
-{
-    size_t reset_size = 0;
-    size_t remaining_reset_size = 0;
-    size_t next_reset_size = 0;
-    while (reset_size != total_reset_size)
-    {
-        remaining_reset_size = total_reset_size - reset_size;
-        next_reset_size = ((remaining_reset_size >= ww_reset_quantum) ?
-            ww_reset_quantum : remaining_reset_size);
-        if (next_reset_size)
-        {
-            reset_write_watch_for_gc_heap(start_address, next_reset_size);
-            reset_size += next_reset_size;
-            switch_one_quantum();
-        }
-    }
-    assert (reset_size == total_reset_size);
-}
-void gc_heap::switch_on_reset (BOOL concurrent_p, size_t* current_total_reset_size, size_t last_reset_size)
-{
-    if (concurrent_p)
-    {
-        *current_total_reset_size += last_reset_size;
-        dprintf (2, ("reset %zd bytes so far", *current_total_reset_size));
-        if (*current_total_reset_size > ww_reset_quantum)
-        {
-            switch_one_quantum();
-            *current_total_reset_size = 0;
-        }
-    }
-}
-void gc_heap::reset_write_watch (BOOL concurrent_p)
-{
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    assert(!concurrent_p);
-#endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    dprintf (2, ("bgc lowest: %p, bgc highest: %p",
-        background_saved_lowest_address, background_saved_highest_address));
-    size_t reset_size = 0;
-    for (int i = get_start_generation_index(); i < total_generation_count; i++)
-    {
-        heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (i)));
-        while (seg)
-        {
-            uint8_t* base_address = align_lower_page (heap_segment_mem (seg));
-            base_address = max (base_address, background_saved_lowest_address);
-            uint8_t* high_address = ((seg == ephemeral_heap_segment) ?
-                alloc_allocated : heap_segment_allocated (seg));
-            high_address = min (high_address, background_saved_highest_address);
-            if (base_address < high_address)
-            {
-                size_t reset_size = 0;
-                size_t region_size = high_address - base_address;
-                dprintf (3, ("h%d, gen: %x, ww: [%zx(%zd)", heap_number, i, (size_t)base_address, region_size));
-                reset_write_watch_for_gc_heap(base_address, region_size);
-                switch_on_reset (concurrent_p, &reset_size, region_size);
-            }
-            seg = heap_segment_next_rw (seg);
-            concurrent_print_time_delta (i == max_generation ? "CRWW soh": "CRWW uoh");
-        }
-    }
-}
-#endif //BACKGROUND_GC
-#endif //WRITE_WATCH
-#ifdef BACKGROUND_GC
-void gc_heap::restart_vm()
-{
-    dprintf (3, ("Restarting EE"));
-    STRESS_LOG0(LF_GC, LL_INFO10000, "Concurrent GC: Restarting EE\n");
-    ee_proceed_event.Set();
-}
-inline
-void fire_alloc_wait_event (alloc_wait_reason awr, BOOL begin_p)
-{
-    if (awr != awr_ignored)
-    {
-        if (begin_p)
-        {
-            FIRE_EVENT(BGCAllocWaitBegin, awr);
-        }
-        else
-        {
-            FIRE_EVENT(BGCAllocWaitEnd, awr);
-        }
-    }
-}
-void gc_heap::fire_alloc_wait_event_begin (alloc_wait_reason awr)
-{
-    fire_alloc_wait_event (awr, TRUE);
-}
-void gc_heap::fire_alloc_wait_event_end (alloc_wait_reason awr)
-{
-    fire_alloc_wait_event (awr, FALSE);
-}
-#endif //BACKGROUND_GC
-void gc_heap::make_generation (int gen_num, heap_segment* seg, uint8_t* start)
-{
-    generation* gen = generation_of (gen_num);
-    gen->gen_num = gen_num;
-#ifndef USE_REGIONS
-    gen->allocation_start = start;
-    gen->plan_allocation_start = 0;
-#endif //USE_REGIONS
-    gen->allocation_context.alloc_ptr = 0;
-    gen->allocation_context.alloc_limit = 0;
-    gen->allocation_context.alloc_bytes = 0;
-    gen->allocation_context.alloc_bytes_uoh = 0;
-    gen->allocation_context_start_region = 0;
-    gen->start_segment = seg;
-#ifdef USE_REGIONS
-    dprintf (REGIONS_LOG, ("g%d start seg is %zx-%p", gen_num, (size_t)seg, heap_segment_mem (seg)));
-    gen->tail_region = seg;
-    gen->plan_start_segment = 0;
-    gen->tail_ro_region = 0;
-#endif //USE_REGIONS
-    gen->allocation_segment = seg;
-    gen->free_list_space = 0;
-    gen->free_list_allocated = 0;
-    gen->end_seg_allocated = 0;
-    gen->condemned_allocated = 0;
-    gen->sweep_allocated = 0;
-    gen->free_obj_space = 0;
-    gen->allocation_size = 0;
-    gen->pinned_allocation_sweep_size = 0;
-    gen->pinned_allocation_compact_size = 0;
-    gen->allocate_end_seg_p = FALSE;
-    gen->free_list_allocator.clear();
-#ifdef DOUBLY_LINKED_FL
-    gen->set_bgc_mark_bit_p = FALSE;
-#endif //DOUBLY_LINKED_FL
-#ifdef FREE_USAGE_STATS
-    memset (gen->gen_free_spaces, 0, sizeof (gen->gen_free_spaces));
-    memset (gen->gen_current_pinned_free_spaces, 0, sizeof (gen->gen_current_pinned_free_spaces));
-    memset (gen->gen_plugs, 0, sizeof (gen->gen_plugs));
-#endif //FREE_USAGE_STATS
-}
-void gc_heap::adjust_ephemeral_limits ()
-{
-#ifndef USE_REGIONS
-    ephemeral_low = generation_allocation_start (generation_of (max_generation - 1));
-    ephemeral_high = heap_segment_reserved (ephemeral_heap_segment);
-    dprintf (3, ("new ephemeral low: %zx new ephemeral high: %zx",
-        (size_t)ephemeral_low, (size_t)ephemeral_high))
-#ifndef MULTIPLE_HEAPS
-    stomp_write_barrier_ephemeral(ephemeral_low, ephemeral_high);
-#endif // MULTIPLE_HEAPS
-#endif //USE_REGIONS
-}
-#if defined(TRACE_GC) || defined(GC_CONFIG_DRIVEN)
-FILE* CreateLogFile(const GCConfigStringHolder& temp_logfile_name, bool is_config)
-{
-    FILE* logFile;
-    if (!temp_logfile_name.Get())
-    {
-        return nullptr;
-    }
-    char logfile_name[MAX_LONGPATH+1];
-    const char* suffix = is_config ? ".config.log" : ".log";
-    _snprintf_s(logfile_name, MAX_LONGPATH+1, _TRUNCATE, "%s%s", temp_logfile_name.Get(), suffix);
-    logFile = fopen(logfile_name, "wb");
-    return logFile;
-}
-#endif //TRACE_GC || GC_CONFIG_DRIVEN
-uint32_t adjust_heaps_hard_limit_worker (uint32_t nhp, size_t limit)
-{
-    if (!limit)
-        return nhp;
-    size_t aligned_limit =  align_on_segment_hard_limit (limit);
-    uint32_t nhp_oh = (uint32_t)(aligned_limit / min_segment_size_hard_limit);
-    nhp = min (nhp_oh, nhp);
-    return (max (nhp, 1));
-}
-uint32_t gc_heap::adjust_heaps_hard_limit (uint32_t nhp)
-{
-#ifdef MULTIPLE_HEAPS
-    if (heap_hard_limit_oh[soh])
-    {
-        for (int i = 0; i < (total_oh_count - 1); i++)
-        {
-            nhp = adjust_heaps_hard_limit_worker (nhp, heap_hard_limit_oh[i]);
-        }
-    }
-    else if (heap_hard_limit)
-    {
-        nhp = adjust_heaps_hard_limit_worker (nhp, heap_hard_limit);
-    }
-#endif
-    return nhp;
-}
-size_t gc_heap::adjust_segment_size_hard_limit_va (size_t seg_size)
-{
-    return (use_large_pages_p ?
-            align_on_segment_hard_limit (seg_size) :
-            round_up_power2 (seg_size));
-}
-size_t gc_heap::adjust_segment_size_hard_limit (size_t limit, uint32_t nhp)
-{
-    if (!limit)
-    {
-        limit = min_segment_size_hard_limit;
-    }
-    size_t seg_size = align_on_segment_hard_limit (limit) / nhp;
-    return adjust_segment_size_hard_limit_va (seg_size);
-}
-#ifdef USE_REGIONS
-bool allocate_initial_regions(int number_of_heaps)
-{
-    initial_regions = new (nothrow) uint8_t*[number_of_heaps][total_generation_count][2];
-    if (initial_regions == nullptr)
-    {
-        return false;
-    }
-    for (int i = 0; i < number_of_heaps; i++)
-    {
-        bool succeed = global_region_allocator.allocate_large_region(
-            poh_generation,
-            &initial_regions[i][poh_generation][0],
-            &initial_regions[i][poh_generation][1], allocate_forward, 0, nullptr);
-        assert(succeed);
-    }
-    for (int i = 0; i < number_of_heaps; i++)
-    {
-        for (int gen_num = max_generation; gen_num >= 0; gen_num--)
-        {
-            bool succeed = global_region_allocator.allocate_basic_region(
-                gen_num,
-                &initial_regions[i][gen_num][0],
-                &initial_regions[i][gen_num][1], nullptr);
-            assert(succeed);
-        }
-    }
-    for (int i = 0; i < number_of_heaps; i++)
-    {
-        bool succeed = global_region_allocator.allocate_large_region(
-            loh_generation,
-            &initial_regions[i][loh_generation][0],
-            &initial_regions[i][loh_generation][1], allocate_forward, 0, nullptr);
-        assert(succeed);
-    }
-    return true;
-}
-#endif
-HRESULT gc_heap::initialize_gc (size_t soh_segment_size,
-                                size_t loh_segment_size,
-                                size_t poh_segment_size
-#ifdef MULTIPLE_HEAPS
-                                ,int number_of_heaps
-#endif //MULTIPLE_HEAPS
-)
-{
-#if defined(TRACE_GC) && defined(SIMPLE_DPRINTF)
-    if (GCConfig::GetLogEnabled())
-    {
-        gc_log = CreateLogFile(GCConfig::GetLogFile(), false);
-        if (gc_log == NULL)
-        {
-            GCToEEInterface::LogErrorToHost("Cannot create log file");
-            return E_FAIL;
-        }
-        gc_log_file_size = static_cast<size_t>(GCConfig::GetLogFileSize());
-        if (gc_log_file_size <= 0 || gc_log_file_size > 500)
-        {
-            GCToEEInterface::LogErrorToHost("Invalid log file size (valid size needs to be larger than 0 and smaller than 500)");
-            fclose (gc_log);
-            return E_FAIL;
-        }
-        gc_log_lock.Initialize();
-        gc_log_buffer = new (nothrow) uint8_t [gc_log_buffer_size];
-        if (!gc_log_buffer)
-        {
-            fclose(gc_log);
-            return E_OUTOFMEMORY;
-        }
-        memset (gc_log_buffer, '*', gc_log_buffer_size);
-        max_gc_buffers = gc_log_file_size * 1024 * 1024 / gc_log_buffer_size;
-    }
-#endif //TRACE_GC && SIMPLE_DPRINTF
-#ifdef GC_CONFIG_DRIVEN
-    if (GCConfig::GetConfigLogEnabled())
-    {
-        gc_config_log = CreateLogFile(GCConfig::GetConfigLogFile(), true);
-        if (gc_config_log == NULL)
-        {
-            GCToEEInterface::LogErrorToHost("Cannot create log file");
-            return E_FAIL;
-        }
-        gc_config_log_buffer = new (nothrow) uint8_t [gc_config_log_buffer_size];
-        if (!gc_config_log_buffer)
-        {
-            fclose(gc_config_log);
-            return E_OUTOFMEMORY;
-        }
-        compact_ratio = static_cast<int>(GCConfig::GetCompactRatio());
-        cprintf (("%2s | %6s | %1s | %1s | %2s | %2s | %2s | %2s | %2s || %5s | %5s | %5s | %5s | %5s | %5s | %5s | %5s | %5s |",
-                "h#", // heap index
-                "GC", // GC index
-                "g", // generation
-                "C",  // compaction (empty means sweeping), 'M' means it was mandatory, 'W' means it was not
-                "EX", // heap expansion
-                "NF", // normal fit
-                "BF", // best fit (if it indicates neither NF nor BF it means it had to acquire a new seg.
-                "ML", // mark list
-                "DM", // demotion
-                "PreS", // short object before pinned plug
-                "PostS", // short object after pinned plug
-                "Merge", // merged pinned plugs
-                "Conv", // converted to pinned plug
-                "Pre", // plug before pinned plug but not after
-                "Post", // plug after pinned plug but not before
-                "PrPo", // plug both before and after pinned plug
-                "PreP", // pre short object padded
-                "PostP" // post short object padded
-                ));
-    }
-#endif //GC_CONFIG_DRIVEN
-    HRESULT hres = S_OK;
-    conserve_mem_setting = (int)GCConfig::GetGCConserveMem();
-#ifdef DYNAMIC_HEAP_COUNT
-    dynamic_adaptation_mode = (int)GCConfig::GetGCDynamicAdaptationMode();
-    if (GCConfig::GetHeapCount() != 0)
-    {
-        dynamic_adaptation_mode = 0;
-    }
-    if ((dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes) && (conserve_mem_setting == 0))
-        conserve_mem_setting = 5;
-#endif //DYNAMIC_HEAP_COUNT
-    if (conserve_mem_setting < 0)
-        conserve_mem_setting = 0;
-    if (conserve_mem_setting > 9)
-        conserve_mem_setting = 9;
-    dprintf (1, ("conserve_mem_setting = %d", conserve_mem_setting));
-#ifdef WRITE_WATCH
-    hardware_write_watch_api_supported();
-#ifdef BACKGROUND_GC
-    if (can_use_write_watch_for_gc_heap() && GCConfig::GetConcurrentGC())
-    {
-        gc_can_use_concurrent = true;
-#ifndef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-        virtual_alloc_hardware_write_watch = true;
-#endif // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    }
-    else
-    {
-        gc_can_use_concurrent = false;
-    }
-    GCConfig::SetConcurrentGC(gc_can_use_concurrent);
-#else //BACKGROUND_GC
-    GCConfig::SetConcurrentGC(false);
-#endif //BACKGROUND_GC
-#endif //WRITE_WATCH
-#ifdef BACKGROUND_GC
-    segment_info_size = OS_PAGE_SIZE;
-#else
-    segment_info_size = Align (sizeof (heap_segment), get_alignment_constant (FALSE));
-#endif //BACKGROUND_GC
-    reserved_memory = 0;
-    size_t initial_heap_size = soh_segment_size + loh_segment_size + poh_segment_size;
-    uint16_t* heap_no_to_numa_node = nullptr;
-#ifdef MULTIPLE_HEAPS
-    reserved_memory_limit = initial_heap_size * number_of_heaps;
-    if (!heap_select::init(number_of_heaps))
-        return E_OUTOFMEMORY;
-    if (GCToOSInterface::CanEnableGCNumaAware())
-        heap_no_to_numa_node = heap_select::heap_no_to_numa_node;
-#else //MULTIPLE_HEAPS
-    reserved_memory_limit = initial_heap_size;
-    int number_of_heaps = 1;
-#endif //MULTIPLE_HEAPS
-#ifndef COMMITTED_BYTES_SHADOW
-    if (heap_hard_limit)
-#endif //!COMMITTED_BYTES_SHADOW
-    {
-        check_commit_cs.Initialize();
-    }
-    decommit_lock.Initialize();
-#ifdef USE_REGIONS
-    if (regions_range)
-    {
-        size_t reserve_size = regions_range;
-        uint8_t* reserve_range = (uint8_t*)virtual_alloc (reserve_size, use_large_pages_p);
-        if (!reserve_range)
-            return E_OUTOFMEMORY;
-        if (!global_region_allocator.init (reserve_range, (reserve_range + reserve_size),
-                                           ((size_t)1 << min_segment_size_shr),
-                                           &g_gc_lowest_address, &g_gc_highest_address))
-            return E_OUTOFMEMORY;
-        if (!allocate_initial_regions(number_of_heaps))
-            return E_OUTOFMEMORY;
-    }
-    else
-    {
-        assert (!"cannot use regions without specifying the range!!!");
-        GCToEEInterface::LogErrorToHost("Cannot use regions without specifying the range (using DOTNET_GCRegionRange)");
-        return E_FAIL;
-    }
-#else //USE_REGIONS
-    bool separated_poh_p = use_large_pages_p &&
-                           heap_hard_limit_oh[soh] &&
-                           (GCConfig::GetGCHeapHardLimitPOH() == 0) &&
-                           (GCConfig::GetGCHeapHardLimitPOHPercent() == 0);
-    if (!reserve_initial_memory (soh_segment_size, loh_segment_size, poh_segment_size, number_of_heaps,
-                                 use_large_pages_p, separated_poh_p, heap_no_to_numa_node))
-        return E_OUTOFMEMORY;
-    if (use_large_pages_p)
-    {
-        if (heap_hard_limit_oh[soh])
-        {
-            heap_hard_limit_oh[soh] = soh_segment_size * number_of_heaps;
-            heap_hard_limit_oh[loh] = loh_segment_size * number_of_heaps;
-            heap_hard_limit_oh[poh] = poh_segment_size * number_of_heaps;
-            heap_hard_limit = heap_hard_limit_oh[soh] + heap_hard_limit_oh[loh] + heap_hard_limit_oh[poh];
-        }
-        else
-        {
-            assert (heap_hard_limit);
-            heap_hard_limit = (soh_segment_size + loh_segment_size + poh_segment_size) * number_of_heaps;
-        }
-    }
-#endif //USE_REGIONS
-#ifdef CARD_BUNDLE
-#ifdef MULTIPLE_HEAPS
-    uint64_t th = (uint64_t)MH_TH_CARD_BUNDLE*number_of_heaps;
-#else
-    uint64_t th = (uint64_t)SH_TH_CARD_BUNDLE;
-#endif //MULTIPLE_HEAPS
-    if (can_use_write_watch_for_card_table() && reserved_memory >= th)
-    {
-        settings.card_bundles = TRUE;
-    }
-    else
-    {
-        settings.card_bundles = FALSE;
-    }
-#endif //CARD_BUNDLE
-    settings.first_init();
-    int latency_level_from_config = static_cast<int>(GCConfig::GetLatencyLevel());
-    if (latency_level_from_config >= latency_level_first && latency_level_from_config <= latency_level_last)
-    {
-        gc_heap::latency_level = static_cast<gc_latency_level>(latency_level_from_config);
-    }
-    init_static_data();
-    g_gc_card_table = make_card_table (g_gc_lowest_address, g_gc_highest_address);
-    if (!g_gc_card_table)
-        return E_OUTOFMEMORY;
-    gc_started = FALSE;
-#ifdef MULTIPLE_HEAPS
-    g_heaps = new (nothrow) gc_heap* [number_of_heaps];
-    if (!g_heaps)
-        return E_OUTOFMEMORY;
-#ifdef _PREFAST_
-#pragma warning(push)
-#pragma warning(disable:22011) // Suppress PREFast warning about integer underflow/overflow
-#endif // _PREFAST_
-#if !defined(USE_REGIONS) || defined(_DEBUG)
-    g_promoted = new (nothrow) size_t [number_of_heaps*16];
-    if (!g_promoted)
-        return E_OUTOFMEMORY;
-#endif //!USE_REGIONS || _DEBUG
-#ifdef BACKGROUND_GC
-    g_bpromoted = new (nothrow) size_t [number_of_heaps*16];
-    if (!g_bpromoted)
-        return E_OUTOFMEMORY;
-#endif
-#ifdef MH_SC_MARK
-    g_mark_stack_busy = new (nothrow) int[(number_of_heaps+2)*HS_CACHE_LINE_SIZE/sizeof(int)];
-#endif //MH_SC_MARK
-#ifdef _PREFAST_
-#pragma warning(pop)
-#endif // _PREFAST_
-#ifdef MH_SC_MARK
-    if (!g_mark_stack_busy)
-        return E_OUTOFMEMORY;
-#endif //MH_SC_MARK
-    if (!create_thread_support (number_of_heaps))
-        return E_OUTOFMEMORY;
-#endif //MULTIPLE_HEAPS
-#ifdef MULTIPLE_HEAPS
-    yp_spin_count_unit = 32 * number_of_heaps;
-#else
-    yp_spin_count_unit = 32 * g_num_processors;
-#endif //MULTIPLE_HEAPS
-    int64_t spin_count_unit_from_config = GCConfig::GetGCSpinCountUnit();
-    gc_heap::spin_count_unit_config_p = (spin_count_unit_from_config > 0) && (spin_count_unit_from_config <= MAX_YP_SPIN_COUNT_UNIT);
-    if (gc_heap::spin_count_unit_config_p)
-    {
-        yp_spin_count_unit = static_cast<int32_t>(spin_count_unit_from_config);
-    }
-    original_spin_count_unit = yp_spin_count_unit;
-#if defined(__linux__)
-    GCToEEInterface::UpdateGCEventStatus(static_cast<int>(GCEventStatus::GetEnabledLevel(GCEventProvider_Default)),
-                                         static_cast<int>(GCEventStatus::GetEnabledKeywords(GCEventProvider_Default)),
-                                         static_cast<int>(GCEventStatus::GetEnabledLevel(GCEventProvider_Private)),
-                                         static_cast<int>(GCEventStatus::GetEnabledKeywords(GCEventProvider_Private)));
-#endif // __linux__
-#ifdef USE_VXSORT
-    InitSupportedInstructionSet ((int32_t)GCConfig::GetGCEnabledInstructionSets());
-#endif
-    if (!init_semi_shared())
-    {
-        GCToEEInterface::LogErrorToHost("PER_HEAP_ISOLATED data members initialization failed");
-        hres = E_FAIL;
-    }
-    return hres;
-}
-int
-gc_heap::init_semi_shared()
-{
-    int ret = 0;
-#ifdef BGC_SERVO_TUNING
-    uint32_t current_memory_load = 0;
-    uint32_t sweep_flr_goal = 0;
-    uint32_t sweep_flr_goal_loh = 0;
-#endif //BGC_SERVO_TUNING
-#ifndef USE_REGIONS
-    eph_gen_starts_size = (Align (min_obj_size)) * max_generation;
-#endif //!USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-    mark_list_size = min (100*1024, max (8192, soh_segment_size/(2*10*32)));
-#ifdef DYNAMIC_HEAP_COUNT
-    if (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes)
-    {
-        g_mark_list_total_size = mark_list_size;
-    }
-    else
-#endif //DYNAMIC_HEAP_COUNT
-    {
-        g_mark_list_total_size = mark_list_size*n_heaps;
-    }
-    g_mark_list = make_mark_list (g_mark_list_total_size);
-    min_balance_threshold = alloc_quantum_balance_units * CLR_SIZE * 2;
-    g_mark_list_copy = make_mark_list (g_mark_list_total_size);
-    if (!g_mark_list_copy)
-    {
-        goto cleanup;
-    }
-#else //MULTIPLE_HEAPS
-    mark_list_size = min(100*1024, max (8192, soh_segment_size/(64*32)));
-    g_mark_list_total_size = mark_list_size;
-    g_mark_list = make_mark_list (mark_list_size);
-#endif //MULTIPLE_HEAPS
-    dprintf (3, ("mark_list_size: %zd", mark_list_size));
-    if (!g_mark_list)
-    {
-        goto cleanup;
-    }
-#ifdef MULTIPLE_HEAPS
-    max_decommit_step_size = ((DECOMMIT_SIZE_PER_MILLISECOND * DECOMMIT_TIME_STEP_MILLISECONDS) / n_heaps);
-    max_decommit_step_size = max (max_decommit_step_size, MIN_DECOMMIT_SIZE);
-#endif //MULTIPLE_HEAPS
-#ifdef FEATURE_BASICFREEZE
-    seg_table = sorted_table::make_sorted_table();
-    if (!seg_table)
-        goto cleanup;
-#endif //FEATURE_BASICFREEZE
-#ifndef USE_REGIONS
-    segment_standby_list = 0;
-#endif //USE_REGIONS
-    if (!full_gc_approach_event.CreateManualEventNoThrow(FALSE))
-    {
-        goto cleanup;
-    }
-    if (!full_gc_end_event.CreateManualEventNoThrow(FALSE))
-    {
-        goto cleanup;
-    }
-    fgn_loh_percent = 0;
-    full_gc_approach_event_set = false;
-    memset (full_gc_counts, 0, sizeof (full_gc_counts));
-#ifndef USE_REGIONS
-    should_expand_in_full_gc = FALSE;
-#endif //!USE_REGIONS
-#ifdef FEATURE_LOH_COMPACTION
-    loh_compaction_always_p = GCConfig::GetLOHCompactionMode() != 0;
-    loh_compaction_mode = loh_compaction_default;
-#endif //FEATURE_LOH_COMPACTION
-    loh_size_threshold = (size_t)GCConfig::GetLOHThreshold();
-    assert (loh_size_threshold >= LARGE_OBJECT_SIZE);
-#ifdef BGC_SERVO_TUNING
-    memset (bgc_tuning::gen_calc, 0, sizeof (bgc_tuning::gen_calc));
-    memset (bgc_tuning::gen_stats, 0, sizeof (bgc_tuning::gen_stats));
-    memset (bgc_tuning::current_bgc_end_data, 0, sizeof (bgc_tuning::current_bgc_end_data));
-    bgc_tuning::enable_fl_tuning = (GCConfig::GetBGCFLTuningEnabled() != 0);
-    bgc_tuning::memory_load_goal = (uint32_t)GCConfig::GetBGCMemGoal();
-    bgc_tuning::memory_load_goal_slack = (uint32_t)GCConfig::GetBGCMemGoalSlack();
-    bgc_tuning::ml_kp = (double)GCConfig::GetBGCMLkp() / 1000.0;
-    bgc_tuning::ml_ki = (double)GCConfig::GetBGCMLki() / 1000.0;
-    bgc_tuning::ratio_correction_step = (double)GCConfig::GetBGCG2RatioStep() / 100.0;
-    bgc_tuning::above_goal_kp = (double)GCConfig::GetBGCFLkp() / 1000000.0;
-    bgc_tuning::enable_ki = (GCConfig::GetBGCFLEnableKi() != 0);
-    bgc_tuning::above_goal_ki = (double)GCConfig::GetBGCFLki() / 1000000.0;
-    bgc_tuning::enable_kd = (GCConfig::GetBGCFLEnableKd() != 0);
-    bgc_tuning::above_goal_kd = (double)GCConfig::GetBGCFLkd() / 100.0;
-    bgc_tuning::enable_smooth = (GCConfig::GetBGCFLEnableSmooth() != 0);
-    bgc_tuning::num_gen1s_smooth_factor = (double)GCConfig::GetBGCFLSmoothFactor() / 100.0;
-    bgc_tuning::enable_tbh = (GCConfig::GetBGCFLEnableTBH() != 0);
-    bgc_tuning::enable_ff = (GCConfig::GetBGCFLEnableFF() != 0);
-    bgc_tuning::above_goal_ff = (double)GCConfig::GetBGCFLff() / 100.0;
-    bgc_tuning::enable_gradual_d = (GCConfig::GetBGCFLGradualD() != 0);
-    sweep_flr_goal = (uint32_t)GCConfig::GetBGCFLSweepGoal();
-    sweep_flr_goal_loh = (uint32_t)GCConfig::GetBGCFLSweepGoalLOH();
-    bgc_tuning::gen_calc[0].sweep_flr_goal = ((sweep_flr_goal == 0) ? 20.0 : (double)sweep_flr_goal);
-    bgc_tuning::gen_calc[1].sweep_flr_goal = ((sweep_flr_goal_loh == 0) ? 20.0 : (double)sweep_flr_goal_loh);
-    bgc_tuning::available_memory_goal = (uint64_t)((double)gc_heap::total_physical_mem * (double)(100 - bgc_tuning::memory_load_goal) / 100);
-    get_memory_info (&current_memory_load);
-    dprintf (BGC_TUNING_LOG, ("BTL tuning %s!!!",
-        (bgc_tuning::enable_fl_tuning ? "enabled" : "disabled")));
-#ifdef SIMPLE_DPRINTF
-    dprintf (BGC_TUNING_LOG, ("BTL tuning parameters: mem goal: %d%%(%zd), +/-%d%%, gen2 correction factor: %.2f, sweep flr goal: %d%%, smooth factor: %.3f(%s), TBH: %s, FF: %.3f(%s), ml: kp %.5f, ki %.10f",
-        bgc_tuning::memory_load_goal,
-        bgc_tuning::available_memory_goal,
-        bgc_tuning::memory_load_goal_slack,
-        bgc_tuning::ratio_correction_step,
-        (int)bgc_tuning::gen_calc[0].sweep_flr_goal,
-        bgc_tuning::num_gen1s_smooth_factor,
-        (bgc_tuning::enable_smooth ? "enabled" : "disabled"),
-        (bgc_tuning::enable_tbh ? "enabled" : "disabled"),
-        bgc_tuning::above_goal_ff,
-        (bgc_tuning::enable_ff ? "enabled" : "disabled"),
-        bgc_tuning::ml_kp,
-        bgc_tuning::ml_ki));
-    dprintf (BGC_TUNING_LOG, ("BTL tuning parameters: kp: %.5f, ki: %.5f (%s), kd: %.3f (kd-%s, gd-%s), ff: %.3f",
-        bgc_tuning::above_goal_kp,
-        bgc_tuning::above_goal_ki,
-        (bgc_tuning::enable_ki ? "enabled" : "disabled"),
-        bgc_tuning::above_goal_kd,
-        (bgc_tuning::enable_kd ? "enabled" : "disabled"),
-        (bgc_tuning::enable_gradual_d ? "enabled" : "disabled"),
-        bgc_tuning::above_goal_ff));
-#endif //SIMPLE_DPRINTF
-    if (bgc_tuning::enable_fl_tuning && (current_memory_load < bgc_tuning::memory_load_goal))
-    {
-        uint32_t distance_to_goal = bgc_tuning::memory_load_goal - current_memory_load;
-        bgc_tuning::stepping_interval = max (distance_to_goal / 10, 1);
-        bgc_tuning::last_stepping_mem_load = current_memory_load;
-        bgc_tuning::last_stepping_bgc_count = 0;
-        dprintf (BGC_TUNING_LOG, ("current ml: %d, %d to goal, interval: %d",
-            current_memory_load, distance_to_goal, bgc_tuning::stepping_interval));
-    }
-    else
-    {
-        dprintf (BGC_TUNING_LOG, ("current ml: %d, >= goal: %d, disable stepping",
-            current_memory_load, bgc_tuning::memory_load_goal));
-        bgc_tuning::use_stepping_trigger_p = false;
-    }
-#endif //BGC_SERVO_TUNING
-#ifdef BACKGROUND_GC
-    memset (ephemeral_fgc_counts, 0, sizeof (ephemeral_fgc_counts));
-    bgc_alloc_spin_count = static_cast<uint32_t>(GCConfig::GetBGCSpinCount());
-    bgc_alloc_spin = static_cast<uint32_t>(GCConfig::GetBGCSpin());
-    {
-        int number_bgc_threads = get_num_heaps();
-        if (!create_bgc_threads_support (number_bgc_threads))
-        {
-            goto cleanup;
-        }
-    }
-#endif //BACKGROUND_GC
-    memset (&current_no_gc_region_info, 0, sizeof (current_no_gc_region_info));
-#ifdef GC_CONFIG_DRIVEN
-    compact_or_sweep_gcs[0] = 0;
-    compact_or_sweep_gcs[1] = 0;
-#endif //GC_CONFIG_DRIVEN
-#if defined(SHORT_PLUGS) && !defined(USE_REGIONS)
-    short_plugs_pad_ratio = (double)DESIRED_PLUG_LENGTH / (double)(DESIRED_PLUG_LENGTH - Align (min_obj_size));
-#endif //SHORT_PLUGS && !USE_REGIONS
-    generation_skip_ratio_threshold = (int)GCConfig::GetGCLowSkipRatio();
-#ifdef FEATURE_EVENT_TRACE
-    gc_time_info = new (nothrow) uint64_t[max_compact_time_type];
-    if (!gc_time_info)
-    {
-        goto cleanup;
-    }
-#ifdef BACKGROUND_GC
-    bgc_time_info = new (nothrow) uint64_t[max_bgc_time_type];
-    if (!bgc_time_info)
-    {
-        goto cleanup;
-    }
-#endif //BACKGROUND_GC
-#ifdef FEATURE_LOH_COMPACTION
-    loh_compact_info = new (nothrow) etw_loh_compact_info [get_num_heaps()];
-    if (!loh_compact_info)
-    {
-        goto cleanup;
-    }
-#endif //FEATURE_LOH_COMPACTION
-#endif //FEATURE_EVENT_TRACE
-    reset_mm_p = TRUE;
-    ret = 1;
-cleanup:
-    if (!ret)
-    {
-        if (full_gc_approach_event.IsValid())
-        {
-            full_gc_approach_event.CloseEvent();
-        }
-        if (full_gc_end_event.IsValid())
-        {
-            full_gc_end_event.CloseEvent();
-        }
-    }
-    return ret;
-}
-gc_heap* gc_heap::make_gc_heap (
-#ifdef MULTIPLE_HEAPS
-                                GCHeap* vm_hp,
-                                int heap_number
-#endif //MULTIPLE_HEAPS
-                                )
-{
-    gc_heap* res = 0;
-#ifdef MULTIPLE_HEAPS
-    res = new (nothrow) gc_heap;
-    if (!res)
-        return 0;
-    res->vm_heap = vm_hp;
-    res->alloc_context_count = 0;
-#ifndef USE_REGIONS
-    res->mark_list_piece_start = new (nothrow) uint8_t**[n_heaps];
-    if (!res->mark_list_piece_start)
-        return 0;
-#ifdef _PREFAST_
-#pragma warning(push)
-#pragma warning(disable:22011) // Suppress PREFast warning about integer underflow/overflow
-#endif // _PREFAST_
-    res->mark_list_piece_end = new (nothrow) uint8_t**[n_heaps + 32]; // +32 is padding to reduce false sharing
-#ifdef _PREFAST_
-#pragma warning(pop)
-#endif // _PREFAST_
-    if (!res->mark_list_piece_end)
-        return 0;
-#endif //!USE_REGIONS
-#endif //MULTIPLE_HEAPS
-    if (res->init_gc_heap (
-#ifdef MULTIPLE_HEAPS
-        heap_number
-#else  //MULTIPLE_HEAPS
-        0
-#endif //MULTIPLE_HEAPS
-        )==0)
-    {
-        return 0;
-    }
-#ifdef MULTIPLE_HEAPS
-    return res;
-#else
-    return (gc_heap*)1;
-#endif //MULTIPLE_HEAPS
-}
-uint32_t
-gc_heap::wait_for_gc_done(int32_t timeOut)
-{
-    bool cooperative_mode = enable_preemptive ();
-    uint32_t dwWaitResult = NOERROR;
-    gc_heap* wait_heap = NULL;
-    while (gc_heap::gc_started)
-    {
-#ifdef MULTIPLE_HEAPS
-        wait_heap = GCHeap::GetHeap(heap_select::select_heap(NULL))->pGenGCHeap;
-        dprintf(2, ("waiting for the gc_done_event on heap %d", wait_heap->heap_number));
-#endif // MULTIPLE_HEAPS
-#ifdef _PREFAST_
-        PREFIX_ASSUME(wait_heap != NULL);
-#endif // _PREFAST_
-        dwWaitResult = wait_heap->gc_done_event.Wait(timeOut, FALSE);
-    }
-    disable_preemptive (cooperative_mode);
-    return dwWaitResult;
-}
-void
-gc_heap::set_gc_done()
-{
-    enter_gc_done_event_lock();
-    if (!gc_done_event_set)
-    {
-        gc_done_event_set = true;
-        dprintf (2, ("heap %d: setting gc_done_event", heap_number));
-        gc_done_event.Set();
-    }
-    exit_gc_done_event_lock();
-}
-void
-gc_heap::reset_gc_done()
-{
-    enter_gc_done_event_lock();
-    if (gc_done_event_set)
-    {
-        gc_done_event_set = false;
-        dprintf (2, ("heap %d: resetting gc_done_event", heap_number));
-        gc_done_event.Reset();
-    }
-    exit_gc_done_event_lock();
-}
-void
-gc_heap::enter_gc_done_event_lock()
-{
-    uint32_t dwSwitchCount = 0;
-retry:
-    if (Interlocked::CompareExchange(&gc_done_event_lock, 0, -1) >= 0)
-    {
-        while (gc_done_event_lock >= 0)
-        {
-            if  (g_num_processors > 1)
-            {
-                int spin_count = yp_spin_count_unit;
-                for (int j = 0; j < spin_count; j++)
-                {
-                    if  (gc_done_event_lock < 0)
-                        break;
-                    YieldProcessor();           // indicate to the processor that we are spinning
-                }
-                if  (gc_done_event_lock >= 0)
-                    GCToOSInterface::YieldThread(++dwSwitchCount);
-            }
-            else
-                GCToOSInterface::YieldThread(++dwSwitchCount);
-        }
-        goto retry;
-    }
-}
-void
-gc_heap::exit_gc_done_event_lock()
-{
-    gc_done_event_lock = -1;
-}
-#ifndef MULTIPLE_HEAPS
-#ifdef RECORD_LOH_STATE
-int gc_heap::loh_state_index = 0;
-gc_heap::loh_state_info gc_heap::last_loh_states[max_saved_loh_states];
-#endif //RECORD_LOH_STATE
-VOLATILE(int32_t) gc_heap::gc_done_event_lock;
-VOLATILE(bool) gc_heap::gc_done_event_set;
-GCEvent gc_heap::gc_done_event;
-#endif //!MULTIPLE_HEAPS
-VOLATILE(bool) gc_heap::internal_gc_done;
-void gc_heap::add_saved_spinlock_info (
-            bool loh_p,
-            msl_enter_state enter_state,
-            msl_take_state take_state,
-            enter_msl_status msl_status)
-{
-#ifdef SPINLOCK_HISTORY
-    if (!loh_p || (msl_status == msl_retry_different_heap))
-    {
-        return;
-    }
-    spinlock_info* current = &last_spinlock_info[spinlock_info_index];
-    current->enter_state = enter_state;
-    current->take_state = take_state;
-    current->current_uoh_alloc_state = current_uoh_alloc_state;
-    current->thread_id.SetToCurrentThread();
-    current->loh_p = loh_p;
-    dprintf (SPINLOCK_LOG, ("[%d]%s %s %s",
-        heap_number,
-        (loh_p ? "loh" : "soh"),
-        ((enter_state == me_acquire) ? "E" : "L"),
-        msl_take_state_str[take_state]));
-    spinlock_info_index++;
-    assert (spinlock_info_index <= max_saved_spinlock_info);
-    if (spinlock_info_index >= max_saved_spinlock_info)
-    {
-        spinlock_info_index = 0;
-    }
-#else
-    UNREFERENCED_PARAMETER(enter_state);
-    UNREFERENCED_PARAMETER(take_state);
-#endif //SPINLOCK_HISTORY
-}
-int
-gc_heap::init_gc_heap (int h_number)
-{
-#ifdef MULTIPLE_HEAPS
-#ifdef _DEBUG
-    memset (committed_by_oh_per_heap, 0, sizeof (committed_by_oh_per_heap));
-#endif
-    g_heaps [h_number] = this;
-    time_bgc_last = 0;
-#ifdef SPINLOCK_HISTORY
-    spinlock_info_index = 0;
-    memset (last_spinlock_info, 0, sizeof(last_spinlock_info));
-#endif //SPINLOCK_HISTORY
-#ifndef USE_REGIONS
-    ephemeral_low = (uint8_t*)1;
-    ephemeral_high = MAX_PTR;
-#endif //!USE_REGIONS
-    gc_low = 0;
-    gc_high = 0;
-    ephemeral_heap_segment = 0;
-    oomhist_index_per_heap = 0;
-    freeable_uoh_segment = 0;
-    condemned_generation_num = 0;
-    blocking_collection = FALSE;
-    generation_skip_ratio = 100;
-#ifdef FEATURE_CARD_MARKING_STEALING
-    n_eph_soh = 0;
-    n_gen_soh = 0;
-    n_eph_loh = 0;
-    n_gen_loh = 0;
-#endif //FEATURE_CARD_MARKING_STEALING
-    mark_stack_tos = 0;
-    mark_stack_bos = 0;
-    mark_stack_array_length = 0;
-    mark_stack_array = 0;
-#if defined (_DEBUG) && defined (VERIFY_HEAP)
-    verify_pinned_queue_p = FALSE;
-#endif // _DEBUG && VERIFY_HEAP
-#ifdef FEATURE_LOH_COMPACTION
-    loh_pinned_queue_tos = 0;
-    loh_pinned_queue_bos = 0;
-    loh_pinned_queue_length = 0;
-    loh_pinned_queue_decay = LOH_PIN_DECAY;
-    loh_pinned_queue = 0;
-#endif //FEATURE_LOH_COMPACTION
-    min_overflow_address = MAX_PTR;
-    max_overflow_address = 0;
-    gen0_bricks_cleared = FALSE;
-    gen0_must_clear_bricks = 0;
-    allocation_quantum = CLR_SIZE;
-    more_space_lock_soh = gc_lock;
-    more_space_lock_uoh = gc_lock;
-    ro_segments_in_range = FALSE;
-    loh_alloc_since_cg = 0;
-#ifndef USE_REGIONS
-    new_heap_segment = NULL;
-#endif //!USE_REGIONS
-    gen0_allocated_after_gc_p = false;
-#ifdef RECORD_LOH_STATE
-    loh_state_index = 0;
-#endif //RECORD_LOH_STATE
-#ifdef USE_REGIONS
-    new_gen0_regions_in_plns = 0;
-    new_regions_in_prr = 0;
-    new_regions_in_threading = 0;
-    special_sweep_p = false;
-#endif //USE_REGIONS
-#endif //MULTIPLE_HEAPS
-#ifdef MULTIPLE_HEAPS
-    if (h_number > n_heaps)
-    {
-        assert (!"Number of heaps exceeded");
-        return 0;
-    }
-    heap_number = h_number;
-#endif //MULTIPLE_HEAPS
-    memset (etw_allocation_running_amount, 0, sizeof (etw_allocation_running_amount));
-    memset (allocated_since_last_gc, 0, sizeof (allocated_since_last_gc));
-    memset (&oom_info, 0, sizeof (oom_info));
-    memset (&fgm_result, 0, sizeof (fgm_result));
-    memset (oomhist_per_heap, 0, sizeof (oomhist_per_heap));
-    if (!gc_done_event.CreateManualEventNoThrow(FALSE))
-    {
-        return 0;
-    }
-    gc_done_event_lock = -1;
-    gc_done_event_set = false;
-#ifdef DYNAMIC_HEAP_COUNT
-    if (h_number != 0)
-    {
-        if (!gc_idle_thread_event.CreateAutoEventNoThrow (FALSE))
-        {
-            return 0;
-        }
-#ifdef BACKGROUND_GC
-        if (!bgc_idle_thread_event.CreateAutoEventNoThrow (FALSE))
-        {
-            return 0;
-        }
-#endif //BACKGROUND_GC
-        dprintf (9999, ("creating idle events for h%d", h_number));
-    }
-#endif //DYNAMIC_HEAP_COUNT
-    if (!init_dynamic_data())
-    {
-        return 0;
-    }
-    uint32_t* ct = &g_gc_card_table [card_word (card_of (g_gc_lowest_address))];
-    own_card_table (ct);
-    card_table = translate_card_table (ct);
-    brick_table = card_table_brick_table (ct);
-    highest_address = card_table_highest_address (ct);
-    lowest_address = card_table_lowest_address (ct);
-#ifdef CARD_BUNDLE
-    card_bundle_table = translate_card_bundle_table (card_table_card_bundle_table (ct), g_gc_lowest_address);
-    assert (&card_bundle_table [card_bundle_word (cardw_card_bundle (card_word (card_of (g_gc_lowest_address))))] ==
-            card_table_card_bundle_table (ct));
-#endif //CARD_BUNDLE
-#ifdef BACKGROUND_GC
-    background_saved_highest_address = nullptr;
-    background_saved_lowest_address = nullptr;
-    if (gc_can_use_concurrent)
-        mark_array = translate_mark_array (card_table_mark_array (&g_gc_card_table[card_word (card_of (g_gc_lowest_address))]));
-    else
-        mark_array = NULL;
-#endif //BACKGROUND_GC
-#ifdef USE_REGIONS
-#ifdef STRESS_REGIONS
-    disable_preemptive (true);
-    pinning_handles_for_alloc = new (nothrow) (OBJECTHANDLE[PINNING_HANDLE_INITIAL_LENGTH]);
-    for (int i = 0; i < PINNING_HANDLE_INITIAL_LENGTH; i++)
-    {
-        pinning_handles_for_alloc[i] = g_gcGlobalHandleStore->CreateHandleOfType (0, HNDTYPE_PINNED);
-    }
-    enable_preemptive();
-    ph_index_per_heap = 0;
-    pinning_seg_interval = 2;
-    num_gen0_regions = 0;
-    sip_seg_interval = 2;
-    sip_seg_maxgen_interval = 3;
-    num_condemned_regions = 0;
-#endif //STRESS_REGIONS
-    end_gen0_region_space = 0;
-    end_gen0_region_committed_space = 0;
-    gen0_pinned_free_space = 0;
-    gen0_large_chunk_found = false;
-    if (!initial_make_soh_regions (__this) ||
-        !initial_make_uoh_regions (loh_generation, __this) ||
-        !initial_make_uoh_regions (poh_generation, __this))
-    {
-        return 0;
-    }
-#else //USE_REGIONS
-    heap_segment* seg = make_initial_segment (soh_gen0, h_number, __this);
-    if (!seg)
-        return 0;
-    FIRE_EVENT(GCCreateSegment_V1, heap_segment_mem(seg),
-                              (size_t)(heap_segment_reserved (seg) - heap_segment_mem(seg)),
-                              gc_etw_segment_small_object_heap);
-    seg_mapping_table_add_segment (seg, __this);
-#ifdef MULTIPLE_HEAPS
-    assert (heap_segment_heap (seg) == __this);
-#endif //MULTIPLE_HEAPS
-    uint8_t*  start = heap_segment_mem (seg);
-    for (int i = max_generation; i >= 0; i--)
-    {
-        make_generation (i, seg, start);
-        start += Align (min_obj_size);
-    }
-    heap_segment_allocated (seg) = start;
-    alloc_allocated = start;
-    heap_segment_used (seg) = start - plug_skew;
-    ephemeral_heap_segment = seg;
-    heap_segment* lseg = make_initial_segment(loh_generation, h_number, __this);
-    if (!lseg)
-        return 0;
-    lseg->flags |= heap_segment_flags_loh;
-    FIRE_EVENT(GCCreateSegment_V1, heap_segment_mem(lseg),
-                              (size_t)(heap_segment_reserved (lseg) - heap_segment_mem(lseg)),
-                              gc_etw_segment_large_object_heap);
-    heap_segment* pseg = make_initial_segment (poh_generation, h_number, __this);
-    if (!pseg)
-        return 0;
-    pseg->flags |= heap_segment_flags_poh;
-    FIRE_EVENT(GCCreateSegment_V1, heap_segment_mem(pseg),
-                              (size_t)(heap_segment_reserved (pseg) - heap_segment_mem(pseg)),
-                              gc_etw_segment_pinned_object_heap);
-    seg_mapping_table_add_segment (lseg, __this);
-    seg_mapping_table_add_segment (pseg, __this);
-    make_generation (loh_generation, lseg, heap_segment_mem (lseg));
-    make_generation (poh_generation, pseg, heap_segment_mem (pseg));
-    heap_segment_allocated (lseg) = heap_segment_mem (lseg) + Align (min_obj_size, get_alignment_constant (FALSE));
-    heap_segment_used (lseg) = heap_segment_allocated (lseg) - plug_skew;
-    heap_segment_allocated (pseg) = heap_segment_mem (pseg) + Align (min_obj_size, get_alignment_constant (FALSE));
-    heap_segment_used (pseg) = heap_segment_allocated (pseg) - plug_skew;
-    for (int gen_num = 0; gen_num < total_generation_count; gen_num++)
-    {
-        generation*  gen = generation_of (gen_num);
-        make_unused_array (generation_allocation_start (gen), Align (min_obj_size));
-    }
-#ifdef MULTIPLE_HEAPS
-    assert (heap_segment_heap (lseg) == __this);
-    assert (heap_segment_heap (pseg) == __this);
-#endif //MULTIPLE_HEAPS
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-    generation_alloc_context (generation_of (soh_gen0))->set_alloc_heap(vm_heap);
-    generation_alloc_context (generation_of (loh_generation))->set_alloc_heap(vm_heap);
-    generation_alloc_context (generation_of (poh_generation))->set_alloc_heap(vm_heap);
-#endif //MULTIPLE_HEAPS
-    generation_of (max_generation)->free_list_allocator = allocator(NUM_GEN2_ALIST, BASE_GEN2_ALIST_BITS, gen2_alloc_list, max_generation);
-    generation_of (loh_generation)->free_list_allocator = allocator(NUM_LOH_ALIST, BASE_LOH_ALIST_BITS, loh_alloc_list);
-    generation_of (poh_generation)->free_list_allocator = allocator(NUM_POH_ALIST, BASE_POH_ALIST_BITS, poh_alloc_list);
-    total_alloc_bytes_soh = 0;
-    total_alloc_bytes_uoh = 0;
-#ifdef MULTIPLE_HEAPS
-#ifdef STRESS_DYNAMIC_HEAP_COUNT
-    uoh_msl_before_gc_p = false;
-#endif //STRESS_DYNAMIC_HEAP_COUNT
-#else //MULTIPLE_HEAPS
-    allocation_running_amount = dd_min_size (dynamic_data_of (0));
-#endif //!MULTIPLE_HEAPS
-    fgn_maxgen_percent = 0;
-    fgn_last_alloc = dd_min_size (dynamic_data_of (0));
-    mark* arr = new (nothrow) (mark [MARK_STACK_INITIAL_LENGTH]);
-    if (!arr)
-        return 0;
-    make_mark_stack(arr);
-#ifdef BACKGROUND_GC
-#ifdef BGC_SERVO_TUNING
-    loh_a_no_bgc = 0;
-    loh_a_bgc_marking = 0;
-    loh_a_bgc_planning = 0;
-    bgc_maxgen_end_fl_size = 0;
-#endif //BGC_SERVO_TUNING
-    freeable_soh_segment = 0;
-    gchist_index_per_heap = 0;
-    if (gc_can_use_concurrent)
-    {
-        uint8_t** b_arr = new (nothrow) (uint8_t * [MARK_STACK_INITIAL_LENGTH]);
-        if (!b_arr)
-            return 0;
-        make_background_mark_stack(b_arr);
-    }
-#endif //BACKGROUND_GC
-#ifndef USE_REGIONS
-    ephemeral_low = generation_allocation_start(generation_of(max_generation - 1));
-    ephemeral_high = heap_segment_reserved(ephemeral_heap_segment);
-#endif //!USE_REGIONS
-    if (heap_number == 0)
-    {
-        stomp_write_barrier_initialize(
-#if defined(USE_REGIONS)
-            ephemeral_low, ephemeral_high,
-            map_region_to_generation_skewed, (uint8_t)min_segment_size_shr
-#elif defined(MULTIPLE_HEAPS)
-            reinterpret_cast<uint8_t*>(1), reinterpret_cast<uint8_t*>(~0)
-#else
-            ephemeral_low, ephemeral_high
-#endif //MULTIPLE_HEAPS || USE_REGIONS
-        );
-    }
-#ifdef MULTIPLE_HEAPS
-    if (!create_gc_thread ())
-        return 0;
-#endif //MULTIPLE_HEAPS
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-    HRESULT hr = AllocateCFinalize(&finalize_queue);
-    if (FAILED(hr))
-        return 0;
-#endif // FEATURE_PREMORTEM_FINALIZATION
-#ifdef USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-    min_fl_list = 0;
-    num_fl_items_rethreaded_stage2 = 0;
-    free_list_space_per_heap = nullptr;
-#endif //MULTIPLE_HEAPS
-#else //USE_REGIONS
-    max_free_space_items = MAX_NUM_FREE_SPACES;
-    bestfit_seg = new (nothrow) seg_free_spaces (heap_number);
-    if (!bestfit_seg)
-    {
-        return 0;
-    }
-    if (!bestfit_seg->alloc())
-    {
-        return 0;
-    }
-#endif //USE_REGIONS
-    last_gc_before_oom = FALSE;
-    sufficient_gen0_space_p = FALSE;
-#ifdef MULTIPLE_HEAPS
-#ifdef HEAP_ANALYZE
-    heap_analyze_success = TRUE;
-    internal_root_array  = 0;
-    internal_root_array_index = 0;
-    internal_root_array_length = initial_internal_roots;
-    current_obj          = 0;
-    current_obj_size     = 0;
-#endif //HEAP_ANALYZE
-#endif // MULTIPLE_HEAPS
-#ifdef BACKGROUND_GC
-    bgc_thread_id.Clear();
-    if (!create_bgc_thread_support())
-    {
-        return 0;
-    }
-    bgc_alloc_lock = new (nothrow) exclusive_sync;
-    if (!bgc_alloc_lock)
-    {
-        return 0;
-    }
-    bgc_alloc_lock->init();
-    bgc_thread_running = 0;
-    bgc_thread = 0;
-    bgc_threads_timeout_cs.Initialize();
-    current_bgc_state = bgc_not_in_process;
-    background_soh_alloc_count = 0;
-    background_uoh_alloc_count = 0;
-    bgc_overflow_count = 0;
-    end_loh_size = dd_min_size (dynamic_data_of (loh_generation));
-    end_poh_size = dd_min_size (dynamic_data_of (poh_generation));
-    current_sweep_pos = 0;
-#ifdef DOUBLY_LINKED_FL
-    current_sweep_seg = 0;
-#endif //DOUBLY_LINKED_FL
-#endif //BACKGROUND_GC
-#ifdef GC_CONFIG_DRIVEN
-    memset(interesting_data_per_heap, 0, sizeof (interesting_data_per_heap));
-    memset(compact_reasons_per_heap, 0, sizeof (compact_reasons_per_heap));
-    memset(expand_mechanisms_per_heap, 0, sizeof (expand_mechanisms_per_heap));
-    memset(interesting_mechanism_bits_per_heap, 0, sizeof (interesting_mechanism_bits_per_heap));
-#endif //GC_CONFIG_DRIVEN
-    return 1;
-}
-void
-gc_heap::destroy_semi_shared()
-{
-    if (g_mark_list)
-        delete g_mark_list;
-    if (seg_mapping_table)
-        delete seg_mapping_table;
-#ifdef FEATURE_BASICFREEZE
-    seg_table->delete_sorted_table();
-#endif //FEATURE_BASICFREEZE
-}
-void
-gc_heap::self_destroy()
-{
-#ifdef BACKGROUND_GC
-    kill_gc_thread();
-#endif //BACKGROUND_GC
-    if (gc_done_event.IsValid())
-    {
-        gc_done_event.CloseEvent();
-    }
-    for (int i = get_start_generation_index(); i < total_generation_count; i++)
-    {
-        heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (i)));
-        PREFIX_ASSUME(seg != NULL);
-        while (seg)
-        {
-            heap_segment* next_seg = heap_segment_next_rw (seg);
-            delete_heap_segment (seg);
-            seg = next_seg;
-        }
-    }
-    release_card_table (card_table);
-    delete[] mark_stack_array;
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-    if (finalize_queue)
-        delete finalize_queue;
-#endif // FEATURE_PREMORTEM_FINALIZATION
-}
-void
-gc_heap::destroy_gc_heap(gc_heap* heap)
-{
-    heap->self_destroy();
-    delete heap;
-}
-void gc_heap::shutdown_gc()
-{
-    destroy_semi_shared();
-#ifdef MULTIPLE_HEAPS
-    delete g_heaps;
-    destroy_thread_support();
-    n_heaps = 0;
-#endif //MULTIPLE_HEAPS
-    destroy_initial_memory();
-    GCToOSInterface::Shutdown();
-}
-inline
-BOOL gc_heap::size_fit_p (size_t size REQD_ALIGN_AND_OFFSET_DCL, uint8_t* alloc_pointer, uint8_t* alloc_limit,
-                          uint8_t* old_loc, int use_padding)
-{
-    BOOL already_padded = FALSE;
-#ifdef SHORT_PLUGS
-    if ((old_loc != 0) && (use_padding & USE_PADDING_FRONT))
-    {
-        alloc_pointer = alloc_pointer + Align (min_obj_size);
-        already_padded = TRUE;
-    }
-#endif //SHORT_PLUGS
-    if (!((old_loc == 0) || same_large_alignment_p (old_loc, alloc_pointer)))
-        size = size + switch_alignment_size (already_padded);
-#ifdef FEATURE_STRUCTALIGN
-    alloc_pointer = StructAlign(alloc_pointer, requiredAlignment, alignmentOffset);
-#endif // FEATURE_STRUCTALIGN
-    if (alloc_limit < alloc_pointer)
-    {
-        return FALSE;
-    }
-    if (old_loc != 0)
-    {
-        return (((size_t)(alloc_limit - alloc_pointer) >= (size + ((use_padding & USE_PADDING_TAIL)? Align(min_obj_size) : 0)))
-#ifdef SHORT_PLUGS
-                ||((!(use_padding & USE_PADDING_FRONT)) && ((alloc_pointer + size) == alloc_limit))
-#else //SHORT_PLUGS
-                ||((alloc_pointer + size) == alloc_limit)
-#endif //SHORT_PLUGS
-            );
-    }
-    else
-    {
-        assert (size == Align (min_obj_size));
-        return ((size_t)(alloc_limit - alloc_pointer) >= size);
-    }
-}
-inline
-BOOL gc_heap::a_size_fit_p (size_t size, uint8_t* alloc_pointer, uint8_t* alloc_limit,
-                            int align_const)
-{
-    if (alloc_limit < alloc_pointer)
-    {
-        return FALSE;
-    }
-    return ((size_t)(alloc_limit - alloc_pointer) >= (size + Align(min_obj_size, align_const)));
-}
-BOOL gc_heap::grow_heap_segment (heap_segment* seg, uint8_t* high_address, bool* hard_limit_exceeded_p)
-{
-    assert (high_address <= heap_segment_reserved (seg));
-    if (hard_limit_exceeded_p)
-        *hard_limit_exceeded_p = false;
-    if (align_on_page (high_address) > heap_segment_reserved (seg))
-        return FALSE;
-    if (high_address <= heap_segment_committed (seg))
-        return TRUE;
-    size_t c_size = align_on_page ((size_t)(high_address - heap_segment_committed (seg)));
-    c_size = max (c_size, commit_min_th);
-    c_size = min (c_size, (size_t)(heap_segment_reserved (seg) - heap_segment_committed (seg)));
-    if (c_size == 0)
-        return FALSE;
-    STRESS_LOG2(LF_GC, LL_INFO10000,
-                "Growing heap_segment: %zx high address: %zx\n",
-                (size_t)seg, (size_t)high_address);
-    bool ret = virtual_commit (heap_segment_committed (seg), c_size, heap_segment_oh (seg), heap_number, hard_limit_exceeded_p);
-    if (ret)
-    {
-        heap_segment_committed (seg) += c_size;
-        STRESS_LOG1(LF_GC, LL_INFO10000, "New commit: %zx\n",
-                    (size_t)heap_segment_committed (seg));
-        assert (heap_segment_committed (seg) <= heap_segment_reserved (seg));
-        assert (high_address <= heap_segment_committed (seg));
-#if defined(MULTIPLE_HEAPS) && !defined(USE_REGIONS)
-        assert (!gradual_decommit_in_progress_p ||
-                (seg != ephemeral_heap_segment) ||
-                (heap_segment_committed (seg) <= heap_segment_decommit_target (seg)));
-#endif //MULTIPLE_HEAPS && !USE_REGIONS
-    }
-    return !!ret;
-}
-inline
-int gc_heap::grow_heap_segment (heap_segment* seg, uint8_t* allocated, uint8_t* old_loc, size_t size,
-                                BOOL pad_front_p  REQD_ALIGN_AND_OFFSET_DCL)
-{
-    BOOL already_padded = FALSE;
-#ifdef SHORT_PLUGS
-    if ((old_loc != 0) && pad_front_p)
-    {
-        allocated = allocated + Align (min_obj_size);
-        already_padded = TRUE;
-    }
-#endif //SHORT_PLUGS
-    if (!((old_loc == 0) || same_large_alignment_p (old_loc, allocated)))
-        size += switch_alignment_size (already_padded);
-#ifdef FEATURE_STRUCTALIGN
-    size_t pad = ComputeStructAlignPad(allocated, requiredAlignment, alignmentOffset);
-    return grow_heap_segment (seg, allocated + pad + size);
-#else // FEATURE_STRUCTALIGN
-    return grow_heap_segment (seg, allocated + size);
-#endif // FEATURE_STRUCTALIGN
-}
-void gc_heap::thread_free_item_front (generation* gen, uint8_t* free_start, size_t free_size)
-{
-    make_unused_array (free_start, free_size);
-    generation_free_list_space (gen) += free_size;
-    generation_allocator(gen)->thread_item_front (free_start, free_size);
-    add_gen_free (gen->gen_num, free_size);
-    if (gen->gen_num == max_generation)
-    {
-        dprintf (2, ("AO h%d: gen2F+: %p(%zd)->%zd, FO: %zd",
-            heap_number, free_start, free_size,
-            generation_free_list_space (gen), generation_free_obj_space (gen)));
-    }
-}
-#ifdef DOUBLY_LINKED_FL
-void gc_heap::thread_item_front_added (generation* gen, uint8_t* free_start, size_t free_size)
-{
-    make_unused_array (free_start, free_size);
-    generation_free_list_space (gen) += free_size;
-    int bucket_index = generation_allocator(gen)->thread_item_front_added (free_start, free_size);
-    if (gen->gen_num == max_generation)
-    {
-        dprintf (2, ("AO [h%d] gen2FL+: %p(%zd)->%zd",
-            heap_number, free_start, free_size, generation_free_list_space (gen)));
-    }
-    add_gen_free (gen->gen_num, free_size);
-}
-#endif //DOUBLY_LINKED_FL
-void gc_heap::make_free_obj (generation* gen, uint8_t* free_start, size_t free_size)
-{
-    make_unused_array (free_start, free_size);
-    generation_free_obj_space (gen) += free_size;
-    if (gen->gen_num == max_generation)
-    {
-        dprintf (2, ("AO [h%d] gen2FO+: %p(%zd)->%zd",
-            heap_number, free_start, free_size, generation_free_obj_space (gen)));
-    }
-}
-void gc_heap::adjust_limit (uint8_t* start, size_t limit_size, generation* gen)
-{
-    dprintf (3, ("gc Expanding segment allocation"));
-    heap_segment* seg = generation_allocation_segment (gen);
-    if ((generation_allocation_limit (gen) != start) || (start != heap_segment_plan_allocated (seg)))
-    {
-        if (generation_allocation_limit (gen) == heap_segment_plan_allocated (seg))
-        {
-            assert (generation_allocation_pointer (gen) >= heap_segment_mem (seg));
-            assert (generation_allocation_pointer (gen) <= heap_segment_committed (seg));
-            heap_segment_plan_allocated (generation_allocation_segment (gen)) = generation_allocation_pointer (gen);
-        }
-        else
-        {
-            uint8_t*  hole = generation_allocation_pointer (gen);
-            size_t  size = (generation_allocation_limit (gen) - generation_allocation_pointer (gen));
-            if (size != 0)
-            {
-                dprintf (3, ("filling up hole: %p, size %zx", hole, size));
-                size_t allocated_size = generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen);
-#ifdef DOUBLY_LINKED_FL
-                if (gen->gen_num == max_generation)
-                {
-                    if (allocated_size <= min_free_item_no_prev)
-                    {
-                        size_t* filler_free_obj_size_location = (size_t*)(generation_allocation_context_start_region (gen) +
-                                                                          min_free_item_no_prev);
-                        size_t filler_free_obj_size = 0;
-                        if (size >= (Align (min_free_list) + Align (min_obj_size)))
-                        {
-                            filler_free_obj_size = Align (min_obj_size);
-                            size_t fl_size = size - filler_free_obj_size;
-                            thread_item_front_added (gen, (hole + filler_free_obj_size), fl_size);
-                        }
-                        else
-                        {
-                            filler_free_obj_size = size;
-                        }
-                        generation_free_obj_space (gen) += filler_free_obj_size;
-                        *filler_free_obj_size_location = filler_free_obj_size;
-                        uint8_t* old_loc = generation_last_free_list_allocated (gen);
-                        uint8_t* saved_plug_and_gap = nullptr;
-                        if (saved_pinned_plug_index != INVALID_SAVED_PINNED_PLUG_INDEX)
-                        {
-                            saved_plug_and_gap = pinned_plug (pinned_plug_of (saved_pinned_plug_index)) - sizeof(plug_and_gap);
-                            dprintf (3333, ("[h%d] sppi: %zd mtos: %zd old_loc: %p pp: %p(%zd) offs: %zd",
-                                heap_number,
-                                saved_pinned_plug_index,
-                                mark_stack_tos,
-                                old_loc,
-                                pinned_plug (pinned_plug_of (saved_pinned_plug_index)),
-                                pinned_len (pinned_plug_of (saved_pinned_plug_index)),
-                                old_loc - saved_plug_and_gap));
-                        }
-                        size_t offset = old_loc - saved_plug_and_gap;
-                        if (offset < sizeof(gap_reloc_pair))
-                        {
-                            assert (offset <= sizeof(plug_and_gap) - min_obj_size);
-                            set_free_obj_in_compact_bit ((uint8_t*)(&pinned_plug_of (saved_pinned_plug_index)->saved_pre_plug_reloc) + offset);
-                        }
-                        else
-                        {
-#ifdef _DEBUG
-                            header(old_loc)->Validate();
-#endif //_DEBUG
-                            set_free_obj_in_compact_bit (old_loc);
-                        }
-                        dprintf (3333, ("[h%d] ac: %p->%p((%zd < %zd), Pset %p s->%zd", heap_number,
-                            generation_allocation_context_start_region (gen), generation_allocation_pointer (gen),
-                            allocated_size, min_free_item_no_prev, filler_free_obj_size_location, filler_free_obj_size));
-                    }
-                    else
-                    {
-                        if (size >= Align (min_free_list))
-                        {
-                            thread_item_front_added (gen, hole, size);
-                        }
-                        else
-                        {
-                            make_free_obj (gen, hole, size);
-                        }
-                    }
-                }
-                else
-#endif //DOUBLY_LINKED_FL
-                {
-                    if (size >= Align (min_free_list))
-                    {
-                        if (allocated_size < min_free_item_no_prev)
-                        {
-                            if (size >= (Align (min_free_list) + Align (min_obj_size)))
-                            {
-                                make_free_obj (gen, hole, min_obj_size);
-                                thread_free_item_front (gen, (hole + Align (min_obj_size)),
-                                    (size - Align (min_obj_size)));
-                            }
-                            else
-                            {
-                                dprintf (3, ("allocated size too small, can't put back rest on free list %zx",
-                                    allocated_size));
-                                make_free_obj (gen, hole, size);
-                            }
-                        }
-                        else
-                        {
-                            dprintf (3, ("threading hole in front of free list"));
-                            thread_free_item_front (gen, hole, size);
-                        }
-                    }
-                    else
-                    {
-                        make_free_obj (gen, hole, size);
-                    }
-                }
-            }
-        }
-        generation_allocation_pointer (gen) = start;
-        generation_allocation_context_start_region (gen) = start;
-    }
-    generation_allocation_limit (gen) = (start + limit_size);
-}
-void verify_mem_cleared (uint8_t* start, size_t size)
-{
-    if (!Aligned (size))
-    {
-        FATAL_GC_ERROR();
-    }
-    PTR_PTR curr_ptr = (PTR_PTR) start;
-    for (size_t i = 0; i < size / sizeof(PTR_PTR); i++)
-    {
-        if (*(curr_ptr++) != 0)
-        {
-            FATAL_GC_ERROR();
-        }
-    }
-}
-#if defined (VERIFY_HEAP) && defined (BACKGROUND_GC)
-void gc_heap::set_batch_mark_array_bits (uint8_t* start, uint8_t* end)
-{
-    size_t start_mark_bit = mark_bit_of (start);
-    size_t end_mark_bit = mark_bit_of (end);
-    unsigned int startbit = mark_bit_bit (start_mark_bit);
-    unsigned int endbit = mark_bit_bit (end_mark_bit);
-    size_t startwrd = mark_bit_word (start_mark_bit);
-    size_t endwrd = mark_bit_word (end_mark_bit);
-    dprintf (3, ("Setting all mark array bits between [%zx:%zx-[%zx:%zx",
-        (size_t)start, (size_t)start_mark_bit,
-        (size_t)end, (size_t)end_mark_bit));
-    unsigned int firstwrd = ~(lowbits (~0, startbit));
-    unsigned int lastwrd = ~(highbits (~0, endbit));
-    if (startwrd == endwrd)
-    {
-        unsigned int wrd = firstwrd & lastwrd;
-        mark_array[startwrd] |= wrd;
-        return;
-    }
-    if (startbit)
-    {
-        mark_array[startwrd] |= firstwrd;
-        startwrd++;
-    }
-    for (size_t wrdtmp = startwrd; wrdtmp < endwrd; wrdtmp++)
-    {
-        mark_array[wrdtmp] = ~(unsigned int)0;
-    }
-    if (endbit)
-    {
-        mark_array[endwrd] |= lastwrd;
-    }
-}
-void gc_heap::check_batch_mark_array_bits (uint8_t* start, uint8_t* end)
-{
-    size_t start_mark_bit = mark_bit_of (start);
-    size_t end_mark_bit = mark_bit_of (end);
-    unsigned int startbit = mark_bit_bit (start_mark_bit);
-    unsigned int endbit = mark_bit_bit (end_mark_bit);
-    size_t startwrd = mark_bit_word (start_mark_bit);
-    size_t endwrd = mark_bit_word (end_mark_bit);
-    unsigned int firstwrd = ~(lowbits (~0, startbit));
-    unsigned int lastwrd = ~(highbits (~0, endbit));
-    if (startwrd == endwrd)
-    {
-        unsigned int wrd = firstwrd & lastwrd;
-        if (mark_array[startwrd] & wrd)
-        {
-            dprintf  (1, ("The %x portion of mark bits at 0x%zx:0x%x(addr: 0x%p) were not cleared",
-                            wrd, startwrd,
-                            mark_array [startwrd], mark_word_address (startwrd)));
-            FATAL_GC_ERROR();
-        }
-        return;
-    }
-    if (startbit)
-    {
-        if (mark_array[startwrd] & firstwrd)
-        {
-            dprintf  (1, ("The %x portion of mark bits at 0x%zx:0x%x(addr: 0x%p) were not cleared",
-                            firstwrd, startwrd,
-                            mark_array [startwrd], mark_word_address (startwrd)));
-            FATAL_GC_ERROR();
-        }
-        startwrd++;
-    }
-    for (size_t wrdtmp = startwrd; wrdtmp < endwrd; wrdtmp++)
-    {
-        if (mark_array[wrdtmp])
-        {
-            dprintf  (1, ("The mark bits at 0x%zx:0x%x(addr: 0x%p) were not cleared",
-                            wrdtmp,
-                            mark_array [wrdtmp], mark_word_address (wrdtmp)));
-            FATAL_GC_ERROR();
-        }
-    }
-    if (endbit)
-    {
-        if (mark_array[endwrd] & lastwrd)
-        {
-            dprintf  (1, ("The %x portion of mark bits at 0x%x:0x%x(addr: 0x%p) were not cleared",
-                            lastwrd, lastwrd,
-                            mark_array [lastwrd], mark_word_address (lastwrd)));
-            FATAL_GC_ERROR();
-        }
-    }
-}
-#endif //VERIFY_HEAP && BACKGROUND_GC
-allocator::allocator (unsigned int num_b, int fbb, alloc_list* b, int gen)
-{
-    assert (num_b < MAX_BUCKET_COUNT);
-    num_buckets = num_b;
-    first_bucket_bits = fbb;
-    buckets = b;
-    gen_number = gen;
-}
-alloc_list& allocator::alloc_list_of (unsigned int bn)
-{
-    assert (bn < num_buckets);
-    if (bn == 0)
-        return first_bucket;
-    else
-        return buckets [bn-1];
-}
-size_t& allocator::alloc_list_damage_count_of (unsigned int bn)
-{
-    assert (bn < num_buckets);
-    if (bn == 0)
-        return first_bucket.alloc_list_damage_count();
-    else
-        return buckets [bn-1].alloc_list_damage_count();
-}
-void allocator::unlink_item (unsigned int bn, uint8_t* item, uint8_t* prev_item, BOOL use_undo_p)
-{
-    alloc_list* al = &alloc_list_of (bn);
-    uint8_t* next_item = free_list_slot(item);
-#ifdef DOUBLY_LINKED_FL
-    BOOL repair_list = !discard_if_no_fit_p ();
-#endif //DOUBLY_LINKED_FL
-    if (prev_item)
-    {
-        if (use_undo_p && (free_list_undo (prev_item) == UNDO_EMPTY))
-        {
-            assert (item == free_list_slot (prev_item));
-            free_list_undo (prev_item) = item;
-            alloc_list_damage_count_of (bn)++;
-        }
-        free_list_slot (prev_item) = next_item;
-    }
-    else
-    {
-        al->alloc_list_head() = next_item;
-    }
-    if (al->alloc_list_tail() == item)
-    {
-        al->alloc_list_tail() = prev_item;
-    }
-#ifdef DOUBLY_LINKED_FL
-    if (repair_list)
-    {
-        if (!use_undo_p)
-        {
-            free_list_prev (item) = PREV_EMPTY;
-        }
-    }
-    if (gen_number == max_generation)
-    {
-        dprintf (3, ("[g%2d, b%2d]UL: %p->%p->%p (h: %p, t: %p)",
-            gen_number, bn, free_list_prev (item), item, free_list_slot (item),
-            al->alloc_list_head(), al->alloc_list_tail()));
-        dprintf (3, ("[g%2d, b%2d]UL: exit, h->N: %p, h->P: %p, t->N: %p, t->P: %p",
-            gen_number, bn,
-            (al->alloc_list_head() ? free_list_slot (al->alloc_list_head()) : 0),
-            (al->alloc_list_head() ? free_list_prev (al->alloc_list_head()) : 0),
-            (al->alloc_list_tail() ? free_list_slot (al->alloc_list_tail()) : 0),
-            (al->alloc_list_tail() ? free_list_prev (al->alloc_list_tail()) : 0)));
-    }
-#endif //DOUBLY_LINKED_FL
-    if (al->alloc_list_head() == 0)
-    {
-        assert (al->alloc_list_tail() == 0);
-    }
-}
-#ifdef DOUBLY_LINKED_FL
-void allocator::unlink_item_no_undo (unsigned int bn, uint8_t* item, size_t size)
-{
-    alloc_list* al = &alloc_list_of (bn);
-    uint8_t* next_item = free_list_slot (item);
-    uint8_t* prev_item = free_list_prev (item);
-#ifdef FL_VERIFICATION
-    {
-        uint8_t* start = al->alloc_list_head();
-        BOOL found_p = FALSE;
-        while (start)
-        {
-            if (start == item)
-            {
-                found_p = TRUE;
-                break;
-            }
-            start = free_list_slot (start);
-        }
-        if (!found_p)
-        {
-            dprintf (1, ("could not find %p in b%d!!!", item, a_l_number));
-            FATAL_GC_ERROR();
-        }
-    }
-#endif //FL_VERIFICATION
-    if (prev_item)
-    {
-        free_list_slot (prev_item) = next_item;
-    }
-    else
-    {
-        al->alloc_list_head() = next_item;
-    }
-    if (next_item)
-    {
-        free_list_prev (next_item) = prev_item;
-    }
-    if (al->alloc_list_tail() == item)
-    {
-        al->alloc_list_tail() = prev_item;
-    }
-    free_list_prev (item) = PREV_EMPTY;
-    if (gen_number == max_generation)
-    {
-        dprintf (3333, ("[g%2d, b%2d]ULN: %p->%p->%p (h: %p, t: %p)",
-            gen_number, bn, free_list_prev (item), item, free_list_slot (item),
-            al->alloc_list_head(), al->alloc_list_tail()));
-        dprintf (3333, ("[g%2d, b%2d]ULN: exit: h->N: %p, h->P: %p, t->N: %p, t->P: %p",
-            gen_number, bn,
-            (al->alloc_list_head() ? free_list_slot (al->alloc_list_head()) : 0),
-            (al->alloc_list_head() ? free_list_prev (al->alloc_list_head()) : 0),
-            (al->alloc_list_tail() ? free_list_slot (al->alloc_list_tail()) : 0),
-            (al->alloc_list_tail() ? free_list_prev (al->alloc_list_tail()) : 0)));
-    }
-}
-void allocator::unlink_item_no_undo (uint8_t* item, size_t size)
-{
-    unsigned int bn = first_suitable_bucket (size);
-    unlink_item_no_undo (bn, item, size);
-}
-void allocator::unlink_item_no_undo_added (unsigned int bn, uint8_t* item, uint8_t* previous_item)
-{
-    alloc_list* al = &alloc_list_of (bn);
-    uint8_t* next_item = free_list_slot (item);
-    uint8_t* prev_item = free_list_prev (item);
-    assert (prev_item == previous_item);
-    if (prev_item)
-    {
-        free_list_slot (prev_item) = next_item;
-    }
-    else
-    {
-        al->added_alloc_list_head() = next_item;
-    }
-    if (next_item)
-    {
-        free_list_prev (next_item) = prev_item;
-    }
-    if (al->added_alloc_list_tail() == item)
-    {
-        al->added_alloc_list_tail() = prev_item;
-    }
-    free_list_prev (item) = PREV_EMPTY;
-    if (gen_number == max_generation)
-    {
-        dprintf (3333, ("[g%2d, b%2d]ULNA: %p->%p->%p (h: %p, t: %p)",
-            gen_number, bn, free_list_prev (item), item, free_list_slot (item),
-            al->added_alloc_list_head(), al->added_alloc_list_tail()));
-        dprintf (3333, ("[g%2d, b%2d]ULNA: exit: h->N: %p, h->P: %p, t->N: %p, t->P: %p",
-            gen_number, bn,
-            (al->added_alloc_list_head() ? free_list_slot (al->added_alloc_list_head()) : 0),
-            (al->added_alloc_list_head() ? free_list_prev (al->added_alloc_list_head()) : 0),
-            (al->added_alloc_list_tail() ? free_list_slot (al->added_alloc_list_tail()) : 0),
-            (al->added_alloc_list_tail() ? free_list_prev (al->added_alloc_list_tail()) : 0)));
-    }
-}
-int allocator::thread_item_front_added (uint8_t* item, size_t size)
-{
-    unsigned int a_l_number = first_suitable_bucket (size);
-    alloc_list* al = &alloc_list_of (a_l_number);
-    free_list_slot (item) = al->added_alloc_list_head();
-    free_list_prev (item) = 0;
-    free_list_undo (item) = UNDO_EMPTY;
-    if (al->added_alloc_list_head() != 0)
-    {
-        free_list_prev (al->added_alloc_list_head()) = item;
-    }
-    al->added_alloc_list_head() = item;
-    if (al->added_alloc_list_tail() == 0)
-    {
-        al->added_alloc_list_tail() = item;
-    }
-    if (gen_number == max_generation)
-    {
-        dprintf (3333, ("[g%2d, b%2d]TFFA: exit: %p->%p->%p (h: %p, t: %p)",
-            gen_number, a_l_number,
-            free_list_prev (item), item, free_list_slot (item),
-            al->added_alloc_list_head(), al->added_alloc_list_tail()));
-        dprintf (3333, ("[g%2d, b%2d]TFFA: h->N: %p, h->P: %p, t->N: %p, t->P: %p",
-            gen_number, a_l_number,
-            (al->added_alloc_list_head() ? free_list_slot (al->added_alloc_list_head()) : 0),
-            (al->added_alloc_list_head() ? free_list_prev (al->added_alloc_list_head()) : 0),
-            (al->added_alloc_list_tail() ? free_list_slot (al->added_alloc_list_tail()) : 0),
-            (al->added_alloc_list_tail() ? free_list_prev (al->added_alloc_list_tail()) : 0)));
-    }
-    return a_l_number;
-}
-#if defined(MULTIPLE_HEAPS) && defined(USE_REGIONS)
-void allocator::count_items (gc_heap* this_hp, size_t* fl_items_count, size_t* fl_items_for_oh_count)
-{
-    uint64_t start_us = GetHighPrecisionTimeStamp();
-    uint64_t end_us = 0;
-    int align_const = get_alignment_constant (gen_number == max_generation);
-    size_t num_fl_items = 0;
-    size_t num_fl_items_for_oh = 0;
-    for (unsigned int i = 0; i < num_buckets; i++)
-    {
-        uint8_t* free_item = alloc_list_head_of (i);
-        while (free_item)
-        {
-            assert (((CObjectHeader*)free_item)->IsFree());
-            num_fl_items++;
-            heap_segment* region = gc_heap::region_of (free_item);
-            dprintf (3, ("b#%2d FL %Ix region %Ix heap %d -> %d",
-                i, free_item, (size_t)region, this_hp->heap_number, region->heap->heap_number));
-            if (region->heap != this_hp)
-            {
-                num_fl_items_for_oh++;
-            }
-            free_item = free_list_slot (free_item);
-        }
-    }
-    end_us = GetHighPrecisionTimeStamp();
-    dprintf (3, ("total - %Id items out of %Id items are from a different heap in %I64d us",
-        num_fl_items_for_oh, num_fl_items, (end_us - start_us)));
-    *fl_items_count = num_fl_items;
-    *fl_items_for_oh_count = num_fl_items_for_oh;
-}
-void min_fl_list_info::thread_item (uint8_t* item)
-{
-    free_list_slot (item) = 0;
-    free_list_undo (item) = UNDO_EMPTY;
-    assert (item != head);
-    free_list_prev (item) = tail;
-    if (head == 0)
-    {
-        head = item;
-    }
-    else
-    {
-        assert ((free_list_slot(head) != 0) || (tail == head));
-        assert (item != tail);
-        assert (free_list_slot(tail) == 0);
-        free_list_slot (tail) = item;
-    }
-    tail = item;
-}
-void min_fl_list_info::thread_item_no_prev (uint8_t* item)
-{
-    free_list_slot (item) = 0;
-    free_list_undo (item) = UNDO_EMPTY;
-    assert (item != head);
-    if (head == 0)
-    {
-        head = item;
-    }
-    else
-    {
-        assert ((free_list_slot(head) != 0) || (tail == head));
-        assert (item != tail);
-        assert (free_list_slot(tail) == 0);
-        free_list_slot (tail) = item;
-    }
-    tail = item;
-}
-void allocator::rethread_items (size_t* num_total_fl_items, size_t* num_total_fl_items_rethreaded, gc_heap* current_heap,
-                                min_fl_list_info* min_fl_list, size_t *free_list_space_per_heap, int num_heaps)
-{
-    uint64_t start_us = GetHighPrecisionTimeStamp();
-    uint64_t end_us = 0;
-    int align_const = get_alignment_constant (gen_number == max_generation);
-    size_t num_fl_items = 0;
-    size_t num_fl_items_rethreaded = 0;
-    assert (num_buckets <= MAX_BUCKET_COUNT);
-    for (unsigned int i = 0; i < num_buckets; i++)
-    {
-        min_fl_list_info* current_bucket_min_fl_list = min_fl_list + (i * num_heaps);
-        uint8_t* free_item = alloc_list_head_of (i);
-        uint8_t* prev_item = nullptr;
-        while (free_item)
-        {
-            assert (((CObjectHeader*)free_item)->IsFree());
-            num_fl_items++;
-            heap_segment* region = gc_heap::region_of (free_item);
-            dprintf (3, ("b#%2d FL %Ix region %Ix heap %d -> %d",
-                i, free_item, (size_t)region, current_heap->heap_number, region->heap->heap_number));
-            if (region->heap != current_heap)
-            {
-                num_fl_items_rethreaded++;
-                size_t size_o = Align(size (free_item), align_const);
-                uint8_t* next_item = free_list_slot (free_item);
-                int hn = region->heap->heap_number;
-                if (is_doubly_linked_p())
-                {
-                    unlink_item_no_undo (free_item, size_o);
-                    current_bucket_min_fl_list[hn].thread_item (free_item);
-                }
-                else
-                {
-                    unlink_item (i, free_item, prev_item, FALSE);
-                    current_bucket_min_fl_list[hn].thread_item_no_prev (free_item);
-                }
-                free_list_space_per_heap[hn] += size_o;
-                free_item = next_item;
-            }
-            else
-            {
-                prev_item = free_item;
-                free_item = free_list_slot (free_item);
-            }
-        }
-    }
-    end_us = GetHighPrecisionTimeStamp();
-    dprintf (8888, ("h%d total %Id items rethreaded out of %Id items in %I64d us (%I64dms)",
-        current_heap->heap_number, num_fl_items_rethreaded, num_fl_items, (end_us - start_us), ((end_us - start_us) / 1000)));
-    (*num_total_fl_items) += num_fl_items;
-    (*num_total_fl_items_rethreaded) += num_fl_items_rethreaded;
-}
-void allocator::merge_items (gc_heap* current_heap, int to_num_heaps, int from_num_heaps)
-{
-    int this_hn = current_heap->heap_number;
-    for (unsigned int i = 0; i < num_buckets; i++)
-    {
-        alloc_list* al = &alloc_list_of (i);
-        uint8_t*& head = al->alloc_list_head ();
-        uint8_t*& tail = al->alloc_list_tail ();
-        for (int other_hn = 0; other_hn < from_num_heaps; other_hn++)
-        {
-            min_fl_list_info* current_bucket_min_fl_list = gc_heap::g_heaps[other_hn]->min_fl_list + (i * to_num_heaps);
-            min_fl_list_info* current_heap_bucket_min_fl_list = &current_bucket_min_fl_list[this_hn];
-            uint8_t* head_other_heap = current_heap_bucket_min_fl_list->head;
-            if (head_other_heap)
-            {
-                if (is_doubly_linked_p())
-                {
-                    free_list_prev (head_other_heap) = tail;
-                }
-                uint8_t* saved_head = head;
-                uint8_t* saved_tail = tail;
-                if (head)
-                {
-                    free_list_slot (tail) = head_other_heap;
-                }
-                else
-                {
-                    head = head_other_heap;
-                }
-                tail = current_heap_bucket_min_fl_list->tail;
-            }
-        }
-    }
-}
-#endif //MULTIPLE_HEAPS && USE_REGIONS
-#endif //DOUBLY_LINKED_FL
-void allocator::clear()
-{
-    for (unsigned int i = 0; i < num_buckets; i++)
-    {
-        alloc_list_head_of (i) = 0;
-        alloc_list_tail_of (i) = 0;
-    }
-}
-void allocator::thread_item (uint8_t* item, size_t size)
-{
-    unsigned int a_l_number = first_suitable_bucket (size);
-    alloc_list* al = &alloc_list_of (a_l_number);
-    uint8_t*& head = al->alloc_list_head();
-    uint8_t*& tail = al->alloc_list_tail();
-    if (al->alloc_list_head() == 0)
-    {
-        assert (al->alloc_list_tail() == 0);
-    }
-    free_list_slot (item) = 0;
-    free_list_undo (item) = UNDO_EMPTY;
-    assert (item != head);
-#ifdef DOUBLY_LINKED_FL
-    if (gen_number == max_generation)
-    {
-        free_list_prev (item) = tail;
-    }
-#endif //DOUBLY_LINKED_FL
-    if (head == 0)
-    {
-        head = item;
-    }
-    else
-    {
-        assert ((free_list_slot(head) != 0) || (tail == head));
-        assert (item != tail);
-        assert (free_list_slot(tail) == 0);
-        free_list_slot (tail) = item;
-    }
-    tail = item;
-#ifdef DOUBLY_LINKED_FL
-    if (gen_number == max_generation)
-    {
-        dprintf (3333, ("[g%2d, b%2d]TFE: %p->%p->%p (h: %p, t: %p)",
-            gen_number, a_l_number,
-            free_list_prev (item), item, free_list_slot (item),
-            al->alloc_list_head(), al->alloc_list_tail()));
-        dprintf (3333, ("[g%2d, b%2d]TFE: exit: h->N: %p, h->P: %p, t->N: %p, t->P: %p",
-            gen_number, a_l_number,
-            (al->alloc_list_head() ? free_list_slot (al->alloc_list_head()) : 0),
-            (al->alloc_list_head() ? free_list_prev (al->alloc_list_head()) : 0),
-            (al->alloc_list_tail() ? free_list_slot (al->alloc_list_tail()) : 0),
-            (al->alloc_list_tail() ? free_list_prev (al->alloc_list_tail()) : 0)));
-    }
-#endif //DOUBLY_LINKED_FL
-}
-void allocator::thread_item_front (uint8_t* item, size_t size)
-{
-    unsigned int a_l_number = first_suitable_bucket (size);
-    alloc_list* al = &alloc_list_of (a_l_number);
-    if (al->alloc_list_head() == 0)
-    {
-        assert (al->alloc_list_tail() == 0);
-    }
-    free_list_slot (item) = al->alloc_list_head();
-    free_list_undo (item) = UNDO_EMPTY;
-    if (al->alloc_list_tail() == 0)
-    {
-        assert (al->alloc_list_head() == 0);
-        al->alloc_list_tail() = al->alloc_list_head();
-    }
-#ifdef DOUBLY_LINKED_FL
-    if (gen_number == max_generation)
-    {
-        if (al->alloc_list_head() != 0)
-        {
-            free_list_prev (al->alloc_list_head()) = item;
-        }
-    }
-#endif //DOUBLY_LINKED_FL
-    al->alloc_list_head() = item;
-    if (al->alloc_list_tail() == 0)
-    {
-        al->alloc_list_tail() = item;
-    }
-#ifdef DOUBLY_LINKED_FL
-    if (gen_number == max_generation)
-    {
-        free_list_prev (item) = 0;
-        dprintf (3333, ("[g%2d, b%2d]TFF: exit: %p->%p->%p (h: %p, t: %p)",
-            gen_number, a_l_number,
-            free_list_prev (item), item, free_list_slot (item),
-            al->alloc_list_head(), al->alloc_list_tail()));
-        dprintf (3333, ("[g%2d, b%2d]TFF: h->N: %p, h->P: %p, t->N: %p, t->P: %p",
-            gen_number, a_l_number,
-            (al->alloc_list_head() ? free_list_slot (al->alloc_list_head()) : 0),
-            (al->alloc_list_head() ? free_list_prev (al->alloc_list_head()) : 0),
-            (al->alloc_list_tail() ? free_list_slot (al->alloc_list_tail()) : 0),
-            (al->alloc_list_tail() ? free_list_prev (al->alloc_list_tail()) : 0)));
-    }
-#endif //DOUBLY_LINKED_FL
-}
-void allocator::copy_to_alloc_list (alloc_list* toalist)
-{
-    for (unsigned int i = 0; i < num_buckets; i++)
-    {
-        toalist [i] = alloc_list_of (i);
-#ifdef FL_VERIFICATION
-        size_t damage_count = alloc_list_damage_count_of (i);
-        assert (damage_count == 0);
-        uint8_t* free_item = alloc_list_head_of (i);
-        size_t count = 0;
-        while (free_item)
-        {
-            count++;
-            free_item = free_list_slot (free_item);
-        }
-        toalist[i].item_count = count;
-#endif //FL_VERIFICATION
-    }
-}
-void allocator::copy_from_alloc_list (alloc_list* fromalist)
-{
-    BOOL repair_list = !discard_if_no_fit_p ();
-#ifdef DOUBLY_LINKED_FL
-    BOOL bgc_repair_p = FALSE;
-    if (gen_number == max_generation)
-    {
-        bgc_repair_p = TRUE;
-        if (alloc_list_damage_count_of (0) != 0)
-        {
-            GCToOSInterface::DebugBreak();
-        }
-        uint8_t* b0_head = alloc_list_head_of (0);
-        if (b0_head)
-        {
-            free_list_prev (b0_head) = 0;
-        }
-        added_alloc_list_head_of (0) = 0;
-        added_alloc_list_tail_of (0) = 0;
-    }
-    unsigned int start_index = (bgc_repair_p ? 1 : 0);
-#else
-    unsigned int start_index = 0;
-#endif //DOUBLY_LINKED_FL
-    for (unsigned int i = start_index; i < num_buckets; i++)
-    {
-        size_t count = alloc_list_damage_count_of (i);
-        alloc_list_of (i) = fromalist [i];
-        assert (alloc_list_damage_count_of (i) == 0);
-        if (repair_list)
-        {
-            uint8_t* free_item = alloc_list_head_of (i);
-            while (free_item && count)
-            {
-                assert (((CObjectHeader*)free_item)->IsFree());
-                if ((free_list_undo (free_item) != UNDO_EMPTY))
-                {
-                    count--;
-                    free_list_slot (free_item) = free_list_undo (free_item);
-                    free_list_undo (free_item) = UNDO_EMPTY;
-                }
-                free_item = free_list_slot (free_item);
-            }
-#ifdef DOUBLY_LINKED_FL
-            if (bgc_repair_p)
-            {
-                added_alloc_list_head_of (i) = 0;
-                added_alloc_list_tail_of (i) = 0;
-            }
-#endif //DOUBLY_LINKED_FL
-#ifdef FL_VERIFICATION
-            free_item = alloc_list_head_of (i);
-            size_t item_count = 0;
-            while (free_item)
-            {
-                item_count++;
-                free_item = free_list_slot (free_item);
-            }
-            assert (item_count == alloc_list_of (i).item_count);
-#endif //FL_VERIFICATION
-        }
-#ifdef DEBUG
-        uint8_t* tail_item = alloc_list_tail_of (i);
-        assert ((tail_item == 0) || (free_list_slot (tail_item) == 0));
-#endif
-    }
-}
-void allocator::commit_alloc_list_changes()
-{
-    BOOL repair_list = !discard_if_no_fit_p ();
-#ifdef DOUBLY_LINKED_FL
-    BOOL bgc_repair_p = FALSE;
-    if (gen_number == max_generation)
-    {
-        bgc_repair_p = TRUE;
-    }
-#endif //DOUBLY_LINKED_FL
-    if (repair_list)
-    {
-        for (unsigned int i = 0; i < num_buckets; i++)
-        {
-            uint8_t* free_item = alloc_list_head_of (i);
-#ifdef DOUBLY_LINKED_FL
-            if (bgc_repair_p)
-            {
-                dprintf (3, ("C[b%2d] ENTRY: h: %p t: %p", i,
-                    alloc_list_head_of (i), alloc_list_tail_of (i)));
-            }
-            if (free_item && bgc_repair_p)
-            {
-                if (free_list_prev (free_item) != 0)
-                    free_list_prev (free_item) = 0;
-            }
-#endif //DOUBLY_LINKED_FL
-            size_t count = alloc_list_damage_count_of (i);
-            while (free_item && count)
-            {
-                assert (((CObjectHeader*)free_item)->IsFree());
-                if (free_list_undo (free_item) != UNDO_EMPTY)
-                {
-                    free_list_undo (free_item) = UNDO_EMPTY;
-#ifdef DOUBLY_LINKED_FL
-                    if (bgc_repair_p)
-                    {
-                        uint8_t* next_item = free_list_slot (free_item);
-                        if (next_item && (free_list_prev (next_item) != free_item))
-                            free_list_prev (next_item) = free_item;
-                    }
-#endif //DOUBLY_LINKED_FL
-                    count--;
-                }
-                free_item = free_list_slot (free_item);
-            }
-            alloc_list_damage_count_of (i) = 0;
-#ifdef DOUBLY_LINKED_FL
-            if (bgc_repair_p)
-            {
-                uint8_t* head = alloc_list_head_of (i);
-                uint8_t* tail_added = added_alloc_list_tail_of (i);
-                if (tail_added)
-                {
-                    assert (free_list_slot (tail_added) == 0);
-                    if (head)
-                    {
-                        free_list_slot (tail_added) = head;
-                        free_list_prev (head) = tail_added;
-                    }
-                }
-                uint8_t* head_added = added_alloc_list_head_of (i);
-                if (head_added)
-                {
-                    alloc_list_head_of (i) = head_added;
-                    uint8_t* final_head = alloc_list_head_of (i);
-                    if (alloc_list_tail_of (i) == 0)
-                    {
-                        alloc_list_tail_of (i) = tail_added;
-                    }
-                }
-                added_alloc_list_head_of (i) = 0;
-                added_alloc_list_tail_of (i) = 0;
-            }
-#endif //DOUBLY_LINKED_FL
-        }
-    }
-}
-#ifdef USE_REGIONS
-void allocator::thread_sip_fl (heap_segment* region)
-{
-    uint8_t* region_fl_head = region->free_list_head;
-    uint8_t* region_fl_tail = region->free_list_tail;
-    if (!region_fl_head)
-    {
-        assert (!region_fl_tail);
-        assert (region->free_list_size == 0);
-        return;
-    }
-    if (num_buckets == 1)
-    {
-        dprintf (REGIONS_LOG, ("threading gen%d region %p onto gen%d FL",
-            heap_segment_gen_num (region), heap_segment_mem (region), gen_number));
-        alloc_list* al = &alloc_list_of (0);
-        uint8_t*& head = al->alloc_list_head();
-        uint8_t*& tail = al->alloc_list_tail();
-        if (tail == 0)
-        {
-            assert (head == 0);
-            head = region_fl_head;
-        }
-        else
-        {
-            free_list_slot (tail) = region_fl_head;
-        }
-        tail = region_fl_tail;
-    }
-    else
-    {
-        dprintf (REGIONS_LOG, ("threading gen%d region %p onto gen%d bucketed FL",
-            heap_segment_gen_num (region), heap_segment_mem (region), gen_number));
-        uint8_t* region_fl_item = region_fl_head;
-        size_t total_free_size = 0;
-        while (region_fl_item)
-        {
-            uint8_t* next_fl_item = free_list_slot (region_fl_item);
-            size_t size_item = size (region_fl_item);
-            thread_item (region_fl_item, size_item);
-            total_free_size += size_item;
-            region_fl_item = next_fl_item;
-        }
-        assert (total_free_size == region->free_list_size);
-    }
-}
-#endif //USE_REGIONS
-#ifdef FEATURE_EVENT_TRACE
-uint16_t allocator::count_largest_items (etw_bucket_info* bucket_info,
-                                         size_t max_size,
-                                         size_t max_item_count,
-                                         size_t* recorded_fl_info_size)
-{
-    assert (gen_number == max_generation);
-    size_t size_counted_total = 0;
-    size_t items_counted_total = 0;
-    uint16_t bucket_info_index = 0;
-    for (int i = (num_buckets - 1); i >= 0; i--)
-    {
-        uint32_t items_counted = 0;
-        size_t size_counted = 0;
-        uint8_t* free_item = alloc_list_head_of ((unsigned int)i);
-        while (free_item)
-        {
-            assert (((CObjectHeader*)free_item)->IsFree());
-            size_t free_item_size = Align (size (free_item));
-            size_counted_total += free_item_size;
-            size_counted += free_item_size;
-            items_counted_total++;
-            items_counted++;
-            if ((size_counted_total > max_size) || (items_counted > max_item_count))
-            {
-                bucket_info[bucket_info_index++].set ((uint16_t)i, items_counted, size_counted);
-                *recorded_fl_info_size = size_counted_total;
-                return bucket_info_index;
-            }
-            free_item = free_list_slot (free_item);
-        }
-        if (items_counted)
-        {
-            bucket_info[bucket_info_index++].set ((uint16_t)i, items_counted, size_counted);
-        }
-    }
-    *recorded_fl_info_size = size_counted_total;
-    return bucket_info_index;
-}
-#endif //FEATURE_EVENT_TRACE
-void gc_heap::adjust_limit_clr (uint8_t* start, size_t limit_size, size_t size,
-                                alloc_context* acontext, uint32_t flags,
-                                heap_segment* seg, int align_const, int gen_number)
-{
-    bool uoh_p = (gen_number > 0);
-    GCSpinLock* msl = uoh_p ? &more_space_lock_uoh : &more_space_lock_soh;
-    uint64_t& total_alloc_bytes = uoh_p ? total_alloc_bytes_uoh : total_alloc_bytes_soh;
-    size_t aligned_min_obj_size = Align(min_obj_size, align_const);
-    if (seg)
-    {
-        assert (heap_segment_used (seg) <= heap_segment_committed (seg));
-    }
-#ifdef MULTIPLE_HEAPS
-    if (gen_number == 0)
-    {
-        if (!gen0_allocated_after_gc_p)
-        {
-            gen0_allocated_after_gc_p = true;
-        }
-    }
-#endif //MULTIPLE_HEAPS
-    dprintf (3, ("Expanding segment allocation [%zx, %zx[", (size_t)start,
-               (size_t)start + limit_size - aligned_min_obj_size));
-    if ((acontext->alloc_limit != start) &&
-        (acontext->alloc_limit + aligned_min_obj_size)!= start)
-    {
-        uint8_t*  hole = acontext->alloc_ptr;
-        if (hole != 0)
-        {
-            size_t  ac_size = (acontext->alloc_limit - acontext->alloc_ptr);
-            dprintf (3, ("filling up hole [%zx, %zx[", (size_t)hole, (size_t)hole + ac_size + aligned_min_obj_size));
-            acontext->alloc_bytes -= ac_size;
-            total_alloc_bytes -= ac_size;
-            size_t free_obj_size = ac_size + aligned_min_obj_size;
-            make_unused_array (hole, free_obj_size);
-            generation_free_obj_space (generation_of (gen_number)) += free_obj_size;
-        }
-        acontext->alloc_ptr = start;
-    }
-    else
-    {
-        if (gen_number == 0)
-        {
-#ifdef USE_REGIONS
-            if (acontext->alloc_ptr == 0)
-            {
-                acontext->alloc_ptr = start;
-            }
-            else
-#endif //USE_REGIONS
-            {
-                size_t pad_size = aligned_min_obj_size;
-                dprintf (3, ("contiguous ac: making min obj gap %p->%p(%zd)",
-                    acontext->alloc_ptr, (acontext->alloc_ptr + pad_size), pad_size));
-                make_unused_array (acontext->alloc_ptr, pad_size);
-                acontext->alloc_ptr += pad_size;
-            }
-        }
-    }
-    acontext->alloc_limit = (start + limit_size - aligned_min_obj_size);
-    size_t added_bytes = limit_size - ((gen_number <= max_generation) ? aligned_min_obj_size : 0);
-    acontext->alloc_bytes += added_bytes;
-    total_alloc_bytes     += added_bytes;
-    size_t etw_allocation_amount = 0;
-    bool fire_event_p = update_alloc_info (gen_number, added_bytes, &etw_allocation_amount);
-    uint8_t* saved_used = 0;
-    if (seg)
-    {
-        saved_used = heap_segment_used (seg);
-    }
-    if (seg == ephemeral_heap_segment)
-    {
-        if (heap_segment_used (seg) < (alloc_allocated - plug_skew))
-        {
-            heap_segment_used (seg) = alloc_allocated - plug_skew;
-            assert (heap_segment_mem (seg) <= heap_segment_used (seg));
-            assert (heap_segment_used (seg) <= heap_segment_reserved (seg));
-        }
-    }
-#ifdef BACKGROUND_GC
-    else if (seg)
-    {
-        uint8_t* old_allocated = heap_segment_allocated (seg) - plug_skew - limit_size;
-#ifdef FEATURE_LOH_COMPACTION
-        if (gen_number == loh_generation)
-        {
-            old_allocated -= Align (loh_padding_obj_size, align_const);
-        }
-#endif //FEATURE_LOH_COMPACTION
-        assert (heap_segment_used (seg) >= old_allocated);
-    }
-#endif //BACKGROUND_GC
-    uint8_t* clear_start = start - plug_skew;
-    uint8_t* clear_limit = start + limit_size - plug_skew;
-    if (flags & GC_ALLOC_ZEROING_OPTIONAL)
-    {
-        uint8_t* obj_start = acontext->alloc_ptr;
-        assert(start >= obj_start);
-        uint8_t* obj_end = obj_start + size - plug_skew;
-        assert(obj_end >= clear_start);
-        if(obj_start == start)
-        {
-            *(PTR_PTR)clear_start = 0;
-        }
-        dprintf(3, ("zeroing optional: skipping object at %p->%p(%zd)",
-            clear_start, obj_end, obj_end - clear_start));
-        clear_start = obj_end;
-    }
-    heap_segment* gen0_segment = ephemeral_heap_segment;
-#ifdef BACKGROUND_GC
-    {
-        if (uoh_p && gc_heap::background_running_p())
-        {
-            uint8_t* obj = acontext->alloc_ptr;
-            uint8_t* result = obj;
-            uint8_t* current_lowest_address = background_saved_lowest_address;
-            uint8_t* current_highest_address = background_saved_highest_address;
-            if (current_c_gc_state == c_gc_state_planning)
-            {
-                dprintf (3, ("Concurrent allocation of a large object %zx",
-                            (size_t)obj));
-                if ((result < current_highest_address) && (result >= current_lowest_address))
-                {
-#ifdef DOUBLY_LINKED_FL
-                    heap_segment* seg = seg_mapping_table_segment_of (result);
-                    uint8_t* background_allocated = heap_segment_background_allocated(seg);
-                    if (background_allocated != 0)
-#endif //DOUBLY_LINKED_FL
-                    {
-                        dprintf(3, ("Setting mark bit at address %zx",
-                            (size_t)(&mark_array[mark_word_of(result)])));
-                        mark_array_set_marked(result);
-                    }
-                }
-            }
-        }
-    }
-#endif //BACKGROUND_GC
-    if ((seg == 0) || (clear_limit <= heap_segment_used (seg)))
-    {
-        add_saved_spinlock_info (uoh_p, me_release, mt_clr_mem, msl_entered);
-        leave_spin_lock (msl);
-        if (clear_start < clear_limit)
-        {
-            dprintf(3, ("clearing memory at %p for %zd bytes", clear_start, clear_limit - clear_start));
-            memclr(clear_start, clear_limit - clear_start);
-        }
-    }
-    else
-    {
-        uint8_t* used = heap_segment_used (seg);
-        heap_segment_used (seg) = clear_limit;
-        add_saved_spinlock_info (uoh_p, me_release, mt_clr_mem, msl_entered);
-        leave_spin_lock (msl);
-        if (clear_start < used)
-        {
-            if (used != saved_used)
-            {
-                FATAL_GC_ERROR();
-            }
-            dprintf (2, ("clearing memory before used at %p for %zd bytes", clear_start, used - clear_start));
-            memclr (clear_start, used - clear_start);
-        }
-    }
-#ifdef FEATURE_EVENT_TRACE
-    if (fire_event_p)
-    {
-        fire_etw_allocation_event (etw_allocation_amount, gen_number, acontext->alloc_ptr, size);
-    }
-#endif //FEATURE_EVENT_TRACE
-    if (seg == gen0_segment ||
-       ((seg == nullptr) && (gen_number == 0) && (limit_size >= CLR_SIZE / 2)))
-    {
-        if (gen0_must_clear_bricks > 0)
-        {
-            size_t b = brick_of (acontext->alloc_ptr);
-            set_brick (b, acontext->alloc_ptr - brick_address (b));
-            b++;
-            dprintf (3, ("Allocation Clearing bricks [%zx, %zx[",
-                         b, brick_of (align_on_brick (start + limit_size))));
-            volatile short* x = &brick_table [b];
-            short* end_x = &brick_table [brick_of (align_on_brick (start + limit_size))];
-            for (;x < end_x;x++)
-                *x = -1;
-        }
-        else
-        {
-            gen0_bricks_cleared = FALSE;
-        }
-    }
-}
-size_t gc_heap::new_allocation_limit (size_t size, size_t physical_limit, int gen_number)
-{
-    dynamic_data* dd = dynamic_data_of (gen_number);
-    ptrdiff_t new_alloc = dd_new_allocation (dd);
-    assert (new_alloc == (ptrdiff_t)Align (new_alloc, get_alignment_constant (gen_number < uoh_start_generation)));
-    ptrdiff_t logical_limit = max (new_alloc, (ptrdiff_t)size);
-    size_t limit = min (logical_limit, (ptrdiff_t)physical_limit);
-    assert (limit == Align (limit, get_alignment_constant (gen_number <= max_generation)));
-    return limit;
-}
-size_t gc_heap::limit_from_size (size_t size, uint32_t flags, size_t physical_limit, int gen_number,
-                                 int align_const)
-{
-    size_t padded_size = size + Align (min_obj_size, align_const);
-    assert ((gen_number != 0) || (physical_limit >= padded_size));
-    size_t min_size_to_allocate = ((gen_number == 0 && !(flags & GC_ALLOC_ZEROING_OPTIONAL)) ? allocation_quantum : 0);
-    size_t desired_size_to_allocate  = max (padded_size, min_size_to_allocate);
-    size_t new_physical_limit = min (physical_limit, desired_size_to_allocate);
-    size_t new_limit = new_allocation_limit (padded_size,
-                                             new_physical_limit,
-                                             gen_number);
-    assert (new_limit >= (size + Align (min_obj_size, align_const)));
-    dprintf (3, ("h%d requested to allocate %zd bytes, actual size is %zd, phy limit: %zd",
-        heap_number, size, new_limit, physical_limit));
-    return new_limit;
-}
-void gc_heap::add_to_oom_history_per_heap()
-{
-    oom_history* current_hist = &oomhist_per_heap[oomhist_index_per_heap];
-    memcpy (current_hist, &oom_info, sizeof (oom_info));
-    oomhist_index_per_heap++;
-    if (oomhist_index_per_heap == max_oom_history_count)
-    {
-        oomhist_index_per_heap = 0;
-    }
-}
-void gc_heap::handle_oom (oom_reason reason, size_t alloc_size,
-                          uint8_t* allocated, uint8_t* reserved)
-{
-    if (reason == oom_budget)
-    {
-        alloc_size = dd_min_size (dynamic_data_of (0)) / 2;
-    }
-    if ((reason == oom_budget) && ((!fgm_result.loh_p) && (fgm_result.fgm != fgm_no_failure)))
-    {
-        reason = oom_low_mem;
-    }
-    oom_info.reason = reason;
-    oom_info.allocated = allocated;
-    oom_info.reserved = reserved;
-    oom_info.alloc_size = alloc_size;
-    oom_info.gc_index = settings.gc_index;
-    oom_info.fgm = fgm_result.fgm;
-    oom_info.size = fgm_result.size;
-    oom_info.available_pagefile_mb = fgm_result.available_pagefile_mb;
-    oom_info.loh_p = fgm_result.loh_p;
-    add_to_oom_history_per_heap();
-    fgm_result.fgm = fgm_no_failure;
-    if (GCConfig::GetBreakOnOOM())
-    {
-        GCToOSInterface::DebugBreak();
-    }
-}
-#ifdef BACKGROUND_GC
-BOOL gc_heap::background_allowed_p()
-{
-    return ( gc_can_use_concurrent && ((settings.pause_mode == pause_interactive) || (settings.pause_mode == pause_sustained_low_latency)) );
-}
-#endif //BACKGROUND_GC
-void gc_heap::check_for_full_gc (int gen_num, size_t size)
-{
-    BOOL should_notify = FALSE;
-    BOOL alloc_factor = TRUE;
-    int n_initial = gen_num;
-    BOOL local_blocking_collection = FALSE;
-    BOOL local_elevation_requested = FALSE;
-    int new_alloc_remain_percent = 0;
-    if (full_gc_approach_event_set)
-    {
-        return;
-    }
-    if (gen_num < max_generation)
-    {
-        gen_num = max_generation;
-    }
-    dynamic_data* dd_full = dynamic_data_of (gen_num);
-    ptrdiff_t new_alloc_remain = 0;
-    uint32_t pct = (gen_num >= uoh_start_generation) ? fgn_loh_percent : fgn_maxgen_percent;
-    for (int gen_index = 0; gen_index < total_generation_count; gen_index++)
-    {
-        dprintf (2, ("FGN: h#%d: gen%d: %zd(%zd)",
-                     heap_number, gen_index,
-                     dd_new_allocation (dynamic_data_of (gen_index)),
-                     dd_desired_allocation (dynamic_data_of (gen_index))));
-    }
-    if (n_initial == 0)
-    {
-        dprintf (2, ("FGN: gen0 last recorded alloc: %zd", fgn_last_alloc));
-        dynamic_data* dd_0 = dynamic_data_of (n_initial);
-        if (((fgn_last_alloc - dd_new_allocation (dd_0)) < fgn_check_quantum) &&
-            (dd_new_allocation (dd_0) >= 0))
-        {
-            return;
-        }
-        else
-        {
-            fgn_last_alloc = dd_new_allocation (dd_0);
-            dprintf (2, ("FGN: gen0 last recorded alloc is now: %zd", fgn_last_alloc));
-        }
-        size = 0;
-    }
-    int n = 0;
-    for (int i = 1; i <= max_generation; i++)
-    {
-            if (get_new_allocation (i) <= 0)
-            {
-                n = i;
-            }
-            else
-                break;
-    }
-    dprintf (2, ("FGN: h#%d: gen%d budget exceeded", heap_number, n));
-    if (gen_num == max_generation)
-    {
-        if (n < (max_generation - 1))
-        {
-            goto check_other_factors;
-        }
-    }
-    new_alloc_remain = dd_new_allocation (dd_full) - size;
-    new_alloc_remain_percent = (int)(((float)(new_alloc_remain) / (float)dd_desired_allocation (dd_full)) * 100);
-    dprintf (2, ("FGN: alloc threshold for gen%d is %d%%, current threshold is %d%%",
-                 gen_num, pct, new_alloc_remain_percent));
-    if (new_alloc_remain_percent <= (int)pct)
-    {
-#ifdef BACKGROUND_GC
-        if (background_allowed_p())
-        {
-            goto check_other_factors;
-        }
-#endif //BACKGROUND_GC
-        should_notify = TRUE;
-        goto done;
-    }
-check_other_factors:
-    dprintf (2, ("FGC: checking other factors"));
-    n = generation_to_condemn (n,
-                               &local_blocking_collection,
-                               &local_elevation_requested,
-                               TRUE);
-    if (local_elevation_requested && (n == max_generation))
-    {
-        if (settings.should_lock_elevation)
-        {
-            int local_elevation_locked_count = settings.elevation_locked_count + 1;
-            if (local_elevation_locked_count != 6)
-            {
-                dprintf (2, ("FGN: lock count is %d - Condemning max_generation-1",
-                    local_elevation_locked_count));
-                n = max_generation - 1;
-            }
-        }
-    }
-    dprintf (2, ("FGN: we estimate gen%d will be collected", n));
-#ifdef BACKGROUND_GC
-    if ((n == max_generation) &&
-        (gc_heap::background_running_p()))
-    {
-        n = max_generation - 1;
-        dprintf (2, ("FGN: bgc - 1 instead of 2"));
-    }
-    if ((n == max_generation) && !local_blocking_collection)
-    {
-        if (!background_allowed_p())
-        {
-            local_blocking_collection = TRUE;
-        }
-    }
-#endif //BACKGROUND_GC
-    dprintf (2, ("FGN: we estimate gen%d will be collected: %s",
-                       n,
-                       (local_blocking_collection ? "blocking" : "background")));
-    if ((n == max_generation) && local_blocking_collection)
-    {
-        alloc_factor = FALSE;
-        should_notify = TRUE;
-        goto done;
-    }
-done:
-    if (should_notify)
-    {
-        dprintf (2, ("FGN: gen%d detecting full GC approaching(%s) (GC#%zd) (%d%% left in gen%d)",
-                     n_initial,
-                     (alloc_factor ? "alloc" : "other"),
-                     dd_collection_count (dynamic_data_of (0)),
-                     new_alloc_remain_percent,
-                     gen_num));
-        send_full_gc_notification (n_initial, alloc_factor);
-    }
-}
-void gc_heap::send_full_gc_notification (int gen_num, BOOL due_to_alloc_p)
-{
-    if (!full_gc_approach_event_set)
-    {
-        assert (full_gc_approach_event.IsValid());
-        FIRE_EVENT(GCFullNotify_V1, gen_num, due_to_alloc_p);
-        full_gc_end_event.Reset();
-        full_gc_approach_event.Set();
-        full_gc_approach_event_set = true;
-    }
-}
-wait_full_gc_status gc_heap::full_gc_wait (GCEvent *event, int time_out_ms)
-{
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hp = gc_heap::g_heaps[0];
-#else
-    gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-    if (hp->fgn_maxgen_percent == 0)
-    {
-        return wait_full_gc_na;
-    }
-    uint32_t wait_result = user_thread_wait(event, FALSE, time_out_ms);
-    if ((wait_result == WAIT_OBJECT_0) || (wait_result == WAIT_TIMEOUT))
-    {
-        if (hp->fgn_maxgen_percent == 0)
-        {
-            return wait_full_gc_cancelled;
-        }
-        if (wait_result == WAIT_OBJECT_0)
-        {
-#ifdef BACKGROUND_GC
-            if (fgn_last_gc_was_concurrent)
-            {
-                fgn_last_gc_was_concurrent = FALSE;
-                return wait_full_gc_na;
-            }
-            else
-#endif //BACKGROUND_GC
-            {
-                return wait_full_gc_success;
-            }
-        }
-        else
-        {
-            return wait_full_gc_timeout;
-        }
-    }
-    else
-    {
-        return wait_full_gc_failed;
-    }
-}
-size_t gc_heap::get_full_compact_gc_count()
-{
-    return full_gc_counts[gc_type_compacting];
-}
-inline
-BOOL gc_heap::short_on_end_of_seg (heap_segment* seg)
-{
-    uint8_t* allocated = heap_segment_allocated (seg);
-#ifdef USE_REGIONS
-    assert (end_gen0_region_space != uninitialized_end_gen0_region_space);
-    BOOL sufficient_p = sufficient_space_regions_for_allocation (end_gen0_region_space, end_space_after_gc());
-#else
-    BOOL sufficient_p = sufficient_space_end_seg (allocated,
-                                                  heap_segment_committed (seg),
-                                                  heap_segment_reserved (seg),
-                                                  end_space_after_gc());
-#endif //USE_REGIONS
-    if (!sufficient_p)
-    {
-        if (sufficient_gen0_space_p)
-        {
-            dprintf (GTC_LOG, ("gen0 has enough free space"));
-        }
-        sufficient_p = sufficient_gen0_space_p;
-    }
-    return !sufficient_p;
-}
-inline
-BOOL gc_heap::a_fit_free_list_p (int gen_number,
-                                 size_t size,
-                                 alloc_context* acontext,
-                                 uint32_t flags,
-                                 int align_const)
-{
-    BOOL can_fit = FALSE;
-    generation* gen = generation_of (gen_number);
-    allocator* gen_allocator = generation_allocator (gen);
-    for (unsigned int a_l_idx = gen_allocator->first_suitable_bucket(size); a_l_idx < gen_allocator->number_of_buckets(); a_l_idx++)
-    {
-        uint8_t* free_list = gen_allocator->alloc_list_head_of (a_l_idx);
-        uint8_t* prev_free_item = 0;
-        while (free_list != 0)
-        {
-            dprintf (3, ("considering free list %zx", (size_t)free_list));
-            size_t free_list_size = unused_array_size (free_list);
-            if ((size + Align (min_obj_size, align_const)) <= free_list_size)
-            {
-                dprintf (3, ("Found adequate unused area: [%zx, size: %zd",
-                                (size_t)free_list, free_list_size));
-                gen_allocator->unlink_item (a_l_idx, free_list, prev_free_item, FALSE);
-                size_t limit = limit_from_size (size, flags, free_list_size, gen_number, align_const);
-                dd_new_allocation (dynamic_data_of (gen_number)) -= limit;
-                uint8_t*  remain = (free_list + limit);
-                size_t remain_size = (free_list_size - limit);
-                if (remain_size >= Align(min_free_list, align_const))
-                {
-                    make_unused_array (remain, remain_size);
-                    gen_allocator->thread_item_front (remain, remain_size);
-                    assert (remain_size >= Align (min_obj_size, align_const));
-                }
-                else
-                {
-                    limit += remain_size;
-                }
-                generation_free_list_space (gen) -= limit;
-                assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
-                adjust_limit_clr (free_list, limit, size, acontext, flags, 0, align_const, gen_number);
-                can_fit = TRUE;
-                goto end;
-            }
-            else if (gen_allocator->discard_if_no_fit_p())
-            {
-                assert (prev_free_item == 0);
-                dprintf (3, ("couldn't use this free area, discarding"));
-                generation_free_obj_space (gen) += free_list_size;
-                gen_allocator->unlink_item (a_l_idx, free_list, prev_free_item, FALSE);
-                generation_free_list_space (gen) -= free_list_size;
-                assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
-            }
-            else
-            {
-                prev_free_item = free_list;
-            }
-            free_list = free_list_slot (free_list);
-        }
-    }
-end:
-    return can_fit;
-}
-#ifdef BACKGROUND_GC
-void gc_heap::bgc_uoh_alloc_clr (uint8_t* alloc_start,
-                                 size_t size,
-                                 alloc_context* acontext,
-                                 uint32_t flags,
-                                 int gen_number,
-                                 int align_const,
-                                 int lock_index,
-                                 BOOL check_used_p,
-                                 heap_segment* seg)
-{
-    make_unused_array (alloc_start, size);
-#ifdef DOUBLY_LINKED_FL
-    clear_prev_bit (alloc_start, size);
-#endif //DOUBLY_LINKED_FL
-    size_t size_of_array_base = sizeof(ArrayBase);
-    bgc_alloc_lock->uoh_alloc_done_with_index (lock_index);
-    size_t size_to_skip = size_of_array_base;
-    size_t size_to_clear = size - size_to_skip - plug_skew;
-    size_t saved_size_to_clear = size_to_clear;
-    if (check_used_p)
-    {
-        uint8_t* end = alloc_start + size - plug_skew;
-        uint8_t* used = heap_segment_used (seg);
-        if (used < end)
-        {
-            if ((alloc_start + size_to_skip) < used)
-            {
-                size_to_clear = used - (alloc_start + size_to_skip);
-            }
-            else
-            {
-                size_to_clear = 0;
-            }
-            dprintf (2, ("bgc uoh: setting used to %p", end));
-            heap_segment_used (seg) = end;
-        }
-        dprintf (2, ("bgc uoh: used: %p, alloc: %p, end of alloc: %p, clear %zd bytes",
-                     used, alloc_start, end, size_to_clear));
-    }
-    else
-    {
-        dprintf (2, ("bgc uoh: [%p-[%p(%zd)", alloc_start, alloc_start+size, size));
-    }
-#ifdef VERIFY_HEAP
-    if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
-    {
-        if (size_to_clear < saved_size_to_clear)
-        {
-            size_to_clear = saved_size_to_clear;
-        }
-    }
-#endif //VERIFY_HEAP
-    size_t allocated_size = size - Align (min_obj_size, align_const);
-    total_alloc_bytes_uoh += allocated_size;
-    size_t etw_allocation_amount = 0;
-    bool fire_event_p = update_alloc_info (gen_number, allocated_size, &etw_allocation_amount);
-    dprintf (SPINLOCK_LOG, ("[%d]Lmsl to clear uoh obj", heap_number));
-    add_saved_spinlock_info (true, me_release, mt_clr_large_mem, msl_entered);
-    leave_spin_lock (&more_space_lock_uoh);
-#ifdef FEATURE_EVENT_TRACE
-    if (fire_event_p)
-    {
-        fire_etw_allocation_event (etw_allocation_amount, gen_number, alloc_start, size);
-    }
-#endif //FEATURE_EVENT_TRACE
-    ((void**) alloc_start)[-1] = 0;     //clear the sync block
-    if (!(flags & GC_ALLOC_ZEROING_OPTIONAL))
-    {
-        memclr(alloc_start + size_to_skip, size_to_clear);
-    }
-#ifdef MULTIPLE_HEAPS
-    assert (heap_of (alloc_start) == this);
-#endif // MULTIPLE_HEAPS
-    bgc_alloc_lock->uoh_alloc_set (alloc_start);
-    acontext->alloc_ptr = alloc_start;
-    acontext->alloc_limit = (alloc_start + size - Align (min_obj_size, align_const));
-    clear_unused_array(alloc_start, size);
-}
-#endif //BACKGROUND_GC
-BOOL gc_heap::a_fit_free_list_uoh_p (size_t size,
-                                       alloc_context* acontext,
-                                       uint32_t flags,
-                                       int align_const,
-                                       int gen_number)
-{
-    BOOL can_fit = FALSE;
-    generation* gen = generation_of (gen_number);
-    allocator* allocator = generation_allocator (gen);
-#ifdef FEATURE_LOH_COMPACTION
-    size_t loh_pad = gen_number == loh_generation ? Align (loh_padding_obj_size, align_const) : 0;
-#endif //FEATURE_LOH_COMPACTION
-#ifdef BACKGROUND_GC
-    int cookie = -1;
-#endif //BACKGROUND_GC
-    for (unsigned int a_l_idx = allocator->first_suitable_bucket(size); a_l_idx < allocator->number_of_buckets(); a_l_idx++)
-    {
-        uint8_t* free_list = allocator->alloc_list_head_of (a_l_idx);
-        uint8_t* prev_free_item = 0;
-        while (free_list != 0)
-        {
-            dprintf (3, ("considering free list %zx", (size_t)free_list));
-            size_t free_list_size = unused_array_size(free_list);
-            ptrdiff_t diff = free_list_size - size;
-#ifdef FEATURE_LOH_COMPACTION
-            diff -= loh_pad;
-#endif //FEATURE_LOH_COMPACTION
-            if ((diff == 0) || (diff >= (ptrdiff_t)Align (min_obj_size, align_const)))
-            {
-#ifdef BACKGROUND_GC
-#ifdef MULTIPLE_HEAPS
-                assert (heap_of (free_list) == this);
-#endif // MULTIPLE_HEAPS
-                cookie = bgc_alloc_lock->uoh_alloc_set (free_list);
-                bgc_track_uoh_alloc();
-#endif //BACKGROUND_GC
-                allocator->unlink_item (a_l_idx, free_list, prev_free_item, FALSE);
-                remove_gen_free (gen_number, free_list_size);
-                size_t limit = limit_from_size (size - Align(min_obj_size, align_const), flags, free_list_size,
-                                                gen_number, align_const);
-                dd_new_allocation (dynamic_data_of (gen_number)) -= limit;
-                size_t saved_free_list_size = free_list_size;
-#ifdef FEATURE_LOH_COMPACTION
-                if (loh_pad)
-                {
-                    make_unused_array (free_list, loh_pad);
-                    generation_free_obj_space (gen) += loh_pad;
-                    limit -= loh_pad;
-                    free_list += loh_pad;
-                    free_list_size -= loh_pad;
-                }
-#endif //FEATURE_LOH_COMPACTION
-                uint8_t*  remain = (free_list + limit);
-                size_t remain_size = (free_list_size - limit);
-                if (remain_size != 0)
-                {
-                    assert (remain_size >= Align (min_obj_size, align_const));
-                    make_unused_array (remain, remain_size);
-                }
-                if (remain_size >= Align(min_free_list, align_const))
-                {
-                    uoh_thread_gap_front (remain, remain_size, gen);
-                    add_gen_free (gen_number, remain_size);
-                    assert (remain_size >= Align (min_obj_size, align_const));
-                }
-                else
-                {
-                    generation_free_obj_space (gen) += remain_size;
-                }
-                generation_free_list_space (gen) -= saved_free_list_size;
-                assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
-                generation_free_list_allocated (gen) += limit;
-                dprintf (3, ("found fit on loh at %p", free_list));
-#ifdef BACKGROUND_GC
-                if (cookie != -1)
-                {
-                    bgc_uoh_alloc_clr (free_list, limit, acontext, flags, gen_number, align_const, cookie, FALSE, 0);
-                }
-                else
-#endif //BACKGROUND_GC
-                {
-                    adjust_limit_clr (free_list, limit, size, acontext, flags, 0, align_const, gen_number);
-                }
-                acontext->alloc_limit += Align (min_obj_size, align_const);
-                can_fit = TRUE;
-                goto exit;
-            }
-            prev_free_item = free_list;
-            free_list = free_list_slot (free_list);
-        }
-    }
-exit:
-    return can_fit;
-}
-BOOL gc_heap::a_fit_segment_end_p (int gen_number,
-                                   heap_segment* seg,
-                                   size_t size,
-                                   alloc_context* acontext,
-                                   uint32_t flags,
-                                   int align_const,
-                                   BOOL* commit_failed_p)
-{
-    *commit_failed_p = FALSE;
-    size_t limit = 0;
-    bool hard_limit_short_seg_end_p = false;
-#ifdef BACKGROUND_GC
-    int cookie = -1;
-#endif //BACKGROUND_GC
-    uint8_t*& allocated = ((gen_number == 0) ?
-                                    alloc_allocated :
-                                    heap_segment_allocated(seg));
-    size_t pad = Align (min_obj_size, align_const);
-#ifdef FEATURE_LOH_COMPACTION
-    size_t loh_pad = Align (loh_padding_obj_size, align_const);
-    if (gen_number == loh_generation)
-    {
-        pad += loh_pad;
-    }
-#endif //FEATURE_LOH_COMPACTION
-    uint8_t* end = heap_segment_committed (seg) - pad;
-    if (a_size_fit_p (size, allocated, end, align_const))
-    {
-        limit = limit_from_size (size,
-                                 flags,
-                                 (end - allocated),
-                                 gen_number, align_const);
-        goto found_fit;
-    }
-    end = heap_segment_reserved (seg) - pad;
-    if ((heap_segment_reserved (seg) != heap_segment_committed (seg)) && (a_size_fit_p (size, allocated, end, align_const)))
-    {
-        limit = limit_from_size (size,
-                                 flags,
-                                 (end - allocated),
-                                 gen_number, align_const);
-        if (grow_heap_segment (seg, (allocated + limit), &hard_limit_short_seg_end_p))
-        {
-            goto found_fit;
-        }
-        else
-        {
-#ifdef USE_REGIONS
-            *commit_failed_p = TRUE;
-#else
-            if (!hard_limit_short_seg_end_p)
-            {
-                dprintf (2, ("can't grow segment, doing a full gc"));
-                *commit_failed_p = TRUE;
-            }
-            else
-            {
-                assert (heap_hard_limit);
-            }
-#endif // USE_REGIONS
-        }
-    }
-    goto found_no_fit;
-found_fit:
-    dd_new_allocation (dynamic_data_of (gen_number)) -= limit;
-#ifdef BACKGROUND_GC
-    if (gen_number != 0)
-    {
-#ifdef MULTIPLE_HEAPS
-        assert (heap_of (allocated) == this);
-#endif // MULTIPLE_HEAPS
-        cookie = bgc_alloc_lock->uoh_alloc_set (allocated);
-        bgc_track_uoh_alloc();
-    }
-#endif //BACKGROUND_GC
-#ifdef FEATURE_LOH_COMPACTION
-    if (gen_number == loh_generation)
-    {
-        make_unused_array (allocated, loh_pad);
-        generation_free_obj_space (generation_of (gen_number)) += loh_pad;
-        allocated += loh_pad;
-        limit -= loh_pad;
-    }
-#endif //FEATURE_LOH_COMPACTION
-#if defined (VERIFY_HEAP) && defined (_DEBUG)
-    ((void**)allocated)[-1] = 0;    // clear the sync block
-    VOLATILE_MEMORY_BARRIER();
-#endif //VERIFY_HEAP && _DEBUG
-    uint8_t* old_alloc;
-    old_alloc = allocated;
-    dprintf (3, ("found fit at end of seg: %p", old_alloc));
-#ifdef BACKGROUND_GC
-    if (cookie != -1)
-    {
-        allocated += limit;
-        bgc_uoh_alloc_clr (old_alloc, limit, acontext, flags, gen_number, align_const, cookie, TRUE, seg);
-    }
-    else
-#endif //BACKGROUND_GC
-    {
-        if ((flags & GC_ALLOC_ZEROING_OPTIONAL) &&
-            ((allocated == acontext->alloc_limit) ||
-             (allocated == (acontext->alloc_limit + Align (min_obj_size, align_const)))))
-        {
-            assert(gen_number == 0);
-            assert(allocated > acontext->alloc_ptr);
-            size_t extra = allocated - acontext->alloc_ptr;
-            limit -= extra;
-            dynamic_data* dd = dynamic_data_of (0);
-            dd_new_allocation (dd) += extra;
-            limit += Align(min_obj_size, align_const);
-        }
-        allocated += limit;
-        adjust_limit_clr (old_alloc, limit, size, acontext, flags, seg, align_const, gen_number);
-    }
-    return TRUE;
-found_no_fit:
-    return FALSE;
-}
-BOOL gc_heap::uoh_a_fit_segment_end_p (int gen_number,
-                                       size_t size,
-                                       alloc_context* acontext,
-                                       uint32_t flags,
-                                       int align_const,
-                                       BOOL* commit_failed_p,
-                                       oom_reason* oom_r)
-{
-    *commit_failed_p = FALSE;
-    generation* gen = generation_of (gen_number);
-    heap_segment* seg = generation_allocation_segment (gen);
-    BOOL can_allocate_p = FALSE;
-    while (seg)
-    {
-#ifdef BACKGROUND_GC
-        if (seg->flags & heap_segment_flags_uoh_delete)
-        {
-            dprintf (3, ("h%d skipping seg %zx to be deleted", heap_number, (size_t)seg));
-        }
-        else
-#endif //BACKGROUND_GC
-        {
-            if (a_fit_segment_end_p (gen_number, seg, (size - Align (min_obj_size, align_const)),
-                                        acontext, flags, align_const, commit_failed_p))
-            {
-                acontext->alloc_limit += Align (min_obj_size, align_const);
-                can_allocate_p = TRUE;
-                break;
-            }
-            if (*commit_failed_p)
-            {
-                *oom_r = oom_cant_commit;
-                break;
-            }
-        }
-        seg = heap_segment_next_rw (seg);
-    }
-    if (can_allocate_p)
-    {
-        generation_end_seg_allocated (gen) += size;
-    }
-    return can_allocate_p;
-}
-#ifdef BACKGROUND_GC
-inline
-enter_msl_status gc_heap::wait_for_background (alloc_wait_reason awr, bool loh_p)
-{
-    GCSpinLock* msl = loh_p ? &more_space_lock_uoh : &more_space_lock_soh;
-    enter_msl_status msl_status = msl_entered;
-    dprintf (2, ("BGC is already in progress, waiting for it to finish"));
-    add_saved_spinlock_info (loh_p, me_release, mt_wait_bgc, msl_status);
-    leave_spin_lock (msl);
-    background_gc_wait (awr);
-    msl_status = enter_spin_lock_msl (msl);
-    add_saved_spinlock_info (loh_p, me_acquire, mt_wait_bgc, msl_status);
-    return msl_status;
-}
-bool gc_heap::wait_for_bgc_high_memory (alloc_wait_reason awr, bool loh_p, enter_msl_status* msl_status)
-{
-    bool wait_p = false;
-    if (gc_heap::background_running_p())
-    {
-        uint32_t memory_load;
-        get_memory_info (&memory_load);
-        if (memory_load >= m_high_memory_load_th)
-        {
-            wait_p = true;
-            dprintf (GTC_LOG, ("high mem - wait for BGC to finish, wait reason: %d", awr));
-            *msl_status = wait_for_background (awr, loh_p);
-        }
-    }
-    return wait_p;
-}
-#endif //BACKGROUND_GC
-BOOL gc_heap::trigger_ephemeral_gc (gc_reason gr, enter_msl_status* msl_status)
-{
-#ifdef BACKGROUND_GC
-    wait_for_bgc_high_memory (awr_loh_oos_bgc, false, msl_status);
-    if (*msl_status == msl_retry_different_heap) return FALSE;
-#endif //BACKGROUND_GC
-    BOOL did_full_compact_gc = FALSE;
-    dprintf (1, ("h%d triggering a gen1 GC", heap_number));
-    size_t last_full_compact_gc_count = get_full_compact_gc_count();
-    vm_heap->GarbageCollectGeneration(max_generation - 1, gr);
-#ifdef MULTIPLE_HEAPS
-    *msl_status = enter_spin_lock_msl (&more_space_lock_soh);
-    if (*msl_status == msl_retry_different_heap) return FALSE;
-    add_saved_spinlock_info (false, me_acquire, mt_t_eph_gc, *msl_status);
-#endif //MULTIPLE_HEAPS
-    size_t current_full_compact_gc_count = get_full_compact_gc_count();
-    if (current_full_compact_gc_count > last_full_compact_gc_count)
-    {
-        dprintf (2, ("attempted to trigger an ephemeral GC and got a full compacting GC"));
-        did_full_compact_gc = TRUE;
-    }
-    return did_full_compact_gc;
-}
-BOOL gc_heap::soh_try_fit (int gen_number,
-                           size_t size,
-                           alloc_context* acontext,
-                           uint32_t flags,
-                           int align_const,
-                           BOOL* commit_failed_p,
-                           BOOL* short_seg_end_p)
-{
-    BOOL can_allocate = TRUE;
-    if (short_seg_end_p)
-    {
-        *short_seg_end_p = FALSE;
-    }
-    can_allocate = a_fit_free_list_p (gen_number, size, acontext, flags, align_const);
-    if (!can_allocate)
-    {
-        if (short_seg_end_p)
-        {
-            *short_seg_end_p = short_on_end_of_seg (ephemeral_heap_segment);
-        }
-        if (!short_seg_end_p || !(*short_seg_end_p))
-        {
-#ifdef USE_REGIONS
-            while (ephemeral_heap_segment)
-#endif //USE_REGIONS
-            {
-                can_allocate = a_fit_segment_end_p (gen_number, ephemeral_heap_segment, size,
-                                                    acontext, flags, align_const, commit_failed_p);
-#ifdef USE_REGIONS
-                if (can_allocate)
-                {
-                    break;
-                }
-                dprintf (REGIONS_LOG, ("h%d fixing region %p end to alloc ptr: %p, alloc_allocated %p",
-                    heap_number, heap_segment_mem (ephemeral_heap_segment), acontext->alloc_ptr,
-                    alloc_allocated));
-                fix_allocation_context (acontext, TRUE, FALSE);
-                fix_youngest_allocation_area();
-                heap_segment* next_seg = heap_segment_next (ephemeral_heap_segment);
-                bool new_seg = false;
-                if (!next_seg)
-                {
-                    assert (ephemeral_heap_segment == generation_tail_region (generation_of (gen_number)));
-                    next_seg = get_new_region (gen_number);
-                    new_seg = true;
-                }
-                if (next_seg)
-                {
-                    dprintf (REGIONS_LOG, ("eph seg %p -> next %p",
-                        heap_segment_mem (ephemeral_heap_segment), heap_segment_mem (next_seg)));
-                    ephemeral_heap_segment = next_seg;
-                    if (new_seg)
-                    {
-                        GCToEEInterface::DiagAddNewRegion(
-                            heap_segment_gen_num (next_seg),
-                            heap_segment_mem (next_seg),
-                            heap_segment_allocated (next_seg),
-                            heap_segment_reserved (next_seg)
-                        );
-                    }
-                }
-                else
-                {
-                    *commit_failed_p = TRUE;
-                    dprintf (REGIONS_LOG, ("couldn't get a new ephemeral region"));
-                    return FALSE;
-                }
-                alloc_allocated = heap_segment_allocated (ephemeral_heap_segment);
-                dprintf (REGIONS_LOG, ("h%d alloc_allocated is now %p", heap_number, alloc_allocated));
-#endif //USE_REGIONS
-            }
-        }
-    }
-    return can_allocate;
-}
-allocation_state gc_heap::allocate_soh (int gen_number,
-                                          size_t size,
-                                          alloc_context* acontext,
-                                          uint32_t flags,
-                                          int align_const)
-{
-    enter_msl_status msl_status = msl_entered;
-#if defined (BACKGROUND_GC) && !defined (MULTIPLE_HEAPS)
-    if (gc_heap::background_running_p())
-    {
-        background_soh_alloc_count++;
-        if ((background_soh_alloc_count % bgc_alloc_spin_count) == 0)
-        {
-            add_saved_spinlock_info (false, me_release, mt_alloc_small, msl_status);
-            leave_spin_lock (&more_space_lock_soh);
-            bool cooperative_mode = enable_preemptive();
-            GCToOSInterface::Sleep (bgc_alloc_spin);
-            disable_preemptive (cooperative_mode);
-            msl_status = enter_spin_lock_msl (&more_space_lock_soh);
-            if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
-            add_saved_spinlock_info (false, me_acquire, mt_alloc_small, msl_status);
-        }
-        else
-        {
-        }
-    }
-#endif //BACKGROUND_GC && !MULTIPLE_HEAPS
-    gc_reason gr = reason_oos_soh;
-    oom_reason oom_r = oom_no_failure;
-    allocation_state soh_alloc_state = a_state_start;
-    while (1)
-    {
-        dprintf (3, ("[h%d]soh state is %s", heap_number, allocation_state_str[soh_alloc_state]));
-        switch (soh_alloc_state)
-        {
-            case a_state_can_allocate:
-            case a_state_cant_allocate:
-            {
-                goto exit;
-            }
-            case a_state_start:
-            {
-                soh_alloc_state = a_state_try_fit;
-                break;
-            }
-            case a_state_try_fit:
-            {
-                BOOL commit_failed_p = FALSE;
-                BOOL can_use_existing_p = FALSE;
-                can_use_existing_p = soh_try_fit (gen_number, size, acontext, flags,
-                                                  align_const, &commit_failed_p,
-                                                  NULL);
-                soh_alloc_state = (can_use_existing_p ?
-                                        a_state_can_allocate :
-                                        (commit_failed_p ?
-                                            a_state_trigger_full_compact_gc :
-                                            a_state_trigger_ephemeral_gc));
-                break;
-            }
-            case a_state_try_fit_after_bgc:
-            {
-                BOOL commit_failed_p = FALSE;
-                BOOL can_use_existing_p = FALSE;
-                BOOL short_seg_end_p = FALSE;
-                can_use_existing_p = soh_try_fit (gen_number, size, acontext, flags,
-                                                  align_const, &commit_failed_p,
-                                                  &short_seg_end_p);
-                soh_alloc_state = (can_use_existing_p ?
-                                        a_state_can_allocate :
-                                        (short_seg_end_p ?
-                                            a_state_trigger_2nd_ephemeral_gc :
-                                            a_state_trigger_full_compact_gc));
-                break;
-            }
-            case a_state_try_fit_after_cg:
-            {
-                BOOL commit_failed_p = FALSE;
-                BOOL can_use_existing_p = FALSE;
-                BOOL short_seg_end_p = FALSE;
-                can_use_existing_p = soh_try_fit (gen_number, size, acontext, flags,
-                                                  align_const, &commit_failed_p,
-                                                  &short_seg_end_p);
-                if (can_use_existing_p)
-                {
-                    soh_alloc_state = a_state_can_allocate;
-                }
-#ifdef MULTIPLE_HEAPS
-                else if (gen0_allocated_after_gc_p)
-                {
-                    soh_alloc_state = a_state_trigger_ephemeral_gc;
-                }
-#endif //MULTIPLE_HEAPS
-                else if (short_seg_end_p)
-                {
-                    soh_alloc_state = a_state_cant_allocate;
-                    oom_r = oom_budget;
-                }
-                else
-                {
-                    assert (commit_failed_p || heap_hard_limit);
-                    soh_alloc_state = a_state_cant_allocate;
-                    oom_r = oom_cant_commit;
-                }
-                break;
-            }
-            case a_state_check_and_wait_for_bgc:
-            {
-                BOOL bgc_in_progress_p = FALSE;
-                BOOL did_full_compacting_gc = FALSE;
-                bgc_in_progress_p = check_and_wait_for_bgc (awr_gen0_oos_bgc, &did_full_compacting_gc, false, &msl_status);
-                if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
-                soh_alloc_state = (did_full_compacting_gc ?
-                                        a_state_try_fit_after_cg :
-                                        a_state_try_fit_after_bgc);
-                break;
-            }
-            case a_state_trigger_ephemeral_gc:
-            {
-                BOOL commit_failed_p = FALSE;
-                BOOL can_use_existing_p = FALSE;
-                BOOL short_seg_end_p = FALSE;
-                BOOL bgc_in_progress_p = FALSE;
-                BOOL did_full_compacting_gc = FALSE;
-                did_full_compacting_gc = trigger_ephemeral_gc (gr, &msl_status);
-                if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
-                if (did_full_compacting_gc)
-                {
-                    soh_alloc_state = a_state_try_fit_after_cg;
-                }
-                else
-                {
-                    can_use_existing_p = soh_try_fit (gen_number, size, acontext, flags,
-                                                      align_const, &commit_failed_p,
-                                                      &short_seg_end_p);
-#ifdef BACKGROUND_GC
-                    bgc_in_progress_p = gc_heap::background_running_p();
-#endif //BACKGROUND_GC
-                    if (can_use_existing_p)
-                    {
-                        soh_alloc_state = a_state_can_allocate;
-                    }
-                    else
-                    {
-                        if (short_seg_end_p)
-                        {
-#ifndef USE_REGIONS
-                            if (should_expand_in_full_gc)
-                            {
-                                dprintf (2, ("gen1 GC wanted to expand!"));
-                                soh_alloc_state = a_state_trigger_full_compact_gc;
-                            }
-                            else
-#endif //!USE_REGIONS
-                            {
-                                soh_alloc_state = (bgc_in_progress_p ?
-                                                        a_state_check_and_wait_for_bgc :
-                                                        a_state_trigger_full_compact_gc);
-                            }
-                        }
-                        else if (commit_failed_p)
-                        {
-                            soh_alloc_state = a_state_trigger_full_compact_gc;
-                        }
-                        else
-                        {
-#ifdef MULTIPLE_HEAPS
-                            assert (gen0_allocated_after_gc_p);
-                            soh_alloc_state = a_state_trigger_ephemeral_gc;
-#else //MULTIPLE_HEAPS
-                            assert (!"shouldn't get here");
-#endif //MULTIPLE_HEAPS
-                        }
-                    }
-                }
-                break;
-            }
-            case a_state_trigger_2nd_ephemeral_gc:
-            {
-                BOOL commit_failed_p = FALSE;
-                BOOL can_use_existing_p = FALSE;
-                BOOL short_seg_end_p = FALSE;
-                BOOL did_full_compacting_gc = FALSE;
-                did_full_compacting_gc = trigger_ephemeral_gc (gr, &msl_status);
-                if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
-                if (did_full_compacting_gc)
-                {
-                    soh_alloc_state = a_state_try_fit_after_cg;
-                }
-                else
-                {
-                    can_use_existing_p = soh_try_fit (gen_number, size, acontext, flags,
-                                                      align_const, &commit_failed_p,
-                                                      &short_seg_end_p);
-                    if (short_seg_end_p || commit_failed_p)
-                    {
-                        soh_alloc_state = a_state_trigger_full_compact_gc;
-                    }
-                    else
-                    {
-                        assert (can_use_existing_p);
-                        soh_alloc_state = a_state_can_allocate;
-                    }
-                }
-                break;
-            }
-            case a_state_trigger_full_compact_gc:
-            {
-                if (fgn_maxgen_percent)
-                {
-                    dprintf (2, ("FGN: SOH doing last GC before we throw OOM"));
-                    send_full_gc_notification (max_generation, FALSE);
-                }
-                BOOL got_full_compacting_gc = FALSE;
-                got_full_compacting_gc = trigger_full_compact_gc (gr, &oom_r, false, &msl_status);
-                if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
-                soh_alloc_state = (got_full_compacting_gc ? a_state_try_fit_after_cg : a_state_cant_allocate);
-                break;
-            }
-            default:
-            {
-                assert (!"Invalid state!");
-                break;
-            }
-        }
-    }
-exit:
-    if (soh_alloc_state == a_state_cant_allocate)
-    {
-        assert (oom_r != oom_no_failure);
-        handle_oom (oom_r,
-                    size,
-                    heap_segment_allocated (ephemeral_heap_segment),
-                    heap_segment_reserved (ephemeral_heap_segment));
-        add_saved_spinlock_info (false, me_release, mt_alloc_small_cant, msl_entered);
-        leave_spin_lock (&more_space_lock_soh);
-    }
-    assert ((soh_alloc_state == a_state_can_allocate) ||
-            (soh_alloc_state == a_state_cant_allocate) ||
-            (soh_alloc_state == a_state_retry_allocate));
-    return soh_alloc_state;
-}
-#ifdef BACKGROUND_GC
-inline
-void gc_heap::bgc_track_uoh_alloc()
-{
-    if (current_c_gc_state == c_gc_state_planning)
-    {
-        Interlocked::Increment (&uoh_alloc_thread_count);
-        dprintf (3, ("h%d: inc lc: %d", heap_number, (int32_t)uoh_alloc_thread_count));
-    }
-}
-inline
-void gc_heap::bgc_untrack_uoh_alloc()
-{
-    if (current_c_gc_state == c_gc_state_planning)
-    {
-        Interlocked::Decrement (&uoh_alloc_thread_count);
-        dprintf (3, ("h%d: dec lc: %d", heap_number, (int32_t)uoh_alloc_thread_count));
-    }
-}
-int bgc_allocate_spin(size_t min_gc_size, size_t bgc_begin_size, size_t bgc_size_increased, size_t end_size)
-{
-    if ((bgc_begin_size + bgc_size_increased) < (min_gc_size * 10))
-    {
-        return 0;
-    }
-    if ((bgc_begin_size >= (2 * end_size)) || (bgc_size_increased >= bgc_begin_size))
-    {
-        if (bgc_begin_size >= (2 * end_size))
-        {
-            dprintf (3, ("alloc-ed too much before bgc started"));
-        }
-        else
-        {
-            dprintf (3, ("alloc-ed too much after bgc started"));
-        }
-        return -1;
-    }
-    else
-    {
-        return (int)(((float)bgc_size_increased / (float)bgc_begin_size) * 10);
-    }
-}
-int gc_heap::bgc_loh_allocate_spin()
-{
-    size_t min_gc_size = dd_min_size (dynamic_data_of (loh_generation));
-    size_t bgc_begin_size = bgc_begin_loh_size;
-    size_t bgc_size_increased = bgc_loh_size_increased;
-    size_t end_size = end_loh_size;
-    return bgc_allocate_spin(min_gc_size, bgc_begin_size, bgc_size_increased, end_size);
-}
-int gc_heap::bgc_poh_allocate_spin()
-{
-    size_t min_gc_size = dd_min_size (dynamic_data_of (poh_generation));
-    size_t bgc_begin_size = bgc_begin_poh_size;
-    size_t bgc_size_increased = bgc_poh_size_increased;
-    size_t end_size = end_poh_size;
-    return bgc_allocate_spin(min_gc_size, bgc_begin_size, bgc_size_increased, end_size);
-}
-#endif //BACKGROUND_GC
-size_t gc_heap::get_uoh_seg_size (size_t size)
-{
-    size_t default_seg_size =
-#ifdef USE_REGIONS
-        global_region_allocator.get_large_region_alignment();
-#else
-        min_uoh_segment_size;
-#endif //USE_REGIONS
-    size_t align_size =  default_seg_size;
-    int align_const = get_alignment_constant (FALSE);
-    size_t large_seg_size = align_on_page (
-        max (default_seg_size,
-            ((size + 2 * Align(min_obj_size, align_const) + OS_PAGE_SIZE +
-            align_size) / align_size * align_size)));
-    return large_seg_size;
-}
-BOOL gc_heap::uoh_get_new_seg (int gen_number,
-                               size_t size,
-                               BOOL* did_full_compact_gc,
-                               oom_reason* oom_r,
-                               enter_msl_status* msl_status)
-{
-    *did_full_compact_gc = FALSE;
-    size_t seg_size = get_uoh_seg_size (size);
-    heap_segment* new_seg = get_uoh_segment (gen_number, seg_size, did_full_compact_gc, msl_status);
-    if (*msl_status == msl_retry_different_heap) return FALSE;
-    if (new_seg && (gen_number == loh_generation))
-    {
-        loh_alloc_since_cg += seg_size;
-    }
-    else
-    {
-        *oom_r = oom_loh;
-    }
-    return (new_seg != 0);
-}
-BOOL gc_heap::retry_full_compact_gc (size_t size)
-{
-    size_t seg_size = get_uoh_seg_size (size);
-    if (loh_alloc_since_cg >= (2 * (uint64_t)seg_size))
-    {
-        return TRUE;
-    }
-#ifdef MULTIPLE_HEAPS
-    uint64_t total_alloc_size = 0;
-    for (int i = 0; i < n_heaps; i++)
-    {
-        total_alloc_size += g_heaps[i]->loh_alloc_since_cg;
-    }
-    if (total_alloc_size >= (2 * (uint64_t)seg_size))
-    {
-        return TRUE;
-    }
-#endif //MULTIPLE_HEAPS
-    return FALSE;
-}
-BOOL gc_heap::check_and_wait_for_bgc (alloc_wait_reason awr,
-                                      BOOL* did_full_compact_gc,
-                                      bool loh_p,
-                                      enter_msl_status* msl_status)
-{
-    BOOL bgc_in_progress = FALSE;
-    *did_full_compact_gc = FALSE;
-#ifdef BACKGROUND_GC
-    if (gc_heap::background_running_p())
-    {
-        bgc_in_progress = TRUE;
-        size_t last_full_compact_gc_count = get_full_compact_gc_count();
-        *msl_status = wait_for_background (awr, loh_p);
-        size_t current_full_compact_gc_count = get_full_compact_gc_count();
-        if (current_full_compact_gc_count > last_full_compact_gc_count)
-        {
-            *did_full_compact_gc = TRUE;
-        }
-    }
-#endif //BACKGROUND_GC
-    return bgc_in_progress;
-}
-BOOL gc_heap::uoh_try_fit (int gen_number,
-                           size_t size,
-                           alloc_context* acontext,
-                           uint32_t flags,
-                           int align_const,
-                           BOOL* commit_failed_p,
-                           oom_reason* oom_r)
-{
-    BOOL can_allocate = TRUE;
-    if (!a_fit_free_list_uoh_p (size, acontext, flags, align_const, gen_number))
-    {
-        can_allocate = uoh_a_fit_segment_end_p (gen_number, size,
-                                                acontext, flags, align_const,
-                                                commit_failed_p, oom_r);
-#ifdef BACKGROUND_GC
-        if (can_allocate && gc_heap::background_running_p())
-        {
-            if (gen_number == poh_generation)
-            {
-                bgc_poh_size_increased += size;
-            }
-            else
-            {
-                bgc_loh_size_increased += size;
-            }
-        }
-#endif //BACKGROUND_GC
-    }
-    return can_allocate;
-}
-BOOL gc_heap::trigger_full_compact_gc (gc_reason gr,
-                                       oom_reason* oom_r,
-                                       bool loh_p,
-                                       enter_msl_status* msl_status)
-{
-    BOOL did_full_compact_gc = FALSE;
-    size_t last_full_compact_gc_count = get_full_compact_gc_count();
-    if (!last_gc_before_oom)
-    {
-        last_gc_before_oom = TRUE;
-    }
-#ifdef BACKGROUND_GC
-    if (gc_heap::background_running_p())
-    {
-        *msl_status = wait_for_background (((gr == reason_oos_soh) ? awr_gen0_oos_bgc : awr_loh_oos_bgc), loh_p);
-        dprintf (2, ("waited for BGC - done"));
-        if (*msl_status == msl_retry_different_heap) return FALSE;
-    }
-#endif //BACKGROUND_GC
-    GCSpinLock* msl = loh_p ? &more_space_lock_uoh : &more_space_lock_soh;
-    size_t current_full_compact_gc_count = get_full_compact_gc_count();
-    if (current_full_compact_gc_count > last_full_compact_gc_count)
-    {
-        dprintf (3, ("a full compacting GC triggered while waiting for BGC (%zd->%zd)", last_full_compact_gc_count, current_full_compact_gc_count));
-        assert (current_full_compact_gc_count > last_full_compact_gc_count);
-        did_full_compact_gc = TRUE;
-        goto exit;
-    }
-    dprintf (3, ("h%d full GC", heap_number));
-    *msl_status = trigger_gc_for_alloc (max_generation, gr, msl, loh_p, mt_t_full_gc);
-    current_full_compact_gc_count = get_full_compact_gc_count();
-    if (current_full_compact_gc_count == last_full_compact_gc_count)
-    {
-        dprintf (2, ("attempted to trigger a full compacting GC but didn't get it"));
-        *oom_r = oom_unproductive_full_gc;
-    }
-    else
-    {
-        dprintf (3, ("h%d: T full compacting GC (%zd->%zd)",
-            heap_number,
-            last_full_compact_gc_count,
-            current_full_compact_gc_count));
-        assert (current_full_compact_gc_count > last_full_compact_gc_count);
-        did_full_compact_gc = TRUE;
-    }
-exit:
-    return did_full_compact_gc;
-}
-#ifdef RECORD_LOH_STATE
-void gc_heap::add_saved_loh_state (allocation_state loh_state_to_save, EEThreadId thread_id)
-{
-    if (loh_state_to_save != a_state_can_allocate)
-    {
-        last_loh_states[loh_state_index].alloc_state = loh_state_to_save;
-        last_loh_states[loh_state_index].gc_index = VolatileLoadWithoutBarrier (&settings.gc_index);
-        last_loh_states[loh_state_index].thread_id = thread_id;
-        loh_state_index++;
-        if (loh_state_index == max_saved_loh_states)
-        {
-            loh_state_index = 0;
-        }
-        assert (loh_state_index < max_saved_loh_states);
-    }
-}
-#endif //RECORD_LOH_STATE
-bool gc_heap::should_retry_other_heap (int gen_number, size_t size)
-{
-#ifdef MULTIPLE_HEAPS
-    if (heap_hard_limit)
-    {
-        size_t min_size = dd_min_size (g_heaps[0]->dynamic_data_of (gen_number));
-        size_t slack_space = max (commit_min_th, min_size);
-        bool retry_p = ((current_total_committed + size) < (heap_hard_limit - slack_space));
-        dprintf (1, ("%zd - %zd - total committed %zd - size %zd = %zd, %s",
-            heap_hard_limit, slack_space, current_total_committed, size,
-            (heap_hard_limit - slack_space - current_total_committed - size),
-            (retry_p ? "retry" : "no retry")));
-        return retry_p;
-    }
-    else
-#endif //MULTIPLE_HEAPS
-    {
-        return false;
-    }
-}
-allocation_state gc_heap::allocate_uoh (int gen_number,
-                                          size_t size,
-                                          alloc_context* acontext,
-                                          uint32_t flags,
-                                          int align_const)
-{
-    enter_msl_status msl_status = msl_entered;
-    allocation_state uoh_alloc_state = a_state_start;
-#ifdef SPINLOCK_HISTORY
-    current_uoh_alloc_state = uoh_alloc_state;
-#endif //SPINLOCK_HISTORY
-#ifdef RECORD_LOH_STATE
-    EEThreadId current_thread_id;
-    current_thread_id.SetToCurrentThread ();
-#endif //RECORD_LOH_STATE
-#ifdef BACKGROUND_GC
-    if (gc_heap::background_running_p())
-    {
-#ifdef BGC_SERVO_TUNING
-        bool planning_p = (current_c_gc_state == c_gc_state_planning);
-#endif //BGC_SERVO_TUNING
-        background_uoh_alloc_count++;
-        {
-#ifdef BGC_SERVO_TUNING
-            if (planning_p)
-            {
-                loh_a_bgc_planning += size;
-            }
-            else
-            {
-                loh_a_bgc_marking += size;
-            }
-#endif //BGC_SERVO_TUNING
-            int spin_for_allocation = (gen_number == loh_generation) ?
-                bgc_loh_allocate_spin() :
-                bgc_poh_allocate_spin();
-            if (spin_for_allocation > 0)
-            {
-                add_saved_spinlock_info (true, me_release, mt_alloc_large, msl_status);
-                leave_spin_lock (&more_space_lock_uoh);
-                bool cooperative_mode = enable_preemptive();
-                GCToOSInterface::YieldThread (spin_for_allocation);
-                disable_preemptive (cooperative_mode);
-                msl_status = enter_spin_lock_msl (&more_space_lock_uoh);
-                if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
-                add_saved_spinlock_info (true, me_acquire, mt_alloc_large, msl_status);
-                dprintf (SPINLOCK_LOG, ("[%d]spin Emsl uoh", heap_number));
-            }
-            else if (spin_for_allocation < 0)
-            {
-                msl_status = wait_for_background (awr_uoh_alloc_during_bgc, true);
-                check_msl_status ("uoh a_state_acquire_seg", size);
-            }
-        }
-    }
-#ifdef BGC_SERVO_TUNING
-    else
-    {
-        loh_a_no_bgc += size;
-    }
-#endif //BGC_SERVO_TUNING
-#endif //BACKGROUND_GC
-    gc_reason gr = reason_oos_loh;
-    generation* gen = generation_of (gen_number);
-    oom_reason oom_r = oom_no_failure;
-    size_t current_full_compact_gc_count = 0;
-    while (1)
-    {
-        dprintf (3, ("[h%d]loh state is %s", heap_number, allocation_state_str[uoh_alloc_state]));
-#ifdef SPINLOCK_HISTORY
-        current_uoh_alloc_state = uoh_alloc_state;
-#endif //SPINLOCK_HISTORY
-#ifdef RECORD_LOH_STATE
-        current_uoh_alloc_state = uoh_alloc_state;
-        add_saved_loh_state (uoh_alloc_state, current_thread_id);
-#endif //RECORD_LOH_STATE
-        switch (uoh_alloc_state)
-        {
-            case a_state_can_allocate:
-            case a_state_cant_allocate:
-            {
-                goto exit;
-            }
-            case a_state_start:
-            {
-                uoh_alloc_state = a_state_try_fit;
-                break;
-            }
-            case a_state_try_fit:
-            {
-                BOOL commit_failed_p = FALSE;
-                BOOL can_use_existing_p = FALSE;
-                can_use_existing_p = uoh_try_fit (gen_number, size, acontext, flags,
-                                                  align_const, &commit_failed_p, &oom_r);
-                uoh_alloc_state = (can_use_existing_p ?
-                                        a_state_can_allocate :
-                                        (commit_failed_p ?
-                                            a_state_trigger_full_compact_gc :
-                                            a_state_acquire_seg));
-                assert ((uoh_alloc_state == a_state_can_allocate) == (acontext->alloc_ptr != 0));
-                break;
-            }
-            case a_state_try_fit_new_seg:
-            {
-                BOOL commit_failed_p = FALSE;
-                BOOL can_use_existing_p = FALSE;
-                can_use_existing_p = uoh_try_fit (gen_number, size, acontext, flags,
-                                                  align_const, &commit_failed_p, &oom_r);
-                uoh_alloc_state = (can_use_existing_p ? a_state_can_allocate : a_state_try_fit);
-                assert ((uoh_alloc_state == a_state_can_allocate) == (acontext->alloc_ptr != 0));
-                break;
-            }
-            case a_state_try_fit_after_cg:
-            {
-                BOOL commit_failed_p = FALSE;
-                BOOL can_use_existing_p = FALSE;
-                can_use_existing_p = uoh_try_fit (gen_number, size, acontext, flags,
-                                                  align_const, &commit_failed_p, &oom_r);
-                uoh_alloc_state = (can_use_existing_p ?
-                                        a_state_can_allocate :
-                                        (commit_failed_p ?
-                                            a_state_cant_allocate :
-                                            a_state_acquire_seg_after_cg));
-                assert ((uoh_alloc_state == a_state_can_allocate) == (acontext->alloc_ptr != 0));
-                break;
-            }
-            case a_state_try_fit_after_bgc:
-            {
-                BOOL commit_failed_p = FALSE;
-                BOOL can_use_existing_p = FALSE;
-                can_use_existing_p = uoh_try_fit (gen_number, size, acontext, flags,
-                                                  align_const, &commit_failed_p, &oom_r);
-                uoh_alloc_state = (can_use_existing_p ?
-                                        a_state_can_allocate :
-                                        (commit_failed_p ?
-                                            a_state_trigger_full_compact_gc :
-                                            a_state_acquire_seg_after_bgc));
-                assert ((uoh_alloc_state == a_state_can_allocate) == (acontext->alloc_ptr != 0));
-                break;
-            }
-            case a_state_acquire_seg:
-            {
-                BOOL can_get_new_seg_p = FALSE;
-                BOOL did_full_compacting_gc = FALSE;
-                current_full_compact_gc_count = get_full_compact_gc_count();
-                can_get_new_seg_p = uoh_get_new_seg (gen_number, size, &did_full_compacting_gc, &oom_r, &msl_status);
-                check_msl_status ("uoh a_state_acquire_seg", size);
-                uoh_alloc_state = (can_get_new_seg_p ?
-                                        a_state_try_fit_new_seg :
-                                        (did_full_compacting_gc ?
-                                            a_state_check_retry_seg :
-                                            a_state_check_and_wait_for_bgc));
-                break;
-            }
-            case a_state_acquire_seg_after_cg:
-            {
-                BOOL can_get_new_seg_p = FALSE;
-                BOOL did_full_compacting_gc = FALSE;
-                current_full_compact_gc_count = get_full_compact_gc_count();
-                can_get_new_seg_p = uoh_get_new_seg (gen_number, size, &did_full_compacting_gc, &oom_r, &msl_status);
-                check_msl_status ("uoh a_state_acquire_seg_after_cg", size);
-                uoh_alloc_state = (can_get_new_seg_p ?
-                                        a_state_try_fit_after_cg :
-                                        a_state_check_retry_seg);
-                break;
-            }
-            case a_state_acquire_seg_after_bgc:
-            {
-                BOOL can_get_new_seg_p = FALSE;
-                BOOL did_full_compacting_gc = FALSE;
-                current_full_compact_gc_count = get_full_compact_gc_count();
-                can_get_new_seg_p = uoh_get_new_seg (gen_number, size, &did_full_compacting_gc, &oom_r, &msl_status);
-                check_msl_status ("uoh a_state_acquire_seg_after_bgc", size);
-                uoh_alloc_state = (can_get_new_seg_p ?
-                                        a_state_try_fit_new_seg :
-                                        (did_full_compacting_gc ?
-                                            a_state_check_retry_seg :
-                                            a_state_trigger_full_compact_gc));
-                assert ((uoh_alloc_state != a_state_cant_allocate) || (oom_r != oom_no_failure));
-                break;
-            }
-            case a_state_check_and_wait_for_bgc:
-            {
-                BOOL bgc_in_progress_p = FALSE;
-                BOOL did_full_compacting_gc = FALSE;
-                bgc_in_progress_p = check_and_wait_for_bgc (awr_loh_oos_bgc, &did_full_compacting_gc, true, &msl_status);
-                check_msl_status ("uoh a_state_check_and_wait_for_bgc", size);
-                uoh_alloc_state = (!bgc_in_progress_p ?
-                                        a_state_trigger_full_compact_gc :
-                                        (did_full_compacting_gc ?
-                                            a_state_try_fit_after_cg :
-                                            a_state_try_fit_after_bgc));
-                break;
-            }
-            case a_state_trigger_full_compact_gc:
-            {
-                if (fgn_maxgen_percent)
-                {
-                    dprintf (2, ("FGN: LOH doing last GC before we throw OOM"));
-                    send_full_gc_notification (max_generation, FALSE);
-                }
-                BOOL got_full_compacting_gc = FALSE;
-                got_full_compacting_gc = trigger_full_compact_gc (gr, &oom_r, true, &msl_status);
-                check_msl_status ("uoh a_state_trigger_full_compact_gc", size);
-                uoh_alloc_state = (got_full_compacting_gc ? a_state_try_fit_after_cg : a_state_cant_allocate);
-                assert ((uoh_alloc_state != a_state_cant_allocate) || (oom_r != oom_no_failure));
-                break;
-            }
-            case a_state_check_retry_seg:
-            {
-                BOOL should_retry_gc = retry_full_compact_gc (size);
-                BOOL should_retry_get_seg = FALSE;
-                if (!should_retry_gc)
-                {
-                    size_t last_full_compact_gc_count = current_full_compact_gc_count;
-                    current_full_compact_gc_count = get_full_compact_gc_count();
-                    if (current_full_compact_gc_count > last_full_compact_gc_count)
-                    {
-                        should_retry_get_seg = TRUE;
-                    }
-                }
-                uoh_alloc_state = (should_retry_gc ?
-                                        a_state_trigger_full_compact_gc :
-                                        (should_retry_get_seg ?
-                                            a_state_try_fit_after_cg :
-                                            a_state_cant_allocate));
-                assert ((uoh_alloc_state != a_state_cant_allocate) || (oom_r != oom_no_failure));
-                break;
-            }
-            default:
-            {
-                assert (!"Invalid state!");
-                break;
-            }
-        }
-    }
-exit:
-    if (uoh_alloc_state == a_state_cant_allocate)
-    {
-        assert (oom_r != oom_no_failure);
-        if ((oom_r != oom_cant_commit) && should_retry_other_heap (gen_number, size))
-        {
-            uoh_alloc_state = a_state_retry_allocate;
-        }
-        else
-        {
-            handle_oom (oom_r,
-                        size,
-                        0,
-                        0);
-        }
-        add_saved_spinlock_info (true, me_release, mt_alloc_large_cant, msl_entered);
-        leave_spin_lock (&more_space_lock_uoh);
-    }
-    assert ((uoh_alloc_state == a_state_can_allocate) ||
-            (uoh_alloc_state == a_state_cant_allocate) ||
-            (uoh_alloc_state == a_state_retry_allocate));
-    return uoh_alloc_state;
-}
-enter_msl_status gc_heap::trigger_gc_for_alloc (int gen_number, gc_reason gr,
-                                    GCSpinLock* msl, bool loh_p,
-                                    msl_take_state take_state)
-{
-    enter_msl_status msl_status = msl_entered;
-#ifdef BACKGROUND_GC
-    if (loh_p)
-    {
-#ifdef MULTIPLE_HEAPS
-#ifdef STRESS_DYNAMIC_HEAP_COUNT
-        uoh_msl_before_gc_p = true;
-#endif //STRESS_DYNAMIC_HEAP_COUNT
-        dprintf (5555, ("h%d uoh alloc before GC", heap_number));
-#endif //MULTIPLE_HEAPS
-        add_saved_spinlock_info (loh_p, me_release, take_state, msl_status);
-        leave_spin_lock (msl);
-    }
-#endif //BACKGROUND_GC
-#ifdef MULTIPLE_HEAPS
-    if (!loh_p)
-    {
-        add_saved_spinlock_info (loh_p, me_release, take_state, msl_status);
-        leave_spin_lock (msl);
-    }
-#endif //MULTIPLE_HEAPS
-    vm_heap->GarbageCollectGeneration (gen_number, gr);
-#ifdef MULTIPLE_HEAPS
-    if (!loh_p)
-    {
-        msl_status = enter_spin_lock_msl (msl);
-        add_saved_spinlock_info (loh_p, me_acquire, take_state, msl_status);
-    }
-#endif //MULTIPLE_HEAPS
-#ifdef BACKGROUND_GC
-    if (loh_p)
-    {
-        msl_status = enter_spin_lock_msl (msl);
-        add_saved_spinlock_info (loh_p, me_acquire, take_state, msl_status);
-    }
-#endif //BACKGROUND_GC
-    return msl_status;
-}
-inline
-bool gc_heap::update_alloc_info (int gen_number, size_t allocated_size, size_t* etw_allocation_amount)
-{
-    bool exceeded_p = false;
-    int oh_index = gen_to_oh (gen_number);
-    allocated_since_last_gc[oh_index] += allocated_size;
-    size_t& etw_allocated = etw_allocation_running_amount[oh_index];
-    etw_allocated += allocated_size;
-    if (etw_allocated > etw_allocation_tick)
-    {
-        *etw_allocation_amount = etw_allocated;
-        exceeded_p = true;
-        etw_allocated = 0;
-    }
-    return exceeded_p;
-}
-allocation_state gc_heap::try_allocate_more_space (alloc_context* acontext, size_t size,
-                                    uint32_t flags, int gen_number)
-{
-    enter_msl_status msl_status = msl_entered;
-    if (gc_heap::gc_started)
-    {
-        wait_for_gc_done();
-        return a_state_retry_allocate;
-    }
-    bool loh_p = (gen_number > 0);
-    GCSpinLock* msl = loh_p ? &more_space_lock_uoh : &more_space_lock_soh;
-#ifdef SYNCHRONIZATION_STATS
-    int64_t msl_acquire_start = GCToOSInterface::QueryPerformanceCounter();
-#endif //SYNCHRONIZATION_STATS
-    msl_status = enter_spin_lock_msl (msl);
-    check_msl_status ("TAMS", size);
-    add_saved_spinlock_info (loh_p, me_acquire, mt_try_alloc, msl_status);
-    dprintf (SPINLOCK_LOG, ("[%d]Emsl for alloc", heap_number));
-#ifdef SYNCHRONIZATION_STATS
-    int64_t msl_acquire = GCToOSInterface::QueryPerformanceCounter() - msl_acquire_start;
-    total_msl_acquire += msl_acquire;
-    num_msl_acquired++;
-    if (msl_acquire > 200)
-    {
-        num_high_msl_acquire++;
-    }
-    else
-    {
-        num_low_msl_acquire++;
-    }
-#endif //SYNCHRONIZATION_STATS
-    /*
-    if (gc_heap::gc_started)
-    {
-#ifdef SYNCHRONIZATION_STATS
-        good_suspension++;
-#endif //SYNCHRONIZATION_STATS
-        BOOL fStress = (g_pConfig->GetGCStressLevel() & GCConfig::GCSTRESS_TRANSITION) != 0;
-        if (!fStress)
-        {
-            wait_for_gc_done();
-#ifdef MULTIPLE_HEAPS
-            return -1;
-#endif //MULTIPLE_HEAPS
-        }
-    }
-    */
-    dprintf (3, ("requested to allocate %zd bytes on gen%d", size, gen_number));
-    int align_const = get_alignment_constant (gen_number <= max_generation);
-    if (fgn_maxgen_percent)
-    {
-        check_for_full_gc (gen_number, size);
-    }
-#ifdef BGC_SERVO_TUNING
-    if ((gen_number != 0) && bgc_tuning::should_trigger_bgc_loh())
-    {
-        msl_status = trigger_gc_for_alloc (max_generation, reason_bgc_tuning_loh, msl, loh_p, mt_try_servo_budget);
-        if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
-    }
-    else
-#endif //BGC_SERVO_TUNING
-    {
-        bool trigger_on_budget_loh_p =
-#ifdef BGC_SERVO_TUNING
-            !bgc_tuning::enable_fl_tuning;
-#else
-            true;
-#endif //BGC_SERVO_TUNING
-        bool check_budget_p = true;
-        if (gen_number != 0)
-        {
-            check_budget_p = trigger_on_budget_loh_p;
-        }
-        if (check_budget_p && !(new_allocation_allowed (gen_number)))
-        {
-            if (fgn_maxgen_percent && (gen_number == 0))
-            {
-                check_for_full_gc (gen_number, size);
-            }
-#ifdef BACKGROUND_GC
-            bool recheck_p = wait_for_bgc_high_memory (awr_gen0_alloc, loh_p, &msl_status);
-            if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
-#endif //BACKGROUND_GC
-#ifdef SYNCHRONIZATION_STATS
-            bad_suspension++;
-#endif //SYNCHRONIZATION_STATS
-            dprintf (2, ("h%d running out of budget on gen%d, gc", heap_number, gen_number));
-#ifdef BACKGROUND_GC
-            bool trigger_gc_p = true;
-            if (recheck_p)
-                trigger_gc_p = !(new_allocation_allowed (gen_number));
-            if (trigger_gc_p)
-#endif //BACKGROUND_GC
-            {
-                if (!settings.concurrent || (gen_number == 0))
-                {
-                    msl_status = trigger_gc_for_alloc (0, ((gen_number == 0) ? reason_alloc_soh : reason_alloc_loh),
-                                                       msl, loh_p, mt_try_budget);
-                    if (msl_status == msl_retry_different_heap) return a_state_retry_allocate;
-                }
-            }
-        }
-    }
-    allocation_state can_allocate = ((gen_number == 0) ?
-        allocate_soh (gen_number, size, acontext, flags, align_const) :
-        allocate_uoh (gen_number, size, acontext, flags, align_const));
-    return can_allocate;
-}
-#ifdef MULTIPLE_HEAPS
-void gc_heap::balance_heaps (alloc_context* acontext)
-{
-    if (acontext->alloc_count < 4)
-    {
-        if (acontext->alloc_count == 0)
-        {
-            int home_hp_num = heap_select::select_heap (acontext);
-            acontext->set_home_heap (GCHeap::GetHeap (home_hp_num));
-            gc_heap* hp = acontext->get_home_heap ()->pGenGCHeap;
-            acontext->set_alloc_heap (acontext->get_home_heap ());
-            hp->alloc_context_count++;
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-            uint16_t ideal_proc_no = 0;
-            GCToOSInterface::GetCurrentThreadIdealProc (&ideal_proc_no);
-            uint32_t proc_no = GCToOSInterface::GetCurrentProcessorNumber ();
-            add_to_hb_numa (proc_no, ideal_proc_no,
-                home_hp_num, false, true, false);
-            dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMPafter GC: 1st alloc on p%3d, h%d, ip: %d",
-                proc_no, home_hp_num, ideal_proc_no));
-#endif //HEAP_BALANCE_INSTRUMENTATION
-        }
-    }
-    else
-    {
-        BOOL set_home_heap = FALSE;
-        gc_heap* home_hp = NULL;
-        int proc_hp_num = 0;
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-        bool alloc_count_p = true;
-        bool multiple_procs_p = false;
-        bool set_ideal_p = false;
-        uint32_t proc_no = GCToOSInterface::GetCurrentProcessorNumber ();
-        uint32_t last_proc_no = proc_no;
-#endif //HEAP_BALANCE_INSTRUMENTATION
-        if (heap_select::can_find_heap_fast ())
-        {
-            assert (acontext->get_home_heap () != NULL);
-            home_hp = acontext->get_home_heap ()->pGenGCHeap;
-            proc_hp_num = heap_select::select_heap (acontext);
-            if (home_hp != gc_heap::g_heaps[proc_hp_num])
-            {
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-                alloc_count_p = false;
-#endif //HEAP_BALANCE_INSTRUMENTATION
-                set_home_heap = TRUE;
-            }
-            else if ((acontext->alloc_count & 15) == 0)
-                set_home_heap = TRUE;
-        }
-        else
-        {
-            if ((acontext->alloc_count & 3) == 0)
-                set_home_heap = TRUE;
-        }
-        if (set_home_heap)
-        {
-            /*
-                        if (n_heaps > MAX_SUPPORTED_CPUS)
-                        {
-                            acontext->home_heap = GCHeap::GetHeap( heap_select::select_heap(acontext));
-                            acontext->alloc_heap = acontext->home_heap;
-                        }
-                        else
-            */
-            {
-                gc_heap* org_hp = acontext->get_alloc_heap ()->pGenGCHeap;
-                int org_hp_num = org_hp->heap_number;
-                int final_alloc_hp_num = org_hp_num;
-                dynamic_data* dd = org_hp->dynamic_data_of (0);
-                ptrdiff_t org_size = dd_new_allocation (dd);
-                ptrdiff_t total_size = (ptrdiff_t)dd_desired_allocation (dd);
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-                dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMP[p%3d] ph h%3d, hh: %3d, ah: %3d (%dmb-%dmb), ac: %5d(%s)",
-                    proc_no, proc_hp_num, home_hp->heap_number,
-                    org_hp_num, (total_size / 1024 / 1024), (org_size / 1024 / 1024),
-                    acontext->alloc_count,
-                    ((proc_hp_num == home_hp->heap_number) ? "AC" : "H")));
-#endif //HEAP_BALANCE_INSTRUMENTATION
-                int org_alloc_context_count;
-                int max_alloc_context_count;
-                gc_heap* max_hp;
-                int max_hp_num = 0;
-                ptrdiff_t max_size;
-                size_t local_delta = max (((size_t)org_size >> 6), min_gen0_balance_delta);
-                size_t delta = local_delta;
-                if (((size_t)org_size + 2 * delta) >= (size_t)total_size)
-                {
-                    acontext->alloc_count++;
-                    return;
-                }
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-                proc_no = GCToOSInterface::GetCurrentProcessorNumber ();
-                if (proc_no != last_proc_no)
-                {
-                    dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMPSP: %d->%d", last_proc_no, proc_no));
-                    multiple_procs_p = true;
-                    last_proc_no = proc_no;
-                }
-                int new_home_hp_num = heap_select::proc_no_to_heap_no[proc_no];
-#else
-                int new_home_hp_num = heap_select::select_heap(acontext);
-#endif //HEAP_BALANCE_INSTRUMENTATION
-                gc_heap* new_home_hp = gc_heap::g_heaps[new_home_hp_num];
-                acontext->set_home_heap (new_home_hp->vm_heap);
-                int start, end, finish;
-                heap_select::get_heap_range_for_heap (new_home_hp_num, &start, &end);
-                finish = start + n_heaps;
-                do
-                {
-                    max_hp = org_hp;
-                    max_hp_num = org_hp_num;
-                    max_size = org_size + delta;
-                    org_alloc_context_count = org_hp->alloc_context_count;
-                    max_alloc_context_count = org_alloc_context_count;
-                    if (org_hp == new_home_hp)
-                        max_size = max_size + delta;
-                    if (max_alloc_context_count > 1)
-                        max_size /= max_alloc_context_count;
-                    if (org_hp != new_home_hp)
-                    {
-                        dd = new_home_hp->dynamic_data_of(0);
-                        ptrdiff_t size = dd_new_allocation(dd);
-                        size += delta * 2;
-                        int new_home_hp_alloc_context_count = new_home_hp->alloc_context_count;
-                        if (new_home_hp_alloc_context_count > 0)
-                            size /= (new_home_hp_alloc_context_count + 1);
-                        if (size > max_size)
-                        {
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-                            dprintf(HEAP_BALANCE_TEMP_LOG, ("TEMPorg h%d(%dmb), m h%d(%dmb)",
-                                org_hp_num, (max_size / 1024 / 1024),
-                                new_home_hp_num, (size / 1024 / 1024)));
-#endif //HEAP_BALANCE_INSTRUMENTATION
-                            max_hp = new_home_hp;
-                            max_size = size;
-                            max_hp_num = new_home_hp_num;
-                            max_alloc_context_count = new_home_hp_alloc_context_count;
-                        }
-                    }
-                    enum
-                    {
-                        LOCAL_NUMA_NODE,
-                        REMOTE_NUMA_NODE
-                    };
-                    for (int pass = LOCAL_NUMA_NODE; pass <= REMOTE_NUMA_NODE; pass++)
-                    {
-                        int count = end - start;
-                        int max_tries = min(count, 4);
-                        int heap_num = start + ((acontext->alloc_count >> 2) + new_home_hp_num) % count;
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-                        dprintf(HEAP_BALANCE_TEMP_LOG, ("TEMP starting at h%d (home_heap_num = %d, alloc_count = %d)", heap_num, new_home_hp_num, acontext->alloc_count));
-#endif //HEAP_BALANCE_INSTRUMENTATION
-                        for (int tries = max_tries; --tries >= 0; heap_num++)
-                        {
-                            if (heap_num >= end)
-                                heap_num -= count;
-                            while (heap_num >= n_heaps)
-                                heap_num -= n_heaps;
-                            assert (heap_num < n_heaps);
-                            gc_heap* hp = gc_heap::g_heaps[heap_num];
-                            dd = hp->dynamic_data_of(0);
-                            ptrdiff_t size = dd_new_allocation(dd);
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-                            dprintf(HEAP_BALANCE_TEMP_LOG, ("TEMP looking at h%d(%dmb)",
-                                heap_num, (size / 1024 / 1024)));
-#endif //HEAP_BALANCE_INSTRUMENTATION
-                            if (size <= max_size)
-                                continue;
-                            int hp_alloc_context_count = hp->alloc_context_count;
-                            if (hp_alloc_context_count > 0)
-                            {
-                                size /= (hp_alloc_context_count + 1);
-                            }
-                            if (size > max_size)
-                            {
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-                                dprintf(HEAP_BALANCE_TEMP_LOG, ("TEMPorg h%d(%dmb), m h%d(%dmb)",
-                                    org_hp_num, (max_size / 1024 / 1024),
-                                    hp->heap_number, (size / 1024 / 1024)));
-#endif //HEAP_BALANCE_INSTRUMENTATION
-                                max_hp = hp;
-                                max_size = size;
-                                max_hp_num = max_hp->heap_number;
-                                max_alloc_context_count = hp_alloc_context_count;
-                            }
-                        }
-                        if ((max_hp == org_hp) && (end < finish))
-                        {
-                            start = end; end = finish;
-                            delta = local_delta * 2; // Make it twice as hard to balance to remote nodes on NUMA.
-                        }
-                        else
-                        {
-                            break;
-                        }
-                    }
-                }
-                while (org_alloc_context_count != org_hp->alloc_context_count ||
-                       max_alloc_context_count != max_hp->alloc_context_count);
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-                uint16_t ideal_proc_no_before_set_ideal = 0;
-                GCToOSInterface::GetCurrentThreadIdealProc (&ideal_proc_no_before_set_ideal);
-#endif //HEAP_BALANCE_INSTRUMENTATION
-                if (max_hp != org_hp)
-                {
-                    final_alloc_hp_num = max_hp->heap_number;
-                    org_hp->alloc_context_count--;
-                    max_hp->alloc_context_count++;
-                    acontext->set_alloc_heap (GCHeap::GetHeap (final_alloc_hp_num));
-                    if (!gc_thread_no_affinitize_p)
-                    {
-                        uint16_t src_proc_no = heap_select::find_proc_no_from_heap_no (org_hp->heap_number);
-                        uint16_t dst_proc_no = heap_select::find_proc_no_from_heap_no (max_hp->heap_number);
-                        dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMPSW! h%d(p%d)->h%d(p%d)",
-                            org_hp_num, src_proc_no, final_alloc_hp_num, dst_proc_no));
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-                        int current_proc_no_before_set_ideal = GCToOSInterface::GetCurrentProcessorNumber ();
-                        if (current_proc_no_before_set_ideal != last_proc_no)
-                        {
-                            dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMPSPa: %d->%d", last_proc_no, current_proc_no_before_set_ideal));
-                            multiple_procs_p = true;
-                        }
-#endif //HEAP_BALANCE_INSTRUMENTATION
-                        if (!GCToOSInterface::SetCurrentThreadIdealAffinity (src_proc_no, dst_proc_no))
-                        {
-                            dprintf (HEAP_BALANCE_TEMP_LOG, ("TEMPFailed to set the ideal processor for heap %d %d->%d",
-                                org_hp->heap_number, (int)src_proc_no, (int)dst_proc_no));
-                        }
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-                        else
-                        {
-                            set_ideal_p = true;
-                        }
-#endif //HEAP_BALANCE_INSTRUMENTATION
-                    }
-                }
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-                add_to_hb_numa (proc_no, ideal_proc_no_before_set_ideal,
-                    final_alloc_hp_num, multiple_procs_p, alloc_count_p, set_ideal_p);
-#endif //HEAP_BALANCE_INSTRUMENTATION
-            }
-        }
-    }
-    acontext->alloc_count++;
-}
-ptrdiff_t gc_heap::get_balance_heaps_uoh_effective_budget (int generation_num)
-{
-#ifndef USE_REGIONS
-    if (heap_hard_limit)
-    {
-        const ptrdiff_t free_list_space = generation_free_list_space (generation_of (generation_num));
-        heap_segment* seg = generation_start_segment (generation_of (generation_num));
-        assert (heap_segment_next (seg) == nullptr);
-        const ptrdiff_t allocated = heap_segment_allocated (seg) - seg->mem;
-        return free_list_space - allocated;
-    }
-    else
-#endif // !USE_REGIONS
-    {
-        return dd_new_allocation (dynamic_data_of (generation_num));
-    }
-}
-gc_heap* gc_heap::balance_heaps_uoh (alloc_context* acontext, size_t alloc_size, int generation_num)
-{
-    const int home_hp_num = heap_select::select_heap(acontext);
-    dprintf (3, ("[h%d] LA: %zd", home_hp_num, alloc_size));
-    gc_heap* home_hp = GCHeap::GetHeap(home_hp_num)->pGenGCHeap;
-    dynamic_data* dd = home_hp->dynamic_data_of (generation_num);
-    const ptrdiff_t home_hp_size = home_hp->get_balance_heaps_uoh_effective_budget (generation_num);
-    size_t delta = dd_min_size (dd) / 2;
-    int start, end;
-    heap_select::get_heap_range_for_heap(home_hp_num, &start, &end);
-    const int finish = start + n_heaps;
-try_again:
-    gc_heap* max_hp = home_hp;
-    ptrdiff_t max_size = home_hp_size + delta;
-    dprintf (3, ("home hp: %d, max size: %zd",
-        home_hp_num,
-        max_size));
-    for (int i = start; i < end; i++)
-    {
-        gc_heap* hp = GCHeap::GetHeap(i%n_heaps)->pGenGCHeap;
-        const ptrdiff_t size = hp->get_balance_heaps_uoh_effective_budget (generation_num);
-        dprintf (3, ("hp: %d, size: %zd", hp->heap_number, size));
-        if (size > max_size)
-        {
-            max_hp = hp;
-            max_size = size;
-            dprintf (3, ("max hp: %d, max size: %zd",
-                max_hp->heap_number,
-                max_size));
-        }
-    }
-    if ((max_hp == home_hp) && (end < finish))
-    {
-        start = end; end = finish;
-        delta = dd_min_size (dd) * 3 / 2; // Make it harder to balance to remote nodes on NUMA.
-        goto try_again;
-    }
-    if (max_hp != home_hp)
-    {
-        dprintf (3, ("uoh: %d(%zd)->%d(%zd)",
-            home_hp->heap_number, dd_new_allocation (home_hp->dynamic_data_of (generation_num)),
-            max_hp->heap_number, dd_new_allocation (max_hp->dynamic_data_of (generation_num))));
-    }
-    return max_hp;
-}
-gc_heap* gc_heap::balance_heaps_uoh_hard_limit_retry (alloc_context* acontext, size_t alloc_size, int generation_num)
-{
-    assert (heap_hard_limit);
-#ifdef USE_REGIONS
-    return balance_heaps_uoh (acontext, alloc_size, generation_num);
-#else //USE_REGIONS
-    const int home_heap = heap_select::select_heap(acontext);
-    dprintf (3, ("[h%d] balance_heaps_loh_hard_limit_retry alloc_size: %zd", home_heap, alloc_size));
-    int start, end;
-    heap_select::get_heap_range_for_heap (home_heap, &start, &end);
-    const int finish = start + n_heaps;
-    gc_heap* max_hp = nullptr;
-    size_t max_end_of_seg_space = alloc_size; // Must be more than this much, or return NULL
-try_again:
-    {
-        for (int i = start; i < end; i++)
-        {
-            gc_heap* hp = GCHeap::GetHeap (i%n_heaps)->pGenGCHeap;
-            heap_segment* seg = generation_start_segment (hp->generation_of (generation_num));
-            assert (heap_segment_next (seg) == nullptr);
-            const size_t end_of_seg_space = heap_segment_reserved (seg) - heap_segment_allocated (seg);
-            if (end_of_seg_space >= max_end_of_seg_space)
-            {
-                dprintf (3, ("Switching heaps in hard_limit_retry! To: [h%d], New end_of_seg_space: %zd", hp->heap_number, end_of_seg_space));
-                max_end_of_seg_space = end_of_seg_space;
-                max_hp = hp;
-            }
-        }
-    }
-    if ((max_hp == nullptr) && (end < finish))
-    {
-        start = end; end = finish;
-        goto try_again;
-    }
-    return max_hp;
-#endif //USE_REGIONS
-}
-#endif //MULTIPLE_HEAPS
-BOOL gc_heap::allocate_more_space(alloc_context* acontext, size_t size,
-                                   uint32_t flags, int alloc_generation_number)
-{
-    allocation_state status = a_state_start;
-    int retry_count = 0;
-    gc_heap* saved_alloc_heap = 0;
-    do
-    {
-#ifdef MULTIPLE_HEAPS
-        if (alloc_generation_number == 0)
-        {
-            balance_heaps (acontext);
-            status = acontext->get_alloc_heap ()->pGenGCHeap->try_allocate_more_space (acontext, size, flags, alloc_generation_number);
-        }
-        else
-        {
-            uint64_t start_us = GetHighPrecisionTimeStamp ();
-            gc_heap* alloc_heap;
-            if (heap_hard_limit && (status == a_state_retry_allocate))
-            {
-                alloc_heap = balance_heaps_uoh_hard_limit_retry (acontext, size, alloc_generation_number);
-                if (alloc_heap == nullptr || (retry_count++ == UOH_ALLOCATION_RETRY_MAX_COUNT))
-                {
-                    return false;
-                }
-            }
-            else
-            {
-                alloc_heap = balance_heaps_uoh (acontext, size, alloc_generation_number);
-                dprintf (3, ("uoh alloc %Id on h%d", size, alloc_heap->heap_number));
-                saved_alloc_heap = alloc_heap;
-            }
-            bool alloced_on_retry = (status == a_state_retry_allocate);
-            status = alloc_heap->try_allocate_more_space (acontext, size, flags, alloc_generation_number);
-            dprintf (3, ("UOH h%d %Id returned from TAMS, s %d", alloc_heap->heap_number, size, status));
-            uint64_t end_us = GetHighPrecisionTimeStamp ();
-            if (status == a_state_retry_allocate)
-            {
-                dprintf (5555, ("UOH h%d alloc %Id retry!", alloc_heap->heap_number, size));
-            }
-            else
-            {
-                if (alloced_on_retry)
-                {
-                    dprintf (5555, ("UOH h%d allocated %Id on retry (%I64dus)", alloc_heap->heap_number, size, (end_us - start_us)));
-                }
-            }
-        }
-#else
-        status = try_allocate_more_space (acontext, size, flags, alloc_generation_number);
-#endif //MULTIPLE_HEAPS
-    }
-    while (status == a_state_retry_allocate);
-    return (status == a_state_can_allocate);
-}
-inline
-CObjectHeader* gc_heap::allocate (size_t jsize, alloc_context* acontext, uint32_t flags)
-{
-    size_t size = Align (jsize);
-    assert (size >= Align (min_obj_size));
-    {
-    retry:
-        uint8_t*  result = acontext->alloc_ptr;
-        acontext->alloc_ptr+=size;
-        if (acontext->alloc_ptr <= acontext->alloc_limit)
-        {
-            CObjectHeader* obj = (CObjectHeader*)result;
-            assert (obj != 0);
-            return obj;
-        }
-        else
-        {
-            acontext->alloc_ptr -= size;
-#ifdef _MSC_VER
-#pragma inline_depth(0)
-#endif //_MSC_VER
-            if (! allocate_more_space (acontext, size, flags, 0))
-                return 0;
-#ifdef _MSC_VER
-#pragma inline_depth(20)
-#endif //_MSC_VER
-            goto retry;
-        }
-    }
-}
-void  gc_heap::leave_allocation_segment (generation* gen)
-{
-    adjust_limit (0, 0, gen);
-}
-void gc_heap::init_free_and_plug()
-{
-#ifdef FREE_USAGE_STATS
-    int i = (settings.concurrent ? max_generation : 0);
-    for (; i <= settings.condemned_generation; i++)
-    {
-        generation* gen = generation_of (i);
-#ifdef DOUBLY_LINKED_FL
-        print_free_and_plug ("BGC");
-#else
-        memset (gen->gen_free_spaces, 0, sizeof (gen->gen_free_spaces));
-#endif //DOUBLY_LINKED_FL
-        memset (gen->gen_plugs, 0, sizeof (gen->gen_plugs));
-        memset (gen->gen_current_pinned_free_spaces, 0, sizeof (gen->gen_current_pinned_free_spaces));
-    }
-    if (settings.condemned_generation != max_generation)
-    {
-        for (int i = (settings.condemned_generation + 1); i <= max_generation; i++)
-        {
-            generation* gen = generation_of (i);
-            memset (gen->gen_plugs, 0, sizeof (gen->gen_plugs));
-        }
-    }
-#endif //FREE_USAGE_STATS
-}
-void gc_heap::print_free_and_plug (const char* msg)
-{
-#ifdef FREE_USAGE_STATS
-    int older_gen = ((settings.condemned_generation == max_generation) ? max_generation : (settings.condemned_generation + 1));
-    for (int i = 0; i <= older_gen; i++)
-    {
-        generation* gen = generation_of (i);
-        for (int j = 0; j < NUM_GEN_POWER2; j++)
-        {
-            if ((gen->gen_free_spaces[j] != 0) || (gen->gen_plugs[j] != 0))
-            {
-                dprintf (2, ("[%s][h%d][%s#%d]gen%d: 2^%d: F: %zd, P: %zd",
-                    msg,
-                    heap_number,
-                    (settings.concurrent ? "BGC" : "GC"),
-                    settings.gc_index,
-                    i,
-                    (j + 9), gen->gen_free_spaces[j], gen->gen_plugs[j]));
-            }
-        }
-    }
-#else
-    UNREFERENCED_PARAMETER(msg);
-#endif //FREE_USAGE_STATS
-}
-int gc_heap::find_bucket (size_t size)
-{
-    size_t sz = BASE_GEN_SIZE;
-    int i = 0;
-    for (; i < (NUM_GEN_POWER2 - 1); i++)
-    {
-        if (size < sz)
-        {
-            break;
-        }
-        sz = sz * 2;
-    }
-    return i;
-}
-void gc_heap::add_gen_plug (int gen_number, size_t plug_size)
-{
-#ifdef FREE_USAGE_STATS
-    dprintf (3, ("adding plug size %zd to gen%d", plug_size, gen_number));
-    generation* gen = generation_of (gen_number);
-    size_t sz = BASE_GEN_SIZE;
-    int i = find_bucket (plug_size);
-    (gen->gen_plugs[i])++;
-#else
-    UNREFERENCED_PARAMETER(gen_number);
-    UNREFERENCED_PARAMETER(plug_size);
-#endif //FREE_USAGE_STATS
-}
-void gc_heap::add_item_to_current_pinned_free (int gen_number, size_t free_size)
-{
-#ifdef FREE_USAGE_STATS
-    generation* gen = generation_of (gen_number);
-    size_t sz = BASE_GEN_SIZE;
-    int i = find_bucket (free_size);
-    (gen->gen_current_pinned_free_spaces[i])++;
-    generation_pinned_free_obj_space (gen) += free_size;
-    dprintf (3, ("left pin free %zd(2^%d) to gen%d, total %zd bytes (%zd)",
-        free_size, (i + 10), gen_number,
-        generation_pinned_free_obj_space (gen),
-        gen->gen_current_pinned_free_spaces[i]));
-#else
-    UNREFERENCED_PARAMETER(gen_number);
-    UNREFERENCED_PARAMETER(free_size);
-#endif //FREE_USAGE_STATS
-}
-void gc_heap::add_gen_free (int gen_number, size_t free_size)
-{
-#ifdef FREE_USAGE_STATS
-    dprintf (3, ("adding free size %zd to gen%d", free_size, gen_number));
-    if (free_size < min_free_list)
-        return;
-    generation* gen = generation_of (gen_number);
-    size_t sz = BASE_GEN_SIZE;
-    int i = find_bucket (free_size);
-    (gen->gen_free_spaces[i])++;
-    if (gen_number == max_generation)
-    {
-        dprintf (3, ("Mb b%d: f+ %zd (%zd)",
-            i, free_size, gen->gen_free_spaces[i]));
-    }
-#else
-    UNREFERENCED_PARAMETER(gen_number);
-    UNREFERENCED_PARAMETER(free_size);
-#endif //FREE_USAGE_STATS
-}
-void gc_heap::remove_gen_free (int gen_number, size_t free_size)
-{
-#ifdef FREE_USAGE_STATS
-    dprintf (3, ("removing free %zd from gen%d", free_size, gen_number));
-    if (free_size < min_free_list)
-        return;
-    generation* gen = generation_of (gen_number);
-    size_t sz = BASE_GEN_SIZE;
-    int i = find_bucket (free_size);
-    (gen->gen_free_spaces[i])--;
-    if (gen_number == max_generation)
-    {
-        dprintf (3, ("Mb b%d: f- %zd (%zd)",
-            i, free_size, gen->gen_free_spaces[i]));
-    }
-#else
-    UNREFERENCED_PARAMETER(gen_number);
-    UNREFERENCED_PARAMETER(free_size);
-#endif //FREE_USAGE_STATS
-}
-#ifdef DOUBLY_LINKED_FL
-BOOL gc_heap::should_set_bgc_mark_bit (uint8_t* o)
-{
-    if (!current_sweep_seg)
-    {
-        assert (current_bgc_state == bgc_not_in_process);
-        return FALSE;
-    }
-    if (in_range_for_segment (o, current_sweep_seg))
-    {
-        if ((o >= current_sweep_pos) && (o < heap_segment_background_allocated (current_sweep_seg)))
-        {
-#ifndef USE_REGIONS
-            if (current_sweep_seg == saved_sweep_ephemeral_seg)
-            {
-                return (o < saved_sweep_ephemeral_start);
-            }
-            else
-#endif //!USE_REGIONS
-            {
-                return TRUE;
-            }
-        }
-        else
-            return FALSE;
-    }
-    else
-    {
-        if ((o >= background_saved_lowest_address) && (o < background_saved_highest_address))
-        {
-            heap_segment* seg = seg_mapping_table_segment_of (o);
-            uint8_t* background_allocated = heap_segment_background_allocated (seg);
-            if (background_allocated == 0)
-                return FALSE;
-            else if (o >= background_allocated)
-                return FALSE;
-            else
-                return (!heap_segment_swept_p (seg));
-        }
-        else
-            return FALSE;
-    }
-}
-#endif //DOUBLY_LINKED_FL
-uint8_t* gc_heap::allocate_in_older_generation (generation* gen, size_t size,
-                                                int from_gen_number,
-                                                uint8_t* old_loc REQD_ALIGN_AND_OFFSET_DCL)
-{
-    size = Align (size);
-    assert (size >= Align (min_obj_size));
-    assert (from_gen_number < max_generation);
-    assert (from_gen_number >= 0);
-    assert (generation_of (from_gen_number + 1) == gen);
-#ifdef DOUBLY_LINKED_FL
-    BOOL consider_bgc_mark_p        = FALSE;
-    BOOL check_current_sweep_p      = FALSE;
-    BOOL check_saved_sweep_p        = FALSE;
-    BOOL try_added_list_p       = (gen->gen_num == max_generation);
-    BOOL record_free_list_allocated_p = ((gen->gen_num == max_generation) &&
-                                         (current_c_gc_state == c_gc_state_planning));
-#endif //DOUBLY_LINKED_FL
-    allocator* gen_allocator = generation_allocator (gen);
-    BOOL discard_p = gen_allocator->discard_if_no_fit_p ();
-#ifdef SHORT_PLUGS
-    int pad_in_front = ((old_loc != 0) && ((from_gen_number+1) != max_generation)) ? USE_PADDING_FRONT : 0;
-#else //SHORT_PLUGS
-    int pad_in_front = 0;
-#endif //SHORT_PLUGS
-    size_t real_size = size + Align (min_obj_size);
-    if (pad_in_front)
-        real_size += Align (min_obj_size);
-#ifdef RESPECT_LARGE_ALIGNMENT
-    real_size += switch_alignment_size (pad_in_front);
-#endif //RESPECT_LARGE_ALIGNMENT
-    if (! (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, generation_allocation_pointer (gen),
-                       generation_allocation_limit (gen), old_loc, USE_PADDING_TAIL | pad_in_front)))
-    {
-        for (unsigned int a_l_idx = gen_allocator->first_suitable_bucket(real_size * 2);
-             a_l_idx < gen_allocator->number_of_buckets(); a_l_idx++)
-        {
-            uint8_t* free_list = 0;
-            uint8_t* prev_free_item = 0;
-            BOOL use_undo_p = !discard_p;
-#ifdef DOUBLY_LINKED_FL
-            if (a_l_idx == 0)
-            {
-                use_undo_p = FALSE;
-            }
-            if (try_added_list_p)
-            {
-                free_list = gen_allocator->added_alloc_list_head_of (a_l_idx);
-                while (free_list != 0)
-                {
-                    dprintf (3, ("considering free list in added list%zx", (size_t)free_list));
-                    size_t free_list_size = unused_array_size (free_list);
-                    if (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, free_list, (free_list + free_list_size),
-                                    old_loc, USE_PADDING_TAIL | pad_in_front))
-                    {
-                        dprintf (4, ("F:%zx-%zd",
-                                    (size_t)free_list, free_list_size));
-                        gen_allocator->unlink_item_no_undo_added (a_l_idx, free_list, prev_free_item);
-                        generation_free_list_space (gen) -= free_list_size;
-                        assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
-                        remove_gen_free (gen->gen_num, free_list_size);
-                        if (record_free_list_allocated_p)
-                        {
-                            generation_set_bgc_mark_bit_p (gen) = should_set_bgc_mark_bit (free_list);
-                            dprintf (3333, ("SFA: %p->%p(%d)", free_list, (free_list + free_list_size),
-                                (generation_set_bgc_mark_bit_p (gen) ? 1 : 0)));
-                        }
-                        adjust_limit (free_list, free_list_size, gen);
-                        generation_allocate_end_seg_p (gen) = FALSE;
-                        goto finished;
-                    }
-                    else if (a_l_idx == 0)
-                    {
-                        dprintf (3, ("couldn't use this free area, discarding"));
-                        generation_free_obj_space (gen) += free_list_size;
-                        gen_allocator->unlink_item_no_undo_added (a_l_idx, free_list, prev_free_item);
-                        generation_free_list_space (gen) -= free_list_size;
-                        assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
-                        remove_gen_free (gen->gen_num, free_list_size);
-                    }
-                    else
-                    {
-                        prev_free_item = free_list;
-                    }
-                    free_list = free_list_slot (free_list);
-                }
-            }
-#endif //DOUBLY_LINKED_FL
-            free_list = gen_allocator->alloc_list_head_of (a_l_idx);
-            prev_free_item = 0;
-            while (free_list != 0)
-            {
-                dprintf (3, ("considering free list %zx", (size_t)free_list));
-                size_t free_list_size = unused_array_size (free_list);
-                if (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, free_list, (free_list + free_list_size),
-                                old_loc, USE_PADDING_TAIL | pad_in_front))
-                {
-                    dprintf (4, ("F:%zx-%zd",
-                                    (size_t)free_list, free_list_size));
-                    gen_allocator->unlink_item (a_l_idx, free_list, prev_free_item, use_undo_p);
-                    generation_free_list_space (gen) -= free_list_size;
-                    assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
-                    remove_gen_free (gen->gen_num, free_list_size);
-#ifdef DOUBLY_LINKED_FL
-                    if (!discard_p && !use_undo_p)
-                    {
-                        gen2_removed_no_undo += free_list_size;
-                        dprintf (3, ("h%d: remove with no undo %zd = %zd",
-                            heap_number, free_list_size, gen2_removed_no_undo));
-                    }
-                    if (record_free_list_allocated_p)
-                    {
-                        generation_set_bgc_mark_bit_p (gen) = should_set_bgc_mark_bit (free_list);
-                        dprintf (3333, ("SF: %p(%d)", free_list, (generation_set_bgc_mark_bit_p (gen) ? 1 : 0)));
-                    }
-#endif //DOUBLY_LINKED_FL
-                    adjust_limit (free_list, free_list_size, gen);
-                    generation_allocate_end_seg_p (gen) = FALSE;
-                    goto finished;
-                }
-                else if (discard_p || (a_l_idx == 0))
-                {
-                    dprintf (3, ("couldn't use this free area, discarding"));
-                    generation_free_obj_space (gen) += free_list_size;
-                    gen_allocator->unlink_item (a_l_idx, free_list, prev_free_item, FALSE);
-                    generation_free_list_space (gen) -= free_list_size;
-                    assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
-                    remove_gen_free (gen->gen_num, free_list_size);
-#ifdef DOUBLY_LINKED_FL
-                    if (!discard_p)
-                    {
-                        gen2_removed_no_undo += free_list_size;
-                        dprintf (3, ("h%d: b0 remove with no undo %zd = %zd",
-                            heap_number, free_list_size, gen2_removed_no_undo));
-                    }
-#endif //DOUBLY_LINKED_FL
-                }
-                else
-                {
-                    prev_free_item = free_list;
-                }
-                free_list = free_list_slot (free_list);
-            }
-        }
-#ifdef USE_REGIONS
-        heap_segment* seg = generation_allocation_segment (gen);
-        dprintf (3, ("end of seg, starting from alloc seg %p", heap_segment_mem (seg)));
-        assert (seg != ephemeral_heap_segment);
-        while (true)
-#else
-        heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-        if (seg != generation_allocation_segment (gen))
-        {
-            leave_allocation_segment (gen);
-            generation_allocation_segment (gen) = seg;
-        }
-        while (seg != ephemeral_heap_segment)
-#endif //USE_REGIONS
-        {
-            if (size_fit_p(size REQD_ALIGN_AND_OFFSET_ARG, heap_segment_plan_allocated (seg),
-                           heap_segment_committed (seg), old_loc, USE_PADDING_TAIL | pad_in_front))
-            {
-                adjust_limit (heap_segment_plan_allocated (seg),
-                              (heap_segment_committed (seg) - heap_segment_plan_allocated (seg)),
-                              gen);
-                generation_allocate_end_seg_p (gen) = TRUE;
-                heap_segment_plan_allocated (seg) =
-                    heap_segment_committed (seg);
-                dprintf (3, ("seg %p is used for end of seg alloc", heap_segment_mem (seg)));
-                goto finished;
-            }
-            else
-            {
-                if (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, heap_segment_plan_allocated (seg),
-                                heap_segment_reserved (seg), old_loc, USE_PADDING_TAIL | pad_in_front) &&
-                    grow_heap_segment (seg, heap_segment_plan_allocated (seg), old_loc, size, pad_in_front REQD_ALIGN_AND_OFFSET_ARG))
-                {
-                    adjust_limit (heap_segment_plan_allocated (seg),
-                                  (heap_segment_committed (seg) - heap_segment_plan_allocated (seg)),
-                                  gen);
-                    generation_allocate_end_seg_p (gen) = TRUE;
-                    heap_segment_plan_allocated (seg) =
-                        heap_segment_committed (seg);
-                    dprintf (3, ("seg %p is used for end of seg alloc after grow, %p",
-                        heap_segment_mem (seg), heap_segment_committed (seg)));
-                    goto finished;
-                }
-                else
-                {
-                    leave_allocation_segment (gen);
-                    heap_segment*   next_seg = heap_segment_next_rw (seg);
-#ifdef USE_REGIONS
-                    assert (next_seg != ephemeral_heap_segment);
-#endif //USE_REGIONS
-                    if (next_seg)
-                    {
-                        generation_allocation_segment (gen) = next_seg;
-                        generation_allocation_pointer (gen) = heap_segment_mem (next_seg);
-                        generation_allocation_limit (gen) = generation_allocation_pointer (gen);
-                        dprintf (3, ("alloc region advanced to %p", heap_segment_mem (next_seg)));
-                    }
-                    else
-                    {
-                        size = 0;
-                        goto finished;
-                    }
-                }
-            }
-            seg = generation_allocation_segment (gen);
-        }
-        size = 0;
-        goto finished;
-    }
-finished:
-    if (0 == size)
-    {
-        return 0;
-    }
-    else
-    {
-        uint8_t*  result = generation_allocation_pointer (gen);
-        size_t pad = 0;
-#ifdef SHORT_PLUGS
-        if ((pad_in_front & USE_PADDING_FRONT) &&
-            (((generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen))==0) ||
-             ((generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen))>=DESIRED_PLUG_LENGTH)))
-        {
-            pad = Align (min_obj_size);
-            set_plug_padded (old_loc);
-        }
-#endif //SHORT_PLUGS
-#ifdef FEATURE_STRUCTALIGN
-        _ASSERTE(!old_loc || alignmentOffset != 0);
-        _ASSERTE(old_loc || requiredAlignment == DATA_ALIGNMENT);
-        if (old_loc != 0)
-        {
-            size_t pad1 = ComputeStructAlignPad(result+pad, requiredAlignment, alignmentOffset);
-            set_node_aligninfo (old_loc, requiredAlignment, pad1);
-            pad += pad1;
-        }
-#else // FEATURE_STRUCTALIGN
-        if (!((old_loc == 0) || same_large_alignment_p (old_loc, result+pad)))
-        {
-            pad += switch_alignment_size (pad != 0);
-            set_node_realigned (old_loc);
-            dprintf (3, ("Allocation realignment old_loc: %zx, new_loc:%zx",
-                         (size_t)old_loc, (size_t)(result+pad)));
-            assert (same_large_alignment_p (result + pad, old_loc));
-        }
-#endif // FEATURE_STRUCTALIGN
-        dprintf (3, ("Allocate %zd bytes", size));
-        if ((old_loc == 0) || (pad != 0))
-        {
-            generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
-        }
-        generation_allocation_pointer (gen) += size + pad;
-        assert (generation_allocation_pointer (gen) <= generation_allocation_limit (gen));
-        generation_free_obj_space (gen) += pad;
-        if (generation_allocate_end_seg_p (gen))
-        {
-            generation_end_seg_allocated (gen) += size;
-        }
-        else
-        {
-#ifdef DOUBLY_LINKED_FL
-            if (generation_set_bgc_mark_bit_p (gen))
-            {
-                dprintf (2, ("IOM: %p(->%p(%zd) (%zx-%zx)", old_loc, result, pad,
-                        (size_t)(&mark_array [mark_word_of (result)]),
-                        (size_t)(mark_array [mark_word_of (result)])));
-                set_plug_bgc_mark_bit (old_loc);
-            }
-            generation_last_free_list_allocated (gen) = old_loc;
-#endif //DOUBLY_LINKED_FL
-            generation_free_list_allocated (gen) += size;
-        }
-        generation_allocation_size (gen) += size;
-        dprintf (3, ("aio: ptr: %p, limit: %p, sr: %p",
-            generation_allocation_pointer (gen), generation_allocation_limit (gen),
-            generation_allocation_context_start_region (gen)));
-        return (result + pad);
-    }
-}
-#ifndef USE_REGIONS
-void gc_heap::repair_allocation_in_expanded_heap (generation* consing_gen)
-{
-    int  gen_number = max_generation - 1;
-    while (gen_number>= 0)
-    {
-        generation* gen = generation_of (gen_number);
-        if (0 == generation_plan_allocation_start (gen))
-        {
-            realloc_plan_generation_start (gen, consing_gen);
-            assert (generation_plan_allocation_start (gen));
-        }
-        gen_number--;
-    }
-    size_t  size = (generation_allocation_limit (consing_gen) - generation_allocation_pointer (consing_gen));
-    heap_segment* seg = generation_allocation_segment (consing_gen);
-    if (generation_allocation_limit (consing_gen) == heap_segment_plan_allocated (seg))
-    {
-        if (size != 0)
-        {
-            heap_segment_plan_allocated (seg) = generation_allocation_pointer (consing_gen);
-        }
-    }
-    else
-    {
-        assert (settings.condemned_generation == max_generation);
-        uint8_t* first_address = generation_allocation_limit (consing_gen);
-        size_t mi = 0;
-        mark* m = 0;
-        while (mi != mark_stack_tos)
-        {
-            m = pinned_plug_of (mi);
-            if ((pinned_plug (m) == first_address))
-                break;
-            else
-                mi++;
-        }
-        assert (mi != mark_stack_tos);
-        pinned_len (m) = size;
-    }
-}
-uint8_t* gc_heap::allocate_in_expanded_heap (generation* gen,
-                                          size_t size,
-                                          BOOL& adjacentp,
-                                          uint8_t* old_loc,
-#ifdef SHORT_PLUGS
-                                          BOOL set_padding_on_saved_p,
-                                          mark* pinned_plug_entry,
-#endif //SHORT_PLUGS
-                                          BOOL consider_bestfit,
-                                          int active_new_gen_number
-                                          REQD_ALIGN_AND_OFFSET_DCL)
-{
-    dprintf (3, ("aie: P: %p, size: %zx", old_loc, size));
-    size = Align (size);
-    assert (size >= Align (min_obj_size));
-#ifdef SHORT_PLUGS
-    int pad_in_front = ((old_loc != 0) && (active_new_gen_number != max_generation)) ? USE_PADDING_FRONT : 0;
-#else //SHORT_PLUGS
-    int pad_in_front = 0;
-#endif //SHORT_PLUGS
-    if (consider_bestfit && use_bestfit)
-    {
-        assert (bestfit_seg);
-        dprintf (SEG_REUSE_LOG_1, ("reallocating 0x%p in expanded heap, size: %zd",
-                    old_loc, size));
-        return bestfit_seg->fit (old_loc,
-                                 size REQD_ALIGN_AND_OFFSET_ARG);
-    }
-    heap_segment* seg = generation_allocation_segment (gen);
-    if (! (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, generation_allocation_pointer (gen),
-                       generation_allocation_limit (gen), old_loc,
-                       ((generation_allocation_limit (gen) !=
-                          heap_segment_plan_allocated (seg))? USE_PADDING_TAIL : 0) | pad_in_front)))
-    {
-        dprintf (3, ("aie: can't fit: ptr: %p, limit: %p", generation_allocation_pointer (gen),
-            generation_allocation_limit (gen)));
-        adjacentp = FALSE;
-        uint8_t* first_address = (generation_allocation_limit (gen) ?
-                               generation_allocation_limit (gen) :
-                               heap_segment_mem (seg));
-        assert (in_range_for_segment (first_address, seg));
-        uint8_t* end_address   = heap_segment_reserved (seg);
-        dprintf (3, ("aie: first_addr: %p, gen alloc limit: %p, end_address: %p",
-            first_address, generation_allocation_limit (gen), end_address));
-        size_t mi = 0;
-        mark* m = 0;
-        if (heap_segment_allocated (seg) != heap_segment_mem (seg))
-        {
-            assert (settings.condemned_generation == max_generation);
-            while (mi != mark_stack_tos)
-            {
-                m = pinned_plug_of (mi);
-                if ((pinned_plug (m) >= first_address) && (pinned_plug (m) < end_address))
-                {
-                    dprintf (3, ("aie: found pin: %p", pinned_plug (m)));
-                    break;
-                }
-                else
-                    mi++;
-            }
-            if (mi != mark_stack_tos)
-            {
-                size_t  hsize = (generation_allocation_limit (gen) - generation_allocation_pointer (gen));
-                {
-                    dprintf(3,("gc filling up hole"));
-                    ptrdiff_t mi1 = (ptrdiff_t)mi;
-                    while ((mi1 >= 0) &&
-                           (pinned_plug (pinned_plug_of(mi1)) != generation_allocation_limit (gen)))
-                    {
-                        dprintf (3, ("aie: checking pin %p", pinned_plug (pinned_plug_of(mi1))));
-                        mi1--;
-                    }
-                    if (mi1 >= 0)
-                    {
-                        size_t saved_pinned_len = pinned_len (pinned_plug_of(mi1));
-                        pinned_len (pinned_plug_of(mi1)) = hsize;
-                        dprintf (3, ("changing %p len %zx->%zx",
-                            pinned_plug (pinned_plug_of(mi1)),
-                            saved_pinned_len, pinned_len (pinned_plug_of(mi1))));
-                    }
-                }
-            }
-        }
-        else
-        {
-            assert (generation_allocation_limit (gen) ==
-                    generation_allocation_pointer (gen));
-            mi = mark_stack_tos;
-        }
-        while ((mi != mark_stack_tos) && in_range_for_segment (pinned_plug (m), seg))
-        {
-            size_t len = pinned_len (m);
-            uint8_t*  free_list = (pinned_plug (m) - len);
-            dprintf (3, ("aie: testing free item: %p->%p(%zx)",
-                free_list, (free_list + len), len));
-            if (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, free_list, (free_list + len), old_loc, USE_PADDING_TAIL | pad_in_front))
-            {
-                dprintf (3, ("aie: Found adequate unused area: %zx, size: %zd",
-                            (size_t)free_list, len));
-                {
-                    generation_allocation_pointer (gen) = free_list;
-                    generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
-                    generation_allocation_limit (gen) = (free_list + len);
-                }
-                goto allocate_in_free;
-            }
-            mi++;
-            m = pinned_plug_of (mi);
-        }
-        generation_allocation_pointer (gen) = heap_segment_plan_allocated (seg);
-        generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
-        heap_segment_plan_allocated (seg) = heap_segment_committed (seg);
-        generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
-        dprintf (3, ("aie: switching to end of seg: %p->%p(%zx)",
-            generation_allocation_pointer (gen), generation_allocation_limit (gen),
-            (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
-        if (!size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, generation_allocation_pointer (gen),
-                         generation_allocation_limit (gen), old_loc, USE_PADDING_TAIL | pad_in_front))
-        {
-            dprintf (3, ("aie: ptr: %p, limit: %p, can't alloc", generation_allocation_pointer (gen),
-                generation_allocation_limit (gen)));
-            assert (!"Can't allocate if no free space");
-            return 0;
-        }
-    }
-    else
-    {
-        adjacentp = TRUE;
-    }
-allocate_in_free:
-    {
-        uint8_t*  result = generation_allocation_pointer (gen);
-        size_t pad = 0;
-#ifdef SHORT_PLUGS
-        if ((pad_in_front & USE_PADDING_FRONT) &&
-            (((generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen))==0) ||
-             ((generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen))>=DESIRED_PLUG_LENGTH)))
-        {
-            pad = Align (min_obj_size);
-            set_padding_in_expand (old_loc, set_padding_on_saved_p, pinned_plug_entry);
-        }
-#endif //SHORT_PLUGS
-#ifdef FEATURE_STRUCTALIGN
-        _ASSERTE(!old_loc || alignmentOffset != 0);
-        _ASSERTE(old_loc || requiredAlignment == DATA_ALIGNMENT);
-        if (old_loc != 0)
-        {
-            size_t pad1 = ComputeStructAlignPad(result+pad, requiredAlignment, alignmentOffset);
-            set_node_aligninfo (old_loc, requiredAlignment, pad1);
-            pad += pad1;
-            adjacentp = FALSE;
-        }
-#else // FEATURE_STRUCTALIGN
-        if (!((old_loc == 0) || same_large_alignment_p (old_loc, result+pad)))
-        {
-            pad += switch_alignment_size (pad != 0);
-            set_node_realigned (old_loc);
-            dprintf (3, ("Allocation realignment old_loc: %zx, new_loc:%zx",
-                         (size_t)old_loc, (size_t)(result+pad)));
-            assert (same_large_alignment_p (result + pad, old_loc));
-            adjacentp = FALSE;
-        }
-#endif // FEATURE_STRUCTALIGN
-        if ((old_loc == 0) || (pad != 0))
-        {
-            generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
-        }
-        generation_allocation_pointer (gen) += size + pad;
-        assert (generation_allocation_pointer (gen) <= generation_allocation_limit (gen));
-        dprintf (3, ("Allocated in expanded heap %zx:%zd", (size_t)(result+pad), size));
-        dprintf (3, ("aie: ptr: %p, limit: %p, sr: %p",
-            generation_allocation_pointer (gen), generation_allocation_limit (gen),
-            generation_allocation_context_start_region (gen)));
-        return result + pad;
-    }
-}
-generation*  gc_heap::ensure_ephemeral_heap_segment (generation* consing_gen)
-{
-    heap_segment* seg = generation_allocation_segment (consing_gen);
-    if (seg != ephemeral_heap_segment)
-    {
-        assert (generation_allocation_pointer (consing_gen)>= heap_segment_mem (seg));
-        assert (generation_allocation_pointer (consing_gen)<= heap_segment_committed (seg));
-        heap_segment_plan_allocated (seg) = generation_allocation_pointer (consing_gen);
-        generation* new_consing_gen = generation_of (max_generation - 1);
-        generation_allocation_pointer (new_consing_gen) =
-                heap_segment_mem (ephemeral_heap_segment);
-        generation_allocation_limit (new_consing_gen) =
-            generation_allocation_pointer (new_consing_gen);
-        generation_allocation_context_start_region (new_consing_gen) =
-            generation_allocation_pointer (new_consing_gen);
-        generation_allocation_segment (new_consing_gen) = ephemeral_heap_segment;
-        return new_consing_gen;
-    }
-    else
-        return consing_gen;
-}
-#endif //!USE_REGIONS
-inline
-void gc_heap::init_alloc_info (generation* gen, heap_segment* seg)
-{
-    generation_allocation_segment (gen) = seg;
-    generation_allocation_pointer (gen) = heap_segment_mem (seg);
-    generation_allocation_limit (gen) = generation_allocation_pointer (gen);
-    generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
-}
-inline
-heap_segment* gc_heap::get_next_alloc_seg (generation* gen)
-{
-#ifdef USE_REGIONS
-    heap_segment* saved_region = generation_allocation_segment (gen);
-    int gen_num = heap_segment_gen_num (saved_region);
-    heap_segment* region = saved_region;
-    while (1)
-    {
-        region = heap_segment_non_sip (region);
-        if (region)
-        {
-            break;
-        }
-        else
-        {
-            if (gen_num > 0)
-            {
-                gen_num--;
-                region = generation_start_segment (generation_of (gen_num));
-                dprintf (REGIONS_LOG, ("h%d next alloc region: switching to next gen%d start %zx(%p)",
-                    heap_number, heap_segment_gen_num (region), (size_t)region,
-                    heap_segment_mem (region)));
-            }
-            else
-            {
-                assert (!"ran out regions when getting the next alloc seg!");
-            }
-        }
-    }
-    if (region != saved_region)
-    {
-        dprintf (REGIONS_LOG, ("init allocate region for gen%d to %p(%d)",
-            gen->gen_num, heap_segment_mem (region), heap_segment_gen_num (region)));
-        init_alloc_info (gen, region);
-    }
-    return region;
-#else
-    return generation_allocation_segment (gen);
-#endif //USE_REGIONS
-}
-uint8_t* gc_heap::allocate_in_condemned_generations (generation* gen,
-                                                  size_t size,
-                                                  int from_gen_number,
-#ifdef SHORT_PLUGS
-                                                  BOOL* convert_to_pinned_p,
-                                                  uint8_t* next_pinned_plug,
-                                                  heap_segment* current_seg,
-#endif //SHORT_PLUGS
-                                                  uint8_t* old_loc
-                                                  REQD_ALIGN_AND_OFFSET_DCL)
-{
-#ifndef USE_REGIONS
-    if (settings.promotion)
-    {
-        assert (generation_plan_allocation_start (youngest_generation) == 0);
-    }
-#endif //!USE_REGIONS
-    size = Align (size);
-    assert (size >= Align (min_obj_size));
-    int to_gen_number = from_gen_number;
-    if (from_gen_number != (int)max_generation)
-    {
-        to_gen_number = from_gen_number + (settings.promotion ? 1 : 0);
-    }
-    dprintf (3, ("aic gen%d: s: %zd, ac: %p-%p", gen->gen_num, size,
-            generation_allocation_pointer (gen), generation_allocation_limit (gen)));
-#ifdef SHORT_PLUGS
-    int pad_in_front = ((old_loc != 0) && (to_gen_number != max_generation)) ? USE_PADDING_FRONT : 0;
-#else //SHORT_PLUGS
-    int pad_in_front = 0;
-#endif //SHORT_PLUGS
-    if ((from_gen_number != -1) && (from_gen_number != (int)max_generation) && settings.promotion)
-    {
-        generation_condemned_allocated (generation_of (from_gen_number + (settings.promotion ? 1 : 0))) += size;
-        generation_allocation_size (generation_of (from_gen_number + (settings.promotion ? 1 : 0))) += size;
-    }
-retry:
-    {
-        heap_segment* seg = get_next_alloc_seg (gen);
-        if (! (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, generation_allocation_pointer (gen),
-                           generation_allocation_limit (gen), old_loc,
-                           ((generation_allocation_limit (gen) != heap_segment_plan_allocated (seg))?USE_PADDING_TAIL:0)|pad_in_front)))
-        {
-            if ((! (pinned_plug_que_empty_p()) &&
-                 (generation_allocation_limit (gen) ==
-                  pinned_plug (oldest_pin()))))
-            {
-                size_t entry = deque_pinned_plug();
-                mark* pinned_plug_entry = pinned_plug_of (entry);
-                size_t len = pinned_len (pinned_plug_entry);
-                uint8_t* plug = pinned_plug (pinned_plug_entry);
-                set_new_pin_info (pinned_plug_entry, generation_allocation_pointer (gen));
-#ifdef USE_REGIONS
-                if (to_gen_number == 0)
-                {
-                    update_planned_gen0_free_space (pinned_len (pinned_plug_entry), plug);
-                    dprintf (REGIONS_LOG, ("aic: not promotion, gen0 added free space %zd at %p",
-                                    pinned_len (pinned_plug_entry), plug));
-                }
-#endif //USE_REGIONS
-#ifdef FREE_USAGE_STATS
-                generation_allocated_in_pinned_free (gen) += generation_allocated_since_last_pin (gen);
-                dprintf (3, ("allocated %zd so far within pin %zx, total->%zd",
-                    generation_allocated_since_last_pin (gen),
-                    plug,
-                    generation_allocated_in_pinned_free (gen)));
-                generation_allocated_since_last_pin (gen) = 0;
-                add_item_to_current_pinned_free (gen->gen_num, pinned_len (pinned_plug_of (entry)));
-#endif //FREE_USAGE_STATS
-                dprintf (3, ("mark stack bos: %zd, tos: %zd, aic: p %p len: %zx->%zx",
-                    mark_stack_bos, mark_stack_tos, plug, len, pinned_len (pinned_plug_of (entry))));
-                assert(mark_stack_array[entry].len == 0 ||
-                       mark_stack_array[entry].len >= Align(min_obj_size));
-                generation_allocation_pointer (gen) = plug + len;
-                generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
-                generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
-                set_allocator_next_pin (gen);
-                int frgn = object_gennum (plug);
-                if ((frgn != (int)max_generation) && settings.promotion)
-                {
-                    generation_pinned_allocation_sweep_size (generation_of (frgn + 1)) += len;
-#ifdef USE_REGIONS
-                    int togn = (in_range_for_segment (plug, seg) ? to_gen_number : object_gennum_plan (plug));
-#else
-                    int togn = object_gennum_plan (plug);
-#endif //USE_REGIONS
-                    if (frgn < togn)
-                    {
-                        generation_pinned_allocation_compact_size (generation_of (togn)) += len;
-                    }
-                }
-                goto retry;
-            }
-            if (generation_allocation_limit (gen) != heap_segment_plan_allocated (seg))
-            {
-                generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
-                dprintf (3, ("changed limit to plan alloc: %p", generation_allocation_limit (gen)));
-            }
-            else
-            {
-                if (heap_segment_plan_allocated (seg) != heap_segment_committed (seg))
-                {
-                    heap_segment_plan_allocated (seg) = heap_segment_committed (seg);
-                    generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
-                    dprintf (3, ("changed limit to commit: %p", generation_allocation_limit (gen)));
-                }
-                else
-                {
-#if !defined(RESPECT_LARGE_ALIGNMENT) && !defined(USE_REGIONS)
-                    assert (gen != youngest_generation);
-#endif //!RESPECT_LARGE_ALIGNMENT && !USE_REGIONS
-                    if (size_fit_p (size REQD_ALIGN_AND_OFFSET_ARG, generation_allocation_pointer (gen),
-                                    heap_segment_reserved (seg), old_loc, USE_PADDING_TAIL | pad_in_front) &&
-                        (grow_heap_segment (seg, generation_allocation_pointer (gen), old_loc,
-                                            size, pad_in_front REQD_ALIGN_AND_OFFSET_ARG)))
-                    {
-                        dprintf (3, ("Expanded segment allocation by committing more memory"));
-                        heap_segment_plan_allocated (seg) = heap_segment_committed (seg);
-                        generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
-                    }
-                    else
-                    {
-                        heap_segment*   next_seg = heap_segment_next (seg);
-                        dprintf (REGIONS_LOG, ("aic next: %p(%p,%p) -> %p(%p,%p)",
-                            heap_segment_mem (seg), heap_segment_allocated (seg), heap_segment_plan_allocated (seg),
-                            (next_seg ? heap_segment_mem (next_seg) : 0),
-                            (next_seg ? heap_segment_allocated (next_seg) : 0),
-                            (next_seg ? heap_segment_plan_allocated (next_seg) : 0)));
-                        assert (generation_allocation_pointer (gen)>=
-                                heap_segment_mem (seg));
-                        if (!pinned_plug_que_empty_p() &&
-                            ((pinned_plug (oldest_pin()) <
-                              heap_segment_allocated (seg)) &&
-                             (pinned_plug (oldest_pin()) >=
-                              generation_allocation_pointer (gen))))
-                        {
-                            LOG((LF_GC, LL_INFO10, "remaining pinned plug %zx while leaving segment on allocation",
-                                         pinned_plug (oldest_pin())));
-                            FATAL_GC_ERROR();
-                        }
-                        assert (generation_allocation_pointer (gen)>=
-                                heap_segment_mem (seg));
-                        assert (generation_allocation_pointer (gen)<=
-                                heap_segment_committed (seg));
-                        heap_segment_plan_allocated (seg) = generation_allocation_pointer (gen);
-#ifdef USE_REGIONS
-                        set_region_plan_gen_num (seg, to_gen_number);
-                        if ((next_seg == 0) && (heap_segment_gen_num (seg) > 0))
-                        {
-                            next_seg = generation_start_segment (generation_of (heap_segment_gen_num (seg) - 1));
-                            dprintf (REGIONS_LOG, ("h%d aic: switching to next gen%d start %zx(%p)",
-                                heap_number, heap_segment_gen_num (next_seg), (size_t)next_seg,
-                                heap_segment_mem (next_seg)));
-                        }
-#endif //USE_REGIONS
-                        if (next_seg)
-                        {
-                            init_alloc_info (gen, next_seg);
-                        }
-                        else
-                        {
-#ifdef USE_REGIONS
-                            assert (!"should not happen for regions!");
-#else
-                            return 0; //should only happen during allocation of generation 0 gap
-#endif //USE_REGIONS
-                        }
-                    }
-                }
-            }
-            set_allocator_next_pin (gen);
-            goto retry;
-        }
-    }
-    {
-        assert (generation_allocation_pointer (gen)>=
-                heap_segment_mem (generation_allocation_segment (gen)));
-        uint8_t* result = generation_allocation_pointer (gen);
-        size_t pad = 0;
-#ifdef SHORT_PLUGS
-        if ((pad_in_front & USE_PADDING_FRONT) &&
-            (((generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen))==0) ||
-             ((generation_allocation_pointer (gen) - generation_allocation_context_start_region (gen))>=DESIRED_PLUG_LENGTH)))
-        {
-            ptrdiff_t dist = old_loc - result;
-            if (dist == 0)
-            {
-                dprintf (3, ("old alloc: %p, same as new alloc, not padding", old_loc));
-                pad = 0;
-            }
-            else
-            {
-                if ((dist > 0) && (dist < (ptrdiff_t)Align (min_obj_size)))
-                {
-                    dprintf (1, ("old alloc: %p, only %zd bytes > new alloc! Shouldn't happen", old_loc, dist));
-                    FATAL_GC_ERROR();
-                }
-                pad = Align (min_obj_size);
-                set_plug_padded (old_loc);
-            }
-        }
-#endif //SHORT_PLUGS
-#ifdef FEATURE_STRUCTALIGN
-        _ASSERTE(!old_loc || alignmentOffset != 0);
-        _ASSERTE(old_loc || requiredAlignment == DATA_ALIGNMENT);
-        if ((old_loc != 0))
-        {
-            size_t pad1 = ComputeStructAlignPad(result+pad, requiredAlignment, alignmentOffset);
-            set_node_aligninfo (old_loc, requiredAlignment, pad1);
-            pad += pad1;
-        }
-#else // FEATURE_STRUCTALIGN
-        if (!((old_loc == 0) || same_large_alignment_p (old_loc, result+pad)))
-        {
-            pad += switch_alignment_size (pad != 0);
-            set_node_realigned(old_loc);
-            dprintf (3, ("Allocation realignment old_loc: %zx, new_loc:%zx",
-                         (size_t)old_loc, (size_t)(result+pad)));
-            assert (same_large_alignment_p (result + pad, old_loc));
-        }
-#endif // FEATURE_STRUCTALIGN
-#ifdef SHORT_PLUGS
-        if ((next_pinned_plug != 0) && (pad != 0) && (generation_allocation_segment (gen) == current_seg))
-        {
-            assert (old_loc != 0);
-            ptrdiff_t dist_to_next_pin = (ptrdiff_t)(next_pinned_plug - (generation_allocation_pointer (gen) + size + pad));
-            assert (dist_to_next_pin >= 0);
-            if ((dist_to_next_pin >= 0) && (dist_to_next_pin < (ptrdiff_t)Align (min_obj_size)))
-            {
-                dprintf (3, ("%p->(%p,%p),%p(%zx)(%zx),NP->PP",
-                    old_loc,
-                    generation_allocation_pointer (gen),
-                    generation_allocation_limit (gen),
-                    next_pinned_plug,
-                    size,
-                    dist_to_next_pin));
-                clear_plug_padded (old_loc);
-                pad = 0;
-                *convert_to_pinned_p = TRUE;
-                record_interesting_data_point (idp_converted_pin);
-                return 0;
-            }
-        }
-#endif //SHORT_PLUGS
-        if ((old_loc == 0) || (pad != 0))
-        {
-            generation_allocation_context_start_region (gen) = generation_allocation_pointer (gen);
-        }
-        generation_allocation_pointer (gen) += size + pad;
-        assert (generation_allocation_pointer (gen) <= generation_allocation_limit (gen));
-        if ((pad > 0) && (to_gen_number >= 0))
-        {
-            generation_free_obj_space (generation_of (to_gen_number)) += pad;
-        }
-#ifdef FREE_USAGE_STATS
-        generation_allocated_since_last_pin (gen) += size;
-#endif //FREE_USAGE_STATS
-        dprintf (3, ("aic: old: %p ptr: %p, limit: %p, sr: %p, res: %p, pad: %zd",
-            old_loc,
-            generation_allocation_pointer (gen), generation_allocation_limit (gen),
-            generation_allocation_context_start_region (gen),
-            result, (size_t)pad));
-        assert (result + pad);
-        return result + pad;
-    }
-}
-int gc_heap::joined_generation_to_condemn (BOOL should_evaluate_elevation,
-                                           int initial_gen,
-                                           int current_gen,
-                                           BOOL* blocking_collection_p
-                                           STRESS_HEAP_ARG(int n_original))
-{
-    gc_data_global.gen_to_condemn_reasons.init();
-#ifdef BGC_SERVO_TUNING
-    if (settings.entry_memory_load == 0)
-    {
-        uint32_t current_memory_load = 0;
-        uint64_t current_available_physical = 0;
-        get_memory_info (&current_memory_load, &current_available_physical);
-        settings.entry_memory_load = current_memory_load;
-        settings.entry_available_physical_mem = current_available_physical;
-    }
-#endif //BGC_SERVO_TUNING
-    int n = current_gen;
-#ifdef MULTIPLE_HEAPS
-    BOOL joined_last_gc_before_oom = FALSE;
-    for (int i = 0; i < n_heaps; i++)
-    {
-        if (g_heaps[i]->last_gc_before_oom)
-        {
-            dprintf (GTC_LOG, ("h%d is setting blocking to TRUE", i));
-            joined_last_gc_before_oom = TRUE;
-            break;
-        }
-    }
-#else
-    BOOL joined_last_gc_before_oom = last_gc_before_oom;
-#endif //MULTIPLE_HEAPS
-    if (joined_last_gc_before_oom && settings.pause_mode != pause_low_latency)
-    {
-        assert (*blocking_collection_p);
-    }
-    if (should_evaluate_elevation && (n == max_generation))
-    {
-        dprintf (GTC_LOG, ("lock: %d(%d)",
-            (settings.should_lock_elevation ? 1 : 0),
-            settings.elevation_locked_count));
-        if (settings.should_lock_elevation)
-        {
-            settings.elevation_locked_count++;
-            if (settings.elevation_locked_count == 6)
-            {
-                settings.elevation_locked_count = 0;
-            }
-            else
-            {
-                n = max_generation - 1;
-                gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_avoid_unproductive);
-                settings.elevation_reduced = TRUE;
-            }
-        }
-        else
-        {
-            settings.elevation_locked_count = 0;
-        }
-    }
-    else
-    {
-        settings.should_lock_elevation = FALSE;
-        settings.elevation_locked_count = 0;
-    }
-    if (provisional_mode_triggered && (n == max_generation))
-    {
-        if ((initial_gen == max_generation) || (settings.reason == reason_alloc_loh))
-        {
-            dprintf (GTC_LOG, ("full GC induced, not reducing gen"));
-            if (initial_gen == max_generation)
-            {
-                gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_pm_induced_fullgc_p);
-            }
-            else
-            {
-                gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_pm_alloc_loh);
-            }
-            *blocking_collection_p = TRUE;
-        }
-        else if (
-#ifndef USE_REGIONS
-                 should_expand_in_full_gc ||
-#endif //!USE_REGIONS
-                 joined_last_gc_before_oom)
-        {
-            dprintf (GTC_LOG, ("need full blocking GCs to expand heap or avoid OOM, not reducing gen"));
-            assert (*blocking_collection_p);
-        }
-        else
-        {
-            dprintf (GTC_LOG, ("reducing gen in PM: %d->%d->%d", initial_gen, n, (max_generation - 1)));
-            gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_gen1_in_pm);
-            n = max_generation - 1;
-        }
-    }
-#ifndef USE_REGIONS
-    if (should_expand_in_full_gc)
-    {
-        should_expand_in_full_gc = FALSE;
-    }
-#endif //!USE_REGIONS
-    if (heap_hard_limit)
-    {
-        dprintf (GTC_LOG, ("committed %zd is %d%% of limit %zd",
-            current_total_committed, (int)((float)current_total_committed * 100.0 / (float)heap_hard_limit),
-            heap_hard_limit));
-        bool full_compact_gc_p = false;
-        if (joined_last_gc_before_oom)
-        {
-            gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_limit_before_oom);
-            full_compact_gc_p = true;
-        }
-        else if ((current_total_committed * 10) >= (heap_hard_limit * 9))
-        {
-            size_t loh_frag = get_total_gen_fragmentation (loh_generation);
-            if ((loh_frag * 8) >= heap_hard_limit)
-            {
-                dprintf (GTC_LOG, ("loh frag: %zd > 1/8 of limit %zd", loh_frag, (heap_hard_limit / 8)));
-                gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_limit_loh_frag);
-                full_compact_gc_p = true;
-            }
-            else
-            {
-                size_t est_loh_reclaim = get_total_gen_estimated_reclaim (loh_generation);
-                if ((est_loh_reclaim * 8) >= heap_hard_limit)
-                {
-                    gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_limit_loh_reclaim);
-                    full_compact_gc_p = true;
-                }
-                dprintf (GTC_LOG, ("loh est reclaim: %zd, 1/8 of limit %zd", est_loh_reclaim, (heap_hard_limit / 8)));
-            }
-        }
-        if (full_compact_gc_p)
-        {
-            n = max_generation;
-            *blocking_collection_p = TRUE;
-            settings.loh_compaction = TRUE;
-            dprintf (GTC_LOG, ("compacting LOH due to hard limit"));
-        }
-    }
-    if ((conserve_mem_setting != 0) && (n == max_generation))
-    {
-        float frag_limit = 1.0f - conserve_mem_setting / 10.0f;
-        size_t loh_size = get_total_gen_size (loh_generation);
-        size_t gen2_size = get_total_gen_size (max_generation);
-        float loh_frag_ratio = 0.0f;
-        float combined_frag_ratio = 0.0f;
-        if (loh_size != 0)
-        {
-            size_t loh_frag  = get_total_gen_fragmentation (loh_generation);
-            size_t gen2_frag = get_total_gen_fragmentation (max_generation);
-            loh_frag_ratio = (float)loh_frag / (float)loh_size;
-            combined_frag_ratio = (float)(gen2_frag + loh_frag) / (float)(gen2_size + loh_size);
-        }
-        if (combined_frag_ratio > frag_limit)
-        {
-            dprintf (GTC_LOG, ("combined frag: %f > limit %f, loh frag: %f", combined_frag_ratio, frag_limit, loh_frag_ratio));
-            gc_data_global.gen_to_condemn_reasons.set_condition (gen_max_high_frag_p);
-            n = max_generation;
-            *blocking_collection_p = TRUE;
-            if (loh_frag_ratio > frag_limit)
-            {
-                settings.loh_compaction = TRUE;
-                dprintf (GTC_LOG, ("compacting LOH due to GCConserveMem setting"));
-            }
-        }
-    }
-#ifdef BGC_SERVO_TUNING
-    if (bgc_tuning::should_trigger_ngc2())
-    {
-        gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_servo_ngc);
-        n = max_generation;
-        *blocking_collection_p = TRUE;
-    }
-    if ((n < max_generation) && !gc_heap::background_running_p() &&
-        bgc_tuning::stepping_trigger (settings.entry_memory_load, get_current_gc_index (max_generation)))
-    {
-        gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_servo_initial);
-        n = max_generation;
-        saved_bgc_tuning_reason = reason_bgc_stepping;
-    }
-    if ((n < max_generation) && bgc_tuning::should_trigger_bgc())
-    {
-        gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_servo_bgc);
-        n = max_generation;
-    }
-    if (n == (max_generation - 1))
-    {
-        if (bgc_tuning::should_delay_alloc (max_generation))
-        {
-            gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_servo_postpone);
-            n -= 1;
-        }
-    }
-#endif //BGC_SERVO_TUNING
-    if ((n == max_generation) && (*blocking_collection_p == FALSE))
-    {
-        settings.should_lock_elevation = FALSE;
-        settings.elevation_locked_count = 0;
-        dprintf (GTC_LOG, ("doing bgc, reset elevation"));
-    }
-#ifdef STRESS_HEAP
-#ifdef BACKGROUND_GC
-    if (n_original != max_generation &&
-        g_pConfig->GetGCStressLevel() && gc_can_use_concurrent)
-    {
-#ifndef FEATURE_NATIVEAOT
-        if (*blocking_collection_p)
-        {
-            GCStressPolicy::GlobalDisable();
-        }
-        else
-#endif // !FEATURE_NATIVEAOT
-        {
-            gc_data_global.gen_to_condemn_reasons.set_condition(gen_joined_stress);
-            n = max_generation;
-        }
-    }
-#endif //BACKGROUND_GC
-#endif //STRESS_HEAP
-#ifdef BACKGROUND_GC
-    if ((n == max_generation) && background_running_p())
-    {
-        n = max_generation - 1;
-        dprintf (GTC_LOG, ("bgc in progress - 1 instead of 2"));
-    }
-#endif //BACKGROUND_GC
-    return n;
-}
-inline
-size_t get_survived_size (gc_history_per_heap* hist)
-{
-    size_t surv_size = 0;
-    gc_generation_data* gen_data;
-    for (int gen_number = 0; gen_number < total_generation_count; gen_number++)
-    {
-        gen_data = &(hist->gen_data[gen_number]);
-        surv_size += (gen_data->size_after -
-                      gen_data->free_list_space_after -
-                      gen_data->free_obj_space_after);
-    }
-    return surv_size;
-}
-size_t gc_heap::get_total_survived_size()
-{
-    size_t total_surv_size = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-        gc_history_per_heap* current_gc_data_per_heap = hp->get_gc_data_per_heap();
-        total_surv_size += get_survived_size (current_gc_data_per_heap);
-    }
-#else
-    gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
-    total_surv_size = get_survived_size (current_gc_data_per_heap);
-#endif //MULTIPLE_HEAPS
-    return total_surv_size;
-}
-size_t gc_heap::get_total_allocated_since_last_gc()
-{
-    size_t total_allocated_size = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        total_allocated_size += hp->allocated_since_last_gc[0] + hp->allocated_since_last_gc[1];
-        hp->allocated_since_last_gc[0] = 0;
-        hp->allocated_since_last_gc[1] = 0;
-    }
-    return total_allocated_size;
-}
-size_t gc_heap::get_current_allocated()
-{
-    dynamic_data* dd = dynamic_data_of (0);
-    size_t current_alloc = dd_desired_allocation (dd) - dd_new_allocation (dd);
-    for (int i = uoh_start_generation; i < total_generation_count; i++)
-    {
-        dynamic_data* dd = dynamic_data_of (i);
-        current_alloc += dd_desired_allocation (dd) - dd_new_allocation (dd);
-    }
-    return current_alloc;
-}
-size_t gc_heap::get_total_allocated()
-{
-    size_t total_current_allocated = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-        total_current_allocated += hp->get_current_allocated();
-    }
-#else
-    total_current_allocated = get_current_allocated();
-#endif //MULTIPLE_HEAPS
-    return total_current_allocated;
-}
-size_t gc_heap::get_total_promoted()
-{
-    size_t total_promoted_size = 0;
-    int highest_gen = ((settings.condemned_generation == max_generation) ?
-                       (total_generation_count - 1) : settings.condemned_generation);
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        for (int gen_number = 0; gen_number <= highest_gen; gen_number++)
-        {
-            total_promoted_size += dd_promoted_size (hp->dynamic_data_of (gen_number));
-        }
-    }
-    return total_promoted_size;
-}
-#ifdef BGC_SERVO_TUNING
-size_t gc_heap::get_total_generation_size (int gen_number)
-{
-    size_t total_generation_size = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        total_generation_size += hp->generation_size (gen_number);
-    }
-    return total_generation_size;
-}
-size_t gc_heap::get_total_servo_alloc (int gen_number)
-{
-    size_t total_alloc = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        generation* gen = hp->generation_of (gen_number);
-        total_alloc += generation_free_list_allocated (gen);
-        total_alloc += generation_end_seg_allocated (gen);
-        total_alloc += generation_condemned_allocated (gen);
-        total_alloc += generation_sweep_allocated (gen);
-    }
-    return total_alloc;
-}
-size_t gc_heap::get_total_bgc_promoted()
-{
-    size_t total_bgc_promoted = 0;
-#ifdef MULTIPLE_HEAPS
-    int num_heaps = gc_heap::n_heaps;
-#else //MULTIPLE_HEAPS
-    int num_heaps = 1;
-#endif //MULTIPLE_HEAPS
-    for (int i = 0; i < num_heaps; i++)
-    {
-        total_bgc_promoted += bpromoted_bytes (i);
-    }
-    return total_bgc_promoted;
-}
-size_t gc_heap::get_total_surv_size (int gen_number)
-{
-    size_t total_surv_size = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        total_surv_size += dd_current_size (hp->dynamic_data_of (gen_number));
-    }
-    return total_surv_size;
-}
-size_t gc_heap::get_total_begin_data_size (int gen_number)
-{
-    size_t total_begin_data_size = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        total_begin_data_size += dd_begin_data_size (hp->dynamic_data_of (gen_number));
-    }
-    return total_begin_data_size;
-}
-size_t gc_heap::get_total_generation_fl_size (int gen_number)
-{
-    size_t total_generation_fl_size = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        total_generation_fl_size += generation_free_list_space (hp->generation_of (gen_number));
-    }
-    return total_generation_fl_size;
-}
-size_t gc_heap::get_current_gc_index (int gen_number)
-{
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hp = gc_heap::g_heaps[0];
-    return dd_collection_count (hp->dynamic_data_of (gen_number));
-#else
-    return dd_collection_count (dynamic_data_of (gen_number));
-#endif //MULTIPLE_HEAPS
-}
-#endif //BGC_SERVO_TUNING
-size_t gc_heap::current_generation_size (int gen_number)
-{
-    dynamic_data* dd = dynamic_data_of (gen_number);
-    size_t gen_size = (dd_current_size (dd) + dd_desired_allocation (dd)
-                        - dd_new_allocation (dd));
-    return gen_size;
-}
-#ifdef USE_REGIONS
-bool gc_heap::try_get_new_free_region()
-{
-    heap_segment* region = 0;
-    if (free_regions[basic_free_region].get_num_free_regions() > 0)
-    {
-        dprintf (REGIONS_LOG, ("h%d has %zd free regions %p", heap_number, free_regions[basic_free_region].get_num_free_regions(),
-            heap_segment_mem (free_regions[basic_free_region].get_first_free_region())));
-        return true;
-    }
-    else
-    {
-        region = allocate_new_region (__this, 0, false);
-        if (region)
-        {
-            if (init_table_for_region (0, region))
-            {
-                return_free_region (region);
-                dprintf (REGIONS_LOG, ("h%d got a new empty region %p", heap_number, region));
-            }
-            else
-            {
-                region = 0;
-            }
-        }
-    }
-    return (region != 0);
-}
-bool gc_heap::init_table_for_region (int gen_number, heap_segment* region)
-{
-#ifdef BACKGROUND_GC
-    dprintf (GC_TABLE_LOG, ("new seg %Ix, mark_array is %Ix",
-        heap_segment_mem (region), mark_array));
-    if (((region->flags & heap_segment_flags_ma_committed) == 0) &&
-        !commit_mark_array_new_seg (__this, region))
-    {
-        dprintf (GC_TABLE_LOG, ("failed to commit mark array for the new region %Ix-%Ix",
-            get_region_start (region), heap_segment_reserved (region)));
-        decommit_region (region, gen_to_oh (gen_number), heap_number);
-        return false;
-    }
-    if ((region->flags & heap_segment_flags_ma_committed) != 0)
-    {
-        bgc_verify_mark_array_cleared (region, true);
-    }
-#endif //BACKGROUND_GC
-    if (gen_number <= max_generation)
-    {
-        size_t first_brick = brick_of (heap_segment_mem (region));
-        set_brick (first_brick, -1);
-    }
-    else
-    {
-        assert (brick_table[brick_of (heap_segment_mem (region))] == 0);
-    }
-    return true;
-}
-#endif //USE_REGIONS
-#ifdef _PREFAST_
-#pragma warning(push)
-#pragma warning(disable:6326) // "Potential comparison of a constant with another constant" is intentional in this function.
-#endif //_PREFAST_
-/*
-    This is called by when we are actually doing a GC, or when we are just checking whether
-    we would do a full blocking GC, in which case check_only_p is TRUE.
-    The difference between calling this with check_only_p TRUE and FALSE is that when it's
-    TRUE:
-            settings.reason is ignored
-            budgets are not checked (since they are checked before this is called)
-            it doesn't change anything non local like generation_skip_ratio
-*/
-int gc_heap::generation_to_condemn (int n_initial,
-                                    BOOL* blocking_collection_p,
-                                    BOOL* elevation_requested_p,
-                                    BOOL check_only_p)
-{
-    gc_mechanisms temp_settings = settings;
-    gen_to_condemn_tuning temp_condemn_reasons;
-    gc_mechanisms* local_settings = (check_only_p ? &temp_settings : &settings);
-    gen_to_condemn_tuning* local_condemn_reasons = (check_only_p ? &temp_condemn_reasons : &gen_to_condemn_reasons);
-    if (!check_only_p)
-    {
-        if ((local_settings->reason == reason_oos_soh) || (local_settings->reason == reason_oos_loh))
-        {
-            assert (n_initial >= 1);
-        }
-        assert (settings.reason != reason_empty);
-    }
-    local_condemn_reasons->init();
-    int n = n_initial;
-    int n_alloc = n;
-    if (heap_number == 0)
-    {
-        dprintf (GTC_LOG, ("init: %d(%d)", n_initial, settings.reason));
-    }
-    int i = 0;
-    int temp_gen = 0;
-    BOOL low_memory_detected = g_low_memory_status;
-    uint32_t memory_load = 0;
-    uint64_t available_physical = 0;
-    uint64_t available_page_file = 0;
-    BOOL check_memory = FALSE;
-    BOOL high_fragmentation  = FALSE;
-    BOOL v_high_memory_load  = FALSE;
-    BOOL high_memory_load    = FALSE;
-    BOOL low_ephemeral_space = FALSE;
-    BOOL evaluate_elevation  = TRUE;
-    *elevation_requested_p   = FALSE;
-    *blocking_collection_p   = FALSE;
-    BOOL check_max_gen_alloc = TRUE;
-#ifdef STRESS_HEAP
-    int orig_gen = n;
-#endif //STRESS_HEAP
-    if (!check_only_p)
-    {
-        dd_fragmentation (dynamic_data_of (0)) =
-            generation_free_list_space (youngest_generation) +
-            generation_free_obj_space (youngest_generation);
-        for (int i = uoh_start_generation; i < total_generation_count; i++)
-        {
-            dd_fragmentation (dynamic_data_of (i)) =
-                generation_free_list_space (generation_of (i)) +
-                generation_free_obj_space (generation_of (i));
-        }
-        for (i = 0; i < total_generation_count; i++)
-        {
-            dynamic_data* dd = dynamic_data_of (i);
-            dprintf (GTC_LOG, ("h%d: g%d: l: %zd (%zd)",
-                heap_number, i,
-                dd_new_allocation (dd),
-                dd_desired_allocation (dd)));
-            dd_gc_new_allocation (dd) = dd_new_allocation (dd);
-        }
-        local_condemn_reasons->set_gen (gen_initial, n);
-        temp_gen = n;
-#ifdef BACKGROUND_GC
-        if (gc_heap::background_running_p()
-#ifdef BGC_SERVO_TUNING
-            || bgc_tuning::fl_tuning_triggered
-            || (bgc_tuning::enable_fl_tuning && bgc_tuning::use_stepping_trigger_p)
-#endif //BGC_SERVO_TUNING
-            )
-        {
-            check_max_gen_alloc = FALSE;
-        }
-#endif //BACKGROUND_GC
-        if (check_max_gen_alloc)
-        {
-            for (int i = uoh_start_generation; i < total_generation_count; i++)
-            {
-                if (get_new_allocation (i) <= 0)
-                {
-                    n = max_generation;
-                    local_condemn_reasons->set_gen (gen_alloc_budget, n);
-                    dprintf (BGC_TUNING_LOG, ("BTL[GTC]: trigger based on gen%d b: %zd",
-                             (i),
-                             get_new_allocation (i)));
-                    break;
-                }
-            }
-        }
-        for (i = n+1; i <= (check_max_gen_alloc ? max_generation : (max_generation - 1)); i++)
-        {
-            if (get_new_allocation (i) <= 0)
-            {
-                n = i;
-                if (n == max_generation)
-                {
-                    dprintf (BGC_TUNING_LOG, ("BTL[GTC]: trigger based on gen2 b: %zd",
-                            get_new_allocation (max_generation)));
-                }
-            }
-            else
-                break;
-        }
-    }
-    if (n > temp_gen)
-    {
-        local_condemn_reasons->set_gen (gen_alloc_budget, n);
-    }
-    dprintf (GTC_LOG, ("h%d: g%d budget", heap_number, ((get_new_allocation (loh_generation) <= 0) ? 3 : n)));
-    n_alloc = n;
-#if defined(BACKGROUND_GC) && !defined(MULTIPLE_HEAPS)
-    int n_time_max = max_generation;
-    if (!check_only_p)
-    {
-        if (!check_max_gen_alloc)
-        {
-            n_time_max = max_generation - 1;
-        }
-    }
-    if ((local_settings->pause_mode == pause_interactive) ||
-        (local_settings->pause_mode == pause_sustained_low_latency))
-    {
-        dynamic_data* dd0 = dynamic_data_of (0);
-        uint64_t now = GetHighPrecisionTimeStamp();
-        temp_gen = n;
-        for (i = (temp_gen+1); i <= n_time_max; i++)
-        {
-            dynamic_data* dd = dynamic_data_of (i);
-            if ((now > dd_time_clock(dd) + dd_time_clock_interval(dd)) &&
-                (dd_gc_clock (dd0) > (dd_gc_clock (dd) + dd_gc_clock_interval(dd))) &&
-                ((n < max_generation) || ((dd_current_size (dd) < dd_max_size (dd0)))))
-            {
-                n = min (i, n_time_max);
-                dprintf (GTC_LOG, ("time %d", n));
-            }
-        }
-        if (n > temp_gen)
-        {
-            local_condemn_reasons->set_gen (gen_time_tuning, n);
-            if (n == max_generation)
-            {
-                dprintf (BGC_TUNING_LOG, ("BTL[GTC]: trigger based on time"));
-            }
-        }
-    }
-    if (n != n_alloc)
-    {
-        dprintf (GTC_LOG, ("Condemning %d based on time tuning and fragmentation", n));
-    }
-#endif //BACKGROUND_GC && !MULTIPLE_HEAPS
-    if (n < (max_generation - 1))
-    {
-        if (dt_low_card_table_efficiency_p (tuning_deciding_condemned_gen))
-        {
-            n = max (n, max_generation - 1);
-            local_settings->promotion = TRUE;
-            dprintf (GTC_LOG, ("h%d: skip %d, c %d",
-                        heap_number, generation_skip_ratio, n));
-            local_condemn_reasons->set_condition (gen_low_card_p);
-        }
-    }
-    if (!check_only_p)
-    {
-        generation_skip_ratio = 100;
-    }
-    if (dt_low_ephemeral_space_p (check_only_p ?
-                                  tuning_deciding_full_gc :
-                                  tuning_deciding_condemned_gen))
-    {
-        low_ephemeral_space = TRUE;
-        n = max (n, max_generation - 1);
-        local_condemn_reasons->set_condition (gen_low_ephemeral_p);
-        dprintf (GTC_LOG, ("h%d: low eph", heap_number));
-        if (!provisional_mode_triggered)
-        {
-#ifdef BACKGROUND_GC
-            if (!gc_can_use_concurrent || (generation_free_list_space (generation_of (max_generation)) == 0))
-#endif //BACKGROUND_GC
-            {
-                if (dt_high_frag_p (tuning_deciding_condemned_gen,
-                                    max_generation - 1,
-                                    TRUE))
-                {
-                    high_fragmentation = TRUE;
-                    local_condemn_reasons->set_condition (gen_max_high_frag_e_p);
-                    dprintf (GTC_LOG, ("heap%d: gen1 frag", heap_number));
-                }
-            }
-        }
-    }
-#ifdef USE_REGIONS
-    if (!check_only_p)
-    {
-        if (!try_get_new_free_region())
-        {
-            dprintf (GTC_LOG, ("can't get an empty region -> full compacting"));
-            last_gc_before_oom = TRUE;
-        }
-    }
-#endif //USE_REGIONS
-    temp_gen = n;
-    for (i = n+1; i < max_generation; i++)
-    {
-        if (dt_high_frag_p (tuning_deciding_condemned_gen, i))
-        {
-            dprintf (GTC_LOG, ("h%d g%d too frag", heap_number, i));
-            n = i;
-        }
-        else
-            break;
-    }
-    if (low_ephemeral_space)
-    {
-        local_settings->promotion = TRUE;
-    }
-    if (n > temp_gen)
-    {
-        local_condemn_reasons->set_condition (gen_eph_high_frag_p);
-    }
-    if (!check_only_p)
-    {
-        if (settings.pause_mode == pause_low_latency)
-        {
-            if (!is_induced (settings.reason))
-            {
-                n = min (n, max_generation - 1);
-                dprintf (GTC_LOG, ("low latency mode is enabled, condemning %d", n));
-                evaluate_elevation = FALSE;
-                goto exit;
-            }
-        }
-    }
-    check_memory = (check_only_p ?
-                    (n >= 0) :
-                    ((n >= 1) || low_memory_detected));
-    if (check_memory)
-    {
-        get_memory_info (&memory_load, &available_physical, &available_page_file);
-        if (heap_number == 0)
-        {
-            dprintf (GTC_LOG, ("ml: %d", memory_load));
-        }
-#ifdef USE_REGIONS
-        uint32_t va_memory_load = global_region_allocator.get_va_memory_load();
-        if (heap_number == 0)
-        {
-            dprintf (GTC_LOG, ("h%d ML %d, va ML %d", heap_number, memory_load, va_memory_load));
-        }
-        memory_load = max (memory_load, va_memory_load);
-#endif //USE_REGIONS
-        local_settings->entry_available_physical_mem = available_physical;
-        local_settings->entry_memory_load = memory_load;
-        if (memory_load >= high_memory_load_th || low_memory_detected)
-        {
-#ifdef SIMPLE_DPRINTF
-            if (heap_number == 0)
-            {
-                dprintf (GTC_LOG, ("tp: %zd, ap: %zd", total_physical_mem, available_physical));
-            }
-#endif //SIMPLE_DPRINTF
-            high_memory_load = TRUE;
-            if (memory_load >= v_high_memory_load_th || low_memory_detected)
-            {
-                if (!high_fragmentation)
-                {
-                    high_fragmentation = dt_estimate_reclaim_space_p (tuning_deciding_condemned_gen, max_generation);
-                }
-                v_high_memory_load = TRUE;
-            }
-            else
-            {
-                if (!high_fragmentation)
-                {
-                    high_fragmentation = dt_estimate_high_frag_p (tuning_deciding_condemned_gen, max_generation, available_physical);
-                }
-            }
-            if (high_fragmentation)
-            {
-                if (high_memory_load)
-                {
-                    local_condemn_reasons->set_condition (gen_max_high_frag_m_p);
-                }
-                else if (v_high_memory_load)
-                {
-                    local_condemn_reasons->set_condition (gen_max_high_frag_vm_p);
-                }
-            }
-        }
-    }
-    dprintf (GTC_LOG, ("h%d: le: %d, hm: %d, vm: %d, f: %d",
-                 heap_number, low_ephemeral_space, high_memory_load, v_high_memory_load,
-                 high_fragmentation));
-#ifndef USE_REGIONS
-    if (should_expand_in_full_gc)
-    {
-        dprintf (GTC_LOG, ("h%d: expand_in_full - BLOCK", heap_number));
-        *blocking_collection_p = TRUE;
-        evaluate_elevation = FALSE;
-        n = max_generation;
-        local_condemn_reasons->set_condition (gen_expand_fullgc_p);
-    }
-#endif //!USE_REGIONS
-    if (last_gc_before_oom)
-    {
-        dprintf (GTC_LOG, ("h%d: alloc full - BLOCK", heap_number));
-        n = max_generation;
-        *blocking_collection_p = TRUE;
-        if ((local_settings->reason == reason_oos_loh) ||
-            (local_settings->reason == reason_alloc_loh))
-        {
-            evaluate_elevation = FALSE;
-        }
-        local_condemn_reasons->set_condition (gen_before_oom);
-    }
-    if (!check_only_p)
-    {
-        if (is_induced_blocking (settings.reason) &&
-            n_initial == max_generation
-            IN_STRESS_HEAP( && !settings.stress_induced ))
-        {
-            if (heap_number == 0)
-            {
-                dprintf (GTC_LOG, ("induced - BLOCK"));
-            }
-            *blocking_collection_p = TRUE;
-            local_condemn_reasons->set_condition (gen_induced_fullgc_p);
-            evaluate_elevation = FALSE;
-        }
-        if (settings.reason == reason_induced_noforce)
-        {
-            local_condemn_reasons->set_condition (gen_induced_noforce_p);
-            evaluate_elevation = FALSE;
-        }
-    }
-    if (!provisional_mode_triggered && evaluate_elevation && (low_ephemeral_space || high_memory_load || v_high_memory_load))
-    {
-        *elevation_requested_p = TRUE;
-#ifdef HOST_64BIT
-        if (high_memory_load || v_high_memory_load)
-        {
-            dynamic_data* dd_max = dynamic_data_of (max_generation);
-            if (((float)dd_new_allocation (dd_max) / (float)dd_desired_allocation (dd_max)) < 0.9)
-            {
-                dprintf (GTC_LOG, ("%zd left in gen2 alloc (%zd)",
-                    dd_new_allocation (dd_max), dd_desired_allocation (dd_max)));
-                n = max_generation;
-                local_condemn_reasons->set_condition (gen_almost_max_alloc);
-            }
-        }
-        if (n <= max_generation)
-        {
-#endif // HOST_64BIT
-            if (high_fragmentation)
-            {
-                n = max_generation;
-                dprintf (GTC_LOG, ("h%d: f full", heap_number));
-#ifdef BACKGROUND_GC
-                if (high_memory_load || v_high_memory_load)
-                {
-                    dprintf (GTC_LOG, ("h%d: bgc - BLOCK", heap_number));
-                    *blocking_collection_p = TRUE;
-                }
-#else
-                if (v_high_memory_load)
-                {
-                    dprintf (GTC_LOG, ("h%d: - BLOCK", heap_number));
-                    *blocking_collection_p = TRUE;
-                }
-#endif //BACKGROUND_GC
-            }
-            else
-            {
-                n = max (n, max_generation - 1);
-                dprintf (GTC_LOG, ("h%d: nf c %d", heap_number, n));
-            }
-#ifdef HOST_64BIT
-        }
-#endif // HOST_64BIT
-    }
-    if (!provisional_mode_triggered && (n == (max_generation - 1)) && (n_alloc < (max_generation -1)))
-    {
-#ifdef BGC_SERVO_TUNING
-        if (!bgc_tuning::enable_fl_tuning)
-#endif //BGC_SERVO_TUNING
-        {
-            dprintf (GTC_LOG, ("h%d: budget %d, check 2",
-                        heap_number, n_alloc));
-            if (get_new_allocation (max_generation) <= 0)
-            {
-                dprintf (GTC_LOG, ("h%d: budget alloc", heap_number));
-                n = max_generation;
-                local_condemn_reasons->set_condition (gen_max_gen1);
-            }
-        }
-    }
-    if (!provisional_mode_triggered
-#ifdef BGC_SERVO_TUNING
-        && !bgc_tuning::enable_fl_tuning
-#endif //BGC_SERVO_TUNING
-        && (n == max_generation))
-    {
-        if (dt_high_frag_p (tuning_deciding_condemned_gen, n))
-        {
-            dprintf (GTC_LOG, ("h%d: g%d too frag", heap_number, n));
-            local_condemn_reasons->set_condition (gen_max_high_frag_p);
-            if (local_settings->pause_mode != pause_sustained_low_latency)
-            {
-                *blocking_collection_p = TRUE;
-            }
-        }
-    }
-#ifdef BACKGROUND_GC
-    if ((n == max_generation) && !(*blocking_collection_p))
-    {
-        if (heap_number == 0)
-        {
-            BOOL bgc_heap_too_small = TRUE;
-            size_t gen2size = 0;
-            size_t gen3size = 0;
-#ifdef MULTIPLE_HEAPS
-            for (int i = 0; i < n_heaps; i++)
-            {
-                if (((g_heaps[i]->current_generation_size (max_generation)) > bgc_min_per_heap) ||
-                    ((g_heaps[i]->current_generation_size (loh_generation)) > bgc_min_per_heap) ||
-                    ((g_heaps[i]->current_generation_size (poh_generation)) > bgc_min_per_heap))
-                {
-                    bgc_heap_too_small = FALSE;
-                    break;
-                }
-            }
-#else //MULTIPLE_HEAPS
-            if ((current_generation_size (max_generation) > bgc_min_per_heap) ||
-                (current_generation_size (loh_generation) > bgc_min_per_heap) ||
-                (current_generation_size (poh_generation) > bgc_min_per_heap))
-            {
-                bgc_heap_too_small = FALSE;
-            }
-#endif //MULTIPLE_HEAPS
-            if (bgc_heap_too_small)
-            {
-                dprintf (GTC_LOG, ("gen2 and gen3 too small"));
-#ifdef STRESS_HEAP
-                if (!settings.stress_induced)
-#endif //STRESS_HEAP
-                {
-                    *blocking_collection_p = TRUE;
-                }
-                local_condemn_reasons->set_condition (gen_gen2_too_small);
-            }
-        }
-    }
-#endif //BACKGROUND_GC
-exit:
-    if (!check_only_p)
-    {
-#ifdef STRESS_HEAP
-#ifdef BACKGROUND_GC
-        if (orig_gen != max_generation &&
-            g_pConfig->GetGCStressLevel() && gc_can_use_concurrent)
-        {
-            *elevation_requested_p = FALSE;
-        }
-#endif //BACKGROUND_GC
-#endif //STRESS_HEAP
-        if (check_memory)
-        {
-            fgm_result.available_pagefile_mb = (size_t)(available_page_file / (1024 * 1024));
-        }
-        local_condemn_reasons->set_gen (gen_final_per_heap, n);
-        get_gc_data_per_heap()->gen_to_condemn_reasons.init (local_condemn_reasons);
-#ifdef DT_LOG
-        local_condemn_reasons->print (heap_number);
-#endif //DT_LOG
-        if ((local_settings->reason == reason_oos_soh) ||
-            (local_settings->reason == reason_oos_loh))
-        {
-            assert (n >= 1);
-        }
-    }
-    return n;
-}
-#ifdef _PREFAST_
-#pragma warning(pop)
-#endif //_PREFAST_
-inline
-size_t gc_heap::min_reclaim_fragmentation_threshold (uint32_t num_heaps)
-{
-    size_t min_mem_based_on_available =
-        (500 - (settings.entry_memory_load - high_memory_load_th) * 40) * 1024 * 1024 / num_heaps;
-    size_t ten_percent_size = (size_t)((float)generation_size (max_generation) * 0.10);
-    uint64_t three_percent_mem = mem_one_percent * 3 / num_heaps;
-#ifdef SIMPLE_DPRINTF
-    dprintf (GTC_LOG, ("min av: %zd, 10%% gen2: %zd, 3%% mem: %zd",
-        min_mem_based_on_available, ten_percent_size, three_percent_mem));
-#endif //SIMPLE_DPRINTF
-    return (size_t)(min (min_mem_based_on_available, min (ten_percent_size, three_percent_mem)));
-}
-inline
-uint64_t gc_heap::min_high_fragmentation_threshold(uint64_t available_mem, uint32_t num_heaps)
-{
-    return min (available_mem, (256*1024*1024)) / num_heaps;
-}
-enum {
-CORINFO_EXCEPTION_GC = 0xE0004743 // 'GC'
-};
-#ifdef BACKGROUND_GC
-void gc_heap::init_background_gc ()
-{
-    generation* gen = generation_of (max_generation);
-    generation_allocation_pointer (gen)= 0;
-    generation_allocation_limit (gen) = 0;
-    generation_allocation_segment (gen) = heap_segment_rw (generation_start_segment (gen));
-    PREFIX_ASSUME(generation_allocation_segment(gen) != NULL);
-#ifdef DOUBLY_LINKED_FL
-    generation_set_bgc_mark_bit_p (gen) = FALSE;
-#endif //DOUBLY_LINKED_FL
-#ifndef USE_REGIONS
-    for (heap_segment* seg = generation_allocation_segment (gen); seg != ephemeral_heap_segment;
-        seg = heap_segment_next_rw (seg))
-    {
-        heap_segment_plan_allocated (seg) = heap_segment_allocated (seg);
-    }
-#endif //!USE_REGIONS
-    if (heap_number == 0)
-    {
-        dprintf (2, ("heap%d: bgc lowest: %p, highest: %p",
-            heap_number,
-            background_saved_lowest_address,
-            background_saved_highest_address));
-    }
-}
-#endif //BACKGROUND_GC
-inline
-void fire_drain_mark_list_event (size_t mark_list_objects)
-{
-    FIRE_EVENT(BGCDrainMark, mark_list_objects);
-}
-inline
-void fire_revisit_event (size_t dirtied_pages,
-                         size_t marked_objects,
-                         BOOL large_objects_p)
-{
-    FIRE_EVENT(BGCRevisit, dirtied_pages, marked_objects, large_objects_p);
-}
-inline
-void fire_overflow_event (uint8_t* overflow_min,
-                          uint8_t* overflow_max,
-                          size_t marked_objects,
-                          int gen_number)
-{
-    FIRE_EVENT(BGCOverflow_V1, (uint64_t)overflow_min, (uint64_t)overflow_max, marked_objects, gen_number == loh_generation, gen_number);
-}
-void gc_heap::concurrent_print_time_delta (const char* msg)
-{
-#ifdef TRACE_GC
-    uint64_t current_time = GetHighPrecisionTimeStamp();
-    size_t elapsed_time_ms = (size_t)((current_time - time_bgc_last) / 1000);
-    time_bgc_last = current_time;
-    dprintf (2, ("h%d: %s T %zd ms", heap_number, msg, elapsed_time_ms));
-#else
-    UNREFERENCED_PARAMETER(msg);
-#endif //TRACE_GC
-}
-void gc_heap::free_list_info (int gen_num, const char* msg)
-{
-#if defined (BACKGROUND_GC) && defined (TRACE_GC)
-    dprintf (3, ("h%d: %s", heap_number, msg));
-    for (int i = 0; i < total_generation_count; i++)
-    {
-        generation* gen = generation_of (i);
-        if ((generation_allocation_size (gen) == 0) &&
-            (generation_free_list_space (gen) == 0) &&
-            (generation_free_obj_space (gen) == 0))
-        {
-        }
-        else
-        {
-            dprintf (3, ("h%d: g%d: a-%zd, fl-%zd, fo-%zd",
-                heap_number, i,
-                generation_allocation_size (gen),
-                generation_free_list_space (gen),
-                generation_free_obj_space (gen)));
-        }
-    }
-#else
-    UNREFERENCED_PARAMETER(gen_num);
-    UNREFERENCED_PARAMETER(msg);
-#endif // BACKGROUND_GC && TRACE_GC
-}
-void gc_heap::update_collection_counts_for_no_gc()
-{
-    assert (settings.pause_mode == pause_no_gc);
-    settings.condemned_generation = max_generation;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-        g_heaps[i]->update_collection_counts();
-#else //MULTIPLE_HEAPS
-    update_collection_counts();
-#endif //MULTIPLE_HEAPS
-    full_gc_counts[gc_type_blocking]++;
-}
-BOOL gc_heap::should_proceed_with_gc()
-{
-    if (gc_heap::settings.pause_mode == pause_no_gc)
-    {
-        if (current_no_gc_region_info.started)
-        {
-            if (current_no_gc_region_info.soh_withheld_budget != 0)
-            {
-                dprintf(1, ("[no_gc_callback] allocation budget exhausted with withheld, time to trigger callback\n"));
-#ifdef MULTIPLE_HEAPS
-                for (int i = 0; i < gc_heap::n_heaps; i++)
-                {
-                    gc_heap* hp = gc_heap::g_heaps [i];
-#else
-                {
-                    gc_heap* hp = pGenGCHeap;
-#endif
-                    dd_new_allocation (hp->dynamic_data_of (soh_gen0)) += current_no_gc_region_info.soh_withheld_budget;
-                    dd_new_allocation (hp->dynamic_data_of (loh_generation)) += current_no_gc_region_info.loh_withheld_budget;
-                }
-                current_no_gc_region_info.soh_withheld_budget = 0;
-                current_no_gc_region_info.loh_withheld_budget = 0;
-                schedule_no_gc_callback (false);
-                current_no_gc_region_info.callback = nullptr;
-                return FALSE;
-            }
-            else
-            {
-                dprintf(1, ("[no_gc_callback] GC triggered while in no_gc mode. Exiting no_gc mode.\n"));
-                restore_data_for_no_gc();
-                if (current_no_gc_region_info.callback != nullptr)
-                {
-                    dprintf (1, ("[no_gc_callback] detaching callback on exit"));
-                    schedule_no_gc_callback (true);
-                }
-                memset (&current_no_gc_region_info, 0, sizeof (current_no_gc_region_info));
-            }
-        }
-        else
-            return should_proceed_for_no_gc();
-    }
-    return TRUE;
-}
-void gc_heap::update_end_gc_time_per_heap()
-{
-#ifdef DYNAMIC_HEAP_COUNT
-    size_t prev_gen2_end_time = 0;
-    if ((heap_number == 0) && (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes) && (settings.condemned_generation == max_generation))
-    {
-        dynamic_data* dd = dynamic_data_of (max_generation);
-        prev_gen2_end_time = dd_previous_time_clock (dd) + dd_gc_elapsed_time (dd);;
-    }
-#endif //DYNAMIC_HEAP_COUNT
-    for (int gen_number = 0; gen_number <= settings.condemned_generation; gen_number++)
-    {
-        dynamic_data* dd = dynamic_data_of (gen_number);
-        if (heap_number == 0)
-        {
-            dprintf (6666, ("prev gen%d GC end time: prev start %I64d + prev gc elapsed %Id = %I64d",
-                gen_number, dd_previous_time_clock (dd), dd_gc_elapsed_time (dd), (dd_previous_time_clock (dd) + dd_gc_elapsed_time (dd))));
-        }
-        dd_gc_elapsed_time (dd) = (size_t)(end_gc_time - dd_time_clock (dd));
-        if (heap_number == 0)
-        {
-            dprintf (6666, ("updated NGC%d %Id elapsed time to %I64d - %I64d = %I64d", gen_number, dd_gc_clock (dd), end_gc_time, dd_time_clock (dd), dd_gc_elapsed_time (dd)));
-        }
-    }
-#ifdef DYNAMIC_HEAP_COUNT
-    if ((heap_number == 0) && (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes))
-    {
-        dynamic_heap_count_data_t::sample& sample = dynamic_heap_count_data.samples[dynamic_heap_count_data.sample_index];
-        sample.elapsed_between_gcs = end_gc_time - last_suspended_end_time;
-        sample.gc_pause_time = dd_gc_elapsed_time (dynamic_data_of (0));
-        sample.msl_wait_time = get_msl_wait_time();
-        dprintf (6666, ("sample#%d: this GC end %I64d - last sus end %I64d = %I64d, this GC pause %I64d, msl wait %I64d",
-            dynamic_heap_count_data.sample_index, end_gc_time, last_suspended_end_time, sample.elapsed_between_gcs, sample.gc_pause_time, sample.msl_wait_time));
-        last_suspended_end_time = end_gc_time;
-        GCEventFireHeapCountSample_V1 (
-            (uint64_t)VolatileLoadWithoutBarrier (&settings.gc_index),
-            sample.elapsed_between_gcs,
-            sample.gc_pause_time,
-            sample.msl_wait_time);
-        dynamic_heap_count_data.sample_index = (dynamic_heap_count_data.sample_index + 1) % dynamic_heap_count_data_t::sample_size;
-        if (settings.condemned_generation == max_generation)
-        {
-            gc_index_full_gc_end = dd_gc_clock (dynamic_data_of (0));
-            size_t elapsed_between_gen2_gcs = end_gc_time - prev_gen2_end_time;
-            size_t gen2_elapsed_time = sample.gc_pause_time;
-            dynamic_heap_count_data.gen2_gc_percents[dynamic_heap_count_data.gen2_sample_index] = (float)gen2_elapsed_time * 100.0f / elapsed_between_gen2_gcs;
-            dprintf (6666, ("gen2 sample#%d: this GC end %I64d - last gen2 end %I64d = %I64d, GC elapsed %I64d, percent %.3f",
-                dynamic_heap_count_data.gen2_sample_index, end_gc_time, prev_gen2_end_time, elapsed_between_gen2_gcs,
-                gen2_elapsed_time, dynamic_heap_count_data.gen2_gc_percents[dynamic_heap_count_data.gen2_sample_index]));
-            dynamic_heap_count_data.gen2_sample_index = (dynamic_heap_count_data.gen2_sample_index + 1) % dynamic_heap_count_data_t::sample_size;
-        }
-        calculate_new_heap_count ();
-    }
-#endif //DYNAMIC_HEAP_COUNT
-}
-void gc_heap::update_end_ngc_time()
-{
-    end_gc_time = GetHighPrecisionTimeStamp();
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-    last_gc_end_time_us = end_gc_time;
-    dprintf (HEAP_BALANCE_LOG, ("[GC#%zd-%zd-%zd]", settings.gc_index,
-        (last_gc_end_time_us - dd_time_clock (dynamic_data_of (0))),
-        dd_time_clock (dynamic_data_of (0))));
-#endif //HEAP_BALANCE_INSTRUMENTATION
-}
-size_t gc_heap::exponential_smoothing (int gen, size_t collection_count, size_t desired_per_heap)
-{
-    size_t smoothing = min(3, collection_count);
-    size_t desired_total = desired_per_heap * n_heaps;
-    size_t new_smoothed_desired_total = desired_total / smoothing + ((smoothed_desired_total[gen] / smoothing) * (smoothing - 1));
-    smoothed_desired_total[gen] = new_smoothed_desired_total;
-    size_t new_smoothed_desired_per_heap = new_smoothed_desired_total / n_heaps;
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hp = g_heaps[0];
-#else //MULTIPLE_HEAPS
-    gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-    dynamic_data* dd = hp->dynamic_data_of (gen);
-    new_smoothed_desired_per_heap = max (new_smoothed_desired_per_heap, dd_min_size (dd));
-    new_smoothed_desired_per_heap = Align (new_smoothed_desired_per_heap, get_alignment_constant (gen <= soh_gen2));
-    dprintf (2, ("new smoothed_desired_per_heap for gen %d = %zd, desired_per_heap = %zd", gen, new_smoothed_desired_per_heap, desired_per_heap));
-    return new_smoothed_desired_per_heap;
-}
-void gc_heap::gc1()
-{
-#ifdef BACKGROUND_GC
-    assert (settings.concurrent == (uint32_t)(bgc_thread_id.IsCurrentThread()));
-#endif //BACKGROUND_GC
-    verify_soh_segment_list();
-    int n = settings.condemned_generation;
-    if (settings.reason == reason_pm_full_gc)
-    {
-        assert (n == max_generation);
-        init_records();
-        gen_to_condemn_tuning* local_condemn_reasons = &(get_gc_data_per_heap()->gen_to_condemn_reasons);
-        local_condemn_reasons->init();
-        local_condemn_reasons->set_gen (gen_initial, n);
-        local_condemn_reasons->set_gen (gen_final_per_heap, n);
-    }
-    update_collection_counts ();
-#ifdef BACKGROUND_GC
-    bgc_alloc_lock->check();
-#endif //BACKGROUND_GC
-    free_list_info (max_generation, "beginning");
-    vm_heap->GcCondemnedGeneration = settings.condemned_generation;
-    assert (g_gc_card_table == card_table);
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-    assert (g_gc_card_bundle_table == card_bundle_table);
-#endif
-    {
-#ifndef USE_REGIONS
-        if (n == max_generation)
-        {
-            gc_low = lowest_address;
-            gc_high = highest_address;
-        }
-        else
-        {
-            gc_low = generation_allocation_start (generation_of (n));
-            gc_high = heap_segment_reserved (ephemeral_heap_segment);
-        }
-#endif //USE_REGIONS
-#ifdef BACKGROUND_GC
-        if (settings.concurrent)
-        {
-#ifdef TRACE_GC
-            time_bgc_last = GetHighPrecisionTimeStamp();
-#endif //TRACE_GC
-            FIRE_EVENT(BGCBegin);
-            concurrent_print_time_delta ("BGC");
-            concurrent_print_time_delta ("RW");
-            background_mark_phase();
-            free_list_info (max_generation, "after mark phase");
-            background_sweep();
-            free_list_info (max_generation, "after sweep phase");
-        }
-        else
-#endif //BACKGROUND_GC
-        {
-            mark_phase (n, FALSE);
-            check_gen0_bricks();
-            GCScan::GcRuntimeStructuresValid (FALSE);
-            plan_phase (n);
-            GCScan::GcRuntimeStructuresValid (TRUE);
-            check_gen0_bricks();
-        }
-    }
-    for (int gen_number = 0; gen_number <= min (max_generation,n+1); gen_number++)
-    {
-        generation* gn = generation_of (gen_number);
-        if (settings.compaction)
-        {
-            generation_allocation_size (generation_of (gen_number)) += generation_pinned_allocation_compact_size (gn);
-        }
-        else
-        {
-            generation_allocation_size (generation_of (gen_number)) += generation_pinned_allocation_sweep_size (gn);
-        }
-        generation_pinned_allocation_sweep_size (gn) = 0;
-        generation_pinned_allocation_compact_size (gn) = 0;
-    }
-#ifdef BACKGROUND_GC
-    if (settings.concurrent)
-    {
-        dynamic_data* dd = dynamic_data_of (n);
-        end_gc_time = GetHighPrecisionTimeStamp();
-        size_t time_since_last_gen2 = 0;
-#ifdef DYNAMIC_HEAP_COUNT
-        if ((heap_number == 0) && (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes))
-        {
-            time_since_last_gen2 = (size_t)(end_gc_time - (dd_previous_time_clock (dd) + dd_gc_elapsed_time (dd)));
-            dprintf (6666, ("BGC %Id end %I64d - (prev gen2 start %I64d + elapsed %Id = %I64d) = time inbewteen gen2 %Id",
-                dd_gc_clock (dd), end_gc_time, dd_previous_time_clock (dd), dd_gc_elapsed_time (dd), (dd_previous_time_clock (dd) + dd_gc_elapsed_time (dd)), time_since_last_gen2));
-        }
-#endif //DYNAMIC_HEAP_COUNT
-        dd_gc_elapsed_time (dd) = (size_t)(end_gc_time - dd_time_clock (dd));
-#ifdef DYNAMIC_HEAP_COUNT
-        if ((heap_number == 0) && (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes))
-        {
-            dprintf (6666, ("updating BGC %Id elapsed time to %I64d - %I64d = %I64d", dd_gc_clock (dd), end_gc_time, dd_time_clock (dd), dd_gc_elapsed_time (dd)));
-            float bgc_percent = (float)dd_gc_elapsed_time (dd) * 100.0f / (float)time_since_last_gen2;
-            dynamic_heap_count_data.gen2_gc_percents[dynamic_heap_count_data.gen2_sample_index] = bgc_percent;
-            dprintf (6666, ("gen2 sample %d elapsed %Id * 100 / time inbetween gen2 %Id = %.3f",
-                dynamic_heap_count_data.gen2_sample_index, dd_gc_elapsed_time (dd), time_since_last_gen2, bgc_percent));
-            dynamic_heap_count_data.gen2_sample_index = (dynamic_heap_count_data.gen2_sample_index + 1) % dynamic_heap_count_data_t::sample_size;
-            gc_index_full_gc_end = dd_gc_clock (dynamic_data_of (0));
-        }
-#endif //DYNAMIC_HEAP_COUNT
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-        if (heap_number == 0)
-        {
-            last_gc_end_time_us = end_gc_time;
-            dprintf (HEAP_BALANCE_LOG, ("[GC#%zd-%zd-BGC]", settings.gc_index, dd_gc_elapsed_time (dd)));
-        }
-#endif //HEAP_BALANCE_INSTRUMENTATION
-        free_list_info (max_generation, "after computing new dynamic data");
-        gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
-        for (int gen_number = 0; gen_number < max_generation; gen_number++)
-        {
-            dprintf (2, ("end of BGC: gen%d new_alloc: %zd",
-                         gen_number, dd_desired_allocation (dynamic_data_of (gen_number))));
-            current_gc_data_per_heap->gen_data[gen_number].size_after = generation_size (gen_number);
-            current_gc_data_per_heap->gen_data[gen_number].free_list_space_after = generation_free_list_space (generation_of (gen_number));
-            current_gc_data_per_heap->gen_data[gen_number].free_obj_space_after = generation_free_obj_space (generation_of (gen_number));
-        }
-    }
-    else
-#endif //BACKGROUND_GC
-    {
-        free_list_info (max_generation, "end");
-        for (int gen_number = 0; gen_number <= n; gen_number++)
-        {
-            compute_new_dynamic_data (gen_number);
-        }
-        if (n != max_generation)
-        {
-            for (int gen_number = (n + 1); gen_number < total_generation_count; gen_number++)
-            {
-                get_gc_data_per_heap()->gen_data[gen_number].size_after = generation_size (gen_number);
-                get_gc_data_per_heap()->gen_data[gen_number].free_list_space_after = generation_free_list_space (generation_of (gen_number));
-                get_gc_data_per_heap()->gen_data[gen_number].free_obj_space_after = generation_free_obj_space (generation_of (gen_number));
-            }
-        }
-        get_gc_data_per_heap()->maxgen_size_info.running_free_list_efficiency = (uint32_t)(generation_allocator_efficiency (generation_of (max_generation)) * 100);
-        free_list_info (max_generation, "after computing new dynamic data");
-    }
-    if (n < max_generation)
-    {
-        int highest_gen_number =
-#ifdef USE_REGIONS
-            max_generation;
-#else //USE_REGIONS
-            1 + n;
-#endif //USE_REGIONS
-        for (int older_gen_idx = (1 + n); older_gen_idx <= highest_gen_number; older_gen_idx++)
-        {
-            compute_promoted_allocation (older_gen_idx);
-            dynamic_data* dd = dynamic_data_of (older_gen_idx);
-            size_t new_fragmentation = generation_free_list_space (generation_of (older_gen_idx)) +
-                                       generation_free_obj_space (generation_of (older_gen_idx));
-#ifdef BACKGROUND_GC
-            if (current_c_gc_state != c_gc_state_planning)
-#endif //BACKGROUND_GC
-            {
-                if (settings.promotion)
-                {
-                    dd_fragmentation (dd) = new_fragmentation;
-                }
-                else
-                {
-                }
-            }
-        }
-    }
-#ifdef BACKGROUND_GC
-    if (!settings.concurrent)
-#endif //BACKGROUND_GC
-    {
-#ifndef FEATURE_NATIVEAOT
-        assert(GCToEEInterface::IsGCThread());
-#endif // FEATURE_NATIVEAOT
-        adjust_ephemeral_limits();
-    }
-#if defined(BACKGROUND_GC) && !defined(USE_REGIONS)
-    assert (ephemeral_low == generation_allocation_start (generation_of ( max_generation -1)));
-    assert (ephemeral_high == heap_segment_reserved (ephemeral_heap_segment));
-#endif //BACKGROUND_GC && !USE_REGIONS
-    if (fgn_maxgen_percent)
-    {
-        if (settings.condemned_generation == (max_generation - 1))
-        {
-            check_for_full_gc (max_generation - 1, 0);
-        }
-        else if (settings.condemned_generation == max_generation)
-        {
-            if (full_gc_approach_event_set
-#ifdef MULTIPLE_HEAPS
-                && (heap_number == 0)
-#endif //MULTIPLE_HEAPS
-                )
-            {
-                dprintf (2, ("FGN-GC: setting gen2 end event"));
-                full_gc_approach_event.Reset();
-#ifdef BACKGROUND_GC
-                fgn_last_gc_was_concurrent = settings.concurrent ? TRUE : FALSE;
-#endif //BACKGROUND_GC
-                full_gc_end_event.Set();
-                full_gc_approach_event_set = false;
-            }
-        }
-    }
-#ifdef BACKGROUND_GC
-    if (!settings.concurrent)
-#endif //BACKGROUND_GC
-    {
-        if (alloc_contexts_used >= 1)
-        {
-            allocation_quantum = Align (min ((size_t)CLR_SIZE,
-                                            (size_t)max (1024, get_new_allocation (0) / (2 * alloc_contexts_used))),
-                                            get_alignment_constant(FALSE));
-            dprintf (3, ("New allocation quantum: %zd(0x%zx)", allocation_quantum, allocation_quantum));
-        }
-    }
-#ifdef USE_REGIONS
-    if (end_gen0_region_space == uninitialized_end_gen0_region_space)
-    {
-        end_gen0_region_space = get_gen0_end_space (memory_type_reserved);
-    }
-#endif //USE_REGIONS
-    descr_generations ("END");
-    verify_soh_segment_list();
-#ifdef BACKGROUND_GC
-    if (gc_can_use_concurrent)
-    {
-        check_bgc_mark_stack_length();
-    }
-    assert (settings.concurrent == (uint32_t)(bgc_thread_id.IsCurrentThread()));
-#endif //BACKGROUND_GC
-#if defined(VERIFY_HEAP) || (defined (FEATURE_EVENT_TRACE) && defined(BACKGROUND_GC))
-    if (FALSE
-#ifdef VERIFY_HEAP
-        || (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
-#endif
-#if defined(FEATURE_EVENT_TRACE) && defined(BACKGROUND_GC)
-        || (bgc_heap_walk_for_etw_p && settings.concurrent)
-#endif
-        )
-    {
-#ifdef BACKGROUND_GC
-        bool cooperative_mode = true;
-        if (settings.concurrent)
-        {
-            cooperative_mode = enable_preemptive ();
-#ifdef MULTIPLE_HEAPS
-            bgc_t_join.join(this, gc_join_suspend_ee_verify);
-            if (bgc_t_join.joined())
-            {
-                bgc_threads_sync_event.Reset();
-                dprintf(2, ("Joining BGC threads to suspend EE for verify heap"));
-                bgc_t_join.restart();
-            }
-            if (heap_number == 0)
-            {
-                enter_gc_lock_for_verify_heap();
-                suspend_EE();
-                bgc_threads_sync_event.Set();
-            }
-            else
-            {
-                bgc_threads_sync_event.Wait(INFINITE, FALSE);
-                dprintf (2, ("bgc_threads_sync_event is signalled"));
-            }
-#else //MULTIPLE_HEAPS
-            enter_gc_lock_for_verify_heap();
-            suspend_EE();
-#endif //MULTIPLE_HEAPS
-            fix_allocation_contexts (FALSE);
-        }
-#endif //BACKGROUND_GC
-#ifdef BACKGROUND_GC
-        assert (settings.concurrent == (uint32_t)(bgc_thread_id.IsCurrentThread()));
-#ifdef FEATURE_EVENT_TRACE
-        if (bgc_heap_walk_for_etw_p && settings.concurrent)
-        {
-            GCToEEInterface::DiagWalkBGCSurvivors(__this);
-#ifdef MULTIPLE_HEAPS
-            bgc_t_join.join(this, gc_join_after_profiler_heap_walk);
-            if (bgc_t_join.joined())
-            {
-                bgc_t_join.restart();
-            }
-#endif // MULTIPLE_HEAPS
-        }
-#endif // FEATURE_EVENT_TRACE
-#endif //BACKGROUND_GC
-#ifdef VERIFY_HEAP
-        if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
-            verify_heap (FALSE);
-#endif // VERIFY_HEAP
-#ifdef BACKGROUND_GC
-        if (settings.concurrent)
-        {
-            repair_allocation_contexts (TRUE);
-#ifdef MULTIPLE_HEAPS
-            bgc_t_join.join(this, gc_join_restart_ee_verify);
-            if (bgc_t_join.joined())
-            {
-                bgc_threads_sync_event.Reset();
-                dprintf(2, ("Joining BGC threads to restart EE after verify heap"));
-                bgc_t_join.restart();
-            }
-            if (heap_number == 0)
-            {
-                restart_EE();
-                leave_gc_lock_for_verify_heap();
-                bgc_threads_sync_event.Set();
-            }
-            else
-            {
-                bgc_threads_sync_event.Wait(INFINITE, FALSE);
-                dprintf (2, ("bgc_threads_sync_event is signalled"));
-            }
-#else //MULTIPLE_HEAPS
-            restart_EE();
-            leave_gc_lock_for_verify_heap();
-#endif //MULTIPLE_HEAPS
-            disable_preemptive (cooperative_mode);
-        }
-#endif //BACKGROUND_GC
-    }
-#endif //VERIFY_HEAP || (FEATURE_EVENT_TRACE && BACKGROUND_GC)
-#ifdef MULTIPLE_HEAPS
-    if (!settings.concurrent)
-    {
-        gc_t_join.join(this, gc_join_done);
-        if (gc_t_join.joined ())
-        {
-            gc_heap::internal_gc_done = false;
-            int limit = settings.condemned_generation;
-            if (limit == max_generation)
-            {
-                limit = total_generation_count-1;
-            }
-            for (int gen = 0; gen <= limit; gen++)
-            {
-                size_t total_desired = 0;
-                size_t total_already_consumed = 0;
-                for (int i = 0; i < gc_heap::n_heaps; i++)
-                {
-                    gc_heap* hp = gc_heap::g_heaps[i];
-                    dynamic_data* dd = hp->dynamic_data_of (gen);
-                    size_t temp_total_desired = total_desired + dd_desired_allocation (dd);
-                    if (temp_total_desired < total_desired)
-                    {
-                        total_desired = (size_t)MAX_PTR;
-                        break;
-                    }
-                    total_desired = temp_total_desired;
-                    assert ((ptrdiff_t)dd_desired_allocation (dd) >= dd_new_allocation (dd));
-                    size_t already_consumed = dd_desired_allocation (dd) - dd_new_allocation (dd);
-                    size_t temp_total_already_consumed = total_already_consumed + already_consumed;
-                    assert (temp_total_already_consumed >= total_already_consumed);
-                    total_already_consumed = temp_total_already_consumed;
-                }
-                size_t desired_per_heap = Align (total_desired/gc_heap::n_heaps,
-                                                    get_alignment_constant (gen <= max_generation));
-                size_t already_consumed_per_heap = total_already_consumed / gc_heap::n_heaps;
-                if (gen == 0)
-                {
-#if 1 //subsumed by the linear allocation model
-                    desired_per_heap = exponential_smoothing (gen, dd_collection_count (dynamic_data_of(gen)), desired_per_heap);
-#endif //0
-                    if (!heap_hard_limit)
-                    {
-                        gc_heap* hp = gc_heap::g_heaps[0];
-                        dynamic_data* dd = hp->dynamic_data_of (gen);
-                        size_t min_gc_size = dd_min_size(dd);
-                        if ((min_gc_size <= GCToOSInterface::GetCacheSizePerLogicalCpu(TRUE)) &&
-                            desired_per_heap <= 2*min_gc_size)
-                        {
-                            desired_per_heap = min_gc_size;
-                        }
-                    }
-#ifdef HOST_64BIT
-                    desired_per_heap = joined_youngest_desired (desired_per_heap);
-                    dprintf (2, ("final gen0 new_alloc: %zd", desired_per_heap));
-#endif // HOST_64BIT
-                    gc_data_global.final_youngest_desired = desired_per_heap;
-                }
-#if 1 //subsumed by the linear allocation model
-                if (gen >= uoh_start_generation)
-                {
-                    desired_per_heap = exponential_smoothing (gen, dd_collection_count (dynamic_data_of (max_generation)), desired_per_heap);
-                }
-#endif //0
-                for (int i = 0; i < gc_heap::n_heaps; i++)
-                {
-                    gc_heap* hp = gc_heap::g_heaps[i];
-                    dynamic_data* dd = hp->dynamic_data_of (gen);
-                    dd_desired_allocation (dd) = desired_per_heap;
-                    dd_gc_new_allocation (dd) = desired_per_heap;
-#ifdef USE_REGIONS
-                    dd_new_allocation (dd) = desired_per_heap - already_consumed_per_heap;
-#else //USE_REGIONS
-                    dd_new_allocation (dd) = desired_per_heap;
-#endif //USE_REGIONS
-                    if (gen == 0)
-                    {
-                        hp->fgn_last_alloc = desired_per_heap;
-                    }
-                }
-            }
-#ifdef FEATURE_LOH_COMPACTION
-            BOOL all_heaps_compacted_p = TRUE;
-#endif //FEATURE_LOH_COMPACTION
-            int max_gen0_must_clear_bricks = 0;
-            for (int i = 0; i < gc_heap::n_heaps; i++)
-            {
-                gc_heap* hp = gc_heap::g_heaps[i];
-                hp->decommit_ephemeral_segment_pages();
-                hp->rearrange_uoh_segments();
-#ifdef FEATURE_LOH_COMPACTION
-                all_heaps_compacted_p &= hp->loh_compacted_p;
-#endif //FEATURE_LOH_COMPACTION
-                max_gen0_must_clear_bricks = max(max_gen0_must_clear_bricks, hp->gen0_must_clear_bricks);
-            }
-#ifdef USE_REGIONS
-            initGCShadow();
-            distribute_free_regions();
-            verify_region_to_generation_map ();
-            compute_gc_and_ephemeral_range (settings.condemned_generation, true);
-            stomp_write_barrier_ephemeral (ephemeral_low, ephemeral_high,
-                                           map_region_to_generation_skewed, (uint8_t)min_segment_size_shr);
-#endif //USE_REGIONS
-#ifdef FEATURE_LOH_COMPACTION
-            check_loh_compact_mode (all_heaps_compacted_p);
-#endif //FEATURE_LOH_COMPACTION
-            if (max_gen0_must_clear_bricks > 0)
-            {
-                for (int i = 0; i < gc_heap::n_heaps; i++)
-                {
-                    gc_heap* hp = gc_heap::g_heaps[i];
-                    hp->gen0_must_clear_bricks = max_gen0_must_clear_bricks;
-                }
-            }
-            for (int i = 0; i < gc_heap::n_heaps; i++)
-            {
-                g_heaps[i]->descr_generations ("END");
-#ifdef USE_REGIONS
-                if (settings.condemned_generation == max_generation)
-                {
-                    region_free_list::age_free_regions (g_heaps[i]->free_regions);
-                    region_free_list::print (g_heaps[i]->free_regions, i, "END");
-                }
-                else
-                {
-                    g_heaps[i]->free_regions[basic_free_region].age_free_regions();
-                    g_heaps[i]->free_regions[basic_free_region].print (i, "END");
-                }
-#endif //USE_REGIONS
-            }
-            fire_pevents();
-            update_end_ngc_time();
-            pm_full_gc_init_or_clear();
-            gc_t_join.restart();
-        }
-        update_end_gc_time_per_heap();
-        add_to_history_per_heap();
-        alloc_context_count = 0;
-        heap_select::mark_heap (heap_number);
-    }
-#else //MULTIPLE_HEAPS
-    gc_data_global.final_youngest_desired =
-        dd_desired_allocation (dynamic_data_of (0));
-#ifdef FEATURE_LOH_COMPACTION
-    check_loh_compact_mode (loh_compacted_p);
-#endif //FEATURE_LOH_COMPACTION
-    decommit_ephemeral_segment_pages();
-    fire_pevents();
-    if (!(settings.concurrent))
-    {
-        rearrange_uoh_segments();
-#ifdef USE_REGIONS
-        initGCShadow();
-        distribute_free_regions();
-        verify_region_to_generation_map ();
-        compute_gc_and_ephemeral_range (settings.condemned_generation, true);
-        stomp_write_barrier_ephemeral (ephemeral_low, ephemeral_high,
-                                        map_region_to_generation_skewed, (uint8_t)min_segment_size_shr);
-        if (settings.condemned_generation == max_generation)
-        {
-            region_free_list::age_free_regions(free_regions);
-            region_free_list::print(free_regions, 0, "END");
-        }
-        else
-        {
-            free_regions[basic_free_region].age_free_regions();
-            free_regions[basic_free_region].print (0, "END");
-        }
-#endif //USE_REGIONS
-        update_end_ngc_time();
-        update_end_gc_time_per_heap();
-        add_to_history_per_heap();
-        do_post_gc();
-    }
-    pm_full_gc_init_or_clear();
-#ifdef BACKGROUND_GC
-    recover_bgc_settings();
-#endif //BACKGROUND_GC
-#endif //MULTIPLE_HEAPS
-#ifdef USE_REGIONS
-    if (!(settings.concurrent) && (settings.condemned_generation == max_generation))
-    {
-        last_gc_before_oom = FALSE;
-    }
-#endif //USE_REGIONS
-}
-#ifdef DYNAMIC_HEAP_COUNT
-bool gc_heap::prepare_rethread_fl_items()
-{
-    if (!min_fl_list)
-    {
-        min_fl_list = new (nothrow) min_fl_list_info [MAX_BUCKET_COUNT * n_max_heaps];
-        if (min_fl_list == nullptr)
-            return false;
-    }
-    if (!free_list_space_per_heap)
-    {
-        free_list_space_per_heap = new (nothrow) size_t[n_max_heaps];
-        if (free_list_space_per_heap == nullptr)
-            return false;
-    }
-    return true;
-}
-void gc_heap::rethread_fl_items(int gen_idx)
-{
-    uint32_t min_fl_list_size = sizeof (min_fl_list_info) * (MAX_BUCKET_COUNT * n_max_heaps);
-    memset (min_fl_list, 0, min_fl_list_size);
-    memset (free_list_space_per_heap, 0, sizeof(free_list_space_per_heap[0])*n_max_heaps);
-    size_t num_fl_items = 0;
-    size_t num_fl_items_rethreaded = 0;
-    allocator* gen_allocator = generation_allocator (generation_of (gen_idx));
-    gen_allocator->rethread_items (&num_fl_items, &num_fl_items_rethreaded, this, min_fl_list, free_list_space_per_heap, n_heaps);
-    num_fl_items_rethreaded_stage2 = num_fl_items_rethreaded;
-}
-void gc_heap::merge_fl_from_other_heaps (int gen_idx, int to_n_heaps, int from_n_heaps)
-{
-#ifdef _DEBUG
-    uint64_t start_us = GetHighPrecisionTimeStamp ();
-    size_t total_num_fl_items_rethreaded_stage2 = 0;
-    for (int hn = 0; hn < to_n_heaps; hn++)
-    {
-        gc_heap* hp = g_heaps[hn];
-        total_num_fl_items_rethreaded_stage2 += hp->num_fl_items_rethreaded_stage2;
-        min_fl_list_info* current_heap_min_fl_list = hp->min_fl_list;
-        allocator* gen_allocator = generation_allocator (hp->generation_of (gen_idx));
-        int num_buckets = gen_allocator->number_of_buckets();
-        for (int i = 0; i < num_buckets; i++)
-        {
-            min_fl_list_info* current_bucket_min_fl_list = current_heap_min_fl_list + (i * to_n_heaps);
-            for (int other_hn = 0; other_hn < from_n_heaps; other_hn++)
-            {
-                min_fl_list_info* min_fl_other_heap = &current_bucket_min_fl_list[other_hn];
-                if (min_fl_other_heap->head)
-                {
-                    if (other_hn == hn)
-                    {
-                        dprintf (8888, ("h%d has fl items for itself on the temp list?!", hn));
-                        GCToOSInterface::DebugBreak ();
-                    }
-                }
-            }
-        }
-    }
-    uint64_t elapsed = GetHighPrecisionTimeStamp () - start_us;
-    dprintf (8888, ("rethreaded %Id items, merging took %I64dus (%I64dms)",
-        total_num_fl_items_rethreaded_stage2, elapsed, (elapsed / 1000)));
-#endif //_DEBUG
-    for (int hn = 0; hn < to_n_heaps; hn++)
-    {
-        gc_heap* hp = g_heaps[hn];
-        generation* gen = hp->generation_of (gen_idx);
-        dynamic_data* dd = hp->dynamic_data_of (gen_idx);
-        allocator* gen_allocator = generation_allocator (gen);
-        gen_allocator->merge_items (hp, to_n_heaps, from_n_heaps);
-        size_t free_list_space_decrease = 0;
-        if (hn < from_n_heaps)
-        {
-            assert (hp->free_list_space_per_heap[hn] == 0);
-            for (int to_hn = 0; to_hn < to_n_heaps; to_hn++)
-            {
-                free_list_space_decrease += hp->free_list_space_per_heap[to_hn];
-            }
-        }
-        dprintf (8888, ("heap %d gen %d %zd total free list space, %zd moved to other heaps",
-            hn,
-            gen_idx,
-            generation_free_list_space (gen),
-            free_list_space_decrease));
-        assert (free_list_space_decrease <= generation_free_list_space (gen));
-        generation_free_list_space (gen) -= free_list_space_decrease;
-        if (gen_idx != max_generation)
-        {
-            assert (free_list_space_decrease <= dd_fragmentation (dd));
-        }
-        size_t free_list_space_increase = 0;
-        for (int from_hn = 0; from_hn < from_n_heaps; from_hn++)
-        {
-            gc_heap* from_hp = g_heaps[from_hn];
-            free_list_space_increase += from_hp->free_list_space_per_heap[hn];
-        }
-        dprintf (8888, ("heap %d gen %d %zd free list space moved from other heaps", hn, gen_idx, free_list_space_increase));
-        generation_free_list_space (gen) += free_list_space_increase;
-    }
-#ifdef _DEBUG
-    size_t total_fl_items_count = 0;
-    size_t total_fl_items_for_oh_count = 0;
-    for (int hn = 0; hn < to_n_heaps; hn++)
-    {
-        gc_heap* hp = g_heaps[hn];
-        allocator* gen_allocator = generation_allocator (hp->generation_of (gen_idx));
-        size_t fl_items_count = 0;
-        size_t fl_items_for_oh_count = 0;
-        gen_allocator->count_items (hp, &fl_items_count, &fl_items_for_oh_count);
-        total_fl_items_count += fl_items_count;
-        total_fl_items_for_oh_count += fl_items_for_oh_count;
-    }
-    dprintf (8888, ("total %Id fl items, %Id are for other heaps",
-        total_fl_items_count, total_fl_items_for_oh_count));
-    if (total_fl_items_for_oh_count)
-    {
-        GCToOSInterface::DebugBreak ();
-    }
-#endif //_DEBUG
-}
-#endif //DYNAMIC_HEAP_COUNT
-void gc_heap::save_data_for_no_gc()
-{
-    current_no_gc_region_info.saved_pause_mode = settings.pause_mode;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-    {
-        current_no_gc_region_info.saved_gen0_min_size = dd_min_size (g_heaps[i]->dynamic_data_of (0));
-        dd_min_size (g_heaps[i]->dynamic_data_of (0)) = min_balance_threshold;
-        current_no_gc_region_info.saved_gen3_min_size = dd_min_size (g_heaps[i]->dynamic_data_of (loh_generation));
-        dd_min_size (g_heaps[i]->dynamic_data_of (loh_generation)) = 0;
-    }
-#endif //MULTIPLE_HEAPS
-}
-void gc_heap::restore_data_for_no_gc()
-{
-    gc_heap::settings.pause_mode = current_no_gc_region_info.saved_pause_mode;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-    {
-        dd_min_size (g_heaps[i]->dynamic_data_of (0)) = current_no_gc_region_info.saved_gen0_min_size;
-        dd_min_size (g_heaps[i]->dynamic_data_of (loh_generation)) = current_no_gc_region_info.saved_gen3_min_size;
-    }
-#endif //MULTIPLE_HEAPS
-}
-start_no_gc_region_status gc_heap::prepare_for_no_gc_region (uint64_t total_size,
-                                                             BOOL loh_size_known,
-                                                             uint64_t loh_size,
-                                                             BOOL disallow_full_blocking)
-{
-    if (current_no_gc_region_info.started)
-    {
-        return start_no_gc_in_progress;
-    }
-    start_no_gc_region_status status = start_no_gc_success;
-    save_data_for_no_gc();
-    settings.pause_mode = pause_no_gc;
-    current_no_gc_region_info.start_status = start_no_gc_success;
-    uint64_t allocation_no_gc_loh = 0;
-    uint64_t allocation_no_gc_soh = 0;
-    assert(total_size != 0);
-    if (loh_size_known)
-    {
-        assert(loh_size != 0);
-        assert(loh_size <= total_size);
-        allocation_no_gc_loh = loh_size;
-        allocation_no_gc_soh = total_size - loh_size;
-    }
-    else
-    {
-        allocation_no_gc_soh = total_size;
-        allocation_no_gc_loh = total_size;
-    }
-    int soh_align_const = get_alignment_constant (TRUE);
-#ifdef USE_REGIONS
-    size_t max_soh_allocated = SIZE_T_MAX;
-#else
-    size_t max_soh_allocated = soh_segment_size - segment_info_size - eph_gen_starts_size;
-#endif
-    size_t size_per_heap = 0;
-    const double scale_factor = 1.05;
-    int num_heaps = get_num_heaps();
-    uint64_t total_allowed_soh_allocation = (uint64_t)max_soh_allocated * num_heaps;
-    assert(total_allowed_soh_allocation <= SIZE_T_MAX);
-    uint64_t total_allowed_loh_allocation = SIZE_T_MAX;
-    uint64_t total_allowed_soh_alloc_scaled = allocation_no_gc_soh > 0 ? static_cast<uint64_t>(total_allowed_soh_allocation / scale_factor) : 0;
-    uint64_t total_allowed_loh_alloc_scaled = allocation_no_gc_loh > 0 ? static_cast<uint64_t>(total_allowed_loh_allocation / scale_factor) : 0;
-    if (allocation_no_gc_soh > total_allowed_soh_alloc_scaled ||
-        allocation_no_gc_loh > total_allowed_loh_alloc_scaled)
-    {
-        status = start_no_gc_too_large;
-        goto done;
-    }
-    if (allocation_no_gc_soh > 0)
-    {
-        allocation_no_gc_soh = static_cast<uint64_t>(allocation_no_gc_soh * scale_factor);
-        allocation_no_gc_soh = min (allocation_no_gc_soh, total_allowed_soh_alloc_scaled);
-    }
-    if (allocation_no_gc_loh > 0)
-    {
-        allocation_no_gc_loh = static_cast<uint64_t>(allocation_no_gc_loh * scale_factor);
-        allocation_no_gc_loh = min (allocation_no_gc_loh, total_allowed_loh_alloc_scaled);
-    }
-    if (disallow_full_blocking)
-        current_no_gc_region_info.minimal_gc_p = TRUE;
-    if (allocation_no_gc_soh != 0)
-    {
-        current_no_gc_region_info.soh_allocation_size = (size_t)allocation_no_gc_soh;
-        size_per_heap = current_no_gc_region_info.soh_allocation_size;
-#ifdef MULTIPLE_HEAPS
-        size_per_heap /= n_heaps;
-        for (int i = 0; i < n_heaps; i++)
-        {
-            g_heaps[i]->soh_allocation_no_gc = min (Align ((size_per_heap + min_balance_threshold), soh_align_const), max_soh_allocated);
-        }
-#else //MULTIPLE_HEAPS
-        soh_allocation_no_gc = min (Align (size_per_heap, soh_align_const), max_soh_allocated);
-#endif //MULTIPLE_HEAPS
-    }
-    if (allocation_no_gc_loh != 0)
-    {
-        current_no_gc_region_info.loh_allocation_size = (size_t)allocation_no_gc_loh;
-        size_per_heap = current_no_gc_region_info.loh_allocation_size;
-#ifdef MULTIPLE_HEAPS
-        size_per_heap /= n_heaps;
-        for (int i = 0; i < n_heaps; i++)
-            g_heaps[i]->loh_allocation_no_gc = Align (size_per_heap, get_alignment_constant (FALSE));
-#else //MULTIPLE_HEAPS
-        loh_allocation_no_gc = Align (size_per_heap, get_alignment_constant (FALSE));
-#endif //MULTIPLE_HEAPS
-    }
-done:
-    if (status != start_no_gc_success)
-        restore_data_for_no_gc();
-    return status;
-}
-void gc_heap::handle_failure_for_no_gc()
-{
-    gc_heap::restore_data_for_no_gc();
-    memset (&current_no_gc_region_info, 0, sizeof (current_no_gc_region_info));
-}
-start_no_gc_region_status gc_heap::get_start_no_gc_region_status()
-{
-    return current_no_gc_region_info.start_status;
-}
-void gc_heap::record_gcs_during_no_gc()
-{
-    if (current_no_gc_region_info.started)
-    {
-        current_no_gc_region_info.num_gcs++;
-        if (is_induced (settings.reason))
-            current_no_gc_region_info.num_gcs_induced++;
-    }
-}
-BOOL gc_heap::find_loh_free_for_no_gc()
-{
-    allocator* loh_allocator = generation_allocator (generation_of (loh_generation));
-    size_t size = loh_allocation_no_gc;
-    for (unsigned int a_l_idx = loh_allocator->first_suitable_bucket(size); a_l_idx < loh_allocator->number_of_buckets(); a_l_idx++)
-    {
-        uint8_t* free_list = loh_allocator->alloc_list_head_of (a_l_idx);
-        while (free_list)
-        {
-            size_t free_list_size = unused_array_size(free_list);
-            if (free_list_size > size)
-            {
-                dprintf (3, ("free item %zx(%zd) for no gc", (size_t)free_list, free_list_size));
-                return TRUE;
-            }
-            free_list = free_list_slot (free_list);
-        }
-    }
-    return FALSE;
-}
-BOOL gc_heap::find_loh_space_for_no_gc()
-{
-    saved_loh_segment_no_gc = 0;
-    if (find_loh_free_for_no_gc())
-        return TRUE;
-    heap_segment* seg = generation_allocation_segment (generation_of (loh_generation));
-    while (seg)
-    {
-        size_t remaining = heap_segment_reserved (seg) - heap_segment_allocated (seg);
-        if (remaining >= loh_allocation_no_gc)
-        {
-            saved_loh_segment_no_gc = seg;
-            break;
-        }
-        seg = heap_segment_next (seg);
-    }
-    if (!saved_loh_segment_no_gc && current_no_gc_region_info.minimal_gc_p)
-    {
-        saved_loh_segment_no_gc = get_segment_for_uoh (loh_generation, get_uoh_seg_size (loh_allocation_no_gc)
-#ifdef MULTIPLE_HEAPS
-                                                      , this
-#endif //MULTIPLE_HEAPS
-                                                      );
-    }
-    return (saved_loh_segment_no_gc != 0);
-}
-BOOL gc_heap::loh_allocated_for_no_gc()
-{
-    if (!saved_loh_segment_no_gc)
-        return FALSE;
-    heap_segment* seg = generation_allocation_segment (generation_of (loh_generation));
-    do
-    {
-        if (seg == saved_loh_segment_no_gc)
-        {
-            return FALSE;
-        }
-        seg = heap_segment_next (seg);
-    } while (seg);
-    return TRUE;
-}
-BOOL gc_heap::commit_loh_for_no_gc (heap_segment* seg)
-{
-    uint8_t* end_committed = heap_segment_allocated (seg) + loh_allocation_no_gc;
-    assert (end_committed <= heap_segment_reserved (seg));
-    return (grow_heap_segment (seg, end_committed));
-}
-void gc_heap::thread_no_gc_loh_segments()
-{
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap* hp = g_heaps[i];
-        if (hp->loh_allocated_for_no_gc())
-        {
-            hp->thread_uoh_segment (loh_generation, hp->saved_loh_segment_no_gc);
-            hp->saved_loh_segment_no_gc = 0;
-        }
-    }
-#else //MULTIPLE_HEAPS
-    if (loh_allocated_for_no_gc())
-    {
-        thread_uoh_segment (loh_generation, saved_loh_segment_no_gc);
-        saved_loh_segment_no_gc = 0;
-    }
-#endif //MULTIPLE_HEAPS
-}
-void gc_heap::set_loh_allocations_for_no_gc()
-{
-    if (current_no_gc_region_info.loh_allocation_size != 0)
-    {
-        dynamic_data* dd = dynamic_data_of (loh_generation);
-        dd_new_allocation (dd) = loh_allocation_no_gc;
-        dd_gc_new_allocation (dd) = dd_new_allocation (dd);
-    }
-}
-void gc_heap::set_soh_allocations_for_no_gc()
-{
-    if (current_no_gc_region_info.soh_allocation_size != 0)
-    {
-        dynamic_data* dd = dynamic_data_of (0);
-        dd_new_allocation (dd) = soh_allocation_no_gc;
-        dd_gc_new_allocation (dd) = dd_new_allocation (dd);
-#ifdef MULTIPLE_HEAPS
-        alloc_context_count = 0;
-#endif //MULTIPLE_HEAPS
-    }
-}
-void gc_heap::set_allocations_for_no_gc()
-{
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap* hp = g_heaps[i];
-        hp->set_loh_allocations_for_no_gc();
-        hp->set_soh_allocations_for_no_gc();
-    }
-#else //MULTIPLE_HEAPS
-    set_loh_allocations_for_no_gc();
-    set_soh_allocations_for_no_gc();
-#endif //MULTIPLE_HEAPS
-}
-BOOL gc_heap::should_proceed_for_no_gc()
-{
-    BOOL gc_requested = FALSE;
-    BOOL loh_full_gc_requested = FALSE;
-    BOOL soh_full_gc_requested = FALSE;
-    BOOL no_gc_requested = FALSE;
-    BOOL get_new_loh_segments = FALSE;
-#ifdef MULTIPLE_HEAPS
-    gradual_decommit_in_progress_p = FALSE;
-#endif //MULTIPLE_HEAPS
-    gc_heap* hp = nullptr;
-    if (current_no_gc_region_info.soh_allocation_size)
-    {
-#ifdef USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            hp = g_heaps[i];
-#else
-        {
-            hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-            if (!hp->extend_soh_for_no_gc())
-            {
-                soh_full_gc_requested = TRUE;
-#ifdef MULTIPLE_HEAPS
-                break;
-#endif //MULTIPLE_HEAPS
-            }
-        }
-#else //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            hp = g_heaps[i];
-#else //MULTIPLE_HEAPS
-        {
-            hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-            size_t reserved_space = heap_segment_reserved (hp->ephemeral_heap_segment) - hp->alloc_allocated;
-            if (reserved_space < hp->soh_allocation_no_gc)
-            {
-                gc_requested = TRUE;
-#ifdef MULTIPLE_HEAPS
-                break;
-#endif //MULTIPLE_HEAPS
-            }
-        }
-        if (!gc_requested)
-        {
-#ifdef MULTIPLE_HEAPS
-            for (int i = 0; i < n_heaps; i++)
-            {
-                hp = g_heaps[i];
-#else //MULTIPLE_HEAPS
-            {
-                hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-                if (!(hp->grow_heap_segment (hp->ephemeral_heap_segment, (hp->alloc_allocated + hp->soh_allocation_no_gc))))
-                {
-                    soh_full_gc_requested = TRUE;
-#ifdef MULTIPLE_HEAPS
-                    break;
-#endif //MULTIPLE_HEAPS
-                }
-            }
-        }
-#endif //USE_REGIONS
-    }
-    if (!current_no_gc_region_info.minimal_gc_p && gc_requested)
-    {
-        soh_full_gc_requested = TRUE;
-    }
-    no_gc_requested = !(soh_full_gc_requested || gc_requested);
-    if (soh_full_gc_requested && current_no_gc_region_info.minimal_gc_p)
-    {
-        current_no_gc_region_info.start_status = start_no_gc_no_memory;
-        goto done;
-    }
-    if (!soh_full_gc_requested && current_no_gc_region_info.loh_allocation_size)
-    {
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-            if (!hp->find_loh_space_for_no_gc())
-            {
-                loh_full_gc_requested = TRUE;
-                break;
-            }
-        }
-#else //MULTIPLE_HEAPS
-        if (!find_loh_space_for_no_gc())
-            loh_full_gc_requested = TRUE;
-#endif //MULTIPLE_HEAPS
-        if (!loh_full_gc_requested)
-        {
-#ifdef MULTIPLE_HEAPS
-            for (int i = 0; i < n_heaps; i++)
-            {
-                gc_heap* hp = g_heaps[i];
-                if (hp->saved_loh_segment_no_gc &&!hp->commit_loh_for_no_gc (hp->saved_loh_segment_no_gc))
-                {
-                    loh_full_gc_requested = TRUE;
-                    break;
-                }
-            }
-#else //MULTIPLE_HEAPS
-            if (saved_loh_segment_no_gc && !commit_loh_for_no_gc (saved_loh_segment_no_gc))
-                loh_full_gc_requested = TRUE;
-#endif //MULTIPLE_HEAPS
-        }
-    }
-    if (loh_full_gc_requested || soh_full_gc_requested)
-    {
-        if (current_no_gc_region_info.minimal_gc_p)
-            current_no_gc_region_info.start_status = start_no_gc_no_memory;
-    }
-    no_gc_requested = !(loh_full_gc_requested || soh_full_gc_requested || gc_requested);
-    if (current_no_gc_region_info.start_status == start_no_gc_success)
-    {
-        if (no_gc_requested)
-            set_allocations_for_no_gc();
-    }
-done:
-    if ((current_no_gc_region_info.start_status == start_no_gc_success) && !no_gc_requested)
-        return TRUE;
-    else
-    {
-        current_no_gc_region_info.started = TRUE;
-        return FALSE;
-    }
-}
-end_no_gc_region_status gc_heap::end_no_gc_region()
-{
-    dprintf (1, ("end no gc called"));
-    end_no_gc_region_status status = end_no_gc_success;
-    if (!(current_no_gc_region_info.started))
-        status = end_no_gc_not_in_progress;
-    if (current_no_gc_region_info.num_gcs_induced)
-        status = end_no_gc_induced;
-    else if (current_no_gc_region_info.num_gcs)
-        status = end_no_gc_alloc_exceeded;
-    if (settings.pause_mode == pause_no_gc)
-    {
-        restore_data_for_no_gc();
-        if (current_no_gc_region_info.callback != nullptr)
-        {
-            dprintf (1, ("[no_gc_callback] detaching callback on exit"));
-            schedule_no_gc_callback (true);
-        }
-    }
-    memset (&current_no_gc_region_info, 0, sizeof (current_no_gc_region_info));
-    return status;
-}
-void gc_heap::schedule_no_gc_callback (bool abandoned)
-{
-    current_no_gc_region_info.callback->abandoned = abandoned;
-    if (!current_no_gc_region_info.callback->scheduled)
-    {
-        current_no_gc_region_info.callback->scheduled = true;
-        schedule_finalizer_work(current_no_gc_region_info.callback);
-    }
-}
-void gc_heap::schedule_finalizer_work (FinalizerWorkItem* callback)
-{
-    FinalizerWorkItem* prev;
-    do
-    {
-        prev = finalizer_work;
-        callback->next = prev;
-    }
-    while (Interlocked::CompareExchangePointer (&finalizer_work, callback, prev) != prev);
-    if (prev == nullptr)
-    {
-        GCToEEInterface::EnableFinalization(true);
-    }
-}
-void gc_heap::update_collection_counts ()
-{
-    dynamic_data* dd0 = dynamic_data_of (0);
-    dd_gc_clock (dd0) += 1;
-    uint64_t now = GetHighPrecisionTimeStamp();
-    for (int i = 0; i <= settings.condemned_generation;i++)
-    {
-        dynamic_data* dd = dynamic_data_of (i);
-        dd_collection_count (dd)++;
-        if (i == max_generation)
-        {
-            dd_collection_count (dynamic_data_of (loh_generation))++;
-            dd_collection_count(dynamic_data_of(poh_generation))++;
-        }
-        dd_gc_clock (dd) = dd_gc_clock (dd0);
-        dd_previous_time_clock (dd) = dd_time_clock (dd);
-        dd_time_clock (dd) = now;
-    }
-}
-#ifdef USE_REGIONS
-bool gc_heap::extend_soh_for_no_gc()
-{
-    size_t required = soh_allocation_no_gc;
-    heap_segment* region = ephemeral_heap_segment;
-    while (true)
-    {
-        uint8_t* allocated = (region == ephemeral_heap_segment) ?
-                             alloc_allocated :
-                             heap_segment_allocated (region);
-        size_t available = heap_segment_reserved (region) - allocated;
-        size_t commit = min (available, required);
-        if (grow_heap_segment (region, allocated + commit))
-        {
-            required -= commit;
-            if (required == 0)
-            {
-                break;
-            }
-            region = heap_segment_next (region);
-            if (region == nullptr)
-            {
-                region = get_new_region (0);
-                if (region == nullptr)
-                {
-                    break;
-                }
-                else
-                {
-                    GCToEEInterface::DiagAddNewRegion(
-                            0,
-                            heap_segment_mem (region),
-                            heap_segment_allocated (region),
-                            heap_segment_reserved (region)
-                        );
-                }
-            }
-        }
-        else
-        {
-            break;
-        }
-    }
-    return (required == 0);
-}
-#else
-BOOL gc_heap::expand_soh_with_minimal_gc()
-{
-    if ((size_t)(heap_segment_reserved (ephemeral_heap_segment) - heap_segment_allocated (ephemeral_heap_segment)) >= soh_allocation_no_gc)
-        return TRUE;
-    heap_segment* new_seg = soh_get_segment_to_expand();
-    if (new_seg)
-    {
-        if (g_gc_card_table != card_table)
-            copy_brick_card_table();
-        settings.promotion = TRUE;
-        settings.demotion = FALSE;
-        ephemeral_promotion = TRUE;
-        int condemned_gen_number = max_generation - 1;
-        int align_const = get_alignment_constant (TRUE);
-        for (int i = 0; i <= condemned_gen_number; i++)
-        {
-            generation* gen = generation_of (i);
-            saved_ephemeral_plan_start[i] = generation_allocation_start (gen);
-            saved_ephemeral_plan_start_size[i] = Align (size (generation_allocation_start (gen)), align_const);
-        }
-        for (size_t b = brick_of (generation_allocation_start (generation_of (0)));
-             b < brick_of (align_on_brick (heap_segment_allocated (ephemeral_heap_segment)));
-             b++)
-        {
-            set_brick (b, -1);
-        }
-        size_t ephemeral_size = (heap_segment_allocated (ephemeral_heap_segment) -
-                                generation_allocation_start (generation_of (max_generation - 1)));
-        heap_segment_next (ephemeral_heap_segment) = new_seg;
-        ephemeral_heap_segment = new_seg;
-        uint8_t*  start = heap_segment_mem (ephemeral_heap_segment);
-        for (int i = condemned_gen_number; i >= 0; i--)
-        {
-            size_t gen_start_size = Align (min_obj_size);
-            make_generation (i, ephemeral_heap_segment, start);
-            generation* gen = generation_of (i);
-            generation_plan_allocation_start (gen) = start;
-            generation_plan_allocation_start_size (gen) = gen_start_size;
-            start += gen_start_size;
-        }
-        heap_segment_used (ephemeral_heap_segment) = start - plug_skew;
-        heap_segment_plan_allocated (ephemeral_heap_segment) = start;
-        fix_generation_bounds (condemned_gen_number, generation_of (0));
-        dd_gc_new_allocation (dynamic_data_of (max_generation)) -= ephemeral_size;
-        dd_new_allocation (dynamic_data_of (max_generation)) = dd_gc_new_allocation (dynamic_data_of (max_generation));
-        adjust_ephemeral_limits();
-        return TRUE;
-    }
-    else
-    {
-        return FALSE;
-    }
-}
-#endif //USE_REGIONS
-void gc_heap::check_and_set_no_gc_oom()
-{
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap* hp = g_heaps[i];
-        if (hp->no_gc_oom_p)
-        {
-            current_no_gc_region_info.start_status = start_no_gc_no_memory;
-            hp->no_gc_oom_p = false;
-        }
-    }
-#else
-    if (no_gc_oom_p)
-    {
-        current_no_gc_region_info.start_status = start_no_gc_no_memory;
-        no_gc_oom_p = false;
-    }
-#endif //MULTIPLE_HEAPS
-}
-void gc_heap::allocate_for_no_gc_after_gc()
-{
-    if (current_no_gc_region_info.minimal_gc_p)
-        repair_allocation_contexts (TRUE);
-    no_gc_oom_p = false;
-    if (current_no_gc_region_info.start_status != start_no_gc_no_memory)
-    {
-        if (current_no_gc_region_info.soh_allocation_size != 0)
-        {
-#ifdef USE_REGIONS
-            no_gc_oom_p = !extend_soh_for_no_gc();
-#else
-            if (((size_t)(heap_segment_reserved (ephemeral_heap_segment) - heap_segment_allocated (ephemeral_heap_segment)) < soh_allocation_no_gc) ||
-                (!grow_heap_segment (ephemeral_heap_segment, (heap_segment_allocated (ephemeral_heap_segment) + soh_allocation_no_gc))))
-            {
-                no_gc_oom_p = true;
-            }
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-            gc_t_join.join(this, gc_join_after_commit_soh_no_gc);
-            if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-            {
-                check_and_set_no_gc_oom();
-#ifdef MULTIPLE_HEAPS
-                gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-            }
-        }
-        if ((current_no_gc_region_info.start_status == start_no_gc_success) &&
-            !(current_no_gc_region_info.minimal_gc_p) &&
-            (current_no_gc_region_info.loh_allocation_size != 0))
-        {
-            gc_policy = policy_compact;
-            saved_loh_segment_no_gc = 0;
-            if (!find_loh_free_for_no_gc())
-            {
-                heap_segment* seg = generation_allocation_segment (generation_of (loh_generation));
-                BOOL found_seg_p = FALSE;
-                while (seg)
-                {
-                    if ((size_t)(heap_segment_reserved (seg) - heap_segment_allocated (seg)) >= loh_allocation_no_gc)
-                    {
-                        found_seg_p = TRUE;
-                        if (!commit_loh_for_no_gc (seg))
-                        {
-                            no_gc_oom_p = true;
-                            break;
-                        }
-                    }
-                    seg = heap_segment_next (seg);
-                }
-                if (!found_seg_p)
-                    gc_policy = policy_expand;
-            }
-#ifdef MULTIPLE_HEAPS
-            gc_t_join.join(this, gc_join_expand_loh_no_gc);
-            if (gc_t_join.joined())
-            {
-                check_and_set_no_gc_oom();
-                if (current_no_gc_region_info.start_status == start_no_gc_success)
-                {
-                    for (int i = 0; i < n_heaps; i++)
-                    {
-                        gc_heap* hp = g_heaps[i];
-                        if (hp->gc_policy == policy_expand)
-                        {
-                            hp->saved_loh_segment_no_gc = get_segment_for_uoh (loh_generation, get_uoh_seg_size (loh_allocation_no_gc), hp);
-                            if (!(hp->saved_loh_segment_no_gc))
-                            {
-                                current_no_gc_region_info.start_status = start_no_gc_no_memory;
-                                break;
-                            }
-                        }
-                    }
-                }
-                gc_t_join.restart();
-            }
-#else //MULTIPLE_HEAPS
-            check_and_set_no_gc_oom();
-            if ((current_no_gc_region_info.start_status == start_no_gc_success) && (gc_policy == policy_expand))
-            {
-                saved_loh_segment_no_gc = get_segment_for_uoh (loh_generation, get_uoh_seg_size (loh_allocation_no_gc));
-                if (!saved_loh_segment_no_gc)
-                    current_no_gc_region_info.start_status = start_no_gc_no_memory;
-            }
-#endif //MULTIPLE_HEAPS
-            if ((current_no_gc_region_info.start_status == start_no_gc_success) && saved_loh_segment_no_gc)
-            {
-                if (!commit_loh_for_no_gc (saved_loh_segment_no_gc))
-                {
-                    no_gc_oom_p = true;
-                }
-            }
-        }
-    }
-#ifdef MULTIPLE_HEAPS
-    gc_t_join.join(this, gc_join_final_no_gc);
-    if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-        check_and_set_no_gc_oom();
-        if (current_no_gc_region_info.start_status == start_no_gc_success)
-        {
-            set_allocations_for_no_gc();
-            current_no_gc_region_info.started = TRUE;
-        }
-#ifdef MULTIPLE_HEAPS
-        gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-}
-void gc_heap::init_records()
-{
-    memset (&gc_data_per_heap, 0, sizeof (gc_data_per_heap));
-    gc_data_per_heap.heap_index = heap_number;
-    if (heap_number == 0)
-        memset (&gc_data_global, 0, sizeof (gc_data_global));
-#ifdef GC_CONFIG_DRIVEN
-    memset (interesting_data_per_gc, 0, sizeof (interesting_data_per_gc));
-#endif //GC_CONFIG_DRIVEN
-    memset (&fgm_result, 0, sizeof (fgm_result));
-    for (int i = 0; i < total_generation_count; i++)
-    {
-        gc_data_per_heap.gen_data[i].size_before = generation_size (i);
-        generation* gen = generation_of (i);
-        gc_data_per_heap.gen_data[i].free_list_space_before = generation_free_list_space (gen);
-        gc_data_per_heap.gen_data[i].free_obj_space_before = generation_free_obj_space (gen);
-    }
-#ifdef USE_REGIONS
-    end_gen0_region_space = uninitialized_end_gen0_region_space;
-    end_gen0_region_committed_space = 0;
-    gen0_pinned_free_space = 0;
-    gen0_large_chunk_found = false;
-    num_regions_freed_in_sweep = 0;
-#endif //USE_REGIONS
-    sufficient_gen0_space_p = FALSE;
-#ifdef MULTIPLE_HEAPS
-    gen0_allocated_after_gc_p = false;
-#endif //MULTIPLE_HEAPS
-#if defined (_DEBUG) && defined (VERIFY_HEAP)
-    verify_pinned_queue_p = FALSE;
-#endif // _DEBUG && VERIFY_HEAP
-}
-void gc_heap::pm_full_gc_init_or_clear()
-{
-    if (settings.condemned_generation == (max_generation - 1))
-    {
-        if (pm_trigger_full_gc)
-        {
-#ifdef MULTIPLE_HEAPS
-            do_post_gc();
-#endif //MULTIPLE_HEAPS
-            dprintf (GTC_LOG, ("init for PM triggered full GC"));
-            uint32_t saved_entry_memory_load = settings.entry_memory_load;
-            settings.init_mechanisms();
-            settings.reason = reason_pm_full_gc;
-            settings.condemned_generation = max_generation;
-            settings.entry_memory_load = saved_entry_memory_load;
-            assert (settings.entry_memory_load > 0);
-            settings.gc_index += 1;
-            do_pre_gc();
-        }
-    }
-    else if (settings.reason == reason_pm_full_gc)
-    {
-        assert (settings.condemned_generation == max_generation);
-        assert (pm_trigger_full_gc);
-        pm_trigger_full_gc = false;
-        dprintf (GTC_LOG, ("PM triggered full GC done"));
-    }
-}
-void gc_heap::garbage_collect_pm_full_gc()
-{
-    assert (settings.condemned_generation == max_generation);
-    assert (settings.reason == reason_pm_full_gc);
-    assert (!settings.concurrent);
-    gc1();
-}
-void gc_heap::garbage_collect (int n)
-{
-    gc_pause_mode saved_settings_pause_mode = settings.pause_mode;
-    alloc_contexts_used = 0;
-    fix_allocation_contexts (TRUE);
-#ifdef MULTIPLE_HEAPS
-#ifdef JOIN_STATS
-    gc_t_join.start_ts(this);
-#endif //JOIN_STATS
-    check_gen0_bricks();
-    clear_gen0_bricks();
-#endif //MULTIPLE_HEAPS
-    if ((settings.pause_mode == pause_no_gc) && current_no_gc_region_info.minimal_gc_p)
-    {
-#ifdef MULTIPLE_HEAPS
-        gc_t_join.join(this, gc_join_minimal_gc);
-        if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-        {
-#ifndef USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-            for (int i = 0; i < n_heaps; i++)
-            {
-                if (!(g_heaps[i]->expand_soh_with_minimal_gc()))
-                    current_no_gc_region_info.start_status = start_no_gc_no_memory;
-            }
-#else
-            if (!expand_soh_with_minimal_gc())
-                current_no_gc_region_info.start_status = start_no_gc_no_memory;
-#endif //MULTIPLE_HEAPS
-#endif //!USE_REGIONS
-            update_collection_counts_for_no_gc();
-#ifdef MULTIPLE_HEAPS
-            gc_start_event.Reset();
-            gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-        }
-        goto done;
-    }
-    init_records();
-    settings.reason = gc_trigger_reason;
-    num_pinned_objects = 0;
-#ifdef STRESS_HEAP
-    if (settings.reason == reason_gcstress)
-    {
-        settings.reason = reason_induced;
-        settings.stress_induced = TRUE;
-    }
-#endif // STRESS_HEAP
-#ifdef MULTIPLE_HEAPS
-#ifdef STRESS_DYNAMIC_HEAP_COUNT
-    Interlocked::Increment (&heaps_in_this_gc);
-#endif //STRESS_DYNAMIC_HEAP_COUNT
-    dprintf (3, ("Joining for max generation to condemn"));
-    condemned_generation_num = generation_to_condemn (n,
-                                                      &blocking_collection,
-                                                      &elevation_requested,
-                                                      FALSE);
-    gc_t_join.join(this, gc_join_generation_determined);
-    if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-#ifdef FEATURE_BASICFREEZE
-        seg_table->delete_old_slots();
-#endif //FEATURE_BASICFREEZE
-#ifdef MULTIPLE_HEAPS
-#ifdef STRESS_DYNAMIC_HEAP_COUNT
-        dprintf (9999, ("%d heaps, join sees %d, actually joined %d, %d idle threads (%d)",
-            n_heaps, gc_t_join.get_num_threads (), heaps_in_this_gc,
-            VolatileLoadWithoutBarrier(&dynamic_heap_count_data.idle_thread_count), (n_max_heaps - n_heaps)));
-        if (heaps_in_this_gc != n_heaps)
-        {
-            dprintf (9999, ("should have %d heaps but actually have %d!!", n_heaps, heaps_in_this_gc));
-            GCToOSInterface::DebugBreak ();
-        }
-        heaps_in_this_gc = 0;
-#endif //STRESS_DYNAMIC_HEAP_COUNT
-        for (int i = 0; i < n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-            if (g_gc_card_table != hp->card_table)
-                hp->copy_brick_card_table();
-            hp->delay_free_segments();
-        }
-#else //MULTIPLE_HEAPS
-        if (g_gc_card_table != card_table)
-            copy_brick_card_table();
-        delay_free_segments();
-#endif //MULTIPLE_HEAPS
-        BOOL should_evaluate_elevation = TRUE;
-        BOOL should_do_blocking_collection = FALSE;
-#ifdef MULTIPLE_HEAPS
-        int gen_max = condemned_generation_num;
-        for (int i = 0; i < n_heaps; i++)
-        {
-            if (gen_max < g_heaps[i]->condemned_generation_num)
-                gen_max = g_heaps[i]->condemned_generation_num;
-            if (should_evaluate_elevation && !(g_heaps[i]->elevation_requested))
-                should_evaluate_elevation = FALSE;
-            if ((!should_do_blocking_collection) && (g_heaps[i]->blocking_collection))
-                should_do_blocking_collection = TRUE;
-        }
-        settings.condemned_generation = gen_max;
-#else //MULTIPLE_HEAPS
-        settings.condemned_generation = generation_to_condemn (n,
-                                                            &blocking_collection,
-                                                            &elevation_requested,
-                                                            FALSE);
-        should_evaluate_elevation = elevation_requested;
-        should_do_blocking_collection = blocking_collection;
-#endif //MULTIPLE_HEAPS
-        settings.condemned_generation = joined_generation_to_condemn (
-                                            should_evaluate_elevation,
-                                            n,
-                                            settings.condemned_generation,
-                                            &should_do_blocking_collection
-                                            STRESS_HEAP_ARG(n)
-                                            );
-        STRESS_LOG1(LF_GCROOTS|LF_GC|LF_GCALLOC, LL_INFO10,
-                "condemned generation num: %d\n", settings.condemned_generation);
-        record_gcs_during_no_gc();
-        if (settings.condemned_generation > 1)
-            settings.promotion = TRUE;
-#ifdef HEAP_ANALYZE
-        if (GCToEEInterface::AnalyzeSurvivorsRequested(settings.condemned_generation))
-        {
-            heap_analyze_enabled = TRUE;
-        }
-#endif // HEAP_ANALYZE
-        GCToEEInterface::DiagGCStart(settings.condemned_generation, settings.reason == reason_induced);
-#ifdef BACKGROUND_GC
-        if ((settings.condemned_generation == max_generation) &&
-            (should_do_blocking_collection == FALSE) &&
-            gc_can_use_concurrent &&
-            !temp_disable_concurrent_p &&
-            ((settings.pause_mode == pause_interactive) || (settings.pause_mode == pause_sustained_low_latency)))
-        {
-            keep_bgc_threads_p = TRUE;
-            c_write (settings.concurrent, TRUE);
-            memset (&bgc_data_global, 0, sizeof(bgc_data_global));
-            memcpy (&bgc_data_global, &gc_data_global, sizeof(gc_data_global));
-        }
-#endif //BACKGROUND_GC
-        settings.gc_index = (uint32_t)dd_collection_count (dynamic_data_of (0)) + 1;
-#ifdef MULTIPLE_HEAPS
-        hb_log_balance_activities();
-        hb_log_new_allocation();
-#endif //MULTIPLE_HEAPS
-        GCToEEInterface::GcStartWork (settings.condemned_generation,
-                                max_generation);
-        do_pre_gc();
-#ifdef MULTIPLE_HEAPS
-        dprintf (9999, ("in GC, resetting gc_start"));
-        gc_start_event.Reset();
-        dprintf(3, ("Starting all gc threads for gc"));
-        gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-    descr_generations ("BEGIN");
-#if defined(TRACE_GC) && defined(USE_REGIONS)
-    if (heap_number == 0)
-    {
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            gc_heap *hp = g_heaps[i];
-#else //MULTIPLE_HEAPS
-        {
-            gc_heap* hp = pGenGCHeap;
-            const int i = 0;
-#endif //MULTIPLE_HEAPS
-            if (settings.condemned_generation == max_generation)
-            {
-                region_free_list::print(hp->free_regions, i, "BEGIN");
-            }
-            else
-            {
-                hp->free_regions[basic_free_region].print (i, "BEGIN");
-            }
-        }
-    }
-#endif // TRACE_GC && USE_REGIONS
-#ifdef VERIFY_HEAP
-    if ((GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC) &&
-       !(GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_POST_GC_ONLY))
-    {
-        verify_heap (TRUE);
-    }
-    if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_BARRIERCHECK)
-        checkGCWriteBarrier();
-#endif // VERIFY_HEAP
-#ifdef BACKGROUND_GC
-    if (settings.concurrent)
-    {
-        assert (settings.condemned_generation == max_generation);
-        settings.compaction = FALSE;
-        saved_bgc_settings = settings;
-#ifdef MULTIPLE_HEAPS
-        if (heap_number == 0)
-        {
-            for (int i = 0; i < n_heaps; i++)
-            {
-                prepare_bgc_thread (g_heaps[i]);
-            }
-            dprintf (2, ("setting bgc_threads_sync_event"));
-            bgc_threads_sync_event.Set();
-        }
-        else
-        {
-            bgc_threads_sync_event.Wait(INFINITE, FALSE);
-            dprintf (2, ("bgc_threads_sync_event is signalled"));
-        }
-#else
-        prepare_bgc_thread(0);
-#endif //MULTIPLE_HEAPS
-#ifdef MULTIPLE_HEAPS
-        gc_t_join.join(this, gc_join_start_bgc);
-        if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-        {
-            do_concurrent_p = TRUE;
-            do_ephemeral_gc_p = FALSE;
-#ifdef MULTIPLE_HEAPS
-            dprintf(2, ("Joined to perform a background GC"));
-            for (int i = 0; i < n_heaps; i++)
-            {
-                gc_heap* hp = g_heaps[i];
-                if (!(hp->bgc_thread) || !hp->commit_mark_array_bgc_init())
-                {
-                    do_concurrent_p = FALSE;
-                    break;
-                }
-                else
-                {
-                    hp->background_saved_lowest_address = hp->lowest_address;
-                    hp->background_saved_highest_address = hp->highest_address;
-                }
-            }
-#else
-            do_concurrent_p = (!!bgc_thread && commit_mark_array_bgc_init());
-            if (do_concurrent_p)
-            {
-                background_saved_lowest_address = lowest_address;
-                background_saved_highest_address = highest_address;
-            }
-#endif //MULTIPLE_HEAPS
-            if (do_concurrent_p)
-            {
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-                SoftwareWriteWatch::EnableForGCHeap();
-#endif //FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-#ifdef MULTIPLE_HEAPS
-                for (int i = 0; i < n_heaps; i++)
-                    g_heaps[i]->current_bgc_state = bgc_initialized;
-#else
-                current_bgc_state = bgc_initialized;
-#endif //MULTIPLE_HEAPS
-                int gen = check_for_ephemeral_alloc();
-                dont_restart_ee_p = TRUE;
-                if (gen == -1)
-                {
-#ifdef MULTIPLE_HEAPS
-                    for (int i = 0; i < n_heaps; i++)
-                    {
-                        generation_allocation_pointer (g_heaps[i]->generation_of (0)) =  0;
-                        generation_allocation_limit (g_heaps[i]->generation_of (0)) = 0;
-                    }
-#else
-                    generation_allocation_pointer (youngest_generation) =  0;
-                    generation_allocation_limit (youngest_generation) = 0;
-#endif //MULTIPLE_HEAPS
-                }
-                else
-                {
-                    do_ephemeral_gc_p = TRUE;
-                    settings.init_mechanisms();
-                    settings.condemned_generation = gen;
-                    settings.gc_index = (size_t)dd_collection_count (dynamic_data_of (0)) + 2;
-                    do_pre_gc();
-                    dprintf (GTC_LOG, ("doing gen%d before doing a bgc", gen));
-                }
-                if (!do_ephemeral_gc_p)
-                {
-                    do_background_gc();
-                }
-            }
-            else
-            {
-                settings.compaction = TRUE;
-                c_write (settings.concurrent, FALSE);
-            }
-#ifdef MULTIPLE_HEAPS
-            gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-        }
-        if (do_concurrent_p)
-        {
-            memset (&bgc_data_per_heap, 0, sizeof (bgc_data_per_heap));
-            memcpy (&bgc_data_per_heap, &gc_data_per_heap, sizeof(gc_data_per_heap));
-            if (do_ephemeral_gc_p)
-            {
-                dprintf (2, ("GC threads running, doing gen%d GC", settings.condemned_generation));
-                gen_to_condemn_reasons.init();
-                gen_to_condemn_reasons.set_condition (gen_before_bgc);
-                gc_data_per_heap.gen_to_condemn_reasons.init (&gen_to_condemn_reasons);
-                gc1();
-#ifdef MULTIPLE_HEAPS
-                gc_t_join.join(this, gc_join_bgc_after_ephemeral);
-                if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-                {
-#ifdef MULTIPLE_HEAPS
-                    do_post_gc();
-#endif //MULTIPLE_HEAPS
-                    settings = saved_bgc_settings;
-                    assert (settings.concurrent);
-                    do_background_gc();
-#ifdef MULTIPLE_HEAPS
-                    gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-                }
-            }
-        }
-        else
-        {
-            dprintf (2, ("couldn't create BGC threads, reverting to doing a blocking GC"));
-            gc1();
-        }
-    }
-    else
-#endif //BACKGROUND_GC
-    {
-        gc1();
-    }
-#ifndef MULTIPLE_HEAPS
-    allocation_running_time = GCToOSInterface::GetLowPrecisionTimeStamp();
-    allocation_running_amount = dd_new_allocation (dynamic_data_of (0));
-    fgn_last_alloc = dd_new_allocation (dynamic_data_of (0));
-#endif //MULTIPLE_HEAPS
-done:
-    if (saved_settings_pause_mode == pause_no_gc)
-        allocate_for_no_gc_after_gc();
-}
-#define mark_stack_empty_p() (mark_stack_base == mark_stack_tos)
-inline
-size_t gc_heap::get_promoted_bytes()
-{
-#ifdef USE_REGIONS
-    if (!survived_per_region)
-    {
-        dprintf (REGIONS_LOG, ("no space to store promoted bytes"));
-        return 0;
-    }
-    dprintf (3, ("h%d getting surv", heap_number));
-    size_t promoted = 0;
-    for (size_t i = 0; i < region_count; i++)
-    {
-        if (survived_per_region[i] > 0)
-        {
-            heap_segment* region = get_region_at_index (i);
-            dprintf (REGIONS_LOG, ("h%d region[%zd] %p(g%d)(%s) surv: %zd(%p)",
-                heap_number, i,
-                heap_segment_mem (region),
-                heap_segment_gen_num (region),
-                (heap_segment_loh_p (region) ? "LOH" : (heap_segment_poh_p (region) ? "POH" :"SOH")),
-                survived_per_region[i],
-                &survived_per_region[i]));
-            promoted += survived_per_region[i];
-        }
-    }
-#ifdef _DEBUG
-    dprintf (REGIONS_LOG, ("h%d global recorded %zd, regions recorded %zd",
-        heap_number, promoted_bytes (heap_number), promoted));
-    assert (promoted_bytes (heap_number) == promoted);
-#endif //_DEBUG
-    return promoted;
-#else //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-    return g_promoted [heap_number*16];
-#else //MULTIPLE_HEAPS
-    return g_promoted;
-#endif //MULTIPLE_HEAPS
-#endif //USE_REGIONS
-}
-#ifdef USE_REGIONS
-void gc_heap::sync_promoted_bytes()
-{
-    int condemned_gen_number = settings.condemned_generation;
-    int highest_gen_number = ((condemned_gen_number == max_generation) ?
-                              (total_generation_count - 1) : settings.condemned_generation);
-    int stop_gen_idx = get_stop_generation_index (condemned_gen_number);
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap* hp = g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        for (int gen_idx = highest_gen_number; gen_idx >= stop_gen_idx; gen_idx--)
-        {
-            generation* condemned_gen = hp->generation_of (gen_idx);
-            heap_segment* current_region = heap_segment_rw (generation_start_segment (condemned_gen));
-            while (current_region)
-            {
-                size_t region_index = get_basic_region_index_for_address (heap_segment_mem (current_region));
-#ifdef MULTIPLE_HEAPS
-                size_t total_surv = 0;
-                size_t total_old_card_surv = 0;
-                for (int hp_idx = 0; hp_idx < n_heaps; hp_idx++)
-                {
-                    total_surv += g_heaps[hp_idx]->survived_per_region[region_index];
-                    total_old_card_surv += g_heaps[hp_idx]->old_card_survived_per_region[region_index];
-                }
-                heap_segment_survived (current_region) = total_surv;
-                heap_segment_old_card_survived (current_region) = (int)total_old_card_surv;
-#else
-                heap_segment_survived (current_region) = survived_per_region[region_index];
-                heap_segment_old_card_survived (current_region) =
-                    (int)(old_card_survived_per_region[region_index]);
-#endif //MULTIPLE_HEAPS
-                dprintf (REGIONS_LOG, ("region #%zd %p surv %zd, old card surv %d",
-                    region_index,
-                    heap_segment_mem (current_region),
-                    heap_segment_survived (current_region),
-                    heap_segment_old_card_survived (current_region)));
-                current_region = heap_segment_next (current_region);
-            }
-        }
-    }
-}
-#ifdef MULTIPLE_HEAPS
-void gc_heap::set_heap_for_contained_basic_regions (heap_segment* region, gc_heap* hp)
-{
-    uint8_t* region_start = get_region_start (region);
-    uint8_t* region_end = heap_segment_reserved (region);
-    int num_basic_regions = (int)((region_end - region_start) >> min_segment_size_shr);
-    for (int i = 0; i < num_basic_regions; i++)
-    {
-        uint8_t* basic_region_start = region_start + ((size_t)i << min_segment_size_shr);
-        heap_segment* basic_region = get_region_info (basic_region_start);
-        heap_segment_heap (basic_region) = hp;
-    }
-}
-heap_segment* gc_heap::unlink_first_rw_region (int gen_idx)
-{
-    generation* gen = generation_of (gen_idx);
-    heap_segment* prev_region = generation_tail_ro_region (gen);
-    heap_segment* region = nullptr;
-    if (prev_region)
-    {
-        assert (heap_segment_read_only_p (prev_region));
-        region = heap_segment_next (prev_region);
-        assert (region != nullptr);
-        if (heap_segment_next (region) == nullptr)
-        {
-            assert (region == generation_tail_region (gen));
-            return nullptr;
-        }
-        heap_segment_next (prev_region) = heap_segment_next (region);
-    }
-    else
-    {
-        region = generation_start_segment (gen);
-        assert (region != nullptr);
-        if (heap_segment_next (region) == nullptr)
-        {
-            assert (region == generation_tail_region (gen));
-            return nullptr;
-        }
-        generation_start_segment (gen) = heap_segment_next (region);
-    }
-    assert (region != generation_tail_region (gen));
-    assert (!heap_segment_read_only_p (region));
-    dprintf (REGIONS_LOG, ("unlink_first_rw_region on heap: %d gen: %d region: %p", heap_number, gen_idx, heap_segment_mem (region)));
-#if defined(_DEBUG) && defined(HOST_64BIT)
-#ifndef COMMITTED_BYTES_SHADOW
-    if (heap_hard_limit)
-#endif //!COMMITTED_BYTES_SHADOW
-    {
-        int old_oh = heap_segment_oh (region);
-        int old_heap = heap_segment_heap (region)->heap_number;
-        dprintf(3, ("commit-accounting:  from %d to temp [%p, %p) for heap %d", old_oh, get_region_start (region), heap_segment_committed (region), old_heap));
-        size_t committed = heap_segment_committed (region) - get_region_start (region);
-        check_commit_cs.Enter();
-        assert (g_heaps[old_heap]->committed_by_oh_per_heap[old_oh] >= committed);
-        g_heaps[old_heap]->committed_by_oh_per_heap[old_oh] -= committed;
-        check_commit_cs.Leave();
-    }
-#endif // _DEBUG && HOST_64BIT
-    set_heap_for_contained_basic_regions (region, nullptr);
-    return region;
-}
-void gc_heap::thread_rw_region_front (int gen_idx, heap_segment* region)
-{
-    generation* gen = generation_of (gen_idx);
-    assert (!heap_segment_read_only_p (region));
-    heap_segment* prev_region = generation_tail_ro_region (gen);
-    if (prev_region)
-    {
-        heap_segment_next (region) = heap_segment_next (prev_region);
-        heap_segment_next (prev_region) = region;
-    }
-    else
-    {
-        heap_segment_next (region) = generation_start_segment (gen);
-        generation_start_segment (gen) = region;
-    }
-    if (heap_segment_next (region) == nullptr)
-    {
-        generation_tail_region (gen) = region;
-    }
-    dprintf (REGIONS_LOG, ("thread_rw_region_front on heap: %d gen: %d region: %p", heap_number, gen_idx, heap_segment_mem (region)));
-#if defined(_DEBUG) && defined(HOST_64BIT)
-#ifndef COMMITTED_BYTES_SHADOW
-    if (heap_hard_limit)
-#endif //!COMMITTED_BYTES_SHADOW
-    {
-        int new_oh = gen_to_oh (gen_idx);
-        int new_heap = this->heap_number;
-        dprintf(3, ("commit-accounting:  from temp to %d [%p, %p) for heap %d", new_oh, get_region_start (region), heap_segment_committed (region), new_heap));
-        size_t committed = heap_segment_committed (region) - get_region_start (region);
-        check_commit_cs.Enter();
-        assert (heap_segment_heap (region) == nullptr);
-        g_heaps[new_heap]->committed_by_oh_per_heap[new_oh] += committed;
-        check_commit_cs.Leave();
-    }
-#endif // _DEBUG && HOST_64BIT
-    set_heap_for_contained_basic_regions (region, this);
-}
-#endif // MULTIPLE_HEAPS
-void gc_heap::equalize_promoted_bytes(int condemned_gen_number)
-{
-#ifdef MULTIPLE_HEAPS
-    int highest_gen_number = ((condemned_gen_number == max_generation) ?
-        (total_generation_count - 1) : condemned_gen_number);
-    int stop_gen_idx = get_stop_generation_index (condemned_gen_number);
-    for (int gen_idx = highest_gen_number; gen_idx >= stop_gen_idx; gen_idx--)
-    {
-        size_t total_surv = 0;
-        size_t max_surv_per_heap = 0;
-        size_t surv_per_heap[MAX_SUPPORTED_CPUS];
-        for (int i = 0; i < n_heaps; i++)
-        {
-            surv_per_heap[i] = 0;
-            gc_heap* hp = g_heaps[i];
-            generation* condemned_gen = hp->generation_of (gen_idx);
-            heap_segment* current_region = heap_segment_rw (generation_start_segment (condemned_gen));
-            while (current_region)
-            {
-                total_surv += heap_segment_survived (current_region);
-                surv_per_heap[i] += heap_segment_survived (current_region);
-                current_region = heap_segment_next (current_region);
-            }
-            max_surv_per_heap = max (max_surv_per_heap, surv_per_heap[i]);
-            dprintf (REGIONS_LOG, ("gen: %d heap %d surv: %zd", gen_idx, i, surv_per_heap[i]));
-        }
-        size_t avg_surv_per_heap = (total_surv + n_heaps - 1) / n_heaps;
-        if (avg_surv_per_heap != 0)
-        {
-            dprintf (REGIONS_LOG, ("before equalize: gen: %d avg surv: %zd max_surv: %zd imbalance: %zd", gen_idx, avg_surv_per_heap, max_surv_per_heap, max_surv_per_heap*100/avg_surv_per_heap));
-        }
-        heap_segment* surplus_regions = nullptr;
-        size_t max_deficit = 0;
-        size_t max_survived = 0;
-        for (int i = 0; i < n_heaps; i++)
-        {
-            while (surv_per_heap[i] > avg_surv_per_heap)
-            {
-                heap_segment* region = g_heaps[i]->unlink_first_rw_region (gen_idx);
-                if (region == nullptr)
-                {
-                    break;
-                }
-                assert (surv_per_heap[i] >= heap_segment_survived (region));
-                dprintf (REGIONS_LOG, ("heap: %d surv: %zd - %zd = %zd",
-                    i,
-                    surv_per_heap[i],
-                    heap_segment_survived (region),
-                    surv_per_heap[i] - heap_segment_survived (region)));
-                surv_per_heap[i] -= heap_segment_survived (region);
-                heap_segment_next (region) = surplus_regions;
-                surplus_regions = region;
-                max_survived = max (max_survived, heap_segment_survived (region));
-            }
-            if (surv_per_heap[i] < avg_surv_per_heap)
-            {
-                size_t deficit = avg_surv_per_heap - surv_per_heap[i];
-                max_deficit = max (max_deficit, deficit);
-            }
-        }
-        for (int i = 0; i < n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-            generation* gen = hp->generation_of (gen_idx);
-            if (heap_segment_rw (generation_start_segment (gen)) == nullptr)
-            {
-                heap_segment* start_region = surplus_regions;
-                if (start_region != nullptr)
-                {
-                    surplus_regions = heap_segment_next (start_region);
-                }
-                else
-                {
-                    for (int j = 0; j < n_heaps; j++)
-                    {
-                        start_region = g_heaps[j]->unlink_first_rw_region (gen_idx);
-                        if (start_region != nullptr)
-                        {
-                            surv_per_heap[j] -= heap_segment_survived (start_region);
-                            size_t deficit = avg_surv_per_heap - surv_per_heap[j];
-                            max_deficit = max (max_deficit, deficit);
-                            break;
-                        }
-                    }
-                }
-                assert (start_region);
-                dprintf (3, ("making sure heap %d gen %d has at least one region by adding region %zx", start_region));
-                heap_segment_next (start_region) = nullptr;
-                set_heap_for_contained_basic_regions (start_region, hp);
-                max_survived = max (max_survived, heap_segment_survived (start_region));
-                hp->thread_start_region (gen, start_region);
-                surv_per_heap[i] += heap_segment_survived (start_region);
-            }
-        }
-        const int NUM_SIZE_CLASSES = 16;
-        heap_segment* surplus_regions_by_size_class[NUM_SIZE_CLASSES];
-        memset (surplus_regions_by_size_class, 0, sizeof(surplus_regions_by_size_class));
-        double survived_scale_factor = ((double)NUM_SIZE_CLASSES) / (max_survived + 1);
-        heap_segment* next_region;
-        for (heap_segment* region = surplus_regions; region != nullptr; region = next_region)
-        {
-            size_t size_class = (size_t)(heap_segment_survived (region)*survived_scale_factor);
-            assert ((0 <= size_class) && (size_class < NUM_SIZE_CLASSES));
-            next_region = heap_segment_next (region);
-            heap_segment_next (region) = surplus_regions_by_size_class[size_class];
-            surplus_regions_by_size_class[size_class] = region;
-        }
-        int next_heap_in_size_class[MAX_SUPPORTED_CPUS];
-        int heaps_by_deficit_size_class[NUM_SIZE_CLASSES];
-        for (int i = 0; i < NUM_SIZE_CLASSES; i++)
-        {
-            heaps_by_deficit_size_class[i] = -1;
-        }
-        double deficit_scale_factor = ((double)NUM_SIZE_CLASSES) / (max_deficit + 1);
-        for (int i = 0; i < n_heaps; i++)
-        {
-            if (avg_surv_per_heap > surv_per_heap[i])
-            {
-                size_t deficit = avg_surv_per_heap - surv_per_heap[i];
-                int size_class = (int)(deficit*deficit_scale_factor);
-                assert ((0 <= size_class) && (size_class < NUM_SIZE_CLASSES));
-                next_heap_in_size_class[i] = heaps_by_deficit_size_class[size_class];
-                heaps_by_deficit_size_class[size_class] = i;
-            }
-        }
-        int region_size_class = NUM_SIZE_CLASSES - 1;
-        int heap_size_class = NUM_SIZE_CLASSES - 1;
-        while (region_size_class >= 0)
-        {
-            heap_segment* region = surplus_regions_by_size_class[region_size_class];
-            if (region == nullptr)
-            {
-                region_size_class--;
-                continue;
-            }
-            int heap_num;
-            while (true)
-            {
-                if (heap_size_class < 0)
-                {
-                    heap_num = 0;
-                    break;
-                }
-                heap_num = heaps_by_deficit_size_class[heap_size_class];
-                if (heap_num >= 0)
-                {
-                    break;
-                }
-                heap_size_class--;
-            }
-            surplus_regions_by_size_class[region_size_class] = heap_segment_next (region);
-            g_heaps[heap_num]->thread_rw_region_front (gen_idx, region);
-            dprintf (REGIONS_LOG, ("heap: %d surv: %zd + %zd = %zd",
-                heap_num,
-                surv_per_heap[heap_num],
-                heap_segment_survived (region),
-                surv_per_heap[heap_num] + heap_segment_survived (region)));
-            surv_per_heap[heap_num] += heap_segment_survived (region);
-            if (heap_size_class < 0)
-            {
-                continue;
-            }
-            if (surv_per_heap[heap_num] >= avg_surv_per_heap)
-            {
-                heaps_by_deficit_size_class[heap_size_class] = next_heap_in_size_class[heap_num];
-                continue;
-            }
-            size_t new_deficit = avg_surv_per_heap - surv_per_heap[heap_num];
-            int new_heap_size_class = (int)(new_deficit*deficit_scale_factor);
-            if (new_heap_size_class != heap_size_class)
-            {
-                assert (new_heap_size_class < heap_size_class);
-                assert ((0 <= new_heap_size_class) && (new_heap_size_class < NUM_SIZE_CLASSES));
-                heaps_by_deficit_size_class[heap_size_class] = next_heap_in_size_class[heap_num];
-                next_heap_in_size_class[heap_num] = heaps_by_deficit_size_class[new_heap_size_class];
-                heaps_by_deficit_size_class[new_heap_size_class] = heap_num;
-            }
-        }
-        for (int i = 0; i < n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-            hp->verify_regions (gen_idx, true, true);
-        }
-#ifdef TRACE_GC
-        max_surv_per_heap = 0;
-        for (int i = 0; i < n_heaps; i++)
-        {
-            max_surv_per_heap = max (max_surv_per_heap, surv_per_heap[i]);
-        }
-        if (avg_surv_per_heap != 0)
-        {
-            dprintf (REGIONS_LOG, ("after equalize: gen: %d avg surv: %zd max_surv: %zd imbalance: %zd", gen_idx, avg_surv_per_heap, max_surv_per_heap, max_surv_per_heap*100/avg_surv_per_heap));
-        }
-#endif // TRACE_GC
-    }
-#endif //MULTIPLE_HEAPS
-}
-#ifdef DYNAMIC_HEAP_COUNT
-#define DECOMMISSIONED_VALUE 0xdec0dec0dec0dec0
-static const size_t DECOMMISSIONED_SIZE_T = DECOMMISSIONED_VALUE;
-static const ptrdiff_t DECOMMISSIONED_PTRDIFF_T = (ptrdiff_t)DECOMMISSIONED_VALUE;
-static const ptrdiff_t DECOMMISSIONED_UINT64_T = (uint64_t)DECOMMISSIONED_VALUE;
-static uint8_t* const DECOMMISSIONED_UINT8_T_P = (uint8_t*)DECOMMISSIONED_VALUE;
-static uint8_t** const DECOMMISSIONED_UINT8_T_PP = (uint8_t**)DECOMMISSIONED_VALUE;
-static PTR_heap_segment const DECOMMISSIONED_REGION_P = (PTR_heap_segment)DECOMMISSIONED_VALUE;
-static mark* const DECOMMISSIONED_MARK_P = (mark*)DECOMMISSIONED_VALUE;
-static const BOOL DECOMMISSIONED_BOOL = 0xdec0dec0;
-static const BOOL DECOMMISSIONED_INT = (int)0xdec0dec0;
-static const float DECOMMISSIONED_FLOAT = (float)DECOMMISSIONED_VALUE;
-static const ptrdiff_t UNINITIALIZED_VALUE  = 0xbaadbaadbaadbaad;
-void gc_heap::check_decommissioned_heap()
-{
-    assert (generation_skip_ratio               == DECOMMISSIONED_INT);
-    assert (gen0_must_clear_bricks              == DECOMMISSIONED_INT);
-    assert (freeable_uoh_segment                == DECOMMISSIONED_REGION_P);
-#ifdef BACKGROUND_GC
-    assert (freeable_soh_segment                == DECOMMISSIONED_REGION_P);
-#endif //BACKGROUND_GC
-#ifdef FEATURE_LOH_COMPACTION
-    assert (loh_pinned_queue_length             == DECOMMISSIONED_SIZE_T);
-    assert (loh_pinned_queue_decay              == DECOMMISSIONED_INT);
-    assert (loh_pinned_queue                    == DECOMMISSIONED_MARK_P);
-#endif //FEATURE_LOH_COMPACTION
-    assert (gen0_bricks_cleared                 == DECOMMISSIONED_BOOL);
-    assert (alloc_allocated                     == DECOMMISSIONED_UINT8_T_P);
-    assert (ephemeral_heap_segment              == DECOMMISSIONED_REGION_P);
-#ifdef USE_REGIONS
-#endif //USE_REGIONS
-    assert (more_space_lock_soh.lock            == lock_decommissioned);
-    assert (more_space_lock_uoh.lock            == lock_decommissioned);
-    assert (soh_allocation_no_gc                == DECOMMISSIONED_SIZE_T);
-    assert (loh_allocation_no_gc                == DECOMMISSIONED_SIZE_T);
-    for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-    {
-        generation* gen = generation_of (gen_idx);
-        assert (generation_start_segment                   (gen) == DECOMMISSIONED_REGION_P);
-        assert (generation_allocation_segment              (gen) == DECOMMISSIONED_REGION_P);
-        assert (generation_tail_region                     (gen) == DECOMMISSIONED_REGION_P);
-        assert (generation_tail_ro_region                  (gen) == DECOMMISSIONED_REGION_P);
-        assert (generation_allocation_context_start_region (gen) == DECOMMISSIONED_UINT8_T_P);
-        assert (gen->plan_start_segment                          == DECOMMISSIONED_REGION_P);
-        assert (generation_free_list_allocated             (gen) == DECOMMISSIONED_SIZE_T);
-        assert (generation_end_seg_allocated               (gen) == DECOMMISSIONED_SIZE_T);
-        assert (generation_allocate_end_seg_p              (gen) == DECOMMISSIONED_BOOL);
-        assert (generation_condemned_allocated             (gen) == DECOMMISSIONED_SIZE_T);
-        assert (generation_sweep_allocated                 (gen) == DECOMMISSIONED_SIZE_T);
-        assert (generation_free_list_space                 (gen) == DECOMMISSIONED_SIZE_T);
-        assert (generation_free_obj_space                  (gen) == DECOMMISSIONED_SIZE_T);
-        assert (generation_allocation_size                 (gen) == DECOMMISSIONED_SIZE_T);
-        assert (generation_pinned_allocation_compact_size  (gen) == DECOMMISSIONED_SIZE_T);
-        assert (generation_pinned_allocation_sweep_size    (gen) == DECOMMISSIONED_SIZE_T);
-        assert (gen->gen_num                                     == DECOMMISSIONED_INT);
-#ifdef DOUBLY_LINKED_FL
-        assert (generation_set_bgc_mark_bit_p              (gen) == DECOMMISSIONED_BOOL);
-        assert (generation_last_free_list_allocated        (gen) == DECOMMISSIONED_UINT8_T_P);
-#endif //DOUBLY_LINKED_FL
-        dynamic_data* dd = dynamic_data_of (gen_idx);
-        assert (dd_new_allocation                  (dd) == DECOMMISSIONED_PTRDIFF_T);
-        assert (dd_gc_new_allocation               (dd) == DECOMMISSIONED_PTRDIFF_T);
-        assert (dd_surv                     (dd) == (float)DECOMMISSIONED_VALUE);
-        assert (dd_desired_allocation              (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_begin_data_size                 (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_survived_size                   (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_pinned_survived_size            (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_artificial_pinned_survived_size (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_added_pinned_size               (dd) == DECOMMISSIONED_SIZE_T);
-#ifdef SHORT_PLUGS
-        assert (dd_padding_size                    (dd) == DECOMMISSIONED_SIZE_T);
-#endif //SHORT_PLUGS
-#if defined (RESPECT_LARGE_ALIGNMENT) || defined (FEATURE_STRUCTALIGN)
-        assert (dd_num_npinned_plugs               (dd) == DECOMMISSIONED_SIZE_T);
-#endif //RESPECT_LARGE_ALIGNMENT || FEATURE_STRUCTALIGN
-        assert (dd_current_size                    (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_collection_count                (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_promoted_size                   (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_freach_previous_promotion       (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_fragmentation                   (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_gc_clock                        (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_time_clock                      (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_previous_time_clock             (dd) == DECOMMISSIONED_SIZE_T);
-        assert (dd_gc_elapsed_time                 (dd) == DECOMMISSIONED_SIZE_T);
-    }
-}
-void gc_heap::decommission_heap()
-{
-    set_gc_done();
-    generation_skip_ratio               = DECOMMISSIONED_INT;
-    gen0_must_clear_bricks              = DECOMMISSIONED_INT;
-    freeable_uoh_segment                = DECOMMISSIONED_REGION_P;
-    memset ((void *)gen2_alloc_list, DECOMMISSIONED_INT, sizeof(gen2_alloc_list[0])*(NUM_GEN2_ALIST - 1));
-#ifdef BACKGROUND_GC
-    freeable_soh_segment                = DECOMMISSIONED_REGION_P;
-#endif //BACKGROUND_GC
-#ifdef FEATURE_LOH_COMPACTION
-    loh_pinned_queue_length             = DECOMMISSIONED_SIZE_T;
-    loh_pinned_queue_decay              = DECOMMISSIONED_INT;
-    loh_pinned_queue                    = DECOMMISSIONED_MARK_P;
-#endif //FEATURE_LOH_COMPACTION
-    gen0_bricks_cleared                 = DECOMMISSIONED_BOOL;
-    memset ((void *)loh_alloc_list, DECOMMISSIONED_INT, sizeof(loh_alloc_list));
-    memset ((void *)poh_alloc_list, DECOMMISSIONED_INT, sizeof(poh_alloc_list));
-    alloc_allocated                     = DECOMMISSIONED_UINT8_T_P;
-    ephemeral_heap_segment              = DECOMMISSIONED_REGION_P;
-#ifdef USE_REGIONS
-    memset ((void *)free_regions, DECOMMISSIONED_INT, sizeof(free_regions));
-#endif //USE_REGIONS
-    assert (more_space_lock_soh.lock    == lock_free);
-    more_space_lock_soh.lock            = lock_decommissioned;
-    assert (more_space_lock_uoh.lock    == lock_free);
-    more_space_lock_uoh.lock            = lock_decommissioned;
-    soh_allocation_no_gc                = DECOMMISSIONED_SIZE_T;
-    loh_allocation_no_gc                = DECOMMISSIONED_SIZE_T;
-    for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-    {
-        generation* gen = generation_of (gen_idx);
-        generation_allocator (gen)->clear();
-        memset (generation_alloc_context           (gen),  DECOMMISSIONED_INT, sizeof(alloc_context));
-        generation_start_segment                   (gen) = DECOMMISSIONED_REGION_P;
-        generation_allocation_segment              (gen) = DECOMMISSIONED_REGION_P;
-        generation_allocation_context_start_region (gen) = DECOMMISSIONED_UINT8_T_P;
-        generation_tail_region                     (gen) = DECOMMISSIONED_REGION_P;
-        gen->plan_start_segment                          = DECOMMISSIONED_REGION_P;
-        generation_tail_ro_region                  (gen) = DECOMMISSIONED_REGION_P;
-        generation_free_list_allocated             (gen) = DECOMMISSIONED_SIZE_T;
-        generation_end_seg_allocated               (gen) = DECOMMISSIONED_SIZE_T;
-        generation_allocate_end_seg_p              (gen) = DECOMMISSIONED_BOOL;
-        generation_condemned_allocated             (gen) = DECOMMISSIONED_SIZE_T;
-        generation_sweep_allocated                 (gen) = DECOMMISSIONED_SIZE_T;
-        generation_free_list_space                 (gen) = DECOMMISSIONED_SIZE_T;
-        generation_free_obj_space                  (gen) = DECOMMISSIONED_SIZE_T;
-        generation_allocation_size                 (gen) = DECOMMISSIONED_SIZE_T;
-        generation_pinned_allocation_compact_size  (gen) = DECOMMISSIONED_SIZE_T;
-        generation_pinned_allocation_sweep_size    (gen) = DECOMMISSIONED_SIZE_T;
-        gen->gen_num                                     = DECOMMISSIONED_INT;
-#ifdef DOUBLY_LINKED_FL
-        generation_set_bgc_mark_bit_p              (gen) = DECOMMISSIONED_BOOL;
-        generation_last_free_list_allocated        (gen) = DECOMMISSIONED_UINT8_T_P;
-#endif //DOUBLY_LINKED_FL
-        dynamic_data* dd = dynamic_data_of (gen_idx);
-        dd_new_allocation                  (dd) = DECOMMISSIONED_SIZE_T;
-        dd_gc_new_allocation               (dd) = DECOMMISSIONED_PTRDIFF_T;
-        dd_surv                     (dd) = (float)DECOMMISSIONED_VALUE;
-        dd_desired_allocation              (dd) = DECOMMISSIONED_SIZE_T;
-        dd_begin_data_size                 (dd) = DECOMMISSIONED_SIZE_T;
-        dd_survived_size                   (dd) = DECOMMISSIONED_SIZE_T;
-        dd_pinned_survived_size            (dd) = DECOMMISSIONED_SIZE_T;
-        dd_artificial_pinned_survived_size (dd) = DECOMMISSIONED_SIZE_T;
-        dd_added_pinned_size               (dd) = DECOMMISSIONED_SIZE_T;
-#ifdef SHORT_PLUGS
-        dd_padding_size                    (dd) = DECOMMISSIONED_SIZE_T;
-#endif //SHORT_PLUGS
-#if defined (RESPECT_LARGE_ALIGNMENT) || defined (FEATURE_STRUCTALIGN)
-        dd_num_npinned_plugs               (dd) = DECOMMISSIONED_SIZE_T;
-#endif //RESPECT_LARGE_ALIGNMENT || FEATURE_STRUCTALIGN
-        dd_current_size                    (dd) = DECOMMISSIONED_SIZE_T;
-        dd_collection_count                (dd) = DECOMMISSIONED_SIZE_T;
-        dd_promoted_size                   (dd) = DECOMMISSIONED_SIZE_T;
-        dd_freach_previous_promotion       (dd) = DECOMMISSIONED_SIZE_T;
-        dd_fragmentation                   (dd) = DECOMMISSIONED_SIZE_T;
-        dd_gc_clock                        (dd) = DECOMMISSIONED_SIZE_T;
-        dd_time_clock                      (dd) = DECOMMISSIONED_SIZE_T;
-        dd_previous_time_clock             (dd) = DECOMMISSIONED_SIZE_T;
-        dd_gc_elapsed_time                 (dd) = DECOMMISSIONED_SIZE_T;
-    }
-}
-void gc_heap::recommission_heap()
-{
-    generation_skip_ratio               = 0;
-    gen0_must_clear_bricks              = 0;
-    freeable_uoh_segment                = nullptr;
-    memset ((void *)gen2_alloc_list, 0, sizeof(gen2_alloc_list));
-#ifdef BACKGROUND_GC
-    freeable_soh_segment                = nullptr;
-#endif //BACKGROUND_GC
-#ifdef FEATURE_LOH_COMPACTION
-    loh_pinned_queue_length             = 0;
-    loh_pinned_queue_decay              = 0;
-    loh_pinned_queue                    = 0;
-#endif //FEATURE_LOH_COMPACTION
-    gen0_bricks_cleared                 = FALSE;
-    memset ((void *)loh_alloc_list, 0, sizeof(loh_alloc_list));
-    memset ((void *)poh_alloc_list, 0, sizeof(poh_alloc_list));
-    alloc_allocated                     = 0;
-    ephemeral_heap_segment              = nullptr;
-    for (int kind = 0; kind < count_free_region_kinds; kind++)
-    {
-        free_regions[kind].reset();
-    }
-    more_space_lock_soh.lock            = lock_free;
-    more_space_lock_uoh.lock            = lock_free;
-    soh_allocation_no_gc                = 0;
-    loh_allocation_no_gc                = 0;
-    bgc_alloc_lock->init();
-    gc_heap* heap0 = g_heaps[0];
-    for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-    {
-        generation* gen = generation_of (gen_idx);
-        generation_allocator (gen)->clear();
-        memset (generation_alloc_context           (gen), 0, sizeof(alloc_context));
-        generation_start_segment                   (gen) = nullptr;
-        generation_tail_ro_region                  (gen) = nullptr;
-        generation_tail_region                     (gen) = nullptr;
-        generation_allocation_segment              (gen) = nullptr;
-        generation_allocation_context_start_region (gen) = nullptr;
-        gen->plan_start_segment                          = nullptr;
-        generation_free_list_allocated             (gen) = 0;
-        generation_end_seg_allocated               (gen) = 0;
-        generation_allocate_end_seg_p              (gen) = 0;
-        generation_condemned_allocated             (gen) = 0;
-        generation_sweep_allocated                 (gen) = 0;
-        generation_free_list_space                 (gen) = 0;
-        generation_free_obj_space                  (gen) = 0;
-        generation_allocation_size                 (gen) = 0;
-        generation_pinned_allocation_compact_size  (gen) = 0;
-        generation_pinned_allocation_sweep_size    (gen) = 0;
-        gen->gen_num                                     = gen_idx;
-#ifdef DOUBLY_LINKED_FL
-        generation_set_bgc_mark_bit_p              (gen) = FALSE;
-        generation_last_free_list_allocated        (gen) = nullptr;
-#endif //DOUBLY_LINKED_FL
-        dynamic_data* dd = dynamic_data_of (gen_idx);
-        dynamic_data* heap0_dd = heap0->dynamic_data_of (gen_idx);
-        dd_time_clock     (dd) = dd_time_clock (heap0_dd);
-        dd_collection_count (dd) = dd_collection_count (heap0_dd);
-        dd_promoted_size                   (dd) = 0;
-        dd_fragmentation                   (dd) = 0;
-        dd_gc_clock                        (dd) = dd_gc_clock (heap0_dd);
-        dd_new_allocation                  (dd) = UNINITIALIZED_VALUE;
-        dd_desired_allocation              (dd) = UNINITIALIZED_VALUE;
-        dd_gc_new_allocation               (dd) = UNINITIALIZED_VALUE;
-        dd_surv                     (dd) = (float)UNINITIALIZED_VALUE;
-        dd_begin_data_size                 (dd) = UNINITIALIZED_VALUE;
-        dd_survived_size                   (dd) = UNINITIALIZED_VALUE;
-        dd_pinned_survived_size            (dd) = UNINITIALIZED_VALUE;
-        dd_artificial_pinned_survived_size (dd) = UNINITIALIZED_VALUE;
-        dd_added_pinned_size               (dd) = UNINITIALIZED_VALUE;
-#ifdef SHORT_PLUGS
-        dd_padding_size                    (dd) = UNINITIALIZED_VALUE;
-#endif //SHORT_PLUGS
-#if defined (RESPECT_LARGE_ALIGNMENT) || defined (FEATURE_STRUCTALIGN)
-        dd_num_npinned_plugs               (dd) = UNINITIALIZED_VALUE;
-#endif //RESPECT_LARGE_ALIGNMENT || FEATURE_STRUCTALIGN
-        dd_current_size                    (dd) = UNINITIALIZED_VALUE;
-        dd_freach_previous_promotion       (dd) = UNINITIALIZED_VALUE;
-        dd_previous_time_clock             (dd) = UNINITIALIZED_VALUE;
-        dd_gc_elapsed_time                 (dd) = UNINITIALIZED_VALUE;
-    }
-#ifdef SPINLOCK_HISTORY
-    spinlock_info_index = 0;
-    current_uoh_alloc_state = (allocation_state)-1;
-#endif //SPINLOCK_HISTORY
-#ifdef RECORD_LOH_STATE
-    loh_state_index = 0;
-#endif //RECORD_LOH_STATE
-}
-float median_of_3 (float a, float b, float c)
-{
-#define compare_and_swap(i, j)          \
-        {                               \
-            if (i < j)                  \
-            {                           \
-                float t = i;            \
-                          i = j;        \
-                              j = t;    \
-            }                           \
-        }
-    compare_and_swap (b, a);
-    compare_and_swap (c, a);
-    compare_and_swap (c, b);
-#undef compare_and_swap
-    return b;
-}
-size_t gc_heap::get_num_completed_gcs ()
-{
-    size_t num_completed_gcs = settings.gc_index;
-#ifdef BACKGROUND_GC
-    if (g_heaps[0]->is_bgc_in_progress ())
-    {
-        num_completed_gcs--;
-        dprintf (6666, ("BGC in prog, completed GCs -> %Id", num_completed_gcs));
-    }
-#endif //BACKGROUND_GC
-    return num_completed_gcs;
-}
-int gc_heap::calculate_new_heap_count ()
-{
-    assert (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes);
-    size_t num_completed_gcs = get_num_completed_gcs ();
-    dprintf (6666, ("current GC %Id(completed: %Id), prev completed GCs %Id, last full GC happened at index %Id",
-        VolatileLoadWithoutBarrier (&settings.gc_index), num_completed_gcs, dynamic_heap_count_data.prev_num_completed_gcs, gc_index_full_gc_end));
-    if (num_completed_gcs < (dynamic_heap_count_data.prev_num_completed_gcs + dynamic_heap_count_data_t::sample_size))
-    {
-        dprintf (6666, ("not enough GCs, skipping"));
-        return n_heaps;
-    }
-    float median_gen2_tcp_percent = 0.0f;
-    if (gc_index_full_gc_end >= (settings.gc_index - dynamic_heap_count_data_t::sample_size))
-    {
-        median_gen2_tcp_percent = dynamic_heap_count_data.get_median_gen2_gc_percent ();
-    }
-    float throughput_cost_percents[dynamic_heap_count_data_t::sample_size];
-    for (int i = 0; i < dynamic_heap_count_data_t::sample_size; i++)
-    {
-        dynamic_heap_count_data_t::sample& sample = dynamic_heap_count_data.samples[i];
-        throughput_cost_percents[i] = (sample.elapsed_between_gcs ? (((float)sample.msl_wait_time / n_heaps + sample.gc_pause_time) * 100.0f / (float)sample.elapsed_between_gcs) : 0.0f);
-        assert (throughput_cost_percents[i] >= 0.0);
-        if (throughput_cost_percents[i] > 100.0)
-            throughput_cost_percents[i] = 100.0;
-        dprintf (6666, ("sample %d: msl %I64d / %d + pause %I64d / elapsed %I64d = throughput_cost_percent: %.3f", i,
-            sample.msl_wait_time, n_heaps, sample.gc_pause_time, sample.elapsed_between_gcs, throughput_cost_percents[i]));
-    }
-    float median_throughput_cost_percent = median_of_3 (throughput_cost_percents[0], throughput_cost_percents[1], throughput_cost_percents[2]);
-    const float smoothing = 3;
-    float smoothed_median_throughput_cost_percent = dynamic_heap_count_data.smoothed_median_throughput_cost_percent;
-    if (smoothed_median_throughput_cost_percent != 0.0f)
-    {
-        smoothed_median_throughput_cost_percent = median_throughput_cost_percent / smoothing + (smoothed_median_throughput_cost_percent / smoothing) * (smoothing - 1);
-    }
-    else
-    {
-        smoothed_median_throughput_cost_percent = median_throughput_cost_percent;
-    }
-    dprintf (6666, ("median tcp: %.3f, smoothed tcp: %.3f, gen2 tcp %.3f(%.3f, %.3f, %.3f)",
-        median_throughput_cost_percent, smoothed_median_throughput_cost_percent, median_gen2_tcp_percent,
-        dynamic_heap_count_data.gen2_gc_percents[0], dynamic_heap_count_data.gen2_gc_percents[1], dynamic_heap_count_data.gen2_gc_percents[2]));
-    size_t heap_size = 0;
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap* hp = g_heaps[i];
-        for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-        {
-            dynamic_data* dd = hp->dynamic_data_of (gen_idx);
-            heap_size += dd_current_size (dd) + dd_desired_allocation (dd);
-            dprintf (3, ("h%d g%d current: %zd desired allocation: %zd", i, gen_idx, dd_promoted_size (dd), dd_desired_allocation (dd)));
-        }
-    }
-    size_t heap_space_cost_per_heap = dd_min_size (g_heaps[0]->dynamic_data_of (0));
-    float percent_heap_space_cost_per_heap = heap_space_cost_per_heap * 100.0f / heap_size;
-    int step_up = (n_heaps + 1) / 2;
-    int extra_heaps = 1 + (n_max_heaps >= 32);
-    step_up = min (step_up, n_max_heaps - extra_heaps - n_heaps);
-    int step_down = (n_heaps + 1) / 3;
-    float tcp_reduction_per_step_up = smoothed_median_throughput_cost_percent * step_up / (n_heaps + step_up);
-    float tcp_increase_per_step_down = smoothed_median_throughput_cost_percent * step_down / (n_heaps - step_down);
-    float scp_increase_per_step_up = percent_heap_space_cost_per_heap * step_up;
-    float scp_decrease_per_step_down = percent_heap_space_cost_per_heap * step_down;
-    dprintf (6666, ("[CHP] u %d, d %d | space cost %Id / heap %Id(%.2fmb) = scp %.3f (u: %.3f, d: %.3f) | stcp %.3f, u * %.1f = %.3f, d * %.1f = %.3f",
-        step_up, step_down,
-        heap_space_cost_per_heap, heap_size, ((float)heap_size / (float)1000 / (float)1000), percent_heap_space_cost_per_heap,
-        scp_increase_per_step_up, scp_decrease_per_step_down,
-        smoothed_median_throughput_cost_percent,
-        ((float)step_up / (float)(n_heaps + step_up)), tcp_reduction_per_step_up,
-        ((float)step_down / (float)(n_heaps - step_down)), tcp_increase_per_step_down));
-#ifdef STRESS_DYNAMIC_HEAP_COUNT
-    int new_n_heaps = (int)gc_rand::get_rand (n_max_heaps - 1) + 1;
-    if ((new_n_heaps < n_heaps) && (dynamic_heap_count_data.lowest_heap_with_msl_uoh != -1))
-    {
-        new_n_heaps = min (dynamic_heap_count_data.lowest_heap_with_msl_uoh, new_n_heaps);
-        new_n_heaps = max (new_n_heaps, 1);
-    }
-    dprintf (6666, ("stress %d -> %d", n_heaps, new_n_heaps));
-#else //STRESS_DYNAMIC_HEAP_COUNT
-    int new_n_heaps = n_heaps;
-    if (median_throughput_cost_percent > 10.0f)
-    {
-        new_n_heaps = (int)(n_heaps * (median_throughput_cost_percent / 5.0));
-        dprintf (6666, ("[CHP0] tcp %.3f -> %d * %.3f = %d", median_throughput_cost_percent, n_heaps, (median_throughput_cost_percent / 5.0), new_n_heaps));
-        new_n_heaps = min (new_n_heaps, n_max_heaps - extra_heaps);
-    }
-    else if ((smoothed_median_throughput_cost_percent > 5.0f) || (median_gen2_tcp_percent > 10.0f))
-    {
-        if (smoothed_median_throughput_cost_percent > 5.0f)
-        {
-            dprintf (6666, ("[CHP1] stcp %.3f > 5, %d + %d = %d", smoothed_median_throughput_cost_percent, n_heaps, step_up, (n_heaps + step_up)));
-        }
-        else
-        {
-            dprintf (6666, ("[CHP2] tcp %.3f > 10, %d + %d = %d", median_gen2_tcp_percent, n_heaps, step_up, (n_heaps + step_up)));
-        }
-        new_n_heaps += step_up;
-    }
-    else if ((tcp_reduction_per_step_up - scp_increase_per_step_up) >= 1.0f)
-    {
-        dprintf (6666, ("[CHP3] % .3f - % .3f = % .3f, % d + % d = % d",
-            tcp_reduction_per_step_up, scp_increase_per_step_up, (tcp_reduction_per_step_up - scp_increase_per_step_up),
-            n_heaps, step_up, (n_heaps + step_up)));
-        new_n_heaps += step_up;
-    }
-    else if ((smoothed_median_throughput_cost_percent < 1.0f) &&
-        (median_gen2_tcp_percent < 5.0f) &&
-        ((scp_decrease_per_step_down - tcp_increase_per_step_down) >= 1.0f))
-    {
-        dprintf (6666, ("[CHP4] stcp %.3f tcp %.3f, %.3f - %.3f = %.3f, %d + %d = %d",
-            smoothed_median_throughput_cost_percent, median_gen2_tcp_percent,
-            scp_decrease_per_step_down, tcp_increase_per_step_down, (scp_decrease_per_step_down - tcp_increase_per_step_down),
-            n_heaps, step_up, (n_heaps + step_up)));
-        new_n_heaps -= step_down;
-    }
-    assert (new_n_heaps >= 1);
-    assert (new_n_heaps <= n_max_heaps);
-#endif //STRESS_DYNAMIC_HEAP_COUNT
-    dynamic_heap_count_data.median_throughput_cost_percent = median_throughput_cost_percent;
-    dynamic_heap_count_data.smoothed_median_throughput_cost_percent = smoothed_median_throughput_cost_percent;
-    dynamic_heap_count_data.percent_heap_space_cost_per_heap = percent_heap_space_cost_per_heap;
-    dynamic_heap_count_data.tcp_reduction_per_step_up = tcp_reduction_per_step_up;
-    dynamic_heap_count_data.tcp_increase_per_step_down = tcp_increase_per_step_down;
-    dynamic_heap_count_data.scp_increase_per_step_up = scp_increase_per_step_up;
-    dynamic_heap_count_data.scp_decrease_per_step_down = scp_decrease_per_step_down;
-    GCEventFireHeapCountTuning_V1 (
-        (uint16_t)dynamic_heap_count_data.new_n_heaps,
-        (uint64_t)VolatileLoadWithoutBarrier (&settings.gc_index),
-        dynamic_heap_count_data.median_throughput_cost_percent,
-        dynamic_heap_count_data.smoothed_median_throughput_cost_percent,
-        dynamic_heap_count_data.tcp_reduction_per_step_up,
-        dynamic_heap_count_data.tcp_increase_per_step_down,
-        dynamic_heap_count_data.scp_increase_per_step_up,
-        dynamic_heap_count_data.scp_decrease_per_step_down
-    );
-    dynamic_heap_count_data.prev_num_completed_gcs = num_completed_gcs;
-    if (new_n_heaps != n_heaps)
-    {
-        dprintf (6666, ("should change! %d->%d", n_heaps, new_n_heaps));
-        dynamic_heap_count_data.heap_count_to_change_to = new_n_heaps;
-        dynamic_heap_count_data.should_change_heap_count = true;
-    }
-    return new_n_heaps;
-}
-void gc_heap::check_heap_count ()
-{
-    dynamic_heap_count_data.new_n_heaps = dynamic_heap_count_data.heap_count_to_change_to;
-    assert (dynamic_heap_count_data.new_n_heaps != n_heaps);
-    if (dynamic_heap_count_data.new_n_heaps != n_heaps)
-    {
-        dprintf (9999, ("h0 suspending EE in check"));
-        GCToEEInterface::SuspendEE(SUSPEND_FOR_GC_PREP);
-        dprintf (9999, ("h0 suspended EE in check"));
-#ifdef BACKGROUND_GC
-        if (gc_heap::background_running_p())
-        {
-            dynamic_heap_count_data.new_n_heaps = n_heaps;
-            dprintf (6666, ("can't change heap count! BGC in progress"));
-            GCToEEInterface::RestartEE(TRUE);
-        }
-#endif //BACKGROUND_GC
-    }
-    if (dynamic_heap_count_data.new_n_heaps != n_heaps)
-    {
-        dprintf (6666, ("prep to change from %d to %d", n_heaps, dynamic_heap_count_data.new_n_heaps));
-        if (!prepare_to_change_heap_count (dynamic_heap_count_data.new_n_heaps))
-        {
-            dynamic_heap_count_data.new_n_heaps = n_heaps;
-        }
-    }
-    if (dynamic_heap_count_data.new_n_heaps == n_heaps)
-    {
-        dynamic_heap_count_data.prev_num_completed_gcs = get_num_completed_gcs ();
-        dynamic_heap_count_data.should_change_heap_count = false;
-        dprintf (6666, ("heap count stays the same %d, no work to do, set prev completed to %Id", dynamic_heap_count_data.new_n_heaps, dynamic_heap_count_data.prev_num_completed_gcs));
-        return;
-    }
-    int new_n_heaps = dynamic_heap_count_data.new_n_heaps;
-    assert (!(dynamic_heap_count_data.init_only_p));
-    {
-        dprintf (9999, ("changing join hp %d->%d", n_heaps, new_n_heaps));
-        int max_threads_to_wake = max (n_heaps, new_n_heaps);
-        gc_t_join.update_n_threads (max_threads_to_wake);
-        assert (dynamic_heap_count_data.new_n_heaps != n_heaps);
-        if (n_heaps < new_n_heaps)
-        {
-            int saved_idle_thread_count = dynamic_heap_count_data.idle_thread_count;
-            Interlocked::ExchangeAdd (&dynamic_heap_count_data.idle_thread_count, (n_heaps - new_n_heaps));
-            dprintf (9999, ("GC thread %d setting idle events for h%d-h%d, total idle %d -> %d", heap_number, n_heaps, (new_n_heaps - 1),
-                saved_idle_thread_count, VolatileLoadWithoutBarrier (&dynamic_heap_count_data.idle_thread_count)));
-            for (int heap_idx = n_heaps; heap_idx < new_n_heaps; heap_idx++)
-            {
-                g_heaps[heap_idx]->gc_idle_thread_event.Set();
-#ifdef BACKGROUND_GC
-                g_heaps[heap_idx]->bgc_idle_thread_event.Set();
-#endif //BACKGROUND_GC
-            }
-        }
-        gc_start_event.Set();
-    }
-    int old_n_heaps = n_heaps;
-    (dynamic_heap_count_data.heap_count_change_count)++;
-    change_heap_count (dynamic_heap_count_data.new_n_heaps);
-    GCToEEInterface::RestartEE(TRUE);
-    dprintf (9999, ("h0 restarted EE"));
-    dynamic_heap_count_data.smoothed_median_throughput_cost_percent = dynamic_heap_count_data.smoothed_median_throughput_cost_percent / n_heaps * old_n_heaps;
-    dprintf (6666, ("h0 finished changing, set should change to false!"));
-    dynamic_heap_count_data.should_change_heap_count = false;
-}
-bool gc_heap::prepare_to_change_heap_count (int new_n_heaps)
-{
-    dprintf (9999, ("trying to change heap count %d -> %d", n_heaps, new_n_heaps));
-    int old_n_heaps = n_heaps;
-    for (int i = 0; i < old_n_heaps; i++)
-    {
-        gc_heap* hp = g_heaps[i];
-        if (!hp->prepare_rethread_fl_items())
-        {
-            return false;
-        }
-    }
-    if (new_n_heaps < old_n_heaps)
-    {
-        int to_heap_number = 0;
-        for (int i = new_n_heaps; i < old_n_heaps; i++)
-        {
-            gc_heap* from_hp = g_heaps[i];
-            gc_heap* to_hp = g_heaps[to_heap_number];
-            if (!to_hp->finalize_queue->MergeFinalizationData (from_hp->finalize_queue))
-            {
-                dprintf (3, ("failed to merge finalization from heap %d into heap %d", i, to_heap_number));
-                return false;
-            }
-            to_heap_number = (to_heap_number + 1) % new_n_heaps;
-        }
-    }
-    for (int i = 0; i < old_n_heaps; i++)
-    {
-        gc_heap* hp = g_heaps[i];
-        hp->delay_free_segments ();
-    }
-    ptrdiff_t region_count_in_gen[total_generation_count];
-    for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-    {
-        region_count_in_gen[gen_idx] = 0;
-    }
-    if (old_n_heaps < new_n_heaps)
-    {
-        for (int i = 0; i < old_n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-            for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-            {
-                generation* gen = hp->generation_of (gen_idx);
-                for (heap_segment* region = heap_segment_rw (generation_start_segment (gen));
-                     region != nullptr;
-                     region = heap_segment_next (region))
-                {
-                    region_count_in_gen[gen_idx]++;
-                }
-            }
-        }
-        bool success = true;
-        for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-        {
-            const size_t size = gen_idx > soh_gen2 ? global_region_allocator.get_large_region_alignment() : 0;
-            while (region_count_in_gen[gen_idx] < new_n_heaps)
-            {
-                int kind = gen_idx > soh_gen2 ? large_free_region : basic_free_region;
-                bool found_free_regions = false;
-                for (int i = 0; i < old_n_heaps; i++)
-                {
-                    gc_heap* hp = g_heaps[i];
-                    if (hp->free_regions[kind].get_num_free_regions() > 0)
-                    {
-                        heap_segment* region = hp->get_new_region (gen_idx, size);
-                        assert (region != nullptr);
-                        region_count_in_gen[gen_idx]++;
-                        found_free_regions = true;
-                        if (region_count_in_gen[gen_idx] == new_n_heaps)
-                            break;
-                    }
-                }
-                if (!found_free_regions)
-                {
-                    break;
-                }
-            }
-            while (region_count_in_gen[gen_idx] < new_n_heaps)
-            {
-                if (g_heaps[0]->get_new_region (gen_idx, size) == nullptr)
-                {
-                    success = false;
-                    break;
-                }
-                region_count_in_gen[gen_idx]++;
-            }
-            if (!success)
-            {
-                return false;
-            }
-        }
-    }
-    return true;
-}
-bool gc_heap::change_heap_count (int new_n_heaps)
-{
-    dprintf (9999, ("BEG heap%d changing %d->%d", heap_number, n_heaps, new_n_heaps));
-    int old_n_heaps = n_heaps;
-    bool init_only_p = dynamic_heap_count_data.init_only_p;
-    {
-        gc_t_join.join (this, gc_join_merge_temp_fl);
-        if (gc_t_join.joined ())
-        {
-#ifdef BACKGROUND_GC
-            bgc_t_join.update_n_threads (new_n_heaps);
-#endif //BACKGROUND_GC
-            dynamic_heap_count_data.init_only_p = false;
-            dprintf (9999, ("in change h%d resetting gc_start, update bgc join to %d heaps", heap_number, new_n_heaps));
-            gc_start_event.Reset();
-            gc_t_join.restart ();
-        }
-    }
-    assert (dynamic_heap_count_data.new_n_heaps != old_n_heaps);
-    dprintf (9999, ("Waiting h0 heap%d changing %d->%d", heap_number, n_heaps, new_n_heaps));
-    if (heap_number == 0)
-    {
-        dprintf (3, ("switching heap count from %d to %d heaps", old_n_heaps, new_n_heaps));
-        int from_heap_number = 0;
-        for (int i = old_n_heaps; i < new_n_heaps; i++)
-        {
-            gc_heap* to_hp = g_heaps[i];
-            gc_heap* from_hp = g_heaps[from_heap_number];
-            if (!from_hp->finalize_queue->SplitFinalizationData (to_hp->finalize_queue))
-            {
-                dprintf (3, ("failed to split finalization data between heaps %d and %d", from_heap_number, i));
-            }
-            from_heap_number = (from_heap_number + 1) % old_n_heaps;
-        }
-        BOOL unified_gen0_bricks_cleared = TRUE;
-        for (int i = 0; i < old_n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-            if (!init_only_p)
-            {
-                hp->fix_allocation_contexts (TRUE);
-            }
-            if (unified_gen0_bricks_cleared && (hp->gen0_bricks_cleared == FALSE))
-            {
-                unified_gen0_bricks_cleared = FALSE;
-            }
-            for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-            {
-                generation* gen = hp->generation_of (gen_idx);
-                for (heap_segment* region = heap_segment_rw (generation_start_segment (gen));
-                     region != nullptr;
-                     region = heap_segment_next (region))
-                {
-                    heap_segment_survived (region) = heap_segment_allocated (region) - heap_segment_mem (region);
-                }
-            }
-        }
-        if (old_n_heaps < new_n_heaps)
-        {
-            for (int i = old_n_heaps; i < new_n_heaps; i++)
-            {
-                gc_heap* hp = g_heaps[i];
-                hp->check_decommissioned_heap();
-                hp->recommission_heap();
-            }
-        }
-        if (new_n_heaps < old_n_heaps)
-        {
-            assert (new_n_heaps > 0);
-            for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-            {
-                for (int i = new_n_heaps; i < old_n_heaps; i++)
-                {
-                    gc_heap* hp = g_heaps[i];
-                    int dest_heap_number = i % new_n_heaps;
-                    gc_heap* hpd = g_heaps[dest_heap_number];
-                    generation* hpd_gen = hpd->generation_of (gen_idx);
-                    generation* gen = hp->generation_of (gen_idx);
-                    heap_segment* start_region = generation_start_segment (gen);
-                    heap_segment* tail_ro_region = generation_tail_ro_region (gen);
-                    heap_segment* tail_region = generation_tail_region (gen);
-                    for (heap_segment* region = start_region; region != nullptr; region = heap_segment_next(region))
-                    {
-                        set_heap_for_contained_basic_regions (region, hpd);
-                    }
-                    if (tail_ro_region != nullptr)
-                    {
-                        heap_segment* start_rw_region = heap_segment_next (tail_ro_region);
-                        heap_segment* hpd_tail_ro_region = generation_tail_ro_region (hpd_gen);
-                        if (hpd_tail_ro_region != nullptr)
-                        {
-                            heap_segment_next (tail_ro_region) = heap_segment_next (hpd_tail_ro_region);
-                            heap_segment_next (hpd_tail_ro_region) = start_region;
-                        }
-                        else
-                        {
-                            heap_segment_next (tail_ro_region) = generation_start_segment (hpd_gen);
-                            generation_start_segment (hpd_gen) = start_region;
-                        }
-                        generation_tail_ro_region (hpd_gen) = tail_ro_region;
-                        start_region = start_rw_region;
-                    }
-                    heap_segment* hpd_tail_region = generation_tail_region (hpd_gen);
-                    heap_segment_next (hpd_tail_region) = start_region;
-                    generation_tail_region (hpd_gen) = tail_region;
-                    generation_start_segment (gen) = nullptr;
-                    generation_tail_ro_region (gen) = nullptr;
-                    generation_tail_region (gen) = nullptr;
-                }
-            }
-        }
-        for (int i = new_n_heaps; i < old_n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-            int dest_heap_number = i % new_n_heaps;
-            gc_heap* hpd = g_heaps[dest_heap_number];
-            for (int kind = 0; kind < count_free_region_kinds; kind++)
-            {
-                hpd->free_regions[kind].transfer_regions(&hp->free_regions[kind]);
-            }
-        }
-        dprintf (9999, ("h%d changing %d->%d", heap_number, n_heaps, new_n_heaps));
-        n_heaps = new_n_heaps;
-        equalize_promoted_bytes (max_generation);
-        for (int i = 0; i < new_n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-            hp->gen0_bricks_cleared = unified_gen0_bricks_cleared;
-            generation* gen0 = hp->generation_of (0);
-            if ((hp->ephemeral_heap_segment == nullptr) ||
-                (heap_segment_heap (hp->ephemeral_heap_segment) != hp))
-            {
-                hp->ephemeral_heap_segment = heap_segment_rw (generation_start_segment (gen0));
-                hp->alloc_allocated = heap_segment_allocated (hp->ephemeral_heap_segment);
-            }
-            for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-            {
-                generation* gen = hp->generation_of (gen_idx);
-                heap_segment *allocation_region = generation_allocation_segment (gen);
-                if ((allocation_region == nullptr) ||
-                    (heap_segment_heap (allocation_region) != hp))
-                {
-                    generation_allocation_segment (gen) = heap_segment_rw (generation_start_segment (gen));
-                }
-                generation_free_obj_space (gen) = 0;
-            }
-        }
-    }
-    dprintf (3, ("individual heap%d changing %d->%d", heap_number, n_heaps, new_n_heaps));
-    if (!init_only_p)
-    {
-        gc_t_join.join (this, gc_join_merge_temp_fl);
-        if (gc_t_join.joined ())
-        {
-            gc_t_join.restart ();
-        }
-        for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-        {
-            if (heap_number < old_n_heaps)
-            {
-                dprintf (3, ("h%d calling per heap work!", heap_number));
-                rethread_fl_items (gen_idx);
-            }
-            gc_t_join.join (this, gc_join_merge_temp_fl);
-            if (gc_t_join.joined ())
-            {
-                merge_fl_from_other_heaps (gen_idx, new_n_heaps, old_n_heaps);
-                gc_t_join.restart ();
-            }
-        }
-#ifdef BACKGROUND_GC
-        bgc_alloc_lock->check();
-#endif //BACKGROUND_GC
-    }
-    if (heap_number == 0)
-    {
-        ptrdiff_t budget_per_heap[total_generation_count];
-        for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-        {
-            ptrdiff_t total_budget = 0;
-            for (int i = 0; i < old_n_heaps; i++)
-            {
-                gc_heap* hp = g_heaps[i];
-                dynamic_data* dd = hp->dynamic_data_of (gen_idx);
-                total_budget += dd_new_allocation (dd);
-            }
-            int max_n_heaps = max (old_n_heaps, new_n_heaps);
-            budget_per_heap[gen_idx] = Align (total_budget/max_n_heaps, get_alignment_constant (gen_idx <= max_generation));
-            dprintf (6666, ("g%d: total budget: %zd budget per heap: %zd", gen_idx, total_budget, budget_per_heap[gen_idx]));
-        }
-        for (int i = 0; i < new_n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-            for (int gen_idx = 0; gen_idx < total_generation_count; gen_idx++)
-            {
-                dynamic_data* dd = hp->dynamic_data_of (gen_idx);
-                dd_new_allocation (dd) = max (budget_per_heap[gen_idx], (ptrdiff_t)dd_min_size (dd));
-                dd_desired_allocation (dd) = dd_new_allocation (dd);
-                generation* gen = hp->generation_of (gen_idx);
-                size_t gen_size = hp->generation_size (gen_idx);
-                dd_fragmentation (dd) = generation_free_list_space (gen);
-                assert (gen_size >= dd_fragmentation (dd));
-                dd_current_size (dd) = gen_size - dd_fragmentation (dd);
-                dprintf (6666, ("h%d g%d: new allocation: %zd generation_size: %zd fragmentation: %zd current_size: %zd",
-                    i,
-                    gen_idx,
-                    dd_new_allocation (dd),
-                    gen_size,
-                    dd_fragmentation (dd),
-                    dd_current_size (dd)));
-            }
-        }
-        for (int i = n_heaps; i < old_n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-            hp->decommission_heap();
-        }
-        if (!init_only_p)
-        {
-            fix_allocation_contexts_heaps();
-        }
-        dynamic_heap_count_data.last_n_heaps = old_n_heaps;
-    }
-    if (new_n_heaps < old_n_heaps)
-    {
-        gc_t_join.join (this, gc_join_merge_temp_fl);
-        if (gc_t_join.joined ())
-        {
-            dprintf (9999, ("now changing the join heap count to the smaller one %d", new_n_heaps));
-            gc_t_join.update_n_threads (new_n_heaps);
-            gc_t_join.restart ();
-        }
-    }
-    return true;
-}
-size_t gc_heap::get_msl_wait_time()
-{
-    assert (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes);
-    size_t msl_wait_since_pause = 0;
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap* hp = g_heaps[i];
-        msl_wait_since_pause += hp->more_space_lock_soh.msl_wait_time;
-        hp->more_space_lock_soh.msl_wait_time = 0;
-        msl_wait_since_pause += hp->more_space_lock_uoh.msl_wait_time;
-        hp->more_space_lock_uoh.msl_wait_time = 0;
-    }
-    return msl_wait_since_pause;
-}
-#endif //DYNAMIC_HEAP_COUNT
-#endif //USE_REGIONS
-#if !defined(USE_REGIONS) || defined(_DEBUG)
-inline
-void gc_heap::init_promoted_bytes()
-{
-#ifdef MULTIPLE_HEAPS
-    g_promoted [heap_number*16] = 0;
-#else //MULTIPLE_HEAPS
-    g_promoted = 0;
-#endif //MULTIPLE_HEAPS
-}
-size_t& gc_heap::promoted_bytes (int thread)
-{
-#ifdef MULTIPLE_HEAPS
-    return g_promoted [thread*16];
-#else //MULTIPLE_HEAPS
-    UNREFERENCED_PARAMETER(thread);
-    return g_promoted;
-#endif //MULTIPLE_HEAPS
-}
-#endif //!USE_REGIONS || _DEBUG
-inline
-void gc_heap::add_to_promoted_bytes (uint8_t* object, int thread)
-{
-    size_t obj_size = size (object);
-    add_to_promoted_bytes (object, obj_size, thread);
-}
-inline
-void gc_heap::add_to_promoted_bytes (uint8_t* object, size_t obj_size, int thread)
-{
-    assert (thread == heap_number);
-#ifdef USE_REGIONS
-    if (survived_per_region)
-    {
-        survived_per_region[get_basic_region_index_for_address (object)] += obj_size;
-    }
-#endif //USE_REGIONS
-#if !defined(USE_REGIONS) || defined(_DEBUG)
-#ifdef MULTIPLE_HEAPS
-    g_promoted [heap_number*16] += obj_size;
-#else //MULTIPLE_HEAPS
-    g_promoted += obj_size;
-#endif //MULTIPLE_HEAPS
-#endif //!USE_REGIONS || _DEBUG
-#ifdef _DEBUG
-#endif //_DEBUG
-}
-heap_segment* gc_heap::find_segment (uint8_t* interior, BOOL small_segment_only_p)
-{
-    heap_segment* seg = seg_mapping_table_segment_of (interior);
-    if (seg)
-    {
-        if (small_segment_only_p && heap_segment_uoh_p (seg))
-            return 0;
-    }
-    return seg;
-}
-#if !defined(_DEBUG) && !defined(__GNUC__)
-inline // This causes link errors if global optimization is off
-#endif //!_DEBUG && !__GNUC__
-gc_heap* gc_heap::heap_of (uint8_t* o)
-{
-#ifdef MULTIPLE_HEAPS
-    if (o == 0)
-        return g_heaps [0];
-    gc_heap* hp = seg_mapping_table_heap_of (o);
-    return (hp ? hp : g_heaps[0]);
-#else //MULTIPLE_HEAPS
-    UNREFERENCED_PARAMETER(o);
-    return __this;
-#endif //MULTIPLE_HEAPS
-}
-inline
-gc_heap* gc_heap::heap_of_gc (uint8_t* o)
-{
-#ifdef MULTIPLE_HEAPS
-    if (o == 0)
-        return g_heaps [0];
-    gc_heap* hp = seg_mapping_table_heap_of_gc (o);
-    return (hp ? hp : g_heaps[0]);
-#else //MULTIPLE_HEAPS
-    UNREFERENCED_PARAMETER(o);
-    return __this;
-#endif //MULTIPLE_HEAPS
-}
-uint8_t* gc_heap::find_object (uint8_t* interior)
-{
-    assert (interior != 0);
-    if (!gen0_bricks_cleared)
-    {
-#ifdef MULTIPLE_HEAPS
-        assert (!"Should have already been done in server GC");
-#endif //MULTIPLE_HEAPS
-        clear_gen0_bricks();
-    }
-    gen0_must_clear_bricks = FFIND_DECAY;
-    int brick_entry = get_brick_entry(brick_of (interior));
-    if (brick_entry == 0)
-    {
-        heap_segment* seg = find_segment (interior, FALSE);
-        if (seg)
-        {
-#ifdef FEATURE_CONSERVATIVE_GC
-            if (interior >= heap_segment_allocated(seg))
-                return 0;
-#endif
-            int align_const = get_alignment_constant (heap_segment_read_only_p (seg)
-#ifdef FEATURE_CONSERVATIVE_GC
-                                                       || (GCConfig::GetConservativeGC() && !heap_segment_uoh_p (seg))
-#endif
-                                                      );
-            assert (interior < heap_segment_allocated (seg));
-            uint8_t* o = heap_segment_mem (seg);
-            while (o < heap_segment_allocated (seg))
-            {
-                uint8_t* next_o = o + Align (size (o), align_const);
-                assert (next_o > o);
-                if ((o <= interior) && (interior < next_o))
-                    return o;
-                o = next_o;
-            }
-            return 0;
-        }
-        else
-        {
-            return 0;
-        }
-    }
-    else
-    {
-        heap_segment* seg = find_segment (interior, TRUE);
-        if (seg)
-        {
-#ifdef FEATURE_CONSERVATIVE_GC
-            if (interior >= heap_segment_allocated (seg))
-                return 0;
-#else
-            assert (interior < heap_segment_allocated (seg));
-#endif
-            uint8_t* o = find_first_object (interior, heap_segment_mem (seg));
-            return o;
-        }
-        else
-            return 0;
-    }
-}
-#ifdef MULTIPLE_HEAPS
-#ifdef GC_CONFIG_DRIVEN
-#define m_boundary(o) {if (mark_list_index <= mark_list_end) {*mark_list_index = o;mark_list_index++;} else {mark_list_index++;}}
-#else //GC_CONFIG_DRIVEN
-#define m_boundary(o) {if (mark_list_index <= mark_list_end) {*mark_list_index = o;mark_list_index++;}}
-#endif //GC_CONFIG_DRIVEN
-#define m_boundary_fullgc(o) {}
-#else //MULTIPLE_HEAPS
-#ifdef GC_CONFIG_DRIVEN
-#define m_boundary(o) {if (mark_list_index <= mark_list_end) {*mark_list_index = o;mark_list_index++;} else {mark_list_index++;} if (slow > o) slow = o; if (shigh < o) shigh = o;}
-#else
-#define m_boundary(o) {if (mark_list_index <= mark_list_end) {*mark_list_index = o;mark_list_index++;}if (slow > o) slow = o; if (shigh < o) shigh = o;}
-#endif //GC_CONFIG_DRIVEN
-#define m_boundary_fullgc(o) {if (slow > o) slow = o; if (shigh < o) shigh = o;}
-#endif //MULTIPLE_HEAPS
-inline
-BOOL gc_heap::gc_mark1 (uint8_t* o)
-{
-    BOOL marked = !marked (o);
-    set_marked (o);
-    dprintf (3, ("*%zx*, newly marked: %d", (size_t)o, marked));
-#if defined(USE_REGIONS) && defined(_DEBUG)
-    heap_segment* seg = seg_mapping_table_segment_of (o);
-    if (o > heap_segment_allocated (seg))
-    {
-        dprintf (REGIONS_LOG, ("%p is in seg %zx(%p) but beyond alloc %p!!",
-            o, (size_t)seg, heap_segment_mem (seg), heap_segment_allocated (seg)));
-        GCToOSInterface::DebugBreak();
-    }
-#endif //USE_REGIONS && _DEBUG
-    return marked;
-}
-#ifdef USE_REGIONS
-inline bool is_in_heap_range (uint8_t* o)
-{
-#ifdef FEATURE_BASICFREEZE
-    assert (((g_gc_lowest_address <= o) && (o < g_gc_highest_address)) ||
-        (o == nullptr) || (ro_segment_lookup (o) != nullptr));
-    return ((g_gc_lowest_address <= o) && (o < g_gc_highest_address));
-#else //FEATURE_BASICFREEZE
-    assert ((o == nullptr) || (g_gc_lowest_address <= o) && (o < g_gc_highest_address));
-    return (o != nullptr);
-#endif //FEATURE_BASICFREEZE
-}
-inline bool gc_heap::is_in_gc_range (uint8_t* o)
-{
-#ifdef FEATURE_BASICFREEZE
-    assert (((g_gc_lowest_address <= o) && (o < g_gc_highest_address)) ||
-        (o == nullptr) || (ro_segment_lookup (o) != nullptr));
-#else //FEATURE_BASICFREEZE
-    assert ((o == nullptr) || (g_gc_lowest_address <= o) && (o < g_gc_highest_address));
-#endif //FEATURE_BASICFREEZE
-    return ((gc_low <= o) && (o < gc_high));
-}
-#endif //USE_REGIONS
-inline
-BOOL gc_heap::gc_mark (uint8_t* o, uint8_t* low, uint8_t* high, int condemned_gen)
-{
-#ifdef USE_REGIONS
-    if ((o >= low) && (o < high))
-    {
-        if (condemned_gen != max_generation && get_region_gen_num (o) > condemned_gen)
-        {
-            return FALSE;
-        }
-        BOOL already_marked = marked (o);
-        if (already_marked)
-        {
-            return FALSE;
-        }
-        set_marked (o);
-        return TRUE;
-    }
-    return FALSE;
-#else //USE_REGIONS
-    assert (condemned_gen == -1);
-    BOOL marked = FALSE;
-    if ((o >= low) && (o < high))
-        marked = gc_mark1 (o);
-#ifdef MULTIPLE_HEAPS
-    else if (o)
-    {
-        gc_heap* hp = heap_of_gc (o);
-        assert (hp);
-        if ((o >= hp->gc_low) && (o < hp->gc_high))
-            marked = gc_mark1 (o);
-    }
-#ifdef SNOOP_STATS
-    snoop_stat.objects_checked_count++;
-    if (marked)
-    {
-        snoop_stat.objects_marked_count++;
-    }
-    if (!o)
-    {
-        snoop_stat.zero_ref_count++;
-    }
-#endif //SNOOP_STATS
-#endif //MULTIPLE_HEAPS
-    return marked;
-#endif //USE_REGIONS
-}
-#ifdef BACKGROUND_GC
-inline
-BOOL gc_heap::background_marked (uint8_t* o)
-{
-    return mark_array_marked (o);
-}
-inline
-BOOL gc_heap::background_mark1 (uint8_t* o)
-{
-    BOOL to_mark = !mark_array_marked (o);
-    dprintf (3, ("b*%zx*b(%d)", (size_t)o, (to_mark ? 1 : 0)));
-    if (to_mark)
-    {
-        mark_array_set_marked (o);
-        dprintf (4, ("n*%zx*n", (size_t)o));
-        return TRUE;
-    }
-    else
-        return FALSE;
-}
-inline
-BOOL gc_heap::background_mark (uint8_t* o, uint8_t* low, uint8_t* high)
-{
-    BOOL marked = FALSE;
-    if ((o >= low) && (o < high))
-        marked = background_mark1 (o);
-#ifdef MULTIPLE_HEAPS
-    else if (o)
-    {
-        gc_heap* hp = heap_of (o);
-        assert (hp);
-        if ((o >= hp->background_saved_lowest_address) && (o < hp->background_saved_highest_address))
-            marked = background_mark1 (o);
-    }
-#endif //MULTIPLE_HEAPS
-    return marked;
-}
-#endif //BACKGROUND_GC
-#define new_start() {if (ppstop <= start) {break;} else {parm = start}}
-#define ignore_start 0
-#define use_start 1
-#define go_through_object(mt,o,size,parm,start,start_useful,limit,exp)      \
-{                                                                           \
-    CGCDesc* map = CGCDesc::GetCGCDescFromMT((MethodTable*)(mt));           \
-    CGCDescSeries* cur = map->GetHighestSeries();                           \
-    ptrdiff_t cnt = (ptrdiff_t) map->GetNumSeries();                        \
-                                                                            \
-    if (cnt >= 0)                                                           \
-    {                                                                       \
-        CGCDescSeries* last = map->GetLowestSeries();                       \
-        uint8_t** parm = 0;                                                 \
-        do                                                                  \
-        {                                                                   \
-            assert (parm <= (uint8_t**)((o) + cur->GetSeriesOffset()));     \
-            parm = (uint8_t**)((o) + cur->GetSeriesOffset());               \
-            uint8_t** ppstop =                                              \
-                (uint8_t**)((uint8_t*)parm + cur->GetSeriesSize() + (size));\
-            if (!start_useful || (uint8_t*)ppstop > (start))                \
-            {                                                               \
-                if (start_useful && (uint8_t*)parm < (start)) parm = (uint8_t**)(start);\
-                while (parm < ppstop)                                       \
-                {                                                           \
-                   {exp}                                                    \
-                   parm++;                                                  \
-                }                                                           \
-            }                                                               \
-            cur--;                                                          \
-                                                                            \
-        } while (cur >= last);                                              \
-    }                                                                       \
-    else                                                                    \
-    {                                                                       \
-        /* Handle the repeating case - array of valuetypes */               \
-        uint8_t** parm = (uint8_t**)((o) + cur->startoffset);               \
-        if (start_useful && start > (uint8_t*)parm)                         \
-        {                                                                   \
-            ptrdiff_t cs = mt->RawGetComponentSize();                         \
-            parm = (uint8_t**)((uint8_t*)parm + (((start) - (uint8_t*)parm)/cs)*cs); \
-        }                                                                   \
-        while ((uint8_t*)parm < ((o)+(size)-plug_skew))                     \
-        {                                                                   \
-            for (ptrdiff_t __i = 0; __i > cnt; __i--)                         \
-            {                                                               \
-                HALF_SIZE_T skip =  (cur->val_serie + __i)->skip;           \
-                HALF_SIZE_T nptrs = (cur->val_serie + __i)->nptrs;          \
-                uint8_t** ppstop = parm + nptrs;                            \
-                if (!start_useful || (uint8_t*)ppstop > (start))            \
-                {                                                           \
-                    if (start_useful && (uint8_t*)parm < (start)) parm = (uint8_t**)(start);      \
-                    do                                                      \
-                    {                                                       \
-                       {exp}                                                \
-                       parm++;                                              \
-                    } while (parm < ppstop);                                \
-                }                                                           \
-                parm = (uint8_t**)((uint8_t*)ppstop + skip);                \
-            }                                                               \
-        }                                                                   \
-    }                                                                       \
-}
-#define go_through_object_nostart(mt,o,size,parm,exp) {go_through_object(mt,o,size,parm,o,ignore_start,(o + size),exp); }
-#ifndef COLLECTIBLE_CLASS
-#define go_through_object_cl(mt,o,size,parm,exp)                            \
-{                                                                           \
-    if (header(o)->ContainsPointers())                                      \
-    {                                                                       \
-        go_through_object_nostart(mt,o,size,parm,exp);                      \
-    }                                                                       \
-}
-#else //COLLECTIBLE_CLASS
-#define go_through_object_cl(mt,o,size,parm,exp)                            \
-{                                                                           \
-    if (header(o)->Collectible())                                           \
-    {                                                                       \
-        uint8_t* class_obj = get_class_object (o);                             \
-        uint8_t** parm = &class_obj;                                           \
-        do {exp} while (false);                                             \
-    }                                                                       \
-    if (header(o)->ContainsPointers())                                      \
-    {                                                                       \
-        go_through_object_nostart(mt,o,size,parm,exp);                      \
-    }                                                                       \
-}
-#endif //COLLECTIBLE_CLASS
-void gc_heap::enque_pinned_plug (uint8_t* plug,
-                                 BOOL save_pre_plug_info_p,
-                                 uint8_t* last_object_in_last_plug)
-{
-    if (mark_stack_array_length <= mark_stack_tos)
-    {
-        if (!grow_mark_stack (mark_stack_array, mark_stack_array_length, MARK_STACK_INITIAL_LENGTH))
-        {
-            GCToEEInterface::HandleFatalError((unsigned int)CORINFO_EXCEPTION_GC);
-        }
-    }
-    dprintf (3, ("enqueuing P #%zd(%p): %p. oldest: %zd, LO: %p, pre: %d",
-        mark_stack_tos, &mark_stack_array[mark_stack_tos], plug, mark_stack_bos, last_object_in_last_plug, (save_pre_plug_info_p ? 1 : 0)));
-    mark& m = mark_stack_array[mark_stack_tos];
-    m.first = plug;
-    m.saved_pre_p = save_pre_plug_info_p;
-    if (save_pre_plug_info_p)
-    {
-        size_t special_bits = clear_special_bits (last_object_in_last_plug);
-        memcpy (&(m.saved_pre_plug), &(((plug_and_gap*)plug)[-1]), sizeof (gap_reloc_pair));
-        set_special_bits (last_object_in_last_plug, special_bits);
-        memcpy (&(m.saved_pre_plug_reloc), &(((plug_and_gap*)plug)[-1]), sizeof (gap_reloc_pair));
-        size_t last_obj_size = plug - last_object_in_last_plug;
-        if (last_obj_size < min_pre_pin_obj_size)
-        {
-            record_interesting_data_point (idp_pre_short);
-#ifdef SHORT_PLUGS
-            if (is_plug_padded (last_object_in_last_plug))
-                record_interesting_data_point (idp_pre_short_padded);
-#endif //SHORT_PLUGS
-            dprintf (3, ("encountered a short object %p right before pinned plug %p!",
-                         last_object_in_last_plug, plug));
-            m.set_pre_short();
-#ifdef COLLECTIBLE_CLASS
-            if (is_collectible (last_object_in_last_plug))
-            {
-                m.set_pre_short_collectible();
-            }
-#endif //COLLECTIBLE_CLASS
-            if (contain_pointers (last_object_in_last_plug))
-            {
-                dprintf (3, ("short object: %p(%zx)", last_object_in_last_plug, last_obj_size));
-                go_through_object_nostart (method_table(last_object_in_last_plug), last_object_in_last_plug, last_obj_size, pval,
-                    {
-                        size_t gap_offset = (((size_t)pval - (size_t)(plug - sizeof (gap_reloc_pair) - plug_skew))) / sizeof (uint8_t*);
-                        dprintf (3, ("member: %p->%p, %zd ptrs from beginning of gap", (uint8_t*)pval, *pval, gap_offset));
-                        m.set_pre_short_bit (gap_offset);
-                    }
-                );
-            }
-        }
-    }
-    m.saved_post_p = FALSE;
-}
-void gc_heap::save_post_plug_info (uint8_t* last_pinned_plug, uint8_t* last_object_in_last_plug, uint8_t* post_plug)
-{
-#ifndef _DEBUG
-    UNREFERENCED_PARAMETER(last_pinned_plug);
-#endif //_DEBUG
-    mark& m = mark_stack_array[mark_stack_tos - 1];
-    assert (last_pinned_plug == m.first);
-    m.saved_post_plug_info_start = (uint8_t*)&(((plug_and_gap*)post_plug)[-1]);
-    size_t special_bits = clear_special_bits (last_object_in_last_plug);
-    memcpy (&(m.saved_post_plug), m.saved_post_plug_info_start, sizeof (gap_reloc_pair));
-    set_special_bits (last_object_in_last_plug, special_bits);
-    memcpy (&(m.saved_post_plug_reloc), m.saved_post_plug_info_start, sizeof (gap_reloc_pair));
-    m.saved_post_p = TRUE;
-#ifdef _DEBUG
-    m.saved_post_plug_debug.gap = 1;
-#endif //_DEBUG
-    dprintf (3, ("PP %p has NP %p right after", last_pinned_plug, post_plug));
-    size_t last_obj_size = post_plug - last_object_in_last_plug;
-    if (last_obj_size < min_pre_pin_obj_size)
-    {
-        dprintf (3, ("PP %p last obj %p is too short", last_pinned_plug, last_object_in_last_plug));
-        record_interesting_data_point (idp_post_short);
-#ifdef SHORT_PLUGS
-        if (is_plug_padded (last_object_in_last_plug))
-            record_interesting_data_point (idp_post_short_padded);
-#endif //SHORT_PLUGS
-        m.set_post_short();
-#if defined (_DEBUG) && defined (VERIFY_HEAP)
-        verify_pinned_queue_p = TRUE;
-#endif // _DEBUG && VERIFY_HEAP
-#ifdef COLLECTIBLE_CLASS
-        if (is_collectible (last_object_in_last_plug))
-        {
-            m.set_post_short_collectible();
-        }
-#endif //COLLECTIBLE_CLASS
-        if (contain_pointers (last_object_in_last_plug))
-        {
-            dprintf (3, ("short object: %p(%zx)", last_object_in_last_plug, last_obj_size));
-            go_through_object_nostart (method_table(last_object_in_last_plug), last_object_in_last_plug, last_obj_size, pval,
-                {
-                    size_t gap_offset = (((size_t)pval - (size_t)(post_plug - sizeof (gap_reloc_pair) - plug_skew))) / sizeof (uint8_t*);
-                    dprintf (3, ("member: %p->%p, %zd ptrs from beginning of gap", (uint8_t*)pval, *pval, gap_offset));
-                    m.set_post_short_bit (gap_offset);
-                }
-            );
-        }
-    }
-}
-#if defined(TARGET_AMD64) || defined(TARGET_X86) || defined(TARGET_ARM64) || defined(TARGET_RISCV64)
-#define PREFETCH
-#endif
-#ifdef PREFETCH
-inline void Prefetch(void* addr)
-{
-#ifdef TARGET_WINDOWS
-#if defined(TARGET_AMD64) || defined(TARGET_X86)
-#ifndef _MM_HINT_T0
-#define _MM_HINT_T0 1
-#endif
-    _mm_prefetch((const char*)addr, _MM_HINT_T0);
-#elif defined(TARGET_ARM64)
-    __prefetch((const char*)addr);
-#endif //defined(TARGET_AMD64) || defined(TARGET_X86)
-#elif defined(TARGET_UNIX)
-    __builtin_prefetch(addr);
-#else //!(TARGET_WINDOWS || TARGET_UNIX)
-    UNREFERENCED_PARAMETER(addr);
-#endif //TARGET_WINDOWS
-}
-#else //PREFETCH
-inline void Prefetch (void* addr)
-{
-    UNREFERENCED_PARAMETER(addr);
-}
-#endif //PREFETCH
-#ifdef MH_SC_MARK
-inline
-VOLATILE(uint8_t*)& gc_heap::ref_mark_stack (gc_heap* hp, int index)
-{
-    return ((VOLATILE(uint8_t*)*)(hp->mark_stack_array))[index];
-}
-#endif //MH_SC_MARK
-#define stolen 2
-#define partial 1
-#define partial_object 3
-inline
-uint8_t* ref_from_slot (uint8_t* r)
-{
-    return (uint8_t*)((size_t)r & ~(stolen | partial));
-}
-inline
-BOOL stolen_p (uint8_t* r)
-{
-    return (((size_t)r&2) && !((size_t)r&1));
-}
-inline
-BOOL ready_p (uint8_t* r)
-{
-    return ((size_t)r != 1);
-}
-inline
-BOOL partial_p (uint8_t* r)
-{
-    return (((size_t)r&1) && !((size_t)r&2));
-}
-inline
-BOOL straight_ref_p (uint8_t* r)
-{
-    return (!stolen_p (r) && !partial_p (r));
-}
-inline
-BOOL partial_object_p (uint8_t* r)
-{
-    return (((size_t)r & partial_object) == partial_object);
-}
-inline
-BOOL ref_p (uint8_t* r)
-{
-    return (straight_ref_p (r) || partial_object_p (r));
-}
-mark_queue_t::mark_queue_t()
-#ifdef MARK_PHASE_PREFETCH
-    : curr_slot_index(0)
-#endif //MARK_PHASE_PREFETCH
-{
-#ifdef MARK_PHASE_PREFETCH
-    for (size_t i = 0; i < slot_count; i++)
-    {
-        slot_table[i] = nullptr;
-    }
-#endif //MARK_PHASE_PREFETCH
-}
-FORCEINLINE
-uint8_t *mark_queue_t::queue_mark(uint8_t *o)
-{
-#ifdef MARK_PHASE_PREFETCH
-    Prefetch (o);
-    size_t slot_index = curr_slot_index;
-    uint8_t* old_o = slot_table[slot_index];
-    slot_table[slot_index] = o;
-    curr_slot_index = (slot_index + 1) % slot_count;
-    if (old_o == nullptr)
-        return nullptr;
-#else //MARK_PHASE_PREFETCH
-    uint8_t* old_o = o;
-#endif //MARK_PHASE_PREFETCH
-    BOOL already_marked = marked (old_o);
-    if (already_marked)
-    {
-        return nullptr;
-    }
-    set_marked (old_o);
-    return old_o;
-}
-FORCEINLINE
-uint8_t *mark_queue_t::queue_mark(uint8_t *o, int condemned_gen)
-{
-#ifdef USE_REGIONS
-    if (!is_in_heap_range (o))
-    {
-        return nullptr;
-    }
-    if (condemned_gen != max_generation && gc_heap::get_region_gen_num (o) > condemned_gen)
-    {
-        return nullptr;
-    }
-    return queue_mark(o);
-#else //USE_REGIONS
-    assert (condemned_gen == -1);
-#ifdef MULTIPLE_HEAPS
-    if (o)
-    {
-        gc_heap* hp = gc_heap::heap_of_gc (o);
-        assert (hp);
-        if ((o >= hp->gc_low) && (o < hp->gc_high))
-            return queue_mark (o);
-    }
-#else //MULTIPLE_HEAPS
-    if ((o >= gc_heap::gc_low) && (o < gc_heap::gc_high))
-        return queue_mark (o);
-#endif //MULTIPLE_HEAPS
-    return nullptr;
-#endif //USE_REGIONS
-}
-uint8_t* mark_queue_t::get_next_marked()
-{
-#ifdef MARK_PHASE_PREFETCH
-    size_t slot_index = curr_slot_index;
-    size_t empty_slot_count = 0;
-    while (empty_slot_count < slot_count)
-    {
-        uint8_t* o = slot_table[slot_index];
-        slot_table[slot_index] = nullptr;
-        slot_index = (slot_index + 1) % slot_count;
-        if (o != nullptr)
-        {
-            BOOL already_marked = marked (o);
-            if (!already_marked)
-            {
-                set_marked (o);
-                curr_slot_index = slot_index;
-                return o;
-            }
-        }
-        empty_slot_count++;
-    }
-#endif //MARK_PHASE_PREFETCH
-    return nullptr;
-}
-void mark_queue_t::verify_empty()
-{
-#ifdef MARK_PHASE_PREFETCH
-    for (size_t slot_index = 0; slot_index < slot_count; slot_index++)
-    {
-        assert(slot_table[slot_index] == nullptr);
-    }
-#endif //MARK_PHASE_PREFETCH
-}
-void gc_heap::mark_object_simple1 (uint8_t* oo, uint8_t* start THREAD_NUMBER_DCL)
-{
-    SERVER_SC_MARK_VOLATILE(uint8_t*)* mark_stack_tos = (SERVER_SC_MARK_VOLATILE(uint8_t*)*)mark_stack_array;
-    SERVER_SC_MARK_VOLATILE(uint8_t*)* mark_stack_limit = (SERVER_SC_MARK_VOLATILE(uint8_t*)*)&mark_stack_array[mark_stack_array_length];
-    SERVER_SC_MARK_VOLATILE(uint8_t*)* mark_stack_base = mark_stack_tos;
-#ifdef SORT_MARK_STACK
-    SERVER_SC_MARK_VOLATILE(uint8_t*)* sorted_tos = mark_stack_base;
-#endif //SORT_MARK_STACK
-    BOOL  full_p = (settings.condemned_generation == max_generation);
-    int condemned_gen =
-#ifdef USE_REGIONS
-        settings.condemned_generation;
-#else
-        -1;
-#endif //USE_REGIONS
-    assert ((start >= oo) && (start < oo+size(oo)));
-#ifndef MH_SC_MARK
-    *mark_stack_tos = oo;
-#endif //!MH_SC_MARK
-    while (1)
-    {
-#ifdef MULTIPLE_HEAPS
-#else  //MULTIPLE_HEAPS
-        const int thread = 0;
-#endif //MULTIPLE_HEAPS
-        if (oo && ((size_t)oo != 4))
-        {
-            size_t s = 0;
-            if (stolen_p (oo))
-            {
-                --mark_stack_tos;
-                goto next_level;
-            }
-            else if (!partial_p (oo) && ((s = size (oo)) < (partial_size_th*sizeof (uint8_t*))))
-            {
-                BOOL overflow_p = FALSE;
-                if (mark_stack_tos + (s) /sizeof (uint8_t*) >= (mark_stack_limit  - 1))
-                {
-                    size_t num_components = ((method_table(oo))->HasComponentSize() ? ((CObjectHeader*)oo)->GetNumComponents() : 0);
-                    if (mark_stack_tos + CGCDesc::GetNumPointers(method_table(oo), s, num_components) >= (mark_stack_limit - 1))
-                    {
-                        overflow_p = TRUE;
-                    }
-                }
-                if (overflow_p == FALSE)
-                {
-                    dprintf(3,("pushing mark for %zx ", (size_t)oo));
-                    go_through_object_cl (method_table(oo), oo, s, ppslot,
-                                          {
-                                              uint8_t* o = mark_queue.queue_mark(*ppslot, condemned_gen);
-                                              if (o != nullptr)
-                                              {
-                                                  if (full_p)
-                                                  {
-                                                      m_boundary_fullgc (o);
-                                                  }
-                                                  else
-                                                  {
-                                                      m_boundary (o);
-                                                  }
-                                                  add_to_promoted_bytes (o, thread);
-                                                  if (contain_pointers_or_collectible (o))
-                                                  {
-                                                      *(mark_stack_tos++) = o;
-                                                  }
-                                              }
-                                          }
-                        );
-                }
-                else
-                {
-                    dprintf(3,("mark stack overflow for object %zx ", (size_t)oo));
-                    min_overflow_address = min (min_overflow_address, oo);
-                    max_overflow_address = max (max_overflow_address, oo);
-                }
-            }
-            else
-            {
-                if (partial_p (oo))
-                {
-                    start = ref_from_slot (oo);
-                    oo = ref_from_slot (*(--mark_stack_tos));
-                    dprintf (4, ("oo: %zx, start: %zx\n", (size_t)oo, (size_t)start));
-                    assert ((oo < start) && (start < (oo + size (oo))));
-                }
-#ifdef COLLECTIBLE_CLASS
-                else
-                {
-                    if (is_collectible (oo))
-                    {
-                        uint8_t* class_obj = get_class_object (oo);
-                        if (gc_mark (class_obj, gc_low, gc_high, condemned_gen))
-                        {
-                            if (full_p)
-                            {
-                                m_boundary_fullgc (class_obj);
-                            }
-                            else
-                            {
-                                m_boundary (class_obj);
-                            }
-                            add_to_promoted_bytes (class_obj, thread);
-                            *(mark_stack_tos++) = class_obj;
-                            *mark_stack_tos = oo;
-                        }
-                    }
-                    if (!contain_pointers (oo))
-                    {
-                        goto next_level;
-                    }
-                }
-#endif //COLLECTIBLE_CLASS
-                s = size (oo);
-                BOOL overflow_p = FALSE;
-                if (mark_stack_tos + (num_partial_refs + 2)  >= mark_stack_limit)
-                {
-                    overflow_p = TRUE;
-                }
-                if (overflow_p == FALSE)
-                {
-                    dprintf(3,("pushing mark for %zx ", (size_t)oo));
-                    SERVER_SC_MARK_VOLATILE(uint8_t*)* place = ++mark_stack_tos;
-                    mark_stack_tos++;
-#ifdef MH_SC_MARK
-                    *(place-1) = 0;
-                    *(place) = (uint8_t*)partial;
-#endif //MH_SC_MARK
-                    int i = num_partial_refs;
-                    uint8_t* ref_to_continue = 0;
-                    go_through_object (method_table(oo), oo, s, ppslot,
-                                       start, use_start, (oo + s),
-                                       {
-                                           uint8_t* o = mark_queue.queue_mark(*ppslot, condemned_gen);
-                                           if (o != nullptr)
-                                           {
-                                                if (full_p)
-                                                {
-                                                    m_boundary_fullgc (o);
-                                                }
-                                                else
-                                                {
-                                                    m_boundary (o);
-                                                }
-                                                add_to_promoted_bytes (o, thread);
-                                                if (contain_pointers_or_collectible (o))
-                                                {
-                                                    *(mark_stack_tos++) = o;
-                                                    if (--i == 0)
-                                                    {
-                                                        ref_to_continue = (uint8_t*)((size_t)(ppslot+1) | partial);
-                                                        goto more_to_do;
-                                                    }
-                                                }
-                                           }
-                                       }
-                        );
-                    assert (ref_to_continue == 0);
-#ifdef MH_SC_MARK
-                    assert ((*(place-1)) == (uint8_t*)0);
-#else //MH_SC_MARK
-                    *(place-1) = 0;
-#endif //MH_SC_MARK
-                    *place = 0;
-more_to_do:
-                    if (ref_to_continue)
-                    {
-#ifdef MH_SC_MARK
-                        assert ((*(place-1)) == (uint8_t*)0);
-                        *(place-1) = (uint8_t*)((size_t)oo | partial_object);
-                        assert (((*place) == (uint8_t*)1) || ((*place) == (uint8_t*)2));
-#endif //MH_SC_MARK
-                        *place = ref_to_continue;
-                    }
-                }
-                else
-                {
-                    dprintf(3,("mark stack overflow for object %zx ", (size_t)oo));
-                    min_overflow_address = min (min_overflow_address, oo);
-                    max_overflow_address = max (max_overflow_address, oo);
-                }
-            }
-#ifdef SORT_MARK_STACK
-            if (mark_stack_tos > sorted_tos + mark_stack_array_length/8)
-            {
-                rqsort1 (sorted_tos, mark_stack_tos-1);
-                sorted_tos = mark_stack_tos-1;
-            }
-#endif //SORT_MARK_STACK
-        }
-    next_level:
-        if (!(mark_stack_empty_p()))
-        {
-            oo = *(--mark_stack_tos);
-            start = oo;
-#ifdef SORT_MARK_STACK
-            sorted_tos = min ((size_t)sorted_tos, (size_t)mark_stack_tos);
-#endif //SORT_MARK_STACK
-        }
-        else
-            break;
-    }
-}
-#ifdef MH_SC_MARK
-BOOL same_numa_node_p (int hn1, int hn2)
-{
-    return (heap_select::find_numa_node_from_heap_no (hn1) == heap_select::find_numa_node_from_heap_no (hn2));
-}
-int find_next_buddy_heap (int this_heap_number, int current_buddy, int n_heaps)
-{
-    int hn = (current_buddy+1)%n_heaps;
-    while (hn != current_buddy)
-    {
-        if ((this_heap_number != hn) && (same_numa_node_p (this_heap_number, hn)))
-            return hn;
-        hn = (hn+1)%n_heaps;
-    }
-    return current_buddy;
-}
-void
-gc_heap::mark_steal()
-{
-    mark_stack_busy() = 0;
-    for (int i = 0; i < max_snoop_level; i++)
-    {
-        ((VOLATILE(uint8_t*)*)(mark_stack_array))[i] = 0;
-    }
-    int thpn = find_next_buddy_heap (heap_number, heap_number, n_heaps);
-#ifdef SNOOP_STATS
-        dprintf (SNOOP_LOG, ("(GC%d)heap%d: start snooping %d", settings.gc_index, heap_number, (heap_number+1)%n_heaps));
-        uint64_t begin_tick = GCToOSInterface::GetLowPrecisionTimeStamp();
-#endif //SNOOP_STATS
-    int idle_loop_count = 0;
-    int first_not_ready_level = 0;
-    while (1)
-    {
-        gc_heap* hp = g_heaps [thpn];
-        int level = first_not_ready_level;
-        first_not_ready_level = 0;
-        while (check_next_mark_stack (hp) && (level < (max_snoop_level-1)))
-        {
-            idle_loop_count = 0;
-#ifdef SNOOP_STATS
-            snoop_stat.busy_count++;
-            dprintf (SNOOP_LOG, ("heap%d: looking at next heap level %d stack contents: %zx",
-                                 heap_number, level, (int)((uint8_t**)(hp->mark_stack_array))[level]));
-#endif //SNOOP_STATS
-            uint8_t* o = ref_mark_stack (hp, level);
-            uint8_t* start = o;
-            if (ref_p (o))
-            {
-                mark_stack_busy() = 1;
-                BOOL success = TRUE;
-                uint8_t* next = (ref_mark_stack (hp, level+1));
-                if (ref_p (next))
-                {
-                    if (((size_t)o > 4) && !partial_object_p (o))
-                    {
-                        success = (Interlocked::CompareExchangePointer (&ref_mark_stack (hp, level), (uint8_t*)4, o)==o);
-#ifdef SNOOP_STATS
-                        snoop_stat.interlocked_count++;
-                        if (success)
-                            snoop_stat.normal_count++;
-#endif //SNOOP_STATS
-                    }
-                    else
-                    {
-                        level++;
-#ifdef SNOOP_STATS
-                        snoop_stat.stolen_or_pm_count++;
-#endif //SNOOP_STATS
-                        success = FALSE;
-                    }
-                }
-                else if (stolen_p (next))
-                {
-                    success = FALSE;
-                    level+=2;
-#ifdef SNOOP_STATS
-                    snoop_stat.stolen_entry_count++;
-#endif //SNOOP_STATS
-                }
-                else
-                {
-                    assert (partial_p (next));
-                    start = ref_from_slot (next);
-                    o = ref_from_slot (ref_mark_stack (hp, level));
-                    if (o && start)
-                    {
-                        success = (Interlocked::CompareExchangePointer (&ref_mark_stack (hp, level+1),
-                                                                        (uint8_t*)stolen, next) == next);
-#ifdef SNOOP_STATS
-                        snoop_stat.interlocked_count++;
-                        if (success)
-                        {
-                            snoop_stat.partial_mark_parent_count++;
-                        }
-#endif //SNOOP_STATS
-                    }
-                    else
-                    {
-                        success = FALSE;
-                        if (first_not_ready_level == 0)
-                        {
-                            first_not_ready_level = level;
-                        }
-                        level+=2;
-#ifdef SNOOP_STATS
-                        snoop_stat.pm_not_ready_count++;
-#endif //SNOOP_STATS
-                    }
-                }
-                if (success)
-                {
-#ifdef SNOOP_STATS
-                    dprintf (SNOOP_LOG, ("heap%d: marking %zx from %d [%d] tl:%dms",
-                            heap_number, (size_t)o, (heap_number+1)%n_heaps, level,
-                            (GCToOSInterface::GetLowPrecisionTimeStamp()-begin_tick)));
-                    uint64_t start_tick = GCToOSInterface::GetLowPrecisionTimeStamp();
-#endif //SNOOP_STATS
-                    mark_object_simple1 (o, start, heap_number);
-#ifdef SNOOP_STATS
-                    dprintf (SNOOP_LOG, ("heap%d: done marking %zx from %d [%d] %dms tl:%dms",
-                            heap_number, (size_t)o, (heap_number+1)%n_heaps, level,
-                            (GCToOSInterface::GetLowPrecisionTimeStamp()-start_tick),(GCToOSInterface::GetLowPrecisionTimeStamp()-begin_tick)));
-#endif //SNOOP_STATS
-                    mark_stack_busy() = 0;
-                    for (int i = 0; i < max_snoop_level; i++)
-                    {
-                        if (((uint8_t**)mark_stack_array)[i] != 0)
-                        {
-                            ((VOLATILE(uint8_t*)*)(mark_stack_array))[i] = 0;
-#ifdef SNOOP_STATS
-                            snoop_stat.stack_bottom_clear_count++;
-#endif //SNOOP_STATS
-                        }
-                    }
-                    level = 0;
-                }
-                mark_stack_busy() = 0;
-            }
-            else
-            {
-                level++;
-            }
-        }
-        if ((first_not_ready_level != 0) && hp->mark_stack_busy())
-        {
-            continue;
-        }
-        if (!hp->mark_stack_busy())
-        {
-            first_not_ready_level = 0;
-            idle_loop_count++;
-            if ((idle_loop_count % (6) )==1)
-            {
-#ifdef SNOOP_STATS
-                snoop_stat.switch_to_thread_count++;
-#endif //SNOOP_STATS
-                GCToOSInterface::Sleep(1);
-            }
-            int free_count = 1;
-#ifdef SNOOP_STATS
-            snoop_stat.stack_idle_count++;
-#endif //SNOOP_STATS
-            for (int hpn = (heap_number+1)%n_heaps; hpn != heap_number;)
-            {
-                if (!((g_heaps [hpn])->mark_stack_busy()))
-                {
-                    free_count++;
-#ifdef SNOOP_STATS
-                dprintf (SNOOP_LOG, ("heap%d: %d idle", heap_number, free_count));
-#endif //SNOOP_STATS
-                }
-                else if (same_numa_node_p (hpn, heap_number) || ((idle_loop_count%1000))==999)
-                {
-                    thpn = hpn;
-                    break;
-                }
-                hpn = (hpn+1)%n_heaps;
-                YieldProcessor();
-            }
-            if (free_count == n_heaps)
-            {
-                break;
-            }
-        }
-    }
-}
-inline
-BOOL gc_heap::check_next_mark_stack (gc_heap* next_heap)
-{
-#ifdef SNOOP_STATS
-    snoop_stat.check_level_count++;
-#endif //SNOOP_STATS
-    return (next_heap->mark_stack_busy()>=1);
-}
-#endif //MH_SC_MARK
-#ifdef SNOOP_STATS
-void gc_heap::print_snoop_stat()
-{
-    dprintf (1234, ("%4s | %8s | %8s | %8s | %8s | %8s | %8s | %8s",
-        "heap", "check", "zero", "mark", "stole", "pstack", "nstack", "nonsk"));
-    dprintf (1234, ("%4d | %8d | %8d | %8d | %8d | %8d | %8d | %8d",
-        snoop_stat.heap_index,
-        snoop_stat.objects_checked_count,
-        snoop_stat.zero_ref_count,
-        snoop_stat.objects_marked_count,
-        snoop_stat.stolen_stack_count,
-        snoop_stat.partial_stack_count,
-        snoop_stat.normal_stack_count,
-        snoop_stat.non_stack_count));
-    dprintf (1234, ("%4s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s",
-        "heap", "level", "busy", "xchg", "pmparent", "s_pm", "stolen", "nready", "clear"));
-    dprintf (1234, ("%4d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d\n",
-        snoop_stat.heap_index,
-        snoop_stat.check_level_count,
-        snoop_stat.busy_count,
-        snoop_stat.interlocked_count,
-        snoop_stat.partial_mark_parent_count,
-        snoop_stat.stolen_or_pm_count,
-        snoop_stat.stolen_entry_count,
-        snoop_stat.pm_not_ready_count,
-        snoop_stat.normal_count,
-        snoop_stat.stack_bottom_clear_count));
-    printf ("\n%4s | %8s | %8s | %8s | %8s | %8s\n",
-        "heap", "check", "zero", "mark", "idle", "switch");
-    printf ("%4d | %8d | %8d | %8d | %8d | %8d\n",
-        snoop_stat.heap_index,
-        snoop_stat.objects_checked_count,
-        snoop_stat.zero_ref_count,
-        snoop_stat.objects_marked_count,
-        snoop_stat.stack_idle_count,
-        snoop_stat.switch_to_thread_count);
-    printf ("%4s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s\n",
-        "heap", "level", "busy", "xchg", "pmparent", "s_pm", "stolen", "nready", "normal", "clear");
-    printf ("%4d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d\n",
-        snoop_stat.heap_index,
-        snoop_stat.check_level_count,
-        snoop_stat.busy_count,
-        snoop_stat.interlocked_count,
-        snoop_stat.partial_mark_parent_count,
-        snoop_stat.stolen_or_pm_count,
-        snoop_stat.stolen_entry_count,
-        snoop_stat.pm_not_ready_count,
-        snoop_stat.normal_count,
-        snoop_stat.stack_bottom_clear_count);
-}
-#endif //SNOOP_STATS
-#ifdef HEAP_ANALYZE
-void
-gc_heap::ha_mark_object_simple (uint8_t** po THREAD_NUMBER_DCL)
-{
-    if (!internal_root_array)
-    {
-        internal_root_array = new (nothrow) uint8_t* [internal_root_array_length];
-        if (!internal_root_array)
-        {
-            heap_analyze_success = FALSE;
-        }
-    }
-    if (heap_analyze_success && (internal_root_array_length <= internal_root_array_index))
-    {
-        size_t new_size = 2*internal_root_array_length;
-        uint64_t available_physical = 0;
-        get_memory_info (NULL, &available_physical);
-        if (new_size > (size_t)(available_physical / 10))
-        {
-            heap_analyze_success = FALSE;
-        }
-        else
-        {
-            uint8_t** tmp = new (nothrow) uint8_t* [new_size];
-            if (tmp)
-            {
-                memcpy (tmp, internal_root_array,
-                        internal_root_array_length*sizeof (uint8_t*));
-                delete[] internal_root_array;
-                internal_root_array = tmp;
-                internal_root_array_length = new_size;
-            }
-            else
-            {
-                heap_analyze_success = FALSE;
-            }
-        }
-    }
-    if (heap_analyze_success)
-    {
-        PREFIX_ASSUME(internal_root_array_index < internal_root_array_length);
-        uint8_t* ref = (uint8_t*)po;
-        if (!current_obj ||
-            !((ref >= current_obj) && (ref < (current_obj + current_obj_size))))
-        {
-            gc_heap* hp = gc_heap::heap_of (ref);
-            current_obj = hp->find_object (ref);
-            current_obj_size = size (current_obj);
-            internal_root_array[internal_root_array_index] = current_obj;
-            internal_root_array_index++;
-        }
-    }
-    mark_object_simple (po THREAD_NUMBER_ARG);
-}
-#endif //HEAP_ANALYZE
-void
-gc_heap::mark_object_simple (uint8_t** po THREAD_NUMBER_DCL)
-{
-    int condemned_gen =
-#ifdef USE_REGIONS
-        settings.condemned_generation;
-#else
-        -1;
-#endif //USE_REGIONS
-    uint8_t* o = *po;
-#ifndef MULTIPLE_HEAPS
-    const int thread = 0;
-#endif //MULTIPLE_HEAPS
-    {
-#ifdef SNOOP_STATS
-        snoop_stat.objects_checked_count++;
-#endif //SNOOP_STATS
-        o = mark_queue.queue_mark (o);
-        if (o != nullptr)
-        {
-            m_boundary (o);
-            size_t s = size (o);
-            add_to_promoted_bytes (o, s, thread);
-            {
-                go_through_object_cl (method_table(o), o, s, poo,
-                                        {
-                                            uint8_t* oo = mark_queue.queue_mark(*poo, condemned_gen);
-                                            if (oo != nullptr)
-                                            {
-                                                m_boundary (oo);
-                                                add_to_promoted_bytes (oo, thread);
-                                                if (contain_pointers_or_collectible (oo))
-                                                    mark_object_simple1 (oo, oo THREAD_NUMBER_ARG);
-                                            }
-                                        }
-                    );
-            }
-        }
-    }
-}
-inline
-void gc_heap::mark_object (uint8_t* o THREAD_NUMBER_DCL)
-{
-#ifdef USE_REGIONS
-    if (is_in_gc_range (o) && is_in_condemned_gc (o))
-    {
-        mark_object_simple (&o THREAD_NUMBER_ARG);
-    }
-#else //USE_REGIONS
-    if ((o >= gc_low) && (o < gc_high))
-        mark_object_simple (&o THREAD_NUMBER_ARG);
-#ifdef MULTIPLE_HEAPS
-    else if (o)
-    {
-        gc_heap* hp = heap_of (o);
-        assert (hp);
-        if ((o >= hp->gc_low) && (o < hp->gc_high))
-            mark_object_simple (&o THREAD_NUMBER_ARG);
-    }
-#endif //MULTIPLE_HEAPS
-#endif //USE_REGIONS
-}
-void gc_heap::drain_mark_queue ()
-{
-    int condemned_gen =
-#ifdef USE_REGIONS
-        settings.condemned_generation;
-#else
-        -1;
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-    THREAD_FROM_HEAP;
-#else
-    const int thread = 0;
-#endif //MULTIPLE_HEAPS
-    uint8_t* o;
-    while ((o = mark_queue.get_next_marked()) != nullptr)
-    {
-        m_boundary (o);
-        size_t s = size (o);
-        add_to_promoted_bytes (o, s, thread);
-        if (contain_pointers_or_collectible (o))
-        {
-            go_through_object_cl (method_table(o), o, s, poo,
-                                    {
-                                        uint8_t* oo = mark_queue.queue_mark(*poo, condemned_gen);
-                                        if (oo != nullptr)
-                                        {
-                                            m_boundary (oo);
-                                            add_to_promoted_bytes (oo, thread);
-                                            if (contain_pointers_or_collectible (oo))
-                                                mark_object_simple1 (oo, oo THREAD_NUMBER_ARG);
-                                        }
-                                    }
-                );
-        }
-    }
-}
-#ifdef BACKGROUND_GC
-#ifdef USE_REGIONS
-void gc_heap::set_background_overflow_p (uint8_t* oo)
-{
-    heap_segment* overflow_region = get_region_info_for_address (oo);
-    overflow_region->flags |= heap_segment_flags_overflow;
-    dprintf (3,("setting overflow flag for region %p", heap_segment_mem (overflow_region)));
-    background_overflow_p = TRUE;
-}
-#endif //USE_REGIONS
-void gc_heap::background_mark_simple1 (uint8_t* oo THREAD_NUMBER_DCL)
-{
-    uint8_t** mark_stack_limit = &background_mark_stack_array[background_mark_stack_array_length];
-#ifdef SORT_MARK_STACK
-    uint8_t** sorted_tos = background_mark_stack_array;
-#endif //SORT_MARK_STACK
-    background_mark_stack_tos = background_mark_stack_array;
-    while (1)
-    {
-#ifdef MULTIPLE_HEAPS
-#else  //MULTIPLE_HEAPS
-        const int thread = 0;
-#endif //MULTIPLE_HEAPS
-        if (oo)
-        {
-            size_t s = 0;
-            if ((((size_t)oo & 1) == 0) && ((s = size (oo)) < (partial_size_th*sizeof (uint8_t*))))
-            {
-                BOOL overflow_p = FALSE;
-                if (background_mark_stack_tos + (s) /sizeof (uint8_t*) >= (mark_stack_limit - 1))
-                {
-                    size_t num_components = ((method_table(oo))->HasComponentSize() ? ((CObjectHeader*)oo)->GetNumComponents() : 0);
-                    size_t num_pointers = CGCDesc::GetNumPointers(method_table(oo), s, num_components);
-                    if (background_mark_stack_tos + num_pointers >= (mark_stack_limit - 1))
-                    {
-                        dprintf (2, ("h%d: %zd left, obj (mt: %p) %zd ptrs",
-                            heap_number,
-                            (size_t)(mark_stack_limit - 1 - background_mark_stack_tos),
-                            method_table(oo),
-                            num_pointers));
-                        bgc_overflow_count++;
-                        overflow_p = TRUE;
-                    }
-                }
-                if (overflow_p == FALSE)
-                {
-                    dprintf(3,("pushing mark for %zx ", (size_t)oo));
-                    go_through_object_cl (method_table(oo), oo, s, ppslot,
-                    {
-                        uint8_t* o = *ppslot;
-                        Prefetch(o);
-                        if (background_mark (o,
-                                             background_saved_lowest_address,
-                                             background_saved_highest_address))
-                        {
-                            size_t obj_size = size (o);
-                            bpromoted_bytes (thread) += obj_size;
-                            if (contain_pointers_or_collectible (o))
-                            {
-                                *(background_mark_stack_tos++) = o;
-                            }
-                        }
-                    }
-                        );
-                }
-                else
-                {
-                    dprintf (3,("background mark stack overflow for object %zx ", (size_t)oo));
-#ifdef USE_REGIONS
-                    set_background_overflow_p (oo);
-#else //USE_REGIONS
-                    background_min_overflow_address = min (background_min_overflow_address, oo);
-                    background_max_overflow_address = max (background_max_overflow_address, oo);
-#endif //USE_REGIONS
-                }
-            }
-            else
-            {
-                uint8_t* start = oo;
-                if ((size_t)oo & 1)
-                {
-                    oo = (uint8_t*)((size_t)oo & ~1);
-                    start = *(--background_mark_stack_tos);
-                    dprintf (4, ("oo: %zx, start: %zx\n", (size_t)oo, (size_t)start));
-                }
-#ifdef COLLECTIBLE_CLASS
-                else
-                {
-                    if (is_collectible (oo))
-                    {
-                        uint8_t* class_obj = get_class_object (oo);
-                        if (background_mark (class_obj,
-                                            background_saved_lowest_address,
-                                            background_saved_highest_address))
-                        {
-                            size_t obj_size = size (class_obj);
-                            bpromoted_bytes (thread) += obj_size;
-                            *(background_mark_stack_tos++) = class_obj;
-                        }
-                    }
-                    if (!contain_pointers (oo))
-                    {
-                        goto next_level;
-                    }
-                }
-#endif //COLLECTIBLE_CLASS
-                s = size (oo);
-                BOOL overflow_p = FALSE;
-                if (background_mark_stack_tos + (num_partial_refs + 2)  >= mark_stack_limit)
-                {
-                    size_t num_components = ((method_table(oo))->HasComponentSize() ? ((CObjectHeader*)oo)->GetNumComponents() : 0);
-                    size_t num_pointers = CGCDesc::GetNumPointers(method_table(oo), s, num_components);
-                    dprintf (2, ("h%d: PM: %zd left, obj %p (mt: %p) start: %p, total: %zd",
-                        heap_number,
-                        (size_t)(mark_stack_limit - background_mark_stack_tos),
-                        oo,
-                        method_table(oo),
-                        start,
-                        num_pointers));
-                    bgc_overflow_count++;
-                    overflow_p = TRUE;
-                }
-                if (overflow_p == FALSE)
-                {
-                    dprintf(3,("pushing mark for %zx ", (size_t)oo));
-                    uint8_t** place = background_mark_stack_tos++;
-                    *(place) = start;
-                    *(background_mark_stack_tos++) = (uint8_t*)((size_t)oo | 1);
-                    int num_pushed_refs = num_partial_refs;
-                    int num_processed_refs = num_pushed_refs * 16;
-                    go_through_object (method_table(oo), oo, s, ppslot,
-                                       start, use_start, (oo + s),
-                    {
-                        uint8_t* o = *ppslot;
-                        Prefetch(o);
-                        if (background_mark (o,
-                                            background_saved_lowest_address,
-                                            background_saved_highest_address))
-                        {
-                            size_t obj_size = size (o);
-                            bpromoted_bytes (thread) += obj_size;
-                            if (contain_pointers_or_collectible (o))
-                            {
-                                *(background_mark_stack_tos++) = o;
-                                if (--num_pushed_refs == 0)
-                                {
-                                    *place = (uint8_t*)(ppslot+1);
-                                    goto more_to_do;
-                                }
-                            }
-                        }
-                        if (--num_processed_refs == 0)
-                        {
-                            *place = (uint8_t*)(ppslot + 1);
-                            goto more_to_do;
-                        }
-                        }
-                        );
-                    *place = 0;
-                    *(place+1) = 0;
-                more_to_do:;
-                }
-                else
-                {
-                    dprintf (3,("background mark stack overflow for object %zx ", (size_t)oo));
-#ifdef USE_REGIONS
-                    set_background_overflow_p (oo);
-#else //USE_REGIONS
-                    background_min_overflow_address = min (background_min_overflow_address, oo);
-                    background_max_overflow_address = max (background_max_overflow_address, oo);
-#endif //USE_REGIONS
-                }
-            }
-        }
-#ifdef SORT_MARK_STACK
-        if (background_mark_stack_tos > sorted_tos + mark_stack_array_length/8)
-        {
-            rqsort1 (sorted_tos, background_mark_stack_tos-1);
-            sorted_tos = background_mark_stack_tos-1;
-        }
-#endif //SORT_MARK_STACK
-#ifdef COLLECTIBLE_CLASS
-next_level:
-#endif // COLLECTIBLE_CLASS
-        allow_fgc();
-        if (!(background_mark_stack_tos == background_mark_stack_array))
-        {
-            oo = *(--background_mark_stack_tos);
-#ifdef SORT_MARK_STACK
-            sorted_tos = (uint8_t**)min ((size_t)sorted_tos, (size_t)background_mark_stack_tos);
-#endif //SORT_MARK_STACK
-        }
-        else
-            break;
-    }
-    assert (background_mark_stack_tos == background_mark_stack_array);
-}
-void
-gc_heap::background_mark_simple (uint8_t* o THREAD_NUMBER_DCL)
-{
-#ifdef MULTIPLE_HEAPS
-#else  //MULTIPLE_HEAPS
-    const int thread = 0;
-#endif //MULTIPLE_HEAPS
-    {
-        dprintf (3, ("bmarking %p", o));
-        if (background_mark1 (o))
-        {
-            size_t s = size (o);
-            bpromoted_bytes (thread) += s;
-            if (contain_pointers_or_collectible (o))
-            {
-                background_mark_simple1 (o THREAD_NUMBER_ARG);
-            }
-        }
-        allow_fgc();
-    }
-}
-inline
-uint8_t* gc_heap::background_mark_object (uint8_t* o THREAD_NUMBER_DCL)
-{
-    if ((o >= background_saved_lowest_address) && (o < background_saved_highest_address))
-    {
-        background_mark_simple (o THREAD_NUMBER_ARG);
-    }
-    else
-    {
-        if (o)
-        {
-            dprintf (3, ("or-%p", o));
-        }
-    }
-    return o;
-}
-void gc_heap::background_promote (Object** ppObject, ScanContext* sc, uint32_t flags)
-{
-    UNREFERENCED_PARAMETER(sc);
-    assert (settings.concurrent);
-    THREAD_NUMBER_FROM_CONTEXT;
-#ifndef MULTIPLE_HEAPS
-    const int thread = 0;
-#endif //!MULTIPLE_HEAPS
-    uint8_t* o = (uint8_t*)*ppObject;
-    if (!is_in_find_object_range (o))
-    {
-        return;
-    }
-#ifdef DEBUG_DestroyedHandleValue
-    if (o == (uint8_t*)DEBUG_DestroyedHandleValue)
-        return;
-#endif //DEBUG_DestroyedHandleValue
-    HEAP_FROM_THREAD;
-    gc_heap* hp = gc_heap::heap_of (o);
-    if ((o < hp->background_saved_lowest_address) || (o >= hp->background_saved_highest_address))
-    {
-        return;
-    }
-    if (flags & GC_CALL_INTERIOR)
-    {
-        o = hp->find_object (o);
-        if (o == 0)
-            return;
-    }
-#ifdef FEATURE_CONSERVATIVE_GC
-    if (GCConfig::GetConservativeGC() && ((CObjectHeader*)o)->IsFree())
-    {
-        return;
-    }
-#endif //FEATURE_CONSERVATIVE_GC
-#ifdef _DEBUG
-    ((CObjectHeader*)o)->Validate();
-#endif //_DEBUG
-    STRESS_LOG3(LF_GC|LF_GCROOTS, LL_INFO1000000, "    GCHeap::Promote: Promote GC Root *%p = %p MT = %pT", ppObject, o, o ? ((Object*) o)->GetGCSafeMethodTable() : NULL);
-    hpt->background_mark_simple (o THREAD_NUMBER_ARG);
-}
-void
-gc_heap::scan_background_roots (promote_func* fn, int hn, ScanContext *pSC)
-{
-    ScanContext sc;
-    if (pSC == 0)
-        pSC = &sc;
-    pSC->thread_number = hn;
-    pSC->thread_count = n_heaps;
-    BOOL relocate_p = (fn == &GCHeap::Relocate);
-    dprintf (3, ("Scanning background mark list"));
-    size_t mark_list_finger = 0;
-    while (mark_list_finger < c_mark_list_index)
-    {
-        uint8_t** o = &c_mark_list [mark_list_finger];
-        if (!relocate_p)
-        {
-            size_t s = size (*o);
-            assert (Align (s) >= Align (min_obj_size));
-            dprintf(3,("background root %zx", (size_t)*o));
-        }
-        (*fn) ((Object**)o, pSC, 0);
-        mark_list_finger++;
-    }
-    dprintf (3, ("Scanning background mark stack"));
-    uint8_t** finger = background_mark_stack_array;
-    while (finger < background_mark_stack_tos)
-    {
-        if ((finger + 1) < background_mark_stack_tos)
-        {
-            uint8_t* parent_obj = *(finger + 1);
-            if ((size_t)parent_obj & 1)
-            {
-                uint8_t* place = *finger;
-                size_t place_offset = 0;
-                uint8_t* real_parent_obj = (uint8_t*)((size_t)parent_obj & ~1);
-                if (relocate_p)
-                {
-                    *(finger + 1) = real_parent_obj;
-                    place_offset = place - real_parent_obj;
-                    dprintf(3,("relocating background root %zx", (size_t)real_parent_obj));
-                    (*fn) ((Object**)(finger + 1), pSC, 0);
-                    real_parent_obj = *(finger + 1);
-                    *finger = real_parent_obj + place_offset;
-                    *(finger + 1) = (uint8_t*)((size_t)real_parent_obj | 1);
-                    dprintf(3,("roots changed to %p, %p", *finger, *(finger + 1)));
-                }
-                else
-                {
-                    uint8_t** temp = &real_parent_obj;
-                    dprintf(3,("marking background root %zx", (size_t)real_parent_obj));
-                    (*fn) ((Object**)temp, pSC, 0);
-                }
-                finger += 2;
-                continue;
-            }
-        }
-        dprintf(3,("background root %zx", (size_t)*finger));
-        (*fn) ((Object**)finger, pSC, 0);
-        finger++;
-    }
-}
-void gc_heap::grow_bgc_mark_stack (size_t new_size)
-{
-    if ((background_mark_stack_array_length < new_size) &&
-        ((new_size - background_mark_stack_array_length) > (background_mark_stack_array_length / 2)))
-    {
-        dprintf (2, ("h%d: ov grow to %zd", heap_number, new_size));
-        uint8_t** tmp = new (nothrow) uint8_t* [new_size];
-        if (tmp)
-        {
-            delete [] background_mark_stack_array;
-            background_mark_stack_array = tmp;
-            background_mark_stack_array_length = new_size;
-            background_mark_stack_tos = background_mark_stack_array;
-        }
-    }
-}
-void gc_heap::check_bgc_mark_stack_length()
-{
-    if ((settings.condemned_generation < (max_generation - 1)) || gc_heap::background_running_p())
-        return;
-    size_t total_heap_size = get_total_heap_size();
-    if (total_heap_size < ((size_t)4*1024*1024*1024))
-        return;
-#ifdef MULTIPLE_HEAPS
-    int total_heaps = n_heaps;
-#else
-    int total_heaps = 1;
-#endif //MULTIPLE_HEAPS
-    size_t size_based_on_heap = total_heap_size / (size_t)(100 * 100 * total_heaps * sizeof (uint8_t*));
-    size_t new_size = max (background_mark_stack_array_length, size_based_on_heap);
-    grow_bgc_mark_stack (new_size);
-}
-uint8_t* gc_heap::background_seg_end (heap_segment* seg, BOOL concurrent_p)
-{
-#ifndef USE_REGIONS
-    if (concurrent_p && (seg == saved_overflow_ephemeral_seg))
-    {
-        return background_min_soh_overflow_address;
-    }
-    else
-#endif //!USE_REGIONS
-    {
-        return heap_segment_allocated (seg);
-    }
-}
-uint8_t* gc_heap::background_first_overflow (uint8_t* min_add,
-                                          heap_segment* seg,
-                                          BOOL concurrent_p,
-                                          BOOL small_object_p)
-{
-#ifdef USE_REGIONS
-        return heap_segment_mem (seg);
-#else
-    uint8_t* o = 0;
-    if (small_object_p)
-    {
-        if (in_range_for_segment (min_add, seg))
-        {
-            if (min_add >= heap_segment_allocated (seg))
-            {
-                return min_add;
-            }
-            else
-            {
-                if (concurrent_p &&
-                    ((seg == saved_overflow_ephemeral_seg) && (min_add >= background_min_soh_overflow_address)))
-                {
-                    return background_min_soh_overflow_address;
-                }
-                else
-                {
-                    o = find_first_object (min_add, heap_segment_mem (seg));
-                    return o;
-                }
-            }
-        }
-    }
-    o = max (heap_segment_mem (seg), min_add);
-    return o;
-#endif //USE_REGIONS
-}
-void gc_heap::background_process_mark_overflow_internal (uint8_t* min_add, uint8_t* max_add,
-                                                         BOOL concurrent_p)
-{
-    if (concurrent_p)
-    {
-        current_bgc_state = bgc_overflow_soh;
-    }
-    size_t total_marked_objects = 0;
-#ifdef MULTIPLE_HEAPS
-    int thread = heap_number;
-#endif //MULTIPLE_HEAPS
-    int start_gen_idx = get_start_generation_index();
-#ifdef USE_REGIONS
-    if (concurrent_p)
-        start_gen_idx = max_generation;
-#endif //USE_REGIONS
-    exclusive_sync* loh_alloc_lock = 0;
-#ifndef USE_REGIONS
-    dprintf (2,("Processing Mark overflow [%zx %zx]", (size_t)min_add, (size_t)max_add));
-#endif
-#ifdef MULTIPLE_HEAPS
-    int h_start = (concurrent_p ? heap_number : 0);
-    int h_end = (concurrent_p ? (heap_number + 1) : n_heaps);
-    for (int hi = h_start; hi < h_end; hi++)
-    {
-        gc_heap*  hp = (concurrent_p ? this : g_heaps [(heap_number + hi) % n_heaps]);
-#else
-    {
-        gc_heap*  hp = 0;
-#endif //MULTIPLE_HEAPS
-        BOOL small_object_segments = TRUE;
-        loh_alloc_lock = hp->bgc_alloc_lock;
-        for (int i = start_gen_idx; i < total_generation_count; i++)
-        {
-            int align_const = get_alignment_constant (small_object_segments);
-            generation* gen = hp->generation_of (i);
-            heap_segment* seg = heap_segment_in_range (generation_start_segment (gen));
-            PREFIX_ASSUME(seg != NULL);
-            uint8_t* current_min_add = min_add;
-            uint8_t* current_max_add = max_add;
-            while (seg)
-            {
-#ifdef USE_REGIONS
-                if (heap_segment_overflow_p (seg))
-                {
-                    seg->flags &= ~heap_segment_flags_overflow;
-                    current_min_add = heap_segment_mem (seg);
-                    current_max_add = heap_segment_allocated (seg);
-                    dprintf (2,("Processing Mark overflow [%zx %zx]", (size_t)current_min_add, (size_t)current_max_add));
-                }
-                else
-                {
-                    current_min_add = current_max_add = 0;
-                }
-#endif //USE_REGIONS
-                uint8_t* o = hp->background_first_overflow (current_min_add, seg, concurrent_p, small_object_segments);
-                while ((o < hp->background_seg_end (seg, concurrent_p)) && (o <= current_max_add))
-                {
-                    dprintf (3, ("considering %zx", (size_t)o));
-                    size_t s;
-                    if (concurrent_p && !small_object_segments)
-                    {
-                        loh_alloc_lock->bgc_mark_set (o);
-                        if (((CObjectHeader*)o)->IsFree())
-                        {
-                            s = unused_array_size (o);
-                        }
-                        else
-                        {
-                            s = size (o);
-                        }
-                    }
-                    else
-                    {
-                        s = size (o);
-                    }
-                    if (background_object_marked (o, FALSE) && contain_pointers_or_collectible (o))
-                    {
-                        total_marked_objects++;
-                        go_through_object_cl (method_table(o), o, s, poo,
-                                              uint8_t* oo = *poo;
-                                              background_mark_object (oo THREAD_NUMBER_ARG);
-                                             );
-                    }
-                    if (concurrent_p && !small_object_segments)
-                    {
-                        loh_alloc_lock->bgc_mark_done ();
-                    }
-                    o = o + Align (s, align_const);
-                    if (concurrent_p)
-                    {
-                        allow_fgc();
-                    }
-                }
-#ifdef USE_REGIONS
-                if (current_max_add != 0)
-#endif //USE_REGIONS
-                {
-                    dprintf (2, ("went through overflow objects in segment %p (%d) (so far %zd marked)",
-                        heap_segment_mem (seg), (small_object_segments ? 0 : 1), total_marked_objects));
-                }
-#ifndef USE_REGIONS
-                if (concurrent_p && (seg == hp->saved_overflow_ephemeral_seg))
-                {
-                    break;
-                }
-#endif //!USE_REGIONS
-                seg = heap_segment_next_in_range (seg);
-            }
-            if (concurrent_p)
-            {
-                current_bgc_state = bgc_overflow_uoh;
-            }
-            dprintf (2, ("h%d: SOH: ov-mo: %zd", heap_number, total_marked_objects));
-            fire_overflow_event (min_add, max_add, total_marked_objects, i);
-            if (i >= soh_gen2)
-            {
-                concurrent_print_time_delta (concurrent_p ? "Cov SOH" : "Nov SOH");
-                small_object_segments = FALSE;
-            }
-            total_marked_objects = 0;
-        }
-    }
-}
-BOOL gc_heap::background_process_mark_overflow (BOOL concurrent_p)
-{
-    BOOL grow_mark_array_p = TRUE;
-    if (concurrent_p)
-    {
-        assert (!processed_eph_overflow_p);
-#ifndef USE_REGIONS
-        if ((background_max_overflow_address != 0) &&
-            (background_min_overflow_address != MAX_PTR))
-        {
-            saved_overflow_ephemeral_seg = ephemeral_heap_segment;
-            background_max_soh_overflow_address = heap_segment_reserved (saved_overflow_ephemeral_seg);
-            background_min_soh_overflow_address = generation_allocation_start (generation_of (max_generation - 1));
-        }
-#endif //!USE_REGIONS
-    }
-    else
-    {
-#ifndef USE_REGIONS
-        assert ((saved_overflow_ephemeral_seg == 0) ||
-                ((background_max_soh_overflow_address != 0) &&
-                 (background_min_soh_overflow_address != MAX_PTR)));
-#endif //!USE_REGIONS
-        if (!processed_eph_overflow_p)
-        {
-#ifdef USE_REGIONS
-            if (!background_overflow_p)
-#else
-            if ((background_max_overflow_address == 0) && (background_min_overflow_address == MAX_PTR))
-#endif //USE_REGIONS
-            {
-                dprintf (2, ("final processing mark overflow - no more overflow since last time"));
-                grow_mark_array_p = FALSE;
-            }
-#ifdef USE_REGIONS
-            background_overflow_p = TRUE;
-#else
-            background_min_overflow_address = min (background_min_overflow_address,
-                                                background_min_soh_overflow_address);
-            background_max_overflow_address = max (background_max_overflow_address,
-                                                background_max_soh_overflow_address);
-#endif //!USE_REGIONS
-            processed_eph_overflow_p = TRUE;
-        }
-    }
-    BOOL  overflow_p = FALSE;
-recheck:
-#ifdef USE_REGIONS
-    if (background_overflow_p)
-#else
-    if ((! ((background_max_overflow_address == 0)) ||
-         ! ((background_min_overflow_address == MAX_PTR))))
-#endif
-    {
-        overflow_p = TRUE;
-        if (grow_mark_array_p)
-        {
-            size_t new_size = max (MARK_STACK_INITIAL_LENGTH, 2*background_mark_stack_array_length);
-            if ((new_size * sizeof(mark)) > 100*1024)
-            {
-                size_t new_max_size = (get_total_heap_size() / 10) / sizeof(mark);
-                new_size = min(new_max_size, new_size);
-            }
-            grow_bgc_mark_stack (new_size);
-        }
-        else
-        {
-            grow_mark_array_p = TRUE;
-        }
-#ifdef USE_REGIONS
-        uint8_t*  min_add = 0;
-        uint8_t*  max_add = 0;
-        background_overflow_p = FALSE;
-#else
-        uint8_t*  min_add = background_min_overflow_address;
-        uint8_t*  max_add = background_max_overflow_address;
-        background_max_overflow_address = 0;
-        background_min_overflow_address = MAX_PTR;
-#endif
-        background_process_mark_overflow_internal (min_add, max_add, concurrent_p);
-        if (!concurrent_p)
-        {
-            goto recheck;
-        }
-    }
-    return overflow_p;
-}
-#endif //BACKGROUND_GC
-inline
-void gc_heap::mark_through_object (uint8_t* oo, BOOL mark_class_object_p THREAD_NUMBER_DCL)
-{
-#ifndef COLLECTIBLE_CLASS
-    UNREFERENCED_PARAMETER(mark_class_object_p);
-    BOOL to_mark_class_object = FALSE;
-#else //COLLECTIBLE_CLASS
-    BOOL to_mark_class_object = (mark_class_object_p && (is_collectible(oo)));
-#endif //COLLECTIBLE_CLASS
-    if (contain_pointers (oo) || to_mark_class_object)
-    {
-        dprintf(3,( "Marking through %zx", (size_t)oo));
-        size_t s = size (oo);
-#ifdef COLLECTIBLE_CLASS
-        if (to_mark_class_object)
-        {
-            uint8_t* class_obj = get_class_object (oo);
-            mark_object (class_obj THREAD_NUMBER_ARG);
-        }
-#endif //COLLECTIBLE_CLASS
-        if (contain_pointers (oo))
-        {
-            go_through_object_nostart (method_table(oo), oo, s, po,
-                                uint8_t* o = *po;
-                                mark_object (o THREAD_NUMBER_ARG);
-                                );
-        }
-    }
-}
-size_t gc_heap::get_total_heap_size()
-{
-    size_t total_heap_size = 0;
-#ifdef MULTIPLE_HEAPS
-    int hn = 0;
-    for (hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp2 = gc_heap::g_heaps [hn];
-        for (int i = max_generation; i < total_generation_count; i++)
-        {
-            total_heap_size += hp2->generation_sizes (hp2->generation_of (i));
-        }
-    }
-#else
-    for (int i = max_generation; i < total_generation_count; i++)
-    {
-        total_heap_size += generation_sizes (generation_of (i));
-    }
-#endif //MULTIPLE_HEAPS
-    return total_heap_size;
-}
-size_t gc_heap::get_total_fragmentation()
-{
-    size_t total_fragmentation = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[hn];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        for (int i = 0; i < total_generation_count; i++)
-        {
-            generation* gen = hp->generation_of (i);
-            total_fragmentation += (generation_free_list_space (gen) + generation_free_obj_space (gen));
-        }
-    }
-    return total_fragmentation;
-}
-size_t gc_heap::get_total_gen_fragmentation (int gen_number)
-{
-    size_t total_fragmentation = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[hn];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        generation* gen = hp->generation_of (gen_number);
-        total_fragmentation += (generation_free_list_space (gen) + generation_free_obj_space (gen));
-    }
-    return total_fragmentation;
-}
-#ifdef USE_REGIONS
-int gc_heap::get_total_new_gen0_regions_in_plns ()
-{
-    int total_new_gen0_regions_in_plns = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[hn];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        total_new_gen0_regions_in_plns += hp->new_gen0_regions_in_plns;
-    }
-    return total_new_gen0_regions_in_plns;
-}
-int gc_heap::get_total_new_regions_in_prr ()
-{
-    int total_new_regions_in_prr = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[hn];
-#else //MULTIPLE_HEAPS
-        {
-            gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-            total_new_regions_in_prr += hp->new_regions_in_prr;
-        }
-        return total_new_regions_in_prr;
-}
-int gc_heap::get_total_new_regions_in_threading ()
-{
-    int total_new_regions_in_threading = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[hn];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        total_new_regions_in_threading += hp->new_regions_in_threading;
-    }
-    return total_new_regions_in_threading;
-}
-#endif //USE_REGIONS
-size_t gc_heap::get_total_gen_estimated_reclaim (int gen_number)
-{
-    size_t total_estimated_reclaim = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[hn];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        total_estimated_reclaim += hp->estimated_reclaim (gen_number);
-    }
-    return total_estimated_reclaim;
-}
-size_t gc_heap::get_total_gen_size (int gen_number)
-{
-#ifdef MULTIPLE_HEAPS
-    size_t size = 0;
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[hn];
-        size += hp->generation_size (gen_number);
-    }
-#else
-    size_t size = generation_size (gen_number);
-#endif //MULTIPLE_HEAPS
-    return size;
-}
-size_t gc_heap::committed_size()
-{
-    size_t total_committed = 0;
-    const size_t kB = 1024;
-    for (int i = get_start_generation_index(); i < total_generation_count; i++)
-    {
-        generation* gen = generation_of (i);
-        heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-        size_t gen_committed = 0;
-        size_t gen_allocated = 0;
-        while (seg)
-        {
-            uint8_t* start =
-#ifdef USE_REGIONS
-                get_region_start (seg);
-#else
-                (uint8_t*)seg;
-#endif //USE_REGIONS
-            gen_committed += heap_segment_committed (seg) - start;
-            gen_allocated += heap_segment_allocated (seg) - start;
-            seg = heap_segment_next (seg);
-        }
-        dprintf (3, ("h%d committed in gen%d %zdkB, allocated %zdkB, committed-allocated %zdkB", heap_number, i, gen_committed/kB, gen_allocated/kB, (gen_committed - gen_allocated)/kB));
-        total_committed += gen_committed;
-    }
-#ifdef USE_REGIONS
-    size_t committed_in_free = 0;
-    for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
-    {
-        committed_in_free += free_regions[kind].get_size_committed_in_free();
-    }
-    dprintf (3, ("h%d committed in free %zdkB", heap_number, committed_in_free/kB));
-    total_committed += committed_in_free;
-#endif //USE_REGIONS
-    return total_committed;
-}
-size_t gc_heap::get_total_committed_size()
-{
-    size_t total_committed = 0;
-#ifdef MULTIPLE_HEAPS
-    int hn = 0;
-    for (hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps [hn];
-        total_committed += hp->committed_size();
-    }
-#else
-    total_committed = committed_size();
-#endif //MULTIPLE_HEAPS
-    return total_committed;
-}
-size_t gc_heap::uoh_committed_size (int gen_number, size_t* allocated)
-{
-    generation* gen = generation_of (gen_number);
-    heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-    size_t total_committed = 0;
-    size_t total_allocated = 0;
-    while (seg)
-    {
-        uint8_t* start =
-#ifdef USE_REGIONS
-            get_region_start (seg);
-#else
-            (uint8_t*)seg;
-#endif //USE_REGIONS
-        total_committed += heap_segment_committed (seg) - start;
-        total_allocated += heap_segment_allocated (seg) - start;
-        seg = heap_segment_next (seg);
-    }
-    *allocated = total_allocated;
-    return total_committed;
-}
-void gc_heap::get_memory_info (uint32_t* memory_load,
-                               uint64_t* available_physical,
-                               uint64_t* available_page_file)
-{
-    GCToOSInterface::GetMemoryStatus(is_restricted_physical_mem ? total_physical_mem  : 0,  memory_load, available_physical, available_page_file);
-}
-BOOL gc_heap::process_mark_overflow(int condemned_gen_number)
-{
-    size_t last_promoted_bytes = get_promoted_bytes();
-    BOOL  overflow_p = FALSE;
-recheck:
-    drain_mark_queue();
-    if ((! (max_overflow_address == 0) ||
-         ! (min_overflow_address == MAX_PTR)))
-    {
-        overflow_p = TRUE;
-        size_t new_size =
-            max (MARK_STACK_INITIAL_LENGTH, 2*mark_stack_array_length);
-        if ((new_size * sizeof(mark)) > 100*1024)
-        {
-            size_t new_max_size = (get_total_heap_size() / 10) / sizeof(mark);
-            new_size = min(new_max_size, new_size);
-        }
-        if ((mark_stack_array_length < new_size) &&
-            ((new_size - mark_stack_array_length) > (mark_stack_array_length / 2)))
-        {
-            mark* tmp = new (nothrow) mark [new_size];
-            if (tmp)
-            {
-                delete mark_stack_array;
-                mark_stack_array = tmp;
-                mark_stack_array_length = new_size;
-            }
-        }
-        uint8_t*  min_add = min_overflow_address;
-        uint8_t*  max_add = max_overflow_address;
-        max_overflow_address = 0;
-        min_overflow_address = MAX_PTR;
-        process_mark_overflow_internal (condemned_gen_number, min_add, max_add);
-        goto recheck;
-    }
-    size_t current_promoted_bytes = get_promoted_bytes();
-    if (current_promoted_bytes != last_promoted_bytes)
-        fire_mark_event (ETW::GC_ROOT_OVERFLOW, current_promoted_bytes, last_promoted_bytes);
-    return overflow_p;
-}
-void gc_heap::process_mark_overflow_internal (int condemned_gen_number,
-                                              uint8_t* min_add, uint8_t* max_add)
-{
-#ifdef MULTIPLE_HEAPS
-    int thread = heap_number;
-#endif //MULTIPLE_HEAPS
-    BOOL  full_p = (condemned_gen_number == max_generation);
-    dprintf(3,("Processing Mark overflow [%zx %zx]", (size_t)min_add, (size_t)max_add));
-    size_t obj_count = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int hi = 0; hi < n_heaps; hi++)
-    {
-        gc_heap*  hp = g_heaps [(heap_number + hi) % n_heaps];
-#else
-    {
-        gc_heap*  hp = 0;
-#endif //MULTIPLE_HEAPS
-        int gen_limit = full_p ? total_generation_count : condemned_gen_number + 1;
-        for (int i = get_stop_generation_index (condemned_gen_number); i < gen_limit; i++)
-        {
-            generation* gen = hp->generation_of (i);
-            heap_segment* seg = heap_segment_in_range (generation_start_segment (gen));
-            int align_const = get_alignment_constant (i < uoh_start_generation);
-            PREFIX_ASSUME(seg != NULL);
-            while (seg)
-            {
-                uint8_t*  o = max (heap_segment_mem (seg), min_add);
-                uint8_t*  end = heap_segment_allocated (seg);
-                while ((o < end) && (o <= max_add))
-                {
-                    assert ((min_add <= o) && (max_add >= o));
-                    dprintf (3, ("considering %zx", (size_t)o));
-                    if (marked (o))
-                    {
-                        mark_through_object (o, TRUE THREAD_NUMBER_ARG);
-                        obj_count++;
-                    }
-                    o = o + Align (size (o), align_const);
-                }
-                seg = heap_segment_next_in_range (seg);
-            }
-        }
-#ifndef MULTIPLE_HEAPS
-        assert (obj_count > 0);
-#endif //MULTIPLE_HEAPS
-    }
-}
-#ifdef MULTIPLE_HEAPS
-static VOLATILE(BOOL) s_fUnpromotedHandles = FALSE;
-static VOLATILE(BOOL) s_fUnscannedPromotions = FALSE;
-static VOLATILE(BOOL) s_fScanRequired;
-void gc_heap::scan_dependent_handles (int condemned_gen_number, ScanContext *sc, BOOL initial_scan_p)
-{
-    s_fUnscannedPromotions = TRUE;
-    while (true)
-    {
-        if (GCScan::GcDhUnpromotedHandlesExist(sc))
-            s_fUnpromotedHandles = TRUE;
-        drain_mark_queue();
-        gc_t_join.join(this, gc_join_scan_dependent_handles);
-        if (gc_t_join.joined())
-        {
-            s_fScanRequired = s_fUnscannedPromotions && s_fUnpromotedHandles;
-            s_fUnscannedPromotions = FALSE;
-            s_fUnpromotedHandles = FALSE;
-            if (!s_fScanRequired)
-            {
-                if (!initial_scan_p)
-                {
-                    uint8_t* all_heaps_max = 0;
-                    uint8_t* all_heaps_min = MAX_PTR;
-                    int i;
-                    for (i = 0; i < n_heaps; i++)
-                    {
-                        if (all_heaps_max < g_heaps[i]->max_overflow_address)
-                            all_heaps_max = g_heaps[i]->max_overflow_address;
-                        if (all_heaps_min > g_heaps[i]->min_overflow_address)
-                            all_heaps_min = g_heaps[i]->min_overflow_address;
-                    }
-                    for (i = 0; i < n_heaps; i++)
-                    {
-                        g_heaps[i]->max_overflow_address = all_heaps_max;
-                        g_heaps[i]->min_overflow_address = all_heaps_min;
-                    }
-                }
-            }
-            dprintf(3, ("Starting all gc thread mark stack overflow processing"));
-            gc_t_join.restart();
-        }
-        if (process_mark_overflow(condemned_gen_number))
-            s_fUnscannedPromotions = TRUE;
-        if (!s_fScanRequired)
-            break;
-        gc_t_join.join(this, gc_join_rescan_dependent_handles);
-        if (gc_t_join.joined())
-        {
-            dprintf(3, ("Starting all gc thread for dependent handle promotion"));
-            gc_t_join.restart();
-        }
-        if (GCScan::GcDhUnpromotedHandlesExist(sc))
-            if (GCScan::GcDhReScan(sc))
-                s_fUnscannedPromotions = TRUE;
-    }
-}
-#else //MULTIPLE_HEAPS
-void gc_heap::scan_dependent_handles (int condemned_gen_number, ScanContext *sc, BOOL initial_scan_p)
-{
-    UNREFERENCED_PARAMETER(initial_scan_p);
-    bool fUnscannedPromotions = true;
-    while (GCScan::GcDhUnpromotedHandlesExist(sc) && fUnscannedPromotions)
-    {
-        fUnscannedPromotions = false;
-        if (process_mark_overflow(condemned_gen_number))
-            fUnscannedPromotions = true;
-        mark_queue.verify_empty();
-        if (GCScan::GcDhReScan(sc))
-            fUnscannedPromotions = true;
-    }
-    process_mark_overflow(condemned_gen_number);
-}
-#endif //MULTIPLE_HEAPS
-size_t gc_heap::get_generation_start_size (int gen_number)
-{
-#ifdef USE_REGIONS
-    return 0;
-#else
-    return Align (size (generation_allocation_start (generation_of (gen_number))),
-                  get_alignment_constant (gen_number <= max_generation));
-#endif //!USE_REGIONS
-}
-inline
-int gc_heap::get_num_heaps()
-{
-#ifdef MULTIPLE_HEAPS
-    return n_heaps;
-#else
-    return 1;
-#endif //MULTIPLE_HEAPS
-}
-BOOL gc_heap::decide_on_promotion_surv (size_t threshold)
-{
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        dynamic_data* dd = hp->dynamic_data_of (min ((settings.condemned_generation + 1), max_generation));
-        size_t older_gen_size = dd_current_size (dd) + (dd_desired_allocation (dd) - dd_new_allocation (dd));
-        size_t promoted = hp->total_promoted_bytes;
-        dprintf (2, ("promotion threshold: %zd, promoted bytes: %zd size n+1: %zd",
-            threshold, promoted, older_gen_size));
-        if ((threshold > (older_gen_size)) || (promoted > threshold))
-        {
-            return TRUE;
-        }
-    }
-    return FALSE;
-}
-inline
-void gc_heap::fire_mark_event (int root_type, size_t& current_promoted_bytes, size_t& last_promoted_bytes)
-{
-#ifdef FEATURE_EVENT_TRACE
-    if (informational_event_enabled_p)
-    {
-        current_promoted_bytes = get_promoted_bytes();
-        size_t root_promoted = current_promoted_bytes - last_promoted_bytes;
-        dprintf (3, ("h%d marked root %s: %zd (%zd - %zd)",
-            heap_number, str_root_kinds[root_type], root_promoted,
-            current_promoted_bytes, last_promoted_bytes));
-        FIRE_EVENT(GCMarkWithType, heap_number, root_type, root_promoted);
-        last_promoted_bytes = current_promoted_bytes;
-    }
-#endif // FEATURE_EVENT_TRACE
-}
-#ifdef FEATURE_EVENT_TRACE
-inline
-void gc_heap::record_mark_time (uint64_t& mark_time,
-                                uint64_t& current_mark_time,
-                                uint64_t& last_mark_time)
-{
-    if (informational_event_enabled_p)
-    {
-        current_mark_time = GetHighPrecisionTimeStamp();
-        mark_time = limit_time_to_uint32 (current_mark_time - last_mark_time);
-        dprintf (3, ("%zd - %zd = %zd",
-            current_mark_time, last_mark_time, (current_mark_time - last_mark_time)));
-        last_mark_time = current_mark_time;
-    }
-}
-#endif // FEATURE_EVENT_TRACE
-#ifdef USE_REGIONS
-void gc_heap::verify_region_to_generation_map()
-{
-#ifdef _DEBUG
-    uint8_t* local_ephemeral_low = MAX_PTR;
-    uint8_t* local_ephemeral_high = nullptr;
-    for (int gen_number = soh_gen0; gen_number < total_generation_count; gen_number++)
-    {
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            gc_heap* hp = g_heaps[i];
-#else //MULTIPLE_HEAPS
-        {
-            gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-            generation *gen = hp->generation_of (gen_number);
-            for (heap_segment *region = generation_start_segment (gen); region != nullptr; region = heap_segment_next (region))
-            {
-                if (heap_segment_read_only_p (region))
-                {
-                    continue;
-                }
-                size_t region_index_start = get_basic_region_index_for_address (get_region_start (region));
-                size_t region_index_end = get_basic_region_index_for_address (heap_segment_reserved (region));
-                int gen_num = min (gen_number, soh_gen2);
-                assert (gen_num == heap_segment_gen_num (region));
-                int plan_gen_num = heap_segment_plan_gen_num (region);
-                bool is_demoted = (region->flags & heap_segment_flags_demoted) != 0;
-                bool is_sweep_in_plan = heap_segment_swept_in_plan (region);
-                for (size_t region_index = region_index_start; region_index < region_index_end; region_index++)
-                {
-                    region_info region_info_bits = map_region_to_generation[region_index];
-                    assert ((region_info_bits & RI_GEN_MASK) == gen_num);
-                    assert ((region_info_bits >> RI_PLAN_GEN_SHR) == plan_gen_num);
-                    assert (((region_info_bits & RI_SIP) != 0) == is_sweep_in_plan);
-                    assert (((region_info_bits & RI_DEMOTED) != 0) == is_demoted);
-                }
-            }
-        }
-    }
-#endif //_DEBUG
-}
-void gc_heap::compute_gc_and_ephemeral_range (int condemned_gen_number, bool end_of_gc_p)
-{
-    ephemeral_low = MAX_PTR;
-    ephemeral_high = nullptr;
-    gc_low = MAX_PTR;
-    gc_high = nullptr;
-    if (condemned_gen_number >= soh_gen2 || end_of_gc_p)
-    {
-        gc_low = g_gc_lowest_address;
-        gc_high = g_gc_highest_address;
-    }
-    if (end_of_gc_p)
-    {
-#if 1
-        ephemeral_low = g_gc_lowest_address;
-#else
-        uint8_t* addr = g_gc_lowest_address;
-        while (true)
-        {
-            heap_segment* region = get_region_info (addr);
-            if (is_free_region (region))
-                break;
-            if (heap_segment_gen_num (region) <= soh_gen1)
-                break;
-            addr += ((size_t)1) << min_segment_size_shr;
-        }
-        ephemeral_low = addr;
-#endif
-        ephemeral_high = g_gc_highest_address;
-    }
-    else
-    {
-        for (int gen_number = soh_gen0; gen_number <= soh_gen1; gen_number++)
-        {
-#ifdef MULTIPLE_HEAPS
-            for (int i = 0; i < n_heaps; i++)
-            {
-                gc_heap* hp = g_heaps[i];
-#else //MULTIPLE_HEAPS
-            {
-                gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-                generation *gen = hp->generation_of (gen_number);
-                for (heap_segment *region = generation_start_segment (gen); region != nullptr; region = heap_segment_next (region))
-                {
-                    ephemeral_low = min ((uint8_t*)ephemeral_low, get_region_start (region));
-                    ephemeral_high = max ((uint8_t*)ephemeral_high, heap_segment_reserved (region));
-                    if (gen_number <= condemned_gen_number)
-                    {
-                        gc_low = min (gc_low, get_region_start (region));
-                        gc_high = max (gc_high, heap_segment_reserved (region));
-                    }
-                }
-            }
-        }
-    }
-    dprintf (2, ("ephemeral_low = %p, ephemeral_high = %p, gc_low = %p, gc_high = %p", (uint8_t*)ephemeral_low, (uint8_t*)ephemeral_high, gc_low, gc_high));
-}
-#endif //USE_REGIONS
-void gc_heap::mark_phase (int condemned_gen_number, BOOL mark_only_p)
-{
-    assert (settings.concurrent == FALSE);
-    ScanContext sc;
-    sc.thread_number = heap_number;
-    sc.thread_count = n_heaps;
-    sc.promotion = TRUE;
-    sc.concurrent = FALSE;
-    dprintf (2, (ThreadStressLog::gcStartMarkMsg(), heap_number, condemned_gen_number));
-    BOOL  full_p = (condemned_gen_number == max_generation);
-    int gen_to_init = condemned_gen_number;
-    if (condemned_gen_number == max_generation)
-    {
-        gen_to_init = total_generation_count - 1;
-    }
-    for (int gen_idx = 0; gen_idx <= gen_to_init; gen_idx++)
-    {
-        dynamic_data* dd = dynamic_data_of (gen_idx);
-        dd_begin_data_size (dd) = generation_size (gen_idx) -
-                                   dd_fragmentation (dd) -
-#ifdef USE_REGIONS
-                                   0;
-#else
-                                   get_generation_start_size (gen_idx);
-#endif //USE_REGIONS
-        dprintf (2, ("begin data size for gen%d is %zd", gen_idx, dd_begin_data_size (dd)));
-        dd_survived_size (dd) = 0;
-        dd_pinned_survived_size (dd) = 0;
-        dd_artificial_pinned_survived_size (dd) = 0;
-        dd_added_pinned_size (dd) = 0;
-#ifdef SHORT_PLUGS
-        dd_padding_size (dd) = 0;
-#endif //SHORT_PLUGS
-#if defined (RESPECT_LARGE_ALIGNMENT) || defined (FEATURE_STRUCTALIGN)
-        dd_num_npinned_plugs (dd) = 0;
-#endif //RESPECT_LARGE_ALIGNMENT || FEATURE_STRUCTALIGN
-    }
-    if (gen0_must_clear_bricks > 0)
-        gen0_must_clear_bricks--;
-    size_t last_promoted_bytes = 0;
-    size_t current_promoted_bytes = 0;
-#if !defined(USE_REGIONS) || defined(_DEBUG)
-    init_promoted_bytes();
-#endif //!USE_REGIONS || _DEBUG
-    reset_mark_stack();
-#ifdef SNOOP_STATS
-    memset (&snoop_stat, 0, sizeof(snoop_stat));
-    snoop_stat.heap_index = heap_number;
-#endif //SNOOP_STATS
-#ifdef MH_SC_MARK
-    if (full_p)
-    {
-        for (int i = 0; i < max_snoop_level; i++)
-        {
-            ((uint8_t**)(mark_stack_array))[i] = 0;
-        }
-        mark_stack_busy() = 1;
-    }
-#endif //MH_SC_MARK
-    static uint32_t num_sizedrefs = 0;
-#ifdef MH_SC_MARK
-    static BOOL do_mark_steal_p = FALSE;
-#endif //MH_SC_MARK
-#ifdef FEATURE_CARD_MARKING_STEALING
-    reset_card_marking_enumerators();
-#endif // FEATURE_CARD_MARKING_STEALING
-#ifdef STRESS_REGIONS
-    heap_segment* gen0_region = generation_start_segment (generation_of (0));
-    while (gen0_region)
-    {
-        size_t gen0_region_size = heap_segment_allocated (gen0_region) - heap_segment_mem (gen0_region);
-        if (gen0_region_size > 0)
-        {
-            if ((num_gen0_regions % pinning_seg_interval) == 0)
-            {
-                dprintf (REGIONS_LOG, ("h%d potentially creating pinning in region %zx",
-                    heap_number, heap_segment_mem (gen0_region)));
-                int align_const = get_alignment_constant (TRUE);
-                uint8_t* boundary = heap_segment_mem (gen0_region);
-                uint8_t* obj_to_pin = boundary;
-                int num_pinned_objs = 0;
-                while (obj_to_pin < heap_segment_allocated (gen0_region))
-                {
-                    if (obj_to_pin >= boundary && !((CObjectHeader*)obj_to_pin)->IsFree())
-                    {
-                        pin_by_gc (obj_to_pin);
-                        num_pinned_objs++;
-                        if (num_pinned_objs >= 2)
-                            break;
-                        boundary += (gen0_region_size / 2) + 1;
-                    }
-                    obj_to_pin += Align (size (obj_to_pin), align_const);
-                }
-            }
-        }
-        num_gen0_regions++;
-        gen0_region = heap_segment_next (gen0_region);
-    }
-#endif //STRESS_REGIONS
-#ifdef FEATURE_EVENT_TRACE
-    static uint64_t current_mark_time = 0;
-    static uint64_t last_mark_time = 0;
-#endif //FEATURE_EVENT_TRACE
-#ifdef USE_REGIONS
-    special_sweep_p = false;
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-    gc_t_join.join(this, gc_join_begin_mark_phase);
-    if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-        maxgen_size_inc_p = false;
-#ifdef USE_REGIONS
-        region_count = global_region_allocator.get_used_region_count();
-        grow_mark_list_piece();
-        verify_region_to_generation_map();
-        compute_gc_and_ephemeral_range (condemned_gen_number, false);
-#endif //USE_REGIONS
-        GCToEEInterface::BeforeGcScanRoots(condemned_gen_number, /* is_bgc */ false, /* is_concurrent */ false);
-        num_sizedrefs = GCToEEInterface::GetTotalNumSizedRefHandles();
-#ifdef FEATURE_EVENT_TRACE
-        informational_event_enabled_p = EVENT_ENABLED (GCMarkWithType);
-        if (informational_event_enabled_p)
-        {
-            last_mark_time = GetHighPrecisionTimeStamp();
-            gc_time_info[time_mark_sizedref] = 0;
-        }
-#endif //FEATURE_EVENT_TRACE
-#ifdef MULTIPLE_HEAPS
-#ifdef MH_SC_MARK
-        if (full_p)
-        {
-            size_t total_heap_size = get_total_heap_size();
-            if (total_heap_size > (100 * 1024 * 1024))
-            {
-                do_mark_steal_p = TRUE;
-            }
-            else
-            {
-                do_mark_steal_p = FALSE;
-            }
-        }
-        else
-        {
-            do_mark_steal_p = FALSE;
-        }
-#endif //MH_SC_MARK
-        gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-    {
-        assert (g_mark_list);
-#ifdef MULTIPLE_HEAPS
-        mark_list_size = g_mark_list_total_size / n_heaps;
-        mark_list = &g_mark_list [heap_number*mark_list_size];
-#else
-        mark_list = g_mark_list;
-#endif //MULTIPLE_HEAPS
-        if (condemned_gen_number < max_generation)
-            mark_list_end = &mark_list [mark_list_size-1];
-        else
-            mark_list_end = &mark_list [0];
-        mark_list_index = &mark_list [0];
-#ifdef USE_REGIONS
-        if (g_mark_list_piece != nullptr)
-        {
-#ifdef MULTIPLE_HEAPS
-            mark_list_piece_start = &g_mark_list_piece[heap_number * 2 * g_mark_list_piece_size];
-            mark_list_piece_end = &mark_list_piece_start[g_mark_list_piece_size];
-#endif //MULTIPLE_HEAPS
-            survived_per_region = (size_t*)&g_mark_list_piece[heap_number * 2 * g_mark_list_piece_size];
-            old_card_survived_per_region = (size_t*)&survived_per_region[g_mark_list_piece_size];
-            size_t region_info_to_clear = region_count * sizeof (size_t);
-            memset (survived_per_region, 0, region_info_to_clear);
-            memset (old_card_survived_per_region, 0, region_info_to_clear);
-        }
-        else
-        {
-#ifdef MULTIPLE_HEAPS
-            mark_list_piece_start = nullptr;
-            mark_list_piece_end = nullptr;
-            mark_list_end = &mark_list[0];
-#endif //MULTIPLE_HEAPS
-            survived_per_region = nullptr;
-            old_card_survived_per_region = nullptr;
-        }
-#endif // USE_REGIONS && MULTIPLE_HEAPS
-#ifndef MULTIPLE_HEAPS
-        shigh = (uint8_t*) 0;
-        slow  = MAX_PTR;
-#endif //MULTIPLE_HEAPS
-        if ((condemned_gen_number == max_generation) && (num_sizedrefs > 0))
-        {
-            GCScan::GcScanSizedRefs(GCHeap::Promote, condemned_gen_number, max_generation, &sc);
-            drain_mark_queue();
-            fire_mark_event (ETW::GC_ROOT_SIZEDREF, current_promoted_bytes, last_promoted_bytes);
-#ifdef MULTIPLE_HEAPS
-            gc_t_join.join(this, gc_join_scan_sizedref_done);
-            if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-            {
-#ifdef FEATURE_EVENT_TRACE
-                record_mark_time (gc_time_info[time_mark_sizedref], current_mark_time, last_mark_time);
-#endif //FEATURE_EVENT_TRACE
-#ifdef MULTIPLE_HEAPS
-                dprintf(3, ("Done with marking all sized refs. Starting all gc thread for marking other strong roots"));
-                gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-            }
-        }
-#ifdef FEATURE_BASICFREEZE
-#ifdef USE_REGIONS
-        assert (!ro_segments_in_range);
-#else //USE_REGIONS
-        if (ro_segments_in_range)
-        {
-            dprintf(3,("Marking in range ro segments"));
-            mark_ro_segments();
-        }
-#endif //USE_REGIONS
-#endif //FEATURE_BASICFREEZE
-        dprintf(3,("Marking Roots"));
-        GCScan::GcScanRoots(GCHeap::Promote,
-                                condemned_gen_number, max_generation,
-                                &sc);
-        drain_mark_queue();
-        fire_mark_event (ETW::GC_ROOT_STACK, current_promoted_bytes, last_promoted_bytes);
-#ifdef BACKGROUND_GC
-        if (gc_heap::background_running_p())
-        {
-            scan_background_roots (GCHeap::Promote, heap_number, &sc);
-            drain_mark_queue();
-            fire_mark_event (ETW::GC_ROOT_BGC, current_promoted_bytes, last_promoted_bytes);
-        }
-#endif //BACKGROUND_GC
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-        dprintf(3, ("Marking finalization data"));
-        finalize_queue->GcScanRoots(GCHeap::Promote, heap_number, 0);
-        drain_mark_queue();
-        fire_mark_event (ETW::GC_ROOT_FQ, current_promoted_bytes, last_promoted_bytes);
-#endif // FEATURE_PREMORTEM_FINALIZATION
-        dprintf(3,("Marking handle table"));
-        GCScan::GcScanHandles(GCHeap::Promote,
-                                    condemned_gen_number, max_generation,
-                                    &sc);
-        drain_mark_queue();
-        fire_mark_event (ETW::GC_ROOT_HANDLES, current_promoted_bytes, last_promoted_bytes);
-        if (!full_p)
-        {
-#ifdef USE_REGIONS
-            save_current_survived();
-#endif //USE_REGIONS
-#ifdef FEATURE_CARD_MARKING_STEALING
-            n_eph_soh = 0;
-            n_gen_soh = 0;
-            n_eph_loh = 0;
-            n_gen_loh = 0;
-#endif //FEATURE_CARD_MARKING_STEALING
-#ifdef CARD_BUNDLE
-#ifdef MULTIPLE_HEAPS
-            if (gc_t_join.r_join(this, gc_r_join_update_card_bundle))
-            {
-#endif //MULTIPLE_HEAPS
-#ifndef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-                update_card_table_bundle();
-#endif
-                if (card_bundles_enabled())
-                {
-                    verify_card_bundles();
-                }
-#ifdef MULTIPLE_HEAPS
-                gc_t_join.r_restart();
-            }
-#endif //MULTIPLE_HEAPS
-#endif //CARD_BUNDLE
-            card_fn mark_object_fn = &gc_heap::mark_object_simple;
-#ifdef HEAP_ANALYZE
-            heap_analyze_success = TRUE;
-            if (heap_analyze_enabled)
-            {
-                internal_root_array_index = 0;
-                current_obj = 0;
-                current_obj_size = 0;
-                mark_object_fn = &gc_heap::ha_mark_object_simple;
-            }
-#endif //HEAP_ANALYZE
-#if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
-            if (!card_mark_done_soh)
-#endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
-            {
-                dprintf (3, ("Marking cross generation pointers on heap %d", heap_number));
-                mark_through_cards_for_segments(mark_object_fn, FALSE THIS_ARG);
-#if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
-                card_mark_done_soh = true;
-#endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
-            }
-#if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
-            if (!card_mark_done_uoh)
-#endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
-            {
-                dprintf (3, ("Marking cross generation pointers for uoh objects on heap %d", heap_number));
-                for (int i = uoh_start_generation; i < total_generation_count; i++)
-                {
-#ifndef ALLOW_REFERENCES_IN_POH
-                    if (i != poh_generation)
-#endif //ALLOW_REFERENCES_IN_POH
-                        mark_through_cards_for_uoh_objects(mark_object_fn, i, FALSE THIS_ARG);
-                }
-#if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
-                card_mark_done_uoh = true;
-#endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
-            }
-#if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
-            for (int i = 0; i < gc_heap::n_heaps; i++)
-            {
-                int heap_number_to_look_at = (i + heap_number) % gc_heap::n_heaps;
-                gc_heap* hp = gc_heap::g_heaps[heap_number_to_look_at];
-                if (!hp->card_mark_done_soh)
-                {
-                    dprintf(3, ("Marking cross generation pointers on heap %d", hp->heap_number));
-                    hp->mark_through_cards_for_segments(mark_object_fn, FALSE THIS_ARG);
-                    hp->card_mark_done_soh = true;
-                }
-                if (!hp->card_mark_done_uoh)
-                {
-                    dprintf(3, ("Marking cross generation pointers for large objects on heap %d", hp->heap_number));
-                    for (int i = uoh_start_generation; i < total_generation_count; i++)
-                    {
-#ifndef ALLOW_REFERENCES_IN_POH
-                        if (i != poh_generation)
-#endif //ALLOW_REFERENCES_IN_POH
-                            hp->mark_through_cards_for_uoh_objects(mark_object_fn, i, FALSE THIS_ARG);
-                    }
-                    hp->card_mark_done_uoh = true;
-                }
-            }
-#endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
-#ifdef USE_REGIONS
-            update_old_card_survived();
-#endif //USE_REGIONS
-            drain_mark_queue();
-            fire_mark_event (ETW::GC_ROOT_OLDER, current_promoted_bytes, last_promoted_bytes);
-        }
-    }
-#ifdef MH_SC_MARK
-    if (do_mark_steal_p)
-    {
-        mark_steal();
-        drain_mark_queue();
-        fire_mark_event (ETW::GC_ROOT_STEAL, current_promoted_bytes, last_promoted_bytes);
-    }
-#endif //MH_SC_MARK
-    GCScan::GcDhInitialScan(GCHeap::Promote, condemned_gen_number, max_generation, &sc);
-    scan_dependent_handles(condemned_gen_number, &sc, true);
-    mark_queue.verify_empty();
-    fire_mark_event (ETW::GC_ROOT_DH_HANDLES, current_promoted_bytes, last_promoted_bytes);
-#ifdef MULTIPLE_HEAPS
-    dprintf(3, ("Joining for short weak handle scan"));
-    gc_t_join.join(this, gc_join_null_dead_short_weak);
-    if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-#ifdef FEATURE_EVENT_TRACE
-        record_mark_time (gc_time_info[time_mark_roots], current_mark_time, last_mark_time);
-#endif //FEATURE_EVENT_TRACE
-        uint64_t promoted_bytes_global = 0;
-#ifdef HEAP_ANALYZE
-        heap_analyze_enabled = FALSE;
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            promoted_bytes_global += g_heaps[i]->get_promoted_bytes();
-        }
-#else
-        promoted_bytes_global = get_promoted_bytes();
-#endif //MULTIPLE_HEAPS
-        GCToEEInterface::AnalyzeSurvivorsFinished (settings.gc_index, condemned_gen_number, promoted_bytes_global, GCHeap::ReportGenerationBounds);
-#endif // HEAP_ANALYZE
-        GCToEEInterface::AfterGcScanRoots (condemned_gen_number, max_generation, &sc);
-#ifdef MULTIPLE_HEAPS
-        if (!full_p)
-        {
-            gc_t_join.r_init();
-        }
-        dprintf(3, ("Starting all gc thread for short weak handle scan"));
-        gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-#ifdef FEATURE_CARD_MARKING_STEALING
-    reset_card_marking_enumerators();
-    if (!full_p)
-    {
-        int generation_skip_ratio_soh = ((n_eph_soh > MIN_SOH_CROSS_GEN_REFS) ?
-                                         (int)(((float)n_gen_soh / (float)n_eph_soh) * 100) : 100);
-        int generation_skip_ratio_loh = ((n_eph_loh > MIN_LOH_CROSS_GEN_REFS) ?
-                                         (int)(((float)n_gen_loh / (float)n_eph_loh) * 100) : 100);
-        generation_skip_ratio = min (generation_skip_ratio_soh, generation_skip_ratio_loh);
-        dprintf (2, ("h%d skip ratio soh: %d, loh: %d", heap_number,
-            generation_skip_ratio_soh, generation_skip_ratio_loh));
-    }
-#endif // FEATURE_CARD_MARKING_STEALING
-    GCScan::GcShortWeakPtrScan (condemned_gen_number, max_generation,&sc);
-#ifdef MULTIPLE_HEAPS
-    dprintf(3, ("Joining for finalization"));
-    gc_t_join.join(this, gc_join_scan_finalization);
-    if (gc_t_join.joined())
-    {
-#endif //MULTIPLE_HEAPS
-#ifdef FEATURE_EVENT_TRACE
-        record_mark_time (gc_time_info[time_mark_short_weak], current_mark_time, last_mark_time);
-#endif //FEATURE_EVENT_TRACE
-#ifdef MULTIPLE_HEAPS
-        dprintf(3, ("Starting all gc thread for Finalization"));
-        gc_t_join.restart();
-    }
-#endif //MULTIPLE_HEAPS
-    size_t promoted_bytes_live = get_promoted_bytes();
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-    dprintf (3, ("Finalize marking"));
-    finalize_queue->ScanForFinalization (GCHeap::Promote, condemned_gen_number, mark_only_p, __this);
-    drain_mark_queue();
-    fire_mark_event (ETW::GC_ROOT_NEW_FQ, current_promoted_bytes, last_promoted_bytes);
-    GCToEEInterface::DiagWalkFReachableObjects(__this);
-    scan_dependent_handles(condemned_gen_number, &sc, false);
-    mark_queue.verify_empty();
-    fire_mark_event (ETW::GC_ROOT_DH_HANDLES, current_promoted_bytes, last_promoted_bytes);
-#endif //FEATURE_PREMORTEM_FINALIZATION
-    total_promoted_bytes = get_promoted_bytes();
-#ifdef MULTIPLE_HEAPS
-    static VOLATILE(int32_t) syncblock_scan_p;
-    dprintf(3, ("Joining for weak pointer deletion"));
-    gc_t_join.join(this, gc_join_null_dead_long_weak);
-    if (gc_t_join.joined())
-    {
-        dprintf(3, ("Starting all gc thread for weak pointer deletion"));
-#endif //MULTIPLE_HEAPS
-#ifdef FEATURE_EVENT_TRACE
-        record_mark_time (gc_time_info[time_mark_scan_finalization], current_mark_time, last_mark_time);
-#endif //FEATURE_EVENT_TRACE
-#ifdef USE_REGIONS
-        sync_promoted_bytes();
-        equalize_promoted_bytes(settings.condemned_generation);
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-        syncblock_scan_p = 0;
-        gc_t_join.restart();
-    }
-#endif //MULTIPLE_HEAPS
-    GCScan::GcWeakPtrScan (condemned_gen_number, max_generation, &sc);
-#ifdef MULTIPLE_HEAPS
-    size_t total_mark_list_size = sort_mark_list();
-    if ((syncblock_scan_p == 0) && (Interlocked::Increment(&syncblock_scan_p) == 1))
-#endif //MULTIPLE_HEAPS
-    {
-        GCScan::GcWeakPtrScanBySingleThread(condemned_gen_number, max_generation, &sc);
-    }
-#ifdef MULTIPLE_HEAPS
-    dprintf (3, ("Joining for sync block cache entry scanning"));
-    gc_t_join.join(this, gc_join_null_dead_syncblk);
-    if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-#ifdef FEATURE_EVENT_TRACE
-        record_mark_time (gc_time_info[time_plan - 1], current_mark_time, last_mark_time);
-        gc_time_info[time_plan] = last_mark_time;
-#endif //FEATURE_EVENT_TRACE
-        if (!settings.promotion)
-        {
-            size_t m = 0;
-            for (int n = 0; n <= condemned_gen_number;n++)
-            {
-#ifdef MULTIPLE_HEAPS
-                m +=  (size_t)(dd_min_size (dynamic_data_of (n))*(n+1)*0.1);
-#else
-                m +=  (size_t)(dd_min_size (dynamic_data_of (n))*(n+1)*0.06);
-#endif //MULTIPLE_HEAPS
-            }
-            settings.promotion = decide_on_promotion_surv (m);
-        }
-#ifdef MULTIPLE_HEAPS
-#ifdef SNOOP_STATS
-        if (do_mark_steal_p)
-        {
-            size_t objects_checked_count = 0;
-            size_t zero_ref_count = 0;
-            size_t objects_marked_count = 0;
-            size_t check_level_count = 0;
-            size_t busy_count = 0;
-            size_t interlocked_count = 0;
-            size_t partial_mark_parent_count = 0;
-            size_t stolen_or_pm_count = 0;
-            size_t stolen_entry_count = 0;
-            size_t pm_not_ready_count = 0;
-            size_t normal_count = 0;
-            size_t stack_bottom_clear_count = 0;
-            for (int i = 0; i < n_heaps; i++)
-            {
-                gc_heap* hp = g_heaps[i];
-                hp->print_snoop_stat();
-                objects_checked_count += hp->snoop_stat.objects_checked_count;
-                zero_ref_count += hp->snoop_stat.zero_ref_count;
-                objects_marked_count += hp->snoop_stat.objects_marked_count;
-                check_level_count += hp->snoop_stat.check_level_count;
-                busy_count += hp->snoop_stat.busy_count;
-                interlocked_count += hp->snoop_stat.interlocked_count;
-                partial_mark_parent_count += hp->snoop_stat.partial_mark_parent_count;
-                stolen_or_pm_count += hp->snoop_stat.stolen_or_pm_count;
-                stolen_entry_count += hp->snoop_stat.stolen_entry_count;
-                pm_not_ready_count += hp->snoop_stat.pm_not_ready_count;
-                normal_count += hp->snoop_stat.normal_count;
-                stack_bottom_clear_count += hp->snoop_stat.stack_bottom_clear_count;
-            }
-            fflush (stdout);
-            printf ("-------total stats-------\n");
-            printf ("%8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s | %8s\n",
-                "checked", "zero", "marked", "level", "busy", "xchg", "pmparent", "s_pm", "stolen", "nready", "normal", "clear");
-            printf ("%8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d | %8d\n",
-                objects_checked_count,
-                zero_ref_count,
-                objects_marked_count,
-                check_level_count,
-                busy_count,
-                interlocked_count,
-                partial_mark_parent_count,
-                stolen_or_pm_count,
-                stolen_entry_count,
-                pm_not_ready_count,
-                normal_count,
-                stack_bottom_clear_count);
-        }
-#endif //SNOOP_STATS
-        dprintf(3, ("Starting all threads for end of mark phase"));
-        gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-#if defined(MULTIPLE_HEAPS) && !defined(USE_REGIONS)
-    merge_mark_lists (total_mark_list_size);
-#endif //MULTIPLE_HEAPS && !USE_REGIONS
-    finalization_promoted_bytes = total_promoted_bytes - promoted_bytes_live;
-    mark_queue.verify_empty();
-    dprintf(2,("---- End of mark phase ----"));
-}
-inline
-void gc_heap::pin_object (uint8_t* o, uint8_t** ppObject)
-{
-    dprintf (3, ("Pinning %zx->%zx", (size_t)ppObject, (size_t)o));
-    set_pinned (o);
-#ifdef FEATURE_EVENT_TRACE
-    if(EVENT_ENABLED(PinObjectAtGCTime))
-    {
-        fire_etw_pin_object_event(o, ppObject);
-    }
-#endif // FEATURE_EVENT_TRACE
-    num_pinned_objects++;
-}
-size_t gc_heap::get_total_pinned_objects()
-{
-#ifdef MULTIPLE_HEAPS
-    size_t total_num_pinned_objects = 0;
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-        total_num_pinned_objects += hp->num_pinned_objects;
-    }
-    return total_num_pinned_objects;
-#else //MULTIPLE_HEAPS
-    return num_pinned_objects;
-#endif //MULTIPLE_HEAPS
-}
-void gc_heap::reinit_pinned_objects()
-{
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap::g_heaps[i]->num_pinned_objects = 0;
-    }
-#else //MULTIPLE_HEAPS
-    num_pinned_objects = 0;
-#endif //MULTIPLE_HEAPS
-}
-void gc_heap::reset_mark_stack ()
-{
-    reset_pinned_queue();
-    max_overflow_address = 0;
-    min_overflow_address = MAX_PTR;
-}
-#ifdef FEATURE_STRUCTALIGN
-#if defined (TARGET_AMD64)
-#define brick_bits (12)
-#else
-#define brick_bits (11)
-#endif //TARGET_AMD64
-C_ASSERT(brick_size == (1 << brick_bits));
-#define child_bits (brick_bits + 1 - LOG2_PTRSIZE)
-#define pad_bits (sizeof(short) * 8 - child_bits)
-#define child_from_short(w) (((signed short)(w) / (1 << (pad_bits - LOG2_PTRSIZE))) & ~((1 << LOG2_PTRSIZE) - 1))
-#define pad_mask ((1 << pad_bits) - 1)
-#define pad_from_short(w) ((size_t)(w) & pad_mask)
-#else // FEATURE_STRUCTALIGN
-#define child_from_short(w) (w)
-#endif // FEATURE_STRUCTALIGN
-inline
-short node_left_child(uint8_t* node)
-{
-    return child_from_short(((plug_and_pair*)node)[-1].m_pair.left);
-}
-inline
-void set_node_left_child(uint8_t* node, ptrdiff_t val)
-{
-    assert (val > -(ptrdiff_t)brick_size);
-    assert (val < (ptrdiff_t)brick_size);
-    assert (Aligned (val));
-#ifdef FEATURE_STRUCTALIGN
-    size_t pad = pad_from_short(((plug_and_pair*)node)[-1].m_pair.left);
-    ((plug_and_pair*)node)[-1].m_pair.left = ((short)val << (pad_bits - LOG2_PTRSIZE)) | (short)pad;
-#else // FEATURE_STRUCTALIGN
-    ((plug_and_pair*)node)[-1].m_pair.left = (short)val;
-#endif // FEATURE_STRUCTALIGN
-    assert (node_left_child (node) == val);
-}
-inline
-short node_right_child(uint8_t* node)
-{
-    return child_from_short(((plug_and_pair*)node)[-1].m_pair.right);
-}
-inline
-void set_node_right_child(uint8_t* node, ptrdiff_t val)
-{
-    assert (val > -(ptrdiff_t)brick_size);
-    assert (val < (ptrdiff_t)brick_size);
-    assert (Aligned (val));
-#ifdef FEATURE_STRUCTALIGN
-    size_t pad = pad_from_short(((plug_and_pair*)node)[-1].m_pair.right);
-    ((plug_and_pair*)node)[-1].m_pair.right = ((short)val << (pad_bits - LOG2_PTRSIZE)) | (short)pad;
-#else // FEATURE_STRUCTALIGN
-    ((plug_and_pair*)node)[-1].m_pair.right = (short)val;
-#endif // FEATURE_STRUCTALIGN
-    assert (node_right_child (node) == val);
-}
-#ifdef FEATURE_STRUCTALIGN
-void node_aligninfo (uint8_t* node, int& requiredAlignment, ptrdiff_t& pad)
-{
-    short left = ((plug_and_pair*)node)[-1].m_pair.left;
-    short right = ((plug_and_pair*)node)[-1].m_pair.right;
-    ptrdiff_t pad_shifted = (pad_from_short(left) << pad_bits) | pad_from_short(right);
-    ptrdiff_t aligninfo = pad_shifted * DATA_ALIGNMENT;
-    ptrdiff_t x = aligninfo;
-    x |= x >> 8;
-    x |= x >> 4;
-    x |= x >> 2;
-    x |= x >> 1;
-    requiredAlignment = (int)(x ^ (x >> 1));
-    pad = aligninfo - requiredAlignment;
-    pad += AdjustmentForMinPadSize(pad, requiredAlignment);
-}
-inline
-ptrdiff_t node_alignpad (uint8_t* node)
-{
-    int requiredAlignment;
-    ptrdiff_t alignpad;
-    node_aligninfo (node, requiredAlignment, alignpad);
-    return alignpad;
-}
-void clear_node_aligninfo (uint8_t* node)
-{
-    ((plug_and_pair*)node)[-1].m_pair.left &= ~0 << pad_bits;
-    ((plug_and_pair*)node)[-1].m_pair.right &= ~0 << pad_bits;
-}
-void set_node_aligninfo (uint8_t* node, int requiredAlignment, ptrdiff_t pad)
-{
-    ptrdiff_t aligninfo = (size_t)requiredAlignment + (pad & (requiredAlignment-1));
-    assert (Aligned (aligninfo));
-    ptrdiff_t aligninfo_shifted = aligninfo / DATA_ALIGNMENT;
-    assert (aligninfo_shifted < (1 << (pad_bits + pad_bits)));
-    ptrdiff_t hi = aligninfo_shifted >> pad_bits;
-    assert (pad_from_short(((plug_and_gap*)node)[-1].m_pair.left) == 0);
-    ((plug_and_pair*)node)[-1].m_pair.left |= hi;
-    ptrdiff_t lo = aligninfo_shifted & pad_mask;
-    assert (pad_from_short(((plug_and_gap*)node)[-1].m_pair.right) == 0);
-    ((plug_and_pair*)node)[-1].m_pair.right |= lo;
-#ifdef _DEBUG
-    int requiredAlignment2;
-    ptrdiff_t pad2;
-    node_aligninfo (node, requiredAlignment2, pad2);
-    assert (requiredAlignment == requiredAlignment2);
-    assert (pad == pad2);
-#endif // _DEBUG
-}
-#endif // FEATURE_STRUCTALIGN
-inline
-void loh_set_node_relocation_distance(uint8_t* node, ptrdiff_t val)
-{
-    ptrdiff_t* place = &(((loh_obj_and_pad*)node)[-1].reloc);
-    *place = val;
-}
-inline
-ptrdiff_t loh_node_relocation_distance(uint8_t* node)
-{
-    return (((loh_obj_and_pad*)node)[-1].reloc);
-}
-inline
-ptrdiff_t node_relocation_distance (uint8_t* node)
-{
-    return (((plug_and_reloc*)(node))[-1].reloc & ~3);
-}
-inline
-void set_node_relocation_distance(uint8_t* node, ptrdiff_t val)
-{
-    assert (val == (val & ~3));
-    ptrdiff_t* place = &(((plug_and_reloc*)node)[-1].reloc);
-    *place &= 1;
-    *place |= val;
-}
-#define node_left_p(node) (((plug_and_reloc*)(node))[-1].reloc & 2)
-#define set_node_left(node) ((plug_and_reloc*)(node))[-1].reloc |= 2;
-#ifndef FEATURE_STRUCTALIGN
-void set_node_realigned(uint8_t* node)
-{
-    ((plug_and_reloc*)(node))[-1].reloc |= 1;
-}
-void clear_node_realigned(uint8_t* node)
-{
-#ifdef RESPECT_LARGE_ALIGNMENT
-    ((plug_and_reloc*)(node))[-1].reloc &= ~1;
-#else
-    UNREFERENCED_PARAMETER(node);
-#endif //RESPECT_LARGE_ALIGNMENT
-}
-#endif // FEATURE_STRUCTALIGN
-inline
-size_t  node_gap_size (uint8_t* node)
-{
-    return ((plug_and_gap *)node)[-1].gap;
-}
-void set_gap_size (uint8_t* node, size_t size)
-{
-    assert (Aligned (size));
-    ((plug_and_gap *)node)[-1].reloc = 0;
-    ((plug_and_gap *)node)[-1].lr =0;
-    ((plug_and_gap *)node)[-1].gap = size;
-    assert ((size == 0 )||(size >= sizeof(plug_and_reloc)));
-}
-uint8_t* gc_heap::insert_node (uint8_t* new_node, size_t sequence_number,
-                   uint8_t* tree, uint8_t* last_node)
-{
-    dprintf (3, ("IN: %zx(%zx), T: %zx(%zx), L: %zx(%zx) [%zx]",
-                 (size_t)new_node, brick_of(new_node),
-                 (size_t)tree, brick_of(tree),
-                 (size_t)last_node, brick_of(last_node),
-                 sequence_number));
-    if (power_of_two_p (sequence_number))
-    {
-        set_node_left_child (new_node, (tree - new_node));
-        dprintf (3, ("NT: %zx, LC->%zx", (size_t)new_node, (tree - new_node)));
-        tree = new_node;
-    }
-    else
-    {
-        if (oddp (sequence_number))
-        {
-            set_node_right_child (last_node, (new_node - last_node));
-            dprintf (3, ("%p RC->%zx", last_node, (new_node - last_node)));
-        }
-        else
-        {
-            uint8_t*  earlier_node = tree;
-            size_t imax = logcount(sequence_number) - 2;
-            for (size_t i = 0; i != imax; i++)
-            {
-                earlier_node = earlier_node + node_right_child (earlier_node);
-            }
-            int tmp_offset = node_right_child (earlier_node);
-            assert (tmp_offset); // should never be empty
-            set_node_left_child (new_node, ((earlier_node + tmp_offset ) - new_node));
-            set_node_right_child (earlier_node, (new_node - earlier_node));
-            dprintf (3, ("%p LC->%zx, %p RC->%zx",
-                new_node, ((earlier_node + tmp_offset ) - new_node),
-                earlier_node, (new_node - earlier_node)));
-        }
-    }
-    return tree;
-}
-size_t gc_heap::update_brick_table (uint8_t* tree, size_t current_brick,
-                                    uint8_t* x, uint8_t* plug_end)
-{
-    dprintf (3, ("tree: %p, current b: %zx, x: %p, plug_end: %p",
-        tree, current_brick, x, plug_end));
-    if (tree != NULL)
-    {
-        dprintf (3, ("b- %zx->%zx pointing to tree %p",
-            current_brick, (size_t)(tree - brick_address (current_brick)), tree));
-        set_brick (current_brick, (tree - brick_address (current_brick)));
-    }
-    else
-    {
-        dprintf (3, ("b- %zx->-1", current_brick));
-        set_brick (current_brick, -1);
-    }
-    size_t  b = 1 + current_brick;
-    ptrdiff_t  offset = 0;
-    size_t last_br = brick_of (plug_end-1);
-    current_brick = brick_of (x-1);
-    dprintf (3, ("ubt: %zx->%zx]->%zx]", b, last_br, current_brick));
-    while (b <= current_brick)
-    {
-        if (b <= last_br)
-        {
-            set_brick (b, --offset);
-        }
-        else
-        {
-            set_brick (b,-1);
-        }
-        b++;
-    }
-    return brick_of (x);
-}
-#ifndef USE_REGIONS
-void gc_heap::plan_generation_start (generation* gen, generation* consing_gen, uint8_t* next_plug_to_allocate)
-{
-#ifdef HOST_64BIT
-    if (gen == youngest_generation)
-    {
-        heap_segment* seg = ephemeral_heap_segment;
-        size_t mark_stack_large_bos = mark_stack_bos;
-        size_t large_plug_pos = 0;
-        while (mark_stack_large_bos < mark_stack_tos)
-        {
-            if (mark_stack_array[mark_stack_large_bos].len > demotion_plug_len_th)
-            {
-                while (mark_stack_bos <= mark_stack_large_bos)
-                {
-                    size_t entry = deque_pinned_plug();
-                    size_t len = pinned_len (pinned_plug_of (entry));
-                    uint8_t* plug = pinned_plug (pinned_plug_of(entry));
-                    if (len > demotion_plug_len_th)
-                    {
-                        dprintf (2, ("ps(%d): S %p (%zd)(%p)", gen->gen_num, plug, len, (plug+len)));
-                    }
-                    pinned_len (pinned_plug_of (entry)) = plug - generation_allocation_pointer (consing_gen);
-                    assert(mark_stack_array[entry].len == 0 ||
-                            mark_stack_array[entry].len >= Align(min_obj_size));
-                    generation_allocation_pointer (consing_gen) = plug + len;
-                    generation_allocation_limit (consing_gen) = heap_segment_plan_allocated (seg);
-                    set_allocator_next_pin (consing_gen);
-                }
-            }
-            mark_stack_large_bos++;
-        }
-    }
-#endif // HOST_64BIT
-    generation_plan_allocation_start (gen) =
-        allocate_in_condemned_generations (consing_gen, Align (min_obj_size), -1);
-    generation_plan_allocation_start_size (gen) = Align (min_obj_size);
-    size_t allocation_left = (size_t)(generation_allocation_limit (consing_gen) - generation_allocation_pointer (consing_gen));
-    if (next_plug_to_allocate)
-    {
-        size_t dist_to_next_plug = (size_t)(next_plug_to_allocate - generation_allocation_pointer (consing_gen));
-        if (allocation_left > dist_to_next_plug)
-        {
-            allocation_left = dist_to_next_plug;
-        }
-    }
-    if (allocation_left < Align (min_obj_size))
-    {
-        generation_plan_allocation_start_size (gen) += allocation_left;
-        generation_allocation_pointer (consing_gen) += allocation_left;
-    }
-    dprintf (2, ("plan alloc gen%d(%p) start at %zx (ptr: %p, limit: %p, next: %p)", gen->gen_num,
-        generation_plan_allocation_start (gen),
-        generation_plan_allocation_start_size (gen),
-        generation_allocation_pointer (consing_gen), generation_allocation_limit (consing_gen),
-        next_plug_to_allocate));
-}
-void gc_heap::realloc_plan_generation_start (generation* gen, generation* consing_gen)
-{
-    BOOL adjacentp = FALSE;
-    generation_plan_allocation_start (gen) =
-        allocate_in_expanded_heap (consing_gen, Align(min_obj_size), adjacentp, 0,
-#ifdef SHORT_PLUGS
-                                   FALSE, NULL,
-#endif //SHORT_PLUGS
-                                   FALSE, -1 REQD_ALIGN_AND_OFFSET_ARG);
-    generation_plan_allocation_start_size (gen) = Align (min_obj_size);
-    size_t allocation_left = (size_t)(generation_allocation_limit (consing_gen) - generation_allocation_pointer (consing_gen));
-    if ((allocation_left < Align (min_obj_size)) &&
-         (generation_allocation_limit (consing_gen)!=heap_segment_plan_allocated (generation_allocation_segment (consing_gen))))
-    {
-        generation_plan_allocation_start_size (gen) += allocation_left;
-        generation_allocation_pointer (consing_gen) += allocation_left;
-    }
-    dprintf (1, ("plan re-alloc gen%d start at %p (ptr: %p, limit: %p)", gen->gen_num,
-        generation_plan_allocation_start (consing_gen),
-        generation_allocation_pointer (consing_gen),
-        generation_allocation_limit (consing_gen)));
-}
-void gc_heap::plan_generation_starts (generation*& consing_gen)
-{
-    int  gen_number = settings.condemned_generation;
-    while (gen_number >= 0)
-    {
-        if (gen_number < max_generation)
-        {
-            consing_gen = ensure_ephemeral_heap_segment (consing_gen);
-        }
-        generation* gen = generation_of (gen_number);
-        if (0 == generation_plan_allocation_start (gen))
-        {
-            plan_generation_start (gen, consing_gen, 0);
-            assert (generation_plan_allocation_start (gen));
-        }
-        gen_number--;
-    }
-    heap_segment_plan_allocated (ephemeral_heap_segment) =
-        generation_allocation_pointer (consing_gen);
-}
-void gc_heap::advance_pins_for_demotion (generation* gen)
-{
-    uint8_t* original_youngest_start = generation_allocation_start (youngest_generation);
-    heap_segment* seg = ephemeral_heap_segment;
-    if ((!(pinned_plug_que_empty_p())))
-    {
-        size_t gen1_pinned_promoted = generation_pinned_allocation_compact_size (generation_of (max_generation));
-        size_t gen1_pins_left = dd_pinned_survived_size (dynamic_data_of (max_generation - 1)) - gen1_pinned_promoted;
-        size_t total_space_to_skip = last_gen1_pin_end - generation_allocation_pointer (gen);
-        float pin_frag_ratio = (float)gen1_pins_left / (float)total_space_to_skip;
-        float pin_surv_ratio = (float)gen1_pins_left / (float)(dd_survived_size (dynamic_data_of (max_generation - 1)));
-        if ((pin_frag_ratio > 0.15) && (pin_surv_ratio > 0.30))
-        {
-            while (!pinned_plug_que_empty_p() &&
-                    (pinned_plug (oldest_pin()) < original_youngest_start))
-            {
-                size_t entry = deque_pinned_plug();
-                size_t len = pinned_len (pinned_plug_of (entry));
-                uint8_t* plug = pinned_plug (pinned_plug_of(entry));
-                pinned_len (pinned_plug_of (entry)) = plug - generation_allocation_pointer (gen);
-                assert(mark_stack_array[entry].len == 0 ||
-                        mark_stack_array[entry].len >= Align(min_obj_size));
-                generation_allocation_pointer (gen) = plug + len;
-                generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
-                set_allocator_next_pin (gen);
-                int frgn = object_gennum (plug);
-                if ((frgn != (int)max_generation) && settings.promotion)
-                {
-                    int togn = object_gennum_plan (plug);
-                    generation_pinned_allocation_sweep_size ((generation_of (frgn +1))) += len;
-                    if (frgn < togn)
-                    {
-                        generation_pinned_allocation_compact_size (generation_of (togn)) += len;
-                    }
-                }
-                dprintf (2, ("skipping gap %zu, pin %p (%zd)",
-                    pinned_len (pinned_plug_of (entry)), plug, len));
-            }
-        }
-        dprintf (2, ("ad_p_d: PL: %zd, SL: %zd, pfr: %d, psr: %d",
-            gen1_pins_left, total_space_to_skip, (int)(pin_frag_ratio*100), (int)(pin_surv_ratio*100)));
-    }
-}
-void gc_heap::process_ephemeral_boundaries (uint8_t* x,
-                                            int& active_new_gen_number,
-                                            int& active_old_gen_number,
-                                            generation*& consing_gen,
-                                            BOOL& allocate_in_condemned)
-{
-retry:
-    if ((active_old_gen_number > 0) &&
-        (x >= generation_allocation_start (generation_of (active_old_gen_number - 1))))
-    {
-        dprintf (2, ("crossing gen%d, x is %p", active_old_gen_number - 1, x));
-        if (!pinned_plug_que_empty_p())
-        {
-            dprintf (2, ("oldest pin: %p(%zd)",
-                pinned_plug (oldest_pin()),
-                (x - pinned_plug (oldest_pin()))));
-        }
-        if (active_old_gen_number <= (settings.promotion ? (max_generation - 1) : max_generation))
-        {
-            active_new_gen_number--;
-        }
-        active_old_gen_number--;
-        assert ((!settings.promotion) || (active_new_gen_number>0));
-        if (active_new_gen_number == (max_generation - 1))
-        {
-#ifdef FREE_USAGE_STATS
-            if (settings.condemned_generation == max_generation)
-            {
-                generation* gen_2 = generation_of (max_generation);
-                generation* gen_1 = generation_of (max_generation - 1);
-                size_t total_num_pinned_free_spaces_left = 0;
-                for (int j = 0; j < NUM_GEN_POWER2; j++)
-                {
-                    dprintf (1, ("[h%d][#%zd]2^%d: current: %zd, S: 2: %zd, 1: %zd(%zd)",
-                        heap_number,
-                        settings.gc_index,
-                        (j + 10),
-                        gen_2->gen_current_pinned_free_spaces[j],
-                        gen_2->gen_plugs[j], gen_1->gen_plugs[j],
-                        (gen_2->gen_plugs[j] + gen_1->gen_plugs[j])));
-                    total_num_pinned_free_spaces_left += gen_2->gen_current_pinned_free_spaces[j];
-                }
-                float pinned_free_list_efficiency = 0;
-                size_t total_pinned_free_space = generation_allocated_in_pinned_free (gen_2) + generation_pinned_free_obj_space (gen_2);
-                if (total_pinned_free_space != 0)
-                {
-                    pinned_free_list_efficiency = (float)(generation_allocated_in_pinned_free (gen_2)) / (float)total_pinned_free_space;
-                }
-                dprintf (1, ("[h%d] gen2 allocated %zd bytes with %zd bytes pinned free spaces (effi: %d%%), %zd (%zd) left",
-                            heap_number,
-                            generation_allocated_in_pinned_free (gen_2),
-                            total_pinned_free_space,
-                            (int)(pinned_free_list_efficiency * 100),
-                            generation_pinned_free_obj_space (gen_2),
-                            total_num_pinned_free_spaces_left));
-            }
-#endif //FREE_USAGE_STATS
-            while (!pinned_plug_que_empty_p() &&
-                   (!in_range_for_segment ((pinned_plug (oldest_pin())), ephemeral_heap_segment)))
-            {
-                size_t  entry = deque_pinned_plug();
-                mark*  m = pinned_plug_of (entry);
-                uint8_t*  plug = pinned_plug (m);
-                size_t  len = pinned_len (m);
-                heap_segment* nseg = heap_segment_in_range (generation_allocation_segment (consing_gen));
-                PREFIX_ASSUME(nseg != NULL);
-                while (!((plug >= generation_allocation_pointer (consing_gen))&&
-                        (plug < heap_segment_allocated (nseg))))
-                {
-                    assert (generation_allocation_pointer (consing_gen)>=
-                            heap_segment_mem (nseg));
-                    assert (generation_allocation_pointer (consing_gen)<=
-                            heap_segment_committed (nseg));
-                    heap_segment_plan_allocated (nseg) =
-                        generation_allocation_pointer (consing_gen);
-                    nseg = heap_segment_next_rw (nseg);
-                    generation_allocation_segment (consing_gen) = nseg;
-                    generation_allocation_pointer (consing_gen) =
-                        heap_segment_mem (nseg);
-                }
-                set_new_pin_info (m, generation_allocation_pointer (consing_gen));
-                assert(pinned_len(m) == 0 || pinned_len(m) >= Align(min_obj_size));
-                generation_allocation_pointer (consing_gen) = plug + len;
-                generation_allocation_limit (consing_gen) =
-                    generation_allocation_pointer (consing_gen);
-            }
-            allocate_in_condemned = TRUE;
-            consing_gen = ensure_ephemeral_heap_segment (consing_gen);
-        }
-        if (active_new_gen_number != max_generation)
-        {
-            if (active_new_gen_number == (max_generation - 1))
-            {
-                maxgen_pinned_compact_before_advance = generation_pinned_allocation_compact_size (generation_of (max_generation));
-                if (!demote_gen1_p)
-                    advance_pins_for_demotion (consing_gen);
-            }
-            plan_generation_start (generation_of (active_new_gen_number), consing_gen, x);
-            dprintf (2, ("process eph: allocated gen%d start at %p",
-                active_new_gen_number,
-                generation_plan_allocation_start (generation_of (active_new_gen_number))));
-            if ((demotion_low == MAX_PTR) && !pinned_plug_que_empty_p())
-            {
-                uint8_t* pplug = pinned_plug (oldest_pin());
-                if (object_gennum (pplug) > 0)
-                {
-                    demotion_low = pplug;
-                    dprintf (3, ("process eph: dlow->%p", demotion_low));
-                }
-            }
-            assert (generation_plan_allocation_start (generation_of (active_new_gen_number)));
-        }
-        goto retry;
-    }
-}
-#endif //!USE_REGIONS
-#ifdef FEATURE_BASICFREEZE
-inline
-void gc_heap::seg_set_mark_bits (heap_segment* seg)
-{
-    uint8_t* o = heap_segment_mem (seg);
-    while (o < heap_segment_allocated (seg))
-    {
-        set_marked (o);
-        o = o + Align (size(o));
-    }
-}
-inline
-void gc_heap::seg_clear_mark_bits (heap_segment* seg)
-{
-    uint8_t* o = heap_segment_mem (seg);
-    while (o < heap_segment_allocated (seg))
-    {
-        if (marked (o))
-        {
-            clear_marked (o);
-        }
-        o = o + Align (size (o));
-    }
-}
-void gc_heap::mark_ro_segments()
-{
-#ifdef USE_REGIONS
-    assert (!ro_segments_in_range);
-#else //USE_REGIONS
-    if ((settings.condemned_generation == max_generation) && ro_segments_in_range)
-    {
-        heap_segment* seg = generation_start_segment (generation_of (max_generation));
-        while (seg)
-        {
-            if (!heap_segment_read_only_p (seg))
-                break;
-            if (heap_segment_in_range_p (seg))
-            {
-#ifdef BACKGROUND_GC
-                if (settings.concurrent)
-                {
-                    seg_set_mark_array_bits_soh (seg);
-                }
-                else
-#endif //BACKGROUND_GC
-                {
-                    seg_set_mark_bits (seg);
-                }
-            }
-            seg = heap_segment_next (seg);
-        }
-    }
-#endif //USE_REGIONS
-}
-void gc_heap::sweep_ro_segments()
-{
-#ifdef USE_REGIONS
-    assert (!ro_segments_in_range);
-#else //USE_REGIONS
-    if ((settings.condemned_generation == max_generation) && ro_segments_in_range)
-    {
-        heap_segment* seg = generation_start_segment (generation_of (max_generation));;
-        while (seg)
-        {
-            if (!heap_segment_read_only_p (seg))
-                break;
-            if (heap_segment_in_range_p (seg))
-            {
-#ifdef BACKGROUND_GC
-                if (settings.concurrent)
-                {
-                    seg_clear_mark_array_bits_soh (seg);
-                }
-                else
-#endif //BACKGROUND_GC
-                {
-                    seg_clear_mark_bits (seg);
-                }
-            }
-            seg = heap_segment_next (seg);
-        }
-    }
-#endif //USE_REGIONS
-}
-#endif // FEATURE_BASICFREEZE
-#ifdef FEATURE_LOH_COMPACTION
-inline
-BOOL gc_heap::loh_pinned_plug_que_empty_p()
-{
-    return (loh_pinned_queue_bos == loh_pinned_queue_tos);
-}
-void gc_heap::loh_set_allocator_next_pin()
-{
-    if (!(loh_pinned_plug_que_empty_p()))
-    {
-        mark*  oldest_entry = loh_oldest_pin();
-        uint8_t* plug = pinned_plug (oldest_entry);
-        generation* gen = large_object_generation;
-        if ((plug >= generation_allocation_pointer (gen)) &&
-            (plug <  generation_allocation_limit (gen)))
-        {
-            generation_allocation_limit (gen) = pinned_plug (oldest_entry);
-        }
-        else
-            assert (!((plug < generation_allocation_pointer (gen)) &&
-                      (plug >= heap_segment_mem (generation_allocation_segment (gen)))));
-    }
-}
-size_t gc_heap::loh_deque_pinned_plug ()
-{
-    size_t m = loh_pinned_queue_bos;
-    loh_pinned_queue_bos++;
-    return m;
-}
-inline
-mark* gc_heap::loh_pinned_plug_of (size_t bos)
-{
-    return &loh_pinned_queue[bos];
-}
-inline
-mark* gc_heap::loh_oldest_pin()
-{
-    return loh_pinned_plug_of (loh_pinned_queue_bos);
-}
-BOOL gc_heap::loh_enque_pinned_plug (uint8_t* plug, size_t len)
-{
-    assert(len >= Align(min_obj_size, get_alignment_constant (FALSE)));
-    if (loh_pinned_queue_length <= loh_pinned_queue_tos)
-    {
-        if (!grow_mark_stack (loh_pinned_queue, loh_pinned_queue_length, LOH_PIN_QUEUE_LENGTH))
-        {
-            return FALSE;
-        }
-    }
-    dprintf (3, (" P: %p(%zd)", plug, len));
-    mark& m = loh_pinned_queue[loh_pinned_queue_tos];
-    m.first = plug;
-    m.len = len;
-    loh_pinned_queue_tos++;
-    loh_set_allocator_next_pin();
-    return TRUE;
-}
-inline
-BOOL gc_heap::loh_size_fit_p (size_t size, uint8_t* alloc_pointer, uint8_t* alloc_limit, bool end_p)
-{
-    dprintf (1235, ("trying to fit %zd(%zd) between %p and %p (%zd)",
-        size,
-        (2* AlignQword (loh_padding_obj_size) +  size),
-        alloc_pointer,
-        alloc_limit,
-        (alloc_limit - alloc_pointer)));
-    size_t pad = 1 + (end_p ? 0 : 1);
-    pad *= AlignQword (loh_padding_obj_size);
-    return ((alloc_pointer + pad + size) <= alloc_limit);
-}
-uint8_t* gc_heap::loh_allocate_in_condemned (size_t size)
-{
-    generation* gen = large_object_generation;
-    dprintf (1235, ("E: p:%p, l:%p, s: %zd",
-        generation_allocation_pointer (gen),
-        generation_allocation_limit (gen),
-        size));
-retry:
-    {
-        heap_segment* seg = generation_allocation_segment (gen);
-        if (!(loh_size_fit_p (size, generation_allocation_pointer (gen), generation_allocation_limit (gen),
-                              (generation_allocation_limit (gen) == heap_segment_plan_allocated (seg)))))
-        {
-            if ((!(loh_pinned_plug_que_empty_p()) &&
-                 (generation_allocation_limit (gen) ==
-                  pinned_plug (loh_oldest_pin()))))
-            {
-                mark* m = loh_pinned_plug_of (loh_deque_pinned_plug());
-                size_t len = pinned_len (m);
-                uint8_t* plug = pinned_plug (m);
-                dprintf (1235, ("AIC: %p->%p(%zd)", generation_allocation_pointer (gen), plug, plug - generation_allocation_pointer (gen)));
-                pinned_len (m) = plug - generation_allocation_pointer (gen);
-                generation_allocation_pointer (gen) = plug + len;
-                generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
-                loh_set_allocator_next_pin();
-                dprintf (1235, ("s: p: %p, l: %p (%zd)",
-                    generation_allocation_pointer (gen),
-                    generation_allocation_limit (gen),
-                    (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
-                goto retry;
-            }
-            if (generation_allocation_limit (gen) != heap_segment_plan_allocated (seg))
-            {
-                generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
-                dprintf (1235, ("l->pa(%p)", generation_allocation_limit (gen)));
-            }
-            else
-            {
-                if (heap_segment_plan_allocated (seg) != heap_segment_committed (seg))
-                {
-                    heap_segment_plan_allocated (seg) = heap_segment_committed (seg);
-                    generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
-                    dprintf (1235, ("l->c(%p)", generation_allocation_limit (gen)));
-                }
-                else
-                {
-                    if (loh_size_fit_p (size, generation_allocation_pointer (gen), heap_segment_reserved (seg), true) &&
-                        (grow_heap_segment (seg, (generation_allocation_pointer (gen) + size + 2* AlignQword (loh_padding_obj_size)))))
-                    {
-                        dprintf (1235, ("growing seg from %p to %p\n", heap_segment_committed (seg),
-                                         (generation_allocation_pointer (gen) + size)));
-                        heap_segment_plan_allocated (seg) = heap_segment_committed (seg);
-                        generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
-                        dprintf (1235, ("g: p: %p, l: %p (%zd)",
-                            generation_allocation_pointer (gen),
-                            generation_allocation_limit (gen),
-                            (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
-                    }
-                    else
-                    {
-                        heap_segment* next_seg = heap_segment_next (seg);
-                        assert (generation_allocation_pointer (gen)>=
-                                heap_segment_mem (seg));
-                        if (!loh_pinned_plug_que_empty_p() &&
-                            ((pinned_plug (loh_oldest_pin()) <
-                              heap_segment_allocated (seg)) &&
-                             (pinned_plug (loh_oldest_pin()) >=
-                              generation_allocation_pointer (gen))))
-                        {
-                            LOG((LF_GC, LL_INFO10, "remaining pinned plug %zx while leaving segment on allocation",
-                                         pinned_plug (loh_oldest_pin())));
-                            dprintf (1, ("queue empty: %d", loh_pinned_plug_que_empty_p()));
-                            FATAL_GC_ERROR();
-                        }
-                        assert (generation_allocation_pointer (gen)>=
-                                heap_segment_mem (seg));
-                        assert (generation_allocation_pointer (gen)<=
-                                heap_segment_committed (seg));
-                        heap_segment_plan_allocated (seg) = generation_allocation_pointer (gen);
-                        if (next_seg)
-                        {
-                            generation_allocation_segment (gen) = next_seg;
-                            generation_allocation_pointer (gen) = heap_segment_mem (next_seg);
-                            generation_allocation_limit (gen) = generation_allocation_pointer (gen);
-                            dprintf (1235, ("n: p: %p, l: %p (%zd)",
-                                generation_allocation_pointer (gen),
-                                generation_allocation_limit (gen),
-                                (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
-                        }
-                        else
-                        {
-                            dprintf (1, ("We ran out of space compacting, shouldn't happen"));
-                            FATAL_GC_ERROR();
-                        }
-                    }
-                }
-            }
-            loh_set_allocator_next_pin();
-            dprintf (1235, ("r: p: %p, l: %p (%zd)",
-                generation_allocation_pointer (gen),
-                generation_allocation_limit (gen),
-                (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
-            goto retry;
-        }
-    }
-    {
-        assert (generation_allocation_pointer (gen)>=
-                heap_segment_mem (generation_allocation_segment (gen)));
-        uint8_t* result = generation_allocation_pointer (gen);
-        size_t loh_pad = AlignQword (loh_padding_obj_size);
-        generation_allocation_pointer (gen) += size + loh_pad;
-        assert (generation_allocation_pointer (gen) <= generation_allocation_limit (gen));
-        dprintf (1235, ("p: %p, l: %p (%zd)",
-            generation_allocation_pointer (gen),
-            generation_allocation_limit (gen),
-            (generation_allocation_limit (gen) - generation_allocation_pointer (gen))));
-        assert (result + loh_pad);
-        return result + loh_pad;
-    }
-}
-BOOL gc_heap::loh_compaction_requested()
-{
-    return (loh_compaction_always_p || (loh_compaction_mode != loh_compaction_default));
-}
-inline
-void gc_heap::check_loh_compact_mode (BOOL all_heaps_compacted_p)
-{
-    if (settings.loh_compaction && (loh_compaction_mode == loh_compaction_once))
-    {
-        if (all_heaps_compacted_p)
-        {
-            loh_compaction_mode = loh_compaction_default;
-        }
-    }
-}
-BOOL gc_heap::plan_loh()
-{
-#ifdef FEATURE_EVENT_TRACE
-    uint64_t start_time = 0, end_time;
-    if (informational_event_enabled_p)
-    {
-        memset (loh_compact_info, 0, (sizeof (etw_loh_compact_info) * get_num_heaps()));
-        start_time = GetHighPrecisionTimeStamp();
-    }
-#endif //FEATURE_EVENT_TRACE
-    if (!loh_pinned_queue)
-    {
-        loh_pinned_queue = new (nothrow) (mark [LOH_PIN_QUEUE_LENGTH]);
-        if (!loh_pinned_queue)
-        {
-            dprintf (1, ("Cannot allocate the LOH pinned queue (%zd bytes), no compaction",
-                         LOH_PIN_QUEUE_LENGTH * sizeof (mark)));
-            return FALSE;
-        }
-        loh_pinned_queue_length = LOH_PIN_QUEUE_LENGTH;
-    }
-    loh_pinned_queue_decay = LOH_PIN_DECAY;
-    loh_pinned_queue_tos = 0;
-    loh_pinned_queue_bos = 0;
-    generation* gen        = large_object_generation;
-    heap_segment* start_seg = heap_segment_rw (generation_start_segment (gen));
-    PREFIX_ASSUME(start_seg != NULL);
-    heap_segment* seg      = start_seg;
-    uint8_t* o             = get_uoh_start_object (seg, gen);
-    dprintf (1235, ("before GC LOH size: %zd, free list: %zd, free obj: %zd\n",
-        generation_size (loh_generation),
-        generation_free_list_space (gen),
-        generation_free_obj_space (gen)));
-    while (seg)
-    {
-        heap_segment_plan_allocated (seg) = heap_segment_mem (seg);
-        seg = heap_segment_next (seg);
-    }
-    seg = start_seg;
-    heap_segment_plan_allocated (seg) = o;
-    generation_allocation_pointer (gen) = o;
-    generation_allocation_limit (gen) = generation_allocation_pointer (gen);
-    generation_allocation_segment (gen) = start_seg;
-    uint8_t* free_space_start = o;
-    uint8_t* free_space_end = o;
-    uint8_t* new_address = 0;
-    while (1)
-    {
-        if (o >= heap_segment_allocated (seg))
-        {
-            seg = heap_segment_next (seg);
-            if (seg == 0)
-            {
-                break;
-            }
-            o = heap_segment_mem (seg);
-        }
-        if (marked (o))
-        {
-            free_space_end = o;
-            size_t size = AlignQword (size (o));
-            dprintf (1235, ("%p(%zd) M", o, size));
-            if (pinned (o))
-            {
-                if (!loh_enque_pinned_plug (o, size))
-                {
-                    return FALSE;
-                }
-                new_address = o;
-            }
-            else
-            {
-                new_address = loh_allocate_in_condemned (size);
-            }
-            loh_set_node_relocation_distance (o, (new_address - o));
-            dprintf (1235, ("lobj %p-%p -> %p-%p (%zd)", o, (o + size), new_address, (new_address + size), (new_address - o)));
-            o = o + size;
-            free_space_start = o;
-            if (o < heap_segment_allocated (seg))
-            {
-                assert (!marked (o));
-            }
-        }
-        else
-        {
-            while (o < heap_segment_allocated (seg) && !marked (o))
-            {
-                dprintf (1235, ("%p(%zd) F (%d)", o, AlignQword (size (o)), ((method_table (o) == g_gc_pFreeObjectMethodTable) ? 1 : 0)));
-                o = o + AlignQword (size (o));
-            }
-        }
-    }
-    while (!loh_pinned_plug_que_empty_p())
-    {
-        mark* m = loh_pinned_plug_of (loh_deque_pinned_plug());
-        size_t len = pinned_len (m);
-        uint8_t* plug = pinned_plug (m);
-        heap_segment* nseg = heap_segment_rw (generation_allocation_segment (gen));
-        while ((plug < generation_allocation_pointer (gen)) ||
-               (plug >= heap_segment_allocated (nseg)))
-        {
-            assert ((plug < heap_segment_mem (nseg)) ||
-                    (plug > heap_segment_reserved (nseg)));
-            assert (generation_allocation_pointer (gen)>=
-                    heap_segment_mem (nseg));
-            assert (generation_allocation_pointer (gen)<=
-                    heap_segment_committed (nseg));
-            heap_segment_plan_allocated (nseg) =
-                generation_allocation_pointer (gen);
-            nseg = heap_segment_next_rw (nseg);
-            generation_allocation_segment (gen) = nseg;
-            generation_allocation_pointer (gen) =
-                heap_segment_mem (nseg);
-        }
-        dprintf (1235, ("SP: %p->%p(%zd)", generation_allocation_pointer (gen), plug, plug - generation_allocation_pointer (gen)));
-        pinned_len (m) = plug - generation_allocation_pointer (gen);
-        generation_allocation_pointer (gen) = plug + len;
-    }
-    heap_segment_plan_allocated (generation_allocation_segment (gen)) = generation_allocation_pointer (gen);
-    generation_allocation_pointer (gen) = 0;
-    generation_allocation_limit (gen) = 0;
-#ifdef FEATURE_EVENT_TRACE
-    if (informational_event_enabled_p)
-    {
-        end_time = GetHighPrecisionTimeStamp();
-        loh_compact_info[heap_number].time_plan = limit_time_to_uint32 (end_time - start_time);
-    }
-#endif //FEATURE_EVENT_TRACE
-    return TRUE;
-}
-void gc_heap::compact_loh()
-{
-    assert (loh_compaction_requested() || heap_hard_limit || conserve_mem_setting);
-#ifdef FEATURE_EVENT_TRACE
-    uint64_t start_time = 0, end_time;
-    if (informational_event_enabled_p)
-    {
-        start_time = GetHighPrecisionTimeStamp();
-    }
-#endif //FEATURE_EVENT_TRACE
-    generation* gen        = large_object_generation;
-    heap_segment* start_seg = heap_segment_rw (generation_start_segment (gen));
-    PREFIX_ASSUME(start_seg != NULL);
-    heap_segment* seg      = start_seg;
-    heap_segment* prev_seg = 0;
-    uint8_t* o             = get_uoh_start_object (seg, gen);
-    uint8_t* free_space_start = o;
-    uint8_t* free_space_end = o;
-    generation_allocator (gen)->clear();
-    generation_free_list_space (gen) = 0;
-    generation_free_obj_space (gen) = 0;
-    loh_pinned_queue_bos = 0;
-    while (1)
-    {
-        if (o >= heap_segment_allocated (seg))
-        {
-            heap_segment* next_seg = heap_segment_next (seg);
-            if ((heap_segment_plan_allocated (seg) == heap_segment_mem (seg)) &&
-                (seg != start_seg) && !heap_segment_read_only_p (seg))
-            {
-                dprintf (3, ("Preparing empty large segment %zx", (size_t)seg));
-                assert (prev_seg);
-                heap_segment_next (prev_seg) = next_seg;
-                heap_segment_next (seg) = freeable_uoh_segment;
-                freeable_uoh_segment = seg;
-#ifdef USE_REGIONS
-                update_start_tail_regions (gen, seg, prev_seg, next_seg);
-#endif //USE_REGIONS
-            }
-            else
-            {
-                if (!heap_segment_read_only_p (seg))
-                {
-                    if (heap_segment_plan_allocated (seg) > heap_segment_allocated (seg))
-                    {
-                        if ((heap_segment_plan_allocated (seg) - plug_skew)  > heap_segment_used (seg))
-                        {
-                            heap_segment_used (seg) = heap_segment_plan_allocated (seg) - plug_skew;
-                        }
-                    }
-                    heap_segment_allocated (seg) = heap_segment_plan_allocated (seg);
-                    dprintf (3, ("Trimming seg to %p[", heap_segment_allocated (seg)));
-                    decommit_heap_segment_pages (seg, 0);
-                    dprintf (1236, ("CLOH: seg: %p, alloc: %p, used: %p, committed: %p",
-                        seg,
-                        heap_segment_allocated (seg),
-                        heap_segment_used (seg),
-                        heap_segment_committed (seg)));
-                    dprintf (1236, ("CLOH: used is set to %p", heap_segment_used (seg)));
-                }
-                prev_seg = seg;
-            }
-            seg = next_seg;
-            if (seg == 0)
-                break;
-            else
-            {
-                o = heap_segment_mem (seg);
-            }
-        }
-        if (marked (o))
-        {
-            free_space_end = o;
-            size_t size = AlignQword (size (o));
-            size_t loh_pad;
-            uint8_t* reloc = o;
-            clear_marked (o);
-            if (pinned (o))
-            {
-                mark* m = loh_pinned_plug_of (loh_deque_pinned_plug());
-                uint8_t* plug = pinned_plug (m);
-                assert (plug == o);
-                loh_pad = pinned_len (m);
-                clear_pinned (o);
-            }
-            else
-            {
-                loh_pad = AlignQword (loh_padding_obj_size);
-                reloc += loh_node_relocation_distance (o);
-                gcmemcopy (reloc, o, size, TRUE);
-            }
-            thread_gap ((reloc - loh_pad), loh_pad, gen);
-            o = o + size;
-            free_space_start = o;
-            if (o < heap_segment_allocated (seg))
-            {
-                assert (!marked (o));
-            }
-        }
-        else
-        {
-            while (o < heap_segment_allocated (seg) && !marked (o))
-            {
-                o = o + AlignQword (size (o));
-            }
-        }
-    }
-#ifdef FEATURE_EVENT_TRACE
-    if (informational_event_enabled_p)
-    {
-        end_time = GetHighPrecisionTimeStamp();
-        loh_compact_info[heap_number].time_compact = limit_time_to_uint32 (end_time - start_time);
-    }
-#endif //FEATURE_EVENT_TRACE
-    assert (loh_pinned_plug_que_empty_p());
-    dprintf (1235, ("after GC LOH size: %zd, free list: %zd, free obj: %zd\n\n",
-        generation_size (loh_generation),
-        generation_free_list_space (gen),
-        generation_free_obj_space (gen)));
-}
-#ifdef FEATURE_EVENT_TRACE
-inline
-void gc_heap::loh_reloc_survivor_helper (uint8_t** pval, size_t& total_refs, size_t& zero_refs)
-{
-    uint8_t* val = *pval;
-    if (!val)
-        zero_refs++;
-    total_refs++;
-    reloc_survivor_helper (pval);
-}
-#endif //FEATURE_EVENT_TRACE
-void gc_heap::relocate_in_loh_compact()
-{
-    generation* gen        = large_object_generation;
-    heap_segment* seg      = heap_segment_rw (generation_start_segment (gen));
-    uint8_t* o              = get_uoh_start_object (seg, gen);
-#ifdef FEATURE_EVENT_TRACE
-    size_t total_refs = 0;
-    size_t zero_refs = 0;
-    uint64_t start_time = 0, end_time;
-    if (informational_event_enabled_p)
-    {
-        start_time = GetHighPrecisionTimeStamp();
-    }
-#endif //FEATURE_EVENT_TRACE
-    while (1)
-    {
-        if (o >= heap_segment_allocated (seg))
-        {
-            seg = heap_segment_next (seg);
-            if (seg == 0)
-            {
-                break;
-            }
-            o = heap_segment_mem (seg);
-        }
-        if (marked (o))
-        {
-            size_t size = AlignQword (size (o));
-            check_class_object_demotion (o);
-            if (contain_pointers (o))
-            {
-#ifdef FEATURE_EVENT_TRACE
-                if (informational_event_enabled_p)
-                {
-                    go_through_object_nostart (method_table (o), o, size(o), pval,
-                    {
-                        loh_reloc_survivor_helper (pval, total_refs, zero_refs);
-                    });
-                }
-                else
-#endif //FEATURE_EVENT_TRACE
-                {
-                    go_through_object_nostart (method_table (o), o, size(o), pval,
-                    {
-                        reloc_survivor_helper (pval);
-                    });
-                }
-            }
-            o = o + size;
-            if (o < heap_segment_allocated (seg))
-            {
-                assert (!marked (o));
-            }
-        }
-        else
-        {
-            while (o < heap_segment_allocated (seg) && !marked (o))
-            {
-                o = o + AlignQword (size (o));
-            }
-        }
-    }
-#ifdef FEATURE_EVENT_TRACE
-    if (informational_event_enabled_p)
-    {
-        end_time = GetHighPrecisionTimeStamp();
-        loh_compact_info[heap_number].time_relocate = limit_time_to_uint32 (end_time - start_time);
-        loh_compact_info[heap_number].total_refs = total_refs;
-        loh_compact_info[heap_number].zero_refs = zero_refs;
-    }
-#endif //FEATURE_EVENT_TRACE
-    dprintf (1235, ("after GC LOH size: %zd, free list: %zd, free obj: %zd\n\n",
-        generation_size (loh_generation),
-        generation_free_list_space (gen),
-        generation_free_obj_space (gen)));
-}
-void gc_heap::walk_relocation_for_loh (void* profiling_context, record_surv_fn fn)
-{
-    generation* gen        = large_object_generation;
-    heap_segment* seg      = heap_segment_rw (generation_start_segment (gen));
-    uint8_t* o             = get_uoh_start_object (seg, gen);
-    while (1)
-    {
-        if (o >= heap_segment_allocated (seg))
-        {
-            seg = heap_segment_next (seg);
-            if (seg == 0)
-            {
-                break;
-            }
-            o = heap_segment_mem (seg);
-        }
-        if (marked (o))
-        {
-            size_t size = AlignQword (size (o));
-            ptrdiff_t reloc = loh_node_relocation_distance (o);
-            STRESS_LOG_PLUG_MOVE(o, (o + size), -reloc);
-            fn (o, (o + size), reloc, profiling_context, !!settings.compaction, false);
-            o = o + size;
-            if (o < heap_segment_allocated (seg))
-            {
-                assert (!marked (o));
-            }
-        }
-        else
-        {
-            while (o < heap_segment_allocated (seg) && !marked (o))
-            {
-                o = o + AlignQword (size (o));
-            }
-        }
-    }
-}
-BOOL gc_heap::loh_object_p (uint8_t* o)
-{
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hp = gc_heap::g_heaps [0];
-    int brick_entry = hp->brick_table[hp->brick_of (o)];
-#else //MULTIPLE_HEAPS
-    int brick_entry = brick_table[brick_of (o)];
-#endif //MULTIPLE_HEAPS
-    return (brick_entry == 0);
-}
-#endif //FEATURE_LOH_COMPACTION
-void gc_heap::convert_to_pinned_plug (BOOL& last_npinned_plug_p,
-                                      BOOL& last_pinned_plug_p,
-                                      BOOL& pinned_plug_p,
-                                      size_t ps,
-                                      size_t& artificial_pinned_size)
-{
-    last_npinned_plug_p = FALSE;
-    last_pinned_plug_p = TRUE;
-    pinned_plug_p = TRUE;
-    artificial_pinned_size = ps;
-}
-void gc_heap::store_plug_gap_info (uint8_t* plug_start,
-                                   uint8_t* plug_end,
-                                   BOOL& last_npinned_plug_p,
-                                   BOOL& last_pinned_plug_p,
-                                   uint8_t*& last_pinned_plug,
-                                   BOOL& pinned_plug_p,
-                                   uint8_t* last_object_in_last_plug,
-                                   BOOL& merge_with_last_pin_p,
-                                   size_t last_plug_len)
-{
-    UNREFERENCED_PARAMETER(last_plug_len);
-    if (!last_npinned_plug_p && !last_pinned_plug_p)
-    {
-        dprintf (3, ("Free: %zx", (plug_start - plug_end)));
-        assert ((plug_start == plug_end) || ((size_t)(plug_start - plug_end) >= Align (min_obj_size)));
-        set_gap_size (plug_start, plug_start - plug_end);
-    }
-    if (pinned (plug_start))
-    {
-        BOOL save_pre_plug_info_p = FALSE;
-        if (last_npinned_plug_p || last_pinned_plug_p)
-        {
-            save_pre_plug_info_p = TRUE;
-        }
-        pinned_plug_p = TRUE;
-        last_npinned_plug_p = FALSE;
-        if (last_pinned_plug_p)
-        {
-            dprintf (3, ("last plug %p was also pinned, should merge", last_pinned_plug));
-            merge_with_last_pin_p = TRUE;
-        }
-        else
-        {
-            last_pinned_plug_p = TRUE;
-            last_pinned_plug = plug_start;
-            enque_pinned_plug (last_pinned_plug, save_pre_plug_info_p, last_object_in_last_plug);
-            if (save_pre_plug_info_p)
-            {
-#ifdef DOUBLY_LINKED_FL
-                if (last_object_in_last_plug == generation_last_free_list_allocated(generation_of(max_generation)))
-                {
-                    saved_pinned_plug_index = mark_stack_tos;
-                }
-#endif //DOUBLY_LINKED_FL
-                set_gap_size (plug_start, sizeof (gap_reloc_pair));
-            }
-        }
-    }
-    else
-    {
-        if (last_pinned_plug_p)
-        {
-            save_post_plug_info (last_pinned_plug, last_object_in_last_plug, plug_start);
-            set_gap_size (plug_start, sizeof (gap_reloc_pair));
-            verify_pins_with_post_plug_info("after saving post plug info");
-        }
-        last_npinned_plug_p = TRUE;
-        last_pinned_plug_p = FALSE;
-    }
-}
-void gc_heap::record_interesting_data_point (interesting_data_point idp)
-{
-#ifdef GC_CONFIG_DRIVEN
-    (interesting_data_per_gc[idp])++;
-#else
-    UNREFERENCED_PARAMETER(idp);
-#endif //GC_CONFIG_DRIVEN
-}
-#ifdef USE_REGIONS
-void gc_heap::skip_pins_in_alloc_region (generation* consing_gen, int plan_gen_num)
-{
-    heap_segment* alloc_region = generation_allocation_segment (consing_gen);
-    while (!pinned_plug_que_empty_p())
-    {
-        uint8_t* oldest_plug = pinned_plug (oldest_pin());
-        if ((oldest_plug >= generation_allocation_pointer (consing_gen)) &&
-            (oldest_plug < heap_segment_allocated (alloc_region)))
-        {
-            mark* m =       pinned_plug_of (deque_pinned_plug());
-            uint8_t* plug = pinned_plug (m);
-            size_t len =    pinned_len (m);
-            set_new_pin_info (m, generation_allocation_pointer (consing_gen));
-            dprintf (REGIONS_LOG, ("pin %p b: %zx->%zx", plug, brick_of (plug),
-                (size_t)(brick_table[brick_of (plug)])));
-            generation_allocation_pointer (consing_gen) = plug + len;
-        }
-        else
-        {
-            break;
-        }
-    }
-    dprintf (REGIONS_LOG, ("finished with alloc region %p, (%s) plan gen -> %d",
-        heap_segment_mem (alloc_region),
-        (heap_segment_swept_in_plan (alloc_region) ? "SIP" : "non SIP"),
-        (heap_segment_swept_in_plan (alloc_region) ?
-            heap_segment_plan_gen_num (alloc_region) : plan_gen_num)));
-    set_region_plan_gen_num_sip (alloc_region, plan_gen_num);
-    heap_segment_plan_allocated (alloc_region) = generation_allocation_pointer (consing_gen);
-}
-void gc_heap::decide_on_demotion_pin_surv (heap_segment* region, int* no_pinned_surv_region_count)
-{
-    int new_gen_num = 0;
-    int pinned_surv = heap_segment_pinned_survived (region);
-    if (pinned_surv == 0)
-    {
-        (*no_pinned_surv_region_count)++;
-        dprintf (REGIONS_LOG, ("region %Ix will be empty", heap_segment_mem (region)));
-    }
-    size_t basic_region_size = (size_t)1 << min_segment_size_shr;
-    int pinned_ratio = (int)(((double)pinned_surv * 100.0) / (double)basic_region_size);
-    dprintf (REGIONS_LOG, ("h%d g%d region %Ix(%Ix) ps: %d (%d) (%s)", heap_number,
-        heap_segment_gen_num (region), (size_t)region, heap_segment_mem (region), pinned_surv, pinned_ratio,
-        ((pinned_ratio >= demotion_pinned_ratio_th) ? "ND" : "D")));
-    if (pinned_ratio >= demotion_pinned_ratio_th)
-    {
-        if (settings.promotion)
-        {
-            new_gen_num = get_plan_gen_num (heap_segment_gen_num (region));
-        }
-    }
-    set_region_plan_gen_num (region, new_gen_num);
-}
-void gc_heap::process_last_np_surv_region (generation* consing_gen,
-                                           int current_plan_gen_num,
-                                           int next_plan_gen_num)
-{
-    heap_segment* alloc_region = generation_allocation_segment (consing_gen);
-    uint8_t* consing_gen_alloc_ptr = generation_allocation_pointer (consing_gen);
-    assert ((consing_gen_alloc_ptr >= heap_segment_mem (alloc_region)) &&
-            (consing_gen_alloc_ptr <= heap_segment_reserved (alloc_region)));
-    dprintf (REGIONS_LOG, ("h%d PLN: (%s) plan gen%d->%d, consing alloc region: %p, ptr: %p (%Id) (consing gen: %d)",
-        heap_number, (settings.promotion ? "promotion" : "no promotion"), current_plan_gen_num, next_plan_gen_num,
-        heap_segment_mem (alloc_region),
-        generation_allocation_pointer (consing_gen),
-        (generation_allocation_pointer (consing_gen) - heap_segment_mem (alloc_region)),
-        consing_gen->gen_num));
-    if (current_plan_gen_num != next_plan_gen_num)
-    {
-        if (generation_allocation_pointer (consing_gen) == heap_segment_mem (alloc_region))
-        {
-            dprintf (REGIONS_LOG, ("h%d alloc region %p unused, using it to plan %d",
-                heap_number, heap_segment_mem (alloc_region), next_plan_gen_num));
-            return;
-        }
-        skip_pins_in_alloc_region (consing_gen, current_plan_gen_num);
-        heap_segment* next_region = heap_segment_next_non_sip (alloc_region);
-        if (!next_region)
-        {
-            int gen_num = heap_segment_gen_num (alloc_region);
-            if (gen_num > 0)
-            {
-                next_region = generation_start_segment (generation_of (gen_num - 1));
-                dprintf (REGIONS_LOG, ("h%d consing switching to next gen%d seg %p",
-                    heap_number, heap_segment_gen_num (next_region), heap_segment_mem (next_region)));
-            }
-            else
-            {
-                if (settings.promotion)
-                {
-                    assert (next_plan_gen_num == 0);
-                    next_region = get_new_region (0);
-                    if (next_region)
-                    {
-                        dprintf (REGIONS_LOG, ("h%d getting a new region for gen0 plan start seg to %p",
-                            heap_number, heap_segment_mem (next_region)));
-                        regions_per_gen[0]++;
-                        new_gen0_regions_in_plns++;
-                    }
-                    else
-                    {
-                        dprintf (REGIONS_LOG, ("h%d couldn't get a region to plan gen0, special sweep on",
-                            heap_number));
-                        special_sweep_p = true;
-                    }
-                }
-                else
-                {
-                    assert (!"ran out of regions for non promotion case??");
-                }
-            }
-        }
-        else
-        {
-            dprintf (REGIONS_LOG, ("h%d consing switching to next seg %p in gen%d to alloc in",
-                heap_number, heap_segment_mem (next_region), heap_segment_gen_num (next_region)));
-        }
-        if (next_region)
-        {
-            init_alloc_info (consing_gen, next_region);
-            dprintf (REGIONS_LOG, ("h%d consing(%d) alloc seg: %p(%p, %p), ptr: %p, planning gen%d",
-                heap_number, consing_gen->gen_num,
-                heap_segment_mem (generation_allocation_segment (consing_gen)),
-                heap_segment_allocated (generation_allocation_segment (consing_gen)),
-                heap_segment_plan_allocated (generation_allocation_segment (consing_gen)),
-                generation_allocation_pointer (consing_gen), next_plan_gen_num));
-        }
-        else
-        {
-            assert (special_sweep_p);
-        }
-    }
-}
-void gc_heap::process_remaining_regions (int current_plan_gen_num, generation* consing_gen)
-{
-    assert ((current_plan_gen_num == 0) || (!settings.promotion && (current_plan_gen_num == -1)));
-    if (special_sweep_p)
-    {
-        assert (pinned_plug_que_empty_p());
-    }
-    dprintf (REGIONS_LOG, ("h%d PRR: (%s) plan %d: consing alloc seg: %p, ptr: %p",
-        heap_number, (settings.promotion ? "promotion" : "no promotion"), current_plan_gen_num,
-        heap_segment_mem (generation_allocation_segment (consing_gen)),
-        generation_allocation_pointer (consing_gen)));
-    if (current_plan_gen_num == -1)
-    {
-        assert (!settings.promotion);
-        current_plan_gen_num = 0;
-        heap_segment* alloc_region = generation_allocation_segment (consing_gen);
-        if (generation_allocation_pointer (consing_gen) > heap_segment_mem (alloc_region))
-        {
-            skip_pins_in_alloc_region (consing_gen, current_plan_gen_num);
-            heap_segment* next_region = heap_segment_next_non_sip (alloc_region);
-            if ((next_region == 0) && (heap_segment_gen_num (alloc_region) > 0))
-            {
-                next_region = generation_start_segment (generation_of (heap_segment_gen_num (alloc_region) - 1));
-            }
-            if (next_region)
-            {
-                init_alloc_info (consing_gen, next_region);
-            }
-            else
-            {
-                assert (pinned_plug_que_empty_p ());
-                if (!pinned_plug_que_empty_p ())
-                {
-                    dprintf (REGIONS_LOG, ("we still have a pin at %Ix but no more regions!?", pinned_plug (oldest_pin ())));
-                    GCToOSInterface::DebugBreak ();
-                }
-                generation_allocation_segment (consing_gen) = 0;
-                generation_allocation_pointer (consing_gen) = 0;
-                generation_allocation_limit (consing_gen) = 0;
-            }
-        }
-    }
-    dprintf (REGIONS_LOG, ("h%d regions in g2: %d, g1: %d, g0: %d, before processing remaining regions",
-        heap_number, planned_regions_per_gen[2], planned_regions_per_gen[1], planned_regions_per_gen[0]));
-    dprintf (REGIONS_LOG, ("h%d g2: surv %Id(p: %Id, %.2f%%), g1: surv %Id(p: %Id, %.2f%%), g0: surv %Id(p: %Id, %.2f%%)",
-        heap_number,
-        dd_survived_size (dynamic_data_of (2)), dd_pinned_survived_size (dynamic_data_of (2)),
-        (dd_survived_size (dynamic_data_of (2)) ? ((double)dd_pinned_survived_size (dynamic_data_of (2)) * 100.0 / (double)dd_survived_size (dynamic_data_of (2))) : 0),
-        dd_survived_size (dynamic_data_of (1)), dd_pinned_survived_size (dynamic_data_of (1)),
-        (dd_survived_size (dynamic_data_of (2)) ? ((double)dd_pinned_survived_size (dynamic_data_of (1)) * 100.0 / (double)dd_survived_size (dynamic_data_of (1))) : 0),
-        dd_survived_size (dynamic_data_of (0)), dd_pinned_survived_size (dynamic_data_of (0)),
-        (dd_survived_size (dynamic_data_of (2)) ? ((double)dd_pinned_survived_size (dynamic_data_of (0)) * 100.0 / (double)dd_survived_size (dynamic_data_of (0))) : 0)));
-    int to_be_empty_regions = 0;
-    while (!pinned_plug_que_empty_p())
-    {
-        uint8_t* oldest_plug = pinned_plug (oldest_pin());
-        heap_segment* nseg = heap_segment_rw (generation_allocation_segment (consing_gen));
-        dprintf (3, ("h%d oldest pin: %p, consing alloc %p, ptr %p, limit %p",
-            heap_number, oldest_plug, heap_segment_mem (nseg),
-            generation_allocation_pointer (consing_gen),
-            generation_allocation_limit (consing_gen)));
-        while ((oldest_plug < generation_allocation_pointer (consing_gen)) ||
-               (oldest_plug >= heap_segment_allocated (nseg)))
-        {
-            assert ((oldest_plug < heap_segment_mem (nseg)) ||
-                    (oldest_plug > heap_segment_reserved (nseg)));
-            assert (generation_allocation_pointer (consing_gen)>=
-                    heap_segment_mem (nseg));
-            assert (generation_allocation_pointer (consing_gen)<=
-                    heap_segment_committed (nseg));
-            dprintf (3, ("h%d PRR: in loop, seg %p pa %p -> alloc ptr %p, plan gen %d->%d",
-                heap_number, heap_segment_mem (nseg),
-                heap_segment_plan_allocated (nseg),
-                generation_allocation_pointer (consing_gen),
-                heap_segment_plan_gen_num (nseg),
-                current_plan_gen_num));
-            assert (!heap_segment_swept_in_plan (nseg));
-            heap_segment_plan_allocated (nseg) = generation_allocation_pointer (consing_gen);
-            decide_on_demotion_pin_surv (nseg, &to_be_empty_regions);
-            heap_segment* next_seg = heap_segment_next_non_sip (nseg);
-            if ((next_seg == 0) && (heap_segment_gen_num (nseg) > 0))
-            {
-                next_seg = generation_start_segment (generation_of (heap_segment_gen_num (nseg) - 1));
-                dprintf (3, ("h%d PRR: switching to next gen%d start %zx",
-                    heap_number, heap_segment_gen_num (next_seg), (size_t)next_seg));
-            }
-            assert (next_seg != 0);
-            nseg = next_seg;
-            generation_allocation_segment (consing_gen) = nseg;
-            generation_allocation_pointer (consing_gen) = heap_segment_mem (nseg);
-        }
-        mark* m = pinned_plug_of (deque_pinned_plug());
-        uint8_t* plug = pinned_plug (m);
-        size_t len = pinned_len (m);
-        set_new_pin_info (m, generation_allocation_pointer (consing_gen));
-        size_t free_size = pinned_len (m);
-        update_planned_gen0_free_space (free_size, plug);
-        dprintf (2, ("h%d plug %p-%p(%zu), free space before %p-%p(%zu)",
-            heap_number, plug, (plug + len), len,
-            generation_allocation_pointer (consing_gen), plug, free_size));
-        generation_allocation_pointer (consing_gen) = plug + len;
-        generation_allocation_limit (consing_gen) =
-            generation_allocation_pointer (consing_gen);
-    }
-    heap_segment* current_region = generation_allocation_segment (consing_gen);
-    if (special_sweep_p)
-    {
-        assert ((current_region == 0) || (heap_segment_next_rw (current_region) == 0));
-        return;
-    }
-    dprintf (REGIONS_LOG, ("after going through the rest of regions - regions in g2: %d, g1: %d, g0: %d, to be empty %d now",
-        planned_regions_per_gen[2], planned_regions_per_gen[1], planned_regions_per_gen[0], to_be_empty_regions));
-    if (current_region)
-    {
-        decide_on_demotion_pin_surv (current_region, &to_be_empty_regions);
-        if (!heap_segment_swept_in_plan (current_region))
-        {
-            heap_segment_plan_allocated (current_region) = generation_allocation_pointer (consing_gen);
-            dprintf (REGIONS_LOG, ("h%d setting alloc seg %p plan alloc to %p",
-                heap_number, heap_segment_mem (current_region),
-                heap_segment_plan_allocated (current_region)));
-        }
-        dprintf (REGIONS_LOG, ("before going through the rest of empty regions - regions in g2: %d, g1: %d, g0: %d, to be empty %d now",
-            planned_regions_per_gen[2], planned_regions_per_gen[1], planned_regions_per_gen[0], to_be_empty_regions));
-        heap_segment* region_no_pins = heap_segment_next (current_region);
-        int region_no_pins_gen_num = heap_segment_gen_num (current_region);
-        do
-        {
-            region_no_pins = heap_segment_non_sip (region_no_pins);
-            if (region_no_pins)
-            {
-                set_region_plan_gen_num (region_no_pins, current_plan_gen_num);
-                to_be_empty_regions++;
-                heap_segment_plan_allocated (region_no_pins) = heap_segment_mem (region_no_pins);
-                dprintf (REGIONS_LOG, ("h%d setting empty seg %p(no pins) plan gen to 0, plan alloc to %p",
-                    heap_number, heap_segment_mem (region_no_pins),
-                    heap_segment_plan_allocated (region_no_pins)));
-                region_no_pins = heap_segment_next (region_no_pins);
-            }
-            if (!region_no_pins)
-            {
-                if (region_no_pins_gen_num > 0)
-                {
-                    region_no_pins_gen_num--;
-                    region_no_pins = generation_start_segment (generation_of (region_no_pins_gen_num));
-                }
-                else
-                    break;
-            }
-        } while (region_no_pins);
-    }
-    if (to_be_empty_regions)
-    {
-        if (planned_regions_per_gen[0] == 0)
-        {
-            dprintf (REGIONS_LOG, ("we didn't seem to find any gen to plan gen0 yet we have empty regions?!"));
-        }
-        assert (planned_regions_per_gen[0]);
-    }
-    int saved_planned_regions_per_gen[max_generation + 1];
-    memcpy (saved_planned_regions_per_gen, planned_regions_per_gen, sizeof (saved_planned_regions_per_gen));
-    assert (saved_planned_regions_per_gen[0] >= to_be_empty_regions);
-    saved_planned_regions_per_gen[0] -= to_be_empty_regions;
-    int plan_regions_needed = 0;
-    for (int gen_idx = settings.condemned_generation; gen_idx >= 0; gen_idx--)
-    {
-        if (saved_planned_regions_per_gen[gen_idx] == 0)
-        {
-            dprintf (REGIONS_LOG, ("g%d has 0 planned regions!!!", gen_idx));
-            plan_regions_needed++;
-        }
-    }
-    dprintf (1, ("we still need %d regions, %d will be empty", plan_regions_needed, to_be_empty_regions));
-    if (plan_regions_needed > to_be_empty_regions)
-    {
-        dprintf (REGIONS_LOG, ("h%d %d regions will be empty but we still need %d regions!!", heap_number, to_be_empty_regions, plan_regions_needed));
-        plan_regions_needed -= to_be_empty_regions;
-        while (plan_regions_needed && get_new_region (0))
-        {
-            new_regions_in_prr++;
-            plan_regions_needed--;
-        }
-        if (plan_regions_needed > 0)
-        {
-            dprintf (REGIONS_LOG, ("h%d %d regions short for having at least one region per gen, special sweep on",
-                heap_number));
-            special_sweep_p = true;
-        }
-    }
-#ifdef _DEBUG
-    {
-        dprintf (REGIONS_LOG, ("regions in g2: %d[%d], g1: %d[%d], g0: %d[%d]",
-            planned_regions_per_gen[2], regions_per_gen[2],
-            planned_regions_per_gen[1], regions_per_gen[1],
-            planned_regions_per_gen[0], regions_per_gen[0]));
-        int total_regions = 0;
-        int total_planned_regions = 0;
-        for (int i = max_generation; i >= 0; i--)
-        {
-            total_regions += regions_per_gen[i];
-            total_planned_regions += planned_regions_per_gen[i];
-        }
-        if (total_regions != total_planned_regions)
-        {
-            dprintf (REGIONS_LOG, ("planned %d regions, saw %d total",
-                total_planned_regions, total_regions));
-        }
-    }
-#endif //_DEBUG
-}
-void gc_heap::grow_mark_list_piece()
-{
-    if (g_mark_list_piece_total_size < region_count * 2 * get_num_heaps())
-    {
-        delete[] g_mark_list_piece;
-        size_t alloc_count = max ((g_mark_list_piece_size * 2), region_count);
-        g_mark_list_piece = new (nothrow) uint8_t * *[alloc_count * 2 * get_num_heaps()];
-        if (g_mark_list_piece != nullptr)
-        {
-            g_mark_list_piece_size = alloc_count;
-        }
-        else
-        {
-            g_mark_list_piece_size = 0;
-        }
-        g_mark_list_piece_total_size = g_mark_list_piece_size * 2 * get_num_heaps();
-    }
-    g_mark_list_piece_size = g_mark_list_piece_total_size / (2 * get_num_heaps());
-}
-void gc_heap::save_current_survived()
-{
-    if (!survived_per_region) return;
-    size_t region_info_to_copy = region_count * sizeof (size_t);
-    memcpy (old_card_survived_per_region, survived_per_region, region_info_to_copy);
-#ifdef _DEBUG
-    for (size_t region_index = 0; region_index < region_count; region_index++)
-    {
-        if (survived_per_region[region_index] != 0)
-        {
-            dprintf (REGIONS_LOG, ("region#[%3zd]: %zd", region_index, survived_per_region[region_index]));
-        }
-    }
-    dprintf (REGIONS_LOG, ("global reported %zd", promoted_bytes (heap_number)));
-#endif //_DEBUG
-}
-void gc_heap::update_old_card_survived()
-{
-    if (!survived_per_region) return;
-    for (size_t region_index = 0; region_index < region_count; region_index++)
-    {
-        old_card_survived_per_region[region_index] = survived_per_region[region_index] -
-                                                     old_card_survived_per_region[region_index];
-        if (survived_per_region[region_index] != 0)
-        {
-            dprintf (REGIONS_LOG, ("region#[%3zd]: %zd (card: %zd)",
-                region_index, survived_per_region[region_index], old_card_survived_per_region[region_index]));
-        }
-    }
-}
-void gc_heap::update_planned_gen0_free_space (size_t free_size, uint8_t* plug)
-{
-    gen0_pinned_free_space += free_size;
-    if (!gen0_large_chunk_found)
-    {
-        gen0_large_chunk_found = (free_size >= END_SPACE_AFTER_GC_FL);
-        if (gen0_large_chunk_found)
-        {
-            dprintf (3, ("h%d found large pin free space: %zd at %p",
-                heap_number, free_size, plug));
-        }
-    }
-}
-void gc_heap::get_gen0_end_plan_space()
-{
-    end_gen0_region_space = 0;
-    for (int gen_idx = settings.condemned_generation; gen_idx >= 0; gen_idx--)
-    {
-        generation* gen = generation_of (gen_idx);
-        heap_segment* region = heap_segment_rw (generation_start_segment (gen));
-        while (region)
-        {
-            if (heap_segment_plan_gen_num (region) == 0)
-            {
-                size_t end_plan_space = heap_segment_reserved (region) - heap_segment_plan_allocated (region);
-                if (!gen0_large_chunk_found)
-                {
-                    gen0_large_chunk_found = (end_plan_space >= END_SPACE_AFTER_GC_FL);
-                    if (gen0_large_chunk_found)
-                    {
-                        dprintf (REGIONS_LOG, ("h%d found large end space: %zd in region %p",
-                            heap_number, end_plan_space, heap_segment_mem (region)));
-                    }
-                }
-                dprintf (REGIONS_LOG, ("h%d found end space: %zd in region %p, total %zd->%zd",
-                    heap_number, end_plan_space, heap_segment_mem (region), end_gen0_region_space,
-                    (end_gen0_region_space + end_plan_space)));
-                end_gen0_region_space += end_plan_space;
-            }
-            region = heap_segment_next (region);
-        }
-    }
-}
-size_t gc_heap::get_gen0_end_space(memory_type type)
-{
-    size_t end_space = 0;
-    heap_segment* seg = generation_start_segment (generation_of (0));
-    while (seg)
-    {
-        uint8_t* allocated = heap_segment_allocated (seg);
-        uint8_t* end = (type == memory_type_reserved) ? heap_segment_reserved (seg) : heap_segment_committed (seg);
-        end_space += end - allocated;
-        dprintf (REGIONS_LOG, ("h%d gen0 seg %p, end %p-%p=%zx, end_space->%zd",
-            heap_number, heap_segment_mem (seg),
-            end, allocated,
-            (end - allocated),
-            end_space));
-        seg = heap_segment_next (seg);
-    }
-    return end_space;
-}
-#endif //USE_REGIONS
-inline
-uint8_t* gc_heap::find_next_marked (uint8_t* x, uint8_t* end,
-                                    BOOL use_mark_list,
-                                    uint8_t**& mark_list_next,
-                                    uint8_t** mark_list_index)
-{
-    if (use_mark_list)
-    {
-        uint8_t* old_x = x;
-        while ((mark_list_next < mark_list_index) &&
-            (*mark_list_next <= x))
-        {
-            mark_list_next++;
-        }
-        x = end;
-        if ((mark_list_next < mark_list_index)
-#ifdef MULTIPLE_HEAPS
-            && (*mark_list_next < end) //for multiple segments
-#endif //MULTIPLE_HEAPS
-            )
-        x = *mark_list_next;
-#ifdef BACKGROUND_GC
-        if (current_c_gc_state == c_gc_state_marking)
-        {
-            assert(gc_heap::background_running_p());
-            bgc_clear_batch_mark_array_bits (old_x, x);
-        }
-#endif //BACKGROUND_GC
-    }
-    else
-    {
-        uint8_t* xl = x;
-#ifdef BACKGROUND_GC
-        if (current_c_gc_state == c_gc_state_marking)
-        {
-            assert (gc_heap::background_running_p());
-            while ((xl < end) && !marked (xl))
-            {
-                dprintf (4, ("-%zx-", (size_t)xl));
-                assert ((size (xl) > 0));
-                background_object_marked (xl, TRUE);
-                xl = xl + Align (size (xl));
-                Prefetch (xl);
-            }
-        }
-        else
-#endif //BACKGROUND_GC
-        {
-            while ((xl < end) && !marked (xl))
-            {
-                dprintf (4, ("-%zx-", (size_t)xl));
-                assert ((size (xl) > 0));
-                xl = xl + Align (size (xl));
-                Prefetch (xl);
-            }
-        }
-        assert (xl <= end);
-        x = xl;
-    }
-    return x;
-}
-#ifdef FEATURE_EVENT_TRACE
-void gc_heap::init_bucket_info()
-{
-    memset (bucket_info, 0, sizeof (bucket_info));
-}
-void gc_heap::add_plug_in_condemned_info (generation* gen, size_t plug_size)
-{
-    uint32_t bucket_index = generation_allocator (gen)->first_suitable_bucket (plug_size);
-    (bucket_info[bucket_index].count)++;
-    bucket_info[bucket_index].size += plug_size;
-}
-#endif //FEATURE_EVENT_TRACE
-inline void save_allocated(heap_segment* seg)
-{
-#ifndef MULTIPLE_HEAPS
-    if (!heap_segment_saved_allocated(seg))
-#endif // !MULTIPLE_HEAPS
-    {
-        heap_segment_saved_allocated (seg) = heap_segment_allocated (seg);
-    }
-}
-#ifdef _PREFAST_
-#pragma warning(push)
-#pragma warning(disable:21000) // Suppress PREFast warning about overly large function
-#endif //_PREFAST_
-void gc_heap::plan_phase (int condemned_gen_number)
-{
-    size_t old_gen2_allocated = 0;
-    size_t old_gen2_size = 0;
-    if (condemned_gen_number == (max_generation - 1))
-    {
-        old_gen2_allocated = generation_free_list_allocated (generation_of (max_generation));
-        old_gen2_size = generation_size (max_generation);
-    }
-    assert (settings.concurrent == FALSE);
-    dprintf (2,(ThreadStressLog::gcStartPlanMsg(), heap_number,
-                condemned_gen_number, settings.promotion ? 1 : 0));
-    generation*  condemned_gen1 = generation_of (condemned_gen_number);
-    BOOL use_mark_list = FALSE;
-#ifdef GC_CONFIG_DRIVEN
-    dprintf (3, ("total number of marked objects: %zd (%zd)",
-                 (mark_list_index - &mark_list[0]), (mark_list_end - &mark_list[0])));
-    if (mark_list_index >= (mark_list_end + 1))
-    {
-        mark_list_index = mark_list_end + 1;
-#ifndef MULTIPLE_HEAPS // in Server GC, we check for mark list overflow in sort_mark_list
-        mark_list_overflow = true;
-#endif
-    }
-#else //GC_CONFIG_DRIVEN
-    dprintf (3, ("mark_list length: %zd",
-                 (mark_list_index - &mark_list[0])));
-#endif //GC_CONFIG_DRIVEN
-    if ((condemned_gen_number < max_generation) &&
-        (mark_list_index <= mark_list_end))
-    {
-#ifndef MULTIPLE_HEAPS
-#ifdef USE_VXSORT
-        do_vxsort (mark_list, mark_list_index - mark_list, slow, shigh);
-#else //USE_VXSORT
-        _sort (&mark_list[0], mark_list_index - 1, 0);
-#endif //USE_VXSORT
-        dprintf (3, ("using mark list at GC #%zd", (size_t)settings.gc_index));
-#endif //!MULTIPLE_HEAPS
-        use_mark_list = TRUE;
-        get_gc_data_per_heap()->set_mechanism_bit(gc_mark_list_bit);
-    }
-    else
-    {
-        dprintf (3, ("mark_list not used"));
-    }
-#ifdef FEATURE_BASICFREEZE
-    sweep_ro_segments();
-#endif //FEATURE_BASICFREEZE
-#ifndef MULTIPLE_HEAPS
-    int condemned_gen_index = get_stop_generation_index (condemned_gen_number);
-    for (; condemned_gen_index <= condemned_gen_number; condemned_gen_index++)
-    {
-        generation* current_gen = generation_of (condemned_gen_index);
-        if (shigh != (uint8_t*)0)
-        {
-            heap_segment* seg = heap_segment_rw (generation_start_segment (current_gen));
-            PREFIX_ASSUME(seg != NULL);
-            heap_segment* fseg = seg;
-            do
-            {
-                heap_segment_saved_allocated(seg) = 0;
-                if (in_range_for_segment (slow, seg))
-                {
-                    uint8_t* start_unmarked = 0;
-#ifdef USE_REGIONS
-                    start_unmarked = heap_segment_mem (seg);
-#else //USE_REGIONS
-                    if (seg == fseg)
-                    {
-                        uint8_t* o = generation_allocation_start (current_gen);
-                        o += get_soh_start_obj_len (o);
-                        if (slow > o)
-                        {
-                            start_unmarked = o;
-                            assert ((slow - o) >= (int)Align (min_obj_size));
-                        }
-                    }
-                    else
-                    {
-                        assert (condemned_gen_number == max_generation);
-                        start_unmarked = heap_segment_mem (seg);
-                    }
-#endif //USE_REGIONS
-                    if (start_unmarked)
-                    {
-                        size_t unmarked_size = slow - start_unmarked;
-                        if (unmarked_size > 0)
-                        {
-#ifdef BACKGROUND_GC
-                            if (current_c_gc_state == c_gc_state_marking)
-                            {
-                                bgc_clear_batch_mark_array_bits (start_unmarked, slow);
-                            }
-#endif //BACKGROUND_GC
-                            make_unused_array (start_unmarked, unmarked_size);
-                        }
-                    }
-                }
-                if (in_range_for_segment (shigh, seg))
-                {
-#ifdef BACKGROUND_GC
-                    if (current_c_gc_state == c_gc_state_marking)
-                    {
-                        bgc_clear_batch_mark_array_bits ((shigh + Align (size (shigh))), heap_segment_allocated (seg));
-                    }
-#endif //BACKGROUND_GC
-                    save_allocated(seg);
-                    heap_segment_allocated (seg) = shigh + Align (size (shigh));
-                }
-                if (!((heap_segment_reserved (seg) >= slow) &&
-                    (heap_segment_mem (seg) <= shigh)))
-                {
-#ifdef BACKGROUND_GC
-                    if (current_c_gc_state == c_gc_state_marking)
-                    {
-#ifdef USE_REGIONS
-                        bgc_clear_batch_mark_array_bits (heap_segment_mem (seg), heap_segment_allocated (seg));
-#else //USE_REGIONS
-                        assert (!"cannot happen with segments");
-#endif //USE_REGIONS
-                    }
-#endif //BACKGROUND_GC
-                    save_allocated(seg);
-                    heap_segment_allocated (seg) =  heap_segment_mem (seg);
-                }
-                seg = heap_segment_next_rw (seg);
-            } while (seg);
-        }
-        else
-        {
-            heap_segment* seg = heap_segment_rw (generation_start_segment (current_gen));
-            PREFIX_ASSUME(seg != NULL);
-            heap_segment* sseg = seg;
-            do
-            {
-                heap_segment_saved_allocated(seg) = 0;
-                uint8_t* start_unmarked = heap_segment_mem (seg);
-#ifndef USE_REGIONS
-                if (seg == sseg)
-                {
-                    uint8_t* o = generation_allocation_start (current_gen);
-                    o += get_soh_start_obj_len (o);
-                    start_unmarked = o;
-                }
-#endif //!USE_REGIONS
-#ifdef BACKGROUND_GC
-                if (current_c_gc_state == c_gc_state_marking)
-                {
-                    bgc_clear_batch_mark_array_bits (start_unmarked, heap_segment_allocated (seg));
-                }
-#endif //BACKGROUND_GC
-                save_allocated(seg);
-                heap_segment_allocated (seg) = start_unmarked;
-                seg = heap_segment_next_rw (seg);
-            } while (seg);
-        }
-    }
-#endif //MULTIPLE_HEAPS
-    heap_segment*  seg1 = heap_segment_rw (generation_start_segment (condemned_gen1));
-    PREFIX_ASSUME(seg1 != NULL);
-    uint8_t*  end = heap_segment_allocated (seg1);
-    uint8_t*  first_condemned_address = get_soh_start_object (seg1, condemned_gen1);
-    uint8_t*  x = first_condemned_address;
-#ifdef USE_REGIONS
-    memset (regions_per_gen, 0, sizeof (regions_per_gen));
-    memset (planned_regions_per_gen, 0, sizeof (planned_regions_per_gen));
-    memset (sip_maxgen_regions_per_gen, 0, sizeof (sip_maxgen_regions_per_gen));
-    memset (reserved_free_regions_sip, 0, sizeof (reserved_free_regions_sip));
-    int pinned_survived_region = 0;
-    uint8_t** mark_list_index = nullptr;
-    uint8_t** mark_list_next = nullptr;
-    if (use_mark_list)
-        mark_list_next = get_region_mark_list (use_mark_list, x, end, &mark_list_index);
-#else // USE_REGIONS
-    assert (!marked (x));
-    uint8_t** mark_list_next = &mark_list[0];
-#endif //USE_REGIONS
-    uint8_t*  plug_end = x;
-    uint8_t*  tree = 0;
-    size_t  sequence_number = 0;
-    uint8_t*  last_node = 0;
-    size_t  current_brick = brick_of (x);
-    BOOL  allocate_in_condemned = ((condemned_gen_number == max_generation)||
-                                   (settings.promotion == FALSE));
-    int  active_old_gen_number = condemned_gen_number;
-    int  active_new_gen_number = (allocate_in_condemned ? condemned_gen_number:
-                                  (1 + condemned_gen_number));
-    generation*  older_gen = 0;
-    generation* consing_gen = condemned_gen1;
-    alloc_list  r_free_list [MAX_SOH_BUCKET_COUNT];
-    size_t r_free_list_space = 0;
-    size_t r_free_obj_space = 0;
-    size_t r_older_gen_free_list_allocated = 0;
-    size_t r_older_gen_condemned_allocated = 0;
-    size_t r_older_gen_end_seg_allocated = 0;
-    uint8_t*  r_allocation_pointer = 0;
-    uint8_t*  r_allocation_limit = 0;
-    uint8_t* r_allocation_start_region = 0;
-    heap_segment*  r_allocation_segment = 0;
-#ifdef FREE_USAGE_STATS
-    size_t r_older_gen_free_space[NUM_GEN_POWER2];
-#endif //FREE_USAGE_STATS
-    if ((condemned_gen_number < max_generation))
-    {
-        older_gen = generation_of (min (max_generation, 1 + condemned_gen_number));
-        generation_allocator (older_gen)->copy_to_alloc_list (r_free_list);
-        r_free_list_space = generation_free_list_space (older_gen);
-        r_free_obj_space = generation_free_obj_space (older_gen);
-#ifdef FREE_USAGE_STATS
-        memcpy (r_older_gen_free_space, older_gen->gen_free_spaces, sizeof (r_older_gen_free_space));
-#endif //FREE_USAGE_STATS
-        generation_allocate_end_seg_p (older_gen) = FALSE;
-#ifdef DOUBLY_LINKED_FL
-        if (older_gen->gen_num == max_generation)
-        {
-            generation_set_bgc_mark_bit_p (older_gen) = FALSE;
-            generation_last_free_list_allocated (older_gen) = 0;
-        }
-#endif //DOUBLY_LINKED_FL
-        r_older_gen_free_list_allocated = generation_free_list_allocated (older_gen);
-        r_older_gen_condemned_allocated = generation_condemned_allocated (older_gen);
-        r_older_gen_end_seg_allocated = generation_end_seg_allocated (older_gen);
-        r_allocation_limit = generation_allocation_limit (older_gen);
-        r_allocation_pointer = generation_allocation_pointer (older_gen);
-        r_allocation_start_region = generation_allocation_context_start_region (older_gen);
-        r_allocation_segment = generation_allocation_segment (older_gen);
-#ifdef USE_REGIONS
-        if (older_gen->gen_num == max_generation)
-        {
-            check_seg_gen_num (r_allocation_segment);
-        }
-#endif //USE_REGIONS
-        heap_segment* start_seg = heap_segment_rw (generation_start_segment (older_gen));
-        PREFIX_ASSUME(start_seg != NULL);
-#ifdef USE_REGIONS
-        heap_segment* skip_seg = 0;
-        assert (generation_allocation_pointer (older_gen) == 0);
-        assert (generation_allocation_limit (older_gen) == 0);
-#else //USE_REGIONS
-        heap_segment* skip_seg = ephemeral_heap_segment;
-        if (start_seg != ephemeral_heap_segment)
-        {
-            assert (condemned_gen_number == (max_generation - 1));
-        }
-#endif //USE_REGIONS
-        if (start_seg != skip_seg)
-        {
-            while (start_seg && (start_seg != skip_seg))
-            {
-                assert (heap_segment_allocated (start_seg) >=
-                        heap_segment_mem (start_seg));
-                assert (heap_segment_allocated (start_seg) <=
-                        heap_segment_reserved (start_seg));
-                heap_segment_plan_allocated (start_seg) =
-                    heap_segment_allocated (start_seg);
-                start_seg = heap_segment_next_rw (start_seg);
-            }
-        }
-    }
-    {
-        int condemned_gen_index1 = get_stop_generation_index (condemned_gen_number);
-        for (; condemned_gen_index1 <= condemned_gen_number; condemned_gen_index1++)
-        {
-            generation* current_gen = generation_of (condemned_gen_index1);
-            heap_segment*  seg2 = heap_segment_rw (generation_start_segment (current_gen));
-            PREFIX_ASSUME(seg2 != NULL);
-            while (seg2)
-            {
-#ifdef USE_REGIONS
-                regions_per_gen[condemned_gen_index1]++;
-                dprintf (REGIONS_LOG, ("h%d PS: gen%d %p-%p (%d, surv: %d), %d regions",
-                    heap_number, condemned_gen_index1,
-                    heap_segment_mem (seg2), heap_segment_allocated (seg2),
-                    (heap_segment_allocated (seg2) - heap_segment_mem (seg2)),
-                    (int)heap_segment_survived (seg2), regions_per_gen[condemned_gen_index1]));
-#endif //USE_REGIONS
-                heap_segment_plan_allocated (seg2) =
-                    heap_segment_mem (seg2);
-                seg2 = heap_segment_next_rw (seg2);
-            }
-        }
-    }
-    int  condemned_gn = condemned_gen_number;
-    int bottom_gen = 0;
-    init_free_and_plug();
-    while (condemned_gn >= bottom_gen)
-    {
-        generation*  condemned_gen2 = generation_of (condemned_gn);
-        generation_allocator (condemned_gen2)->clear();
-        generation_free_list_space (condemned_gen2) = 0;
-        generation_free_obj_space (condemned_gen2) = 0;
-        generation_allocation_size (condemned_gen2) = 0;
-        generation_condemned_allocated (condemned_gen2) = 0;
-        generation_sweep_allocated (condemned_gen2) = 0;
-        generation_free_list_allocated(condemned_gen2) = 0;
-        generation_end_seg_allocated (condemned_gen2) = 0;
-        generation_pinned_allocation_sweep_size (condemned_gen2) = 0;
-        generation_pinned_allocation_compact_size (condemned_gen2) = 0;
-#ifdef FREE_USAGE_STATS
-        generation_pinned_free_obj_space (condemned_gen2) = 0;
-        generation_allocated_in_pinned_free (condemned_gen2) = 0;
-        generation_allocated_since_last_pin (condemned_gen2) = 0;
-#endif //FREE_USAGE_STATS
-#ifndef USE_REGIONS
-        generation_plan_allocation_start (condemned_gen2) = 0;
-#endif //!USE_REGIONS
-        generation_allocation_segment (condemned_gen2) =
-            heap_segment_rw (generation_start_segment (condemned_gen2));
-        PREFIX_ASSUME(generation_allocation_segment(condemned_gen2) != NULL);
-#ifdef USE_REGIONS
-        generation_allocation_pointer (condemned_gen2) =
-            heap_segment_mem (generation_allocation_segment (condemned_gen2));
-#else //USE_REGIONS
-        if (generation_start_segment (condemned_gen2) != ephemeral_heap_segment)
-        {
-            generation_allocation_pointer (condemned_gen2) =
-                heap_segment_mem (generation_allocation_segment (condemned_gen2));
-        }
-        else
-        {
-            generation_allocation_pointer (condemned_gen2) = generation_allocation_start (condemned_gen2);
-        }
-#endif //USE_REGIONS
-        generation_allocation_limit (condemned_gen2) = generation_allocation_pointer (condemned_gen2);
-        generation_allocation_context_start_region (condemned_gen2) = generation_allocation_pointer (condemned_gen2);
-        condemned_gn--;
-    }
-    BOOL allocate_first_generation_start = FALSE;
-    if (allocate_in_condemned)
-    {
-        allocate_first_generation_start = TRUE;
-    }
-    dprintf(3,( " From %zx to %zx", (size_t)x, (size_t)end));
-#ifdef USE_REGIONS
-    if (should_sweep_in_plan (seg1))
-    {
-        sweep_region_in_plan (seg1, use_mark_list, mark_list_next, mark_list_index);
-        x = end;
-    }
-#else
-    demotion_low = MAX_PTR;
-    demotion_high = heap_segment_allocated (ephemeral_heap_segment);
-    demote_gen1_p = !(settings.promotion &&
-        (settings.condemned_generation == (max_generation - 1)) &&
-        gen_to_condemn_reasons.is_only_condition(gen_low_card_p));
-    total_ephemeral_size = 0;
-#endif //!USE_REGIONS
-    print_free_and_plug ("BP");
-#ifndef USE_REGIONS
-    for (int gen_idx = 0; gen_idx <= max_generation; gen_idx++)
-    {
-        generation* temp_gen = generation_of (gen_idx);
-        dprintf (2, ("gen%d start %p, plan start %p",
-            gen_idx,
-            generation_allocation_start (temp_gen),
-            generation_plan_allocation_start (temp_gen)));
-    }
-#endif //!USE_REGIONS
-#ifdef FEATURE_EVENT_TRACE
-    bool record_fl_info_p = (EVENT_ENABLED (GCFitBucketInfo) && (condemned_gen_number == (max_generation - 1)));
-    size_t recorded_fl_info_size = 0;
-    if (record_fl_info_p)
-        init_bucket_info();
-    bool fire_pinned_plug_events_p = EVENT_ENABLED(PinPlugAtGCTime);
-#endif //FEATURE_EVENT_TRACE
-    size_t last_plug_len = 0;
-#ifdef DOUBLY_LINKED_FL
-    gen2_removed_no_undo = 0;
-    saved_pinned_plug_index = INVALID_SAVED_PINNED_PLUG_INDEX;
-#endif //DOUBLY_LINKED_FL
-    while (1)
-    {
-        if (x >= end)
-        {
-            if (!use_mark_list)
-            {
-                assert (x == end);
-            }
-#ifdef USE_REGIONS
-            if (heap_segment_swept_in_plan (seg1))
-            {
-                assert (heap_segment_gen_num (seg1) == active_old_gen_number);
-                dynamic_data* dd_active_old = dynamic_data_of (active_old_gen_number);
-                dd_survived_size (dd_active_old) += heap_segment_survived (seg1);
-                dprintf (REGIONS_LOG, ("region %p-%p SIP",
-                    heap_segment_mem (seg1), heap_segment_allocated (seg1)));
-            }
-            else
-#endif //USE_REGIONS
-            {
-                assert (heap_segment_allocated (seg1) == end);
-                save_allocated(seg1);
-                heap_segment_allocated (seg1) = plug_end;
-                current_brick = update_brick_table (tree, current_brick, x, plug_end);
-                dprintf (REGIONS_LOG, ("region %p-%p(%p) non SIP",
-                    heap_segment_mem (seg1), heap_segment_allocated (seg1),
-                    heap_segment_plan_allocated (seg1)));
-                dprintf (3, ("end of seg: new tree, sequence# 0"));
-                sequence_number = 0;
-                tree = 0;
-            }
-#ifdef USE_REGIONS
-            heap_segment_pinned_survived (seg1) = pinned_survived_region;
-            dprintf (REGIONS_LOG, ("h%d setting seg %p pin surv: %d",
-                heap_number, heap_segment_mem (seg1), pinned_survived_region));
-            pinned_survived_region = 0;
-            if (heap_segment_mem (seg1) == heap_segment_allocated (seg1))
-            {
-                num_regions_freed_in_sweep++;
-            }
-#endif //USE_REGIONS
-            if (heap_segment_next_rw (seg1))
-            {
-                seg1 = heap_segment_next_rw (seg1);
-                end = heap_segment_allocated (seg1);
-                plug_end = x = heap_segment_mem (seg1);
-                current_brick = brick_of (x);
-#ifdef USE_REGIONS
-                if (use_mark_list)
-                    mark_list_next = get_region_mark_list (use_mark_list, x, end, &mark_list_index);
-                if (should_sweep_in_plan (seg1))
-                {
-                    sweep_region_in_plan (seg1, use_mark_list, mark_list_next, mark_list_index);
-                    x = end;
-                }
-#endif //USE_REGIONS
-                dprintf(3,( " From %zx to %zx", (size_t)x, (size_t)end));
-                continue;
-            }
-            else
-            {
-#ifdef USE_REGIONS
-                int saved_active_new_gen_number = active_new_gen_number;
-                BOOL saved_allocate_in_condemned = allocate_in_condemned;
-                dprintf (REGIONS_LOG, ("h%d finished planning gen%d regions into gen%d, alloc_in_condemned: %d",
-                    heap_number, active_old_gen_number, active_new_gen_number, allocate_in_condemned));
-                if (active_old_gen_number <= (settings.promotion ? (max_generation - 1) : max_generation))
-                {
-                    dprintf (REGIONS_LOG, ("h%d active old: %d, new: %d->%d, allocate_in_condemned %d->1",
-                        heap_number, active_old_gen_number,
-                        active_new_gen_number, (active_new_gen_number - 1),
-                        allocate_in_condemned));
-                    active_new_gen_number--;
-                    allocate_in_condemned = TRUE;
-                }
-                if (active_new_gen_number >= 0)
-                {
-                    process_last_np_surv_region (consing_gen, saved_active_new_gen_number, active_new_gen_number);
-                }
-                if (active_old_gen_number == 0)
-                {
-                    process_remaining_regions (active_new_gen_number, consing_gen);
-                    break;
-                }
-                else
-                {
-                    active_old_gen_number--;
-                    seg1 = heap_segment_rw (generation_start_segment (generation_of (active_old_gen_number)));
-                    end = heap_segment_allocated (seg1);
-                    plug_end = x = heap_segment_mem (seg1);
-                    current_brick = brick_of (x);
-                    if (use_mark_list)
-                        mark_list_next = get_region_mark_list (use_mark_list, x, end, &mark_list_index);
-                    if (should_sweep_in_plan (seg1))
-                    {
-                        sweep_region_in_plan (seg1, use_mark_list, mark_list_next, mark_list_index);
-                        x = end;
-                    }
-                    dprintf (REGIONS_LOG,("h%d switching to gen%d start region %p, %p-%p",
-                        heap_number, active_old_gen_number, heap_segment_mem (seg1), x, end));
-                    continue;
-                }
-#else //USE_REGIONS
-                break;
-#endif //USE_REGIONS
-            }
-        }
-        BOOL last_npinned_plug_p = FALSE;
-        BOOL last_pinned_plug_p = FALSE;
-        uint8_t* last_pinned_plug = 0;
-        size_t num_pinned_plugs_in_plug = 0;
-        uint8_t* last_object_in_plug = 0;
-        while ((x < end) && marked (x))
-        {
-            uint8_t*  plug_start = x;
-            uint8_t*  saved_plug_end = plug_end;
-            BOOL   pinned_plug_p = FALSE;
-            BOOL   npin_before_pin_p = FALSE;
-            BOOL   saved_last_npinned_plug_p = last_npinned_plug_p;
-            uint8_t*  saved_last_object_in_plug = last_object_in_plug;
-            BOOL   merge_with_last_pin_p = FALSE;
-            size_t added_pinning_size = 0;
-            size_t artificial_pinned_size = 0;
-            store_plug_gap_info (plug_start, plug_end, last_npinned_plug_p, last_pinned_plug_p,
-                                 last_pinned_plug, pinned_plug_p, last_object_in_plug,
-                                 merge_with_last_pin_p, last_plug_len);
-#ifdef FEATURE_STRUCTALIGN
-            int requiredAlignment = ((CObjectHeader*)plug_start)->GetRequiredAlignment();
-            size_t alignmentOffset = OBJECT_ALIGNMENT_OFFSET;
-#endif // FEATURE_STRUCTALIGN
-            {
-                uint8_t* xl = x;
-                while ((xl < end) && marked (xl) && (pinned (xl) == pinned_plug_p))
-                {
-                    assert (xl < end);
-                    if (pinned(xl))
-                    {
-                        clear_pinned (xl);
-                    }
-#ifdef FEATURE_STRUCTALIGN
-                    else
-                    {
-                        int obj_requiredAlignment = ((CObjectHeader*)xl)->GetRequiredAlignment();
-                        if (obj_requiredAlignment > requiredAlignment)
-                        {
-                            requiredAlignment = obj_requiredAlignment;
-                            alignmentOffset = xl - plug_start + OBJECT_ALIGNMENT_OFFSET;
-                        }
-                    }
-#endif // FEATURE_STRUCTALIGN
-                    clear_marked (xl);
-                    dprintf(4, ("+%zx+", (size_t)xl));
-                    assert ((size (xl) > 0));
-                    assert ((size (xl) <= loh_size_threshold));
-                    last_object_in_plug = xl;
-                    xl = xl + Align (size (xl));
-                    Prefetch (xl);
-                }
-                BOOL next_object_marked_p = ((xl < end) && marked (xl));
-                if (pinned_plug_p)
-                {
-                    if (next_object_marked_p)
-                    {
-                        clear_marked (xl);
-                        last_object_in_plug = xl;
-                        size_t extra_size = Align (size (xl));
-                        xl = xl + extra_size;
-                        added_pinning_size = extra_size;
-                    }
-                }
-                else
-                {
-                    if (next_object_marked_p)
-                        npin_before_pin_p = TRUE;
-                }
-                assert (xl <= end);
-                x = xl;
-            }
-            dprintf (3, ( "%zx[", (size_t)plug_start));
-            plug_end = x;
-            size_t ps = plug_end - plug_start;
-            last_plug_len = ps;
-            dprintf (3, ( "%zx[(%zx)", (size_t)x, ps));
-            uint8_t*  new_address = 0;
-            if (!pinned_plug_p)
-            {
-                if (allocate_in_condemned &&
-                    (settings.condemned_generation == max_generation) &&
-                    (ps > OS_PAGE_SIZE))
-                {
-                    ptrdiff_t reloc = plug_start - generation_allocation_pointer (consing_gen);
-                    if ((ps > (8*OS_PAGE_SIZE)) &&
-                        (reloc > 0) &&
-                        ((size_t)reloc < (ps/16)))
-                    {
-                        dprintf (3, ("Pinning %zx; reloc would have been: %zx",
-                                     (size_t)plug_start, reloc));
-                        assert (!saved_last_npinned_plug_p);
-                        if (last_pinned_plug)
-                        {
-                            dprintf (3, ("artificially pinned plug merged with last pinned plug"));
-                            merge_with_last_pin_p = TRUE;
-                        }
-                        else
-                        {
-                            enque_pinned_plug (plug_start, FALSE, 0);
-                            last_pinned_plug = plug_start;
-                        }
-                        convert_to_pinned_plug (last_npinned_plug_p, last_pinned_plug_p, pinned_plug_p,
-                                                ps, artificial_pinned_size);
-                    }
-                }
-            }
-#ifndef USE_REGIONS
-            if (allocate_first_generation_start)
-            {
-                allocate_first_generation_start = FALSE;
-                plan_generation_start (condemned_gen1, consing_gen, plug_start);
-                assert (generation_plan_allocation_start (condemned_gen1));
-            }
-            if (seg1 == ephemeral_heap_segment)
-            {
-                process_ephemeral_boundaries (plug_start, active_new_gen_number,
-                                              active_old_gen_number,
-                                              consing_gen,
-                                              allocate_in_condemned);
-            }
-#endif //!USE_REGIONS
-            dprintf (3, ("adding %zd to gen%d surv", ps, active_old_gen_number));
-            dynamic_data* dd_active_old = dynamic_data_of (active_old_gen_number);
-            dd_survived_size (dd_active_old) += ps;
-            BOOL convert_to_pinned_p = FALSE;
-            BOOL allocated_in_older_p = FALSE;
-            if (!pinned_plug_p)
-            {
-#if defined (RESPECT_LARGE_ALIGNMENT) || defined (FEATURE_STRUCTALIGN)
-                dd_num_npinned_plugs (dd_active_old)++;
-#endif //RESPECT_LARGE_ALIGNMENT || FEATURE_STRUCTALIGN
-                add_gen_plug (active_old_gen_number, ps);
-                if (allocate_in_condemned)
-                {
-                    verify_pins_with_post_plug_info("before aic");
-                    new_address =
-                        allocate_in_condemned_generations (consing_gen,
-                                                           ps,
-                                                           active_old_gen_number,
-#ifdef SHORT_PLUGS
-                                                           &convert_to_pinned_p,
-                                                           (npin_before_pin_p ? plug_end : 0),
-                                                           seg1,
-#endif //SHORT_PLUGS
-                                                           plug_start REQD_ALIGN_AND_OFFSET_ARG);
-                    verify_pins_with_post_plug_info("after aic");
-                }
-                else
-                {
-                    new_address = allocate_in_older_generation (older_gen, ps, active_old_gen_number, plug_start REQD_ALIGN_AND_OFFSET_ARG);
-                    if (new_address != 0)
-                    {
-                        allocated_in_older_p = TRUE;
-                        if (settings.condemned_generation == (max_generation - 1))
-                        {
-                            dprintf (3, (" NA: %p-%p -> %zx, %zx (%zx)",
-                                plug_start, plug_end,
-                                (size_t)new_address, (size_t)new_address + (plug_end - plug_start),
-                                (size_t)(plug_end - plug_start)));
-                        }
-                    }
-                    else
-                    {
-                        if (generation_allocator(older_gen)->discard_if_no_fit_p())
-                        {
-                            allocate_in_condemned = TRUE;
-                        }
-                        new_address = allocate_in_condemned_generations (consing_gen, ps, active_old_gen_number,
-#ifdef SHORT_PLUGS
-                                                                         &convert_to_pinned_p,
-                                                                         (npin_before_pin_p ? plug_end : 0),
-                                                                         seg1,
-#endif //SHORT_PLUGS
-                                                                         plug_start REQD_ALIGN_AND_OFFSET_ARG);
-                    }
-                }
-#ifdef FEATURE_EVENT_TRACE
-                if (record_fl_info_p && !allocated_in_older_p)
-                {
-                    add_plug_in_condemned_info (older_gen, ps);
-                    recorded_fl_info_size += ps;
-                }
-#endif //FEATURE_EVENT_TRACE
-                if (convert_to_pinned_p)
-                {
-                    assert (last_npinned_plug_p != FALSE);
-                    assert (last_pinned_plug_p == FALSE);
-                    convert_to_pinned_plug (last_npinned_plug_p, last_pinned_plug_p, pinned_plug_p,
-                                            ps, artificial_pinned_size);
-                    enque_pinned_plug (plug_start, FALSE, 0);
-                    last_pinned_plug = plug_start;
-                }
-                else
-                {
-                    if (!new_address)
-                    {
-                        assert (generation_allocation_segment (consing_gen) ==
-                                ephemeral_heap_segment);
-                        assert ((generation_allocation_pointer (consing_gen) + Align (ps)) <
-                                heap_segment_allocated (ephemeral_heap_segment));
-                        assert ((generation_allocation_pointer (consing_gen) + Align (ps)) >
-                                (heap_segment_allocated (ephemeral_heap_segment) + Align (min_obj_size)));
-                    }
-                    else
-                    {
-                        dprintf (3, (ThreadStressLog::gcPlanPlugMsg(),
-                            (size_t)(node_gap_size (plug_start)),
-                            plug_start, plug_end, (size_t)new_address, (size_t)(plug_start - new_address),
-                                (size_t)new_address + ps, ps,
-                                (is_plug_padded (plug_start) ? 1 : 0), x,
-                                (allocated_in_older_p ? "O" : "C")));
-#ifdef SHORT_PLUGS
-                        if (is_plug_padded (plug_start))
-                        {
-                            dprintf (3, ("%p was padded", plug_start));
-                            dd_padding_size (dd_active_old) += Align (min_obj_size);
-                        }
-#endif //SHORT_PLUGS
-                    }
-                }
-            }
-            if (pinned_plug_p)
-            {
-#ifdef FEATURE_EVENT_TRACE
-                if (fire_pinned_plug_events_p)
-                {
-                    FIRE_EVENT(PinPlugAtGCTime, plug_start, plug_end,
-                               (merge_with_last_pin_p ? 0 : (uint8_t*)node_gap_size (plug_start)));
-                }
-#endif //FEATURE_EVENT_TRACE
-                if (merge_with_last_pin_p)
-                {
-                    merge_with_last_pinned_plug (last_pinned_plug, ps);
-                }
-                else
-                {
-                    assert (last_pinned_plug == plug_start);
-                    set_pinned_info (plug_start, ps, consing_gen);
-                }
-                new_address = plug_start;
-                dprintf (3, (ThreadStressLog::gcPlanPinnedPlugMsg(),
-                            (size_t)(node_gap_size (plug_start)), (size_t)plug_start,
-                            (size_t)plug_end, ps,
-                            (merge_with_last_pin_p ? 1 : 0)));
-                dprintf (3, ("adding %zd to gen%d pinned surv", plug_end - plug_start, active_old_gen_number));
-                size_t pinned_plug_size = plug_end - plug_start;
-#ifdef USE_REGIONS
-                pinned_survived_region += (int)pinned_plug_size;
-#endif //USE_REGIONS
-                dd_pinned_survived_size (dd_active_old) += pinned_plug_size;
-                dd_added_pinned_size (dd_active_old) += added_pinning_size;
-                dd_artificial_pinned_survived_size (dd_active_old) += artificial_pinned_size;
-#ifndef USE_REGIONS
-                if (!demote_gen1_p && (active_old_gen_number == (max_generation - 1)))
-                {
-                    last_gen1_pin_end = plug_end;
-                }
-#endif //!USE_REGIONS
-            }
-#ifdef _DEBUG
-            assert (!((new_address > plug_start) &&
-                (new_address < heap_segment_reserved (seg1))));
-#endif //_DEBUG
-            if (!merge_with_last_pin_p)
-            {
-                if (current_brick != brick_of (plug_start))
-                {
-                    current_brick = update_brick_table (tree, current_brick, plug_start, saved_plug_end);
-                    sequence_number = 0;
-                    tree = 0;
-                }
-                set_node_relocation_distance (plug_start, (new_address - plug_start));
-                if (last_node && (node_relocation_distance (last_node) ==
-                                  (node_relocation_distance (plug_start) +
-                                   (ptrdiff_t)node_gap_size (plug_start))))
-                {
-                    dprintf (3, ("%p Lb", plug_start));
-                    set_node_left (plug_start);
-                }
-                if (0 == sequence_number)
-                {
-                    dprintf (2, ("sn: 0, tree is set to %p", plug_start));
-                    tree = plug_start;
-                }
-                verify_pins_with_post_plug_info("before insert node");
-                tree = insert_node (plug_start, ++sequence_number, tree, last_node);
-                dprintf (3, ("tree is %p (b: %zx) after insert_node(lc: %p, rc: %p)",
-                    tree, brick_of (tree),
-                    (tree + node_left_child (tree)), (tree + node_right_child (tree))));
-                last_node = plug_start;
-#ifdef _DEBUG
-                if (!pinned_plug_p)
-                {
-                    if (mark_stack_tos > 0)
-                    {
-                        mark& m = mark_stack_array[mark_stack_tos - 1];
-                        if (m.has_post_plug_info())
-                        {
-                            uint8_t* post_plug_info_start = m.saved_post_plug_info_start;
-                            size_t* current_plug_gap_start = (size_t*)(plug_start - sizeof (plug_and_gap));
-                            if ((uint8_t*)current_plug_gap_start == post_plug_info_start)
-                            {
-                                dprintf (3, ("Ginfo: %zx, %zx, %zx",
-                                    *current_plug_gap_start, *(current_plug_gap_start + 1),
-                                    *(current_plug_gap_start + 2)));
-                                memcpy (&(m.saved_post_plug_debug), current_plug_gap_start, sizeof (gap_reloc_pair));
-                            }
-                        }
-                    }
-                }
-#endif //_DEBUG
-                verify_pins_with_post_plug_info("after insert node");
-            }
-        }
-        if (num_pinned_plugs_in_plug > 1)
-        {
-            dprintf (3, ("more than %zd pinned plugs in this plug", num_pinned_plugs_in_plug));
-        }
-        x = find_next_marked (x, end, use_mark_list, mark_list_next, mark_list_index);
-    }
-#ifndef USE_REGIONS
-    while (!pinned_plug_que_empty_p())
-    {
-        if (settings.promotion)
-        {
-            uint8_t* pplug = pinned_plug (oldest_pin());
-            if (in_range_for_segment (pplug, ephemeral_heap_segment))
-            {
-                consing_gen = ensure_ephemeral_heap_segment (consing_gen);
-                while (active_new_gen_number > 0)
-                {
-                    active_new_gen_number--;
-                    if (active_new_gen_number == (max_generation - 1))
-                    {
-                        maxgen_pinned_compact_before_advance = generation_pinned_allocation_compact_size (generation_of (max_generation));
-                        if (!demote_gen1_p)
-                            advance_pins_for_demotion (consing_gen);
-                    }
-                    generation* gen = generation_of (active_new_gen_number);
-                    plan_generation_start (gen, consing_gen, 0);
-                    if (demotion_low == MAX_PTR)
-                    {
-                        demotion_low = pplug;
-                        dprintf (3, ("end plan: dlow->%p", demotion_low));
-                    }
-                    dprintf (2, ("(%d)gen%d plan start: %zx",
-                                  heap_number, active_new_gen_number, (size_t)generation_plan_allocation_start (gen)));
-                    assert (generation_plan_allocation_start (gen));
-                }
-            }
-        }
-        if (pinned_plug_que_empty_p())
-            break;
-        size_t  entry = deque_pinned_plug();
-        mark*  m = pinned_plug_of (entry);
-        uint8_t*  plug = pinned_plug (m);
-        size_t  len = pinned_len (m);
-        heap_segment* nseg = heap_segment_rw (generation_allocation_segment (consing_gen));
-        while ((plug < generation_allocation_pointer (consing_gen)) ||
-               (plug >= heap_segment_allocated (nseg)))
-        {
-            assert ((plug < heap_segment_mem (nseg)) ||
-                    (plug > heap_segment_reserved (nseg)));
-            assert (generation_allocation_pointer (consing_gen)>=
-                    heap_segment_mem (nseg));
-            assert (generation_allocation_pointer (consing_gen)<=
-                    heap_segment_committed (nseg));
-            heap_segment_plan_allocated (nseg) =
-                generation_allocation_pointer (consing_gen);
-            nseg = heap_segment_next_rw (nseg);
-            generation_allocation_segment (consing_gen) = nseg;
-            generation_allocation_pointer (consing_gen) =
-                heap_segment_mem (nseg);
-        }
-        set_new_pin_info (m, generation_allocation_pointer (consing_gen));
-        dprintf (2, ("pin %p b: %zx->%zx", plug, brick_of (plug),
-            (size_t)(brick_table[brick_of (plug)])));
-        generation_allocation_pointer (consing_gen) = plug + len;
-        generation_allocation_limit (consing_gen) =
-            generation_allocation_pointer (consing_gen);
-        int frgn = object_gennum (plug);
-        if ((frgn != (int)max_generation) && settings.promotion)
-        {
-            generation_pinned_allocation_sweep_size ((generation_of (frgn +1))) += len;
-        }
-    }
-    plan_generation_starts (consing_gen);
-#endif //!USE_REGIONS
-    descr_generations ("AP");
-    print_free_and_plug ("AP");
-    {
-#ifdef SIMPLE_DPRINTF
-        for (int gen_idx = 0; gen_idx <= max_generation; gen_idx++)
-        {
-            generation* temp_gen = generation_of (gen_idx);
-            dynamic_data* temp_dd = dynamic_data_of (gen_idx);
-            int added_pinning_ratio = 0;
-            int artificial_pinned_ratio = 0;
-            if (dd_pinned_survived_size (temp_dd) != 0)
-            {
-                added_pinning_ratio = (int)((float)dd_added_pinned_size (temp_dd) * 100 / (float)dd_pinned_survived_size (temp_dd));
-                artificial_pinned_ratio = (int)((float)dd_artificial_pinned_survived_size (temp_dd) * 100 / (float)dd_pinned_survived_size (temp_dd));
-            }
-            size_t padding_size =
-#ifdef SHORT_PLUGS
-                dd_padding_size (temp_dd);
-#else
-                0;
-#endif //SHORT_PLUGS
-            dprintf (1, ("gen%d: NON PIN alloc: %zd, pin com: %zd, sweep: %zd, surv: %zd, pinsurv: %zd(%d%% added, %d%% art), np surv: %zd, pad: %zd",
-                gen_idx,
-                generation_allocation_size (temp_gen),
-                generation_pinned_allocation_compact_size (temp_gen),
-                generation_pinned_allocation_sweep_size (temp_gen),
-                dd_survived_size (temp_dd),
-                dd_pinned_survived_size (temp_dd),
-                added_pinning_ratio,
-                artificial_pinned_ratio,
-                (dd_survived_size (temp_dd) - dd_pinned_survived_size (temp_dd)),
-                padding_size));
-#ifndef USE_REGIONS
-            dprintf (1, ("gen%d: %p, %p(%zd)",
-                gen_idx,
-                generation_allocation_start (temp_gen),
-                generation_plan_allocation_start (temp_gen),
-                (size_t)(generation_plan_allocation_start (temp_gen) - generation_allocation_start (temp_gen))));
-#endif //USE_REGIONS
-        }
-#endif //SIMPLE_DPRINTF
-    }
-    if (settings.condemned_generation == (max_generation - 1 ))
-    {
-        generation* older_gen = generation_of (settings.condemned_generation + 1);
-        size_t rejected_free_space = generation_free_obj_space (older_gen) - r_free_obj_space;
-        size_t free_list_allocated = generation_free_list_allocated (older_gen) - r_older_gen_free_list_allocated;
-        size_t end_seg_allocated = generation_end_seg_allocated (older_gen) - r_older_gen_end_seg_allocated;
-        size_t condemned_allocated = generation_condemned_allocated (older_gen) - r_older_gen_condemned_allocated;
-        size_t growth = end_seg_allocated + condemned_allocated;
-        if (growth > 0)
-        {
-            dprintf (1, ("gen2 grew %zd (end seg alloc: %zd, condemned alloc: %zd",
-                         growth, end_seg_allocated, condemned_allocated));
-            maxgen_size_inc_p = true;
-        }
-        else
-        {
-            dprintf (1, ("gen2 didn't grow (end seg alloc: %zd, , condemned alloc: %zd, gen1 c alloc: %zd",
-                         end_seg_allocated, condemned_allocated,
-                         generation_condemned_allocated (generation_of (max_generation - 1))));
-        }
-        dprintf (2, ("older gen's free alloc: %zd->%zd, seg alloc: %zd->%zd, condemned alloc: %zd->%zd",
-                    r_older_gen_free_list_allocated, generation_free_list_allocated (older_gen),
-                    r_older_gen_end_seg_allocated, generation_end_seg_allocated (older_gen),
-                    r_older_gen_condemned_allocated, generation_condemned_allocated (older_gen)));
-        dprintf (2, ("this GC did %zd free list alloc(%zd bytes free space rejected)",
-            free_list_allocated, rejected_free_space));
-        maxgen_size_increase* maxgen_size_info = &(get_gc_data_per_heap()->maxgen_size_info);
-        maxgen_size_info->free_list_allocated = free_list_allocated;
-        maxgen_size_info->free_list_rejected = rejected_free_space;
-        maxgen_size_info->end_seg_allocated = end_seg_allocated;
-        maxgen_size_info->condemned_allocated = condemned_allocated;
-        maxgen_size_info->pinned_allocated = maxgen_pinned_compact_before_advance;
-        maxgen_size_info->pinned_allocated_advance = generation_pinned_allocation_compact_size (generation_of (max_generation)) - maxgen_pinned_compact_before_advance;
-#ifdef FREE_USAGE_STATS
-        int free_list_efficiency = 0;
-        if ((free_list_allocated + rejected_free_space) != 0)
-            free_list_efficiency = (int)(((float) (free_list_allocated) / (float)(free_list_allocated + rejected_free_space)) * (float)100);
-        int running_free_list_efficiency = (int)(generation_allocator_efficiency(older_gen)*100);
-        dprintf (1, ("gen%d free list alloc effi: %d%%, current effi: %d%%",
-                    older_gen->gen_num,
-                    free_list_efficiency, running_free_list_efficiency));
-        dprintf (1, ("gen2 free list change"));
-        for (int j = 0; j < NUM_GEN_POWER2; j++)
-        {
-            dprintf (1, ("[h%d][#%zd]: 2^%d: F: %zd->%zd(%zd), P: %zd",
-                heap_number,
-                settings.gc_index,
-                (j + 10), r_older_gen_free_space[j], older_gen->gen_free_spaces[j],
-                (ptrdiff_t)(r_older_gen_free_space[j] - older_gen->gen_free_spaces[j]),
-                (generation_of(max_generation - 1))->gen_plugs[j]));
-        }
-#endif //FREE_USAGE_STATS
-    }
-    size_t fragmentation =
-        generation_fragmentation (generation_of (condemned_gen_number),
-                                  consing_gen,
-                                  heap_segment_allocated (ephemeral_heap_segment));
-    dprintf (2,("Fragmentation: %zd", fragmentation));
-    dprintf (2,("---- End of Plan phase ----"));
-    assert(IsGCInProgress());
-    BOOL should_expand = FALSE;
-    BOOL should_compact= FALSE;
-#ifndef USE_REGIONS
-    ephemeral_promotion = FALSE;
-#endif //!USE_REGIONS
-#ifdef HOST_64BIT
-    if ((!settings.concurrent) &&
-#ifdef USE_REGIONS
-        !special_sweep_p &&
-#endif //USE_REGIONS
-        !provisional_mode_triggered &&
-        ((condemned_gen_number < max_generation) &&
-         ((settings.gen0_reduction_count > 0) || (settings.entry_memory_load >= 95))))
-    {
-        dprintf (GTC_LOG, ("gen0 reduction count is %d, condemning %d, mem load %d",
-                     settings.gen0_reduction_count,
-                     condemned_gen_number,
-                     settings.entry_memory_load));
-        should_compact = TRUE;
-        get_gc_data_per_heap()->set_mechanism (gc_heap_compact,
-            ((settings.gen0_reduction_count > 0) ? compact_fragmented_gen0 : compact_high_mem_load));
-#ifndef USE_REGIONS
-        if ((condemned_gen_number >= (max_generation - 1)) &&
-            dt_low_ephemeral_space_p (tuning_deciding_expansion))
-        {
-            dprintf (GTC_LOG, ("Not enough space for all ephemeral generations with compaction"));
-            should_expand = TRUE;
-        }
-#endif //!USE_REGIONS
-    }
-    else
-#endif // HOST_64BIT
-    {
-        should_compact = decide_on_compacting (condemned_gen_number, fragmentation, should_expand);
-    }
-    if (condemned_gen_number == max_generation)
-    {
-#ifdef FEATURE_LOH_COMPACTION
-        if (settings.loh_compaction)
-        {
-            should_compact = TRUE;
-            get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_loh_forced);
-        }
-        else
-#endif //FEATURE_LOH_COMPACTION
-        {
-            GCToEEInterface::DiagWalkUOHSurvivors(__this, loh_generation);
-            sweep_uoh_objects (loh_generation);
-        }
-        GCToEEInterface::DiagWalkUOHSurvivors(__this, poh_generation);
-        sweep_uoh_objects (poh_generation);
-    }
-    else
-    {
-        settings.loh_compaction = FALSE;
-    }
-#ifdef MULTIPLE_HEAPS
-#ifndef USE_REGIONS
-    new_heap_segment = NULL;
-#endif //!USE_REGIONS
-    if (should_compact && should_expand)
-        gc_policy = policy_expand;
-    else if (should_compact)
-        gc_policy = policy_compact;
-    else
-        gc_policy = policy_sweep;
-    dprintf (3, ("Joining for compaction decision"));
-    gc_t_join.join(this, gc_join_decide_on_compaction);
-    if (gc_t_join.joined())
-    {
-#ifndef USE_REGIONS
-        if (condemned_gen_number == max_generation)
-        {
-            for (int i = 0; i < n_heaps; i++)
-            {
-                g_heaps [i]->rearrange_uoh_segments ();
-            }
-        }
-#endif //!USE_REGIONS
-        if (maxgen_size_inc_p && provisional_mode_triggered
-#ifdef BACKGROUND_GC
-            && !is_bgc_in_progress()
-#endif //BACKGROUND_GC
-            )
-        {
-            pm_trigger_full_gc = true;
-            dprintf (GTC_LOG, ("in PM: maxgen size inc, doing a sweeping gen1 and trigger NGC2"));
-        }
-        else
-        {
-#ifdef USE_REGIONS
-            bool joined_special_sweep_p = false;
-#else
-            settings.demotion = FALSE;
-#endif //USE_REGIONS
-            int pol_max = policy_sweep;
-#ifdef GC_CONFIG_DRIVEN
-            BOOL is_compaction_mandatory = FALSE;
-#endif //GC_CONFIG_DRIVEN
-            int i;
-            for (i = 0; i < n_heaps; i++)
-            {
-                if (pol_max < g_heaps[i]->gc_policy)
-                    pol_max = policy_compact;
-#ifdef USE_REGIONS
-                joined_special_sweep_p |= g_heaps[i]->special_sweep_p;
-#else
-                if (g_heaps[i]->demotion_high >= g_heaps[i]->demotion_low)
-                {
-                    (g_heaps[i]->get_gc_data_per_heap())->set_mechanism_bit (gc_demotion_bit);
-                    settings.demotion = TRUE;
-                }
-#endif //USE_REGIONS
-#ifdef GC_CONFIG_DRIVEN
-                if (!is_compaction_mandatory)
-                {
-                    int compact_reason = (g_heaps[i]->get_gc_data_per_heap())->get_mechanism (gc_heap_compact);
-                    if (compact_reason >= 0)
-                    {
-                        if (gc_heap_compact_reason_mandatory_p[compact_reason])
-                            is_compaction_mandatory = TRUE;
-                    }
-                }
-#endif //GC_CONFIG_DRIVEN
-            }
-#ifdef GC_CONFIG_DRIVEN
-            if (!is_compaction_mandatory)
-            {
-                if (should_do_sweeping_gc (pol_max >= policy_compact))
-                {
-                    pol_max = policy_sweep;
-                }
-                else
-                {
-                    if (pol_max == policy_sweep)
-                        pol_max = policy_compact;
-                }
-            }
-#endif //GC_CONFIG_DRIVEN
-            for (i = 0; i < n_heaps; i++)
-            {
-#ifdef USE_REGIONS
-                g_heaps[i]->special_sweep_p = joined_special_sweep_p;
-                if (joined_special_sweep_p)
-                {
-                    g_heaps[i]->gc_policy = policy_sweep;
-                }
-                else
-#endif //USE_REGIONS
-                if (pol_max > g_heaps[i]->gc_policy)
-                    g_heaps[i]->gc_policy = pol_max;
-#ifndef USE_REGIONS
-                if (g_heaps[i]->gc_policy == policy_expand)
-                {
-                    g_heaps[i]->new_heap_segment =
-                        g_heaps[i]->soh_get_segment_to_expand();
-                    if (!g_heaps[i]->new_heap_segment)
-                    {
-                        set_expand_in_full_gc (condemned_gen_number);
-                        g_heaps[i]->gc_policy = policy_compact;
-                    }
-                }
-#endif //!USE_REGIONS
-            }
-            BOOL is_full_compacting_gc = FALSE;
-            if ((gc_policy >= policy_compact) && (condemned_gen_number == max_generation))
-            {
-                full_gc_counts[gc_type_compacting]++;
-                is_full_compacting_gc = TRUE;
-            }
-            for (i = 0; i < n_heaps; i++)
-            {
-                if (g_gc_card_table!= g_heaps[i]->card_table)
-                {
-                    g_heaps[i]->copy_brick_card_table();
-                }
-                if (is_full_compacting_gc)
-                {
-                    g_heaps[i]->loh_alloc_since_cg = 0;
-                }
-            }
-        }
-#ifdef FEATURE_EVENT_TRACE
-        if (informational_event_enabled_p)
-        {
-            gc_time_info[time_sweep] = GetHighPrecisionTimeStamp();
-            gc_time_info[time_plan] = gc_time_info[time_sweep] - gc_time_info[time_plan];
-        }
-#endif //FEATURE_EVENT_TRACE
-        dprintf(3, ("Starting all gc threads after compaction decision"));
-        gc_t_join.restart();
-    }
-    should_compact = (gc_policy >= policy_compact);
-    should_expand  = (gc_policy >= policy_expand);
-#else //MULTIPLE_HEAPS
-#ifndef USE_REGIONS
-    if (condemned_gen_number == max_generation)
-    {
-        rearrange_uoh_segments ();
-    }
-#endif //!USE_REGIONS
-    if (maxgen_size_inc_p && provisional_mode_triggered
-#ifdef BACKGROUND_GC
-        && !is_bgc_in_progress()
-#endif //BACKGROUND_GC
-        )
-    {
-        pm_trigger_full_gc = true;
-        dprintf (GTC_LOG, ("in PM: maxgen size inc, doing a sweeping gen1 and trigger NGC2"));
-    }
-    else
-    {
-#ifndef USE_REGIONS
-        settings.demotion = ((demotion_high >= demotion_low) ? TRUE : FALSE);
-        if (settings.demotion)
-            get_gc_data_per_heap()->set_mechanism_bit (gc_demotion_bit);
-#endif //!USE_REGIONS
-#ifdef GC_CONFIG_DRIVEN
-        BOOL is_compaction_mandatory = FALSE;
-        int compact_reason = get_gc_data_per_heap()->get_mechanism (gc_heap_compact);
-        if (compact_reason >= 0)
-            is_compaction_mandatory = gc_heap_compact_reason_mandatory_p[compact_reason];
-        if (!is_compaction_mandatory)
-        {
-            if (should_do_sweeping_gc (should_compact))
-                should_compact = FALSE;
-            else
-                should_compact = TRUE;
-        }
-#endif //GC_CONFIG_DRIVEN
-        if (should_compact && (condemned_gen_number == max_generation))
-        {
-            full_gc_counts[gc_type_compacting]++;
-            loh_alloc_since_cg = 0;
-        }
-    }
-#ifdef FEATURE_EVENT_TRACE
-    if (informational_event_enabled_p)
-    {
-        gc_time_info[time_sweep] = GetHighPrecisionTimeStamp();
-        gc_time_info[time_plan] = gc_time_info[time_sweep] - gc_time_info[time_plan];
-    }
-#endif //FEATURE_EVENT_TRACE
-#ifdef USE_REGIONS
-    if (special_sweep_p)
-    {
-        should_compact = FALSE;
-    }
-#endif //!USE_REGIONS
-#endif //MULTIPLE_HEAPS
-#ifdef FEATURE_LOH_COMPACTION
-    loh_compacted_p = FALSE;
-#endif //FEATURE_LOH_COMPACTION
-    if (condemned_gen_number == max_generation)
-    {
-#ifdef FEATURE_LOH_COMPACTION
-        if (settings.loh_compaction)
-        {
-            if (should_compact && plan_loh())
-            {
-                loh_compacted_p = TRUE;
-            }
-            else
-            {
-                GCToEEInterface::DiagWalkUOHSurvivors(__this, loh_generation);
-                sweep_uoh_objects (loh_generation);
-            }
-        }
-        else
-        {
-            if (loh_pinned_queue)
-            {
-                loh_pinned_queue_decay--;
-                if (!loh_pinned_queue_decay)
-                {
-                    delete loh_pinned_queue;
-                    loh_pinned_queue = 0;
-                }
-            }
-        }
-#endif //FEATURE_LOH_COMPACTION
-    }
-    if (!pm_trigger_full_gc && pm_stress_on && provisional_mode_triggered)
-    {
-        if ((settings.condemned_generation == (max_generation - 1)) &&
-            ((settings.gc_index % 5) == 0)
-#ifdef BACKGROUND_GC
-            && !is_bgc_in_progress()
-#endif //BACKGROUND_GC
-            )
-        {
-            pm_trigger_full_gc = true;
-        }
-    }
-    if (settings.condemned_generation == (max_generation - 1))
-    {
-        if (provisional_mode_triggered)
-        {
-            if (should_expand)
-            {
-                should_expand = FALSE;
-                dprintf (GTC_LOG, ("h%d in PM cannot expand", heap_number));
-            }
-        }
-        if (pm_trigger_full_gc)
-        {
-            should_compact = FALSE;
-            dprintf (GTC_LOG, ("h%d PM doing sweeping", heap_number));
-        }
-    }
-    if (should_compact)
-    {
-        dprintf (2,( "**** Doing Compacting GC ****"));
-#if defined(USE_REGIONS) && defined(BACKGROUND_GC)
-        if (should_update_end_mark_size())
-        {
-            background_soh_size_end_mark += generation_end_seg_allocated (older_gen) -
-                                            r_older_gen_end_seg_allocated;
-        }
-#endif //USE_REGIONS && BACKGROUND_GC
-#ifndef USE_REGIONS
-        if (should_expand)
-        {
-#ifndef MULTIPLE_HEAPS
-            heap_segment* new_heap_segment = soh_get_segment_to_expand();
-#endif //!MULTIPLE_HEAPS
-            if (new_heap_segment)
-            {
-                consing_gen = expand_heap(condemned_gen_number,
-                                          consing_gen,
-                                          new_heap_segment);
-            }
-            if (ephemeral_heap_segment != new_heap_segment)
-            {
-                set_expand_in_full_gc (condemned_gen_number);
-                should_expand = FALSE;
-            }
-        }
-#endif //!USE_REGIONS
-        generation_allocation_limit (condemned_gen1) =
-            generation_allocation_pointer (condemned_gen1);
-        if ((condemned_gen_number < max_generation))
-        {
-            generation_allocator (older_gen)->commit_alloc_list_changes();
-            fix_older_allocation_area (older_gen);
-#ifdef FEATURE_EVENT_TRACE
-            if (record_fl_info_p)
-            {
-                uint16_t non_zero_buckets = 0;
-                for (uint16_t bucket_index = 0; bucket_index < NUM_GEN2_ALIST; bucket_index++)
-                {
-                    if (bucket_info[bucket_index].count != 0)
-                    {
-                        if (bucket_index != non_zero_buckets)
-                        {
-                            bucket_info[non_zero_buckets].set (bucket_index,
-                                                            bucket_info[bucket_index].count,
-                                                            bucket_info[bucket_index].size);
-                        }
-                        else
-                        {
-                            bucket_info[bucket_index].index = bucket_index;
-                        }
-                        non_zero_buckets++;
-                    }
-                }
-                if (non_zero_buckets)
-                {
-                    FIRE_EVENT(GCFitBucketInfo,
-                            (uint16_t)etw_bucket_kind::plugs_in_condemned,
-                            recorded_fl_info_size,
-                            non_zero_buckets,
-                            (uint32_t)(sizeof (etw_bucket_info)),
-                            (void *)bucket_info);
-                    init_bucket_info();
-                }
-                size_t max_size_to_count = generation_free_list_space (older_gen) / 4;
-                non_zero_buckets =
-                    generation_allocator (older_gen)->count_largest_items (bucket_info,
-                                                                        max_size_to_count,
-                                                                        max_etw_item_count,
-                                                                        &recorded_fl_info_size);
-                if (non_zero_buckets)
-                {
-                    FIRE_EVENT(GCFitBucketInfo,
-                            (uint16_t)etw_bucket_kind::largest_fl_items,
-                            recorded_fl_info_size,
-                            non_zero_buckets,
-                            (uint32_t)(sizeof (etw_bucket_info)),
-                            (void *)bucket_info);
-                }
-            }
-#endif //FEATURE_EVENT_TRACE
-        }
-#ifndef USE_REGIONS
-        assert (generation_allocation_segment (consing_gen) ==
-                ephemeral_heap_segment);
-#endif //!USE_REGIONS
-        GCToEEInterface::DiagWalkSurvivors(__this, true);
-        relocate_phase (condemned_gen_number, first_condemned_address);
-        compact_phase (condemned_gen_number, first_condemned_address,
-                       (!settings.demotion && settings.promotion));
-        fix_generation_bounds (condemned_gen_number, consing_gen);
-        assert (generation_allocation_limit (youngest_generation) ==
-                generation_allocation_pointer (youngest_generation));
-#ifndef USE_REGIONS
-        if (condemned_gen_number >= (max_generation -1))
-        {
-#ifdef MULTIPLE_HEAPS
-            gc_t_join.join(this, gc_join_rearrange_segs_compaction);
-            if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-            {
-#ifdef FEATURE_EVENT_TRACE
-                if (informational_event_enabled_p)
-                {
-                    uint64_t current_time = GetHighPrecisionTimeStamp();
-                    gc_time_info[time_compact] = current_time - gc_time_info[time_compact];
-                }
-#endif //FEATURE_EVENT_TRACE
-#ifdef MULTIPLE_HEAPS
-                for (int i = 0; i < n_heaps; i++)
-                {
-#ifdef USE_REGIONS
-                    g_heaps [i]->rearrange_uoh_segments();
-#endif //USE_REGIONS
-                    g_heaps [i]->rearrange_heap_segments (TRUE);
-                }
-#else //MULTIPLE_HEAPS
-#ifdef USE_REGIONS
-                rearrange_uoh_segments();
-#endif //USE_REGIONS
-                rearrange_heap_segments (TRUE);
-#endif //MULTIPLE_HEAPS
-#ifdef MULTIPLE_HEAPS
-                gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-            }
-            if (should_expand)
-            {
-                for (int i = 0; i < max_generation; i++)
-                {
-                    generation* gen = generation_of (i);
-                    generation_start_segment (gen) = ephemeral_heap_segment;
-                    generation_allocation_segment (gen) = ephemeral_heap_segment;
-                }
-            }
-        }
-#endif //!USE_REGIONS
-        {
-#ifdef USE_REGIONS
-            end_gen0_region_committed_space = get_gen0_end_space (memory_type_committed);
-            dprintf(REGIONS_LOG, ("h%d computed the end_gen0_region_committed_space value to be %zd", heap_number, end_gen0_region_committed_space));
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-            dprintf(3, ("Joining after end of compaction"));
-            gc_t_join.join(this, gc_join_adjust_handle_age_compact);
-            if (gc_t_join.joined())
-            {
-#endif //MULTIPLE_HEAPS
-#ifdef FEATURE_EVENT_TRACE
-                if (informational_event_enabled_p && (condemned_gen_number < (max_generation -1)))
-                {
-                    uint64_t current_time = GetHighPrecisionTimeStamp();
-                    gc_time_info[time_compact] = current_time - gc_time_info[time_compact];
-                }
-#endif //FEATURE_EVENT_TRACE
-#if defined(_DEBUG) && defined(USE_REGIONS)
-                if (heap_hard_limit)
-                {
-                    size_t committed = 0;
-                    for (int i = 0; i < total_generation_count; i++)
-                    {
-                        int oh = i - max_generation;
-#ifdef MULTIPLE_HEAPS
-                        for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-                        {
-                            gc_heap* hp = gc_heap::g_heaps[hn];
-#else
-                        {
-                            gc_heap* hp = pGenGCHeap;
-#endif // MULTIPLE_HEAPS
-                            heap_segment* region = generation_start_segment (hp->generation_of (i));
-                            while (region)
-                            {
-                                if (!heap_segment_read_only_p (region))
-                                {
-                                    committed += heap_segment_committed (region) - get_region_start (region);
-                                }
-                                region = heap_segment_next (region);
-                            }
-#ifdef BACKGROUND_GC
-                            if (oh == soh)
-                            {
-                                heap_segment* freeable = hp->freeable_soh_segment;
-                                while (freeable)
-                                {
-                                    committed += (heap_segment_committed (freeable) - get_region_start (freeable));
-                                    freeable = heap_segment_next (freeable);
-                                }
-                            }
-                            else
-                            {
-                                heap_segment* freeable = hp->freeable_uoh_segment;
-                                while (freeable)
-                                {
-                                    if (heap_segment_oh (freeable) == oh)
-                                    {
-                                        committed += (heap_segment_committed (freeable) - get_region_start (freeable));
-                                    }
-                                    freeable = heap_segment_next (freeable);
-                                }
-                            }
-#endif //BACKGROUND_GC
-                        }
-                        if (i >= max_generation)
-                        {
-                            assert (committed_by_oh[oh] == committed);
-                            committed = 0;
-                        }
-                    }
-                }
-#endif // _DEBUG && USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-                dprintf(3, ("Restarting after Promotion granted"));
-                gc_t_join.restart();
-            }
-#endif //MULTIPLE_HEAPS
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-            finalize_queue->UpdatePromotedGenerations (condemned_gen_number,
-                                                       (!settings.demotion && settings.promotion));
-#endif // FEATURE_PREMORTEM_FINALIZATION
-            ScanContext sc;
-            sc.thread_number = heap_number;
-            sc.thread_count = n_heaps;
-            sc.promotion = FALSE;
-            sc.concurrent = FALSE;
-            if (settings.promotion && !settings.demotion)
-            {
-                dprintf (2, ("Promoting EE roots for gen %d",
-                             condemned_gen_number));
-                GCScan::GcPromotionsGranted(condemned_gen_number,
-                                                max_generation, &sc);
-            }
-            else if (settings.demotion)
-            {
-                dprintf (2, ("Demoting EE roots for gen %d",
-                             condemned_gen_number));
-                GCScan::GcDemote (condemned_gen_number, max_generation, &sc);
-            }
-        }
-        {
-            reset_pinned_queue_bos();
-#ifndef USE_REGIONS
-            unsigned int  gen_number = min (max_generation, 1 + condemned_gen_number);
-            generation*  gen = generation_of (gen_number);
-            uint8_t*  low = generation_allocation_start (generation_of (gen_number-1));
-            uint8_t*  high =  heap_segment_allocated (ephemeral_heap_segment);
-#endif //!USE_REGIONS
-            while (!pinned_plug_que_empty_p())
-            {
-                mark*  m = pinned_plug_of (deque_pinned_plug());
-                size_t len = pinned_len (m);
-                uint8_t*  arr = (pinned_plug (m) - len);
-                dprintf(3,("free [%zx %zx[ pin",
-                            (size_t)arr, (size_t)arr + len));
-                if (len != 0)
-                {
-                    assert (len >= Align (min_obj_size));
-                    make_unused_array (arr, len);
-                    size_t start_brick = brick_of (arr);
-                    size_t end_brick = brick_of (arr + len);
-                    if (end_brick != start_brick)
-                    {
-                        dprintf (3,
-                                    ("Fixing bricks [%zx, %zx[ to point to unused array %zx",
-                                    start_brick, end_brick, (size_t)arr));
-                        set_brick (start_brick,
-                                    arr - brick_address (start_brick));
-                        size_t brick = start_brick+1;
-                        while (brick < end_brick)
-                        {
-                            set_brick (brick, start_brick - brick);
-                            brick++;
-                        }
-                    }
-#ifdef USE_REGIONS
-                    int gen_number = object_gennum_plan (arr);
-                    generation* gen = generation_of (gen_number);
-#else
-                    if ((heap_segment_mem (ephemeral_heap_segment) <= arr) &&
-                        (heap_segment_reserved (ephemeral_heap_segment) > arr))
-                    {
-                        while ((low <= arr) && (high > arr))
-                        {
-                            gen_number--;
-                            assert ((gen_number >= 1) || (demotion_low != MAX_PTR) ||
-                                    settings.demotion || !settings.promotion);
-                            dprintf (3, ("new free list generation %d", gen_number));
-                            gen = generation_of (gen_number);
-                            if (gen_number >= 1)
-                                low = generation_allocation_start (generation_of (gen_number-1));
-                            else
-                                low = high;
-                        }
-                    }
-                    else
-                    {
-                        dprintf (3, ("new free list generation %d", max_generation));
-                        gen_number = max_generation;
-                        gen = generation_of (gen_number);
-                    }
-#endif //USE_REGIONS
-                    dprintf(3,("h%d threading %p (%zd) before pin in gen %d",
-                        heap_number, arr, len, gen_number));
-                    thread_gap (arr, len, gen);
-                    add_gen_free (gen_number, len);
-                }
-            }
-        }
-        clear_gen1_cards();
-    }
-    else
-    {
-        settings.promotion = TRUE;
-        settings.compaction = FALSE;
-#ifdef USE_REGIONS
-        settings.demotion = FALSE;
-#endif //USE_REGIONS
-        ScanContext sc;
-        sc.thread_number = heap_number;
-        sc.thread_count = n_heaps;
-        sc.promotion = FALSE;
-        sc.concurrent = FALSE;
-        dprintf (2, ("**** Doing Mark and Sweep GC****"));
-        if ((condemned_gen_number < max_generation))
-        {
-#ifdef FREE_USAGE_STATS
-            memcpy (older_gen->gen_free_spaces, r_older_gen_free_space, sizeof (r_older_gen_free_space));
-#endif //FREE_USAGE_STATS
-            generation_allocator (older_gen)->copy_from_alloc_list (r_free_list);
-            generation_free_list_space (older_gen) = r_free_list_space;
-            generation_free_obj_space (older_gen) = r_free_obj_space;
-#ifdef DOUBLY_LINKED_FL
-            if (condemned_gen_number == (max_generation - 1))
-            {
-                dprintf (2, ("[h%d] no undo, FL %zd-%zd -> %zd, FO %zd+%zd=%zd",
-                    heap_number,
-                    generation_free_list_space (older_gen), gen2_removed_no_undo,
-                    (generation_free_list_space (older_gen) - gen2_removed_no_undo),
-                    generation_free_obj_space (older_gen), gen2_removed_no_undo,
-                    (generation_free_obj_space (older_gen) + gen2_removed_no_undo)));
-                generation_free_list_space (older_gen) -= gen2_removed_no_undo;
-                generation_free_obj_space (older_gen) += gen2_removed_no_undo;
-            }
-#endif //DOUBLY_LINKED_FL
-            generation_free_list_allocated (older_gen) = r_older_gen_free_list_allocated;
-            generation_end_seg_allocated (older_gen) = r_older_gen_end_seg_allocated;
-            generation_condemned_allocated (older_gen) = r_older_gen_condemned_allocated;
-            generation_sweep_allocated (older_gen) += dd_survived_size (dynamic_data_of (condemned_gen_number));
-            generation_allocation_limit (older_gen) = r_allocation_limit;
-            generation_allocation_pointer (older_gen) = r_allocation_pointer;
-            generation_allocation_context_start_region (older_gen) = r_allocation_start_region;
-            generation_allocation_segment (older_gen) = r_allocation_segment;
-#ifdef USE_REGIONS
-            if (older_gen->gen_num == max_generation)
-            {
-                check_seg_gen_num (r_allocation_segment);
-            }
-#endif //USE_REGIONS
-        }
-        if ((condemned_gen_number < max_generation))
-        {
-            fix_older_allocation_area (older_gen);
-        }
-        GCToEEInterface::DiagWalkSurvivors(__this, false);
-        make_free_lists (condemned_gen_number);
-        size_t total_recovered_sweep_size = recover_saved_pinned_info();
-        if (total_recovered_sweep_size > 0)
-        {
-            generation_free_obj_space (generation_of (max_generation)) -= total_recovered_sweep_size;
-            dprintf (2, ("h%d: deduct %zd for pin, fo->%zd",
-                heap_number, total_recovered_sweep_size,
-                generation_free_obj_space (generation_of (max_generation))));
-        }
-#ifdef USE_REGIONS
-        end_gen0_region_committed_space = get_gen0_end_space (memory_type_committed);
-        dprintf(REGIONS_LOG, ("h%d computed the end_gen0_region_committed_space value to be %zd", heap_number, end_gen0_region_committed_space));
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-        dprintf(3, ("Joining after end of sweep"));
-        gc_t_join.join(this, gc_join_adjust_handle_age_sweep);
-        if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-        {
-#ifdef FEATURE_EVENT_TRACE
-            if (informational_event_enabled_p)
-            {
-                uint64_t current_time = GetHighPrecisionTimeStamp();
-                gc_time_info[time_sweep] = current_time - gc_time_info[time_sweep];
-            }
-#endif //FEATURE_EVENT_TRACE
-#ifdef USE_REGIONS
-            if (!special_sweep_p)
-#endif //USE_REGIONS
-            {
-                GCScan::GcPromotionsGranted(condemned_gen_number,
-                                                max_generation, &sc);
-            }
-#ifndef USE_REGIONS
-            if (condemned_gen_number >= (max_generation -1))
-            {
-#ifdef MULTIPLE_HEAPS
-                for (int i = 0; i < n_heaps; i++)
-                {
-                    g_heaps[i]->rearrange_heap_segments(FALSE);
-                }
-#else
-                rearrange_heap_segments(FALSE);
-#endif //MULTIPLE_HEAPS
-            }
-#endif //!USE_REGIONS
-#ifdef USE_REGIONS
-            verify_region_to_generation_map ();
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-            dprintf(3, ("Restarting after Promotion granted"));
-            gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-        }
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-#ifdef USE_REGIONS
-        if (!special_sweep_p)
-#endif //USE_REGIONS
-        {
-            finalize_queue->UpdatePromotedGenerations (condemned_gen_number, TRUE);
-        }
-#endif // FEATURE_PREMORTEM_FINALIZATION
-#ifdef USE_REGIONS
-        if (!special_sweep_p)
-#endif //USE_REGIONS
-        {
-            clear_gen1_cards();
-        }
-    }
-}
-#ifdef _PREFAST_
-#pragma warning(pop)
-#endif //_PREFAST_
-/*****************************
-Called after compact phase to fix all generation gaps
-********************************/
-void gc_heap::fix_generation_bounds (int condemned_gen_number,
-                                     generation* consing_gen)
-{
-#ifndef _DEBUG
-    UNREFERENCED_PARAMETER(consing_gen);
-#endif //_DEBUG
-    int gen_number = condemned_gen_number;
-    dprintf (2, ("---- thread regions gen%d GC ----", gen_number));
-#ifdef USE_REGIONS
-    if (settings.promotion && (condemned_gen_number < max_generation))
-    {
-        int older_gen_number = condemned_gen_number + 1;
-        generation* older_gen = generation_of (older_gen_number);
-        heap_segment* last_alloc_region = generation_allocation_segment (older_gen);
-        dprintf (REGIONS_LOG, ("fix till we see alloc region which is %p", heap_segment_mem (last_alloc_region)));
-        heap_segment* region = heap_segment_rw (generation_start_segment (older_gen));
-        while (region)
-        {
-            heap_segment_allocated (region) = heap_segment_plan_allocated (region);
-            if (region == last_alloc_region)
-                break;
-            region = heap_segment_next (region);
-        }
-    }
-    thread_final_regions (true);
-    ephemeral_heap_segment = generation_start_segment (generation_of (0));
-    alloc_allocated = heap_segment_allocated (ephemeral_heap_segment);
-#else //USE_REGIONS
-    assert (generation_allocation_segment (consing_gen) ==
-            ephemeral_heap_segment);
-    int bottom_gen = 0;
-    while (gen_number >= bottom_gen)
-    {
-        generation*  gen = generation_of (gen_number);
-        dprintf(3,("Fixing generation pointers for %d", gen_number));
-        if ((gen_number < max_generation) && ephemeral_promotion)
-        {
-            size_t saved_eph_start_size = saved_ephemeral_plan_start_size[gen_number];
-            make_unused_array (saved_ephemeral_plan_start[gen_number],
-                               saved_eph_start_size);
-            generation_free_obj_space (generation_of (max_generation)) += saved_eph_start_size;
-            dprintf (2, ("[h%d] EP %p(%zd)", heap_number, saved_ephemeral_plan_start[gen_number],
-                saved_ephemeral_plan_start_size[gen_number]));
-        }
-        reset_allocation_pointers (gen, generation_plan_allocation_start (gen));
-        make_unused_array (generation_allocation_start (gen), generation_plan_allocation_start_size (gen));
-        dprintf(3,(" start %zx", (size_t)generation_allocation_start (gen)));
-        gen_number--;
-    }
-#ifdef MULTIPLE_HEAPS
-    if (ephemeral_promotion)
-    {
-        ptrdiff_t delta = 0;
-        heap_segment* old_ephemeral_seg = seg_mapping_table_segment_of (saved_ephemeral_plan_start[max_generation-1]);
-        assert (in_range_for_segment (saved_ephemeral_plan_start[max_generation-1], old_ephemeral_seg));
-        size_t end_card = card_of (align_on_card (heap_segment_plan_allocated (old_ephemeral_seg)));
-        size_t card = card_of (saved_ephemeral_plan_start[max_generation-1]);
-        while (card != end_card)
-        {
-            set_card (card);
-            card++;
-        }
-    }
-#endif //MULTIPLE_HEAPS
-#ifdef BACKGROUND_GC
-    if (should_update_end_mark_size())
-    {
-        background_soh_size_end_mark = generation_size (max_generation);
-    }
-#endif //BACKGROUND_GC
-#endif //!USE_REGIONS
-    {
-        alloc_allocated = heap_segment_plan_allocated(ephemeral_heap_segment);
-#ifdef _DEBUG
-        uint8_t* start = get_soh_start_object (ephemeral_heap_segment, youngest_generation);
-        if (settings.promotion && !settings.demotion)
-        {
-            assert ((start + get_soh_start_obj_len (start)) ==
-                    heap_segment_plan_allocated(ephemeral_heap_segment));
-        }
-#endif //_DEBUG
-        heap_segment_allocated(ephemeral_heap_segment)=
-            heap_segment_plan_allocated(ephemeral_heap_segment);
-    }
-}
-#ifndef USE_REGIONS
-uint8_t* gc_heap::generation_limit (int gen_number)
-{
-    if (settings.promotion)
-    {
-        if (gen_number <= 1)
-            return heap_segment_reserved (ephemeral_heap_segment);
-        else
-            return generation_allocation_start (generation_of ((gen_number - 2)));
-    }
-    else
-    {
-        if (gen_number <= 0)
-            return heap_segment_reserved (ephemeral_heap_segment);
-        else
-            return generation_allocation_start (generation_of ((gen_number - 1)));
-    }
-}
-#endif //!USE_REGIONS
-BOOL gc_heap::ensure_gap_allocation (int condemned_gen_number)
-{
-#ifndef USE_REGIONS
-    uint8_t* start = heap_segment_allocated (ephemeral_heap_segment);
-    size_t size = Align (min_obj_size)*(condemned_gen_number+1);
-    assert ((start + size) <=
-            heap_segment_reserved (ephemeral_heap_segment));
-    if ((start + size) >
-        heap_segment_committed (ephemeral_heap_segment))
-    {
-        if (!grow_heap_segment (ephemeral_heap_segment, start + size))
-        {
-            return FALSE;
-        }
-    }
-#endif //USE_REGIONS
-    return TRUE;
-}
-uint8_t* gc_heap::allocate_at_end (size_t size)
-{
-    uint8_t* start = heap_segment_allocated (ephemeral_heap_segment);
-    size = Align (size);
-    uint8_t* result = start;
-    assert ((start + size) <=
-            heap_segment_reserved (ephemeral_heap_segment));
-    assert ((start + size) <=
-            heap_segment_committed (ephemeral_heap_segment));
-    heap_segment_allocated (ephemeral_heap_segment) += size;
-    return result;
-}
-#ifdef USE_REGIONS
-heap_segment* gc_heap::find_first_valid_region (heap_segment* region, bool compact_p, int* num_returned_regions)
-{
-    check_seg_gen_num (generation_allocation_segment (generation_of (max_generation)));
-    dprintf (REGIONS_LOG, ("  FFVR region %zx(%p), gen%d",
-        (size_t)region, (region ? heap_segment_mem (region) : 0),
-        (region ? heap_segment_gen_num (region) : 0)));
-    if (!region)
-        return 0;
-    heap_segment* current_region = region;
-    do
-    {
-        int gen_num = heap_segment_gen_num (current_region);
-        int plan_gen_num = -1;
-        if (compact_p)
-        {
-            assert (settings.compaction);
-            plan_gen_num = heap_segment_plan_gen_num (current_region);
-            dprintf (REGIONS_LOG, ("  gen%d->%d", gen_num, plan_gen_num));
-        }
-        else
-        {
-            plan_gen_num = (special_sweep_p ? gen_num : get_plan_gen_num (gen_num));
-            dprintf (REGIONS_LOG, ("  gen%d->%d, special_sweep_p %d, swept_in_plan %d",
-                gen_num, plan_gen_num, (int)special_sweep_p,
-                (int)heap_segment_swept_in_plan (current_region)));
-        }
-        uint8_t* allocated = (compact_p ?
-                              heap_segment_plan_allocated (current_region) :
-                              heap_segment_allocated (current_region));
-        if (heap_segment_mem (current_region) == allocated)
-        {
-            heap_segment* region_to_delete = current_region;
-            current_region = heap_segment_next (current_region);
-            return_free_region (region_to_delete);
-            (*num_returned_regions)++;
-            dprintf (REGIONS_LOG, ("  h%d gen%d return region %p to free, current->%p(%p)",
-                heap_number, gen_num, heap_segment_mem (region_to_delete),
-                current_region, (current_region ? heap_segment_mem (current_region) : 0)));
-            if (!current_region)
-                return 0;
-        }
-        else
-        {
-            if (compact_p)
-            {
-                dprintf (REGIONS_LOG, ("  gen%d setting region %p alloc %p to plan %p",
-                    gen_num, heap_segment_mem (current_region),
-                    heap_segment_allocated (current_region),
-                    heap_segment_plan_allocated (current_region)));
-                if (heap_segment_swept_in_plan (current_region))
-                {
-                    assert (heap_segment_allocated (current_region) ==
-                            heap_segment_plan_allocated (current_region));
-                }
-                else
-                {
-                    heap_segment_allocated (current_region) = heap_segment_plan_allocated (current_region);
-                }
-            }
-            else
-            {
-                set_region_plan_gen_num (current_region, plan_gen_num);
-            }
-            if (gen_num >= soh_gen2)
-            {
-                dprintf (REGIONS_LOG, ("  gen%d decommit end of region %p(%p)",
-                    gen_num, current_region, heap_segment_mem (current_region)));
-                decommit_heap_segment_pages (current_region, 0);
-            }
-            dprintf (REGIONS_LOG, ("  set region %p(%p) gen num to %d",
-                current_region, heap_segment_mem (current_region), plan_gen_num));
-            set_region_gen_num (current_region, plan_gen_num);
-            break;
-        }
-    } while (current_region);
-    assert (current_region);
-    if (heap_segment_swept_in_plan (current_region))
-    {
-        int gen_num = heap_segment_gen_num (current_region);
-        dprintf (REGIONS_LOG, ("threading SIP region %p surv %zd onto gen%d",
-            heap_segment_mem (current_region), heap_segment_survived (current_region), gen_num));
-        generation* gen = generation_of (gen_num);
-        generation_allocator (gen)->thread_sip_fl (current_region);
-        generation_free_list_space (gen) += heap_segment_free_list_size (current_region);
-        generation_free_obj_space (gen) += heap_segment_free_obj_size (current_region);
-    }
-    clear_region_sweep_in_plan (current_region);
-    clear_region_demoted (current_region);
-    return current_region;
-}
-void gc_heap::thread_final_regions (bool compact_p)
-{
-    int num_returned_regions = 0;
-    int num_new_regions = 0;
-    for (int i = 0; i < max_generation; i++)
-    {
-        if (reserved_free_regions_sip[i])
-        {
-            return_free_region (reserved_free_regions_sip[i]);
-        }
-    }
-    int condemned_gen_number = settings.condemned_generation;
-    generation_region_info generation_final_regions[max_generation + 1];
-    memset (generation_final_regions, 0, sizeof (generation_final_regions));
-    for (int gen_idx = max_generation; gen_idx > condemned_gen_number; gen_idx--)
-    {
-        generation* gen = generation_of (gen_idx);
-        generation_final_regions[gen_idx].head = heap_segment_rw (generation_start_segment (gen));
-        generation_final_regions[gen_idx].tail = generation_tail_region (gen);
-    }
-#ifdef BACKGROUND_GC
-    heap_segment* max_gen_tail_region = 0;
-    if (should_update_end_mark_size())
-    {
-        max_gen_tail_region = generation_final_regions[max_generation].tail;
-    }
-#endif //BACKGROUND_GC
-    for (int gen_idx = condemned_gen_number; gen_idx >= 0; gen_idx--)
-    {
-        heap_segment* current_region = heap_segment_rw (generation_start_segment (generation_of (gen_idx)));
-        dprintf (REGIONS_LOG, ("gen%d start from %p", gen_idx, heap_segment_mem (current_region)));
-        while ((current_region = find_first_valid_region (current_region, compact_p, &num_returned_regions)))
-        {
-            assert (!compact_p ||
-                    (heap_segment_plan_gen_num (current_region) == heap_segment_gen_num (current_region)));
-            int new_gen_num = heap_segment_plan_gen_num (current_region);
-            generation* new_gen = generation_of (new_gen_num);
-            heap_segment* next_region = heap_segment_next (current_region);
-            if (generation_final_regions[new_gen_num].head)
-            {
-                assert (generation_final_regions[new_gen_num].tail);
-                dprintf (REGIONS_LOG, ("gen%d exists, tail region %p next -> %p",
-                    new_gen_num, heap_segment_mem (generation_final_regions[new_gen_num].tail),
-                    heap_segment_mem (current_region)));
-                heap_segment_next (generation_final_regions[new_gen_num].tail) = current_region;
-                generation_final_regions[new_gen_num].tail = current_region;
-            }
-            else
-            {
-                generation_final_regions[new_gen_num].head = current_region;
-                generation_final_regions[new_gen_num].tail = current_region;
-            }
-            current_region = next_region;
-        }
-    }
-    for (int gen_idx = 0; gen_idx <= max_generation; gen_idx++)
-    {
-        generation* gen = generation_of (gen_idx);
-        if (generation_final_regions[gen_idx].tail)
-        {
-            heap_segment_next (generation_final_regions[gen_idx].tail) = 0;
-        }
-    }
-#ifdef BACKGROUND_GC
-    if (max_gen_tail_region)
-    {
-        max_gen_tail_region = heap_segment_next (max_gen_tail_region);
-        while (max_gen_tail_region)
-        {
-            background_soh_size_end_mark += heap_segment_allocated (max_gen_tail_region) -
-                                            heap_segment_mem (max_gen_tail_region);
-            max_gen_tail_region = heap_segment_next (max_gen_tail_region);
-        }
-    }
-#endif //BACKGROUND_GC
-    for (int gen_idx = 0; gen_idx <= max_generation; gen_idx++)
-    {
-        bool condemned_p = (gen_idx <= condemned_gen_number);
-        assert (condemned_p || generation_final_regions[gen_idx].head);
-        generation* gen = generation_of (gen_idx);
-        heap_segment* start_region = 0;
-        if (generation_final_regions[gen_idx].head)
-        {
-            if (condemned_p)
-            {
-                start_region = generation_final_regions[gen_idx].head;
-                thread_start_region (gen, start_region);
-            }
-            generation_tail_region (gen) = generation_final_regions[gen_idx].tail;
-            dprintf (REGIONS_LOG, ("setting gen%d start %p, tail %p",
-                gen_idx,
-                heap_segment_mem (heap_segment_rw (generation_start_segment (gen))),
-                heap_segment_mem (generation_tail_region (gen))));
-        }
-        else
-        {
-            start_region = get_free_region (gen_idx);
-            assert (start_region);
-            num_new_regions++;
-            thread_start_region (gen, start_region);
-            dprintf (REGIONS_LOG, ("creating new gen%d at %p", gen_idx, heap_segment_mem (start_region)));
-        }
-        if (condemned_p)
-        {
-            uint8_t* gen_start = heap_segment_mem (start_region);
-            reset_allocation_pointers (gen, gen_start);
-        }
-    }
-    int net_added_regions = num_new_regions - num_returned_regions;
-    dprintf (REGIONS_LOG, ("TFR: added %d, returned %d, net %d", num_new_regions, num_returned_regions, net_added_regions));
-    if ((settings.compaction || special_sweep_p) && (net_added_regions > 0))
-    {
-        new_regions_in_threading += net_added_regions;
-        assert (!"we shouldn't be getting new regions in TFR!");
-    }
-    verify_regions (true, false);
-}
-void gc_heap::thread_start_region (generation* gen, heap_segment* region)
-{
-    heap_segment* prev_region = generation_tail_ro_region (gen);
-    if (prev_region)
-    {
-        heap_segment_next (prev_region) = region;
-        dprintf (REGIONS_LOG,("gen%d tail ro %zx(%p) next -> %zx(%p)",
-            gen->gen_num, (size_t)prev_region, heap_segment_mem (prev_region),
-            (size_t)region, heap_segment_mem (region)));
-    }
-    else
-    {
-        generation_start_segment (gen) = region;
-        dprintf (REGIONS_LOG, ("start region of gen%d -> %zx(%p)", gen->gen_num,
-            (size_t)region, heap_segment_mem (region)));
-    }
-    dprintf (REGIONS_LOG, ("tail region of gen%d -> %zx(%p)", gen->gen_num,
-        (size_t)region, heap_segment_mem (region)));
-    generation_tail_region (gen) = region;
-}
-heap_segment* gc_heap::get_new_region (int gen_number, size_t size)
-{
-    heap_segment* new_region = get_free_region (gen_number, size);
-    if (new_region)
-    {
-        switch (gen_number)
-        {
-        default:
-            assert ((new_region->flags & (heap_segment_flags_loh | heap_segment_flags_poh)) == 0);
-            break;
-        case    loh_generation:
-            new_region->flags |= heap_segment_flags_loh;
-            break;
-        case    poh_generation:
-            new_region->flags |= heap_segment_flags_poh;
-            break;
-        }
-        generation* gen = generation_of (gen_number);
-        heap_segment_next (generation_tail_region (gen)) = new_region;
-        generation_tail_region (gen) = new_region;
-        verify_regions (gen_number, false, settings.concurrent);
-    }
-    return new_region;
-}
-heap_segment* gc_heap::allocate_new_region (gc_heap* hp, int gen_num, bool uoh_p, size_t size)
-{
-    uint8_t* start = 0;
-    uint8_t* end = 0;
-    assert (uoh_p || size == 0);
-    bool allocated_p = (uoh_p ?
-        global_region_allocator.allocate_large_region (gen_num, &start, &end, allocate_forward, size, on_used_changed) :
-        global_region_allocator.allocate_basic_region (gen_num, &start, &end, on_used_changed));
-    if (!allocated_p)
-    {
-        return 0;
-    }
-    heap_segment* res = make_heap_segment (start, (end - start), hp, gen_num);
-    dprintf (REGIONS_LOG, ("got a new region %zx %p->%p", (size_t)res, start, end));
-    if (res == nullptr)
-    {
-        global_region_allocator.delete_region (start);
-    }
-    return res;
-}
-void gc_heap::update_start_tail_regions (generation* gen,
-                                         heap_segment* region_to_delete,
-                                         heap_segment* prev_region,
-                                         heap_segment* next_region)
-{
-    if (region_to_delete == heap_segment_rw (generation_start_segment (gen)))
-    {
-        assert (!prev_region);
-        heap_segment* tail_ro_region = generation_tail_ro_region (gen);
-        if (tail_ro_region)
-        {
-            heap_segment_next (tail_ro_region) = next_region;
-            dprintf (REGIONS_LOG, ("gen%d tail ro %zx(%p) next updated to %zx(%p)",
-                gen->gen_num, (size_t)tail_ro_region, heap_segment_mem (tail_ro_region),
-                (size_t)next_region, heap_segment_mem (next_region)));
-        }
-        else
-        {
-            generation_start_segment (gen) = next_region;
-            dprintf (REGIONS_LOG, ("start region of gen%d updated to %zx(%p)", gen->gen_num,
-                (size_t)next_region, heap_segment_mem (next_region)));
-        }
-    }
-    if (region_to_delete == generation_tail_region (gen))
-    {
-        assert (!next_region);
-        generation_tail_region (gen) = prev_region;
-        dprintf (REGIONS_LOG, ("tail region of gen%d updated to %zx(%p)", gen->gen_num,
-            (size_t)prev_region, heap_segment_mem (prev_region)));
-    }
-    verify_regions (false, settings.concurrent);
-}
-inline
-bool gc_heap::should_sweep_in_plan (heap_segment* region)
-{
-    if (!enable_special_regions_p)
-    {
-        return false;
-    }
-    if (settings.reason == reason_induced_aggressive)
-    {
-        return false;
-    }
-    bool sip_p = false;
-    int gen_num = get_region_gen_num (region);
-    int new_gen_num = get_plan_gen_num (gen_num);
-    heap_segment_swept_in_plan (region) = false;
-    dprintf (REGIONS_LOG, ("checking if region %p should be SIP", heap_segment_mem (region)));
-#ifdef STRESS_REGIONS
-    if (0)
-    {
-        num_condemned_regions++;
-        if ((num_condemned_regions % sip_seg_interval) == 0)
-        {
-            set_region_plan_gen_num (region, new_gen_num);
-            sip_p = true;
-        }
-        if ((num_condemned_regions % sip_seg_maxgen_interval) == 0)
-        {
-            set_region_plan_gen_num (region, max_generation);
-            sip_maxgen_regions_per_gen[gen_num]++;
-            sip_p = true;
-        }
-    }
-    else
-#endif //STRESS_REGIONS
-    {
-        size_t basic_region_size = (size_t)1 << min_segment_size_shr;
-        assert (heap_segment_gen_num (region) == heap_segment_plan_gen_num (region));
-        uint8_t surv_ratio = (uint8_t)(((double)heap_segment_survived (region) * 100.0) / (double)basic_region_size);
-        dprintf (2222, ("SSIP: region %p surv %hu / %zd = %d%%(%d)",
-            heap_segment_mem (region),
-            heap_segment_survived (region),
-            basic_region_size,
-            surv_ratio, sip_surv_ratio_th));
-        if (surv_ratio >= sip_surv_ratio_th)
-        {
-            set_region_plan_gen_num (region, new_gen_num);
-            sip_p = true;
-        }
-        if (settings.promotion && (new_gen_num < max_generation))
-        {
-            int old_card_surv_ratio =
-                (int)(((double)heap_segment_old_card_survived (region) * 100.0) / (double)basic_region_size);
-            dprintf (2222, ("SSIP: region %p old card surv %d / %zd = %d%%(%d)",
-                heap_segment_mem (region),
-                heap_segment_old_card_survived (region),
-                basic_region_size,
-                old_card_surv_ratio, sip_surv_ratio_th));
-            if (old_card_surv_ratio >= sip_old_card_surv_ratio_th)
-            {
-                set_region_plan_gen_num (region, max_generation, true);
-                sip_maxgen_regions_per_gen[gen_num]++;
-                sip_p = true;
-            }
-        }
-    }
-    if (sip_p)
-    {
-        if ((new_gen_num < max_generation) &&
-            (sip_maxgen_regions_per_gen[gen_num] == regions_per_gen[gen_num]))
-        {
-            assert (get_region_gen_num (region) == 0);
-            assert (new_gen_num < max_generation);
-            heap_segment* reserved_free_region = get_free_region (gen_num);
-            if (reserved_free_region)
-            {
-                dprintf (REGIONS_LOG, ("all regions in gen%d -> SIP 2, get a new region for it %p",
-                    gen_num, heap_segment_mem (reserved_free_region)));
-                reserved_free_regions_sip[gen_num] = reserved_free_region;
-            }
-            else
-            {
-                sip_maxgen_regions_per_gen[gen_num]--;
-                set_region_plan_gen_num (region, new_gen_num, true);
-            }
-        }
-    }
-    dprintf (REGIONS_LOG, ("region %p %s SIP", heap_segment_mem (region),
-        (sip_p ? "is" : "is not")));
-    return sip_p;
-}
-void heap_segment::thread_free_obj (uint8_t* obj, size_t s)
-{
-    if (s >= min_free_list)
-    {
-        free_list_slot (obj) = 0;
-        if (free_list_head)
-        {
-            assert (free_list_tail);
-            free_list_slot (free_list_tail) = obj;
-        }
-        else
-        {
-            free_list_head = obj;
-        }
-        free_list_tail = obj;
-        free_list_size += s;
-    }
-    else
-    {
-        free_obj_size += s;
-    }
-}
-void gc_heap::sweep_region_in_plan (heap_segment* region,
-                                    BOOL use_mark_list,
-                                    uint8_t**& mark_list_next,
-                                    uint8_t** mark_list_index)
-{
-    set_region_sweep_in_plan (region);
-    region->init_free_list();
-    uint8_t* x = heap_segment_mem (region);
-    uint8_t* last_marked_obj_start = 0;
-    uint8_t* last_marked_obj_end = 0;
-    uint8_t* end = heap_segment_allocated (region);
-    dprintf (2222, ("h%d region %p->%p SIP, gen %d->%d, %s mark list(%p->%p, %p->%p)",
-        heap_number, x, end, heap_segment_gen_num (region), heap_segment_plan_gen_num (region),
-        (use_mark_list ? "using" : "not using"),
-        (uint8_t*)mark_list_next, (mark_list_next ? *mark_list_next : 0),
-        (uint8_t*)mark_list_index, (mark_list_index ? *mark_list_index : 0)));
-#ifdef _DEBUG
-    size_t survived = 0;
-    uint8_t* saved_last_unmarked_obj_start = 0;
-    uint8_t* saved_last_unmarked_obj_end = 0;
-    size_t saved_obj_brick = 0;
-    size_t saved_next_obj_brick = 0;
-#endif //_DEBUG
-    while (x < end)
-    {
-        uint8_t* obj = x;
-        size_t obj_brick = (size_t)obj / brick_size;
-        uint8_t* next_obj = 0;
-        if (marked (obj))
-        {
-            if (pinned(obj))
-            {
-                clear_pinned (obj);
-            }
-            clear_marked (obj);
-            size_t s = size (obj);
-            next_obj = obj + Align (s);
-            last_marked_obj_start = obj;
-            last_marked_obj_end = next_obj;
-#ifdef _DEBUG
-            survived += s;
-#endif //_DEBUG
-            dprintf (4444, ("M: %p-%p(%zd)", obj, next_obj, s));
-        }
-        else
-        {
-            next_obj = find_next_marked (x, end, use_mark_list, mark_list_next, mark_list_index);
-#ifdef _DEBUG
-            saved_last_unmarked_obj_start = obj;
-            saved_last_unmarked_obj_end = next_obj;
-#endif //_DEBUG
-            if ((next_obj > obj) && (next_obj != end))
-            {
-                size_t free_obj_size = next_obj - obj;
-                make_unused_array (obj, free_obj_size);
-                region->thread_free_obj (obj, free_obj_size);
-                dprintf (4444, ("UM threading: %p-%p(%zd)", obj, next_obj, (next_obj - obj)));
-            }
-        }
-        size_t next_obj_brick = (size_t)next_obj / brick_size;
-#ifdef _DEBUG
-        saved_obj_brick = obj_brick;
-        saved_next_obj_brick = next_obj_brick;
-#endif //_DEBUG
-        if (next_obj_brick != obj_brick)
-        {
-            fix_brick_to_highest (obj, next_obj);
-        }
-        x = next_obj;
-    }
-    if (last_marked_obj_start)
-    {
-        size_t last_marked_obj_start_b = brick_of (last_marked_obj_start);
-        size_t last_marked_obj_end_b = brick_of (last_marked_obj_end - 1);
-        dprintf (REGIONS_LOG, ("last live obj %p(%p)-%p, fixing its brick(s) %zx-%zx",
-            last_marked_obj_start, method_table (last_marked_obj_start), last_marked_obj_end,
-            last_marked_obj_start_b, last_marked_obj_end_b));
-        if (last_marked_obj_start_b == last_marked_obj_end_b)
-        {
-            set_brick (last_marked_obj_start_b,
-                    (last_marked_obj_start - brick_address (last_marked_obj_start_b)));
-        }
-        else
-        {
-            set_brick (last_marked_obj_end_b,
-                    (last_marked_obj_start_b - last_marked_obj_end_b));
-        }
-    }
-    else
-    {
-        last_marked_obj_end = heap_segment_mem (region);
-    }
-#ifdef _DEBUG
-    size_t region_index = get_basic_region_index_for_address (heap_segment_mem (region));
-    dprintf (REGIONS_LOG, ("region #%zd %p survived %zd, %s recorded %d",
-        region_index, heap_segment_mem (region), survived,
-        ((survived == heap_segment_survived (region)) ? "same as" : "diff from"),
-        heap_segment_survived (region)));
-#ifdef MULTIPLE_HEAPS
-    assert (survived <= heap_segment_survived (region));
-#else
-    assert (survived == heap_segment_survived (region));
-#endif //MULTIPLE_HEAPS
-#endif //_DEBUG
-    assert (last_marked_obj_end);
-    save_allocated(region);
-    heap_segment_allocated (region) = last_marked_obj_end;
-    heap_segment_plan_allocated (region) = heap_segment_allocated (region);
-    int plan_gen_num = heap_segment_plan_gen_num (region);
-    if (plan_gen_num < heap_segment_gen_num (region))
-    {
-        generation_allocation_size (generation_of (plan_gen_num)) += heap_segment_survived (region);
-        dprintf (REGIONS_LOG, ("sip: g%d alloc size is now %zd", plan_gen_num,
-            generation_allocation_size (generation_of (plan_gen_num))));
-    }
-}
-inline
-void gc_heap::check_demotion_helper_sip (uint8_t** pval, int parent_gen_num, uint8_t* parent_loc)
-{
-    uint8_t* child_object = *pval;
-    if (!is_in_heap_range (child_object))
-        return;
-    assert (child_object != nullptr);
-    int child_object_plan_gen = get_region_plan_gen_num (child_object);
-    if (child_object_plan_gen < parent_gen_num)
-    {
-        set_card (card_of (parent_loc));
-    }
-    dprintf (3, ("SCS %d, %d", child_object_plan_gen, parent_gen_num));
-}
-heap_segment* gc_heap::relocate_advance_to_non_sip (heap_segment* region)
-{
-    THREAD_FROM_HEAP;
-    heap_segment* current_region = region;
-    dprintf (REGIONS_LOG, ("Relocate searching for next non SIP, starting from %p",
-        (region ? heap_segment_mem (region) : 0)));
-    while (current_region)
-    {
-        if (heap_segment_swept_in_plan (current_region))
-        {
-            int gen_num = heap_segment_gen_num (current_region);
-            int plan_gen_num = heap_segment_plan_gen_num (current_region);
-            bool use_sip_demotion = (plan_gen_num > get_plan_gen_num (gen_num));
-            dprintf (REGIONS_LOG, ("region %p is SIP, relocating, gen %d, plan gen: %d(supposed to be %d) %s",
-                heap_segment_mem (current_region), gen_num, plan_gen_num, get_plan_gen_num (gen_num),
-                (use_sip_demotion ? "Sd" : "d")));
-            uint8_t* x = heap_segment_mem (current_region);
-            uint8_t* end = heap_segment_allocated (current_region);
-            while (x < end)
-            {
-                size_t s = size (x);
-                assert (s > 0);
-                uint8_t* next_obj = x + Align (s);
-                Prefetch (next_obj);
-                if (!(((CObjectHeader*)x)->IsFree()))
-                {
-                    if (contain_pointers (x))
-                    {
-                        dprintf (3, ("$%zx$", (size_t)x));
-                        go_through_object_nostart (method_table(x), x, s, pval,
-                        {
-                            uint8_t* child = *pval;
-                            relocate_address (pval THREAD_NUMBER_ARG);
-                            if (use_sip_demotion)
-                                check_demotion_helper_sip (pval, plan_gen_num, (uint8_t*)pval);
-                            else
-                                check_demotion_helper (pval, (uint8_t*)pval);
-                            if (child)
-                            {
-                                dprintf (4444, ("SIP %p(%p)->%p->%p(%p)",
-                                    x, (uint8_t*)pval, child, *pval, method_table (child)));
-                            }
-                        });
-                    }
-                    check_class_object_demotion (x);
-                }
-                x = next_obj;
-            }
-        }
-        else
-        {
-            int gen_num = heap_segment_gen_num (current_region);
-            int plan_gen_num = heap_segment_plan_gen_num (current_region);
-            dprintf (REGIONS_LOG, ("region %p is not SIP, relocating, gen %d, plan gen: %d",
-                heap_segment_mem (current_region), gen_num, plan_gen_num));
-            return current_region;
-        }
-        current_region = heap_segment_next (current_region);
-    }
-    return 0;
-}
-#ifdef STRESS_REGIONS
-void gc_heap::pin_by_gc (uint8_t* object)
-{
-    heap_segment* region = region_of (object);
-    HndAssignHandleGC(pinning_handles_for_alloc[ph_index_per_heap], object);
-    dprintf (REGIONS_LOG, ("h%d pinning object at %zx on eph seg %zx (ph#%d)",
-        heap_number, object, heap_segment_mem (region), ph_index_per_heap));
-    ph_index_per_heap++;
-    if (ph_index_per_heap == PINNING_HANDLE_INITIAL_LENGTH)
-    {
-        ph_index_per_heap = 0;
-    }
-}
-#endif //STRESS_REGIONS
-#endif //USE_REGIONS
-void gc_heap::make_free_lists (int condemned_gen_number)
-{
-    assert (settings.promotion);
-    make_free_args args = {};
-    int stop_gen_idx = get_stop_generation_index (condemned_gen_number);
-    for (int i = condemned_gen_number; i >= stop_gen_idx; i--)
-    {
-        generation* condemned_gen = generation_of (i);
-        heap_segment* current_heap_segment = get_start_segment (condemned_gen);
-#ifdef USE_REGIONS
-    if (!current_heap_segment)
-        continue;
-#endif //USE_REGIONS
-        uint8_t* start_address = get_soh_start_object (current_heap_segment, condemned_gen);
-        size_t current_brick = brick_of (start_address);
-        PREFIX_ASSUME(current_heap_segment != NULL);
-        uint8_t* end_address = heap_segment_allocated (current_heap_segment);
-        size_t  end_brick = brick_of (end_address-1);
-        int current_gen_num = i;
-#ifdef USE_REGIONS
-        args.free_list_gen_number = (special_sweep_p ? current_gen_num : get_plan_gen_num (current_gen_num));
-#else
-        args.free_list_gen_number = get_plan_gen_num (current_gen_num);
-#endif //USE_REGIONS
-        args.free_list_gen = generation_of (args.free_list_gen_number);
-        args.highest_plug = 0;
-#ifdef USE_REGIONS
-        dprintf (REGIONS_LOG, ("starting at gen%d %p -> %p", i, start_address, end_address));
-#else
-        args.current_gen_limit = (((current_gen_num == max_generation)) ?
-                                  MAX_PTR :
-                                  (generation_limit (args.free_list_gen_number)));
-#endif //USE_REGIONS
-#ifndef USE_REGIONS
-        if ((start_address >= end_address) && (condemned_gen_number < max_generation))
-        {
-            break;
-        }
-#endif //!USE_REGIONS
-        while (1)
-        {
-            if ((current_brick > end_brick))
-            {
-#ifndef USE_REGIONS
-                if (args.current_gen_limit == MAX_PTR)
-                {
-                    generation* gen = generation_of (max_generation);
-                    heap_segment* start_seg = heap_segment_rw (generation_start_segment (gen));
-                    PREFIX_ASSUME(start_seg != NULL);
-                    uint8_t* gap = heap_segment_mem (start_seg);
-                    generation_allocation_start (gen) = gap;
-                    heap_segment_allocated (start_seg) = gap + Align (min_obj_size);
-                    make_unused_array (gap, Align (min_obj_size));
-                    reset_allocation_pointers (gen, gap);
-                    dprintf (3, ("Start segment empty, fixing generation start of %d to: %zx",
-                                max_generation, (size_t)gap));
-                    args.current_gen_limit = generation_limit (args.free_list_gen_number);
-                }
-#endif //!USE_REGIONS
-                if (heap_segment_next_non_sip (current_heap_segment))
-                {
-                    current_heap_segment = heap_segment_next_non_sip (current_heap_segment);
-                }
-                else
-                {
-                    break;
-                }
-                current_brick = brick_of (heap_segment_mem (current_heap_segment));
-                end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
-                continue;
-            }
-            {
-                int brick_entry =  brick_table [ current_brick ];
-                if ((brick_entry >= 0))
-                {
-                    make_free_list_in_brick (brick_address (current_brick) + brick_entry-1, &args);
-                    dprintf(3,("Fixing brick entry %zx to %zx",
-                            current_brick, (size_t)args.highest_plug));
-                    set_brick (current_brick,
-                            (args.highest_plug - brick_address (current_brick)));
-                }
-                else
-                {
-                    if ((brick_entry > -32768))
-                    {
-#ifdef _DEBUG
-                        ptrdiff_t offset = brick_of (args.highest_plug) - current_brick;
-                        if ((brick_entry != -32767) && (! ((offset == brick_entry))))
-                        {
-                            assert ((brick_entry == -1));
-                        }
-#endif //_DEBUG
-                        set_brick (current_brick, -1);
-                    }
-                }
-            }
-            current_brick++;
-        }
-    }
-    {
-#ifdef USE_REGIONS
-        check_seg_gen_num (generation_allocation_segment (generation_of (max_generation)));
-        thread_final_regions (false);
-        generation* gen_gen0 = generation_of (0);
-        ephemeral_heap_segment = generation_start_segment (gen_gen0);
-        alloc_allocated = heap_segment_allocated (ephemeral_heap_segment);
-#else //USE_REGIONS
-        int bottom_gen = 0;
-        args.free_list_gen_number--;
-        while (args.free_list_gen_number >= bottom_gen)
-        {
-            uint8_t*  gap = 0;
-            generation* gen2 = generation_of (args.free_list_gen_number);
-            gap = allocate_at_end (Align(min_obj_size));
-            generation_allocation_start (gen2) = gap;
-            reset_allocation_pointers (gen2, gap);
-            dprintf(3,("Fixing generation start of %d to: %zx",
-                       args.free_list_gen_number, (size_t)gap));
-            PREFIX_ASSUME(gap != NULL);
-            make_unused_array (gap, Align (min_obj_size));
-            args.free_list_gen_number--;
-        }
-        uint8_t* start2 = generation_allocation_start (youngest_generation);
-        alloc_allocated = start2 + Align (size (start2));
-#endif //USE_REGIONS
-    }
-}
-void gc_heap::make_free_list_in_brick (uint8_t* tree, make_free_args* args)
-{
-    assert ((tree != NULL));
-    {
-        int  right_node = node_right_child (tree);
-        int left_node = node_left_child (tree);
-        args->highest_plug = 0;
-        if (! (0 == tree))
-        {
-            if (! (0 == left_node))
-            {
-                make_free_list_in_brick (tree + left_node, args);
-            }
-            {
-                uint8_t*  plug = tree;
-                size_t  gap_size = node_gap_size (tree);
-                uint8_t*  gap = (plug - gap_size);
-                args->highest_plug = tree;
-                dprintf (3,("plug: %p (highest p: %p), free %zx len %zd in %d",
-                        plug, args->highest_plug, (size_t)gap, gap_size, args->free_list_gen_number));
-#ifdef SHORT_PLUGS
-                if (is_plug_padded (plug))
-                {
-                    dprintf (3, ("%p padded", plug));
-                    clear_plug_padded (plug);
-                }
-#endif //SHORT_PLUGS
-#ifdef DOUBLY_LINKED_FL
-                if (is_plug_bgc_mark_bit_set (plug))
-                {
-                    dprintf (3333, ("cbgcm: %p", plug));
-                    clear_plug_bgc_mark_bit (plug);
-                }
-                if (is_free_obj_in_compact_bit_set (plug))
-                {
-                    dprintf (3333, ("cfoc: %p", plug));
-                    clear_free_obj_in_compact_bit (plug);
-                }
-#endif //DOUBLY_LINKED_FL
-#ifndef USE_REGIONS
-            gen_crossing:
-                {
-                    if ((args->current_gen_limit == MAX_PTR) ||
-                        ((plug >= args->current_gen_limit) &&
-                         ephemeral_pointer_p (plug)))
-                    {
-                        dprintf(3,(" Crossing Generation boundary at %zx",
-                               (size_t)args->current_gen_limit));
-                        if (!(args->current_gen_limit == MAX_PTR))
-                        {
-                            args->free_list_gen_number--;
-                            args->free_list_gen = generation_of (args->free_list_gen_number);
-                        }
-                        dprintf(3,( " Fixing generation start of %d to: %zx",
-                                args->free_list_gen_number, (size_t)gap));
-                        reset_allocation_pointers (args->free_list_gen, gap);
-                        args->current_gen_limit = generation_limit (args->free_list_gen_number);
-                        if ((gap_size >= (2*Align (min_obj_size))))
-                        {
-                            dprintf(3,(" Splitting the gap in two %zd left",
-                                   gap_size));
-                            make_unused_array (gap, Align(min_obj_size));
-                            gap_size = (gap_size - Align(min_obj_size));
-                            gap = (gap + Align(min_obj_size));
-                        }
-                        else
-                        {
-                            make_unused_array (gap, gap_size);
-                            gap_size = 0;
-                        }
-                        goto gen_crossing;
-                    }
-                }
-#endif //!USE_REGIONS
-                thread_gap (gap, gap_size, args->free_list_gen);
-                add_gen_free (args->free_list_gen->gen_num, gap_size);
-            }
-            if (! (0 == right_node))
-            {
-                make_free_list_in_brick (tree + right_node, args);
-            }
-        }
-    }
-}
-void gc_heap::thread_gap (uint8_t* gap_start, size_t size, generation*  gen)
-{
-#ifndef USE_REGIONS
-    assert (generation_allocation_start (gen));
-#endif
-    if ((size > 0))
-    {
-#ifndef USE_REGIONS
-        assert ((heap_segment_rw (generation_start_segment (gen)) != ephemeral_heap_segment) ||
-                (gap_start > generation_allocation_start (gen)));
-#endif //USE_REGIONS
-        assert (size >= Align (min_obj_size));
-        make_unused_array (gap_start, size,
-                          (!settings.concurrent && (gen != youngest_generation)),
-                          (gen->gen_num == max_generation));
-        dprintf (3, ("fr: [%zx, %zx[", (size_t)gap_start, (size_t)gap_start+size));
-        if ((size >= min_free_list))
-        {
-            generation_free_list_space (gen) += size;
-            generation_allocator (gen)->thread_item (gap_start, size);
-        }
-        else
-        {
-            generation_free_obj_space (gen) += size;
-        }
-    }
-}
-void gc_heap::uoh_thread_gap_front (uint8_t* gap_start, size_t size, generation*  gen)
-{
-#ifndef USE_REGIONS
-    assert (generation_allocation_start (gen));
-#endif
-    if (size >= min_free_list)
-    {
-        generation_free_list_space (gen) += size;
-        generation_allocator (gen)->thread_item_front (gap_start, size);
-    }
-}
-void gc_heap::make_unused_array (uint8_t* x, size_t size, BOOL clearp, BOOL resetp)
-{
-    dprintf (3, (ThreadStressLog::gcMakeUnusedArrayMsg(),
-        (size_t)x, (size_t)(x+size)));
-    assert (size >= Align (min_obj_size));
-    if (resetp)
-    {
-#ifdef BGC_SERVO_TUNING
-        if (!(bgc_tuning::enable_fl_tuning && bgc_tuning::fl_tuning_triggered))
-#endif //BGC_SERVO_TUNING
-        {
-            reset_memory (x, size);
-        }
-    }
-    ((CObjectHeader*)x)->SetFree(size);
-#ifdef HOST_64BIT
-#if BIGENDIAN
-#error "This won't work on big endian platforms"
-#endif
-    size_t size_as_object = (uint32_t)(size - free_object_base_size) + free_object_base_size;
-    if (size_as_object < size)
-    {
-        uint8_t * tmp = x + size_as_object;
-        size_t remaining_size = size - size_as_object;
-        while (remaining_size > UINT32_MAX)
-        {
-            size_t current_size = UINT32_MAX - get_alignment_constant (FALSE)
-                - Align (min_obj_size, get_alignment_constant (FALSE));
-            ((CObjectHeader*)tmp)->SetFree(current_size);
-            remaining_size -= current_size;
-            tmp += current_size;
-        }
-        ((CObjectHeader*)tmp)->SetFree(remaining_size);
-    }
-#endif
-    if (clearp)
-        clear_card_for_addresses (x, x + Align(size));
-}
-void gc_heap::clear_unused_array (uint8_t* x, size_t size)
-{
-    *(((PTR_PTR)x)-1) = 0;
-    ((CObjectHeader*)x)->UnsetFree();
-#ifdef HOST_64BIT
-#if BIGENDIAN
-#error "This won't work on big endian platforms"
-#endif
-    size_t size_as_object = (uint32_t)(size - free_object_base_size) + free_object_base_size;
-    if (size_as_object < size)
-    {
-        uint8_t * tmp = x + size_as_object;
-        size_t remaining_size = size - size_as_object;
-        while (remaining_size > UINT32_MAX)
-        {
-            size_t current_size = UINT32_MAX - get_alignment_constant (FALSE)
-                - Align (min_obj_size, get_alignment_constant (FALSE));
-            ((CObjectHeader*)tmp)->UnsetFree();
-            remaining_size -= current_size;
-            tmp += current_size;
-        }
-        ((CObjectHeader*)tmp)->UnsetFree();
-    }
-#else
-    UNREFERENCED_PARAMETER(size);
-#endif
-}
-inline
-uint8_t* tree_search (uint8_t* tree, uint8_t* old_address)
-{
-    uint8_t* candidate = 0;
-    int cn;
-    while (1)
-    {
-        if (tree < old_address)
-        {
-            if ((cn = node_right_child (tree)) != 0)
-            {
-                assert (candidate < tree);
-                candidate = tree;
-                tree = tree + cn;
-                Prefetch (&((plug_and_pair*)tree)[-1].m_pair.left);
-                continue;
-            }
-            else
-                break;
-        }
-        else if (tree > old_address)
-        {
-            if ((cn = node_left_child (tree)) != 0)
-            {
-                tree = tree + cn;
-                Prefetch (&((plug_and_pair*)tree)[-1].m_pair.left);
-                continue;
-            }
-            else
-                break;
-        } else
-            break;
-    }
-    if (tree <= old_address)
-        return tree;
-    else if (candidate)
-        return candidate;
-    else
-        return tree;
-}
-void gc_heap::relocate_address (uint8_t** pold_address THREAD_NUMBER_DCL)
-{
-    uint8_t* old_address = *pold_address;
-#ifdef USE_REGIONS
-    if (!is_in_gc_range (old_address) || !should_check_brick_for_reloc (old_address))
-    {
-        return;
-    }
-#else //USE_REGIONS
-    if (!((old_address >= gc_low) && (old_address < gc_high)))
-#ifdef MULTIPLE_HEAPS
-    {
-        UNREFERENCED_PARAMETER(thread);
-        if (old_address == 0)
-            return;
-        gc_heap* hp = heap_of (old_address);
-        if ((hp == this) ||
-            !((old_address >= hp->gc_low) && (old_address < hp->gc_high)))
-            return;
-    }
-#else //MULTIPLE_HEAPS
-        return ;
-#endif //MULTIPLE_HEAPS
-#endif //USE_REGIONS
-    size_t  brick = brick_of (old_address);
-    int    brick_entry =  brick_table [ brick ];
-    uint8_t*  new_address = old_address;
-    if (! ((brick_entry == 0)))
-    {
-    retry:
-        {
-            while (brick_entry < 0)
-            {
-                brick = (brick + brick_entry);
-                brick_entry =  brick_table [ brick ];
-            }
-            uint8_t* old_loc = old_address;
-            uint8_t* node = tree_search ((brick_address (brick) + brick_entry-1),
-                                      old_loc);
-            if ((node <= old_loc))
-                new_address = (old_address + node_relocation_distance (node));
-            else
-            {
-                if (node_left_p (node))
-                {
-                    dprintf(3,(" L: %zx", (size_t)node));
-                    new_address = (old_address +
-                                   (node_relocation_distance (node) +
-                                    node_gap_size (node)));
-                }
-                else
-                {
-                    brick = brick - 1;
-                    brick_entry =  brick_table [ brick ];
-                    goto retry;
-                }
-            }
-        }
-        dprintf (4, (ThreadStressLog::gcRelocateReferenceMsg(), pold_address, old_address, new_address));
-        *pold_address = new_address;
-        return;
-    }
-#ifdef FEATURE_LOH_COMPACTION
-    if (settings.loh_compaction)
-    {
-        heap_segment* pSegment = seg_mapping_table_segment_of ((uint8_t*)old_address);
-#ifdef USE_REGIONS
-        if (!pSegment)
-        {
-            return;
-        }
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-        if (heap_segment_heap (pSegment)->loh_compacted_p)
-#else
-        if (loh_compacted_p)
-#endif
-        {
-            size_t flags = pSegment->flags;
-            if ((flags & heap_segment_flags_loh)
-#ifdef FEATURE_BASICFREEZE
-                && !(flags & heap_segment_flags_readonly)
-#endif
-                )
-            {
-                new_address = old_address + loh_node_relocation_distance (old_address);
-                dprintf (4, (ThreadStressLog::gcRelocateReferenceMsg(), pold_address, old_address, new_address));
-                *pold_address = new_address;
-            }
-        }
-    }
-#endif //FEATURE_LOH_COMPACTION
-}
-inline void
-gc_heap::check_class_object_demotion (uint8_t* obj)
-{
-#ifdef COLLECTIBLE_CLASS
-    if (is_collectible(obj))
-    {
-        check_class_object_demotion_internal (obj);
-    }
-#else
-    UNREFERENCED_PARAMETER(obj);
-#endif //COLLECTIBLE_CLASS
-}
-#ifdef COLLECTIBLE_CLASS
-NOINLINE void
-gc_heap::check_class_object_demotion_internal (uint8_t* obj)
-{
-    if (settings.demotion)
-    {
-#ifdef MULTIPLE_HEAPS
-        set_card (card_of (obj));
-#else
-        THREAD_FROM_HEAP;
-        uint8_t* class_obj = get_class_object (obj);
-        dprintf (3, ("%p: got classobj %p", obj, class_obj));
-        uint8_t* temp_class_obj = class_obj;
-        uint8_t** temp = &temp_class_obj;
-        relocate_address (temp THREAD_NUMBER_ARG);
-        check_demotion_helper (temp, obj);
-#endif //MULTIPLE_HEAPS
-    }
-}
-#endif //COLLECTIBLE_CLASS
-inline void
-gc_heap::check_demotion_helper (uint8_t** pval, uint8_t* parent_obj)
-{
-#ifdef USE_REGIONS
-    uint8_t* child_object = *pval;
-    if (!is_in_heap_range (child_object))
-        return;
-    int child_object_plan_gen = get_region_plan_gen_num (child_object);
-    bool child_obj_demoted_p = is_region_demoted (child_object);
-    if (child_obj_demoted_p)
-    {
-        set_card (card_of (parent_obj));
-    }
-    dprintf (3, ("SC %d (%s)", child_object_plan_gen, (child_obj_demoted_p ? "D" : "ND")));
-#else //USE_REGIONS
-    if ((*pval < demotion_high) &&
-        (*pval >= demotion_low))
-    {
-        dprintf(3, ("setting card %zx:%zx",
-                    card_of((uint8_t*)pval),
-                    (size_t)pval));
-        set_card (card_of (parent_obj));
-    }
-#ifdef MULTIPLE_HEAPS
-    else if (settings.demotion)
-    {
-        dprintf (4, ("Demotion active, computing heap_of object"));
-        gc_heap* hp = heap_of (*pval);
-        if ((*pval < hp->demotion_high) &&
-            (*pval >= hp->demotion_low))
-        {
-            dprintf(3, ("setting card %zx:%zx",
-                        card_of((uint8_t*)pval),
-                        (size_t)pval));
-            set_card (card_of (parent_obj));
-        }
-    }
-#endif //MULTIPLE_HEAPS
-#endif //USE_REGIONS
-}
-inline void
-gc_heap::reloc_survivor_helper (uint8_t** pval)
-{
-    THREAD_FROM_HEAP;
-    relocate_address (pval THREAD_NUMBER_ARG);
-    check_demotion_helper (pval, (uint8_t*)pval);
-}
-inline void
-gc_heap::relocate_obj_helper (uint8_t* x, size_t s)
-{
-    THREAD_FROM_HEAP;
-    if (contain_pointers (x))
-    {
-        dprintf (3, ("o$%zx$", (size_t)x));
-        go_through_object_nostart (method_table(x), x, s, pval,
-                            {
-                                uint8_t* child = *pval;
-                                reloc_survivor_helper (pval);
-                                if (child)
-                                {
-                                    dprintf (3, ("%p->%p->%p", (uint8_t*)pval, child, *pval));
-                                }
-                            });
-    }
-    check_class_object_demotion (x);
-}
-inline
-void gc_heap::reloc_ref_in_shortened_obj (uint8_t** address_to_set_card, uint8_t** address_to_reloc)
-{
-    THREAD_FROM_HEAP;
-    uint8_t* old_val = (address_to_reloc ? *address_to_reloc : 0);
-    relocate_address (address_to_reloc THREAD_NUMBER_ARG);
-    if (address_to_reloc)
-    {
-        dprintf (3, ("SR %p: %p->%p", (uint8_t*)address_to_reloc, old_val, *address_to_reloc));
-    }
-    check_demotion_helper (address_to_reloc, (uint8_t*)address_to_set_card);
-}
-void gc_heap::relocate_pre_plug_info (mark* pinned_plug_entry)
-{
-    THREAD_FROM_HEAP;
-    uint8_t* plug = pinned_plug (pinned_plug_entry);
-    uint8_t* pre_plug_start = plug - sizeof (plug_and_gap);
-    pre_plug_start += sizeof (uint8_t*);
-    uint8_t** old_address = &pre_plug_start;
-    uint8_t* old_val = (old_address ? *old_address : 0);
-    relocate_address (old_address THREAD_NUMBER_ARG);
-    if (old_address)
-    {
-        dprintf (3, ("PreR %p: %p->%p, set reloc: %p",
-            (uint8_t*)old_address, old_val, *old_address, (pre_plug_start - sizeof (uint8_t*))));
-    }
-    pinned_plug_entry->set_pre_plug_info_reloc_start (pre_plug_start - sizeof (uint8_t*));
-}
-inline
-void gc_heap::relocate_shortened_obj_helper (uint8_t* x, size_t s, uint8_t* end, mark* pinned_plug_entry, BOOL is_pinned)
-{
-    THREAD_FROM_HEAP;
-    uint8_t* plug = pinned_plug (pinned_plug_entry);
-    if (!is_pinned)
-    {
-        relocate_pre_plug_info (pinned_plug_entry);
-    }
-    verify_pins_with_post_plug_info("after relocate_pre_plug_info");
-    uint8_t* saved_plug_info_start = 0;
-    uint8_t** saved_info_to_relocate = 0;
-    if (is_pinned)
-    {
-        saved_plug_info_start = (uint8_t*)(pinned_plug_entry->get_post_plug_info_start());
-        saved_info_to_relocate = (uint8_t**)(pinned_plug_entry->get_post_plug_reloc_info());
-    }
-    else
-    {
-        saved_plug_info_start = (plug - sizeof (plug_and_gap));
-        saved_info_to_relocate = (uint8_t**)(pinned_plug_entry->get_pre_plug_reloc_info());
-    }
-    uint8_t** current_saved_info_to_relocate = 0;
-    uint8_t* child = 0;
-    dprintf (3, ("x: %p, pp: %p, end: %p", x, plug, end));
-    if (contain_pointers (x))
-    {
-        dprintf (3,("s$%zx$", (size_t)x));
-        go_through_object_nostart (method_table(x), x, s, pval,
-        {
-            dprintf (3, ("obj %p, member: %p->%p", x, (uint8_t*)pval, *pval));
-            if ((uint8_t*)pval >= end)
-            {
-                current_saved_info_to_relocate = saved_info_to_relocate + ((uint8_t*)pval - saved_plug_info_start) / sizeof (uint8_t**);
-                child = *current_saved_info_to_relocate;
-                reloc_ref_in_shortened_obj (pval, current_saved_info_to_relocate);
-                dprintf (3, ("last part: R-%p(saved: %p)->%p ->%p",
-                    (uint8_t*)pval, current_saved_info_to_relocate, child, *current_saved_info_to_relocate));
-            }
-            else
-            {
-                reloc_survivor_helper (pval);
-            }
-        });
-    }
-    check_class_object_demotion (x);
-}
-void gc_heap::relocate_survivor_helper (uint8_t* plug, uint8_t* plug_end)
-{
-    uint8_t*  x = plug;
-    while (x < plug_end)
-    {
-        size_t s = size (x);
-        uint8_t* next_obj = x + Align (s);
-        Prefetch (next_obj);
-        relocate_obj_helper (x, s);
-        assert (s > 0);
-        x = next_obj;
-    }
-}
-void gc_heap::verify_pins_with_post_plug_info (const char* msg)
-{
-#if defined (_DEBUG) && defined (VERIFY_HEAP)
-    if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
-    {
-        if (!verify_pinned_queue_p)
-            return;
-        if (settings.heap_expansion)
-            return;
-        for (size_t i = 0; i < mark_stack_tos; i++)
-        {
-            mark& m = mark_stack_array[i];
-            mark* pinned_plug_entry = pinned_plug_of(i);
-            if (pinned_plug_entry->has_post_plug_info() &&
-                pinned_plug_entry->post_short_p() &&
-                (pinned_plug_entry->saved_post_plug_debug.gap != 1))
-            {
-                uint8_t* next_obj = pinned_plug_entry->get_post_plug_info_start() + sizeof (plug_and_gap);
-                dprintf (3, ("OFP: %p, G: %zx, R: %zx, LC: %d, RC: %d",
-                    next_obj, node_gap_size (next_obj), node_relocation_distance (next_obj),
-                    (int)node_left_child (next_obj), (int)node_right_child (next_obj)));
-                size_t* post_plug_debug = (size_t*)(&m.saved_post_plug_debug);
-                if (node_gap_size (next_obj) != *post_plug_debug)
-                {
-                    dprintf (1, ("obj: %p gap should be %zx but it is %zx",
-                        next_obj, *post_plug_debug, (size_t)(node_gap_size (next_obj))));
-                    FATAL_GC_ERROR();
-                }
-                post_plug_debug++;
-                if (*((size_t*)(next_obj - 3 * sizeof (size_t))) != *post_plug_debug)
-                {
-                    dprintf (1, ("obj: %p reloc should be %zx but it is %zx",
-                        next_obj, *post_plug_debug, (size_t)(node_relocation_distance (next_obj))));
-                    FATAL_GC_ERROR();
-                }
-                if (node_left_child (next_obj) > 0)
-                {
-                    dprintf (1, ("obj: %p, vLC: %d\n", next_obj, (int)(node_left_child (next_obj))));
-                    FATAL_GC_ERROR();
-                }
-            }
-        }
-        dprintf (3, ("%s verified", msg));
-    }
-#else
-    UNREFERENCED_PARAMETER(msg);
-#endif // _DEBUG && VERIFY_HEAP
-}
-#ifdef COLLECTIBLE_CLASS
-inline void
-gc_heap::unconditional_set_card_collectible (uint8_t* obj)
-{
-    if (settings.demotion)
-    {
-        set_card (card_of (obj));
-    }
-}
-#endif //COLLECTIBLE_CLASS
-void gc_heap::relocate_shortened_survivor_helper (uint8_t* plug, uint8_t* plug_end, mark* pinned_plug_entry)
-{
-    uint8_t*  x = plug;
-    uint8_t* p_plug = pinned_plug (pinned_plug_entry);
-    BOOL is_pinned = (plug == p_plug);
-    BOOL check_short_obj_p = (is_pinned ? pinned_plug_entry->post_short_p() : pinned_plug_entry->pre_short_p());
-    plug_end += sizeof (gap_reloc_pair);
-    dprintf (3, ("%s %p-%p short, LO: %s OW", (is_pinned ? "PP" : "NP"), plug, plug_end, (check_short_obj_p ? "is" : "is not")));
-    verify_pins_with_post_plug_info("begin reloc short surv");
-    while (x < plug_end)
-    {
-        if (check_short_obj_p && ((DWORD)(plug_end - x) < (DWORD)min_pre_pin_obj_size))
-        {
-            dprintf (3, ("last obj %p is short", x));
-            if (is_pinned)
-            {
-#ifdef COLLECTIBLE_CLASS
-                if (pinned_plug_entry->post_short_collectible_p())
-                    unconditional_set_card_collectible (x);
-#endif //COLLECTIBLE_CLASS
-                uint8_t** saved_plug_info_start = (uint8_t**)(pinned_plug_entry->get_post_plug_info_start());
-                uint8_t** saved_info_to_relocate = (uint8_t**)(pinned_plug_entry->get_post_plug_reloc_info());
-                for (size_t i = 0; i < pinned_plug_entry->get_max_short_bits(); i++)
-                {
-                    if (pinned_plug_entry->post_short_bit_p (i))
-                    {
-                        reloc_ref_in_shortened_obj ((saved_plug_info_start + i), (saved_info_to_relocate + i));
-                    }
-                }
-            }
-            else
-            {
-#ifdef COLLECTIBLE_CLASS
-                if (pinned_plug_entry->pre_short_collectible_p())
-                    unconditional_set_card_collectible (x);
-#endif //COLLECTIBLE_CLASS
-                relocate_pre_plug_info (pinned_plug_entry);
-                uint8_t** saved_plug_info_start = (uint8_t**)(p_plug - sizeof (plug_and_gap));
-                uint8_t** saved_info_to_relocate = (uint8_t**)(pinned_plug_entry->get_pre_plug_reloc_info());
-                for (size_t i = 0; i < pinned_plug_entry->get_max_short_bits(); i++)
-                {
-                    if (pinned_plug_entry->pre_short_bit_p (i))
-                    {
-                        reloc_ref_in_shortened_obj ((saved_plug_info_start + i), (saved_info_to_relocate + i));
-                    }
-                }
-            }
-            break;
-        }
-        size_t s = size (x);
-        uint8_t* next_obj = x + Align (s);
-        Prefetch (next_obj);
-        if (next_obj >= plug_end)
-        {
-            dprintf (3, ("object %p is at the end of the plug %p->%p",
-                next_obj, plug, plug_end));
-            verify_pins_with_post_plug_info("before reloc short obj");
-            relocate_shortened_obj_helper (x, s, (x + Align (s) - sizeof (plug_and_gap)), pinned_plug_entry, is_pinned);
-        }
-        else
-        {
-            relocate_obj_helper (x, s);
-        }
-        assert (s > 0);
-        x = next_obj;
-    }
-    verify_pins_with_post_plug_info("end reloc short surv");
-}
-void gc_heap::relocate_survivors_in_plug (uint8_t* plug, uint8_t* plug_end,
-                                          BOOL check_last_object_p,
-                                          mark* pinned_plug_entry)
-{
-    dprintf (3,("RP: [%zx(%zx->%zx),%zx(%zx->%zx)[",
-        (size_t)plug, brick_of (plug), (size_t)brick_table[brick_of (plug)],
-        (size_t)plug_end, brick_of (plug_end), (size_t)brick_table[brick_of (plug_end)]));
-    if (check_last_object_p)
-    {
-        relocate_shortened_survivor_helper (plug, plug_end, pinned_plug_entry);
-    }
-    else
-    {
-        relocate_survivor_helper (plug, plug_end);
-    }
-}
-void gc_heap::relocate_survivors_in_brick (uint8_t* tree, relocate_args* args)
-{
-    assert ((tree != NULL));
-    dprintf (3, ("tree: %p, args->last_plug: %p, left: %p, right: %p, gap(t): %zx",
-        tree, args->last_plug,
-        (tree + node_left_child (tree)),
-        (tree + node_right_child (tree)),
-        node_gap_size (tree)));
-    if (node_left_child (tree))
-    {
-        relocate_survivors_in_brick (tree + node_left_child (tree), args);
-    }
-    {
-        uint8_t*  plug = tree;
-        BOOL   has_post_plug_info_p = FALSE;
-        BOOL   has_pre_plug_info_p = FALSE;
-        if (tree == oldest_pinned_plug)
-        {
-            args->pinned_plug_entry = get_oldest_pinned_entry (&has_pre_plug_info_p,
-                                                               &has_post_plug_info_p);
-            assert (tree == pinned_plug (args->pinned_plug_entry));
-            dprintf (3, ("tree is the oldest pin: %p", tree));
-        }
-        if (args->last_plug)
-        {
-            size_t  gap_size = node_gap_size (tree);
-            uint8_t*  gap = (plug - gap_size);
-            dprintf (3, ("tree: %p, gap: %p (%zx)", tree, gap, gap_size));
-            assert (gap_size >= Align (min_obj_size));
-            uint8_t*  last_plug_end = gap;
-            BOOL check_last_object_p = (args->is_shortened || has_pre_plug_info_p);
-            {
-                relocate_survivors_in_plug (args->last_plug, last_plug_end, check_last_object_p, args->pinned_plug_entry);
-            }
-        }
-        else
-        {
-            assert (!has_pre_plug_info_p);
-        }
-        args->last_plug = plug;
-        args->is_shortened = has_post_plug_info_p;
-        if (has_post_plug_info_p)
-        {
-            dprintf (3, ("setting %p as shortened", plug));
-        }
-        dprintf (3, ("last_plug: %p(shortened: %d)", plug, (args->is_shortened ? 1 : 0)));
-    }
-    if (node_right_child (tree))
-    {
-        relocate_survivors_in_brick (tree + node_right_child (tree), args);
-    }
-}
-inline
-void gc_heap::update_oldest_pinned_plug()
-{
-    oldest_pinned_plug = (pinned_plug_que_empty_p() ? 0 : pinned_plug (oldest_pin()));
-}
-heap_segment* gc_heap::get_start_segment (generation* gen)
-{
-    heap_segment* start_heap_segment = heap_segment_rw (generation_start_segment (gen));
-#ifdef USE_REGIONS
-    heap_segment* current_heap_segment = heap_segment_non_sip (start_heap_segment);
-    if (current_heap_segment != start_heap_segment)
-    {
-        dprintf (REGIONS_LOG, ("h%d skipped gen%d SIP regions, start %p->%p",
-            heap_number,
-            (current_heap_segment ? heap_segment_gen_num (current_heap_segment) : -1),
-            heap_segment_mem (start_heap_segment),
-            (current_heap_segment ? heap_segment_mem (current_heap_segment) : 0)));
-    }
-    start_heap_segment = current_heap_segment;
-#endif //USE_REGIONS
-    return start_heap_segment;
-}
-void gc_heap::relocate_survivors (int condemned_gen_number,
-                                  uint8_t* first_condemned_address)
-{
-    reset_pinned_queue_bos();
-    update_oldest_pinned_plug();
-    int stop_gen_idx = get_stop_generation_index (condemned_gen_number);
-#ifndef USE_REGIONS
-    assert (first_condemned_address == generation_allocation_start (generation_of (condemned_gen_number)));
-#endif //!USE_REGIONS
-    for (int i = condemned_gen_number; i >= stop_gen_idx; i--)
-    {
-        generation* condemned_gen = generation_of (i);
-        heap_segment* current_heap_segment = heap_segment_rw (generation_start_segment (condemned_gen));
-#ifdef USE_REGIONS
-        current_heap_segment = relocate_advance_to_non_sip (current_heap_segment);
-        if (!current_heap_segment)
-            continue;
-#endif //USE_REGIONS
-        uint8_t*  start_address = get_soh_start_object (current_heap_segment, condemned_gen);
-        size_t  current_brick = brick_of (start_address);
-        PREFIX_ASSUME(current_heap_segment != NULL);
-        uint8_t*  end_address = heap_segment_allocated (current_heap_segment);
-        size_t  end_brick = brick_of (end_address - 1);
-        relocate_args args;
-        args.is_shortened = FALSE;
-        args.pinned_plug_entry = 0;
-        args.last_plug = 0;
-        while (1)
-        {
-            if (current_brick > end_brick)
-            {
-                if (args.last_plug)
-                {
-                    {
-                        assert (!(args.is_shortened));
-                        relocate_survivors_in_plug (args.last_plug,
-                                                    heap_segment_allocated (current_heap_segment),
-                                                    args.is_shortened,
-                                                    args.pinned_plug_entry);
-                    }
-                    args.last_plug = 0;
-                }
-                heap_segment* next_heap_segment = heap_segment_next (current_heap_segment);
-                if (next_heap_segment)
-                {
-#ifdef USE_REGIONS
-                    next_heap_segment = relocate_advance_to_non_sip (next_heap_segment);
-#endif //USE_REGIONS
-                    if (next_heap_segment)
-                    {
-                        current_heap_segment = next_heap_segment;
-                        current_brick = brick_of (heap_segment_mem (current_heap_segment));
-                        end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
-                        continue;
-                    }
-                    else
-                        break;
-                }
-                else
-                {
-                    break;
-                }
-            }
-            {
-                int brick_entry =  brick_table [ current_brick ];
-                if (brick_entry >= 0)
-                {
-                    relocate_survivors_in_brick (brick_address (current_brick) +
-                                                brick_entry -1,
-                                                &args);
-                }
-            }
-            current_brick++;
-        }
-    }
-}
-void gc_heap::walk_plug (uint8_t* plug, size_t size, BOOL check_last_object_p, walk_relocate_args* args)
-{
-    if (check_last_object_p)
-    {
-        size += sizeof (gap_reloc_pair);
-        mark* entry = args->pinned_plug_entry;
-        if (args->is_shortened)
-        {
-            assert (entry->has_post_plug_info());
-            entry->swap_post_plug_and_saved_for_profiler();
-        }
-        else
-        {
-            assert (entry->has_pre_plug_info());
-            entry->swap_pre_plug_and_saved_for_profiler();
-        }
-    }
-    ptrdiff_t last_plug_relocation = node_relocation_distance (plug);
-    STRESS_LOG_PLUG_MOVE(plug, (plug + size), -last_plug_relocation);
-    ptrdiff_t reloc = settings.compaction ? last_plug_relocation : 0;
-    (args->fn) (plug, (plug + size), reloc, args->profiling_context, !!settings.compaction, false);
-    if (check_last_object_p)
-    {
-        mark* entry = args->pinned_plug_entry;
-        if (args->is_shortened)
-        {
-            entry->swap_post_plug_and_saved_for_profiler();
-        }
-        else
-        {
-            entry->swap_pre_plug_and_saved_for_profiler();
-        }
-    }
-}
-void gc_heap::walk_relocation_in_brick (uint8_t* tree, walk_relocate_args* args)
-{
-    assert ((tree != NULL));
-    if (node_left_child (tree))
-    {
-        walk_relocation_in_brick (tree + node_left_child (tree), args);
-    }
-    uint8_t*  plug = tree;
-    BOOL   has_pre_plug_info_p = FALSE;
-    BOOL   has_post_plug_info_p = FALSE;
-    if (tree == oldest_pinned_plug)
-    {
-        args->pinned_plug_entry = get_oldest_pinned_entry (&has_pre_plug_info_p,
-                                                           &has_post_plug_info_p);
-        assert (tree == pinned_plug (args->pinned_plug_entry));
-    }
-    if (args->last_plug != 0)
-    {
-        size_t gap_size = node_gap_size (tree);
-        uint8_t*  gap = (plug - gap_size);
-        uint8_t*  last_plug_end = gap;
-        size_t last_plug_size = (last_plug_end - args->last_plug);
-        dprintf (3, ("tree: %p, last_plug: %p, gap: %p(%zx), last_plug_end: %p, size: %zx",
-            tree, args->last_plug, gap, gap_size, last_plug_end, last_plug_size));
-        BOOL check_last_object_p = (args->is_shortened || has_pre_plug_info_p);
-        if (!check_last_object_p)
-        {
-            assert (last_plug_size >= Align (min_obj_size));
-        }
-        walk_plug (args->last_plug, last_plug_size, check_last_object_p, args);
-    }
-    else
-    {
-        assert (!has_pre_plug_info_p);
-    }
-    dprintf (3, ("set args last plug to plug: %p", plug));
-    args->last_plug = plug;
-    args->is_shortened = has_post_plug_info_p;
-    if (node_right_child (tree))
-    {
-        walk_relocation_in_brick (tree + node_right_child (tree), args);
-    }
-}
-void gc_heap::walk_relocation (void* profiling_context, record_surv_fn fn)
-{
-    int condemned_gen_number = settings.condemned_generation;
-    int stop_gen_idx = get_stop_generation_index (condemned_gen_number);
-    reset_pinned_queue_bos();
-    update_oldest_pinned_plug();
-    for (int i = condemned_gen_number; i >= stop_gen_idx; i--)
-    {
-        generation* condemned_gen = generation_of (i);
-        heap_segment*  current_heap_segment = heap_segment_rw (generation_start_segment (condemned_gen));
-#ifdef USE_REGIONS
-        current_heap_segment = walk_relocation_sip (current_heap_segment, profiling_context, fn);
-        if (!current_heap_segment)
-            continue;
-#endif // USE_REGIONS
-        uint8_t*  start_address = get_soh_start_object (current_heap_segment, condemned_gen);
-        size_t  current_brick = brick_of (start_address);
-        PREFIX_ASSUME(current_heap_segment != NULL);
-        size_t end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
-        walk_relocate_args args;
-        args.is_shortened = FALSE;
-        args.pinned_plug_entry = 0;
-        args.last_plug = 0;
-        args.profiling_context = profiling_context;
-        args.fn = fn;
-        while (1)
-        {
-            if (current_brick > end_brick)
-            {
-                if (args.last_plug)
-                {
-                    walk_plug (args.last_plug,
-                            (heap_segment_allocated (current_heap_segment) - args.last_plug),
-                            args.is_shortened,
-                            &args);
-                    args.last_plug = 0;
-                }
-                current_heap_segment = heap_segment_next_rw (current_heap_segment);
-#ifdef USE_REGIONS
-                current_heap_segment = walk_relocation_sip (current_heap_segment, profiling_context, fn);
-#endif // USE_REGIONS
-                if (current_heap_segment)
-                {
-                    current_brick = brick_of (heap_segment_mem (current_heap_segment));
-                    end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
-                    continue;
-                }
-                else
-                {
-                    break;
-                }
-            }
-            {
-                int brick_entry =  brick_table [ current_brick ];
-                if (brick_entry >= 0)
-                {
-                    walk_relocation_in_brick (brick_address (current_brick) +
-                                            brick_entry - 1,
-                                            &args);
-                }
-            }
-            current_brick++;
-        }
-    }
-}
-#ifdef USE_REGIONS
-heap_segment* gc_heap::walk_relocation_sip (heap_segment* current_heap_segment, void* profiling_context, record_surv_fn fn)
-{
-    while (current_heap_segment && heap_segment_swept_in_plan (current_heap_segment))
-    {
-        uint8_t* start = heap_segment_mem (current_heap_segment);
-        uint8_t* end = heap_segment_allocated (current_heap_segment);
-        uint8_t* obj = start;
-        uint8_t* plug_start = nullptr;
-        while (obj < end)
-        {
-            if (((CObjectHeader*)obj)->IsFree())
-            {
-                if (plug_start)
-                {
-                    fn (plug_start, obj, 0, profiling_context, false, false);
-                    plug_start = nullptr;
-                }
-            }
-            else
-            {
-                if (!plug_start)
-                {
-                    plug_start = obj;
-                }
-            }
-            obj += Align (size (obj));
-        }
-        if (plug_start)
-        {
-            fn (plug_start, end, 0, profiling_context, false, false);
-        }
-        current_heap_segment = heap_segment_next_rw (current_heap_segment);
-    }
-    return current_heap_segment;
-}
-#endif // USE_REGIONS
-void gc_heap::walk_survivors (record_surv_fn fn, void* context, walk_surv_type type)
-{
-    if (type == walk_for_gc)
-        walk_survivors_relocation (context, fn);
-#if defined(BACKGROUND_GC) && defined(FEATURE_EVENT_TRACE)
-    else if (type == walk_for_bgc)
-        walk_survivors_for_bgc (context, fn);
-#endif //BACKGROUND_GC && FEATURE_EVENT_TRACE
-    else
-        assert (!"unknown type!");
-}
-#if defined(BACKGROUND_GC) && defined(FEATURE_EVENT_TRACE)
-void gc_heap::walk_survivors_for_bgc (void* profiling_context, record_surv_fn fn)
-{
-    assert(settings.concurrent);
-    for (int i = get_start_generation_index(); i < total_generation_count; i++)
-    {
-        int align_const = get_alignment_constant (i == max_generation);
-        heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (i)));
-        while (seg)
-        {
-            uint8_t* o = heap_segment_mem (seg);
-            uint8_t* end = heap_segment_allocated (seg);
-            while (o < end)
-            {
-                if (method_table(o) == g_gc_pFreeObjectMethodTable)
-                {
-                    o += Align (size (o), align_const);
-                    continue;
-                }
-                uint8_t* plug_start = o;
-                while (method_table(o) != g_gc_pFreeObjectMethodTable)
-                {
-                    o += Align (size (o), align_const);
-                    if (o >= end)
-                    {
-                        break;
-                    }
-                }
-                uint8_t* plug_end = o;
-                fn (plug_start,
-                    plug_end,
-                    0,              // Reloc distance == 0 as this is non-compacting
-                    profiling_context,
-                    false,          // Non-compacting
-                    true);          // BGC
-            }
-            seg = heap_segment_next (seg);
-        }
-    }
-}
-#endif //BACKGROUND_GC && FEATURE_EVENT_TRACE
-void gc_heap::relocate_phase (int condemned_gen_number,
-                              uint8_t* first_condemned_address)
-{
-    ScanContext sc;
-    sc.thread_number = heap_number;
-    sc.thread_count = n_heaps;
-    sc.promotion = FALSE;
-    sc.concurrent = FALSE;
-#ifdef MULTIPLE_HEAPS
-    dprintf(3, ("Joining after end of plan"));
-    gc_t_join.join(this, gc_join_begin_relocate_phase);
-    if (gc_t_join.joined())
-    {
-#endif //MULTIPLE_HEAPS
-#ifdef FEATURE_EVENT_TRACE
-        if (informational_event_enabled_p)
-        {
-            gc_time_info[time_relocate] = GetHighPrecisionTimeStamp();
-        }
-#endif //FEATURE_EVENT_TRACE
-#ifdef USE_REGIONS
-        verify_region_to_generation_map();
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-        dprintf(3, ("Restarting for relocation"));
-        gc_t_join.restart();
-    }
-#endif //MULTIPLE_HEAPS
-    dprintf (2, (ThreadStressLog::gcStartRelocateMsg(), heap_number));
-    dprintf(3,("Relocating roots"));
-    GCScan::GcScanRoots(GCHeap::Relocate,
-                            condemned_gen_number, max_generation, &sc);
-    verify_pins_with_post_plug_info("after reloc stack");
-#ifdef BACKGROUND_GC
-    if (gc_heap::background_running_p())
-    {
-        scan_background_roots (GCHeap::Relocate, heap_number, &sc);
-    }
-#endif //BACKGROUND_GC
-#ifdef FEATURE_CARD_MARKING_STEALING
-    {
-        dprintf(3, ("Relocating survivors"));
-        relocate_survivors(condemned_gen_number,
-            first_condemned_address);
-    }
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-    dprintf(3, ("Relocating finalization data"));
-    finalize_queue->RelocateFinalizationData(condemned_gen_number,
-        __this);
-#endif // FEATURE_PREMORTEM_FINALIZATION
-    {
-        dprintf(3, ("Relocating handle table"));
-        GCScan::GcScanHandles(GCHeap::Relocate,
-            condemned_gen_number, max_generation, &sc);
-    }
-#endif // FEATURE_CARD_MARKING_STEALING
-    if (condemned_gen_number != max_generation)
-    {
-#if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
-        if (!card_mark_done_soh)
-#endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
-        {
-            dprintf (3, ("Relocating cross generation pointers on heap %d", heap_number));
-            mark_through_cards_for_segments(&gc_heap::relocate_address, TRUE THIS_ARG);
-            verify_pins_with_post_plug_info("after reloc cards");
-#if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
-            card_mark_done_soh = true;
-#endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
-        }
-    }
-    if (condemned_gen_number != max_generation)
-    {
-#if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
-        if (!card_mark_done_uoh)
-#endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
-        {
-            dprintf (3, ("Relocating cross generation pointers for uoh objects on heap %d", heap_number));
-            for (int i = uoh_start_generation; i < total_generation_count; i++)
-            {
-#ifndef ALLOW_REFERENCES_IN_POH
-                if (i != poh_generation)
-#endif //ALLOW_REFERENCES_IN_POH
-                    mark_through_cards_for_uoh_objects(&gc_heap::relocate_address, i, TRUE THIS_ARG);
-            }
-#if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
-            card_mark_done_uoh = true;
-#endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
-        }
-    }
-    else
-    {
-#ifdef FEATURE_LOH_COMPACTION
-        if (loh_compacted_p)
-        {
-            assert (settings.condemned_generation == max_generation);
-            relocate_in_loh_compact();
-        }
-        else
-#endif //FEATURE_LOH_COMPACTION
-        {
-            relocate_in_uoh_objects (loh_generation);
-        }
-#ifdef ALLOW_REFERENCES_IN_POH
-        relocate_in_uoh_objects (poh_generation);
-#endif
-    }
-#ifndef FEATURE_CARD_MARKING_STEALING
-    {
-        dprintf(3,("Relocating survivors"));
-        relocate_survivors (condemned_gen_number,
-                            first_condemned_address);
-    }
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-        dprintf(3,("Relocating finalization data"));
-        finalize_queue->RelocateFinalizationData (condemned_gen_number,
-                                                       __this);
-#endif // FEATURE_PREMORTEM_FINALIZATION
-    {
-        dprintf(3,("Relocating handle table"));
-        GCScan::GcScanHandles(GCHeap::Relocate,
-                                  condemned_gen_number, max_generation, &sc);
-    }
-#endif // !FEATURE_CARD_MARKING_STEALING
-#if defined(MULTIPLE_HEAPS) && defined(FEATURE_CARD_MARKING_STEALING)
-    if (condemned_gen_number != max_generation)
-    {
-        for (int i = 0; i < gc_heap::n_heaps; i++)
-        {
-            int heap_number_to_look_at = (i + heap_number) % gc_heap::n_heaps;
-            gc_heap* hp = gc_heap::g_heaps[heap_number_to_look_at];
-            if (!hp->card_mark_done_soh)
-            {
-                dprintf(3, ("Relocating cross generation pointers on heap %d", hp->heap_number));
-                hp->mark_through_cards_for_segments(&gc_heap::relocate_address, TRUE THIS_ARG);
-                hp->card_mark_done_soh = true;
-            }
-            if (!hp->card_mark_done_uoh)
-            {
-                dprintf(3, ("Relocating cross generation pointers for uoh objects on heap %d", hp->heap_number));
-                for (int i = uoh_start_generation; i < total_generation_count; i++)
-                {
-#ifndef ALLOW_REFERENCES_IN_POH
-                    if (i != poh_generation)
-#endif //ALLOW_REFERENCES_IN_POH
-                        hp->mark_through_cards_for_uoh_objects(&gc_heap::relocate_address, i, TRUE THIS_ARG);
-                }
-                hp->card_mark_done_uoh = true;
-            }
-        }
-    }
-#endif // MULTIPLE_HEAPS && FEATURE_CARD_MARKING_STEALING
-    dprintf(2, (ThreadStressLog::gcEndRelocateMsg(), heap_number));
-}
-mark* gc_heap::get_next_pinned_entry (uint8_t* tree,
-                                      BOOL* has_pre_plug_info_p,
-                                      BOOL* has_post_plug_info_p,
-                                      BOOL deque_p)
-{
-    if (!pinned_plug_que_empty_p())
-    {
-        mark* oldest_entry = oldest_pin();
-        uint8_t* oldest_plug = pinned_plug (oldest_entry);
-        if (tree == oldest_plug)
-        {
-            *has_pre_plug_info_p =  oldest_entry->has_pre_plug_info();
-            *has_post_plug_info_p = oldest_entry->has_post_plug_info();
-            if (deque_p)
-            {
-                deque_pinned_plug();
-            }
-            dprintf (3, ("found a pinned plug %p, pre: %d, post: %d",
-                tree,
-                (*has_pre_plug_info_p ? 1 : 0),
-                (*has_post_plug_info_p ? 1 : 0)));
-            return oldest_entry;
-        }
-    }
-    return NULL;
-}
-mark* gc_heap::get_oldest_pinned_entry (BOOL* has_pre_plug_info_p,
-                                        BOOL* has_post_plug_info_p)
-{
-    mark* oldest_entry = oldest_pin();
-    *has_pre_plug_info_p =  oldest_entry->has_pre_plug_info();
-    *has_post_plug_info_p = oldest_entry->has_post_plug_info();
-    deque_pinned_plug();
-    update_oldest_pinned_plug();
-    return oldest_entry;
-}
-inline
-void gc_heap::copy_cards_range (uint8_t* dest, uint8_t* src, size_t len, BOOL copy_cards_p)
-{
-    if (copy_cards_p)
-        copy_cards_for_addresses (dest, src, len);
-    else
-        clear_card_for_addresses (dest, dest + len);
-}
-inline
-void  gc_heap::gcmemcopy (uint8_t* dest, uint8_t* src, size_t len, BOOL copy_cards_p)
-{
-    if (dest != src)
-    {
-#ifdef BACKGROUND_GC
-        if (current_c_gc_state == c_gc_state_marking)
-        {
-            copy_mark_bits_for_addresses (dest, src, len);
-        }
-#endif //BACKGROUND_GC
-#ifdef DOUBLY_LINKED_FL
-        BOOL set_bgc_mark_bits_p = is_plug_bgc_mark_bit_set (src);
-        if (set_bgc_mark_bits_p)
-        {
-            clear_plug_bgc_mark_bit (src);
-        }
-        BOOL make_free_obj_p = FALSE;
-        if (len <= min_free_item_no_prev)
-        {
-            make_free_obj_p = is_free_obj_in_compact_bit_set (src);
-            if (make_free_obj_p)
-            {
-                clear_free_obj_in_compact_bit (src);
-            }
-        }
-#endif //DOUBLY_LINKED_FL
-        dprintf(3,(ThreadStressLog::gcMemCopyMsg(), (size_t)src, (size_t)dest, (size_t)src+len, (size_t)dest+len));
-        memcopy (dest - plug_skew, src - plug_skew, len);
-#ifdef DOUBLY_LINKED_FL
-        if (set_bgc_mark_bits_p)
-        {
-            uint8_t* dest_o = dest;
-            uint8_t* dest_end_o = dest + len;
-            while (dest_o < dest_end_o)
-            {
-                uint8_t* next_o = dest_o + Align (size (dest_o));
-                background_mark (dest_o, background_saved_lowest_address, background_saved_highest_address);
-                dest_o = next_o;
-            }
-            dprintf (3333, ("[h%d] GM: %p(%zx-%zx)->%p(%zx-%zx)",
-                heap_number, dest,
-                (size_t)(&mark_array [mark_word_of (dest)]),
-                (size_t)(mark_array [mark_word_of (dest)]),
-                dest_end_o,
-                (size_t)(&mark_array [mark_word_of (dest_o)]),
-                (size_t)(mark_array [mark_word_of (dest_o)])));
-        }
-        if (make_free_obj_p)
-        {
-            size_t* filler_free_obj_size_location = (size_t*)(dest + min_free_item_no_prev);
-            size_t filler_free_obj_size = *filler_free_obj_size_location;
-            make_unused_array ((dest + len), filler_free_obj_size);
-            dprintf (3333, ("[h%d] smallobj, %p(%zd): %p->%p", heap_number,
-                filler_free_obj_size_location, filler_free_obj_size, (dest + len), (dest + len + filler_free_obj_size)));
-        }
-#endif //DOUBLY_LINKED_FL
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-        if (SoftwareWriteWatch::IsEnabledForGCHeap())
-        {
-            SoftwareWriteWatch::SetDirtyRegion(dest, len - plug_skew);
-        }
-#endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-        copy_cards_range (dest, src, len, copy_cards_p);
-    }
-}
-void gc_heap::compact_plug (uint8_t* plug, size_t size, BOOL check_last_object_p, compact_args* args)
-{
-    args->print();
-    uint8_t* reloc_plug = plug + args->last_plug_relocation;
-    if (check_last_object_p)
-    {
-        size += sizeof (gap_reloc_pair);
-        mark* entry = args->pinned_plug_entry;
-        if (args->is_shortened)
-        {
-            assert (entry->has_post_plug_info());
-            entry->swap_post_plug_and_saved();
-        }
-        else
-        {
-            assert (entry->has_pre_plug_info());
-            entry->swap_pre_plug_and_saved();
-        }
-    }
-    int  old_brick_entry =  brick_table [brick_of (plug)];
-    assert (node_relocation_distance (plug) == args->last_plug_relocation);
-#ifdef FEATURE_STRUCTALIGN
-    ptrdiff_t alignpad = node_alignpad(plug);
-    if (alignpad)
-    {
-        make_unused_array (reloc_plug - alignpad, alignpad);
-        if (brick_of (reloc_plug - alignpad) != brick_of (reloc_plug))
-        {
-            fix_brick_to_highest (reloc_plug - alignpad, reloc_plug);
-        }
-    }
-#else // FEATURE_STRUCTALIGN
-    size_t unused_arr_size = 0;
-    BOOL  already_padded_p = FALSE;
-#ifdef SHORT_PLUGS
-    if (is_plug_padded (plug))
-    {
-        already_padded_p = TRUE;
-        clear_plug_padded (plug);
-        unused_arr_size = Align (min_obj_size);
-    }
-#endif //SHORT_PLUGS
-    if (node_realigned (plug))
-    {
-        unused_arr_size += switch_alignment_size (already_padded_p);
-    }
-    if (unused_arr_size != 0)
-    {
-        make_unused_array (reloc_plug - unused_arr_size, unused_arr_size);
-        if (brick_of (reloc_plug - unused_arr_size) != brick_of (reloc_plug))
-        {
-            dprintf (3, ("fix B for padding: %zd: %p->%p",
-                unused_arr_size, (reloc_plug - unused_arr_size), reloc_plug));
-            fix_brick_to_highest (reloc_plug - unused_arr_size, reloc_plug);
-        }
-    }
-#endif // FEATURE_STRUCTALIGN
-#ifdef SHORT_PLUGS
-    if (is_plug_padded (plug))
-    {
-        make_unused_array (reloc_plug - Align (min_obj_size), Align (min_obj_size));
-        if (brick_of (reloc_plug - Align (min_obj_size)) != brick_of (reloc_plug))
-        {
-            fix_brick_to_highest (reloc_plug - Align (min_obj_size), reloc_plug);
-        }
-    }
-#endif //SHORT_PLUGS
-    gcmemcopy (reloc_plug, plug, size, args->copy_cards_p);
-    if (args->check_gennum_p)
-    {
-        int src_gennum = args->src_gennum;
-        if (src_gennum == -1)
-        {
-            src_gennum = object_gennum (plug);
-        }
-        int dest_gennum = object_gennum_plan (reloc_plug);
-        if (src_gennum < dest_gennum)
-        {
-            generation_allocation_size (generation_of (dest_gennum)) += size;
-        }
-    }
-    size_t current_reloc_brick = args->current_compacted_brick;
-    if (brick_of (reloc_plug) != current_reloc_brick)
-    {
-        dprintf (3, ("last reloc B: %zx, current reloc B: %zx",
-            current_reloc_brick, brick_of (reloc_plug)));
-        if (args->before_last_plug)
-        {
-            dprintf (3,(" fixing last brick %zx to point to last plug %p(%zx)",
-                     current_reloc_brick,
-                     args->before_last_plug,
-                     (args->before_last_plug - brick_address (current_reloc_brick))));
-            {
-                set_brick (current_reloc_brick,
-                        args->before_last_plug - brick_address (current_reloc_brick));
-            }
-        }
-        current_reloc_brick = brick_of (reloc_plug);
-    }
-    size_t end_brick = brick_of (reloc_plug + size-1);
-    if (end_brick != current_reloc_brick)
-    {
-        dprintf (3,("plug spanning multiple bricks, fixing first brick %zx to %zx(%zx)",
-                 current_reloc_brick, (size_t)reloc_plug,
-                 (reloc_plug - brick_address (current_reloc_brick))));
-        {
-            set_brick (current_reloc_brick,
-                    reloc_plug - brick_address (current_reloc_brick));
-        }
-        size_t brick = current_reloc_brick + 1;
-        dprintf (3,("setting intervening bricks %zu->%zu to -1",
-            brick, (end_brick - 1)));
-        while (brick < end_brick)
-        {
-            set_brick (brick, -1);
-            brick++;
-        }
-        args->before_last_plug = brick_address (end_brick) -1;
-        current_reloc_brick = end_brick;
-        dprintf (3, ("setting before last to %p, last brick to %zx",
-            args->before_last_plug, current_reloc_brick));
-    }
-    else
-    {
-        dprintf (3, ("still in the same brick: %zx", end_brick));
-        args->before_last_plug = reloc_plug;
-    }
-    args->current_compacted_brick = current_reloc_brick;
-    if (check_last_object_p)
-    {
-        mark* entry = args->pinned_plug_entry;
-        if (args->is_shortened)
-        {
-            entry->swap_post_plug_and_saved();
-        }
-        else
-        {
-            entry->swap_pre_plug_and_saved();
-        }
-    }
-}
-void gc_heap::compact_in_brick (uint8_t* tree, compact_args* args)
-{
-    assert (tree != NULL);
-    int   left_node = node_left_child (tree);
-    int   right_node = node_right_child (tree);
-    ptrdiff_t relocation = node_relocation_distance (tree);
-    args->print();
-    if (left_node)
-    {
-        dprintf (3, ("B: L: %d->%p", left_node, (tree + left_node)));
-        compact_in_brick ((tree + left_node), args);
-    }
-    uint8_t*  plug = tree;
-    BOOL   has_pre_plug_info_p = FALSE;
-    BOOL   has_post_plug_info_p = FALSE;
-    if (tree == oldest_pinned_plug)
-    {
-        args->pinned_plug_entry = get_oldest_pinned_entry (&has_pre_plug_info_p,
-                                                           &has_post_plug_info_p);
-        assert (tree == pinned_plug (args->pinned_plug_entry));
-    }
-    if (args->last_plug != 0)
-    {
-        size_t gap_size = node_gap_size (tree);
-        uint8_t*  gap = (plug - gap_size);
-        uint8_t*  last_plug_end = gap;
-        size_t last_plug_size = (last_plug_end - args->last_plug);
-        assert ((last_plug_size & (sizeof(PTR_PTR) - 1)) == 0);
-        dprintf (3, ("tree: %p, last_plug: %p, gap: %p(%zx), last_plug_end: %p, size: %zx",
-            tree, args->last_plug, gap, gap_size, last_plug_end, last_plug_size));
-        BOOL check_last_object_p = (args->is_shortened || has_pre_plug_info_p);
-        if (!check_last_object_p)
-        {
-            assert (last_plug_size >= Align (min_obj_size));
-        }
-        compact_plug (args->last_plug, last_plug_size, check_last_object_p, args);
-    }
-    else
-    {
-        assert (!has_pre_plug_info_p);
-    }
-    dprintf (3, ("set args last plug to plug: %p, reloc: %zx", plug, relocation));
-    args->last_plug = plug;
-    args->last_plug_relocation = relocation;
-    args->is_shortened = has_post_plug_info_p;
-    if (right_node)
-    {
-        dprintf (3, ("B: R: %d->%p", right_node, (tree + right_node)));
-        compact_in_brick ((tree + right_node), args);
-    }
-}
-size_t gc_heap::recover_saved_pinned_info()
-{
-    reset_pinned_queue_bos();
-    size_t total_recovered_sweep_size = 0;
-    while (!(pinned_plug_que_empty_p()))
-    {
-        mark* oldest_entry = oldest_pin();
-        size_t recovered_sweep_size = oldest_entry->recover_plug_info();
-        if (recovered_sweep_size > 0)
-        {
-            uint8_t* plug = pinned_plug (oldest_entry);
-            if (object_gennum (plug) == max_generation)
-            {
-                dprintf (3, ("recovered %p(%zd) from pin", plug, recovered_sweep_size));
-                total_recovered_sweep_size += recovered_sweep_size;
-            }
-        }
-#ifdef GC_CONFIG_DRIVEN
-        if (oldest_entry->has_pre_plug_info() && oldest_entry->has_post_plug_info())
-            record_interesting_data_point (idp_pre_and_post_pin);
-        else if (oldest_entry->has_pre_plug_info())
-            record_interesting_data_point (idp_pre_pin);
-        else if (oldest_entry->has_post_plug_info())
-            record_interesting_data_point (idp_post_pin);
-#endif //GC_CONFIG_DRIVEN
-        deque_pinned_plug();
-    }
-    return total_recovered_sweep_size;
-}
-void gc_heap::compact_phase (int condemned_gen_number,
-                             uint8_t*  first_condemned_address,
-                             BOOL clear_cards)
-{
-#ifdef MULTIPLE_HEAPS
-    dprintf(3, ("Joining after end of relocation"));
-    gc_t_join.join(this, gc_join_relocate_phase_done);
-    if (gc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-#ifdef FEATURE_EVENT_TRACE
-        if (informational_event_enabled_p)
-        {
-            gc_time_info[time_compact] = GetHighPrecisionTimeStamp();
-            gc_time_info[time_relocate] = gc_time_info[time_compact] - gc_time_info[time_relocate];
-        }
-#endif //FEATURE_EVENT_TRACE
-#ifdef MULTIPLE_HEAPS
-        dprintf(3, ("Restarting for compaction"));
-        gc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-    dprintf (2, (ThreadStressLog::gcStartCompactMsg(), heap_number,
-        first_condemned_address, brick_of (first_condemned_address)));
-#ifdef FEATURE_LOH_COMPACTION
-    if (loh_compacted_p)
-    {
-        compact_loh();
-    }
-#endif //FEATURE_LOH_COMPACTION
-    reset_pinned_queue_bos();
-    update_oldest_pinned_plug();
-    BOOL reused_seg = expand_reused_seg_p();
-    if (reused_seg)
-    {
-        for (int i = 1; i <= max_generation; i++)
-        {
-            generation_allocation_size (generation_of (i)) = 0;
-        }
-    }
-    int stop_gen_idx = get_stop_generation_index (condemned_gen_number);
-    for (int i = condemned_gen_number; i >= stop_gen_idx; i--)
-    {
-        generation* condemned_gen = generation_of (i);
-        heap_segment* current_heap_segment = get_start_segment (condemned_gen);
-#ifdef USE_REGIONS
-        if (!current_heap_segment)
-            continue;
-        size_t   current_brick = brick_of (heap_segment_mem (current_heap_segment));
-#else
-        size_t   current_brick = brick_of (first_condemned_address);
-#endif //USE_REGIONS
-        uint8_t*  end_address = heap_segment_allocated (current_heap_segment);
-#ifndef USE_REGIONS
-        if ((first_condemned_address >= end_address) && (condemned_gen_number < max_generation))
-        {
-            return;
-        }
-#endif //!USE_REGIONS
-        size_t  end_brick = brick_of (end_address-1);
-        compact_args args;
-        args.last_plug = 0;
-        args.before_last_plug = 0;
-        args.current_compacted_brick = ~((size_t)1);
-        args.is_shortened = FALSE;
-        args.pinned_plug_entry = 0;
-        args.copy_cards_p =  (condemned_gen_number >= 1) || !clear_cards;
-        args.check_gennum_p = reused_seg;
-        if (args.check_gennum_p)
-        {
-            args.src_gennum = ((current_heap_segment == ephemeral_heap_segment) ? -1 : 2);
-        }
-#ifdef USE_REGIONS
-        assert (!args.check_gennum_p);
-#endif //USE_REGIONS
-        while (1)
-        {
-            if (current_brick > end_brick)
-            {
-                if (args.last_plug != 0)
-                {
-                    dprintf (3, ("compacting last plug: %p", args.last_plug))
-                    compact_plug (args.last_plug,
-                                  (heap_segment_allocated (current_heap_segment) - args.last_plug),
-                                  args.is_shortened,
-                                  &args);
-                }
-                heap_segment* next_heap_segment = heap_segment_next_non_sip (current_heap_segment);
-                if (next_heap_segment)
-                {
-                    current_heap_segment = next_heap_segment;
-                    current_brick = brick_of (heap_segment_mem (current_heap_segment));
-                    end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
-                    args.last_plug = 0;
-                    if (args.check_gennum_p)
-                    {
-                        args.src_gennum = ((current_heap_segment == ephemeral_heap_segment) ? -1 : 2);
-                    }
-                    continue;
-                }
-                else
-                {
-                    if (args.before_last_plug !=0)
-                    {
-                        dprintf (3, ("Fixing last brick %zx to point to plug %zx",
-                                    args.current_compacted_brick, (size_t)args.before_last_plug));
-                        assert (args.current_compacted_brick != ~1u);
-                        set_brick (args.current_compacted_brick,
-                                   args.before_last_plug - brick_address (args.current_compacted_brick));
-                    }
-                    break;
-                }
-            }
-            {
-                int  brick_entry =  brick_table [ current_brick ];
-                dprintf (3, ("B: %zx(%zx)->%p",
-                    current_brick, (size_t)brick_entry, (brick_address (current_brick) + brick_entry - 1)));
-                if (brick_entry >= 0)
-                {
-                    compact_in_brick ((brick_address (current_brick) + brick_entry -1),
-                                      &args);
-                }
-            }
-            current_brick++;
-        }
-    }
-    recover_saved_pinned_info();
-    concurrent_print_time_delta ("compact end");
-    dprintf (2, (ThreadStressLog::gcEndCompactMsg(), heap_number));
-}
-#ifdef MULTIPLE_HEAPS
-#ifdef _MSC_VER
-#pragma warning(push)
-#pragma warning(disable:4702) // C4702: unreachable code: gc_thread_function may not return
-#endif //_MSC_VER
-void gc_heap::gc_thread_stub (void* arg)
-{
-    gc_heap* heap = (gc_heap*)arg;
-    if (!gc_thread_no_affinitize_p)
-    {
-        set_thread_affinity_for_heap (heap->heap_number, heap_select::find_proc_no_from_heap_no (heap->heap_number));
-    }
-    GCToOSInterface::BoostThreadPriority();
-    void* tmp = _alloca (256*heap->heap_number);
-    heap->gc_thread_function();
-}
-#ifdef _MSC_VER
-#pragma warning(pop)
-#endif //_MSC_VER
-#endif //MULTIPLE_HEAPS
-#ifdef BACKGROUND_GC
-#ifdef _MSC_VER
-#pragma warning(push)
-#pragma warning(disable:4702) // C4702: unreachable code: gc_thread_function may not return
-#endif //_MSC_VER
-void gc_heap::bgc_thread_stub (void* arg)
-{
-    gc_heap* heap = (gc_heap*)arg;
-    heap->bgc_thread = GCToEEInterface::GetThread();
-    assert(heap->bgc_thread != nullptr);
-    heap->bgc_thread_function();
-}
-#ifdef _MSC_VER
-#pragma warning(pop)
-#endif //_MSC_VER
-void gc_heap::background_drain_mark_list (int thread)
-{
-#ifndef MULTIPLE_HEAPS
-    UNREFERENCED_PARAMETER(thread);
-#endif //!MULTIPLE_HEAPS
-    size_t saved_c_mark_list_index = c_mark_list_index;
-    if (saved_c_mark_list_index)
-    {
-        concurrent_print_time_delta ("SML");
-    }
-    while (c_mark_list_index != 0)
-    {
-        size_t current_index = c_mark_list_index - 1;
-        uint8_t* o = c_mark_list [current_index];
-        background_mark_object (o THREAD_NUMBER_ARG);
-        c_mark_list_index--;
-    }
-    if (saved_c_mark_list_index)
-    {
-        concurrent_print_time_delta ("EML");
-    }
-    fire_drain_mark_list_event (saved_c_mark_list_index);
-}
-#ifdef MULTIPLE_HEAPS
-void gc_heap::background_scan_dependent_handles (ScanContext *sc)
-{
-    s_fUnscannedPromotions = TRUE;
-    while (true)
-    {
-        if (GCScan::GcDhUnpromotedHandlesExist(sc))
-            s_fUnpromotedHandles = TRUE;
-        bgc_t_join.join(this, gc_join_scan_dependent_handles);
-        if (bgc_t_join.joined())
-        {
-            s_fScanRequired = s_fUnscannedPromotions && s_fUnpromotedHandles;
-            s_fUnscannedPromotions = FALSE;
-            s_fUnpromotedHandles = FALSE;
-            if (!s_fScanRequired)
-            {
-#ifdef USE_REGIONS
-                BOOL all_heaps_background_overflow_p = FALSE;
-#else //USE_REGIONS
-                uint8_t* all_heaps_max = 0;
-                uint8_t* all_heaps_min = MAX_PTR;
-#endif //USE_REGIONS
-                int i;
-                for (i = 0; i < n_heaps; i++)
-                {
-#ifdef USE_REGIONS
-                    if (g_heaps[i]->background_overflow_p)
-                        all_heaps_background_overflow_p = TRUE;
-#else //USE_REGIONS
-                    if (all_heaps_max < g_heaps[i]->background_max_overflow_address)
-                        all_heaps_max = g_heaps[i]->background_max_overflow_address;
-                    if (all_heaps_min > g_heaps[i]->background_min_overflow_address)
-                        all_heaps_min = g_heaps[i]->background_min_overflow_address;
-#endif //USE_REGIONS
-                }
-                for (i = 0; i < n_heaps; i++)
-                {
-#ifdef USE_REGIONS
-                    g_heaps[i]->background_overflow_p = all_heaps_background_overflow_p;
-#else //USE_REGIONS
-                    g_heaps[i]->background_max_overflow_address = all_heaps_max;
-                    g_heaps[i]->background_min_overflow_address = all_heaps_min;
-#endif //USE_REGIONS
-                }
-            }
-            dprintf(2, ("Starting all gc thread mark stack overflow processing"));
-            bgc_t_join.restart();
-        }
-        if (background_process_mark_overflow (sc->concurrent))
-            s_fUnscannedPromotions = TRUE;
-        if (!s_fScanRequired)
-            break;
-        bgc_t_join.join(this, gc_join_rescan_dependent_handles);
-        if (bgc_t_join.joined())
-        {
-            dprintf(3, ("Starting all gc thread for dependent handle promotion"));
-            bgc_t_join.restart();
-        }
-        if (GCScan::GcDhUnpromotedHandlesExist(sc))
-            if (GCScan::GcDhReScan(sc))
-                s_fUnscannedPromotions = TRUE;
-    }
-}
-#else
-void gc_heap::background_scan_dependent_handles (ScanContext *sc)
-{
-    bool fUnscannedPromotions = true;
-    while (GCScan::GcDhUnpromotedHandlesExist(sc) && fUnscannedPromotions)
-    {
-        fUnscannedPromotions = false;
-        if (background_process_mark_overflow (sc->concurrent))
-            fUnscannedPromotions = true;
-        if (GCScan::GcDhReScan (sc))
-            fUnscannedPromotions = true;
-    }
-    background_process_mark_overflow (sc->concurrent);
-}
-#endif //MULTIPLE_HEAPS
-void gc_heap::recover_bgc_settings()
-{
-    if ((settings.condemned_generation < max_generation) && gc_heap::background_running_p())
-    {
-        dprintf (2, ("restoring bgc settings"));
-        settings = saved_bgc_settings;
-        GCHeap::GcCondemnedGeneration = gc_heap::settings.condemned_generation;
-    }
-}
-void gc_heap::allow_fgc()
-{
-    assert (bgc_thread == GCToEEInterface::GetThread());
-    bool bToggleGC = false;
-    if (g_fSuspensionPending > 0)
-    {
-        bToggleGC = GCToEEInterface::EnablePreemptiveGC();
-        if (bToggleGC)
-        {
-            GCToEEInterface::DisablePreemptiveGC();
-        }
-    }
-}
-BOOL gc_heap::is_bgc_in_progress()
-{
-    return (background_running_p() || (current_bgc_state == bgc_initialized));
-}
-void gc_heap::clear_commit_flag()
-{
-    for (int i = get_start_generation_index(); i < total_generation_count; i++)
-    {
-        generation* gen = generation_of (i);
-        heap_segment* seg = heap_segment_in_range (generation_start_segment (gen));
-        while (seg)
-        {
-            if (seg->flags & heap_segment_flags_ma_committed)
-            {
-                seg->flags &= ~heap_segment_flags_ma_committed;
-            }
-            if (seg->flags & heap_segment_flags_ma_pcommitted)
-            {
-                seg->flags &= ~heap_segment_flags_ma_pcommitted;
-            }
-            seg = heap_segment_next (seg);
-        }
-    }
-}
-void gc_heap::clear_commit_flag_global()
-{
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-    {
-        g_heaps[i]->clear_commit_flag();
-    }
-#else
-    clear_commit_flag();
-#endif //MULTIPLE_HEAPS
-}
-void gc_heap::verify_mark_array_cleared (uint8_t* begin, uint8_t* end, uint32_t* mark_array_addr)
-{
-#ifdef _DEBUG
-    size_t  markw = mark_word_of (begin);
-    size_t  markw_end = mark_word_of (end);
-    while (markw < markw_end)
-    {
-        if (mark_array_addr[markw])
-        {
-            uint8_t* addr = mark_word_address (markw);
-#ifdef USE_REGIONS
-            heap_segment* region = region_of (addr);
-            dprintf (1, ("The mark bits at 0x%zx:0x%x(addr: 0x%p, r: %zx(%p)) were not cleared",
-                            markw, mark_array_addr[markw], addr,
-                            (size_t)region, heap_segment_mem (region)));
-#else
-            dprintf (1, ("The mark bits at 0x%zx:0x%x(addr: 0x%p) were not cleared",
-                            markw, mark_array_addr[markw], addr));
-#endif //USE_REGIONS
-            FATAL_GC_ERROR();
-        }
-        markw++;
-    }
-#else // _DEBUG
-    UNREFERENCED_PARAMETER(begin);
-    UNREFERENCED_PARAMETER(end);
-    UNREFERENCED_PARAMETER(mark_array_addr);
-#endif //_DEBUG
-}
-uint8_t* gc_heap::get_start_address (heap_segment* seg)
-{
-    uint8_t* start =
-#ifdef USE_REGIONS
-        heap_segment_mem (seg);
-#else
-        (heap_segment_read_only_p(seg) ? heap_segment_mem (seg) : (uint8_t*)seg);
-#endif //USE_REGIONS
-    return start;
-}
-BOOL gc_heap::commit_mark_array_new_seg (gc_heap* hp,
-                                         heap_segment* seg,
-                                         uint32_t* new_card_table,
-                                         uint8_t* new_lowest_address)
-{
-    uint8_t* start = get_start_address (seg);
-    uint8_t* end = heap_segment_reserved (seg);
-    uint8_t* lowest = hp->background_saved_lowest_address;
-    uint8_t* highest = hp->background_saved_highest_address;
-    uint8_t* commit_start = NULL;
-    uint8_t* commit_end = NULL;
-    size_t commit_flag = 0;
-    if ((highest >= start) &&
-        (lowest <= end))
-    {
-        if ((start >= lowest) && (end <= highest))
-        {
-            dprintf (GC_TABLE_LOG, ("completely in bgc range: seg %p-%p, bgc: %p-%p",
-                                    start, end, lowest, highest));
-            commit_flag = heap_segment_flags_ma_committed;
-        }
-        else
-        {
-            dprintf (GC_TABLE_LOG, ("partially in bgc range: seg %p-%p, bgc: %p-%p",
-                                    start, end, lowest, highest));
-            commit_flag = heap_segment_flags_ma_pcommitted;
-#ifdef USE_REGIONS
-            assert (!"Region should not have its mark array partially committed.");
-#endif
-        }
-        commit_start = max (lowest, start);
-        commit_end = min (highest, end);
-        if (!commit_mark_array_by_range (commit_start, commit_end, hp->mark_array))
-        {
-            return FALSE;
-        }
-        if (new_card_table == 0)
-        {
-            new_card_table = g_gc_card_table;
-        }
-        if (hp->card_table != new_card_table)
-        {
-            if (new_lowest_address == 0)
-            {
-                new_lowest_address = g_gc_lowest_address;
-            }
-            uint32_t* ct = &new_card_table[card_word (gcard_of (new_lowest_address))];
-            uint32_t* ma = (uint32_t*)((uint8_t*)card_table_mark_array (ct) - size_mark_array_of (0, new_lowest_address));
-            dprintf (GC_TABLE_LOG, ("table realloc-ed: %p->%p, MA: %p->%p",
-                                    hp->card_table, new_card_table,
-                                    hp->mark_array, ma));
-            if (!commit_mark_array_by_range (commit_start, commit_end, ma))
-            {
-                return FALSE;
-            }
-        }
-        seg->flags |= commit_flag;
-    }
-    return TRUE;
-}
-BOOL gc_heap::commit_mark_array_by_range (uint8_t* begin, uint8_t* end, uint32_t* mark_array_addr)
-{
-    size_t beg_word = mark_word_of (begin);
-    size_t end_word = mark_word_of (align_on_mark_word (end));
-    uint8_t* commit_start = align_lower_page ((uint8_t*)&mark_array_addr[beg_word]);
-    uint8_t* commit_end = align_on_page ((uint8_t*)&mark_array_addr[end_word]);
-    size_t size = (size_t)(commit_end - commit_start);
-#ifdef SIMPLE_DPRINTF
-    dprintf (GC_TABLE_LOG, ("range: %p->%p mark word: %zx->%zx(%zd), mark array: %p->%p(%zd), commit %p->%p(%zd)",
-                            begin, end,
-                            beg_word, end_word,
-                            (end_word - beg_word) * sizeof (uint32_t),
-                            &mark_array_addr[beg_word],
-                            &mark_array_addr[end_word],
-                            (size_t)(&mark_array_addr[end_word] - &mark_array_addr[beg_word]),
-                            commit_start, commit_end,
-                            size));
-#endif //SIMPLE_DPRINTF
-    if (virtual_commit (commit_start, size, recorded_committed_bookkeeping_bucket))
-    {
-        verify_mark_array_cleared (begin, end, mark_array_addr);
-        return TRUE;
-    }
-    else
-    {
-        dprintf (GC_TABLE_LOG, ("failed to commit %zd bytes", (end_word - beg_word) * sizeof (uint32_t)));
-        return FALSE;
-    }
-}
-BOOL gc_heap::commit_mark_array_with_check (heap_segment* seg, uint32_t* new_mark_array_addr)
-{
-    uint8_t* start = get_start_address (seg);
-    uint8_t* end = heap_segment_reserved (seg);
-#ifdef MULTIPLE_HEAPS
-    uint8_t* lowest = heap_segment_heap (seg)->background_saved_lowest_address;
-    uint8_t* highest = heap_segment_heap (seg)->background_saved_highest_address;
-#else
-    uint8_t* lowest = background_saved_lowest_address;
-    uint8_t* highest = background_saved_highest_address;
-#endif //MULTIPLE_HEAPS
-    if ((highest >= start) &&
-        (lowest <= end))
-    {
-        start = max (lowest, start);
-        end = min (highest, end);
-        if (!commit_mark_array_by_range (start, end, new_mark_array_addr))
-        {
-            return FALSE;
-        }
-    }
-    return TRUE;
-}
-BOOL gc_heap::commit_mark_array_by_seg (heap_segment* seg, uint32_t* mark_array_addr)
-{
-    dprintf (GC_TABLE_LOG, ("seg: %p->%p; MA: %p",
-        seg,
-        heap_segment_reserved (seg),
-        mark_array_addr));
-    uint8_t* start = get_start_address (seg);
-    return commit_mark_array_by_range (start, heap_segment_reserved (seg), mark_array_addr);
-}
-BOOL gc_heap::commit_mark_array_bgc_init()
-{
-    dprintf (GC_TABLE_LOG, ("BGC init commit: lowest: %p, highest: %p, mark_array: %p",
-                            lowest_address, highest_address, mark_array));
-    for (int i = get_start_generation_index(); i < total_generation_count; i++)
-    {
-        generation* gen = generation_of (i);
-        heap_segment* seg = heap_segment_in_range (generation_start_segment (gen));
-        while (seg)
-        {
-            dprintf (GC_TABLE_LOG, ("h%d gen%d seg: %p(%p-%p), flags: %zd",
-                heap_number, i, seg, heap_segment_mem (seg), heap_segment_allocated (seg), seg->flags));
-            if (!(seg->flags & heap_segment_flags_ma_committed))
-            {
-                if (heap_segment_read_only_p (seg))
-                {
-                    if ((heap_segment_mem (seg) >= lowest_address) &&
-                        (heap_segment_reserved (seg) <= highest_address))
-                    {
-                        if (commit_mark_array_by_seg (seg, mark_array))
-                        {
-                            seg->flags |= heap_segment_flags_ma_committed;
-                        }
-                        else
-                        {
-                            return FALSE;
-                        }
-                    }
-                    else
-                    {
-                        uint8_t* start = max (lowest_address, heap_segment_mem (seg));
-                        uint8_t* end = min (highest_address, heap_segment_reserved (seg));
-                        if (commit_mark_array_by_range (start, end, mark_array))
-                        {
-                            seg->flags |= heap_segment_flags_ma_pcommitted;
-                        }
-                        else
-                        {
-                            return FALSE;
-                        }
-                    }
-                }
-                else
-                {
-                    if (commit_mark_array_by_seg (seg, mark_array))
-                    {
-                        if (seg->flags & heap_segment_flags_ma_pcommitted)
-                        {
-                            seg->flags &= ~heap_segment_flags_ma_pcommitted;
-                        }
-                        seg->flags |= heap_segment_flags_ma_committed;
-                    }
-                    else
-                    {
-                        return FALSE;
-                    }
-                }
-            }
-            seg = heap_segment_next (seg);
-        }
-    }
-    return TRUE;
-}
-BOOL gc_heap::commit_new_mark_array (uint32_t* new_mark_array_addr)
-{
-    dprintf (GC_TABLE_LOG, ("committing existing segs on MA %p", new_mark_array_addr));
-    for (int i = get_start_generation_index(); i < total_generation_count; i++)
-    {
-        generation* gen = generation_of (i);
-        heap_segment* seg = heap_segment_in_range (generation_start_segment (gen));
-        while (seg)
-        {
-            if (!commit_mark_array_with_check (seg, new_mark_array_addr))
-            {
-                return FALSE;
-            }
-            seg = heap_segment_next (seg);
-        }
-    }
-#if defined(MULTIPLE_HEAPS) && !defined(USE_REGIONS)
-    if (new_heap_segment)
-    {
-        if (!commit_mark_array_with_check (new_heap_segment, new_mark_array_addr))
-        {
-            return FALSE;
-        }
-    }
-#endif //MULTIPLE_HEAPS && !USE_REGIONS
-    return TRUE;
-}
-BOOL gc_heap::commit_new_mark_array_global (uint32_t* new_mark_array)
-{
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-    {
-        if (!g_heaps[i]->commit_new_mark_array (new_mark_array))
-        {
-            return FALSE;
-        }
-    }
-#else
-    if (!commit_new_mark_array (new_mark_array))
-    {
-        return FALSE;
-    }
-#endif //MULTIPLE_HEAPS
-    return TRUE;
-}
-void gc_heap::decommit_mark_array_by_seg (heap_segment* seg)
-{
-    if (mark_array == NULL)
-    {
-        return;
-    }
-    dprintf (GC_TABLE_LOG, ("decommitting seg %p(%zx), MA: %p", seg, seg->flags, mark_array));
-    size_t flags = seg->flags;
-    if ((flags & heap_segment_flags_ma_committed) ||
-        (flags & heap_segment_flags_ma_pcommitted))
-    {
-        uint8_t* start = get_start_address (seg);
-        uint8_t* end = heap_segment_reserved (seg);
-        if (flags & heap_segment_flags_ma_pcommitted)
-        {
-            start = max (lowest_address, start);
-            end = min (highest_address, end);
-        }
-        size_t beg_word = mark_word_of (start);
-        size_t end_word = mark_word_of (align_on_mark_word (end));
-        uint8_t* decommit_start = align_on_page ((uint8_t*)&mark_array[beg_word]);
-        uint8_t* decommit_end = align_lower_page ((uint8_t*)&mark_array[end_word]);
-        size_t size = (size_t)(decommit_end - decommit_start);
-#ifdef SIMPLE_DPRINTF
-        dprintf (GC_TABLE_LOG, ("seg: %p mark word: %zx->%zx(%zd), mark array: %p->%p(%zd), decommit %p->%p(%zd)",
-                                seg,
-                                beg_word, end_word,
-                                (end_word - beg_word) * sizeof (uint32_t),
-                                &mark_array[beg_word],
-                                &mark_array[end_word],
-                                (size_t)(&mark_array[end_word] - &mark_array[beg_word]),
-                                decommit_start, decommit_end,
-                                size));
-#endif //SIMPLE_DPRINTF
-        if (decommit_start < decommit_end)
-        {
-            if (!virtual_decommit (decommit_start, size, recorded_committed_bookkeeping_bucket))
-            {
-                dprintf (GC_TABLE_LOG, ("decommit on %p for %zd bytes failed",
-                                        decommit_start, size));
-                assert (!"decommit failed");
-            }
-        }
-        dprintf (GC_TABLE_LOG, ("decommitted [%zx for address [%p", beg_word, seg));
-    }
-}
-bool gc_heap::should_update_end_mark_size()
-{
-    return ((settings.condemned_generation == (max_generation - 1)) && (current_c_gc_state == c_gc_state_planning));
-}
-void gc_heap::background_mark_phase ()
-{
-    verify_mark_array_cleared();
-    ScanContext sc;
-    sc.thread_number = heap_number;
-    sc.thread_count = n_heaps;
-    sc.promotion = TRUE;
-    sc.concurrent = FALSE;
-    THREAD_FROM_HEAP;
-    BOOL cooperative_mode = TRUE;
-#ifndef MULTIPLE_HEAPS
-    const int thread = heap_number;
-#endif //!MULTIPLE_HEAPS
-    dprintf(2,("-(GC%zu)BMark-", VolatileLoad(&settings.gc_index)));
-    assert (settings.concurrent);
-    if (gen0_must_clear_bricks > 0)
-        gen0_must_clear_bricks--;
-    background_soh_alloc_count = 0;
-    background_uoh_alloc_count = 0;
-    bgc_overflow_count = 0;
-    bpromoted_bytes (heap_number) = 0;
-    static uint32_t num_sizedrefs = 0;
-#ifdef USE_REGIONS
-    background_overflow_p = FALSE;
-#else
-    background_min_overflow_address = MAX_PTR;
-    background_max_overflow_address = 0;
-    background_min_soh_overflow_address = MAX_PTR;
-    background_max_soh_overflow_address = 0;
-#endif //USE_REGIONS
-    processed_eph_overflow_p = FALSE;
-    assert (g_mark_list);
-    mark_list = g_mark_list;
-    mark_list_end = &mark_list [0];
-    mark_list_index = &mark_list [0];
-    c_mark_list_index = 0;
-#ifndef MULTIPLE_HEAPS
-    shigh = (uint8_t*) 0;
-    slow  = MAX_PTR;
-#endif //MULTIPLE_HEAPS
-    generation*   gen = generation_of (max_generation);
-    dprintf(3,("BGC: stack marking"));
-    sc.concurrent = TRUE;
-    GCScan::GcScanRoots(background_promote_callback,
-                            max_generation, max_generation,
-                            &sc);
-    dprintf(3,("BGC: finalization marking"));
-    finalize_queue->GcScanRoots(background_promote_callback, heap_number, 0);
-    size_t total_soh_size = generation_sizes (generation_of (max_generation));
-    size_t total_loh_size = generation_size (loh_generation);
-    size_t total_poh_size = generation_size (poh_generation);
-    bgc_begin_loh_size = total_loh_size;
-    bgc_begin_poh_size = total_poh_size;
-    bgc_loh_size_increased = 0;
-    bgc_poh_size_increased = 0;
-    background_soh_size_end_mark = 0;
-    dprintf (GTC_LOG, ("BM: h%d: loh: %zd, soh: %zd, poh: %zd", heap_number, total_loh_size, total_soh_size, total_poh_size));
-    concurrent_print_time_delta ("CS");
-    FIRE_EVENT(BGC1stNonConEnd);
-#ifndef USE_REGIONS
-    saved_overflow_ephemeral_seg = 0;
-#endif //!USE_REGIONS
-    current_bgc_state = bgc_reset_ww;
-#ifdef MULTIPLE_HEAPS
-    bgc_t_join.join(this, gc_join_restart_ee);
-    if (bgc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-        concurrent_print_time_delta ("CRWW begin");
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            g_heaps[i]->reset_write_watch (FALSE);
-        }
-#else
-        reset_write_watch (FALSE);
-#endif //MULTIPLE_HEAPS
-        concurrent_print_time_delta ("CRWW");
-#endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-        num_sizedrefs = GCToEEInterface::GetTotalNumSizedRefHandles();
-        dprintf (GTC_LOG, ("setting cm_in_progress"));
-        c_write (cm_in_progress, TRUE);
-        assert (dont_restart_ee_p);
-        dont_restart_ee_p = FALSE;
-        restart_vm();
-        GCToOSInterface::YieldThread (0);
-#ifdef MULTIPLE_HEAPS
-        dprintf(3, ("Starting all gc threads for gc"));
-        bgc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-#ifdef MULTIPLE_HEAPS
-    bgc_t_join.join(this, gc_join_after_reset);
-    if (bgc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-        disable_preemptive (true);
-#ifndef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-#ifdef WRITE_WATCH
-        concurrent_print_time_delta ("CRWW begin");
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            g_heaps[i]->reset_write_watch (TRUE);
-        }
-#else
-        reset_write_watch (TRUE);
-#endif //MULTIPLE_HEAPS
-        concurrent_print_time_delta ("CRWW");
-#endif //WRITE_WATCH
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            g_heaps[i]->revisit_written_pages (TRUE, TRUE);
-        }
-#else
-        revisit_written_pages (TRUE, TRUE);
-#endif //MULTIPLE_HEAPS
-        concurrent_print_time_delta ("CRW");
-#endif // !FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-#ifdef MULTIPLE_HEAPS
-        for (int i = 0; i < n_heaps; i++)
-        {
-            g_heaps[i]->current_bgc_state = bgc_mark_handles;
-        }
-#else
-        current_bgc_state = bgc_mark_handles;
-#endif //MULTIPLE_HEAPS
-        current_c_gc_state = c_gc_state_marking;
-        enable_preemptive ();
-#ifdef MULTIPLE_HEAPS
-        dprintf(3, ("Joining BGC threads after resetting writewatch"));
-        bgc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-    disable_preemptive (true);
-    if (num_sizedrefs > 0)
-    {
-        GCScan::GcScanSizedRefs(background_promote, max_generation, max_generation, &sc);
-        enable_preemptive ();
-#ifdef MULTIPLE_HEAPS
-        bgc_t_join.join(this, gc_join_scan_sizedref_done);
-        if (bgc_t_join.joined())
-        {
-            dprintf(3, ("Done with marking all sized refs. Starting all bgc thread for marking other strong roots"));
-            bgc_t_join.restart();
-        }
-#endif //MULTIPLE_HEAPS
-        disable_preemptive (true);
-    }
-    dprintf (3,("BGC: handle table marking"));
-    GCScan::GcScanHandles(background_promote,
-                                max_generation, max_generation,
-                                &sc);
-    concurrent_print_time_delta ("CRH");
-    current_bgc_state = bgc_mark_stack;
-    dprintf (2,("concurrent draining mark list"));
-    background_drain_mark_list (thread);
-    concurrent_print_time_delta ("CRS");
-    dprintf (2,("concurrent revisiting dirtied pages"));
-    revisit_written_pages (TRUE);
-    revisit_written_pages (TRUE);
-    concurrent_print_time_delta ("CRre");
-    enable_preemptive ();
-#if defined(MULTIPLE_HEAPS)
-    bgc_t_join.join(this, gc_join_concurrent_overflow);
-    if (bgc_t_join.joined())
-    {
-#ifdef USE_REGIONS
-        BOOL all_heaps_background_overflow_p = FALSE;
-#else //USE_REGIONS
-        uint8_t* all_heaps_max = 0;
-        uint8_t* all_heaps_min = MAX_PTR;
-#endif //USE_REGIONS
-        int i;
-        for (i = 0; i < n_heaps; i++)
-        {
-#ifdef USE_REGIONS
-            if (g_heaps[i]->background_overflow_p)
-                all_heaps_background_overflow_p = TRUE;
-#else //USE_REGIONS
-            dprintf (3, ("heap %d overflow max is %p, min is %p",
-                i,
-                g_heaps[i]->background_max_overflow_address,
-                g_heaps[i]->background_min_overflow_address));
-            if (all_heaps_max < g_heaps[i]->background_max_overflow_address)
-                all_heaps_max = g_heaps[i]->background_max_overflow_address;
-            if (all_heaps_min > g_heaps[i]->background_min_overflow_address)
-                all_heaps_min = g_heaps[i]->background_min_overflow_address;
-#endif //USE_REGIONS
-        }
-        for (i = 0; i < n_heaps; i++)
-        {
-#ifdef USE_REGIONS
-            g_heaps[i]->background_overflow_p = all_heaps_background_overflow_p;
-#else //USE_REGIONS
-            g_heaps[i]->background_max_overflow_address = all_heaps_max;
-            g_heaps[i]->background_min_overflow_address = all_heaps_min;
-#endif //USE_REGIONS
-        }
-        dprintf(3, ("Starting all bgc threads after updating the overflow info"));
-        bgc_t_join.restart();
-    }
-#endif //MULTIPLE_HEAPS
-    disable_preemptive (true);
-    dprintf (2, ("before CRov count: %zu", bgc_overflow_count));
-    bgc_overflow_count = 0;
-    background_process_mark_overflow (TRUE);
-    dprintf (2, ("after CRov count: %zu", bgc_overflow_count));
-    bgc_overflow_count = 0;
-    concurrent_print_time_delta ("CRov");
-    FIRE_EVENT(BGC1stConEnd);
-    dprintf (2, ("Stopping the EE"));
-    enable_preemptive ();
-#ifdef MULTIPLE_HEAPS
-    bgc_t_join.join(this, gc_join_suspend_ee);
-    if (bgc_t_join.joined())
-    {
-        bgc_threads_sync_event.Reset();
-        dprintf(3, ("Joining BGC threads for non concurrent final marking"));
-        bgc_t_join.restart();
-    }
-#endif //MULTIPLE_HEAPS
-    if (heap_number == 0)
-    {
-        enter_spin_lock (&gc_lock);
-        suspended_start_time = GetHighPrecisionTimeStamp();
-        bgc_suspend_EE ();
-        bgc_threads_sync_event.Set();
-    }
-    else
-    {
-        bgc_threads_sync_event.Wait(INFINITE, FALSE);
-        dprintf (2, ("bgc_threads_sync_event is signalled"));
-    }
-    assert (settings.concurrent);
-    assert (settings.condemned_generation == max_generation);
-    dprintf (2, ("clearing cm_in_progress"));
-    c_write (cm_in_progress, FALSE);
-    bgc_alloc_lock->check();
-    current_bgc_state = bgc_final_marking;
-    concurrent_print_time_delta ("CR");
-    FIRE_EVENT(BGC2ndNonConBegin);
-    mark_absorb_new_alloc();
-#ifdef FEATURE_EVENT_TRACE
-    static uint64_t current_mark_time = 0;
-    static uint64_t last_mark_time = 0;
-#endif //FEATURE_EVENT_TRACE
-#ifdef MULTIPLE_HEAPS
-    bgc_t_join.join(this, gc_join_after_absorb);
-    if (bgc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-#ifdef BGC_SERVO_TUNING
-        bgc_tuning::record_bgc_sweep_start();
-#endif //BGC_SERVO_TUNING
-        GCToEEInterface::BeforeGcScanRoots(max_generation, /* is_bgc */ true, /* is_concurrent */ false);
-#ifdef FEATURE_EVENT_TRACE
-        informational_event_enabled_p = EVENT_ENABLED (GCMarkWithType);
-        if (informational_event_enabled_p)
-            last_mark_time = GetHighPrecisionTimeStamp();
-#endif //FEATURE_EVENT_TRACE
-#ifdef MULTIPLE_HEAPS
-        dprintf(3, ("Joining BGC threads after absorb"));
-        bgc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-    sc.concurrent = FALSE;
-    total_soh_size = generation_sizes (generation_of (max_generation));
-    total_loh_size = generation_size (loh_generation);
-    total_poh_size = generation_size (poh_generation);
-    dprintf (GTC_LOG, ("FM: h%d: loh: %zd, soh: %zd, poh: %zd", heap_number, total_loh_size, total_soh_size, total_poh_size));
-#ifdef FEATURE_BASICFREEZE
-#ifdef USE_REGIONS
-    assert (!ro_segments_in_range);
-#else //USE_REGIONS
-    if (ro_segments_in_range)
-    {
-        dprintf (2, ("nonconcurrent marking in range ro segments"));
-        mark_ro_segments();
-        concurrent_print_time_delta ("NRRO");
-    }
-#endif //USE_REGIONS
-#endif //FEATURE_BASICFREEZE
-    dprintf (2, ("nonconcurrent marking stack roots"));
-    GCScan::GcScanRoots(background_promote,
-                            max_generation, max_generation,
-                            &sc);
-    concurrent_print_time_delta ("NRS");
-    finalize_queue->GcScanRoots(background_promote, heap_number, 0);
-    dprintf (2, ("nonconcurrent marking handle table"));
-    GCScan::GcScanHandles(background_promote,
-                                max_generation, max_generation,
-                                &sc);
-    concurrent_print_time_delta ("NRH");
-    dprintf (2,("---- (GC%zu)final going through written pages ----", VolatileLoad(&settings.gc_index)));
-    revisit_written_pages (FALSE);
-    concurrent_print_time_delta ("NRre LOH");
-    dprintf (2, ("before NR 1st Hov count: %zu", bgc_overflow_count));
-    bgc_overflow_count = 0;
-    dprintf (2, ("1st dependent handle scan and process mark overflow"));
-    GCScan::GcDhInitialScan(background_promote, max_generation, max_generation, &sc);
-    background_scan_dependent_handles (&sc);
-    concurrent_print_time_delta ("NR 1st Hov");
-    dprintf (2, ("after NR 1st Hov count: %zu", bgc_overflow_count));
-    bgc_overflow_count = 0;
-#ifdef MULTIPLE_HEAPS
-    bgc_t_join.join(this, gc_join_null_dead_short_weak);
-    if (bgc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-#ifdef FEATURE_EVENT_TRACE
-        bgc_time_info[time_mark_sizedref] = 0;
-        record_mark_time (bgc_time_info[time_mark_roots], current_mark_time, last_mark_time);
-#endif //FEATURE_EVENT_TRACE
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-        SoftwareWriteWatch::DisableForGCHeap();
-#endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-        GCToEEInterface::AfterGcScanRoots (max_generation, max_generation, &sc);
-#ifdef MULTIPLE_HEAPS
-        dprintf(3, ("Joining BGC threads for short weak handle scan"));
-        bgc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-    GCScan::GcShortWeakPtrScan(max_generation, max_generation, &sc);
-    concurrent_print_time_delta ("NR GcShortWeakPtrScan");
-    {
-#ifdef MULTIPLE_HEAPS
-        bgc_t_join.join(this, gc_join_scan_finalization);
-        if (bgc_t_join.joined())
-        {
-#endif //MULTIPLE_HEAPS
-#ifdef FEATURE_EVENT_TRACE
-            record_mark_time (bgc_time_info[time_mark_short_weak], current_mark_time, last_mark_time);
-#endif //FEATURE_EVENT_TRACE
-#ifdef MULTIPLE_HEAPS
-            dprintf(3, ("Joining BGC threads for finalization"));
-            bgc_t_join.restart();
-        }
-#endif //MULTIPLE_HEAPS
-        dprintf(3,("Marking finalization data"));
-        concurrent_print_time_delta ("NRj");
-        finalize_queue->ScanForFinalization (background_promote, max_generation, FALSE, __this);
-        concurrent_print_time_delta ("NRF");
-    }
-    dprintf (2, ("before NR 2nd Hov count: %zu", bgc_overflow_count));
-    bgc_overflow_count = 0;
-    dprintf (2, ("2nd dependent handle scan and process mark overflow"));
-    background_scan_dependent_handles (&sc);
-    concurrent_print_time_delta ("NR 2nd Hov");
-#ifdef MULTIPLE_HEAPS
-    bgc_t_join.join(this, gc_join_null_dead_long_weak);
-    if (bgc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-#ifdef FEATURE_EVENT_TRACE
-        record_mark_time (bgc_time_info[time_mark_scan_finalization], current_mark_time, last_mark_time);
-#endif //FEATURE_EVENT_TRACE
-#ifdef MULTIPLE_HEAPS
-        dprintf(2, ("Joining BGC threads for weak pointer deletion"));
-        bgc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-    GCScan::GcWeakPtrScan (max_generation, max_generation, &sc);
-    concurrent_print_time_delta ("NR GcWeakPtrScan");
-#ifdef MULTIPLE_HEAPS
-    bgc_t_join.join(this, gc_join_null_dead_syncblk);
-    if (bgc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-        dprintf (2, ("calling GcWeakPtrScanBySingleThread"));
-        GCScan::GcWeakPtrScanBySingleThread (max_generation, max_generation, &sc);
-#ifdef FEATURE_EVENT_TRACE
-        record_mark_time (bgc_time_info[time_mark_long_weak], current_mark_time, last_mark_time);
-#endif //FEATURE_EVENT_TRACE
-        concurrent_print_time_delta ("NR GcWeakPtrScanBySingleThread");
-#ifdef MULTIPLE_HEAPS
-        dprintf(2, ("Starting BGC threads for end of background mark phase"));
-        bgc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-    dprintf (2, ("end of bgc mark: loh: %zu, poh: %zu, soh: %zu",
-                 generation_size (loh_generation),
-                 generation_size (poh_generation),
-                 generation_sizes (generation_of (max_generation))));
-    for (int gen_idx = max_generation; gen_idx < total_generation_count; gen_idx++)
-    {
-        generation* gen = generation_of (gen_idx);
-        dynamic_data* dd = dynamic_data_of (gen_idx);
-        dd_begin_data_size (dd) = generation_size (gen_idx) -
-                                  (generation_free_list_space (gen) + generation_free_obj_space (gen)) -
-                                   get_generation_start_size (gen_idx);
-        dd_survived_size (dd) = 0;
-        dd_pinned_survived_size (dd) = 0;
-        dd_artificial_pinned_survived_size (dd) = 0;
-        dd_added_pinned_size (dd) = 0;
-    }
-    for (int i = get_start_generation_index(); i < uoh_start_generation; i++)
-    {
-        heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (i)));
-        PREFIX_ASSUME(seg != NULL);
-        while (seg)
-        {
-            seg->flags &= ~heap_segment_flags_swept;
-#ifndef USE_REGIONS
-            if (heap_segment_allocated (seg) == heap_segment_mem (seg))
-            {
-                FATAL_GC_ERROR();
-            }
-            if (seg == ephemeral_heap_segment)
-            {
-                heap_segment_background_allocated (seg) = generation_allocation_start (generation_of (max_generation - 1));
-            }
-            else
-#endif //!USE_REGIONS
-            {
-                heap_segment_background_allocated (seg) = heap_segment_allocated (seg);
-            }
-            background_soh_size_end_mark += heap_segment_background_allocated (seg) - heap_segment_mem (seg);
-            dprintf (3333, ("h%d gen%d seg %zx (%p) background allocated is %p",
-                            heap_number, i, (size_t)(seg), heap_segment_mem (seg),
-                            heap_segment_background_allocated (seg)));
-            seg = heap_segment_next_rw (seg);
-        }
-    }
-    repair_allocation_contexts (FALSE);
-    dprintf (2, ("end of bgc mark: gen2 free list space: %zu, free obj space: %zu",
-        generation_free_list_space (generation_of (max_generation)),
-        generation_free_obj_space (generation_of (max_generation))));
-    dprintf(2,("---- (GC%zu)End of background mark phase ----", VolatileLoad(&settings.gc_index)));
-}
-void
-gc_heap::suspend_EE ()
-{
-    dprintf (2, ("suspend_EE"));
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hp = gc_heap::g_heaps[0];
-    GCToEEInterface::SuspendEE(SUSPEND_FOR_GC_PREP);
-#else
-    GCToEEInterface::SuspendEE(SUSPEND_FOR_GC_PREP);
-#endif //MULTIPLE_HEAPS
-}
-#ifdef MULTIPLE_HEAPS
-void
-gc_heap::bgc_suspend_EE ()
-{
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap::g_heaps[i]->reset_gc_done();
-    }
-    gc_started = TRUE;
-    dprintf (2, ("bgc_suspend_EE"));
-    GCToEEInterface::SuspendEE(SUSPEND_FOR_GC_PREP);
-    gc_started = FALSE;
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap::g_heaps[i]->set_gc_done();
-    }
-}
-#else
-void
-gc_heap::bgc_suspend_EE ()
-{
-    reset_gc_done();
-    gc_started = TRUE;
-    dprintf (2, ("bgc_suspend_EE"));
-    GCToEEInterface::SuspendEE(SUSPEND_FOR_GC_PREP);
-    gc_started = FALSE;
-    set_gc_done();
-}
-#endif //MULTIPLE_HEAPS
-void
-gc_heap::restart_EE ()
-{
-    dprintf (2, ("restart_EE"));
-#ifdef MULTIPLE_HEAPS
-    GCToEEInterface::RestartEE(FALSE);
-#else
-    GCToEEInterface::RestartEE(FALSE);
-#endif //MULTIPLE_HEAPS
-}
-inline uint8_t* gc_heap::high_page (heap_segment* seg, BOOL concurrent_p)
-{
-#ifdef USE_REGIONS
-    assert (!concurrent_p || (heap_segment_gen_num (seg) >= max_generation));
-#else
-    if (concurrent_p)
-    {
-        uint8_t* end = ((seg == ephemeral_heap_segment) ?
-                     generation_allocation_start (generation_of (max_generation - 1)) :
-                     heap_segment_allocated (seg));
-        return align_lower_page (end);
-    }
-    else
-#endif //USE_REGIONS
-    {
-        return heap_segment_allocated (seg);
-    }
-}
-void gc_heap::revisit_written_page (uint8_t* page,
-                                    uint8_t* end,
-                                    BOOL concurrent_p,
-                                    uint8_t*& last_page,
-                                    uint8_t*& last_object,
-                                    BOOL large_objects_p,
-                                    size_t& num_marked_objects)
-{
-    uint8_t*   start_address = page;
-    uint8_t*   o             = 0;
-    int align_const = get_alignment_constant (!large_objects_p);
-    uint8_t* high_address = end;
-    uint8_t* current_lowest_address = background_saved_lowest_address;
-    uint8_t* current_highest_address = background_saved_highest_address;
-    BOOL no_more_loop_p = FALSE;
-    THREAD_FROM_HEAP;
-#ifndef MULTIPLE_HEAPS
-    const int thread = heap_number;
-#endif //!MULTIPLE_HEAPS
-    if (large_objects_p)
-    {
-        o = last_object;
-    }
-    else
-    {
-        if (((last_page + WRITE_WATCH_UNIT_SIZE) == page)
-            || (start_address <= last_object))
-        {
-            o = last_object;
-        }
-        else
-        {
-            o = find_first_object (start_address, last_object);
-            assert (o >= last_object);
-        }
-    }
-    dprintf (3,("page %zx start: %zx, %zx[ ",
-               (size_t)page, (size_t)o,
-               (size_t)(min (high_address, page + WRITE_WATCH_UNIT_SIZE))));
-    while (o < (min (high_address, page + WRITE_WATCH_UNIT_SIZE)))
-    {
-        size_t s;
-        if (concurrent_p && large_objects_p)
-        {
-            bgc_alloc_lock->bgc_mark_set (o);
-            if (((CObjectHeader*)o)->IsFree())
-            {
-                s = unused_array_size (o);
-            }
-            else
-            {
-                s = size (o);
-            }
-        }
-        else
-        {
-            s = size (o);
-        }
-        dprintf (3,("Considering object %zx(%s)", (size_t)o, (background_object_marked (o, FALSE) ? "bm" : "nbm")));
-        assert (Align (s) >= Align (min_obj_size));
-        uint8_t* next_o =  o + Align (s, align_const);
-        if (next_o >= start_address)
-        {
-#ifdef MULTIPLE_HEAPS
-            if (concurrent_p)
-            {
-                last_object = o;
-            }
-#endif //MULTIPLE_HEAPS
-            if (contain_pointers (o) &&
-                (!((o >= current_lowest_address) && (o < current_highest_address)) ||
-                background_marked (o)))
-            {
-                dprintf (3, ("going through %zx", (size_t)o));
-                go_through_object (method_table(o), o, s, poo, start_address, use_start, (o + s),
-                                    if ((uint8_t*)poo >= min (high_address, page + WRITE_WATCH_UNIT_SIZE))
-                                    {
-                                        no_more_loop_p = TRUE;
-                                        goto end_limit;
-                                    }
-                                    uint8_t* oo = VolatileLoadWithoutBarrier(poo);
-                                    num_marked_objects++;
-                                    background_mark_object (oo THREAD_NUMBER_ARG);
-                                );
-            }
-            else if (
-                concurrent_p &&
-                ((CObjectHeader*)o)->IsFree() &&
-                (next_o > min (high_address, page + WRITE_WATCH_UNIT_SIZE)))
-            {
-                no_more_loop_p = TRUE;
-                goto end_limit;
-            }
-        }
-end_limit:
-        if (concurrent_p && large_objects_p)
-        {
-            bgc_alloc_lock->bgc_mark_done ();
-        }
-        if (no_more_loop_p)
-        {
-            break;
-        }
-        o = next_o;
-    }
-#ifdef MULTIPLE_HEAPS
-    if (concurrent_p)
-    {
-        assert (last_object < (min (high_address, page + WRITE_WATCH_UNIT_SIZE)));
-    }
-    else
-#endif //MULTIPLE_HEAPS
-    {
-        last_object = o;
-    }
-    dprintf (3,("Last object: %zx", (size_t)last_object));
-    last_page = align_write_watch_lower_page (o);
-    if (concurrent_p)
-    {
-        allow_fgc();
-    }
-}
-void gc_heap::revisit_written_pages (BOOL concurrent_p, BOOL reset_only_p)
-{
-    if (concurrent_p && !reset_only_p)
-    {
-        current_bgc_state = bgc_revisit_soh;
-    }
-    size_t total_dirtied_pages = 0;
-    size_t total_marked_objects = 0;
-    bool reset_watch_state = !!concurrent_p;
-    bool is_runtime_suspended = !concurrent_p;
-    BOOL small_object_segments = TRUE;
-    int start_gen_idx = get_start_generation_index();
-#ifdef USE_REGIONS
-    if (concurrent_p && !reset_only_p)
-    {
-        start_gen_idx = max_generation;
-    }
-#endif //USE_REGIONS
-    for (int i = start_gen_idx; i < total_generation_count; i++)
-    {
-        heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (i)));
-        PREFIX_ASSUME(seg != NULL);
-        while (seg)
-        {
-            uint8_t* base_address = (uint8_t*)heap_segment_mem (seg);
-            uintptr_t bcount = array_size;
-            uint8_t* last_page = 0;
-            uint8_t* last_object = heap_segment_mem (seg);
-            uint8_t* high_address = 0;
-            BOOL skip_seg_p = FALSE;
-            if (reset_only_p)
-            {
-                if ((heap_segment_mem (seg) >= background_saved_lowest_address) ||
-                    (heap_segment_reserved (seg) <= background_saved_highest_address))
-                {
-                    dprintf (3, ("h%d: sseg: %p(-%p)", heap_number,
-                        heap_segment_mem (seg), heap_segment_reserved (seg)));
-                    skip_seg_p = TRUE;
-                }
-            }
-            if (!skip_seg_p)
-            {
-                dprintf (3, ("looking at seg %zx", (size_t)last_object));
-                if (reset_only_p)
-                {
-                    base_address = max (base_address, background_saved_lowest_address);
-                    dprintf (3, ("h%d: reset only starting %p", heap_number, base_address));
-                }
-                dprintf (3, ("h%d: starting: %p, seg %p-%p", heap_number, base_address,
-                    heap_segment_mem (seg), heap_segment_reserved (seg)));
-                while (1)
-                {
-                    if (reset_only_p)
-                    {
-                        high_address = ((seg == ephemeral_heap_segment) ? alloc_allocated : heap_segment_allocated (seg));
-                        high_address = min (high_address, background_saved_highest_address);
-                    }
-                    else
-                    {
-                        high_address = high_page (seg, concurrent_p);
-                    }
-                    if ((base_address < high_address) &&
-                        (bcount >= array_size))
-                    {
-                        ptrdiff_t region_size = high_address - base_address;
-                        dprintf (3, ("h%d: gw: [%zx(%zd)", heap_number, (size_t)base_address, (size_t)region_size));
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-                        if (!is_runtime_suspended)
-                        {
-                            enter_spin_lock(&gc_lock);
-                        }
-#endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-                        get_write_watch_for_gc_heap (reset_watch_state, base_address, region_size,
-                                                     (void**)background_written_addresses,
-                                                     &bcount, is_runtime_suspended);
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-                        if (!is_runtime_suspended)
-                        {
-                            leave_spin_lock(&gc_lock);
-                        }
-#endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-                        if (bcount != 0)
-                        {
-                            total_dirtied_pages += bcount;
-                            dprintf (3, ("Found %zu pages [%zx, %zx[",
-                                            bcount, (size_t)base_address, (size_t)high_address));
-                        }
-                        if (!reset_only_p)
-                        {
-                            high_address = high_page (seg, concurrent_p);
-                            for (unsigned i = 0; i < bcount; i++)
-                            {
-                                uint8_t* page = (uint8_t*)background_written_addresses[i];
-                                dprintf (3, ("looking at page %d at %zx(h: %zx)", i,
-                                    (size_t)page, (size_t)high_address));
-                                if (page < high_address)
-                                {
-                                    revisit_written_page (page, high_address, concurrent_p,
-                                                          last_page, last_object,
-                                                          !small_object_segments,
-                                                          total_marked_objects);
-                                }
-                                else
-                                {
-                                    dprintf (3, ("page %d at %zx is >= %zx!", i, (size_t)page, (size_t)high_address));
-                                    assert (!"page shouldn't have exceeded limit");
-                                }
-                            }
-                        }
-                        if (bcount >= array_size){
-                            base_address = background_written_addresses [array_size-1] + WRITE_WATCH_UNIT_SIZE;
-                            bcount = array_size;
-                        }
-                    }
-                    else
-                    {
-                        break;
-                    }
-                }
-            }
-            seg = heap_segment_next_rw (seg);
-        }
-        if (i == soh_gen2)
-        {
-            if (!reset_only_p)
-            {
-                dprintf (GTC_LOG, ("h%d: SOH: dp:%zd; mo: %zd", heap_number, total_dirtied_pages, total_marked_objects));
-                fire_revisit_event (total_dirtied_pages, total_marked_objects, FALSE);
-                concurrent_print_time_delta (concurrent_p ? "CR SOH" : "NR SOH");
-                total_dirtied_pages = 0;
-                total_marked_objects = 0;
-            }
-            if (concurrent_p && !reset_only_p)
-            {
-                current_bgc_state = bgc_revisit_uoh;
-            }
-            small_object_segments = FALSE;
-            dprintf (3, ("now revisiting large object segments"));
-        }
-        else
-        {
-            if (reset_only_p)
-            {
-                dprintf (GTC_LOG, ("h%d: tdp: %zd", heap_number, total_dirtied_pages));
-            }
-            else
-            {
-                dprintf (GTC_LOG, ("h%d: LOH: dp:%zd; mo: %zd", heap_number, total_dirtied_pages, total_marked_objects));
-                fire_revisit_event (total_dirtied_pages, total_marked_objects, TRUE);
-            }
-        }
-    }
-}
-void gc_heap::background_grow_c_mark_list()
-{
-    assert (c_mark_list_index >= c_mark_list_length);
-    BOOL should_drain_p = FALSE;
-    THREAD_FROM_HEAP;
-#ifndef MULTIPLE_HEAPS
-    const int thread = heap_number;
-#endif //!MULTIPLE_HEAPS
-    dprintf (2, ("stack copy buffer overflow"));
-    uint8_t** new_c_mark_list = 0;
-    {
-        FAULT_NOT_FATAL();
-        if (c_mark_list_length >= (SIZE_T_MAX / (2 * sizeof (uint8_t*))))
-        {
-            should_drain_p = TRUE;
-        }
-        else
-        {
-            new_c_mark_list = new (nothrow) uint8_t*[c_mark_list_length*2];
-            if (new_c_mark_list == 0)
-            {
-                should_drain_p = TRUE;
-            }
-        }
-    }
-    if (should_drain_p)
-    {
-        dprintf (2, ("No more memory for the stacks copy, draining.."));
-        background_drain_mark_list (thread);
-    }
-    else
-    {
-        assert (new_c_mark_list);
-        memcpy (new_c_mark_list, c_mark_list, c_mark_list_length*sizeof(uint8_t*));
-        c_mark_list_length = c_mark_list_length*2;
-        dprintf (5555, ("h%d replacing mark list at %Ix with %Ix", heap_number, (size_t)c_mark_list, (size_t)new_c_mark_list));
-        delete[] c_mark_list;
-        c_mark_list = new_c_mark_list;
-    }
-}
-void gc_heap::background_promote_callback (Object** ppObject, ScanContext* sc,
-                                  uint32_t flags)
-{
-    UNREFERENCED_PARAMETER(sc);
-    assert (settings.concurrent);
-    THREAD_NUMBER_FROM_CONTEXT;
-#ifndef MULTIPLE_HEAPS
-    const int thread = 0;
-#endif //!MULTIPLE_HEAPS
-    uint8_t* o = (uint8_t*)*ppObject;
-    if (!is_in_find_object_range (o))
-    {
-        return;
-    }
-    HEAP_FROM_THREAD;
-    gc_heap* hp = gc_heap::heap_of (o);
-    if ((o < hp->background_saved_lowest_address) || (o >= hp->background_saved_highest_address))
-    {
-        return;
-    }
-    if (flags & GC_CALL_INTERIOR)
-    {
-        o = hp->find_object (o);
-        if (o == 0)
-            return;
-    }
-#ifdef FEATURE_CONSERVATIVE_GC
-    if (GCConfig::GetConservativeGC() && ((CObjectHeader*)o)->IsFree())
-    {
-        return;
-    }
-#endif //FEATURE_CONSERVATIVE_GC
-#ifdef _DEBUG
-    ((CObjectHeader*)o)->Validate();
-#endif //_DEBUG
-    dprintf (3, ("Concurrent Background Promote %zx", (size_t)o));
-    if (o && (size (o) > loh_size_threshold))
-    {
-        dprintf (3, ("Brc %zx", (size_t)o));
-    }
-    if (hpt->c_mark_list_index >= hpt->c_mark_list_length)
-    {
-        hpt->background_grow_c_mark_list();
-    }
-    dprintf (3, ("pushing %zx into mark_list", (size_t)o));
-    hpt->c_mark_list [hpt->c_mark_list_index++] = o;
-    STRESS_LOG3(LF_GC|LF_GCROOTS, LL_INFO1000000, "    GCHeap::Background Promote: Promote GC Root *%p = %p MT = %pT", ppObject, o, o ? ((Object*) o)->GetGCSafeMethodTable() : NULL);
-}
-void gc_heap::mark_absorb_new_alloc()
-{
-    fix_allocation_contexts (FALSE);
-    gen0_bricks_cleared = FALSE;
-    clear_gen0_bricks();
-}
-BOOL gc_heap::prepare_bgc_thread(gc_heap* gh)
-{
-    BOOL success = FALSE;
-    BOOL thread_created = FALSE;
-    dprintf (2, ("Preparing gc thread"));
-    gh->bgc_threads_timeout_cs.Enter();
-    if (!(gh->bgc_thread_running))
-    {
-        dprintf (2, ("GC thread not running"));
-        if ((gh->bgc_thread == 0) && create_bgc_thread(gh))
-        {
-            success = TRUE;
-            thread_created = TRUE;
-        }
-    }
-    else
-    {
-        dprintf (3, ("GC thread already running"));
-        success = TRUE;
-    }
-    gh->bgc_threads_timeout_cs.Leave();
-    if(thread_created)
-        FIRE_EVENT(GCCreateConcurrentThread_V1);
-    return success;
-}
-BOOL gc_heap::create_bgc_thread(gc_heap* gh)
-{
-    assert (background_gc_done_event.IsValid());
-    gh->bgc_thread_running = GCToEEInterface::CreateThread(gh->bgc_thread_stub, gh, true, ".NET BGC");
-    return gh->bgc_thread_running;
-}
-BOOL gc_heap::create_bgc_threads_support (int number_of_heaps)
-{
-    BOOL ret = FALSE;
-    dprintf (3, ("Creating concurrent GC thread for the first time"));
-    if (!background_gc_done_event.CreateManualEventNoThrow(TRUE))
-    {
-        goto cleanup;
-    }
-    if (!bgc_threads_sync_event.CreateManualEventNoThrow(FALSE))
-    {
-        goto cleanup;
-    }
-    if (!ee_proceed_event.CreateAutoEventNoThrow(FALSE))
-    {
-        goto cleanup;
-    }
-    if (!bgc_start_event.CreateManualEventNoThrow(FALSE))
-    {
-        goto cleanup;
-    }
-#ifdef MULTIPLE_HEAPS
-    bgc_t_join.init (number_of_heaps, join_flavor_bgc);
-#else
-    UNREFERENCED_PARAMETER(number_of_heaps);
-#endif //MULTIPLE_HEAPS
-    ret = TRUE;
-cleanup:
-    if (!ret)
-    {
-        if (background_gc_done_event.IsValid())
-        {
-            background_gc_done_event.CloseEvent();
-        }
-        if (bgc_threads_sync_event.IsValid())
-        {
-            bgc_threads_sync_event.CloseEvent();
-        }
-        if (ee_proceed_event.IsValid())
-        {
-            ee_proceed_event.CloseEvent();
-        }
-        if (bgc_start_event.IsValid())
-        {
-            bgc_start_event.CloseEvent();
-        }
-    }
-    return ret;
-}
-BOOL gc_heap::create_bgc_thread_support()
-{
-    uint8_t** parr;
-    parr = new (nothrow) uint8_t*[1 + OS_PAGE_SIZE / MIN_OBJECT_SIZE];
-    if (!parr)
-    {
-        return FALSE;
-    }
-    make_c_mark_list (parr);
-    return TRUE;
-}
-int gc_heap::check_for_ephemeral_alloc()
-{
-    int gen = ((settings.reason == reason_oos_soh) ? (max_generation - 1) : -1);
-    if (gen == -1)
-    {
-#ifdef MULTIPLE_HEAPS
-        for (int heap_index = 0; heap_index < n_heaps; heap_index++)
-#endif //MULTIPLE_HEAPS
-        {
-            for (int i = 0; i < max_generation; i++)
-            {
-#ifdef MULTIPLE_HEAPS
-                if (g_heaps[heap_index]->get_new_allocation (i) <= 0)
-#else
-                if (get_new_allocation (i) <= 0)
-#endif //MULTIPLE_HEAPS
-                {
-                    gen = max (gen, i);
-                }
-                else
-                    break;
-            }
-        }
-    }
-    return gen;
-}
-void gc_heap::wait_to_proceed()
-{
-    assert (background_gc_done_event.IsValid());
-    assert (bgc_start_event.IsValid());
-    user_thread_wait(&ee_proceed_event, FALSE);
-}
-void gc_heap::start_c_gc()
-{
-    assert (background_gc_done_event.IsValid());
-    assert (bgc_start_event.IsValid());
-    background_gc_done_event.Wait(INFINITE, FALSE);
-    background_gc_done_event.Reset();
-    bgc_start_event.Set();
-}
-void gc_heap::do_background_gc()
-{
-    dprintf (2, ("starting a BGC"));
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-    {
-        g_heaps[i]->init_background_gc();
-    }
-#else
-    init_background_gc();
-#endif //MULTIPLE_HEAPS
-#ifdef BGC_SERVO_TUNING
-    bgc_tuning::record_bgc_start();
-#endif //BGC_SERVO_TUNING
-    start_c_gc ();
-    wait_to_proceed();
-}
-void gc_heap::kill_gc_thread()
-{
-    background_gc_done_event.CloseEvent();
-    bgc_start_event.CloseEvent();
-    bgc_threads_timeout_cs.Destroy();
-    bgc_thread = 0;
-}
-void gc_heap::bgc_thread_function()
-{
-    assert (background_gc_done_event.IsValid());
-    assert (bgc_start_event.IsValid());
-    dprintf (3, ("gc_thread thread starting..."));
-    BOOL do_exit = FALSE;
-    bool cooperative_mode = true;
-    bgc_thread_id.SetToCurrentThread();
-    dprintf (1, ("bgc_thread_id is set to %x", (uint32_t)GCToOSInterface::GetCurrentThreadIdForLogging()));
-    while (1)
-    {
-        dprintf (3, ("bgc thread: waiting..."));
-        cooperative_mode = enable_preemptive ();
-        uint32_t result = bgc_start_event.Wait(
-#ifdef _DEBUG
-#ifdef MULTIPLE_HEAPS
-                                             INFINITE,
-#else
-                                             2000,
-#endif //MULTIPLE_HEAPS
-#else //_DEBUG
-#ifdef MULTIPLE_HEAPS
-                                             INFINITE,
-#else
-                                             20000,
-#endif //MULTIPLE_HEAPS
-#endif //_DEBUG
-            FALSE);
-        dprintf (2, ("gc thread: finished waiting"));
-        if (result == WAIT_TIMEOUT)
-        {
-            dprintf (1, ("GC thread timeout"));
-            bgc_threads_timeout_cs.Enter();
-            if (!keep_bgc_threads_p)
-            {
-                dprintf (2, ("GC thread exiting"));
-                bgc_thread_running = FALSE;
-                bgc_thread = 0;
-                bgc_thread_id.Clear();
-                do_exit = TRUE;
-            }
-            bgc_threads_timeout_cs.Leave();
-            if (do_exit)
-                break;
-            else
-            {
-                dprintf (3, ("GC thread needed, not exiting"));
-                continue;
-            }
-        }
-        if (!settings.concurrent)
-        {
-            dprintf (3, ("no concurrent GC needed, exiting"));
-            break;
-        }
-        gc_background_running = TRUE;
-        dprintf (2, (ThreadStressLog::gcStartBgcThread(), heap_number,
-            generation_free_list_space (generation_of (max_generation)),
-            generation_free_obj_space (generation_of (max_generation)),
-            dd_fragmentation (dynamic_data_of (max_generation))));
-#ifdef DYNAMIC_HEAP_COUNT
-        if (n_heaps <= heap_number)
-        {
-            dprintf (9999, ("BGC thread %d idle (%d heaps) (gc%Id)", heap_number, n_heaps, VolatileLoadWithoutBarrier (&settings.gc_index)));
-            bgc_idle_thread_event.Wait(INFINITE, FALSE);
-            dprintf (9999, ("BGC thread %d waking from idle (%d heaps) (gc%Id)", heap_number, n_heaps, VolatileLoadWithoutBarrier (&settings.gc_index)));
-            continue;
-        }
-#endif //DYNAMIC_HEAP_COUNT
-        gc1();
-#ifndef DOUBLY_LINKED_FL
-        current_bgc_state = bgc_not_in_process;
-#endif //!DOUBLY_LINKED_FL
-        enable_preemptive ();
-#ifdef MULTIPLE_HEAPS
-        bgc_t_join.join(this, gc_join_done);
-        if (bgc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-        {
-            enter_spin_lock (&gc_lock);
-            dprintf (SPINLOCK_LOG, ("bgc Egc"));
-            bgc_start_event.Reset();
-            do_post_gc();
-#ifdef MULTIPLE_HEAPS
-            for (int gen = max_generation; gen < total_generation_count; gen++)
-            {
-                size_t desired_per_heap = 0;
-                size_t total_desired = 0;
-                gc_heap* hp = 0;
-                dynamic_data* dd;
-                for (int i = 0; i < n_heaps; i++)
-                {
-                    hp = g_heaps[i];
-                    dd = hp->dynamic_data_of (gen);
-                    size_t temp_total_desired = total_desired + dd_desired_allocation (dd);
-                    if (temp_total_desired < total_desired)
-                    {
-                        total_desired = (size_t)MAX_PTR;
-                        break;
-                    }
-                    total_desired = temp_total_desired;
-                }
-                desired_per_heap = Align ((total_desired/n_heaps), get_alignment_constant (FALSE));
-                if (gen >= loh_generation)
-                {
-                    desired_per_heap = exponential_smoothing (gen, dd_collection_count (dynamic_data_of (max_generation)), desired_per_heap);
-                }
-                for (int i = 0; i < n_heaps; i++)
-                {
-                    hp = gc_heap::g_heaps[i];
-                    dd = hp->dynamic_data_of (gen);
-                    dd_desired_allocation (dd) = desired_per_heap;
-                    dd_gc_new_allocation (dd) = desired_per_heap;
-                    dd_new_allocation (dd) = desired_per_heap;
-                }
-            }
-#endif //MULTIPLE_HEAPS
-#ifdef MULTIPLE_HEAPS
-            fire_pevents();
-#endif //MULTIPLE_HEAPS
-            c_write (settings.concurrent, FALSE);
-            gc_background_running = FALSE;
-            keep_bgc_threads_p = FALSE;
-            background_gc_done_event.Set();
-            dprintf (SPINLOCK_LOG, ("bgc Lgc"));
-            leave_spin_lock (&gc_lock);
-#ifdef MULTIPLE_HEAPS
-            dprintf(1, ("End of BGC"));
-            bgc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-        }
-    }
-    FIRE_EVENT(GCTerminateConcurrentThread_V1);
-    dprintf (3, ("bgc_thread thread exiting"));
-    return;
-}
-#ifdef BGC_SERVO_TUNING
-bool gc_heap::bgc_tuning::stepping_trigger (uint32_t current_memory_load, size_t current_gen2_count)
-{
-    if (!bgc_tuning::enable_fl_tuning)
-    {
-        return false;
-    }
-    bool stepping_trigger_p = false;
-    if (use_stepping_trigger_p)
-    {
-        dprintf (BGC_TUNING_LOG, ("current ml: %d, goal: %d",
-            current_memory_load, memory_load_goal));
-        if ((current_memory_load <= (memory_load_goal * 2 / 3)) ||
-            ((memory_load_goal > current_memory_load) &&
-             ((memory_load_goal - current_memory_load) > (stepping_interval * 3))))
-        {
-            int memory_load_delta = (int)current_memory_load - (int)last_stepping_mem_load;
-            if (memory_load_delta >= (int)stepping_interval)
-            {
-                stepping_trigger_p = (current_gen2_count == last_stepping_bgc_count);
-                if (stepping_trigger_p)
-                {
-                    current_gen2_count++;
-                }
-                dprintf (BGC_TUNING_LOG, ("current ml: %u - %u = %d (>= %u), gen2 count: %zu->%zu, stepping trigger: %s ",
-                    current_memory_load, last_stepping_mem_load, memory_load_delta, stepping_interval,
-                    last_stepping_bgc_count, current_gen2_count,
-                    (stepping_trigger_p ? "yes" : "no")));
-                last_stepping_mem_load = current_memory_load;
-                last_stepping_bgc_count = current_gen2_count;
-            }
-        }
-        else
-        {
-            use_stepping_trigger_p = false;
-        }
-    }
-    return stepping_trigger_p;
-}
-bool gc_heap::bgc_tuning::should_trigger_bgc_loh()
-{
-    if (fl_tuning_triggered)
-    {
-#ifdef MULTIPLE_HEAPS
-        gc_heap* hp = g_heaps[0];
-#else
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        if (!(gc_heap::background_running_p()))
-        {
-            size_t current_alloc = get_total_servo_alloc (loh_generation);
-            tuning_calculation* current_gen_calc = &gen_calc[loh_generation - max_generation];
-            if (current_alloc < current_gen_calc->last_bgc_end_alloc)
-            {
-                dprintf (BGC_TUNING_LOG, ("BTL: current alloc: %zd, last alloc: %zd?",
-                    current_alloc, current_gen_calc->last_bgc_end_alloc));
-            }
-            bool trigger_p = ((current_alloc - current_gen_calc->last_bgc_end_alloc) >= current_gen_calc->alloc_to_trigger);
-            dprintf (2, ("BTL3: LOH a %zd, la: %zd(%zd), %zd",
-                    current_alloc, current_gen_calc->last_bgc_end_alloc,
-                    (current_alloc - current_gen_calc->last_bgc_end_alloc),
-                    current_gen_calc->alloc_to_trigger));
-            if (trigger_p)
-            {
-                dprintf (BGC_TUNING_LOG, ("BTL3: LOH detected (%zd - %zd) >= %zd, TRIGGER",
-                        current_alloc, current_gen_calc->last_bgc_end_alloc, current_gen_calc->alloc_to_trigger));
-                return true;
-            }
-        }
-    }
-    return false;
-}
-bool gc_heap::bgc_tuning::should_trigger_bgc()
-{
-    if (!bgc_tuning::enable_fl_tuning || gc_heap::background_running_p())
-    {
-        return false;
-    }
-    if (settings.reason == reason_bgc_tuning_loh)
-    {
-        bgc_tuning::next_bgc_p = true;
-        dprintf (BGC_TUNING_LOG, ("BTL LOH triggered"));
-        return true;
-    }
-    if (!bgc_tuning::next_bgc_p &&
-        !fl_tuning_triggered &&
-        (gc_heap::settings.entry_memory_load >= (memory_load_goal * 2 / 3)) &&
-        (gc_heap::full_gc_counts[gc_type_background] >= 2))
-    {
-        next_bgc_p = true;
-        gen_calc[0].first_alloc_to_trigger = gc_heap::get_total_servo_alloc (max_generation);
-        gen_calc[1].first_alloc_to_trigger = gc_heap::get_total_servo_alloc (loh_generation);
-        dprintf (BGC_TUNING_LOG, ("BTL[GTC] mem high enough: %d(goal: %d), %zd BGCs done, g2a=%zd, g3a=%zd, trigger FL tuning!",
-            gc_heap::settings.entry_memory_load, memory_load_goal,
-            gc_heap::full_gc_counts[gc_type_background],
-            gen_calc[0].first_alloc_to_trigger,
-            gen_calc[1].first_alloc_to_trigger));
-    }
-    if (bgc_tuning::next_bgc_p)
-    {
-        dprintf (BGC_TUNING_LOG, ("BTL started FL tuning"));
-        return true;
-    }
-    if (!fl_tuning_triggered)
-    {
-        return false;
-    }
-    int index = 0;
-    bgc_tuning::tuning_calculation* current_gen_calc = 0;
-    index = 0;
-    current_gen_calc = &bgc_tuning::gen_calc[index];
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hp = g_heaps[0];
-#else
-    gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-    size_t current_gen1_index = dd_collection_count (hp->dynamic_data_of (max_generation - 1));
-    size_t gen1_so_far = current_gen1_index - gen1_index_last_bgc_end;
-    if (current_gen_calc->alloc_to_trigger > 0)
-    {
-        size_t current_alloc = get_total_servo_alloc (max_generation);
-        if ((current_alloc - current_gen_calc->last_bgc_end_alloc) >= current_gen_calc->alloc_to_trigger)
-        {
-            dprintf (BGC_TUNING_LOG, ("BTL2: SOH detected (%zd - %zd) >= %zd, TRIGGER",
-                    current_alloc, current_gen_calc->last_bgc_end_alloc, current_gen_calc->alloc_to_trigger));
-            settings.reason = reason_bgc_tuning_soh;
-            return true;
-        }
-    }
-    return false;
-}
-bool gc_heap::bgc_tuning::should_delay_alloc (int gen_number)
-{
-    if ((gen_number != max_generation) || !bgc_tuning::enable_fl_tuning)
-        return false;
-    if (current_c_gc_state == c_gc_state_planning)
-    {
-        int i = 0;
-#ifdef MULTIPLE_HEAPS
-        for (; i < gc_heap::n_heaps; i++)
-        {
-            gc_heap* hp = gc_heap::g_heaps[i];
-            size_t current_fl_size = generation_free_list_space (hp->generation_of (max_generation));
-            size_t last_bgc_fl_size = hp->bgc_maxgen_end_fl_size;
-#else
-        {
-            size_t current_fl_size = generation_free_list_space (generation_of (max_generation));
-            size_t last_bgc_fl_size = bgc_maxgen_end_fl_size;
-#endif //MULTIPLE_HEAPS
-            if (last_bgc_fl_size)
-            {
-                float current_flr = (float) current_fl_size / (float)last_bgc_fl_size;
-                if (current_flr < 0.4)
-                {
-                    dprintf (BGC_TUNING_LOG, ("BTL%d h%d last fl %zd, curr fl %zd (%.3f) d1",
-                            gen_number, i, last_bgc_fl_size, current_fl_size, current_flr));
-                    return true;
-                }
-            }
-        }
-    }
-    return false;
-}
-void gc_heap::bgc_tuning::update_bgc_start (int gen_number, size_t num_gen1s_since_end)
-{
-    int tuning_data_index = gen_number - max_generation;
-    tuning_calculation* current_gen_calc = &gen_calc[tuning_data_index];
-    tuning_stats* current_gen_stats = &gen_stats[tuning_data_index];
-    size_t total_generation_size = get_total_generation_size (gen_number);
-    ptrdiff_t current_bgc_fl_size = get_total_generation_fl_size (gen_number);
-    double physical_gen_flr = (double)current_bgc_fl_size * 100.0 / (double)total_generation_size;
-    ptrdiff_t artificial_additional_fl = 0;
-    if (fl_tuning_triggered)
-    {
-        artificial_additional_fl = ((current_gen_calc->end_gen_size_goal > total_generation_size) ? (current_gen_calc->end_gen_size_goal - total_generation_size) : 0);
-        total_generation_size += artificial_additional_fl;
-        current_bgc_fl_size += artificial_additional_fl;
-    }
-    current_gen_calc->current_bgc_start_flr = (double)current_bgc_fl_size * 100.0 / (double)total_generation_size;
-    size_t current_alloc = get_total_servo_alloc (gen_number);
-    dprintf (BGC_TUNING_LOG, ("BTL%d: st a: %zd, la: %zd",
-        gen_number, current_alloc, current_gen_stats->last_alloc));
-    current_gen_stats->last_alloc_end_to_start = current_alloc - current_gen_stats->last_alloc;
-    current_gen_stats->last_alloc = current_alloc;
-    current_gen_calc->actual_alloc_to_trigger = current_alloc - current_gen_calc->last_bgc_end_alloc;
-    dprintf (BGC_TUNING_LOG, ("BTL%d: st: %zd g1s (%zd->%zd/gen1) since end, flr: %.3f(afl: %zd, %.3f)",
-             gen_number, actual_num_gen1s_to_trigger,
-             current_gen_stats->last_alloc_end_to_start,
-             (num_gen1s_since_end ? (current_gen_stats->last_alloc_end_to_start / num_gen1s_since_end) : 0),
-             current_gen_calc->current_bgc_start_flr, artificial_additional_fl, physical_gen_flr));
-}
-void gc_heap::bgc_tuning::record_bgc_start()
-{
-    if (!bgc_tuning::enable_fl_tuning)
-        return;
-    uint64_t elapsed_time_so_far = GetHighPrecisionTimeStamp() - process_start_time;
-    size_t current_gen1_index = get_current_gc_index (max_generation - 1);
-    dprintf (BGC_TUNING_LOG, ("BTL: g2t[st][g1 %zd]: %0.3f minutes",
-        current_gen1_index,
-        (double)elapsed_time_so_far / (double)1000000 / (double)60));
-    actual_num_gen1s_to_trigger = current_gen1_index - gen1_index_last_bgc_end;
-    gen1_index_last_bgc_start = current_gen1_index;
-    update_bgc_start (max_generation, actual_num_gen1s_to_trigger);
-    update_bgc_start (loh_generation, actual_num_gen1s_to_trigger);
-}
-double convert_range (double lower, double upper, double num, double percentage)
-{
-    double d = num - lower;
-    if (d < 0.0)
-        return 0.0;
-    else
-    {
-        d = min ((upper - lower), d);
-        return (d * percentage);
-    }
-}
-double calculate_gradual_d (double delta_double, double step)
-{
-    bool changed_sign = false;
-    if (delta_double < 0.0)
-    {
-        delta_double = -delta_double;
-        changed_sign = true;
-    }
-    double res = 0;
-    double current_lower_limit = 0;
-    double current_ratio = 1.0;
-    while (current_ratio > 0.22)
-    {
-        res += convert_range (current_lower_limit, (current_lower_limit + step), delta_double, current_ratio);
-        current_lower_limit += step;
-        current_ratio *= 0.6;
-    }
-    if (changed_sign)
-        res = -res;
-    return res;
-}
-void gc_heap::bgc_tuning::update_bgc_sweep_start (int gen_number, size_t num_gen1s_since_start)
-{
-    int tuning_data_index = gen_number - max_generation;
-    tuning_calculation* current_gen_calc = &gen_calc[tuning_data_index];
-    tuning_stats* current_gen_stats = &gen_stats[tuning_data_index];
-    size_t total_generation_size = 0;
-    ptrdiff_t current_bgc_fl_size = 0;
-    total_generation_size = get_total_generation_size (gen_number);
-    current_bgc_fl_size = get_total_generation_fl_size (gen_number);
-    double physical_gen_flr = (double)current_bgc_fl_size * 100.0 / (double)total_generation_size;
-    ptrdiff_t artificial_additional_fl = 0;
-    if (fl_tuning_triggered)
-    {
-        artificial_additional_fl = ((current_gen_calc->end_gen_size_goal > total_generation_size) ? (current_gen_calc->end_gen_size_goal - total_generation_size) : 0);
-        total_generation_size += artificial_additional_fl;
-        current_bgc_fl_size += artificial_additional_fl;
-    }
-    current_gen_calc->current_bgc_sweep_flr = (double)current_bgc_fl_size * 100.0 / (double)total_generation_size;
-    size_t current_alloc = get_total_servo_alloc (gen_number);
-    dprintf (BGC_TUNING_LOG, ("BTL%d: sw a: %zd, la: %zd",
-        gen_number, current_alloc, current_gen_stats->last_alloc));
-    current_gen_stats->last_alloc_start_to_sweep = current_alloc - current_gen_stats->last_alloc;
-    current_gen_stats->last_alloc = 0;
-#ifdef SIMPLE_DPRINTF
-    dprintf (BGC_TUNING_LOG, ("BTL%d: sflr: %.3f%%->%.3f%% (%zd->%zd, %zd->%zd) (%zd:%zd-%zd/gen1) since start (afl: %zd, %.3f)",
-             gen_number,
-             current_gen_calc->last_bgc_flr, current_gen_calc->current_bgc_sweep_flr,
-             current_gen_calc->last_bgc_size, total_generation_size,
-             current_gen_stats->last_bgc_fl_size, current_bgc_fl_size,
-             num_gen1s_since_start, current_gen_stats->last_alloc_start_to_sweep,
-             (num_gen1s_since_start? (current_gen_stats->last_alloc_start_to_sweep / num_gen1s_since_start) : 0),
-             artificial_additional_fl, physical_gen_flr));
-#endif //SIMPLE_DPRINTF
-}
-void gc_heap::bgc_tuning::record_bgc_sweep_start()
-{
-    if (!bgc_tuning::enable_fl_tuning)
-        return;
-    size_t current_gen1_index = get_current_gc_index (max_generation - 1);
-    size_t num_gen1s_since_start = current_gen1_index - gen1_index_last_bgc_start;
-    gen1_index_last_bgc_sweep = current_gen1_index;
-    uint64_t elapsed_time_so_far = GetHighPrecisionTimeStamp() - process_start_time;
-    dprintf (BGC_TUNING_LOG, ("BTL: g2t[sw][g1 %zd]: %0.3f minutes",
-        current_gen1_index,
-        (double)elapsed_time_so_far / (double)1000000 / (double)60));
-    update_bgc_sweep_start (max_generation, num_gen1s_since_start);
-    update_bgc_sweep_start (loh_generation, num_gen1s_since_start);
-}
-void gc_heap::bgc_tuning::calculate_tuning (int gen_number, bool use_this_loop_p)
-{
-    BOOL use_kd_p = enable_kd;
-    BOOL use_ki_p = enable_ki;
-    BOOL use_smooth_p = enable_smooth;
-    BOOL use_tbh_p = enable_tbh;
-    BOOL use_ff_p = enable_ff;
-    int tuning_data_index = gen_number - max_generation;
-    tuning_calculation* current_gen_calc = &gen_calc[tuning_data_index];
-    tuning_stats* current_gen_stats = &gen_stats[tuning_data_index];
-    bgc_size_data* data = &current_bgc_end_data[tuning_data_index];
-    size_t total_generation_size = data->gen_size;
-    size_t current_bgc_fl = data->gen_fl_size;
-    size_t current_bgc_surv_size = get_total_surv_size (gen_number);
-    size_t current_bgc_begin_data_size = get_total_begin_data_size (gen_number);
-    size_t current_alloc = get_total_servo_alloc (gen_number);
-    dprintf (BGC_TUNING_LOG, ("BTL%d: en a: %zd, la: %zd, lbgca: %zd",
-        gen_number, current_alloc, current_gen_stats->last_alloc, current_gen_calc->last_bgc_end_alloc));
-    double current_bgc_surv_rate = (current_bgc_begin_data_size == 0) ?
-                                    0 : ((double)current_bgc_surv_size * 100.0 / (double)current_bgc_begin_data_size);
-    current_gen_stats->last_alloc_sweep_to_end = current_alloc - current_gen_stats->last_alloc;
-    size_t gen1_index = get_current_gc_index (max_generation - 1);
-    size_t gen2_index = get_current_gc_index (max_generation);
-    size_t num_gen1s_since_sweep = gen1_index - gen1_index_last_bgc_sweep;
-    size_t num_gen1s_bgc_end = gen1_index - gen1_index_last_bgc_end;
-    size_t gen_end_size_goal = current_gen_calc->end_gen_size_goal;
-    double gen_sweep_flr_goal = current_gen_calc->sweep_flr_goal;
-    size_t last_gen_alloc_to_trigger = current_gen_calc->alloc_to_trigger;
-    size_t gen_actual_alloc_to_trigger = current_gen_calc->actual_alloc_to_trigger;
-    size_t last_gen_alloc_to_trigger_0 = current_gen_calc->alloc_to_trigger_0;
-    double current_end_to_sweep_flr = current_gen_calc->last_bgc_flr - current_gen_calc->current_bgc_sweep_flr;
-    bool current_sweep_above_p = (current_gen_calc->current_bgc_sweep_flr > gen_sweep_flr_goal);
-#ifdef SIMPLE_DPRINTF
-    dprintf (BGC_TUNING_LOG, ("BTL%d: sflr: c %.3f (%s), p %s, palloc: %zd, aalloc %zd(%s)",
-        gen_number,
-        current_gen_calc->current_bgc_sweep_flr,
-        (current_sweep_above_p ? "above" : "below"),
-        (current_gen_calc->last_sweep_above_p ? "above" : "below"),
-        last_gen_alloc_to_trigger,
-        current_gen_calc->actual_alloc_to_trigger,
-        (use_this_loop_p ? "this" : "last")));
-    dprintf (BGC_TUNING_LOG, ("BTL%d-en[g1: %zd, g2: %zd]: end fl: %zd (%zd: S-%zd, %.3f%%->%.3f%%)",
-            gen_number,
-            gen1_index, gen2_index, current_bgc_fl,
-            total_generation_size, current_bgc_surv_size,
-            current_gen_stats->last_bgc_surv_rate, current_bgc_surv_rate));
-    dprintf (BGC_TUNING_LOG, ("BTLS%d sflr: %.3f, end-start: %zd(%zd), start-sweep: %zd(%zd), sweep-end: %zd(%zd)",
-            gen_number,
-            current_gen_calc->current_bgc_sweep_flr,
-            (gen1_index_last_bgc_start - gen1_index_last_bgc_end), current_gen_stats->last_alloc_end_to_start,
-            (gen1_index_last_bgc_sweep - gen1_index_last_bgc_start), current_gen_stats->last_alloc_start_to_sweep,
-            num_gen1s_since_sweep, current_gen_stats->last_alloc_sweep_to_end));
-#endif //SIMPLE_DPRINTF
-    size_t saved_alloc_to_trigger = 0;
-    double current_alloc_to_trigger = 0.0;
-    if (!fl_tuning_triggered && use_tbh_p)
-    {
-        current_gen_calc->alloc_to_trigger_0 = current_gen_calc->actual_alloc_to_trigger;
-        dprintf (BGC_TUNING_LOG, ("BTL%d[g1: %zd]: not in FL tuning yet, setting alloc_to_trigger_0 to %zd",
-                 gen_number,
-                 gen1_index, current_gen_calc->alloc_to_trigger_0));
-    }
-    if (fl_tuning_triggered)
-    {
-        BOOL tuning_kd_finished_p = FALSE;
-        double max_alloc_to_trigger = ((double)current_bgc_fl * (100 - gen_sweep_flr_goal) / 100.0);
-        double min_alloc_to_trigger = (double)current_bgc_fl * 0.05;
-        {
-            if (current_gen_calc->current_bgc_sweep_flr < 0.0)
-            {
-                dprintf (BGC_TUNING_LOG, ("BTL%d: sflr is %.3f!!! < 0, make it 0", gen_number, current_gen_calc->current_bgc_sweep_flr));
-                current_gen_calc->current_bgc_sweep_flr = 0.0;
-            }
-            double adjusted_above_goal_kp = above_goal_kp;
-            double above_goal_distance = current_gen_calc->current_bgc_sweep_flr - gen_sweep_flr_goal;
-            if (use_ki_p)
-            {
-                if (current_gen_calc->above_goal_accu_error > max_alloc_to_trigger)
-                {
-                    dprintf (BGC_TUNING_LOG, ("g%d: ae TB! %.1f->%.1f", gen_number, current_gen_calc->above_goal_accu_error, max_alloc_to_trigger));
-                }
-                else if (current_gen_calc->above_goal_accu_error < min_alloc_to_trigger)
-                {
-                    dprintf (BGC_TUNING_LOG, ("g%d: ae TS! %.1f->%.1f", gen_number, current_gen_calc->above_goal_accu_error, min_alloc_to_trigger));
-                }
-                current_gen_calc->above_goal_accu_error = min (max_alloc_to_trigger, current_gen_calc->above_goal_accu_error);
-                current_gen_calc->above_goal_accu_error = max (min_alloc_to_trigger, current_gen_calc->above_goal_accu_error);
-                double above_goal_ki_gain = above_goal_ki * above_goal_distance * current_bgc_fl;
-                double temp_accu_error = current_gen_calc->above_goal_accu_error + above_goal_ki_gain;
-                if ((temp_accu_error > min_alloc_to_trigger) &&
-                    (temp_accu_error < max_alloc_to_trigger))
-                {
-                    current_gen_calc->above_goal_accu_error = temp_accu_error;
-                }
-                else
-                {
-                    dprintf (BGC_TUNING_LOG, ("g%d: aae + %.1f=%.1f, exc", gen_number,
-                            above_goal_ki_gain,
-                            temp_accu_error));
-                }
-            }
-            {
-                saved_alloc_to_trigger = current_gen_calc->alloc_to_trigger;
-                current_alloc_to_trigger = adjusted_above_goal_kp * above_goal_distance * current_bgc_fl;
-                dprintf (BGC_TUNING_LOG, ("BTL%d: sflr %.3f above * %.4f * %zd = %zd bytes in alloc, la: %zd(+%zd), laa: %zd(+%zd)",
-                        gen_number,
-                        (current_gen_calc->current_bgc_sweep_flr - (double)gen_sweep_flr_goal),
-                        adjusted_above_goal_kp,
-                        current_bgc_fl,
-                        (size_t)current_alloc_to_trigger,
-                        saved_alloc_to_trigger,
-                        (size_t)(current_alloc_to_trigger - (double)saved_alloc_to_trigger),
-                        gen_actual_alloc_to_trigger,
-                        (gen_actual_alloc_to_trigger - saved_alloc_to_trigger)));
-                if (use_ki_p)
-                {
-                    current_alloc_to_trigger += current_gen_calc->above_goal_accu_error;
-                    dprintf (BGC_TUNING_LOG, ("BTL%d: +accu err %zd=%zd",
-                            gen_number,
-                            (size_t)(current_gen_calc->above_goal_accu_error),
-                            (size_t)current_alloc_to_trigger));
-                }
-            }
-            if (use_tbh_p)
-            {
-                if (current_gen_calc->last_sweep_above_p != current_sweep_above_p)
-                {
-                    size_t new_alloc_to_trigger_0 = (last_gen_alloc_to_trigger + last_gen_alloc_to_trigger_0) / 2;
-                    dprintf (BGC_TUNING_LOG, ("BTL%d: tbh crossed SP, setting both to %zd", gen_number, new_alloc_to_trigger_0));
-                    current_gen_calc->alloc_to_trigger_0 = new_alloc_to_trigger_0;
-                    current_gen_calc->alloc_to_trigger = new_alloc_to_trigger_0;
-                }
-                tuning_kd_finished_p = TRUE;
-            }
-        }
-        if (!tuning_kd_finished_p)
-        {
-            if (use_kd_p)
-            {
-                saved_alloc_to_trigger = last_gen_alloc_to_trigger;
-                size_t alloc_delta = saved_alloc_to_trigger - gen_actual_alloc_to_trigger;
-                double adjust_ratio = (double)alloc_delta / (double)gen_actual_alloc_to_trigger;
-                double saved_adjust_ratio = adjust_ratio;
-                if (enable_gradual_d)
-                {
-                    adjust_ratio = calculate_gradual_d (adjust_ratio, above_goal_kd);
-                    dprintf (BGC_TUNING_LOG, ("BTL%d: gradual kd - reduced from %.3f to %.3f",
-                            gen_number, saved_adjust_ratio, adjust_ratio));
-                }
-                else
-                {
-                    double kd = above_goal_kd;
-                    double neg_kd = 0 - kd;
-                    if (adjust_ratio > kd) adjust_ratio = kd;
-                    if (adjust_ratio < neg_kd) adjust_ratio = neg_kd;
-                    dprintf (BGC_TUNING_LOG, ("BTL%d: kd - reduced from %.3f to %.3f",
-                            gen_number, saved_adjust_ratio, adjust_ratio));
-                }
-                current_gen_calc->alloc_to_trigger = (size_t)((double)gen_actual_alloc_to_trigger * (1 + adjust_ratio));
-                dprintf (BGC_TUNING_LOG, ("BTL%d: kd %.3f, reduced it to %.3f * %zd, adjust %zd->%zd",
-                        gen_number, saved_adjust_ratio,
-                        adjust_ratio, gen_actual_alloc_to_trigger,
-                        saved_alloc_to_trigger, current_gen_calc->alloc_to_trigger));
-            }
-            if (use_smooth_p && use_this_loop_p)
-            {
-                saved_alloc_to_trigger = current_gen_calc->alloc_to_trigger;
-                size_t gen_smoothed_alloc_to_trigger = current_gen_calc->smoothed_alloc_to_trigger;
-                double current_num_gen1s_smooth_factor = (num_gen1s_smooth_factor > (double)num_bgcs_since_tuning_trigger) ?
-                                                        (double)num_bgcs_since_tuning_trigger : num_gen1s_smooth_factor;
-                current_gen_calc->smoothed_alloc_to_trigger = (size_t)((double)saved_alloc_to_trigger / current_num_gen1s_smooth_factor +
-                    ((double)gen_smoothed_alloc_to_trigger / current_num_gen1s_smooth_factor) * (current_num_gen1s_smooth_factor - 1.0));
-                dprintf (BGC_TUNING_LOG, ("BTL%d: smoothed %zd / %.3f + %zd / %.3f * %.3f adjust %zd->%zd",
-                    gen_number, saved_alloc_to_trigger, current_num_gen1s_smooth_factor,
-                    gen_smoothed_alloc_to_trigger, current_num_gen1s_smooth_factor,
-                    (current_num_gen1s_smooth_factor - 1.0),
-                    saved_alloc_to_trigger, current_gen_calc->smoothed_alloc_to_trigger));
-                current_gen_calc->alloc_to_trigger = current_gen_calc->smoothed_alloc_to_trigger;
-            }
-        }
-        if (use_ff_p)
-        {
-            double next_end_to_sweep_flr = data->gen_flr - gen_sweep_flr_goal;
-            if (next_end_to_sweep_flr > 0.0)
-            {
-                saved_alloc_to_trigger = current_gen_calc->alloc_to_trigger;
-                double ff_ratio = next_end_to_sweep_flr / current_end_to_sweep_flr - 1;
-                if (use_this_loop_p)
-                {
-                    double ff_step = above_goal_ff * 0.5;
-                    double adjusted_above_goal_ff = above_goal_ff;
-                    if (ff_ratio > 0)
-                        adjusted_above_goal_ff -= ff_step;
-                    else
-                        adjusted_above_goal_ff += ff_step;
-                    double adjusted_ff_ratio = ff_ratio * adjusted_above_goal_ff;
-                    current_gen_calc->alloc_to_trigger = saved_alloc_to_trigger + (size_t)((double)saved_alloc_to_trigger * adjusted_ff_ratio);
-                    dprintf (BGC_TUNING_LOG, ("BTL%d: ff (%.3f / %.3f - 1) * %.3f = %.3f adjust %zd->%zd",
-                        gen_number, next_end_to_sweep_flr, current_end_to_sweep_flr, adjusted_above_goal_ff, adjusted_ff_ratio,
-                        saved_alloc_to_trigger, current_gen_calc->alloc_to_trigger));
-                }
-            }
-        }
-        if (use_this_loop_p)
-        {
-            if (current_alloc_to_trigger > max_alloc_to_trigger)
-            {
-                dprintf (BGC_TUNING_LOG, ("BTL%d: TB! %.1f -> %.1f",
-                    gen_number, current_alloc_to_trigger, max_alloc_to_trigger));
-                current_alloc_to_trigger = max_alloc_to_trigger;
-            }
-            if (current_alloc_to_trigger < min_alloc_to_trigger)
-            {
-                dprintf (BGC_TUNING_LOG, ("BTL%d: TS! %zd -> %zd",
-                        gen_number, (ptrdiff_t)current_alloc_to_trigger, (size_t)min_alloc_to_trigger));
-                current_alloc_to_trigger = min_alloc_to_trigger;
-            }
-            current_gen_calc->alloc_to_trigger = (size_t)current_alloc_to_trigger;
-        }
-        else
-        {
-            dprintf (BGC_TUNING_LOG, ("BTL%d: ag, revert %zd->%zd",
-                gen_number, current_gen_calc->alloc_to_trigger, last_gen_alloc_to_trigger));
-            current_gen_calc->alloc_to_trigger = last_gen_alloc_to_trigger;
-        }
-    }
-    if (next_bgc_p)
-    {
-        size_t first_alloc = (size_t)((double)current_gen_calc->first_alloc_to_trigger * 0.75);
-        size_t min_first_alloc = current_bgc_fl / 20;
-        current_gen_calc->alloc_to_trigger = max (first_alloc, min_first_alloc);
-        dprintf (BGC_TUNING_LOG, ("BTL%d[g1: %zd]: BGC end, trigger FL, set gen%d alloc to max (0.75 of first: %zd, 5%% fl: %zd), actual alloc: %zd",
-            gen_number, gen1_index, gen_number,
-            first_alloc, min_first_alloc,
-            current_gen_calc->actual_alloc_to_trigger));
-    }
-    dprintf (BGC_TUNING_LOG, ("BTL%d* %zd, %.3f, %.3f, %.3f, %.3f, %.3f, %zd, %zd, %zd, %zd",
-                              gen_number,
-                              total_generation_size,
-                              current_gen_calc->current_bgc_start_flr,
-                              current_gen_calc->current_bgc_sweep_flr,
-                              current_bgc_end_data[tuning_data_index].gen_flr,
-                              current_gen_stats->last_gen_increase_flr,
-                              current_bgc_surv_rate,
-                              actual_num_gen1s_to_trigger,
-                              num_gen1s_bgc_end,
-                              gen_actual_alloc_to_trigger,
-                              current_gen_calc->alloc_to_trigger));
-    gen1_index_last_bgc_end = gen1_index;
-    current_gen_calc->last_bgc_size = total_generation_size;
-    current_gen_calc->last_bgc_flr = current_bgc_end_data[tuning_data_index].gen_flr;
-    current_gen_calc->last_sweep_above_p = current_sweep_above_p;
-    current_gen_calc->last_bgc_end_alloc = current_alloc;
-    current_gen_stats->last_bgc_physical_size = data->gen_physical_size;
-    current_gen_stats->last_alloc_end_to_start = 0;
-    current_gen_stats->last_alloc_start_to_sweep = 0;
-    current_gen_stats->last_alloc_sweep_to_end = 0;
-    current_gen_stats->last_alloc = current_alloc;
-    current_gen_stats->last_bgc_fl_size = current_bgc_end_data[tuning_data_index].gen_fl_size;
-    current_gen_stats->last_bgc_surv_rate = current_bgc_surv_rate;
-    current_gen_stats->last_gen_increase_flr = 0;
-}
-void gc_heap::bgc_tuning::init_bgc_end_data (int gen_number, bool use_this_loop_p)
-{
-    int index = gen_number - max_generation;
-    bgc_size_data* data = &current_bgc_end_data[index];
-    size_t physical_size = get_total_generation_size (gen_number);
-    ptrdiff_t physical_fl_size = get_total_generation_fl_size (gen_number);
-    data->gen_actual_phys_fl_size = physical_fl_size;
-    if (fl_tuning_triggered && !use_this_loop_p)
-    {
-        tuning_calculation* current_gen_calc = &gen_calc[gen_number - max_generation];
-        if (current_gen_calc->actual_alloc_to_trigger > current_gen_calc->alloc_to_trigger)
-        {
-            dprintf (BGC_TUNING_LOG, ("BTL%d: gen alloc also exceeded %zd (la: %zd), no action",
-                gen_number, current_gen_calc->actual_alloc_to_trigger, current_gen_calc->alloc_to_trigger));
-        }
-        else
-        {
-            size_t remaining_alloc = current_gen_calc->alloc_to_trigger -
-                                     current_gen_calc->actual_alloc_to_trigger;
-            size_t gen_size = current_gen_calc->end_gen_size_goal;
-            double sweep_flr = current_gen_calc->current_bgc_sweep_flr;
-            size_t sweep_fl_size = (size_t)((double)gen_size * sweep_flr / 100.0);
-            if (sweep_fl_size < remaining_alloc)
-            {
-                dprintf (BGC_TUNING_LOG, ("BTL%d: sweep fl %zd < remain alloc %zd", gen_number, sweep_fl_size, remaining_alloc));
-                remaining_alloc = sweep_fl_size - (10 * 1024);
-            }
-            size_t new_sweep_fl_size = sweep_fl_size - remaining_alloc;
-            ptrdiff_t signed_new_sweep_fl_size = sweep_fl_size - remaining_alloc;
-            double new_current_bgc_sweep_flr = (double)new_sweep_fl_size * 100.0 / (double)gen_size;
-            double signed_new_current_bgc_sweep_flr = (double)signed_new_sweep_fl_size * 100.0 / (double)gen_size;
-            dprintf (BGC_TUNING_LOG, ("BTL%d: sg: %zd(%zd), sfl: %zd->%zd(%zd)(%.3f->%.3f(%.3f)), la: %zd, aa: %zd",
-                gen_number, gen_size, physical_size, sweep_fl_size,
-                new_sweep_fl_size, signed_new_sweep_fl_size,
-                sweep_flr, new_current_bgc_sweep_flr, signed_new_current_bgc_sweep_flr,
-                current_gen_calc->alloc_to_trigger, current_gen_calc->actual_alloc_to_trigger));
-            current_gen_calc->actual_alloc_to_trigger = current_gen_calc->alloc_to_trigger;
-            current_gen_calc->current_bgc_sweep_flr = new_current_bgc_sweep_flr;
-            size_t current_bgc_surv_size = get_total_surv_size (gen_number);
-            size_t current_bgc_begin_data_size = get_total_begin_data_size (gen_number);
-            double current_bgc_surv_rate = (current_bgc_begin_data_size == 0) ?
-                                            0 : ((double)current_bgc_surv_size / (double)current_bgc_begin_data_size);
-            size_t remaining_alloc_surv = (size_t)((double)remaining_alloc * current_bgc_surv_rate);
-            physical_fl_size -= remaining_alloc_surv;
-            dprintf (BGC_TUNING_LOG, ("BTL%d: asfl %zd-%zd=%zd, flr %.3f->%.3f, %.3f%% s, fl %zd-%zd->%zd",
-                gen_number, sweep_fl_size, remaining_alloc, new_sweep_fl_size,
-                sweep_flr, current_gen_calc->current_bgc_sweep_flr,
-                (current_bgc_surv_rate * 100.0),
-                (physical_fl_size + remaining_alloc_surv),
-                remaining_alloc_surv, physical_fl_size));
-        }
-    }
-    double physical_gen_flr = (double)physical_fl_size * 100.0 / (double)physical_size;
-    data->gen_physical_size = physical_size;
-    data->gen_physical_fl_size = physical_fl_size;
-    data->gen_physical_flr = physical_gen_flr;
-}
-void gc_heap::bgc_tuning::calc_end_bgc_fl (int gen_number)
-{
-    int index = gen_number - max_generation;
-    bgc_size_data* data = &current_bgc_end_data[index];
-    tuning_calculation* current_gen_calc = &gen_calc[gen_number - max_generation];
-    size_t virtual_size = current_gen_calc->end_gen_size_goal;
-    size_t physical_size = data->gen_physical_size;
-    ptrdiff_t physical_fl_size = data->gen_physical_fl_size;
-    ptrdiff_t virtual_fl_size = (ptrdiff_t)virtual_size - (ptrdiff_t)physical_size;
-    ptrdiff_t end_gen_fl_size = physical_fl_size + virtual_fl_size;
-    if (end_gen_fl_size < 0)
-    {
-        end_gen_fl_size = 0;
-    }
-    data->gen_size = virtual_size;
-    data->gen_fl_size = end_gen_fl_size;
-    data->gen_flr = (double)(data->gen_fl_size) * 100.0 / (double)(data->gen_size);
-    dprintf (BGC_TUNING_LOG, ("BTL%d: vfl: %zd, size %zd->%zd, fl %zd->%zd, flr %.3f->%.3f",
-        gen_number, virtual_fl_size,
-        data->gen_physical_size, data->gen_size,
-        data->gen_physical_fl_size, data->gen_fl_size,
-        data->gen_physical_flr, data->gen_flr));
-}
-double gc_heap::bgc_tuning::calculate_ml_tuning (uint64_t current_available_physical, bool reduce_p,
-                                                 ptrdiff_t* _vfl_from_kp, ptrdiff_t* _vfl_from_ki)
-{
-    ptrdiff_t error = (ptrdiff_t)(current_available_physical - available_memory_goal);
-    size_t gen2_physical_size = current_bgc_end_data[0].gen_physical_size;
-    size_t gen3_physical_size = current_bgc_end_data[1].gen_physical_size;
-    double max_output = (double)(total_physical_mem - available_memory_goal -
-                                 gen2_physical_size - gen3_physical_size);
-    double error_ratio = (double)error / (double)total_physical_mem;
-    bool include_in_i_p = ((error_ratio > 0.005) || (error_ratio < -0.005));
-    dprintf (BGC_TUNING_LOG, ("total phy %zd, mem goal: %zd, curr phy: %zd, g2 phy: %zd, g3 phy: %zd",
-            (size_t)total_physical_mem, (size_t)available_memory_goal,
-            (size_t)current_available_physical,
-            gen2_physical_size, gen3_physical_size));
-    dprintf (BGC_TUNING_LOG, ("BTL: Max output: %zd, ER %zd / %zd = %.3f, %s",
-            (size_t)max_output,
-            error, available_memory_goal, error_ratio,
-            (include_in_i_p ? "inc" : "exc")));
-    if (include_in_i_p)
-    {
-        double error_ki = ml_ki * (double)error;
-        double temp_accu_error = accu_error + error_ki;
-        if ((temp_accu_error > 0) && (temp_accu_error < max_output))
-            accu_error = temp_accu_error;
-        else
-        {
-            dprintf (BGC_TUNING_LOG, ("mae + %zd=%zd, exc",
-                    (size_t)error_ki, (size_t)temp_accu_error));
-        }
-    }
-    if (reduce_p)
-    {
-        double saved_accu_error = accu_error;
-        accu_error = accu_error * 2.0 / 3.0;
-        panic_activated_p = false;
-        accu_error_panic = 0;
-        dprintf (BGC_TUNING_LOG, ("BTL reduced accu ki %zd->%zd", (ptrdiff_t)saved_accu_error, (ptrdiff_t)accu_error));
-    }
-    if (panic_activated_p)
-        accu_error_panic += (double)error;
-    else
-        accu_error_panic = 0.0;
-    double vfl_from_kp = (double)error * ml_kp;
-    double total_virtual_fl_size = vfl_from_kp + accu_error;
-    if (total_virtual_fl_size < 0)
-    {
-        dprintf (BGC_TUNING_LOG, ("BTL vfl %zd < 0", (size_t)total_virtual_fl_size));
-        total_virtual_fl_size = 0;
-    }
-    else if (total_virtual_fl_size > max_output)
-    {
-        dprintf (BGC_TUNING_LOG, ("BTL vfl %zd > max", (size_t)total_virtual_fl_size));
-        total_virtual_fl_size = max_output;
-    }
-    *_vfl_from_kp = (ptrdiff_t)vfl_from_kp;
-    *_vfl_from_ki = (ptrdiff_t)accu_error;
-    return total_virtual_fl_size;
-}
-void gc_heap::bgc_tuning::set_total_gen_sizes (bool use_gen2_loop_p, bool use_gen3_loop_p)
-{
-    size_t gen2_physical_size = current_bgc_end_data[0].gen_physical_size;
-    size_t gen3_physical_size = 0;
-    ptrdiff_t gen3_virtual_fl_size = 0;
-    gen3_physical_size = current_bgc_end_data[1].gen_physical_size;
-    double gen2_size_ratio = (double)gen2_physical_size / ((double)gen2_physical_size + (double)gen3_physical_size);
-    uint32_t current_memory_load = settings.entry_memory_load;
-    uint64_t current_available_physical = settings.entry_available_physical_mem;
-    panic_activated_p = (current_memory_load >= (memory_load_goal + memory_load_goal_slack));
-    if (panic_activated_p)
-    {
-        dprintf (BGC_TUNING_LOG, ("BTL: exceeded slack %zd >= (%zd + %zd)",
-            (size_t)current_memory_load, (size_t)memory_load_goal,
-            (size_t)memory_load_goal_slack));
-    }
-    ptrdiff_t vfl_from_kp = 0;
-    ptrdiff_t vfl_from_ki = 0;
-    double total_virtual_fl_size = calculate_ml_tuning (current_available_physical, false, &vfl_from_kp, &vfl_from_ki);
-    if (use_gen2_loop_p || use_gen3_loop_p)
-    {
-        if (use_gen2_loop_p)
-        {
-            gen2_ratio_correction += ratio_correction_step;
-        }
-        else
-        {
-            gen2_ratio_correction -= ratio_correction_step;
-        }
-        dprintf (BGC_TUNING_LOG, ("BTL: rc: g2 ratio %.3f%% + %d%% = %.3f%%",
-            (gen2_size_ratio * 100.0), (int)(gen2_ratio_correction * 100.0), ((gen2_size_ratio + gen2_ratio_correction) * 100.0)));
-        gen2_ratio_correction = min (0.99, gen2_ratio_correction);
-        gen2_ratio_correction = max (-0.99, gen2_ratio_correction);
-        dprintf (BGC_TUNING_LOG, ("BTL: rc again: g2 ratio %.3f%% + %d%% = %.3f%%",
-            (gen2_size_ratio * 100.0), (int)(gen2_ratio_correction * 100.0), ((gen2_size_ratio + gen2_ratio_correction) * 100.0)));
-        gen2_size_ratio += gen2_ratio_correction;
-        if (gen2_size_ratio <= 0.0)
-        {
-            gen2_size_ratio = 0.01;
-            dprintf (BGC_TUNING_LOG, ("BTL: rc: g2 ratio->0.01"));
-        }
-        if (gen2_size_ratio >= 1.0)
-        {
-            gen2_size_ratio = 0.99;
-            dprintf (BGC_TUNING_LOG, ("BTL: rc: g2 ratio->0.99"));
-        }
-    }
-    ptrdiff_t gen2_virtual_fl_size = (ptrdiff_t)(total_virtual_fl_size * gen2_size_ratio);
-    gen3_virtual_fl_size = (ptrdiff_t)(total_virtual_fl_size * (1.0 - gen2_size_ratio));
-    if (gen2_virtual_fl_size < 0)
-    {
-        ptrdiff_t saved_gen2_virtual_fl_size = gen2_virtual_fl_size;
-        ptrdiff_t half_gen2_physical_size = (ptrdiff_t)((double)gen2_physical_size * 0.5);
-        if (-gen2_virtual_fl_size > half_gen2_physical_size)
-        {
-            gen2_virtual_fl_size = -half_gen2_physical_size;
-        }
-        dprintf (BGC_TUNING_LOG, ("BTL2: n_vfl %zd(%zd)->%zd", saved_gen2_virtual_fl_size, half_gen2_physical_size, gen2_virtual_fl_size));
-        gen2_virtual_fl_size = 0;
-    }
-    if (gen3_virtual_fl_size < 0)
-    {
-        ptrdiff_t saved_gen3_virtual_fl_size = gen3_virtual_fl_size;
-        ptrdiff_t half_gen3_physical_size = (ptrdiff_t)((double)gen3_physical_size * 0.5);
-        if (-gen3_virtual_fl_size > half_gen3_physical_size)
-        {
-            gen3_virtual_fl_size = -half_gen3_physical_size;
-        }
-        dprintf (BGC_TUNING_LOG, ("BTL3: n_vfl %zd(%zd)->%zd", saved_gen3_virtual_fl_size, half_gen3_physical_size, gen3_virtual_fl_size));
-        gen3_virtual_fl_size = 0;
-    }
-    gen_calc[0].end_gen_size_goal = gen2_physical_size + gen2_virtual_fl_size;
-    gen_calc[1].end_gen_size_goal = gen3_physical_size + gen3_virtual_fl_size;
-    calc_end_bgc_fl (max_generation);
-    calc_end_bgc_fl (loh_generation);
-#ifdef SIMPLE_DPRINTF
-    dprintf (BGC_TUNING_LOG, ("BTL: ml: %d (g: %d)(%s), a: %zd (g: %zd, elg: %zd+%zd=%zd, %zd+%zd=%zd, pi=%zd), vfl: %zd=%zd+%zd",
-        current_memory_load, memory_load_goal,
-        ((current_available_physical > available_memory_goal) ? "above" : "below"),
-        current_available_physical, available_memory_goal,
-        gen2_physical_size, gen2_virtual_fl_size, gen_calc[0].end_gen_size_goal,
-        gen3_physical_size, gen3_virtual_fl_size, gen_calc[1].end_gen_size_goal,
-        (ptrdiff_t)accu_error_panic,
-        (ptrdiff_t)total_virtual_fl_size, vfl_from_kp, vfl_from_ki));
-#endif //SIMPLE_DPRINTF
-}
-bool gc_heap::bgc_tuning::should_trigger_ngc2()
-{
-    return panic_activated_p;
-}
-void gc_heap::bgc_tuning::convert_to_fl (bool use_gen2_loop_p, bool use_gen3_loop_p)
-{
-    size_t current_bgc_count = full_gc_counts[gc_type_background];
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-        hp->bgc_maxgen_end_fl_size = generation_free_list_space (hp->generation_of (max_generation));
-    }
-#else
-    bgc_maxgen_end_fl_size = generation_free_list_space (generation_of (max_generation));
-#endif //MULTIPLE_HEAPS
-    init_bgc_end_data (max_generation, use_gen2_loop_p);
-    init_bgc_end_data (loh_generation, use_gen3_loop_p);
-    set_total_gen_sizes (use_gen2_loop_p, use_gen3_loop_p);
-    dprintf (BGC_TUNING_LOG, ("BTL: gen2 %zd, fl %zd(%.3f)->%zd; gen3 %zd, fl %zd(%.3f)->%zd, %zd BGCs",
-        current_bgc_end_data[0].gen_size, current_bgc_end_data[0].gen_fl_size,
-        current_bgc_end_data[0].gen_flr, gen_calc[0].end_gen_size_goal,
-        current_bgc_end_data[1].gen_size, current_bgc_end_data[1].gen_fl_size,
-        current_bgc_end_data[1].gen_flr, gen_calc[1].end_gen_size_goal,
-        current_bgc_count));
-}
-void gc_heap::bgc_tuning::record_and_adjust_bgc_end()
-{
-    if (!bgc_tuning::enable_fl_tuning)
-        return;
-    uint64_t elapsed_time_so_far = GetHighPrecisionTimeStamp() - process_start_time;
-    size_t current_gen1_index = get_current_gc_index (max_generation - 1);
-    dprintf (BGC_TUNING_LOG, ("BTL: g2t[en][g1 %zd]: %0.3f minutes",
-        current_gen1_index,
-        (double)elapsed_time_so_far / (double)1000000 / (double)60));
-    if (fl_tuning_triggered)
-    {
-        num_bgcs_since_tuning_trigger++;
-    }
-    bool use_gen2_loop_p = (settings.reason == reason_bgc_tuning_soh);
-    bool use_gen3_loop_p = (settings.reason == reason_bgc_tuning_loh);
-    dprintf (BGC_TUNING_LOG, ("BTL: reason: %d, gen2 loop: %s; gen3 loop: %s, promoted %zd bytes",
-        (((settings.reason != reason_bgc_tuning_soh) && (settings.reason != reason_bgc_tuning_loh)) ?
-            saved_bgc_tuning_reason : settings.reason),
-        (use_gen2_loop_p ? "yes" : "no"),
-        (use_gen3_loop_p ? "yes" : "no"),
-        get_total_bgc_promoted()));
-    convert_to_fl (use_gen2_loop_p, use_gen3_loop_p);
-    calculate_tuning (max_generation, true);
-    if (total_loh_a_last_bgc > 0)
-    {
-        calculate_tuning (loh_generation, true);
-    }
-    else
-    {
-        dprintf (BGC_TUNING_LOG, ("BTL: gen3 not allocated"));
-    }
-    if (next_bgc_p)
-    {
-        next_bgc_p = false;
-        fl_tuning_triggered = true;
-        dprintf (BGC_TUNING_LOG, ("BTL: FL tuning ENABLED!!!"));
-    }
-    saved_bgc_tuning_reason = -1;
-}
-#endif //BGC_SERVO_TUNING
-#endif //BACKGROUND_GC
-void gc_heap::clear_cards (size_t start_card, size_t end_card)
-{
-    if (start_card < end_card)
-    {
-        size_t start_word = card_word (start_card);
-        size_t end_word = card_word (end_card);
-        if (start_word < end_word)
-        {
-            unsigned bits = card_bit (start_card);
-            card_table [start_word] &= lowbits (~0, bits);
-            for (size_t i = start_word+1; i < end_word; i++)
-                card_table [i] = 0;
-            bits = card_bit (end_card);
-            if (bits != 0)
-            {
-                card_table [end_word] &= highbits (~0, bits);
-            }
-        }
-        else
-        {
-            card_table [start_word] &= (lowbits (~0, card_bit (start_card)) |
-                                        highbits (~0, card_bit (end_card)));
-        }
-#if defined(_DEBUG) && defined(VERIFY_HEAP)
-        if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
-        {
-            size_t  card = start_card;
-            while (card < end_card)
-            {
-                assert (!(card_set_p (card)));
-                card++;
-            }
-        }
-#endif //_DEBUG && VERIFY_HEAP
-        dprintf (3,("Cleared cards [%zx:%zx, %zx:%zx[",
-                  start_card, (size_t)card_address (start_card),
-                  end_card, (size_t)card_address (end_card)));
-    }
-}
-void gc_heap::clear_card_for_addresses (uint8_t* start_address, uint8_t* end_address)
-{
-    size_t   start_card = card_of (align_on_card (start_address));
-    size_t   end_card = card_of (align_lower_card (end_address));
-    clear_cards (start_card, end_card);
-}
-inline
-void gc_heap::copy_cards (size_t dst_card,
-                          size_t src_card,
-                          size_t end_card,
-                          BOOL nextp)
-{
-    if (!(dst_card < end_card))
-        return;
-    unsigned int srcbit = card_bit (src_card);
-    unsigned int dstbit = card_bit (dst_card);
-    size_t srcwrd = card_word (src_card);
-    size_t dstwrd = card_word (dst_card);
-    unsigned int srctmp = card_table[srcwrd];
-    unsigned int dsttmp = card_table[dstwrd];
-    for (size_t card = dst_card; card < end_card; card++)
-    {
-        if (srctmp & (1 << srcbit))
-            dsttmp |= 1 << dstbit;
-        else
-            dsttmp &= ~(1 << dstbit);
-        if (!(++srcbit % 32))
-        {
-            srctmp = card_table[++srcwrd];
-            srcbit = 0;
-        }
-        if (nextp)
-        {
-            if (srctmp & (1 << srcbit))
-                dsttmp |= 1 << dstbit;
-        }
-        if (!(++dstbit % 32))
-        {
-            card_table[dstwrd] = dsttmp;
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-            if (dsttmp != 0)
-            {
-                card_bundle_set(cardw_card_bundle(dstwrd));
-            }
-#endif
-            dstwrd++;
-            dsttmp = card_table[dstwrd];
-            dstbit = 0;
-        }
-    }
-    card_table[dstwrd] = dsttmp;
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-    if (dsttmp != 0)
-    {
-        card_bundle_set(cardw_card_bundle(dstwrd));
-    }
-#endif
-}
-void gc_heap::copy_cards_for_addresses (uint8_t* dest, uint8_t* src, size_t len)
-{
-    ptrdiff_t relocation_distance = src - dest;
-    size_t start_dest_card = card_of (align_on_card (dest));
-    size_t end_dest_card = card_of (dest + len - 1);
-    size_t dest_card = start_dest_card;
-    size_t src_card = card_of (card_address (dest_card)+relocation_distance);
-    dprintf (3,("Copying cards [%zx:%zx->%zx:%zx, ",
-                 src_card, (size_t)src, dest_card, (size_t)dest));
-    dprintf (3,(" %zx->%zx:%zx[",
-              (size_t)src+len, end_dest_card, (size_t)dest+len));
-    dprintf (3, ("dest: %p, src: %p, len: %zx, reloc: %zx, align_on_card(dest) is %p",
-        dest, src, len, relocation_distance, (align_on_card (dest))));
-    dprintf (3, ("start_dest_card: %zx (address: %p), end_dest_card: %zx(addr: %p), card_of (dest): %zx",
-        start_dest_card, card_address (start_dest_card), end_dest_card, card_address (end_dest_card), card_of (dest)));
-    if (start_dest_card != card_of (dest))
-    {
-        if ((card_of (card_address (start_dest_card) + relocation_distance) <= card_of (src + len - 1))&&
-            card_set_p (card_of (card_address (start_dest_card) + relocation_distance)))
-        {
-            dprintf (3, ("card_address (start_dest_card) + reloc is %p, card: %zx(set), src+len-1: %p, card: %zx",
-                    (card_address (start_dest_card) + relocation_distance),
-                    card_of (card_address (start_dest_card) + relocation_distance),
-                    (src + len - 1),
-                    card_of (src + len - 1)));
-            dprintf (3, ("setting card: %zx", card_of (dest)));
-            set_card (card_of (dest));
-        }
-    }
-    if (card_set_p (card_of (src)))
-        set_card (card_of (dest));
-    copy_cards (dest_card, src_card, end_dest_card,
-                ((dest - align_lower_card (dest)) != (src - align_lower_card (src))));
-    if ((card_of (card_address (end_dest_card) + relocation_distance) >= card_of (src)) &&
-        card_set_p (card_of (card_address (end_dest_card) + relocation_distance)))
-    {
-        dprintf (3, ("card_address (end_dest_card) + reloc is %p, card: %zx(set), src: %p, card: %zx",
-                (card_address (end_dest_card) + relocation_distance),
-                card_of (card_address (end_dest_card) + relocation_distance),
-                src,
-                card_of (src)));
-        dprintf (3, ("setting card: %zx", end_dest_card));
-        set_card (end_dest_card);
-    }
-    if (card_set_p (card_of (src + len - 1)))
-        set_card (end_dest_card);
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-    card_bundles_set(cardw_card_bundle(card_word(card_of(dest))), cardw_card_bundle(align_cardw_on_bundle(card_word(end_dest_card))));
-#endif
-}
-#ifdef BACKGROUND_GC
-void gc_heap::copy_mark_bits_for_addresses (uint8_t* dest, uint8_t* src, size_t len)
-{
-    dprintf (3, ("Copying mark_bits for addresses [%zx->%zx, %zx->%zx[",
-                 (size_t)src, (size_t)dest,
-                 (size_t)src+len, (size_t)dest+len));
-    uint8_t* src_o = src;
-    uint8_t* dest_o;
-    uint8_t* src_end = src + len;
-    int align_const = get_alignment_constant (TRUE);
-    ptrdiff_t reloc = dest - src;
-    while (src_o < src_end)
-    {
-        uint8_t*  next_o = src_o + Align (size (src_o), align_const);
-        if (background_object_marked (src_o, TRUE))
-        {
-            dest_o = src_o + reloc;
-            background_mark (dest_o,
-                             background_saved_lowest_address,
-                             background_saved_highest_address);
-            dprintf (3, ("bc*%zx*bc, b*%zx*b", (size_t)src_o, (size_t)(dest_o)));
-        }
-        src_o = next_o;
-    }
-}
-#endif //BACKGROUND_GC
-void gc_heap::fix_brick_to_highest (uint8_t* o, uint8_t* next_o)
-{
-    size_t new_current_brick = brick_of (o);
-    set_brick (new_current_brick,
-               (o - brick_address (new_current_brick)));
-    size_t b = 1 + new_current_brick;
-    size_t limit = brick_of (next_o);
-    dprintf(3,("b:%zx->%zx-%zx",
-               new_current_brick, (size_t)o, (size_t)next_o));
-    while (b < limit)
-    {
-        set_brick (b,(new_current_brick - b));
-        b++;
-    }
-}
-uint8_t* gc_heap::find_first_object (uint8_t* start, uint8_t* first_object)
-{
-    size_t brick = brick_of (start);
-    uint8_t* o = 0;
-    if ((brick == brick_of (first_object) || (start <= first_object)))
-    {
-        o = first_object;
-    }
-    else
-    {
-        ptrdiff_t  min_brick = (ptrdiff_t)brick_of (first_object);
-        ptrdiff_t  prev_brick = (ptrdiff_t)brick - 1;
-        int         brick_entry = 0;
-        while (1)
-        {
-            if (prev_brick < min_brick)
-            {
-                break;
-            }
-            if ((brick_entry = get_brick_entry(prev_brick)) >= 0)
-            {
-                break;
-            }
-            assert (! ((brick_entry == 0)));
-            prev_brick = (brick_entry + prev_brick);
-        }
-        o = ((prev_brick < min_brick) ? first_object :
-                      brick_address (prev_brick) + brick_entry - 1);
-        assert (o <= start);
-    }
-    assert (Align (size (o)) >= Align (min_obj_size));
-    uint8_t*  next_o = o + Align (size (o));
-    size_t curr_cl = (size_t)next_o / brick_size;
-    size_t min_cl = (size_t)first_object / brick_size;
-#ifdef TRACE_GC
-    unsigned int n_o = 1;
-#endif //TRACE_GC
-    uint8_t* next_b = min (align_lower_brick (next_o) + brick_size, start+1);
-    while (next_o <= start)
-    {
-        do
-        {
-#ifdef TRACE_GC
-            n_o++;
-#endif //TRACE_GC
-            o = next_o;
-            assert (Align (size (o)) >= Align (min_obj_size));
-            next_o = o + Align (size (o));
-            Prefetch (next_o);
-        }while (next_o < next_b);
-        if (((size_t)next_o / brick_size) != curr_cl)
-        {
-            if (curr_cl >= min_cl)
-            {
-                fix_brick_to_highest (o, next_o);
-            }
-            curr_cl = (size_t) next_o / brick_size;
-        }
-        next_b = min (align_lower_brick (next_o) + brick_size, start+1);
-    }
-    size_t bo = brick_of (o);
-    dprintf (3, ("%u o, [%zx-[%zx",
-        n_o, bo, brick));
-    if (bo < brick)
-    {
-        set_brick (bo, (o - brick_address(bo)));
-        size_t b = 1 + bo;
-        int x = -1;
-        while (b < brick)
-        {
-            set_brick (b,x--);
-            b++;
-        }
-    }
-    return o;
-}
-#ifdef CARD_BUNDLE
-BOOL gc_heap::find_card_dword (size_t& cardw, size_t cardw_end)
-{
-    dprintf (3, ("gc: %zd, find_card_dword cardw: %zx, cardw_end: %zx",
-                 dd_collection_count (dynamic_data_of (0)), cardw, cardw_end));
-    if (card_bundles_enabled())
-    {
-        size_t cardb = cardw_card_bundle (cardw);
-        size_t end_cardb = cardw_card_bundle (align_cardw_on_bundle (cardw_end));
-        while (1)
-        {
-            while (cardb < end_cardb)
-            {
-                uint32_t cbw = card_bundle_table[card_bundle_word(cardb)] >> card_bundle_bit (cardb);
-                DWORD bit_index;
-                if (BitScanForward (&bit_index, cbw))
-                {
-                    cardb += bit_index;
-                    break;
-                }
-                else
-                {
-                    cardb += sizeof(cbw)*8 - card_bundle_bit (cardb);
-                }
-            }
-            if (cardb >= end_cardb)
-                return FALSE;
-            uint32_t* card_word = &card_table[max(card_bundle_cardw (cardb),cardw)];
-            uint32_t* card_word_end = &card_table[min(card_bundle_cardw (cardb+1),cardw_end)];
-            while ((card_word < card_word_end) && !(*card_word))
-            {
-                card_word++;
-            }
-            if (card_word != card_word_end)
-            {
-                cardw = (card_word - &card_table[0]);
-                return TRUE;
-            }
-            if (cardw == (card_bundle_cardw (cardb) + 1) && !card_table[cardw-1])
-            {
-                cardw--;
-            }
-            card_word_end = &card_table[card_bundle_cardw (cardb+1)];
-            while ((card_word < card_word_end) && !(*card_word))
-            {
-                card_word++;
-            }
-            if ((cardw <= card_bundle_cardw (cardb)) &&
-                (card_word == card_word_end))
-            {
-                dprintf  (3, ("gc: %zd, find_card_dword clear bundle: %zx cardw:[%zx,%zx[",
-                        dd_collection_count (dynamic_data_of (0)),
-                        cardb, card_bundle_cardw (cardb),
-                        card_bundle_cardw (cardb+1)));
-                card_bundle_clear (cardb);
-            }
-            cardb++;
-        }
-    }
-    else
-    {
-        uint32_t* card_word = &card_table[cardw];
-        uint32_t* card_word_end = &card_table [cardw_end];
-        while (card_word < card_word_end)
-        {
-            if ((*card_word) != 0)
-            {
-                cardw = (card_word - &card_table [0]);
-                return TRUE;
-            }
-            card_word++;
-        }
-        return FALSE;
-    }
-}
-#endif //CARD_BUNDLE
-BOOL gc_heap::find_card(uint32_t* card_table,
-                        size_t&   card,
-                        size_t    card_word_end,
-                        size_t&   end_card)
-{
-    uint32_t* last_card_word;
-    uint32_t card_word_value;
-    uint32_t bit_position;
-    if (card_word (card) >= card_word_end)
-        return FALSE;
-    last_card_word = &card_table [card_word (card)];
-    bit_position = card_bit (card);
-#ifdef CARD_BUNDLE
-    if (bit_position == 0)
-    {
-        card_word_value = 0;
-    }
-    else
-#endif
-    {
-        card_word_value = (*last_card_word) >> bit_position;
-    }
-    if (!card_word_value)
-    {
-#ifdef CARD_BUNDLE
-        size_t lcw = card_word(card) + (bit_position != 0);
-        if (gc_heap::find_card_dword (lcw, card_word_end) == FALSE)
-        {
-            return FALSE;
-        }
-        else
-        {
-            last_card_word = &card_table [lcw];
-            card_word_value = *last_card_word;
-        }
-        bit_position = 0;
-#else //CARD_BUNDLE
-        do
-        {
-            ++last_card_word;
-        }
-        while ((last_card_word < &card_table [card_word_end]) && !(*last_card_word));
-        if (last_card_word < &card_table [card_word_end])
-        {
-            card_word_value = *last_card_word;
-        }
-        else
-        {
-            return FALSE;
-        }
-#endif //CARD_BUNDLE
-    }
-    if (card_word_value)
-    {
-        DWORD bit_index;
-        uint8_t res = BitScanForward (&bit_index, card_word_value);
-        assert (res != 0);
-        card_word_value >>= bit_index;
-        bit_position += bit_index;
-    }
-    card = (last_card_word - &card_table[0]) * card_word_width + bit_position;
-    do
-    {
-        bit_position++;
-        card_word_value = card_word_value / 2;
-        if ((bit_position == card_word_width) && (last_card_word < &card_table [card_word_end-1]))
-        {
-            do
-            {
-                card_word_value = *(++last_card_word);
-            } while ((last_card_word < &card_table [card_word_end-1]) &&
-                     (card_word_value == ~0u /* (1 << card_word_width)-1 */));
-            bit_position = 0;
-        }
-    } while (card_word_value & 1);
-    end_card = (last_card_word - &card_table [0])* card_word_width + bit_position;
-    dprintf (3, ("fc: [%zx, %zx[", card, end_card));
-    return TRUE;
-}
-uint8_t* compute_next_end (heap_segment* seg, uint8_t* low)
-{
-    if ((low >=  heap_segment_mem (seg)) &&
-        (low < heap_segment_allocated (seg)))
-        return low;
-    else
-        return heap_segment_allocated (seg);
-}
-#ifndef USE_REGIONS
-uint8_t*
-gc_heap::compute_next_boundary (int gen_number,
-                                BOOL relocating)
-{
-    if (relocating && (gen_number == (settings.condemned_generation + 1)))
-    {
-        generation* gen = generation_of (gen_number - 1);
-        uint8_t* gen_alloc = generation_plan_allocation_start (gen);
-        assert (gen_alloc);
-        return gen_alloc;
-    }
-    else
-    {
-        assert (gen_number > settings.condemned_generation);
-        return generation_allocation_start (generation_of (gen_number - 1 ));
-    }
-}
-#endif //!USE_REGIONS
-inline void
-gc_heap::mark_through_cards_helper (uint8_t** poo, size_t& n_gen,
-                                    size_t& cg_pointers_found,
-                                    card_fn fn, uint8_t* nhigh,
-                                    uint8_t* next_boundary,
-                                    int condemned_gen,
-                                    int current_gen
-                                    CARD_MARKING_STEALING_ARG(gc_heap* hpt))
-{
-#if defined(FEATURE_CARD_MARKING_STEALING) && defined(MULTIPLE_HEAPS)
-    int thread = hpt->heap_number;
-#else
-    THREAD_FROM_HEAP;
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hpt = this;
-#endif //MULTIPLE_HEAPS
-#endif //FEATURE_CARD_MARKING_STEALING && MULTIPLE_HEAPS
-#ifdef USE_REGIONS
-    assert (nhigh == 0);
-    assert (next_boundary == 0);
-    uint8_t* child_object = *poo;
-    if ((child_object < ephemeral_low) || (ephemeral_high <= child_object))
-        return;
-    int child_object_gen = get_region_gen_num (child_object);
-    int saved_child_object_gen = child_object_gen;
-    uint8_t* saved_child_object = child_object;
-    if (child_object_gen <= condemned_gen)
-    {
-        n_gen++;
-        call_fn(hpt,fn) (poo THREAD_NUMBER_ARG);
-    }
-    if (fn == &gc_heap::relocate_address)
-    {
-        child_object_gen = get_region_plan_gen_num (*poo);
-    }
-    if (child_object_gen < current_gen)
-    {
-        cg_pointers_found++;
-        dprintf (4, ("cg pointer %zx found, %zd so far",
-                        (size_t)*poo, cg_pointers_found ));
-    }
-#else //USE_REGIONS
-    assert (condemned_gen == -1);
-    if ((gc_low <= *poo) && (gc_high > *poo))
-    {
-        n_gen++;
-        call_fn(hpt,fn) (poo THREAD_NUMBER_ARG);
-    }
-#ifdef MULTIPLE_HEAPS
-    else if (*poo)
-    {
-        gc_heap* hp = heap_of_gc (*poo);
-        if (hp != this)
-        {
-            if ((hp->gc_low <= *poo) &&
-                (hp->gc_high > *poo))
-            {
-                n_gen++;
-                call_fn(hpt,fn) (poo THREAD_NUMBER_ARG);
-            }
-            if ((fn == &gc_heap::relocate_address) ||
-                ((hp->ephemeral_low <= *poo) &&
-                 (hp->ephemeral_high > *poo)))
-            {
-                cg_pointers_found++;
-            }
-        }
-    }
-#endif //MULTIPLE_HEAPS
-    if ((next_boundary <= *poo) && (nhigh > *poo))
-    {
-        cg_pointers_found ++;
-        dprintf (4, ("cg pointer %zx found, %zd so far",
-                     (size_t)*poo, cg_pointers_found ));
-    }
-#endif //USE_REGIONS
-}
-BOOL gc_heap::card_transition (uint8_t* po, uint8_t* end, size_t card_word_end,
-                               size_t& cg_pointers_found,
-                               size_t& n_eph, size_t& n_card_set,
-                               size_t& card, size_t& end_card,
-                               BOOL& foundp, uint8_t*& start_address,
-                               uint8_t*& limit, size_t& n_cards_cleared
-                               CARD_MARKING_STEALING_ARGS(card_marking_enumerator& card_mark_enumerator, heap_segment* seg, size_t &card_word_end_out))
-{
-    dprintf (3, ("pointer %zx past card %zx, cg %zd", (size_t)po, (size_t)card, cg_pointers_found));
-    BOOL passed_end_card_p = FALSE;
-    foundp = FALSE;
-    if (cg_pointers_found == 0)
-    {
-        dprintf(3,(" CC [%zx, %zx[ ",
-                (size_t)card_address(card), (size_t)po));
-        clear_cards (card, card_of(po));
-        n_card_set -= (card_of (po) - card);
-        n_cards_cleared += (card_of (po) - card);
-    }
-    n_eph +=cg_pointers_found;
-    cg_pointers_found = 0;
-    card = card_of (po);
-    if (card >= end_card)
-    {
-        passed_end_card_p = TRUE;
-        dprintf (3, ("card %zx exceeding end_card %zx",
-                    (size_t)card, (size_t)end_card));
-        foundp = find_card (card_table, card, card_word_end, end_card);
-        if (foundp)
-        {
-            n_card_set+= end_card - card;
-            start_address = card_address (card);
-            dprintf (3, ("NewC: %zx, start: %zx, end: %zx",
-                        (size_t)card, (size_t)start_address,
-                        (size_t)card_address (end_card)));
-        }
-        limit = min (end, card_address (end_card));
-#ifdef FEATURE_CARD_MARKING_STEALING
-        assert(!((card_word(end_card) < card_word_end) &&
-            card_set_p(end_card)));
-        if (!foundp)
-        {
-            card_word_end_out = 0;
-            foundp = find_next_chunk(card_mark_enumerator, seg, n_card_set, start_address, limit, card, end_card, card_word_end_out);
-        }
-#else
-        assert (!((limit < end) &&
-                card_set_p (end_card)));
-#endif
-    }
-    return passed_end_card_p;
-}
-#ifdef FEATURE_CARD_MARKING_STEALING
-bool card_marking_enumerator::move_next(heap_segment* seg, uint8_t*& low, uint8_t*& high)
-{
-    if (segment == nullptr)
-        return false;
-    uint32_t chunk_index = old_chunk_index;
-    old_chunk_index = INVALID_CHUNK_INDEX;
-    if (chunk_index == INVALID_CHUNK_INDEX)
-        chunk_index = Interlocked::Increment((volatile int32_t *)chunk_index_counter);
-    while (true)
-    {
-        uint32_t chunk_index_within_seg = chunk_index - segment_start_chunk_index;
-        uint8_t* start = heap_segment_mem(segment);
-        uint8_t* end = compute_next_end(segment, gc_low);
-        uint8_t* aligned_start = (uint8_t*)((size_t)start & ~(CARD_MARKING_STEALING_GRANULARITY - 1));
-        size_t seg_size = end - aligned_start;
-        uint32_t chunk_count_within_seg = (uint32_t)((seg_size + (CARD_MARKING_STEALING_GRANULARITY - 1)) / CARD_MARKING_STEALING_GRANULARITY);
-        if (chunk_index_within_seg < chunk_count_within_seg)
-        {
-            if (seg == segment)
-            {
-                low = (chunk_index_within_seg == 0) ? start : (aligned_start + (size_t)chunk_index_within_seg * CARD_MARKING_STEALING_GRANULARITY);
-                high = (chunk_index_within_seg + 1 == chunk_count_within_seg) ? end : (aligned_start + (size_t)(chunk_index_within_seg + 1) * CARD_MARKING_STEALING_GRANULARITY);
-                chunk_high = high;
-                dprintf (3, ("cme:mn ci: %u, low: %p, high: %p", chunk_index, low, high));
-                return true;
-            }
-            else
-            {
-#ifdef _DEBUG
-                for (heap_segment* cur_seg = seg; cur_seg != segment; cur_seg = heap_segment_next_in_range(cur_seg))
-                {
-                    assert(cur_seg);
-                }
-#endif //_DEBUG
-                old_chunk_index = chunk_index;
-                dprintf (3, ("cme:mn oci: %u, seg mismatch seg: %p, segment: %p", old_chunk_index, heap_segment_mem (segment), heap_segment_mem (seg)));
-                return false;
-            }
-        }
-        segment = heap_segment_next_in_range(segment);
-        segment_start_chunk_index += chunk_count_within_seg;
-        if (segment == nullptr)
-        {
-            old_chunk_index = chunk_index;
-            dprintf (3, ("cme:mn oci: %u no more segments", old_chunk_index));
-            return false;
-        }
-    }
-}
-bool gc_heap::find_next_chunk(card_marking_enumerator& card_mark_enumerator, heap_segment* seg, size_t& n_card_set,
-    uint8_t*& start_address, uint8_t*& limit,
-    size_t& card, size_t& end_card, size_t& card_word_end)
-{
-    while (true)
-    {
-        if (card_word_end != 0 && find_card(card_table, card, card_word_end, end_card))
-        {
-            assert(end_card <= card_word_end * card_word_width);
-            n_card_set += end_card - card;
-            start_address = card_address(card);
-            dprintf(3, ("NewC: %zx, start: %zx, end: %zx",
-                (size_t)card, (size_t)start_address,
-                (size_t)card_address(end_card)));
-            limit = min(card_mark_enumerator.get_chunk_high(), card_address(end_card));
-            dprintf (3, ("New run of cards on heap %d: [%zx,%zx[", heap_number, (size_t)start_address, (size_t)limit));
-            return true;
-        }
-        uint8_t* chunk_low = nullptr;
-        uint8_t* chunk_high = nullptr;
-        if (!card_mark_enumerator.move_next(seg, chunk_low, chunk_high))
-        {
-            dprintf (3, ("No more chunks on heap %d\n", heap_number));
-            return false;
-        }
-        card = max(card, card_of(chunk_low));
-        card_word_end = (card_of(align_on_card_word(chunk_high)) / card_word_width);
-        dprintf (3, ("Moved to next chunk on heap %d: [%zx,%zx[", heap_number, (size_t)chunk_low, (size_t)chunk_high));
-    }
-}
-#endif // FEATURE_CARD_MARKING_STEALING
-void gc_heap::mark_through_cards_for_segments (card_fn fn, BOOL relocating CARD_MARKING_STEALING_ARG(gc_heap* hpt))
-{
-#ifdef BACKGROUND_GC
-#ifdef USE_REGIONS
-    dprintf (3, ("current_sweep_pos is %p", current_sweep_pos));
-#else
-    dprintf (3, ("current_sweep_pos is %p, saved_sweep_ephemeral_seg is %p(%p)",
-                 current_sweep_pos, saved_sweep_ephemeral_seg, saved_sweep_ephemeral_start));
-#endif //USE_REGIONS
-    for (int i = get_start_generation_index(); i < max_generation; i++)
-    {
-        heap_segment* soh_seg = heap_segment_rw (generation_start_segment (generation_of (i)));
-        PREFIX_ASSUME(soh_seg != NULL);
-        while (soh_seg)
-        {
-            dprintf (3, ("seg %p, bgc_alloc: %p, alloc: %p",
-                soh_seg,
-                heap_segment_background_allocated (soh_seg),
-                heap_segment_allocated (soh_seg)));
-            soh_seg = heap_segment_next_rw (soh_seg);
-        }
-    }
-#endif //BACKGROUND_GC
-    size_t end_card = 0;
-    generation*   oldest_gen        = generation_of (max_generation);
-    int           curr_gen_number   = max_generation;
-#ifdef USE_REGIONS
-    uint8_t* low = 0;
-    uint8_t*      gen_boundary      = 0;
-    uint8_t*      next_boundary     = 0;
-    int condemned_gen               = settings.condemned_generation;
-    uint8_t*      nhigh             = 0;
-#else
-    uint8_t* low = gc_low;
-    uint8_t* high = gc_high;
-    uint8_t*      gen_boundary      = generation_allocation_start(generation_of(curr_gen_number - 1));
-    uint8_t*      next_boundary     = compute_next_boundary(curr_gen_number, relocating);
-    int condemned_gen = -1;
-    uint8_t*      nhigh             = (relocating ?
-                                       heap_segment_plan_allocated (ephemeral_heap_segment) : high);
-#endif //USE_REGIONS
-    heap_segment* seg               = heap_segment_rw (generation_start_segment (oldest_gen));
-    PREFIX_ASSUME(seg != NULL);
-    uint8_t*      beg               = get_soh_start_object (seg, oldest_gen);
-    uint8_t*      end               = compute_next_end (seg, low);
-    uint8_t*      last_object       = beg;
-    size_t  cg_pointers_found = 0;
-    size_t  card_word_end = (card_of (align_on_card_word (end)) / card_word_width);
-    size_t        n_eph             = 0;
-    size_t        n_gen             = 0;
-    size_t        n_card_set        = 0;
-    BOOL          foundp            = FALSE;
-    uint8_t*      start_address     = 0;
-    uint8_t*      limit             = 0;
-    size_t        card              = card_of (beg);
-#ifdef BACKGROUND_GC
-    BOOL consider_bgc_mark_p        = FALSE;
-    BOOL check_current_sweep_p      = FALSE;
-    BOOL check_saved_sweep_p        = FALSE;
-    should_check_bgc_mark (seg, &consider_bgc_mark_p, &check_current_sweep_p, &check_saved_sweep_p);
-#endif //BACKGROUND_GC
-    dprintf(3, ("CMs: %zx->%zx", (size_t)beg, (size_t)end));
-    size_t total_cards_cleared = 0;
-#ifdef FEATURE_CARD_MARKING_STEALING
-    card_marking_enumerator card_mark_enumerator (seg, low, (VOLATILE(uint32_t)*)&card_mark_chunk_index_soh);
-    card_word_end = 0;
-#endif // FEATURE_CARD_MARKING_STEALING
-    while (1)
-    {
-        if (card_of(last_object) > card)
-        {
-            dprintf (3, ("Found %zd cg pointers", cg_pointers_found));
-            if (cg_pointers_found == 0)
-            {
-                uint8_t* last_object_processed = last_object;
-#ifdef FEATURE_CARD_MARKING_STEALING
-                last_object_processed = min(limit, last_object);
-#endif // FEATURE_CARD_MARKING_STEALING
-                dprintf (3, (" Clearing cards [%zx, %zx[ ", (size_t)card_address(card), (size_t)last_object_processed));
-                clear_cards(card, card_of(last_object_processed));
-                n_card_set -= (card_of(last_object_processed) - card);
-                total_cards_cleared += (card_of(last_object_processed) - card);
-            }
-            n_eph += cg_pointers_found;
-            cg_pointers_found = 0;
-            card = card_of (last_object);
-        }
-        if (card >= end_card)
-        {
-#ifdef FEATURE_CARD_MARKING_STEALING
-            foundp = find_next_chunk(card_mark_enumerator, seg, n_card_set, start_address, limit, card, end_card, card_word_end);
-#else // FEATURE_CARD_MARKING_STEALING
-            foundp = find_card(card_table, card, card_word_end, end_card);
-            if (foundp)
-            {
-                n_card_set += end_card - card;
-                start_address = max (beg, card_address (card));
-            }
-            limit = min (end, card_address (end_card));
-#endif // FEATURE_CARD_MARKING_STEALING
-        }
-        if (!foundp || (last_object >= end) || (card_address (card) >= end))
-        {
-            if (foundp && (cg_pointers_found == 0))
-            {
-#ifndef USE_REGIONS
-                end_card = card_of (end);
-#endif
-                dprintf(3,(" Clearing cards [%zx, %zx[ ", (size_t)card_address(card),
-                            (size_t)card_address(end_card)));
-                clear_cards (card, end_card);
-                n_card_set -= (end_card - card);
-                total_cards_cleared += (end_card - card);
-            }
-            n_eph += cg_pointers_found;
-            cg_pointers_found = 0;
-#ifdef FEATURE_CARD_MARKING_STEALING
-            card_mark_enumerator.exhaust_segment(seg);
-#endif // FEATURE_CARD_MARKING_STEALING
-            seg = heap_segment_next_in_range (seg);
-#ifdef USE_REGIONS
-            if (!seg)
-            {
-                curr_gen_number--;
-                if (curr_gen_number > condemned_gen)
-                {
-                    seg = generation_start_segment (generation_of (curr_gen_number));
-#ifdef FEATURE_CARD_MARKING_STEALING
-                    card_mark_enumerator.switch_to_segment(seg);
-#endif // FEATURE_CARD_MARKING_STEALING
-                    dprintf (REGIONS_LOG, ("h%d switching to gen%d start seg %zx",
-                        heap_number, curr_gen_number, (size_t)seg));
-                }
-            }
-#endif //USE_REGIONS
-            if (seg)
-            {
-#ifdef BACKGROUND_GC
-                should_check_bgc_mark (seg, &consider_bgc_mark_p, &check_current_sweep_p, &check_saved_sweep_p);
-#endif //BACKGROUND_GC
-                beg = heap_segment_mem (seg);
-#ifdef USE_REGIONS
-                end = heap_segment_allocated (seg);
-#else
-                end = compute_next_end (seg, low);
-#endif //USE_REGIONS
-#ifdef FEATURE_CARD_MARKING_STEALING
-                card_word_end = 0;
-#else // FEATURE_CARD_MARKING_STEALING
-                card_word_end = card_of (align_on_card_word (end)) / card_word_width;
-#endif // FEATURE_CARD_MARKING_STEALING
-                card = card_of (beg);
-                last_object = beg;
-                end_card = 0;
-                continue;
-            }
-            else
-            {
-                break;
-            }
-        }
-        assert (card_set_p (card));
-        {
-            uint8_t* o = last_object;
-            o = find_first_object (start_address, last_object);
-            assert (o >= last_object);
-#ifndef USE_REGIONS
-            dprintf(3, ("c: %zx, o: %zx, l: %zx[ boundary: %zx",
-                   card, (size_t)o, (size_t)limit, (size_t)gen_boundary));
-#endif //USE_REGIONS
-            while (o < limit)
-            {
-                assert (Align (size (o)) >= Align (min_obj_size));
-                size_t s = size (o);
-                uint8_t* next_o =  o + Align (s);
-                uint8_t* cont_o = next_o;
-                Prefetch (next_o);
-#ifndef USE_REGIONS
-                if ((o >= gen_boundary) &&
-                    (seg == ephemeral_heap_segment))
-                {
-                    dprintf (3, ("switching gen boundary %zx", (size_t)gen_boundary));
-                    curr_gen_number--;
-                    assert ((curr_gen_number > 0));
-                    gen_boundary = generation_allocation_start
-                        (generation_of (curr_gen_number - 1));
-                    next_boundary = (compute_next_boundary
-                                     (curr_gen_number, relocating));
-                }
-#endif //!USE_REGIONS
-                dprintf (4, ("|%zx|", (size_t)o));
-                if (next_o < start_address)
-                {
-                    goto end_object;
-                }
-#ifdef BACKGROUND_GC
-                if (!fgc_should_consider_object (o, seg, consider_bgc_mark_p, check_current_sweep_p, check_saved_sweep_p))
-                {
-                    goto end_object;
-                }
-#endif //BACKGROUND_GC
-#ifdef COLLECTIBLE_CLASS
-                if (is_collectible(o))
-                {
-                    BOOL passed_end_card_p = FALSE;
-                    if (card_of (o) > card)
-                    {
-                        passed_end_card_p = card_transition (o, end, card_word_end,
-                            cg_pointers_found,
-                            n_eph, n_card_set,
-                            card, end_card,
-                            foundp, start_address,
-                            limit, total_cards_cleared
-                            CARD_MARKING_STEALING_ARGS(card_mark_enumerator, seg, card_word_end));
-                    }
-                    if ((!passed_end_card_p || foundp) && (card_of (o) == card))
-                    {
-                        if (fn == &gc_heap::relocate_address)
-                        {
-                            cg_pointers_found++;
-                        }
-                        else
-                        {
-                            uint8_t* class_obj = get_class_object (o);
-                            mark_through_cards_helper (&class_obj, n_gen,
-                                                       cg_pointers_found, fn,
-                                                       nhigh, next_boundary,
-                                                       condemned_gen, curr_gen_number CARD_MARKING_STEALING_ARG(hpt));
-                        }
-                    }
-                    if (passed_end_card_p)
-                    {
-                        if (foundp && (card_address (card) < next_o))
-                        {
-                            goto go_through_refs;
-                        }
-                        else if (foundp && (start_address < limit))
-                        {
-                            cont_o = find_first_object (start_address, o);
-                            goto end_object;
-                        }
-                        else
-                            goto end_limit;
-                    }
-                }
-go_through_refs:
-#endif //COLLECTIBLE_CLASS
-                if (contain_pointers (o))
-                {
-                    dprintf(3,("Going through %zx start_address: %zx", (size_t)o, (size_t)start_address));
-                    {
-                        dprintf (4, ("normal object path"));
-                        go_through_object
-                            (method_table(o), o, s, poo,
-                             start_address, use_start, (o + s),
-                             {
-                                 dprintf (4, ("<%zx>:%zx", (size_t)poo, (size_t)*poo));
-                                 if (card_of ((uint8_t*)poo) > card)
-                                 {
-                                     BOOL passed_end_card_p  = card_transition ((uint8_t*)poo, end,
-                                            card_word_end,
-                                            cg_pointers_found,
-                                            n_eph, n_card_set,
-                                            card, end_card,
-                                            foundp, start_address,
-                                            limit, total_cards_cleared
-                                            CARD_MARKING_STEALING_ARGS(card_mark_enumerator, seg, card_word_end));
-                                     if (passed_end_card_p)
-                                     {
-                                         if (foundp && (card_address (card) < next_o))
-                                         {
-                                             {
-                                                 if (ppstop <= (uint8_t**)start_address)
-                                                     {break;}
-                                                 else if (poo < (uint8_t**)start_address)
-                                                     {poo = (uint8_t**)start_address;}
-                                             }
-                                         }
-                                         else if (foundp && (start_address < limit))
-                                         {
-                                             cont_o = find_first_object (start_address, o);
-                                             goto end_object;
-                                         }
-                                         else
-                                             goto end_limit;
-                                     }
-                                 }
-                                 mark_through_cards_helper (poo, n_gen,
-                                                            cg_pointers_found, fn,
-                                                            nhigh, next_boundary,
-                                                            condemned_gen, curr_gen_number CARD_MARKING_STEALING_ARG(hpt));
-                             }
-                            );
-                    }
-                }
-            end_object:
-                if (((size_t)next_o / brick_size) != ((size_t) o / brick_size))
-                {
-                    if (brick_table [brick_of (o)] <0)
-                        fix_brick_to_highest (o, next_o);
-                }
-                o = cont_o;
-            }
-        end_limit:
-            last_object = o;
-        }
-    }
-    if (!relocating)
-    {
-#ifdef FEATURE_CARD_MARKING_STEALING
-        Interlocked::ExchangeAddPtr(&n_eph_soh, n_eph);
-        Interlocked::ExchangeAddPtr(&n_gen_soh, n_gen);
-        dprintf (3, ("h%d marking h%d Msoh: cross: %zd, useful: %zd, cards set: %zd, cards cleared: %zd, ratio: %d",
-            hpt->heap_number, heap_number, n_eph, n_gen, n_card_set, total_cards_cleared,
-            (n_eph ? (int)(((float)n_gen / (float)n_eph) * 100) : 0)));
-        dprintf (3, ("h%d marking h%d Msoh: total cross %zd, useful: %zd, running ratio: %d",
-            hpt->heap_number, heap_number, (size_t)n_eph_soh, (size_t)n_gen_soh,
-            (n_eph_soh ? (int)(((float)n_gen_soh / (float)n_eph_soh) * 100) : 0)));
-#else
-        generation_skip_ratio = ((n_eph > MIN_SOH_CROSS_GEN_REFS) ? (int)(((float)n_gen / (float)n_eph) * 100) : 100);
-        dprintf (3, ("marking h%d Msoh: cross: %zd, useful: %zd, cards set: %zd, cards cleared: %zd, ratio: %d",
-            heap_number, n_eph, n_gen, n_card_set, total_cards_cleared, generation_skip_ratio));
-#endif //FEATURE_CARD_MARKING_STEALING
-    }
-    else
-    {
-        dprintf (3, ("R: Msoh: cross: %zd, useful: %zd, cards set: %zd, cards cleared: %zd, ratio: %d",
-            n_gen, n_eph, n_card_set, total_cards_cleared, generation_skip_ratio));
-    }
-}
-#ifndef USE_REGIONS
-#ifdef SEG_REUSE_STATS
-size_t gc_heap::dump_buckets (size_t* ordered_indices, int count, size_t* total_size)
-{
-    size_t total_items = 0;
-    *total_size = 0;
-    for (int i = 0; i < count; i++)
-    {
-        total_items += ordered_indices[i];
-        *total_size += ordered_indices[i] << (MIN_INDEX_POWER2 + i);
-        dprintf (SEG_REUSE_LOG_0, ("[%d]%4d 2^%2d", heap_number, ordered_indices[i], (MIN_INDEX_POWER2 + i)));
-    }
-    dprintf (SEG_REUSE_LOG_0, ("[%d]Total %d items, total size is 0x%zx", heap_number, total_items, *total_size));
-    return total_items;
-}
-#endif // SEG_REUSE_STATS
-void gc_heap::count_plug (size_t last_plug_size, uint8_t*& last_plug)
-{
-    if (!pinned_plug_que_empty_p() && (last_plug == pinned_plug (oldest_pin())))
-    {
-        deque_pinned_plug();
-        update_oldest_pinned_plug();
-        dprintf (3, ("deque pin,now oldest pin is %p", pinned_plug (oldest_pin())));
-    }
-    else
-    {
-        size_t plug_size = last_plug_size + Align(min_obj_size);
-        BOOL is_padded = FALSE;
-#ifdef SHORT_PLUGS
-        plug_size += Align (min_obj_size);
-        is_padded = TRUE;
-#endif //SHORT_PLUGS
-#ifdef RESPECT_LARGE_ALIGNMENT
-        plug_size += switch_alignment_size (is_padded);
-#endif //RESPECT_LARGE_ALIGNMENT
-        total_ephemeral_plugs += plug_size;
-        size_t plug_size_power2 = round_up_power2 (plug_size);
-        ordered_plug_indices[relative_index_power2_plug (plug_size_power2)]++;
-        dprintf (SEG_REUSE_LOG_1, ("[%d]count_plug: adding 0x%p - %zd (2^%d) to ordered plug array",
-            heap_number,
-            last_plug,
-            plug_size,
-            (relative_index_power2_plug (plug_size_power2) + MIN_INDEX_POWER2)));
-    }
-}
-void gc_heap::count_plugs_in_brick (uint8_t* tree, uint8_t*& last_plug)
-{
-    assert ((tree != NULL));
-    if (node_left_child (tree))
-    {
-        count_plugs_in_brick (tree + node_left_child (tree), last_plug);
-    }
-    if (last_plug != 0)
-    {
-        uint8_t*  plug = tree;
-        size_t gap_size = node_gap_size (plug);
-        uint8_t*   gap = (plug - gap_size);
-        uint8_t*  last_plug_end = gap;
-        size_t  last_plug_size = (last_plug_end - last_plug);
-        dprintf (3, ("tree: %p, last plug: %p, gap size: %zx, gap: %p, last plug size: %zx",
-            tree, last_plug, gap_size, gap, last_plug_size));
-        if (tree == oldest_pinned_plug)
-        {
-            dprintf (3, ("tree %p is pinned, last plug is %p, size is %zx",
-                tree, last_plug, last_plug_size));
-            mark* m = oldest_pin();
-            if (m->has_pre_plug_info())
-            {
-                last_plug_size += sizeof (gap_reloc_pair);
-                dprintf (3, ("pin %p has pre plug, adjusting plug size to %zx", tree, last_plug_size));
-            }
-        }
-        count_plug (last_plug_size, last_plug);
-    }
-    last_plug = tree;
-    if (node_right_child (tree))
-    {
-        count_plugs_in_brick (tree + node_right_child (tree), last_plug);
-    }
-}
-void gc_heap::build_ordered_plug_indices ()
-{
-    memset (ordered_plug_indices, 0, sizeof(ordered_plug_indices));
-    memset (saved_ordered_plug_indices, 0, sizeof(saved_ordered_plug_indices));
-    uint8_t*  start_address = generation_limit (max_generation);
-    uint8_t* end_address = heap_segment_allocated (ephemeral_heap_segment);
-    size_t  current_brick = brick_of (start_address);
-    size_t  end_brick = brick_of (end_address - 1);
-    uint8_t* last_plug = 0;
-    reset_pinned_queue_bos();
-    while (!pinned_plug_que_empty_p())
-    {
-        mark* m = oldest_pin();
-        if ((m->first >= start_address) && (m->first < end_address))
-        {
-            dprintf (3, ("found a pin %p between %p and %p", m->first, start_address, end_address));
-            break;
-        }
-        else
-            deque_pinned_plug();
-    }
-    update_oldest_pinned_plug();
-    while (current_brick <= end_brick)
-    {
-        int brick_entry =  brick_table [ current_brick ];
-        if (brick_entry >= 0)
-        {
-            count_plugs_in_brick (brick_address (current_brick) + brick_entry -1, last_plug);
-        }
-        current_brick++;
-    }
-    if (last_plug !=0)
-    {
-        count_plug (end_address - last_plug, last_plug);
-    }
-    size_t extra_size = END_SPACE_AFTER_GC_FL;
-    total_ephemeral_plugs += extra_size;
-    dprintf (SEG_REUSE_LOG_0, ("Making sure we can fit a large object after fitting all plugs"));
-    ordered_plug_indices[relative_index_power2_plug (round_up_power2 (extra_size))]++;
-    memcpy (saved_ordered_plug_indices, ordered_plug_indices, sizeof(ordered_plug_indices));
-#ifdef SEG_REUSE_STATS
-    dprintf (SEG_REUSE_LOG_0, ("Plugs:"));
-    size_t total_plug_power2 = 0;
-    dump_buckets (ordered_plug_indices, MAX_NUM_BUCKETS, &total_plug_power2);
-    dprintf (SEG_REUSE_LOG_0, ("plugs: 0x%zx (rounded up to 0x%zx (%d%%))",
-                total_ephemeral_plugs,
-                total_plug_power2,
-                (total_ephemeral_plugs ?
-                    (total_plug_power2 * 100 / total_ephemeral_plugs) :
-                    0)));
-    dprintf (SEG_REUSE_LOG_0, ("-------------------"));
-#endif // SEG_REUSE_STATS
-}
-void gc_heap::init_ordered_free_space_indices ()
-{
-    memset (ordered_free_space_indices, 0, sizeof(ordered_free_space_indices));
-    memset (saved_ordered_free_space_indices, 0, sizeof(saved_ordered_free_space_indices));
-}
-void gc_heap::trim_free_spaces_indices ()
-{
-    trimmed_free_space_index = -1;
-    size_t max_count = max_free_space_items - 1;
-    size_t count = 0;
-    int i = 0;
-    for (i = (MAX_NUM_BUCKETS - 1); i >= 0; i--)
-    {
-        count += ordered_free_space_indices[i];
-        if (count >= max_count)
-        {
-            break;
-        }
-    }
-    ptrdiff_t extra_free_space_items = count - max_count;
-    if (extra_free_space_items > 0)
-    {
-        ordered_free_space_indices[i] -= extra_free_space_items;
-        free_space_items = max_count;
-        trimmed_free_space_index = i;
-    }
-    else
-    {
-        free_space_items = count;
-    }
-    if (i == -1)
-    {
-        i = 0;
-    }
-    free_space_buckets = MAX_NUM_BUCKETS - i;
-    for (--i; i >= 0; i--)
-    {
-        ordered_free_space_indices[i] = 0;
-    }
-    memcpy (saved_ordered_free_space_indices,
-            ordered_free_space_indices,
-            sizeof(ordered_free_space_indices));
-}
-BOOL gc_heap::can_fit_in_spaces_p (size_t* ordered_blocks, int small_index, size_t* ordered_spaces, int big_index)
-{
-    assert (small_index <= big_index);
-    assert (big_index < MAX_NUM_BUCKETS);
-    size_t small_blocks = ordered_blocks[small_index];
-    if (small_blocks == 0)
-    {
-        return TRUE;
-    }
-    size_t big_spaces = ordered_spaces[big_index];
-    if (big_spaces == 0)
-    {
-        return FALSE;
-    }
-    dprintf (SEG_REUSE_LOG_1, ("[%d]Fitting %zu 2^%d plugs into %zu 2^%d free spaces",
-        heap_number,
-        small_blocks, (small_index + MIN_INDEX_POWER2),
-        big_spaces, (big_index + MIN_INDEX_POWER2)));
-    size_t big_to_small = big_spaces << (big_index - small_index);
-    ptrdiff_t extra_small_spaces = big_to_small - small_blocks;
-    dprintf (SEG_REUSE_LOG_1, ("[%d]%zu 2^%d spaces can fit %zu 2^%d blocks",
-        heap_number,
-        big_spaces, (big_index + MIN_INDEX_POWER2), big_to_small, (small_index + MIN_INDEX_POWER2)));
-    BOOL can_fit = (extra_small_spaces >= 0);
-    if (can_fit)
-    {
-        dprintf (SEG_REUSE_LOG_1, ("[%d]Can fit with %zd 2^%d extras blocks",
-            heap_number,
-            extra_small_spaces, (small_index + MIN_INDEX_POWER2)));
-    }
-    int i = 0;
-    dprintf (SEG_REUSE_LOG_1, ("[%d]Setting # of 2^%d spaces to 0", heap_number, (big_index + MIN_INDEX_POWER2)));
-    ordered_spaces[big_index] = 0;
-    if (extra_small_spaces > 0)
-    {
-        dprintf (SEG_REUSE_LOG_1, ("[%d]Setting # of 2^%d blocks to 0", heap_number, (small_index + MIN_INDEX_POWER2)));
-        ordered_blocks[small_index] = 0;
-        for (i = small_index; i < big_index; i++)
-        {
-            if (extra_small_spaces & 1)
-            {
-                dprintf (SEG_REUSE_LOG_1, ("[%d]Increasing # of 2^%d spaces from %zu to %zu",
-                    heap_number,
-                    (i + MIN_INDEX_POWER2), ordered_spaces[i], (ordered_spaces[i] + 1)));
-                ordered_spaces[i] += 1;
-            }
-            extra_small_spaces >>= 1;
-        }
-        dprintf (SEG_REUSE_LOG_1, ("[%d]Finally increasing # of 2^%d spaces from %zu to %zu",
-            heap_number,
-            (i + MIN_INDEX_POWER2), ordered_spaces[i], (ordered_spaces[i] + extra_small_spaces)));
-        ordered_spaces[i] += extra_small_spaces;
-    }
-    else
-    {
-        dprintf (SEG_REUSE_LOG_1, ("[%d]Decreasing # of 2^%d blocks from %zu to %zu",
-            heap_number,
-            (small_index + MIN_INDEX_POWER2),
-            ordered_blocks[small_index],
-            (ordered_blocks[small_index] - big_to_small)));
-        ordered_blocks[small_index] -= big_to_small;
-    }
-#ifdef SEG_REUSE_STATS
-    size_t temp;
-    dprintf (SEG_REUSE_LOG_1, ("[%d]Plugs became:", heap_number));
-    dump_buckets (ordered_blocks, MAX_NUM_BUCKETS, &temp);
-    dprintf (SEG_REUSE_LOG_1, ("[%d]Free spaces became:", heap_number));
-    dump_buckets (ordered_spaces, MAX_NUM_BUCKETS, &temp);
-#endif //SEG_REUSE_STATS
-    return can_fit;
-}
-BOOL gc_heap::can_fit_blocks_p (size_t* ordered_blocks, int block_index, size_t* ordered_spaces, int* space_index)
-{
-    assert (*space_index >= block_index);
-    while (!can_fit_in_spaces_p (ordered_blocks, block_index, ordered_spaces, *space_index))
-    {
-        (*space_index)--;
-        if (*space_index < block_index)
-        {
-            return FALSE;
-        }
-    }
-    return TRUE;
-}
-BOOL gc_heap::can_fit_all_blocks_p (size_t* ordered_blocks, size_t* ordered_spaces, int count)
-{
-#ifdef FEATURE_STRUCTALIGN
-    return FALSE;
-#endif // FEATURE_STRUCTALIGN
-    int space_index = count - 1;
-    for (int block_index = (count - 1); block_index >= 0; block_index--)
-    {
-        if (!can_fit_blocks_p (ordered_blocks, block_index, ordered_spaces, &space_index))
-        {
-            return FALSE;
-        }
-    }
-    return TRUE;
-}
-void gc_heap::build_ordered_free_spaces (heap_segment* seg)
-{
-    assert (bestfit_seg);
-    bestfit_seg->add_buckets (MIN_INDEX_POWER2,
-                        ordered_free_space_indices,
-                        MAX_NUM_BUCKETS,
-                        free_space_items);
-    assert (settings.condemned_generation == max_generation);
-    uint8_t* first_address = heap_segment_mem (seg);
-    uint8_t* end_address   = heap_segment_reserved (seg);
-    reset_pinned_queue_bos();
-    mark* m = 0;
-    size_t eph_gen_starts = eph_gen_starts_size + Align (min_obj_size);
-    BOOL has_fit_gen_starts = FALSE;
-    while (!pinned_plug_que_empty_p())
-    {
-        m = oldest_pin();
-        if ((pinned_plug (m) >= first_address) &&
-            (pinned_plug (m) < end_address) &&
-            (pinned_len (m) >= eph_gen_starts))
-        {
-            assert ((pinned_plug (m) - pinned_len (m)) == bestfit_first_pin);
-            break;
-        }
-        else
-        {
-            deque_pinned_plug();
-        }
-    }
-    if (!pinned_plug_que_empty_p())
-    {
-        bestfit_seg->add ((void*)m, TRUE, TRUE);
-        deque_pinned_plug();
-        m = oldest_pin();
-        has_fit_gen_starts = TRUE;
-    }
-    while (!pinned_plug_que_empty_p() &&
-            ((pinned_plug (m) >= first_address) && (pinned_plug (m) < end_address)))
-    {
-        bestfit_seg->add ((void*)m, TRUE, FALSE);
-        deque_pinned_plug();
-        m = oldest_pin();
-    }
-    if (commit_end_of_seg)
-    {
-        if (!has_fit_gen_starts)
-        {
-            assert (bestfit_first_pin == heap_segment_plan_allocated (seg));
-        }
-        bestfit_seg->add ((void*)seg, FALSE, (!has_fit_gen_starts));
-    }
-#ifdef _DEBUG
-    bestfit_seg->check();
-#endif //_DEBUG
-}
-BOOL gc_heap::try_best_fit (BOOL end_of_segment_p)
-{
-    if (!end_of_segment_p)
-    {
-        trim_free_spaces_indices ();
-    }
-    BOOL can_bestfit = can_fit_all_blocks_p (ordered_plug_indices,
-                                             ordered_free_space_indices,
-                                             MAX_NUM_BUCKETS);
-    return can_bestfit;
-}
-BOOL gc_heap::best_fit (size_t free_space,
-                        size_t largest_free_space,
-                        size_t additional_space,
-                        BOOL* use_additional_space)
-{
-    dprintf (SEG_REUSE_LOG_0, ("gen%d: trying best fit mechanism", settings.condemned_generation));
-    assert (!additional_space || (additional_space && use_additional_space));
-    if (use_additional_space)
-    {
-        *use_additional_space = FALSE;
-    }
-    if (ordered_plug_indices_init == FALSE)
-    {
-        total_ephemeral_plugs = 0;
-        build_ordered_plug_indices();
-        ordered_plug_indices_init = TRUE;
-    }
-    else
-    {
-        memcpy (ordered_plug_indices, saved_ordered_plug_indices, sizeof(ordered_plug_indices));
-    }
-    if (total_ephemeral_plugs == END_SPACE_AFTER_GC_FL)
-    {
-        dprintf (SEG_REUSE_LOG_0, ("No ephemeral plugs to realloc, done"));
-        size_t empty_eph = (END_SPACE_AFTER_GC_FL + (Align (min_obj_size)) * (max_generation + 1));
-        BOOL can_fit_empty_eph = (largest_free_space >= empty_eph);
-        if (!can_fit_empty_eph)
-        {
-            can_fit_empty_eph = (additional_space >= empty_eph);
-            if (can_fit_empty_eph)
-            {
-                *use_additional_space = TRUE;
-            }
-        }
-        return can_fit_empty_eph;
-    }
-    if ((total_ephemeral_plugs + approximate_new_allocation()) >= (free_space + additional_space))
-    {
-        dprintf (SEG_REUSE_LOG_0, ("We won't have enough free space left in this segment after fitting, done"));
-        return FALSE;
-    }
-    if ((free_space + additional_space) == 0)
-    {
-        dprintf (SEG_REUSE_LOG_0, ("No free space in this segment, done"));
-        return FALSE;
-    }
-#ifdef SEG_REUSE_STATS
-    dprintf (SEG_REUSE_LOG_0, ("Free spaces:"));
-    size_t total_free_space_power2 = 0;
-    size_t total_free_space_items =
-        dump_buckets (ordered_free_space_indices,
-                      MAX_NUM_BUCKETS,
-                      &total_free_space_power2);
-    dprintf (SEG_REUSE_LOG_0, ("currently max free spaces is %zd", max_free_space_items));
-    dprintf (SEG_REUSE_LOG_0, ("Ephemeral plugs: 0x%zx, free space: 0x%zx (rounded down to 0x%zx (%zd%%)), additional free_space: 0x%zx",
-                total_ephemeral_plugs,
-                free_space,
-                total_free_space_power2,
-                (free_space ? (total_free_space_power2 * 100 / free_space) : 0),
-                additional_space));
-    size_t saved_all_free_space_indices[MAX_NUM_BUCKETS];
-    memcpy (saved_all_free_space_indices,
-            ordered_free_space_indices,
-            sizeof(saved_all_free_space_indices));
-#endif // SEG_REUSE_STATS
-    if (total_ephemeral_plugs > (free_space + additional_space))
-    {
-        return FALSE;
-    }
-    use_bestfit = try_best_fit(FALSE);
-    if (!use_bestfit && additional_space)
-    {
-        int relative_free_space_index = relative_index_power2_free_space (round_down_power2 (additional_space));
-        if (relative_free_space_index != -1)
-        {
-            int relative_plug_index = 0;
-            size_t plugs_to_fit = 0;
-            for (relative_plug_index = (MAX_NUM_BUCKETS - 1); relative_plug_index >= 0; relative_plug_index--)
-            {
-                plugs_to_fit = ordered_plug_indices[relative_plug_index];
-                if (plugs_to_fit != 0)
-                {
-                    break;
-                }
-            }
-            if ((relative_plug_index > relative_free_space_index) ||
-                ((relative_plug_index == relative_free_space_index) &&
-                (plugs_to_fit > 1)))
-            {
-#ifdef SEG_REUSE_STATS
-                dprintf (SEG_REUSE_LOG_0, ("additional space is 2^%d but we stopped at %d 2^%d plug(s)",
-                            (relative_free_space_index + MIN_INDEX_POWER2),
-                            plugs_to_fit,
-                            (relative_plug_index + MIN_INDEX_POWER2)));
-#endif // SEG_REUSE_STATS
-                goto adjust;
-            }
-            dprintf (SEG_REUSE_LOG_0, ("Adding end of segment (2^%d)", (relative_free_space_index + MIN_INDEX_POWER2)));
-            ordered_free_space_indices[relative_free_space_index]++;
-            use_bestfit = try_best_fit(TRUE);
-            if (use_bestfit)
-            {
-                free_space_items++;
-                if (relative_free_space_index > trimmed_free_space_index)
-                {
-                    *use_additional_space = TRUE;
-                }
-                else
-                {
-                    saved_ordered_free_space_indices[trimmed_free_space_index]++;
-                }
-            }
-        }
-    }
-adjust:
-    if (!use_bestfit)
-    {
-        dprintf (SEG_REUSE_LOG_0, ("couldn't fit..."));
-#ifdef SEG_REUSE_STATS
-        size_t saved_max = max_free_space_items;
-        BOOL temp_bestfit = FALSE;
-        dprintf (SEG_REUSE_LOG_0, ("----Starting experiment process----"));
-        dprintf (SEG_REUSE_LOG_0, ("----Couldn't fit with max free items %zd", max_free_space_items));
-        while (max_free_space_items <= total_free_space_items)
-        {
-            max_free_space_items += max_free_space_items / 2;
-            dprintf (SEG_REUSE_LOG_0, ("----Temporarily increasing max free spaces to %zd", max_free_space_items));
-            memcpy (ordered_free_space_indices,
-                    saved_all_free_space_indices,
-                    sizeof(ordered_free_space_indices));
-            if (try_best_fit(FALSE))
-            {
-                temp_bestfit = TRUE;
-                break;
-            }
-        }
-        if (temp_bestfit)
-        {
-            dprintf (SEG_REUSE_LOG_0, ("----With %zd max free spaces we could fit", max_free_space_items));
-        }
-        else
-        {
-            dprintf (SEG_REUSE_LOG_0, ("----Tried all free spaces and still couldn't fit, lost too much space"));
-        }
-        dprintf (SEG_REUSE_LOG_0, ("----Restoring max free spaces to %zd", saved_max));
-        max_free_space_items = saved_max;
-#endif // SEG_REUSE_STATS
-        if (free_space_items)
-        {
-            max_free_space_items = min (MAX_NUM_FREE_SPACES, free_space_items * 2);
-            max_free_space_items = max (max_free_space_items, MIN_NUM_FREE_SPACES);
-        }
-        else
-        {
-            max_free_space_items = MAX_NUM_FREE_SPACES;
-        }
-    }
-    dprintf (SEG_REUSE_LOG_0, ("Adjusted number of max free spaces to %zd", max_free_space_items));
-    dprintf (SEG_REUSE_LOG_0, ("------End of best fitting process------\n"));
-    return use_bestfit;
-}
-BOOL gc_heap::process_free_space (heap_segment* seg,
-                                  size_t free_space,
-                                  size_t min_free_size,
-                                  size_t min_cont_size,
-                                  size_t* total_free_space,
-                                  size_t* largest_free_space)
-{
-    *total_free_space += free_space;
-    *largest_free_space = max (*largest_free_space, free_space);
-#ifdef SIMPLE_DPRINTF
-    dprintf (SEG_REUSE_LOG_1, ("free space len: %zx, total free space: %zx, largest free space: %zx",
-                free_space, *total_free_space, *largest_free_space));
-#endif //SIMPLE_DPRINTF
-    if ((*total_free_space >= min_free_size) && (*largest_free_space >= min_cont_size))
-    {
-#ifdef SIMPLE_DPRINTF
-        dprintf (SEG_REUSE_LOG_0, ("(gen%d)total free: %zx(min: %zx), largest free: %zx(min: %zx). Found segment %zx to reuse without bestfit",
-            settings.condemned_generation,
-            *total_free_space, min_free_size, *largest_free_space, min_cont_size,
-            (size_t)seg));
-#else
-        UNREFERENCED_PARAMETER(seg);
-#endif //SIMPLE_DPRINTF
-        return TRUE;
-    }
-    int free_space_index = relative_index_power2_free_space (round_down_power2 (free_space));
-    if (free_space_index != -1)
-    {
-        ordered_free_space_indices[free_space_index]++;
-    }
-    return FALSE;
-}
-BOOL gc_heap::can_expand_into_p (heap_segment* seg, size_t min_free_size, size_t min_cont_size,
-                                 allocator* gen_allocator)
-{
-    min_cont_size += END_SPACE_AFTER_GC;
-    use_bestfit = FALSE;
-    commit_end_of_seg = FALSE;
-    bestfit_first_pin = 0;
-    uint8_t* first_address = heap_segment_mem (seg);
-    uint8_t* end_address   = heap_segment_reserved (seg);
-    size_t end_extra_space = end_space_after_gc();
-    if ((heap_segment_reserved (seg) - end_extra_space) <= heap_segment_plan_allocated (seg))
-    {
-        dprintf (SEG_REUSE_LOG_0, ("can_expand_into_p: can't use segment [%p %p, has less than %zu bytes at the end",
-                                   first_address, end_address, end_extra_space));
-        return FALSE;
-    }
-    end_address -= end_extra_space;
-    dprintf (SEG_REUSE_LOG_0, ("can_expand_into_p(gen%d): min free: %zx, min continuous: %zx",
-        settings.condemned_generation, min_free_size, min_cont_size));
-    size_t eph_gen_starts = eph_gen_starts_size;
-    if (settings.condemned_generation == max_generation)
-    {
-        size_t free_space = 0;
-        size_t largest_free_space = free_space;
-        dprintf (SEG_REUSE_LOG_0, ("can_expand_into_p: gen2: testing segment [%p %p", first_address, end_address));
-        reset_pinned_queue_bos();
-        mark* m = 0;
-        BOOL has_fit_gen_starts = FALSE;
-        init_ordered_free_space_indices ();
-        while (!pinned_plug_que_empty_p())
-        {
-            m = oldest_pin();
-            if ((pinned_plug (m) >= first_address) &&
-                (pinned_plug (m) < end_address) &&
-                (pinned_len (m) >= (eph_gen_starts + Align (min_obj_size))))
-            {
-                break;
-            }
-            else
-            {
-                deque_pinned_plug();
-            }
-        }
-        if (!pinned_plug_que_empty_p())
-        {
-            bestfit_first_pin = pinned_plug (m) - pinned_len (m);
-            if (process_free_space (seg,
-                                    pinned_len (m) - eph_gen_starts,
-                                    min_free_size, min_cont_size,
-                                    &free_space, &largest_free_space))
-            {
-                return TRUE;
-            }
-            deque_pinned_plug();
-            m = oldest_pin();
-            has_fit_gen_starts = TRUE;
-        }
-        dprintf (3, ("first pin is %p", pinned_plug (m)));
-        while (!pinned_plug_que_empty_p() &&
-               ((pinned_plug (m) >= first_address) && (pinned_plug (m) < end_address)))
-        {
-            dprintf (3, ("looking at pin %p", pinned_plug (m)));
-            if (process_free_space (seg,
-                                    pinned_len (m),
-                                    min_free_size, min_cont_size,
-                                    &free_space, &largest_free_space))
-            {
-                return TRUE;
-            }
-            deque_pinned_plug();
-            m = oldest_pin();
-        }
-        size_t end_space = (end_address - heap_segment_plan_allocated (seg));
-        size_t additional_space = ((min_free_size > free_space) ? (min_free_size - free_space) : 0);
-        dprintf (SEG_REUSE_LOG_0, ("end space: %zx; additional: %zx", end_space, additional_space));
-        if (end_space >= additional_space)
-        {
-            BOOL can_fit = TRUE;
-            commit_end_of_seg = TRUE;
-            if (largest_free_space < min_cont_size)
-            {
-                if (end_space >= min_cont_size)
-                {
-                    additional_space = max (min_cont_size, additional_space);
-                    dprintf (SEG_REUSE_LOG_0, ("(gen2)Found segment %p to reuse without bestfit, with committing end of seg for eph",
-                        seg));
-                }
-                else
-                {
-                    if (settings.concurrent)
-                    {
-                        can_fit = FALSE;
-                        commit_end_of_seg = FALSE;
-                    }
-                    else
-                    {
-                        size_t additional_space_bestfit = additional_space;
-                        if (!has_fit_gen_starts)
-                        {
-                            if (additional_space_bestfit < (eph_gen_starts + Align (min_obj_size)))
-                            {
-                                dprintf (SEG_REUSE_LOG_0, ("(gen2)Couldn't fit, gen starts not allocated yet and end space is too small: %zd",
-                                        additional_space_bestfit));
-                                return FALSE;
-                            }
-                            bestfit_first_pin = heap_segment_plan_allocated (seg);
-                            additional_space_bestfit -= eph_gen_starts;
-                        }
-                        can_fit = best_fit (free_space,
-                                            largest_free_space,
-                                            additional_space_bestfit,
-                                            &commit_end_of_seg);
-                        if (can_fit)
-                        {
-                            dprintf (SEG_REUSE_LOG_0, ("(gen2)Found segment %p to reuse with bestfit, %s committing end of seg",
-                                seg, (commit_end_of_seg ? "with" : "without")));
-                        }
-                        else
-                        {
-                            dprintf (SEG_REUSE_LOG_0, ("(gen2)Couldn't fit, total free space is %zx", (free_space + end_space)));
-                        }
-                    }
-                }
-            }
-            else
-            {
-                dprintf (SEG_REUSE_LOG_0, ("(gen2)Found segment %p to reuse without bestfit, with committing end of seg", seg));
-            }
-            assert (additional_space <= end_space);
-            if (commit_end_of_seg)
-            {
-                if (!grow_heap_segment (seg, heap_segment_plan_allocated (seg) + additional_space))
-                {
-                    dprintf (2, ("Couldn't commit end of segment?!"));
-                    use_bestfit = FALSE;
-                    return FALSE;
-                }
-                if (use_bestfit)
-                {
-                    size_t free_space_end_of_seg =
-                        heap_segment_committed (seg) - heap_segment_plan_allocated (seg);
-                    int relative_free_space_index = relative_index_power2_free_space (round_down_power2 (free_space_end_of_seg));
-                    saved_ordered_free_space_indices[relative_free_space_index]++;
-                }
-            }
-            if (use_bestfit)
-            {
-                memcpy (ordered_free_space_indices,
-                        saved_ordered_free_space_indices,
-                        sizeof(ordered_free_space_indices));
-                max_free_space_items = max (MIN_NUM_FREE_SPACES, free_space_items * 3 / 2);
-                max_free_space_items = min (MAX_NUM_FREE_SPACES, max_free_space_items);
-                dprintf (SEG_REUSE_LOG_0, ("could fit! %zd free spaces, %zd max", free_space_items, max_free_space_items));
-            }
-            return can_fit;
-        }
-        dprintf (SEG_REUSE_LOG_0, ("(gen2)Couldn't fit, total free space is %zx", (free_space + end_space)));
-        return FALSE;
-    }
-    else
-    {
-        assert (settings.condemned_generation == (max_generation-1));
-        size_t free_space = (end_address - heap_segment_plan_allocated (seg));
-        size_t largest_free_space = free_space;
-        dprintf (SEG_REUSE_LOG_0, ("can_expand_into_p: gen1: testing segment [%p %p", first_address, end_address));
-        uint8_t* free_list = 0;
-        unsigned int a_l_idx = gen_allocator->first_suitable_bucket(eph_gen_starts);
-        for (; a_l_idx < gen_allocator->number_of_buckets(); a_l_idx++)
-        {
-            free_list = gen_allocator->alloc_list_head_of (a_l_idx);
-            while (free_list)
-            {
-                if ((free_list >= first_address) &&
-                    (free_list < end_address) &&
-                    (unused_array_size (free_list) >= eph_gen_starts))
-                {
-                    goto next;
-                }
-                else
-                {
-                    free_list = free_list_slot (free_list);
-                }
-            }
-        }
-next:
-        if (free_list)
-        {
-            init_ordered_free_space_indices ();
-            if (process_free_space (seg,
-                                    unused_array_size (free_list) - eph_gen_starts + Align (min_obj_size),
-                                    min_free_size, min_cont_size,
-                                    &free_space, &largest_free_space))
-            {
-                return TRUE;
-            }
-            free_list = free_list_slot (free_list);
-        }
-        else
-        {
-            dprintf (SEG_REUSE_LOG_0, ("(gen1)Couldn't fit, no free list"));
-            return FALSE;
-        }
-        while (1)
-        {
-            while (free_list)
-            {
-                if ((free_list >= first_address) && (free_list < end_address) &&
-                    process_free_space (seg,
-                                        unused_array_size (free_list),
-                                        min_free_size, min_cont_size,
-                                        &free_space, &largest_free_space))
-                {
-                    return TRUE;
-                }
-                free_list = free_list_slot (free_list);
-            }
-            a_l_idx++;
-            if (a_l_idx < gen_allocator->number_of_buckets())
-            {
-                free_list = gen_allocator->alloc_list_head_of (a_l_idx);
-            }
-            else
-                break;
-        }
-        dprintf (SEG_REUSE_LOG_0, ("(gen1)Couldn't fit, total free space is %zx", free_space));
-        return FALSE;
-        /*
-        BOOL can_fit = best_fit (free_space, 0, NULL);
-        if (can_fit)
-        {
-            dprintf (SEG_REUSE_LOG_0, ("(gen1)Found segment %zx to reuse with bestfit", seg));
-        }
-        else
-        {
-            dprintf (SEG_REUSE_LOG_0, ("(gen1)Couldn't fit, total free space is %zx", free_space));
-        }
-        return can_fit;
-        */
-    }
-}
-void gc_heap::realloc_plug (size_t last_plug_size, uint8_t*& last_plug,
-                            generation* gen, uint8_t* start_address,
-                            unsigned int& active_new_gen_number,
-                            uint8_t*& last_pinned_gap, BOOL& leftp,
-                            BOOL shortened_p
-#ifdef SHORT_PLUGS
-                            , mark* pinned_plug_entry
-#endif //SHORT_PLUGS
-                            )
-{
-    if (!use_bestfit)
-    {
-        if ((active_new_gen_number > 1) &&
-            (last_plug >= generation_limit (active_new_gen_number)))
-        {
-            assert (last_plug >= start_address);
-            active_new_gen_number--;
-            realloc_plan_generation_start (generation_of (active_new_gen_number), gen);
-            assert (generation_plan_allocation_start (generation_of (active_new_gen_number)));
-            leftp = FALSE;
-        }
-    }
-    if (!pinned_plug_que_empty_p() && (last_plug == pinned_plug (oldest_pin())))
-    {
-        size_t  entry = deque_pinned_plug();
-        mark*  m = pinned_plug_of (entry);
-        size_t saved_pinned_len = pinned_len(m);
-        pinned_len(m) = last_plug - last_pinned_gap;
-        if (m->has_post_plug_info())
-        {
-            last_plug_size += sizeof (gap_reloc_pair);
-            dprintf (3, ("ra pinned %p was shortened, adjusting plug size to %zx", last_plug, last_plug_size))
-        }
-        last_pinned_gap = last_plug + last_plug_size;
-        dprintf (3, ("ra found pin %p, len: %zx->%zx, last_p: %p, last_p_size: %zx",
-            pinned_plug (m), saved_pinned_len, pinned_len (m), last_plug, last_plug_size));
-        leftp = FALSE;
-        {
-            size_t end_card = card_of (align_on_card (last_plug + last_plug_size));
-            size_t card = card_of (last_plug);
-            while (card != end_card)
-            {
-                set_card (card);
-                card++;
-            }
-        }
-    }
-    else if (last_plug >= start_address)
-    {
-#ifdef FEATURE_STRUCTALIGN
-        int requiredAlignment;
-        ptrdiff_t pad;
-        node_aligninfo (last_plug, requiredAlignment, pad);
-        uint8_t* reloc_plug = last_plug + node_relocation_distance (last_plug);
-        ptrdiff_t alignmentOffset = ComputeStructAlignPad(reloc_plug, requiredAlignment, 0);
-        if (!alignmentOffset)
-        {
-            alignmentOffset = requiredAlignment;
-        }
-        clear_node_aligninfo (last_plug);
-#else // FEATURE_STRUCTALIGN
-        clear_node_realigned (last_plug);
-#endif // FEATURE_STRUCTALIGN
-        BOOL adjacentp = FALSE;
-        BOOL set_padding_on_saved_p = FALSE;
-        if (shortened_p)
-        {
-            last_plug_size += sizeof (gap_reloc_pair);
-#ifdef SHORT_PLUGS
-            assert (pinned_plug_entry != NULL);
-            if (last_plug_size <= sizeof (plug_and_gap))
-            {
-                set_padding_on_saved_p = TRUE;
-            }
-#endif //SHORT_PLUGS
-            dprintf (3, ("ra plug %p was shortened, adjusting plug size to %zx", last_plug, last_plug_size))
-        }
-#ifdef SHORT_PLUGS
-        clear_padding_in_expand (last_plug, set_padding_on_saved_p, pinned_plug_entry);
-#endif //SHORT_PLUGS
-        uint8_t* new_address = allocate_in_expanded_heap(gen, last_plug_size, adjacentp, last_plug,
-#ifdef SHORT_PLUGS
-                                     set_padding_on_saved_p,
-                                     pinned_plug_entry,
-#endif //SHORT_PLUGS
-                                     TRUE, active_new_gen_number REQD_ALIGN_AND_OFFSET_ARG);
-        dprintf (3, ("ra NA: [%p, %p[: %zx", new_address, (new_address + last_plug_size), last_plug_size));
-        assert (new_address);
-        set_node_relocation_distance (last_plug, new_address - last_plug);
-#ifdef FEATURE_STRUCTALIGN
-        if (leftp && node_alignpad (last_plug) == 0)
-#else // FEATURE_STRUCTALIGN
-        if (leftp && !node_realigned (last_plug))
-#endif // FEATURE_STRUCTALIGN
-        {
-        }
-        dprintf (3,(" Re-allocating %zx->%zx len %zd", (size_t)last_plug, (size_t)new_address, last_plug_size));
-        leftp = adjacentp;
-    }
-}
-void gc_heap::realloc_in_brick (uint8_t* tree, uint8_t*& last_plug,
-                                uint8_t* start_address,
-                                generation* gen,
-                                unsigned int& active_new_gen_number,
-                                uint8_t*& last_pinned_gap, BOOL& leftp)
-{
-    assert (tree != NULL);
-    int   left_node = node_left_child (tree);
-    int   right_node = node_right_child (tree);
-    dprintf (3, ("ra: tree: %p, last_pin_gap: %p, last_p: %p, L: %d, R: %d",
-        tree, last_pinned_gap, last_plug, left_node, right_node));
-    if (left_node)
-    {
-        dprintf (3, ("LN: realloc %p(%p)", (tree + left_node), last_plug));
-        realloc_in_brick ((tree + left_node), last_plug, start_address,
-                          gen, active_new_gen_number, last_pinned_gap,
-                          leftp);
-    }
-    if (last_plug != 0)
-    {
-        uint8_t*  plug = tree;
-        BOOL has_pre_plug_info_p = FALSE;
-        BOOL has_post_plug_info_p = FALSE;
-        mark* pinned_plug_entry = get_next_pinned_entry (tree,
-                                                         &has_pre_plug_info_p,
-                                                         &has_post_plug_info_p,
-                                                         FALSE);
-        size_t gap_size = node_gap_size (plug);
-        uint8_t*   gap = (plug - gap_size);
-        uint8_t*  last_plug_end = gap;
-        size_t  last_plug_size = (last_plug_end - last_plug);
-        dprintf (3, ("ra: plug %p, gap size: %zd, last_pin_gap: %p, last_p: %p, last_p_end: %p, shortened: %d",
-            plug, gap_size, last_pinned_gap, last_plug, last_plug_end, (has_pre_plug_info_p ? 1 : 0)));
-        realloc_plug (last_plug_size, last_plug, gen, start_address,
-                      active_new_gen_number, last_pinned_gap,
-                      leftp, has_pre_plug_info_p
-#ifdef SHORT_PLUGS
-                      , pinned_plug_entry
-#endif //SHORT_PLUGS
-                      );
-    }
-    last_plug = tree;
-    if (right_node)
-    {
-        dprintf (3, ("RN: realloc %p(%p)", (tree + right_node), last_plug));
-        realloc_in_brick ((tree + right_node), last_plug, start_address,
-                          gen, active_new_gen_number, last_pinned_gap,
-                          leftp);
-    }
-}
-void
-gc_heap::realloc_plugs (generation* consing_gen, heap_segment* seg,
-                        uint8_t* start_address, uint8_t* end_address,
-                        unsigned active_new_gen_number)
-{
-    dprintf (3, ("--- Reallocing ---"));
-    if (use_bestfit)
-    {
-        int  gen_number = max_generation - 1;
-        while (gen_number >= 0)
-        {
-            generation* gen = generation_of (gen_number);
-            if (0 == generation_plan_allocation_start (gen))
-            {
-                generation_plan_allocation_start (gen) =
-                    bestfit_first_pin + (max_generation - gen_number - 1) * Align (min_obj_size);
-                generation_plan_allocation_start_size (gen) = Align (min_obj_size);
-                assert (generation_plan_allocation_start (gen));
-            }
-            gen_number--;
-        }
-    }
-    uint8_t* first_address = start_address;
-    reset_pinned_queue_bos();
-    uint8_t* planned_ephemeral_seg_end = heap_segment_plan_allocated (seg);
-    while (!pinned_plug_que_empty_p())
-    {
-        mark* m = oldest_pin();
-        if ((pinned_plug (m) >= planned_ephemeral_seg_end) && (pinned_plug (m) < end_address))
-        {
-            if (pinned_plug (m) < first_address)
-            {
-                first_address = pinned_plug (m);
-            }
-            break;
-        }
-        else
-            deque_pinned_plug();
-    }
-    size_t  current_brick = brick_of (first_address);
-    size_t  end_brick = brick_of (end_address-1);
-    uint8_t*  last_plug = 0;
-    uint8_t* last_pinned_gap = heap_segment_plan_allocated (seg);
-    BOOL leftp = FALSE;
-    dprintf (3, ("start addr: %p, first addr: %p, current oldest pin: %p",
-        start_address, first_address, pinned_plug (oldest_pin())));
-    while (current_brick <= end_brick)
-    {
-        int   brick_entry =  brick_table [ current_brick ];
-        if (brick_entry >= 0)
-        {
-            realloc_in_brick ((brick_address (current_brick) + brick_entry - 1),
-                              last_plug, start_address, consing_gen,
-                              active_new_gen_number, last_pinned_gap,
-                              leftp);
-        }
-        current_brick++;
-    }
-    if (last_plug != 0)
-    {
-        realloc_plug (end_address - last_plug, last_plug, consing_gen,
-                      start_address,
-                      active_new_gen_number, last_pinned_gap,
-                      leftp, FALSE
-#ifdef SHORT_PLUGS
-                      , NULL
-#endif //SHORT_PLUGS
-                      );
-    }
-    assert (last_pinned_gap >= heap_segment_mem (seg));
-    assert (last_pinned_gap <= heap_segment_committed (seg));
-    heap_segment_plan_allocated (seg) = last_pinned_gap;
-}
-void gc_heap::set_expand_in_full_gc (int condemned_gen_number)
-{
-    if (!should_expand_in_full_gc)
-    {
-        if ((condemned_gen_number != max_generation) &&
-            (settings.pause_mode != pause_low_latency) &&
-            (settings.pause_mode != pause_sustained_low_latency))
-        {
-            should_expand_in_full_gc = TRUE;
-        }
-    }
-}
-void gc_heap::save_ephemeral_generation_starts()
-{
-    for (int ephemeral_generation = 0; ephemeral_generation < max_generation; ephemeral_generation++)
-    {
-        saved_ephemeral_plan_start[ephemeral_generation] =
-            generation_plan_allocation_start (generation_of (ephemeral_generation));
-        saved_ephemeral_plan_start_size[ephemeral_generation] =
-            generation_plan_allocation_start_size (generation_of (ephemeral_generation));
-    }
-}
-generation* gc_heap::expand_heap (int condemned_generation,
-                                  generation* consing_gen,
-                                  heap_segment* new_heap_segment)
-{
-#ifndef _DEBUG
-    UNREFERENCED_PARAMETER(condemned_generation);
-#endif //!_DEBUG
-    assert (condemned_generation >= (max_generation -1));
-    unsigned int active_new_gen_number = max_generation; //Set one too high to get generation gap
-    uint8_t*  start_address = generation_limit (max_generation);
-    uint8_t*  end_address = heap_segment_allocated (ephemeral_heap_segment);
-    BOOL should_promote_ephemeral = FALSE;
-    ptrdiff_t eph_size = total_ephemeral_size;
-#ifdef BACKGROUND_GC
-    dprintf(2,("%s: ---- Heap Expansion ----", (gc_heap::background_running_p() ? "FGC" : "NGC")));
-#endif //BACKGROUND_GC
-    settings.heap_expansion = TRUE;
-    dprintf (2, ("Elevation: elevation = el_none"));
-    if (settings.should_lock_elevation && !expand_reused_seg_p())
-        settings.should_lock_elevation = FALSE;
-    heap_segment* new_seg = new_heap_segment;
-    if (!new_seg)
-        return consing_gen;
-    if (g_gc_card_table!= card_table)
-        copy_brick_card_table();
-    BOOL new_segment_p = (heap_segment_next (new_seg) == 0);
-    dprintf (2, ("new_segment_p %zx", (size_t)new_segment_p));
-    assert (generation_plan_allocation_start (generation_of (max_generation-1)));
-    assert (generation_plan_allocation_start (generation_of (max_generation-1)) >=
-            heap_segment_mem (ephemeral_heap_segment));
-    assert (generation_plan_allocation_start (generation_of (max_generation-1)) <=
-            heap_segment_committed (ephemeral_heap_segment));
-    assert (generation_plan_allocation_start (youngest_generation));
-    assert (generation_plan_allocation_start (youngest_generation) <
-            heap_segment_plan_allocated (ephemeral_heap_segment));
-    if (settings.pause_mode == pause_no_gc)
-    {
-        if ((size_t)(heap_segment_reserved (new_seg) - heap_segment_mem (new_seg)) < (eph_size + soh_allocation_no_gc))
-            should_promote_ephemeral = TRUE;
-    }
-    else
-    {
-        if (!use_bestfit)
-        {
-            should_promote_ephemeral = dt_low_ephemeral_space_p (tuning_deciding_promote_ephemeral);
-        }
-    }
-    if (should_promote_ephemeral)
-    {
-        ephemeral_promotion = TRUE;
-        get_gc_data_per_heap()->set_mechanism (gc_heap_expand, expand_new_seg_ep);
-        dprintf (2, ("promoting ephemeral"));
-        save_ephemeral_generation_starts();
-        generation* max_gen = generation_of (max_generation);
-        for (int i = 1; i < max_generation; i++)
-        {
-            generation_free_obj_space (max_gen) +=
-                generation_free_obj_space (generation_of (i));
-            dprintf (2, ("[h%d] maxgen freeobj + %zd=%zd",
-                heap_number, generation_free_obj_space (generation_of (i)),
-                generation_free_obj_space (max_gen)));
-        }
-        heap_segment_used (new_seg) = heap_segment_committed (new_seg);
-    }
-    else
-    {
-        if ((eph_size > 0) && new_segment_p)
-        {
-#ifdef FEATURE_STRUCTALIGN
-            eph_size += ComputeStructAlignPad(heap_segment_mem (new_seg), MAX_STRUCTALIGN, OBJECT_ALIGNMENT_OFFSET);
-#endif // FEATURE_STRUCTALIGN
-#ifdef RESPECT_LARGE_ALIGNMENT
-            eph_size += switch_alignment_size(FALSE);
-#endif //RESPECT_LARGE_ALIGNMENT
-            if (grow_heap_segment (new_seg, heap_segment_mem (new_seg) + eph_size) == 0)
-            {
-                fgm_result.set_fgm (fgm_commit_eph_segment, eph_size, FALSE);
-                return consing_gen;
-            }
-            heap_segment_used (new_seg) = heap_segment_committed (new_seg);
-        }
-        heap_segment_plan_allocated (ephemeral_heap_segment) =
-            generation_plan_allocation_start (generation_of (max_generation-1));
-        dprintf (3, ("Old ephemeral allocated set to %zx",
-                    (size_t)heap_segment_plan_allocated (ephemeral_heap_segment)));
-    }
-    if (new_segment_p)
-    {
-        size_t first_brick = brick_of (heap_segment_mem (new_seg));
-        set_brick (first_brick,
-                heap_segment_mem (new_seg) - brick_address (first_brick));
-    }
-    generation_allocation_limit (consing_gen) =
-        heap_segment_plan_allocated (ephemeral_heap_segment);
-    generation_allocation_pointer (consing_gen) = generation_allocation_limit (consing_gen);
-    generation_allocation_segment (consing_gen) = ephemeral_heap_segment;
-    {
-        int generation_num = max_generation-1;
-        while (generation_num >= 0)
-        {
-            generation* gen = generation_of (generation_num);
-            generation_plan_allocation_start (gen) = 0;
-            generation_num--;
-        }
-    }
-    heap_segment* old_seg = ephemeral_heap_segment;
-    ephemeral_heap_segment = new_seg;
-    consing_gen = ensure_ephemeral_heap_segment (consing_gen);
-    if (!should_promote_ephemeral)
-    {
-        realloc_plugs (consing_gen, old_seg, start_address, end_address,
-                    active_new_gen_number);
-    }
-    if (!use_bestfit)
-    {
-        repair_allocation_in_expanded_heap (consing_gen);
-    }
-#ifdef _DEBUG
-    {
-        int generation_num = max_generation-1;
-        while (generation_num >= 0)
-        {
-            generation* gen = generation_of (generation_num);
-            assert (generation_plan_allocation_start (gen));
-            generation_num--;
-        }
-    }
-#endif // _DEBUG
-    if (!new_segment_p)
-    {
-        dprintf (2, ("Demoting ephemeral segment"));
-        settings.demotion = TRUE;
-        get_gc_data_per_heap()->set_mechanism_bit (gc_demotion_bit);
-        demotion_low = heap_segment_mem (ephemeral_heap_segment);
-        demotion_high = heap_segment_reserved (ephemeral_heap_segment);
-    }
-    else
-    {
-        demotion_low = MAX_PTR;
-        demotion_high = 0;
-#ifndef MULTIPLE_HEAPS
-        settings.demotion = FALSE;
-        get_gc_data_per_heap()->clear_mechanism_bit (gc_demotion_bit);
-#endif //!MULTIPLE_HEAPS
-    }
-    if (!should_promote_ephemeral && new_segment_p)
-    {
-        assert ((ptrdiff_t)total_ephemeral_size <= eph_size);
-    }
-    if (heap_segment_mem (old_seg) == heap_segment_plan_allocated (old_seg))
-    {
-        verify_no_pins (heap_segment_mem (old_seg), heap_segment_reserved (old_seg));
-    }
-    verify_no_pins (heap_segment_plan_allocated (old_seg), heap_segment_reserved(old_seg));
-    dprintf(2,("---- End of Heap Expansion ----"));
-    return consing_gen;
-}
-#endif //!USE_REGIONS
-BOOL gc_heap::expand_reused_seg_p()
-{
-#ifdef USE_REGIONS
-    return FALSE;
-#else
-    BOOL reused_seg = FALSE;
-    int heap_expand_mechanism = gc_data_per_heap.get_mechanism (gc_heap_expand);
-    if ((heap_expand_mechanism == expand_reuse_bestfit) ||
-        (heap_expand_mechanism == expand_reuse_normal))
-    {
-        reused_seg = TRUE;
-    }
-    return reused_seg;
-#endif //USE_REGIONS
-}
-void gc_heap::verify_no_pins (uint8_t* start, uint8_t* end)
-{
-#ifdef VERIFY_HEAP
-    if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
-    {
-        BOOL contains_pinned_plugs = FALSE;
-        size_t mi = 0;
-        mark* m = 0;
-        while (mi != mark_stack_tos)
-        {
-            m = pinned_plug_of (mi);
-            if ((pinned_plug (m) >= start) && (pinned_plug (m) < end))
-            {
-                contains_pinned_plugs = TRUE;
-                break;
-            }
-            else
-                mi++;
-        }
-        if (contains_pinned_plugs)
-        {
-            FATAL_GC_ERROR();
-        }
-    }
-#endif //VERIFY_HEAP
-}
-void gc_heap::set_static_data()
-{
-    static_data* pause_mode_sdata = static_data_table[latency_level];
-    for (int i = 0; i < total_generation_count; i++)
-    {
-        dynamic_data* dd = dynamic_data_of (i);
-        static_data* sdata = &pause_mode_sdata[i];
-        dd->sdata = sdata;
-        dd->min_size = sdata->min_size;
-        dprintf (GTC_LOG, ("PM: %d, gen%d:  min: %zd, max: %zd, fr_l: %zd, fr_b: %d%%",
-            settings.pause_mode,i,
-            dd->min_size, dd_max_size (dd),
-            sdata->fragmentation_limit, (int)(sdata->fragmentation_burden_limit * 100)));
-    }
-}
-void gc_heap::init_static_data()
-{
-    size_t gen0_min_size = get_gen0_min_size();
-    size_t gen0_max_size =
-#ifdef MULTIPLE_HEAPS
-        max (6*1024*1024, min ( Align(soh_segment_size/2), 200*1024*1024));
-#else //MULTIPLE_HEAPS
-        (
-#ifdef BACKGROUND_GC
-            gc_can_use_concurrent ?
-            6*1024*1024 :
-#endif //BACKGROUND_GC
-            max (6*1024*1024,  min ( Align(soh_segment_size/2), 200*1024*1024))
-        );
-#endif //MULTIPLE_HEAPS
-    gen0_max_size = max (gen0_min_size, gen0_max_size);
-    if (heap_hard_limit)
-    {
-        size_t gen0_max_size_seg = soh_segment_size / 4;
-        dprintf (GTC_LOG, ("limit gen0 max %zd->%zd", gen0_max_size, gen0_max_size_seg));
-        gen0_max_size = min (gen0_max_size, gen0_max_size_seg);
-    }
-    size_t gen0_max_size_config = (size_t)GCConfig::GetGCGen0MaxBudget();
-    if (gen0_max_size_config)
-    {
-        gen0_max_size = min (gen0_max_size, gen0_max_size_config);
-#ifdef FEATURE_EVENT_TRACE
-        gen0_max_budget_from_config = gen0_max_size;
-#endif //FEATURE_EVENT_TRACE
-    }
-    gen0_max_size = Align (gen0_max_size);
-    gen0_min_size = min (gen0_min_size, gen0_max_size);
-    size_t gen1_max_size = (size_t)
-#ifdef MULTIPLE_HEAPS
-        max (6*1024*1024, Align(soh_segment_size/2));
-#else //MULTIPLE_HEAPS
-        (
-#ifdef BACKGROUND_GC
-            gc_can_use_concurrent ?
-            6*1024*1024 :
-#endif //BACKGROUND_GC
-            max (6*1024*1024, Align(soh_segment_size/2))
-        );
-#endif //MULTIPLE_HEAPS
-    size_t gen1_max_size_config = (size_t)GCConfig::GetGCGen1MaxBudget();
-    if (gen1_max_size_config)
-    {
-        gen1_max_size = min (gen1_max_size, gen1_max_size_config);
-    }
-    gen1_max_size = Align (gen1_max_size);
-    dprintf (GTC_LOG, ("gen0 min: %zd, max: %zd, gen1 max: %zd",
-        gen0_min_size, gen0_max_size, gen1_max_size));
-    for (int i = latency_level_first; i <= latency_level_last; i++)
-    {
-        static_data_table[i][0].min_size = gen0_min_size;
-        static_data_table[i][0].max_size = gen0_max_size;
-        static_data_table[i][1].max_size = gen1_max_size;
-    }
-}
-bool gc_heap::init_dynamic_data()
-{
-    uint64_t now_raw_ts = RawGetHighPrecisionTimeStamp ();
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-    start_raw_ts = now_raw_ts;
-#endif //HEAP_BALANCE_INSTRUMENTATION
-    uint64_t now = (uint64_t)((double)now_raw_ts * qpf_us);
-    set_static_data();
-    if (heap_number == 0)
-    {
-        process_start_time = now;
-        smoothed_desired_total[0] = dynamic_data_of (0)->min_size * n_heaps;
-#ifdef DYNAMIC_HEAP_COUNT
-        last_suspended_end_time = now;
-#endif //DYNAMIC_HEAP_COUNT
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-        last_gc_end_time_us = now;
-        dprintf (HEAP_BALANCE_LOG, ("qpf=%zd, start: %zd(%d)", qpf, start_raw_ts, now));
-#endif //HEAP_BALANCE_INSTRUMENTATION
-    }
-    for (int i = 0; i < total_generation_count; i++)
-    {
-        dynamic_data* dd = dynamic_data_of (i);
-        dd->gc_clock = 0;
-        dd->time_clock = now;
-        dd->previous_time_clock = now;
-        dd->current_size = 0;
-        dd->promoted_size = 0;
-        dd->collection_count = 0;
-        dd->new_allocation = dd->min_size;
-        dd->gc_new_allocation = dd->new_allocation;
-        dd->desired_allocation = dd->new_allocation;
-        dd->fragmentation = 0;
-    }
-    return true;
-}
-float gc_heap::surv_to_growth (float cst, float limit, float max_limit)
-{
-    if (cst < ((max_limit - limit ) / (limit * (max_limit-1.0f))))
-        return ((limit - limit*cst) / (1.0f - (cst * limit)));
-    else
-        return max_limit;
-}
-static size_t linear_allocation_model (float allocation_fraction, size_t new_allocation,
-                                       size_t previous_desired_allocation, float time_since_previous_collection_secs)
-{
-    if ((allocation_fraction < 0.95) && (allocation_fraction > 0.0))
-    {
-        const float decay_time = 5*60.0f; // previous desired allocation expires over 5 minutes
-        float decay_factor = (decay_time <= time_since_previous_collection_secs) ?
-                                0 :
-                                ((decay_time - time_since_previous_collection_secs) / decay_time);
-        float previous_allocation_factor = (1.0f - allocation_fraction) * decay_factor;
-        dprintf (2, ("allocation fraction: %d, decay factor: %d, previous allocation factor: %d",
-            (int)(allocation_fraction*100.0), (int)(decay_factor*100.0), (int)(previous_allocation_factor*100.0)));
-        new_allocation = (size_t)((1.0 - previous_allocation_factor)*new_allocation + previous_allocation_factor * previous_desired_allocation);
-    }
-    return new_allocation;
-}
-size_t gc_heap::desired_new_allocation (dynamic_data* dd,
-                                        size_t out, int gen_number,
-                                        int pass)
-{
-    gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
-    if (dd_begin_data_size (dd) == 0)
-    {
-        size_t new_allocation = dd_min_size (dd);
-        current_gc_data_per_heap->gen_data[gen_number].new_allocation = new_allocation;
-        return new_allocation;
-    }
-    else
-    {
-        float     cst;
-        size_t    previous_desired_allocation = dd_desired_allocation (dd);
-        size_t    current_size = dd_current_size (dd);
-        float     max_limit = dd_max_limit (dd);
-        float     limit = dd_limit (dd);
-        size_t    min_gc_size = dd_min_size (dd);
-        float     f = 0;
-        size_t    max_size = dd_max_size (dd);
-        size_t    new_allocation = 0;
-        float     time_since_previous_collection_secs = (dd_time_clock (dd) - dd_previous_time_clock (dd))*1e-6f;
-        float allocation_fraction = (float) (dd_desired_allocation (dd) - dd_gc_new_allocation (dd)) / (float) (dd_desired_allocation (dd));
-        if (gen_number >= max_generation)
-        {
-            size_t    new_size = 0;
-            cst = min (1.0f, float (out) / float (dd_begin_data_size (dd)));
-            f = surv_to_growth (cst, limit, max_limit);
-            if (conserve_mem_setting != 0)
-            {
-                float f_conserve = ((10.0f / conserve_mem_setting) - 1) * 0.5f + 1.0f;
-                f = min (f, f_conserve);
-            }
-            size_t max_growth_size = (size_t)(max_size / f);
-            if (current_size >= max_growth_size)
-            {
-                new_size = max_size;
-            }
-            else
-            {
-                new_size = (size_t) min (max ( (f * current_size), min_gc_size), max_size);
-            }
-            assert ((new_size >= current_size) || (new_size == max_size));
-            if (gen_number == max_generation)
-            {
-                new_allocation  =  max((new_size - current_size), min_gc_size);
-                new_allocation = linear_allocation_model (allocation_fraction, new_allocation,
-                                                          dd_desired_allocation (dd), time_since_previous_collection_secs);
-                if (
-#ifdef BGC_SERVO_TUNING
-                    !bgc_tuning::fl_tuning_triggered &&
-#endif //BGC_SERVO_TUNING
-                    (conserve_mem_setting == 0) &&
-                    (dd_fragmentation (dd) > ((size_t)((f-1)*current_size))))
-                {
-                    size_t new_allocation1 = max (min_gc_size,
-                                                  (size_t)((float)new_allocation * current_size /
-                                                           ((float)current_size + 2*dd_fragmentation (dd))));
-                    dprintf (2, ("Reducing max_gen allocation due to fragmentation from %zd to %zd",
-                                 new_allocation, new_allocation1));
-                    new_allocation = new_allocation1;
-                }
-            }
-            else // not a SOH generation
-            {
-                uint32_t memory_load = 0;
-                uint64_t available_physical = 0;
-                get_memory_info (&memory_load, &available_physical);
-#ifdef TRACE_GC
-                if (heap_hard_limit)
-                {
-                    size_t allocated = 0;
-                    size_t committed = uoh_committed_size (gen_number, &allocated);
-                    dprintf (1, ("GC#%zd h%d, GMI: UOH budget, UOH commit %zd (obj %zd, frag %zd), total commit: %zd (recorded: %zd)",
-                        (size_t)settings.gc_index, heap_number,
-                        committed, allocated,
-                        dd_fragmentation (dynamic_data_of (gen_number)),
-                        get_total_committed_size(), (current_total_committed - current_total_committed_bookkeeping)));
-                }
-#endif //TRACE_GC
-                if (heap_number == 0)
-                    settings.exit_memory_load = memory_load;
-                if (available_physical > 1024*1024)
-                    available_physical -= 1024*1024;
-                uint64_t available_free = available_physical + (uint64_t)generation_free_list_space (generation_of (gen_number));
-                if (available_free > (uint64_t)MAX_PTR)
-                {
-                    available_free = (uint64_t)MAX_PTR;
-                }
-                new_allocation = max (min(max((new_size - current_size), dd_desired_allocation (dynamic_data_of (max_generation))),
-                                          (size_t)available_free),
-                                      max ((current_size/4), min_gc_size));
-                new_allocation = linear_allocation_model (allocation_fraction, new_allocation,
-                                                          dd_desired_allocation (dd), time_since_previous_collection_secs);
-            }
-        }
-        else
-        {
-            size_t survivors = out;
-            cst = float (survivors) / float (dd_begin_data_size (dd));
-            f = surv_to_growth (cst, limit, max_limit);
-            new_allocation = (size_t) min (max ((f * (survivors)), min_gc_size), max_size);
-            new_allocation = linear_allocation_model (allocation_fraction, new_allocation,
-                                                      dd_desired_allocation (dd), time_since_previous_collection_secs);
-            if (gen_number == 0)
-            {
-                if (pass == 0)
-                {
-                    size_t free_space = generation_free_list_space (generation_of (gen_number));
-                    dprintf (GTC_LOG, ("frag: %zd, min: %zd", free_space, min_gc_size));
-                    if (free_space > min_gc_size)
-                    {
-                        settings.gen0_reduction_count = 2;
-                    }
-                    else
-                    {
-                        if (settings.gen0_reduction_count > 0)
-                            settings.gen0_reduction_count--;
-                    }
-                }
-                if (settings.gen0_reduction_count > 0)
-                {
-                    dprintf (2, ("Reducing new allocation based on fragmentation"));
-                    new_allocation = min (new_allocation,
-                                          max (min_gc_size, (max_size/3)));
-                }
-#ifdef DYNAMIC_HEAP_COUNT
-                if (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes)
-                {
-                    float f_older_gen = ((10.0f / conserve_mem_setting) - 1) * 0.5f;
-                    size_t older_size = 0;
-                    for (int gen_index_older = 1; gen_index_older < total_generation_count; gen_index_older++)
-                    {
-                        dynamic_data* dd_older = dynamic_data_of (gen_index_older);
-                        older_size += dd_current_size (dd_older);
-                    }
-                    size_t new_allocation_from_older = (size_t)(older_size*f_older_gen);
-                    new_allocation = min (new_allocation, new_allocation_from_older);
-                    new_allocation = max (new_allocation, min_gc_size);
-                    dprintf (2, ("f_older_gen: %d%% older_size: %zd new_allocation: %zd",
-                        (int)(f_older_gen*100),
-                        older_size,
-                        new_allocation));
-                }
-#endif //DYNAMIC_HEAP_COUNT
-            }
-        }
-        size_t new_allocation_ret = Align (new_allocation, get_alignment_constant (gen_number <= max_generation));
-        int gen_data_index = gen_number;
-        gc_generation_data* gen_data = &(current_gc_data_per_heap->gen_data[gen_data_index]);
-        gen_data->new_allocation = new_allocation_ret;
-        dd_surv (dd) = cst;
-        dprintf (1, (ThreadStressLog::gcDesiredNewAllocationMsg(),
-                    heap_number, gen_number, out, current_size, (dd_desired_allocation (dd) - dd_gc_new_allocation (dd)),
-                    (int)(cst*100), (int)(f*100), current_size + new_allocation, new_allocation));
-        return new_allocation_ret;
-    }
-}
-size_t gc_heap::generation_plan_size (int gen_number)
-{
-#ifdef USE_REGIONS
-    size_t result = 0;
-    heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (gen_number)));
-    while (seg)
-    {
-        uint8_t* end = heap_segment_plan_allocated (seg);
-        result += end - heap_segment_mem (seg);
-        dprintf (REGIONS_LOG, ("h%d size + %zd (%p - %p) -> %zd",
-            heap_number, (end - heap_segment_mem (seg)),
-            heap_segment_mem (seg), end, result));
-        seg = heap_segment_next (seg);
-    }
-    return result;
-#else //USE_REGIONS
-    if (0 == gen_number)
-        return max((heap_segment_plan_allocated (ephemeral_heap_segment) -
-                    generation_plan_allocation_start (generation_of (gen_number))),
-                   (int)Align (min_obj_size));
-    else
-    {
-        generation* gen = generation_of (gen_number);
-        if (heap_segment_rw (generation_start_segment (gen)) == ephemeral_heap_segment)
-            return (generation_plan_allocation_start (generation_of (gen_number - 1)) -
-                    generation_plan_allocation_start (generation_of (gen_number)));
-        else
-        {
-            size_t gensize = 0;
-            heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-            PREFIX_ASSUME(seg != NULL);
-            while (seg && (seg != ephemeral_heap_segment))
-            {
-                gensize += heap_segment_plan_allocated (seg) -
-                           heap_segment_mem (seg);
-                seg = heap_segment_next_rw (seg);
-            }
-            if (seg)
-            {
-                gensize += (generation_plan_allocation_start (generation_of (gen_number - 1)) -
-                            heap_segment_mem (ephemeral_heap_segment));
-            }
-            return gensize;
-        }
-    }
-#endif //USE_REGIONS
-}
-size_t gc_heap::generation_size (int gen_number)
-{
-#ifdef USE_REGIONS
-    size_t result = 0;
-    heap_segment* seg = heap_segment_rw (generation_start_segment (generation_of (gen_number)));
-    while (seg)
-    {
-        uint8_t* end = heap_segment_allocated (seg);
-        result += end - heap_segment_mem (seg);
-        dprintf (2, ("h%d size + %zd (%p - %p) -> %zd",
-            heap_number, (end - heap_segment_mem (seg)),
-            heap_segment_mem (seg), end, result));
-        seg = heap_segment_next (seg);
-    }
-    return result;
-#else //USE_REGIONS
-    if (0 == gen_number)
-        return max((heap_segment_allocated (ephemeral_heap_segment) -
-                    generation_allocation_start (generation_of (gen_number))),
-                   (int)Align (min_obj_size));
-    else
-    {
-        generation* gen = generation_of (gen_number);
-        if (heap_segment_rw (generation_start_segment (gen)) == ephemeral_heap_segment)
-            return (generation_allocation_start (generation_of (gen_number - 1)) -
-                    generation_allocation_start (generation_of (gen_number)));
-        else
-        {
-            size_t gensize = 0;
-            heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-            PREFIX_ASSUME(seg != NULL);
-            while (seg && (seg != ephemeral_heap_segment))
-            {
-                gensize += heap_segment_allocated (seg) -
-                           heap_segment_mem (seg);
-                seg = heap_segment_next_rw (seg);
-            }
-            if (seg)
-            {
-                gensize += (generation_allocation_start (generation_of (gen_number - 1)) -
-                            heap_segment_mem (ephemeral_heap_segment));
-            }
-            return gensize;
-        }
-    }
-#endif //USE_REGIONS
-}
-size_t  gc_heap::compute_in (int gen_number)
-{
-    assert (gen_number != 0);
-    dynamic_data* dd = dynamic_data_of (gen_number);
-    size_t in = generation_allocation_size (generation_of (gen_number));
-#ifndef USE_REGIONS
-    if (gen_number == max_generation && ephemeral_promotion)
-    {
-        in = 0;
-        for (int i = 0; i <= max_generation; i++)
-        {
-            dynamic_data* dd = dynamic_data_of (i);
-            in += dd_survived_size (dd);
-            if (i != max_generation)
-            {
-                generation_condemned_allocated (generation_of (gen_number)) += dd_survived_size (dd);
-            }
-        }
-    }
-#endif //!USE_REGIONS
-    dd_gc_new_allocation (dd) -= in;
-    dd_new_allocation (dd) = dd_gc_new_allocation (dd);
-    gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
-    gc_generation_data* gen_data = &(current_gc_data_per_heap->gen_data[gen_number]);
-    gen_data->in = in;
-    generation_allocation_size (generation_of (gen_number)) = 0;
-    return in;
-}
-void  gc_heap::compute_promoted_allocation (int gen_number)
-{
-    compute_in (gen_number);
-}
-#ifdef HOST_64BIT
-inline
-size_t gc_heap::trim_youngest_desired (uint32_t memory_load,
-                                       size_t total_new_allocation,
-                                       size_t total_min_allocation)
-{
-    if (memory_load < MAX_ALLOWED_MEM_LOAD)
-    {
-        size_t remain_memory_load = (MAX_ALLOWED_MEM_LOAD - memory_load) * mem_one_percent;
-        return min (total_new_allocation, remain_memory_load);
-    }
-    else
-    {
-        size_t total_max_allocation = max (mem_one_percent, total_min_allocation);
-        return min (total_new_allocation, total_max_allocation);
-    }
-}
-size_t gc_heap::joined_youngest_desired (size_t new_allocation)
-{
-    dprintf (2, ("Entry memory load: %d; gen0 new_alloc: %zd", settings.entry_memory_load, new_allocation));
-    size_t final_new_allocation = new_allocation;
-    if (new_allocation > MIN_YOUNGEST_GEN_DESIRED)
-    {
-        uint32_t num_heaps = 1;
-#ifdef MULTIPLE_HEAPS
-        num_heaps = gc_heap::n_heaps;
-#endif //MULTIPLE_HEAPS
-        size_t total_new_allocation = new_allocation * num_heaps;
-        size_t total_min_allocation = MIN_YOUNGEST_GEN_DESIRED * num_heaps;
-        if ((settings.entry_memory_load >= MAX_ALLOWED_MEM_LOAD) ||
-            (total_new_allocation > max (youngest_gen_desired_th, total_min_allocation)))
-        {
-            uint32_t memory_load = 0;
-            get_memory_info (&memory_load);
-            settings.exit_memory_load = memory_load;
-            dprintf (2, ("Current memory load: %d", memory_load));
-            size_t final_total =
-                trim_youngest_desired (memory_load, total_new_allocation, total_min_allocation);
-            size_t max_new_allocation =
-#ifdef MULTIPLE_HEAPS
-                                         dd_max_size (g_heaps[0]->dynamic_data_of (0));
-#else //MULTIPLE_HEAPS
-                                         dd_max_size (dynamic_data_of (0));
-#endif //MULTIPLE_HEAPS
-            final_new_allocation  = min (Align ((final_total / num_heaps), get_alignment_constant (TRUE)), max_new_allocation);
-        }
-    }
-    if (final_new_allocation < new_allocation)
-    {
-        settings.gen0_reduction_count = 2;
-    }
-    return final_new_allocation;
-}
-#endif // HOST_64BIT
-inline
-gc_history_global* gc_heap::get_gc_data_global()
-{
-#ifdef BACKGROUND_GC
-    return (settings.concurrent ? &bgc_data_global : &gc_data_global);
-#else
-    return &gc_data_global;
-#endif //BACKGROUND_GC
-}
-inline
-gc_history_per_heap* gc_heap::get_gc_data_per_heap()
-{
-#ifdef BACKGROUND_GC
-    return (settings.concurrent ? &bgc_data_per_heap : &gc_data_per_heap);
-#else
-    return &gc_data_per_heap;
-#endif //BACKGROUND_GC
-}
-void gc_heap::compute_new_dynamic_data (int gen_number)
-{
-    PREFIX_ASSUME(gen_number >= 0);
-    PREFIX_ASSUME(gen_number <= max_generation);
-    dynamic_data* dd = dynamic_data_of (gen_number);
-    generation*   gen = generation_of (gen_number);
-    size_t        in = (gen_number==0) ? 0 : compute_in (gen_number);
-    size_t total_gen_size = generation_size (gen_number);
-    dd_fragmentation (dd) = generation_free_list_space (gen) + generation_free_obj_space (gen);
-    if (dd_fragmentation (dd) <= total_gen_size)
-        dd_current_size (dd) = total_gen_size - dd_fragmentation (dd);
-    else
-        dd_current_size (dd) = 0;
-    gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
-    size_t out = dd_survived_size (dd);
-    gc_generation_data* gen_data = &(current_gc_data_per_heap->gen_data[gen_number]);
-    gen_data->size_after = total_gen_size;
-    gen_data->free_list_space_after = generation_free_list_space (gen);
-    gen_data->free_obj_space_after = generation_free_obj_space (gen);
-    if ((settings.pause_mode == pause_low_latency) && (gen_number <= 1))
-    {
-        dd_desired_allocation (dd) = low_latency_alloc;
-        dd_gc_new_allocation (dd) = dd_desired_allocation (dd);
-        dd_new_allocation (dd) = dd_gc_new_allocation (dd);
-    }
-    else
-    {
-        if (gen_number == 0)
-        {
-            size_t final_promoted = 0;
-            final_promoted = min (finalization_promoted_bytes, out);
-            PREFIX_ASSUME(final_promoted <= out);
-            dprintf (2, ("gen: %d final promoted: %zd", gen_number, final_promoted));
-            dd_freach_previous_promotion (dd) = final_promoted;
-            size_t lower_bound = desired_new_allocation  (dd, out-final_promoted, gen_number, 0);
-            if (settings.condemned_generation == 0)
-            {
-                dd_desired_allocation (dd) = lower_bound;
-            }
-            else
-            {
-                size_t higher_bound = desired_new_allocation (dd, out, gen_number, 1);
-                if (dd_desired_allocation (dd) < lower_bound)
-                {
-                    dd_desired_allocation (dd) = lower_bound;
-                }
-                else if (dd_desired_allocation (dd) > higher_bound)
-                {
-                    dd_desired_allocation (dd) = higher_bound;
-                }
-#if defined (HOST_64BIT) && !defined (MULTIPLE_HEAPS)
-                dd_desired_allocation (dd) = joined_youngest_desired (dd_desired_allocation (dd));
-#endif // HOST_64BIT && !MULTIPLE_HEAPS
-                trim_youngest_desired_low_memory();
-                dprintf (2, ("final gen0 new_alloc: %zd", dd_desired_allocation (dd)));
-            }
-        }
-        else
-        {
-            dd_desired_allocation (dd) = desired_new_allocation (dd, out, gen_number, 0);
-        }
-        dd_gc_new_allocation (dd) = dd_desired_allocation (dd);
-#ifdef USE_REGIONS
-        dd_new_allocation (dd) = dd_gc_new_allocation (dd) - in;
-#else //USE_REGIONS
-        dd_new_allocation (dd) = dd_gc_new_allocation (dd);
-#endif //USE_REGIONS
-    }
-    gen_data->pinned_surv = dd_pinned_survived_size (dd);
-    gen_data->npinned_surv = dd_survived_size (dd) - dd_pinned_survived_size (dd);
-    dd_promoted_size (dd) = out;
-    if (gen_number == max_generation)
-    {
-        for (int i = (gen_number + 1); i < total_generation_count; i++)
-        {
-            dd = dynamic_data_of (i);
-            total_gen_size = generation_size (i);
-            generation* gen = generation_of (i);
-            dd_fragmentation (dd) = generation_free_list_space (gen) +
-                generation_free_obj_space (gen);
-            dd_current_size (dd) = total_gen_size - dd_fragmentation (dd);
-            dd_survived_size (dd) = dd_current_size (dd);
-            in = 0;
-            out = dd_current_size (dd);
-            dd_desired_allocation (dd) = desired_new_allocation (dd, out, i, 0);
-            dd_gc_new_allocation (dd) = Align (dd_desired_allocation (dd),
-                get_alignment_constant (FALSE));
-            dd_new_allocation (dd) = dd_gc_new_allocation (dd);
-            gen_data = &(current_gc_data_per_heap->gen_data[i]);
-            gen_data->size_after = total_gen_size;
-            gen_data->free_list_space_after = generation_free_list_space (gen);
-            gen_data->free_obj_space_after = generation_free_obj_space (gen);
-            gen_data->npinned_surv = out;
-#ifdef BACKGROUND_GC
-            if (i == loh_generation)
-                end_loh_size = total_gen_size;
-            if (i == poh_generation)
-                end_poh_size = total_gen_size;
-#endif //BACKGROUND_GC
-            dd_promoted_size (dd) = out;
-        }
-    }
-}
-void gc_heap::trim_youngest_desired_low_memory()
-{
-    if (g_low_memory_status)
-    {
-        size_t committed_mem = committed_size();
-        dynamic_data* dd = dynamic_data_of (0);
-        size_t current = dd_desired_allocation (dd);
-        size_t candidate = max (Align ((committed_mem / 10), get_alignment_constant(FALSE)), dd_min_size (dd));
-        dd_desired_allocation (dd) = min (current, candidate);
-    }
-}
-ptrdiff_t gc_heap::estimate_gen_growth (int gen_number)
-{
-    dynamic_data* dd_gen = dynamic_data_of (gen_number);
-    generation *gen = generation_of (gen_number);
-    ptrdiff_t new_allocation_gen = dd_new_allocation (dd_gen);
-    ptrdiff_t free_list_space_gen = generation_free_list_space (gen);
-#ifdef USE_REGIONS
-    ptrdiff_t reserved_not_in_use = 0;
-    ptrdiff_t allocated_gen = 0;
-    for (heap_segment* region = generation_start_segment_rw (gen); region != nullptr; region = heap_segment_next (region))
-    {
-        allocated_gen += heap_segment_allocated (region) - heap_segment_mem (region);
-        reserved_not_in_use += heap_segment_reserved (region) - heap_segment_allocated (region);
-    }
-    double free_list_fraction_gen = (allocated_gen == 0) ? 0.0 : (double)(free_list_space_gen) / (double)allocated_gen;
-    ptrdiff_t usable_free_space = (ptrdiff_t)(free_list_fraction_gen * free_list_space_gen);
-    ptrdiff_t budget_gen = new_allocation_gen - usable_free_space - reserved_not_in_use;
-    dprintf (REGIONS_LOG, ("h%2d gen %d budget %zd allocated: %zd, FL: %zd, reserved_not_in_use %zd budget_gen %zd",
-        heap_number, gen_number, new_allocation_gen, allocated_gen, free_list_space_gen, reserved_not_in_use, budget_gen));
-#else  //USE_REGIONS
-    ptrdiff_t budget_gen = new_allocation_gen - (free_list_space_gen / 2);
-    dprintf (REGIONS_LOG, ("budget for gen %d on heap %d is %zd (new %zd, free %zd)",
-        gen_number, heap_number, budget_gen, new_allocation_gen, free_list_space_gen));
-#endif //USE_REGIONS
-    return budget_gen;
-}
-void gc_heap::decommit_ephemeral_segment_pages()
-{
-    if (settings.concurrent || use_large_pages_p || (settings.pause_mode == pause_no_gc))
-    {
-        return;
-    }
-#if defined(MULTIPLE_HEAPS) && defined(USE_REGIONS)
-    for (int gen_number = soh_gen0; gen_number <= soh_gen1; gen_number++)
-    {
-        generation *gen = generation_of (gen_number);
-        heap_segment* tail_region = generation_tail_region (gen);
-        uint8_t* previous_decommit_target = heap_segment_decommit_target (tail_region);
-        for (heap_segment* region = generation_start_segment_rw (gen); region != nullptr; region = heap_segment_next (region))
-        {
-            heap_segment_decommit_target (region) = heap_segment_reserved (region);
-        }
-        ptrdiff_t budget_gen = estimate_gen_growth (gen_number) + loh_size_threshold;
-        if (budget_gen >= 0)
-        {
-            continue;
-        }
-        ptrdiff_t tail_region_size = heap_segment_reserved (tail_region) - heap_segment_mem (tail_region);
-        ptrdiff_t unneeded_tail_size = min (-budget_gen, tail_region_size);
-        uint8_t *decommit_target = heap_segment_reserved (tail_region) - unneeded_tail_size;
-        decommit_target = max (decommit_target, heap_segment_allocated (tail_region));
-        if (decommit_target < previous_decommit_target)
-        {
-            ptrdiff_t target_decrease = previous_decommit_target - decommit_target;
-            decommit_target += target_decrease * 2 / 3;
-        }
-#ifdef STRESS_DECOMMIT
-        decommit_target = heap_segment_mem (tail_region) + gc_rand::get_rand (heap_segment_reserved (tail_region) - heap_segment_mem (tail_region));
-#endif //STRESS_DECOMMIT
-        heap_segment_decommit_target (tail_region) = decommit_target;
-        if (decommit_target < heap_segment_committed (tail_region))
-        {
-            gradual_decommit_in_progress_p = TRUE;
-            dprintf (1, ("h%2d gen %d region %p allocated %zdkB committed %zdkB reduce_commit by %zdkB",
-                heap_number,
-                gen_number,
-                get_region_start (tail_region),
-                (heap_segment_allocated (tail_region) - get_region_start (tail_region))/1024,
-                (heap_segment_committed (tail_region) - get_region_start (tail_region))/1024,
-                (heap_segment_committed (tail_region) - decommit_target)/1024));
-        }
-        dprintf(3, ("h%2d gen %d allocated: %zdkB committed: %zdkB target: %zdkB",
-            heap_number,
-            gen_number,
-            (heap_segment_allocated (tail_region) - heap_segment_mem (tail_region))/1024,
-            (heap_segment_committed (tail_region) - heap_segment_mem (tail_region))/1024,
-            (decommit_target                      - heap_segment_mem (tail_region))/1024));
-    }
-#elif !defined(USE_REGIONS)
-    dynamic_data* dd0 = dynamic_data_of (0);
-    ptrdiff_t desired_allocation = dd_new_allocation (dd0) +
-                                   max (estimate_gen_growth (soh_gen1), 0) +
-                                   loh_size_threshold;
-    size_t slack_space =
-#ifdef HOST_64BIT
-                max(min(min(soh_segment_size/32, dd_max_size (dd0)), (generation_size (max_generation) / 10)), (size_t)desired_allocation);
-#else
-#ifdef FEATURE_CORECLR
-                desired_allocation;
-#else
-                dd_max_size (dd0);
-#endif //FEATURE_CORECLR
-#endif // HOST_64BIT
-    uint8_t *decommit_target = heap_segment_allocated (ephemeral_heap_segment) + slack_space;
-    if (decommit_target < heap_segment_decommit_target (ephemeral_heap_segment))
-    {
-        ptrdiff_t target_decrease = heap_segment_decommit_target (ephemeral_heap_segment) - decommit_target;
-        decommit_target += target_decrease * 2 / 3;
-    }
-    heap_segment_decommit_target (ephemeral_heap_segment) = decommit_target;
-#ifdef MULTIPLE_HEAPS
-    if (decommit_target < heap_segment_committed (ephemeral_heap_segment))
-    {
-        gradual_decommit_in_progress_p = TRUE;
-    }
-#ifdef _DEBUG
-    ephemeral_heap_segment->saved_committed = heap_segment_committed (ephemeral_heap_segment);
-    ephemeral_heap_segment->saved_desired_allocation = dd_desired_allocation (dd0);
-#endif // _DEBUG
-#endif // MULTIPLE_HEAPS
-#ifndef MULTIPLE_HEAPS
-    size_t ephemeral_elapsed = (size_t)((dd_time_clock (dd0) - gc_last_ephemeral_decommit_time) / 1000);
-    gc_last_ephemeral_decommit_time = dd_time_clock (dd0);
-    ptrdiff_t decommit_size = heap_segment_committed (ephemeral_heap_segment) - decommit_target;
-    ptrdiff_t max_decommit_size = min (ephemeral_elapsed, (10*1000)) * DECOMMIT_SIZE_PER_MILLISECOND;
-    decommit_size = min (decommit_size, max_decommit_size);
-    slack_space = heap_segment_committed (ephemeral_heap_segment) - heap_segment_allocated (ephemeral_heap_segment) - decommit_size;
-    decommit_heap_segment_pages (ephemeral_heap_segment, slack_space);
-#endif // !MULTIPLE_HEAPS
-    gc_history_per_heap* current_gc_data_per_heap = get_gc_data_per_heap();
-    current_gc_data_per_heap->extra_gen0_committed = heap_segment_committed (ephemeral_heap_segment) - heap_segment_allocated (ephemeral_heap_segment);
-#endif //MULTIPLE_HEAPS && USE_REGIONS
-}
-bool gc_heap::decommit_step (uint64_t step_milliseconds)
-{
-    if (settings.pause_mode == pause_no_gc)
-    {
-        return false;
-    }
-    size_t decommit_size = 0;
-#ifdef USE_REGIONS
-    const size_t max_decommit_step_size = DECOMMIT_SIZE_PER_MILLISECOND * step_milliseconds;
-    for (int kind = basic_free_region; kind < count_free_region_kinds; kind++)
-    {
-        dprintf (REGIONS_LOG, ("decommit_step %d, regions_to_decommit = %zd",
-            kind, global_regions_to_decommit[kind].get_num_free_regions()));
-        while (global_regions_to_decommit[kind].get_num_free_regions() > 0)
-        {
-            heap_segment* region = global_regions_to_decommit[kind].unlink_region_front();
-            size_t size = decommit_region (region, recorded_committed_free_bucket, -1);
-            decommit_size += size;
-            if (decommit_size >= max_decommit_step_size)
-            {
-                return true;
-            }
-        }
-    }
-    if (use_large_pages_p)
-    {
-        return (decommit_size != 0);
-    }
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-    assert(!use_large_pages_p);
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-        decommit_size += hp->decommit_ephemeral_segment_pages_step ();
-    }
-#endif //MULTIPLE_HEAPS
-    return (decommit_size != 0);
-}
-#ifdef USE_REGIONS
-size_t gc_heap::decommit_region (heap_segment* region, int bucket, int h_number)
-{
-    uint8_t* page_start = align_lower_page (get_region_start (region));
-    uint8_t* decommit_end = heap_segment_committed (region);
-    size_t decommit_size = decommit_end - page_start;
-    bool decommit_succeeded_p = virtual_decommit (page_start, decommit_size, bucket, h_number);
-    bool require_clearing_memory_p = !decommit_succeeded_p || use_large_pages_p;
-    dprintf (REGIONS_LOG, ("decommitted region %p(%p-%p) (%zu bytes) - success: %d",
-        region,
-        page_start,
-        decommit_end,
-        decommit_size,
-        decommit_succeeded_p));
-    if (require_clearing_memory_p)
-    {
-        uint8_t* clear_end = use_large_pages_p ? heap_segment_used (region) : heap_segment_committed (region);
-        size_t clear_size = clear_end - page_start;
-        memclr (page_start, clear_size);
-        heap_segment_used (region) = heap_segment_mem (region);
-        dprintf(REGIONS_LOG, ("cleared region %p(%p-%p) (%zu bytes)",
-            region,
-            page_start,
-            clear_end,
-            clear_size));
-    }
-    else
-    {
-        heap_segment_committed (region) = heap_segment_mem (region);
-    }
-    if ((region->flags & heap_segment_flags_ma_committed) != 0)
-    {
-#ifdef MULTIPLE_HEAPS
-        gc_heap* hp = g_heaps [0];
-#else
-        gc_heap* hp = pGenGCHeap;
-#endif
-        hp->decommit_mark_array_by_seg (region);
-        region->flags &= ~(heap_segment_flags_ma_committed);
-    }
-    if (use_large_pages_p)
-    {
-        assert (heap_segment_used (region) == heap_segment_mem (region));
-    }
-    else
-    {
-        assert (heap_segment_committed (region) == heap_segment_mem (region));
-    }
-    assert ((region->flags & heap_segment_flags_ma_committed) == 0);
-    global_region_allocator.delete_region (get_region_start (region));
-    return decommit_size;
-}
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-size_t gc_heap::decommit_ephemeral_segment_pages_step ()
-{
-    size_t size = 0;
-#ifdef USE_REGIONS
-    for (int gen_number = soh_gen0; gen_number <= soh_gen1; gen_number++)
-    {
-        generation* gen = generation_of (gen_number);
-        heap_segment* seg = generation_tail_region (gen);
-#else // USE_REGIONS
-    {
-        heap_segment* seg = ephemeral_heap_segment;
-        assert (seg->saved_desired_allocation == dd_desired_allocation (dynamic_data_of (0)));
-#endif // USE_REGIONS
-        uint8_t* decommit_target = heap_segment_decommit_target (seg);
-        size_t EXTRA_SPACE = 2 * OS_PAGE_SIZE;
-        decommit_target += EXTRA_SPACE;
-#ifdef STRESS_DECOMMIT
-        decommit_target = heap_segment_mem (seg) + gc_rand::get_rand (heap_segment_reserved (seg) - heap_segment_mem (seg));
-#endif //STRESS_DECOMMIT
-        uint8_t* committed = heap_segment_committed (seg);
-        uint8_t* allocated = (seg == ephemeral_heap_segment) ? alloc_allocated : heap_segment_allocated (seg);
-        if ((allocated <= decommit_target) && (decommit_target < committed))
-        {
-#ifdef USE_REGIONS
-            if (gen_number == soh_gen0)
-            {
-                if (!try_enter_spin_lock (&more_space_lock_soh))
-                {
-                    continue;
-                }
-                add_saved_spinlock_info (false, me_acquire, mt_decommit_step, msl_entered);
-                seg = generation_tail_region (gen);
-#ifndef STRESS_DECOMMIT
-                decommit_target = heap_segment_decommit_target (seg);
-                decommit_target += EXTRA_SPACE;
-#endif
-                committed = heap_segment_committed (seg);
-                allocated = (seg == ephemeral_heap_segment) ? alloc_allocated : heap_segment_allocated (seg);
-            }
-            if ((allocated <= decommit_target) && (decommit_target < committed))
-#else // USE_REGIONS
-            assert (seg->saved_committed == heap_segment_committed (seg));
-#endif // USE_REGIONS
-            {
-                size_t full_decommit_size = (committed - decommit_target);
-                size_t decommit_size = min (max_decommit_step_size, full_decommit_size);
-                uint8_t* new_committed = (committed - decommit_size);
-                size += decommit_heap_segment_pages_worker (seg, new_committed);
-#ifdef _DEBUG
-                seg->saved_committed = committed - size;
-#endif // _DEBUG
-            }
-#ifdef USE_REGIONS
-            if (gen_number == soh_gen0)
-            {
-                add_saved_spinlock_info (false, me_release, mt_decommit_step, msl_entered);
-                leave_spin_lock (&more_space_lock_soh);
-            }
-#endif // USE_REGIONS
-        }
-    }
-    return size;
-}
-#endif //MULTIPLE_HEAPS
-size_t gc_heap::generation_fragmentation (generation* gen,
-                                          generation* consing_gen,
-                                          uint8_t* end)
-{
-    ptrdiff_t frag = 0;
-#ifdef USE_REGIONS
-    for (int gen_num = 0; gen_num <= gen->gen_num; gen_num++)
-    {
-        generation* gen = generation_of (gen_num);
-        heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-        while (seg)
-        {
-            frag += (heap_segment_saved_allocated (seg) -
-                 heap_segment_plan_allocated (seg));
-            dprintf (3, ("h%d g%d adding seg plan frag: %p-%p=%zd -> %zd",
-                heap_number, gen_num,
-                heap_segment_saved_allocated (seg),
-                heap_segment_plan_allocated (seg),
-                (heap_segment_saved_allocated (seg) - heap_segment_plan_allocated (seg)),
-                frag));
-            seg = heap_segment_next_rw (seg);
-        }
-    }
-#else //USE_REGIONS
-    uint8_t* alloc = generation_allocation_pointer (consing_gen);
-    if (in_range_for_segment (alloc, ephemeral_heap_segment))
-    {
-        if (alloc <= heap_segment_allocated(ephemeral_heap_segment))
-            frag = end - alloc;
-        else
-        {
-            frag = 0;
-        }
-        dprintf (3, ("ephemeral frag: %zd", frag));
-    }
-    else
-        frag = (heap_segment_allocated (ephemeral_heap_segment) -
-                heap_segment_mem (ephemeral_heap_segment));
-    heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-    PREFIX_ASSUME(seg != NULL);
-    while (seg != ephemeral_heap_segment)
-    {
-        frag += (heap_segment_allocated (seg) -
-                 heap_segment_plan_allocated (seg));
-        dprintf (3, ("seg: %zx, frag: %zd", (size_t)seg,
-                     (heap_segment_allocated (seg) -
-                      heap_segment_plan_allocated (seg))));
-        seg = heap_segment_next_rw (seg);
-        assert (seg);
-    }
-#endif //USE_REGIONS
-    dprintf (3, ("frag: %zd discounting pinned plugs", frag));
-    size_t bos = 0;
-    while (bos < mark_stack_bos)
-    {
-        frag += (pinned_len (pinned_plug_of (bos)));
-        dprintf (3, ("adding pinned len %zd to frag ->%zd",
-            pinned_len (pinned_plug_of (bos)), frag));
-        bos++;
-    }
-    return frag;
-}
-size_t gc_heap::generation_sizes (generation* gen, bool use_saved_p)
-{
-    size_t result = 0;
-#ifdef USE_REGIONS
-    int gen_num = gen->gen_num;
-    int start_gen_index = ((gen_num > max_generation) ? gen_num : 0);
-    for (int i = start_gen_index; i <= gen_num; i++)
-    {
-        heap_segment* seg = heap_segment_in_range (generation_start_segment (generation_of (i)));
-        while (seg)
-        {
-            uint8_t* end = (use_saved_p ?
-                heap_segment_saved_allocated (seg) : heap_segment_allocated (seg));
-            result += end - heap_segment_mem (seg);
-            dprintf (3, ("h%d gen%d size + %zd (%p - %p) -> %zd",
-                heap_number, i, (end - heap_segment_mem (seg)),
-                heap_segment_mem (seg), end, result));
-            seg = heap_segment_next (seg);
-        }
-    }
-#else //USE_REGIONS
-    if (generation_start_segment (gen ) == ephemeral_heap_segment)
-        result = (heap_segment_allocated (ephemeral_heap_segment) -
-                  generation_allocation_start (gen));
-    else
-    {
-        heap_segment* seg = heap_segment_in_range (generation_start_segment (gen));
-        PREFIX_ASSUME(seg != NULL);
-        while (seg)
-        {
-            result += (heap_segment_allocated (seg) -
-                       heap_segment_mem (seg));
-            seg = heap_segment_next_in_range (seg);
-        }
-    }
-#endif //USE_REGIONS
-    return result;
-}
-#ifdef USE_REGIONS
-bool gc_heap::decide_on_compaction_space()
-{
-    size_t gen0size = approximate_new_allocation();
-    dprintf (REGIONS_LOG, ("gen0size: %zd, free: %zd",
-        gen0size, (num_regions_freed_in_sweep * ((size_t)1 << min_segment_size_shr))));
-    if (sufficient_space_regions ((num_regions_freed_in_sweep * ((size_t)1 << min_segment_size_shr)),
-                                  gen0size))
-    {
-        dprintf (REGIONS_LOG, ("it is sufficient!"));
-        return false;
-    }
-    get_gen0_end_plan_space();
-    if (!gen0_large_chunk_found)
-    {
-        gen0_large_chunk_found = (free_regions[basic_free_region].get_num_free_regions() > 0);
-    }
-    dprintf (REGIONS_LOG, ("gen0_pinned_free_space: %zd, end_gen0_region_space: %zd, gen0size: %zd",
-            gen0_pinned_free_space, end_gen0_region_space, gen0size));
-    if (sufficient_space_regions ((gen0_pinned_free_space + end_gen0_region_space), gen0size) &&
-        gen0_large_chunk_found)
-    {
-        sufficient_gen0_space_p = TRUE;
-    }
-    return true;
-}
-#endif //USE_REGIONS
-size_t gc_heap::estimated_reclaim (int gen_number)
-{
-    dynamic_data* dd = dynamic_data_of (gen_number);
-    size_t gen_allocated = (dd_desired_allocation (dd) - dd_new_allocation (dd));
-    size_t gen_total_size = gen_allocated + dd_current_size (dd);
-    size_t est_gen_surv = (size_t)((float) (gen_total_size) * dd_surv (dd));
-    size_t est_gen_free = gen_total_size - est_gen_surv + dd_fragmentation (dd);
-    dprintf (GTC_LOG, ("h%d gen%d total size: %zd, est dead space: %zd (s: %d, allocated: %zd), frag: %zd",
-                heap_number, gen_number,
-                gen_total_size,
-                est_gen_free,
-                (int)(dd_surv (dd) * 100),
-                gen_allocated,
-                dd_fragmentation (dd)));
-    return est_gen_free;
-}
-bool gc_heap::is_full_compacting_gc_productive()
-{
-#ifdef USE_REGIONS
-    heap_segment* gen1_start_region = generation_start_segment (generation_of (max_generation - 1));
-    if (heap_segment_plan_gen_num (gen1_start_region) == max_generation)
-    {
-        dprintf (REGIONS_LOG, ("gen1 start region %p is now part of gen2, unproductive",
-            heap_segment_mem (gen1_start_region)));
-        return false;
-    }
-    else
-    {
-        heap_segment* gen2_tail_region = generation_tail_region (generation_of (max_generation));
-        if (heap_segment_plan_allocated (gen2_tail_region) >= heap_segment_allocated (gen2_tail_region))
-        {
-            dprintf (REGIONS_LOG, ("last gen2 region extended %p->%p, unproductive",
-                heap_segment_allocated (gen2_tail_region), heap_segment_plan_allocated (gen2_tail_region)));
-            return false;
-        }
-    }
-    return true;
-#else //USE_REGIONS
-    if (generation_plan_allocation_start (generation_of (max_generation - 1)) >=
-        generation_allocation_start (generation_of (max_generation - 1)))
-    {
-        dprintf (1, ("gen1 start %p->%p, gen2 size %zd->%zd, lock elevation",
-                generation_allocation_start (generation_of (max_generation - 1)),
-                generation_plan_allocation_start (generation_of (max_generation - 1)),
-                    generation_size (max_generation),
-                    generation_plan_size (max_generation)));
-        return false;
-    }
-    else
-        return true;
-#endif //USE_REGIONS
-}
-BOOL gc_heap::decide_on_compacting (int condemned_gen_number,
-                                    size_t fragmentation,
-                                    BOOL& should_expand)
-{
-    BOOL should_compact = FALSE;
-    should_expand = FALSE;
-    generation*   gen = generation_of (condemned_gen_number);
-    dynamic_data* dd = dynamic_data_of (condemned_gen_number);
-    size_t gen_sizes     = generation_sizes(gen, true);
-    float  fragmentation_burden = ( ((0 == fragmentation) || (0 == gen_sizes)) ? (0.0f) :
-                                    (float (fragmentation) / gen_sizes) );
-    dprintf (GTC_LOG, ("h%d g%d fragmentation: %zd (%d%%), gen_sizes: %zd",
-        heap_number, settings.condemned_generation,
-        fragmentation, (int)(fragmentation_burden * 100.0),
-        gen_sizes));
-#ifdef USE_REGIONS
-    if (special_sweep_p)
-    {
-        return FALSE;
-    }
-#endif //USE_REGIONS
-#if defined(STRESS_HEAP) && !defined(FEATURE_NATIVEAOT)
-    if (GCStress<cfg_any>::IsEnabled() && !settings.concurrent)
-        should_compact = TRUE;
-#endif //defined(STRESS_HEAP) && !defined(FEATURE_NATIVEAOT)
-    if (GCConfig::GetForceCompact())
-        should_compact = TRUE;
-    if ((condemned_gen_number == max_generation) && last_gc_before_oom)
-    {
-        should_compact = TRUE;
-#ifndef USE_REGIONS
-        last_gc_before_oom = FALSE;
-#endif //!USE_REGIONS
-        get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_last_gc);
-    }
-    if (settings.reason == reason_induced_compacting)
-    {
-        dprintf (2, ("induced compacting GC"));
-        should_compact = TRUE;
-        get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_induced_compacting);
-    }
-    if (settings.reason == reason_induced_aggressive)
-    {
-        dprintf (2, ("aggressive compacting GC"));
-        should_compact = TRUE;
-        get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_aggressive_compacting);
-    }
-    if (settings.reason == reason_pm_full_gc)
-    {
-        assert (condemned_gen_number == max_generation);
-        if (heap_number == 0)
-        {
-            dprintf (GTC_LOG, ("PM doing compacting full GC after a gen1"));
-        }
-        should_compact = TRUE;
-    }
-    dprintf (2, ("Fragmentation: %zu Fragmentation burden %d%%",
-                fragmentation, (int) (100*fragmentation_burden)));
-    if (provisional_mode_triggered && (condemned_gen_number == (max_generation - 1)))
-    {
-        dprintf (GTC_LOG, ("gen1 in PM always compact"));
-        should_compact = TRUE;
-    }
-#ifdef USE_REGIONS
-    if (!should_compact)
-    {
-        should_compact = !!decide_on_compaction_space();
-    }
-#else //USE_REGIONS
-    if (!should_compact)
-    {
-        if (dt_low_ephemeral_space_p (tuning_deciding_compaction))
-        {
-            dprintf(GTC_LOG, ("compacting due to low ephemeral"));
-            should_compact = TRUE;
-            get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_low_ephemeral);
-        }
-    }
-    if (should_compact)
-    {
-        if ((condemned_gen_number >= (max_generation - 1)))
-        {
-            if (dt_low_ephemeral_space_p (tuning_deciding_expansion))
-            {
-                dprintf (GTC_LOG,("Not enough space for all ephemeral generations with compaction"));
-                should_expand = TRUE;
-            }
-        }
-    }
-#endif //USE_REGIONS
-#ifdef HOST_64BIT
-    BOOL high_memory = FALSE;
-#endif // HOST_64BIT
-    if (!should_compact)
-    {
-        dprintf (REGIONS_LOG, ("frag: %zd, fragmentation_burden: %.3f",
-            fragmentation, fragmentation_burden));
-        BOOL frag_exceeded = ((fragmentation >= dd_fragmentation_limit (dd)) &&
-                                (fragmentation_burden >= dd_fragmentation_burden_limit (dd)));
-        if (frag_exceeded)
-        {
-#ifdef BACKGROUND_GC
-            IN_STRESS_HEAP(if (!settings.stress_induced))
-            {
-#endif // BACKGROUND_GC
-            assert (settings.concurrent == FALSE);
-            should_compact = TRUE;
-            get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_high_frag);
-#ifdef BACKGROUND_GC
-            }
-#endif // BACKGROUND_GC
-        }
-#ifdef HOST_64BIT
-        if(!should_compact)
-        {
-            uint32_t num_heaps = 1;
-#ifdef MULTIPLE_HEAPS
-            num_heaps = gc_heap::n_heaps;
-#endif // MULTIPLE_HEAPS
-            ptrdiff_t reclaim_space = generation_size(max_generation) - generation_plan_size(max_generation);
-            if((settings.entry_memory_load >= high_memory_load_th) && (settings.entry_memory_load < v_high_memory_load_th))
-            {
-                if(reclaim_space > (int64_t)(min_high_fragmentation_threshold (entry_available_physical_mem, num_heaps)))
-                {
-                    dprintf(GTC_LOG,("compacting due to fragmentation in high memory"));
-                    should_compact = TRUE;
-                    get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_high_mem_frag);
-                }
-                high_memory = TRUE;
-            }
-            else if(settings.entry_memory_load >= v_high_memory_load_th)
-            {
-                if(reclaim_space > (ptrdiff_t)(min_reclaim_fragmentation_threshold (num_heaps)))
-                {
-                    dprintf(GTC_LOG,("compacting due to fragmentation in very high memory"));
-                    should_compact = TRUE;
-                    get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_vhigh_mem_frag);
-                }
-                high_memory = TRUE;
-            }
-        }
-#endif // HOST_64BIT
-    }
-    if ((should_compact == FALSE) &&
-        (ensure_gap_allocation (condemned_gen_number) == FALSE))
-    {
-        should_compact = TRUE;
-        get_gc_data_per_heap()->set_mechanism (gc_heap_compact, compact_no_gaps);
-    }
-    if (settings.condemned_generation == max_generation)
-    {
-        if (
-#ifdef HOST_64BIT
-            (high_memory && !should_compact) ||
-#endif // HOST_64BIT
-            !is_full_compacting_gc_productive())
-        {
-            settings.should_lock_elevation = TRUE;
-        }
-    }
-    if (settings.pause_mode == pause_no_gc)
-    {
-        should_compact = TRUE;
-        if ((size_t)(heap_segment_reserved (ephemeral_heap_segment) - heap_segment_plan_allocated (ephemeral_heap_segment))
-            < soh_allocation_no_gc)
-        {
-            should_expand = TRUE;
-        }
-    }
-    dprintf (2, ("will %s(%s)", (should_compact ? "compact" : "sweep"), (should_expand ? "ex" : "")));
-    return should_compact;
-}
-size_t align_lower_good_size_allocation (size_t size)
-{
-    return (size/64)*64;
-}
-size_t gc_heap::approximate_new_allocation()
-{
-    dynamic_data* dd0 = dynamic_data_of (0);
-    return max (2*dd_min_size (dd0), ((dd_desired_allocation (dd0)*2)/3));
-}
-bool gc_heap::check_against_hard_limit (size_t space_required)
-{
-    bool can_fit = TRUE;
-    if (heap_hard_limit)
-    {
-        size_t left_in_commit = heap_hard_limit - current_total_committed;
-        int num_heaps = get_num_heaps();
-        left_in_commit /= num_heaps;
-        if (left_in_commit < space_required)
-        {
-            can_fit = FALSE;
-        }
-        dprintf (2, ("h%d end seg %zd, but only %zd left in HARD LIMIT commit, required: %zd %s on eph",
-            heap_number, space_required,
-            left_in_commit, space_required,
-            (can_fit ? "ok" : "short")));
-    }
-    return can_fit;
-}
-#ifdef USE_REGIONS
-bool gc_heap::sufficient_space_regions_for_allocation (size_t end_space, size_t end_space_required)
-{
-    size_t free_regions_space = (free_regions[basic_free_region].get_num_free_regions() * ((size_t)1 << min_segment_size_shr)) +
-                                global_region_allocator.get_free();
-    size_t total_alloc_space = end_space + free_regions_space;
-    dprintf (REGIONS_LOG, ("h%d required %zd, end %zd + free %zd=%zd",
-        heap_number, end_space_required, end_space, free_regions_space, total_alloc_space));
-    size_t total_commit_space = end_gen0_region_committed_space + free_regions[basic_free_region].get_size_committed_in_free();
-    if (total_alloc_space > end_space_required)
-    {
-        if (end_space_required > total_commit_space)
-        {
-            return check_against_hard_limit (end_space_required - total_commit_space);
-        }
-        else
-        {
-            return true;
-        }
-    }
-    else
-    {
-        return false;
-    }
-}
-bool gc_heap::sufficient_space_regions (size_t end_space, size_t end_space_required)
-{
-    size_t free_regions_space = (free_regions[basic_free_region].get_num_free_regions() * ((size_t)1 << min_segment_size_shr)) +
-                                global_region_allocator.get_free();
-    size_t total_alloc_space = end_space + free_regions_space;
-    dprintf (REGIONS_LOG, ("h%d required %zd, end %zd + free %zd=%zd",
-        heap_number, end_space_required, end_space, free_regions_space, total_alloc_space));
-    if (total_alloc_space > end_space_required)
-    {
-        return check_against_hard_limit (end_space_required);
-    }
-    else
-    {
-        return false;
-    }
-}
-#else //USE_REGIONS
-BOOL gc_heap::sufficient_space_end_seg (uint8_t* start, uint8_t* committed, uint8_t* reserved, size_t end_space_required)
-{
-    BOOL can_fit = FALSE;
-    size_t committed_space = (size_t)(committed - start);
-    size_t end_seg_space = (size_t)(reserved - start);
-    if (committed_space > end_space_required)
-    {
-        return true;
-    }
-    else if (end_seg_space > end_space_required)
-    {
-        return check_against_hard_limit (end_space_required - committed_space);
-    }
-    else
-        return false;
-}
-#endif //USE_REGIONS
-size_t gc_heap::end_space_after_gc()
-{
-    return max ((dd_min_size (dynamic_data_of (0))/2), (END_SPACE_AFTER_GC_FL));
-}
-BOOL gc_heap::ephemeral_gen_fit_p (gc_tuning_point tp)
-{
-    uint8_t* start = 0;
-#ifdef USE_REGIONS
-    assert ((tp == tuning_deciding_condemned_gen) || (tp == tuning_deciding_full_gc));
-#else//USE_REGIONS
-    if ((tp == tuning_deciding_condemned_gen) ||
-        (tp == tuning_deciding_compaction))
-    {
-        start = (settings.concurrent ? alloc_allocated : heap_segment_allocated (ephemeral_heap_segment));
-        if (settings.concurrent)
-        {
-            dprintf (2, ("%zd left at the end of ephemeral segment (alloc_allocated)",
-                (size_t)(heap_segment_reserved (ephemeral_heap_segment) - alloc_allocated)));
-        }
-        else
-        {
-            dprintf (2, ("%zd left at the end of ephemeral segment (allocated)",
-                (size_t)(heap_segment_reserved (ephemeral_heap_segment) - heap_segment_allocated (ephemeral_heap_segment))));
-        }
-    }
-    else if (tp == tuning_deciding_expansion)
-    {
-        start = heap_segment_plan_allocated (ephemeral_heap_segment);
-        dprintf (2, ("%zd left at the end of ephemeral segment based on plan",
-            (size_t)(heap_segment_reserved (ephemeral_heap_segment) - start)));
-    }
-    else
-    {
-        assert (tp == tuning_deciding_full_gc);
-        dprintf (2, ("FGC: %zd left at the end of ephemeral segment (alloc_allocated)",
-            (size_t)(heap_segment_reserved (ephemeral_heap_segment) - alloc_allocated)));
-        start = alloc_allocated;
-    }
-    if (start == 0) // empty ephemeral generations
-    {
-        assert (tp == tuning_deciding_expansion);
-        start = generation_allocation_pointer (generation_of (max_generation));
-        assert (start == heap_segment_mem (ephemeral_heap_segment));
-    }
-    if (tp == tuning_deciding_expansion)
-    {
-        assert (settings.condemned_generation >= (max_generation-1));
-        size_t gen0size = approximate_new_allocation();
-        size_t eph_size = gen0size;
-        size_t gen_min_sizes = 0;
-        for (int j = 1; j <= max_generation-1; j++)
-        {
-            gen_min_sizes += 2*dd_min_size (dynamic_data_of(j));
-        }
-        eph_size += gen_min_sizes;
-        dprintf (3, ("h%d deciding on expansion, need %zd (gen0: %zd, 2*min: %zd)",
-            heap_number, gen0size, gen_min_sizes, eph_size));
-        if ((size_t)(heap_segment_reserved (ephemeral_heap_segment) - start) > eph_size)
-        {
-            dprintf (3, ("Enough room before end of segment"));
-            return TRUE;
-        }
-        else
-        {
-            size_t room = align_lower_good_size_allocation
-                (heap_segment_reserved (ephemeral_heap_segment) - start);
-            size_t end_seg = room;
-            size_t largest_alloc = END_SPACE_AFTER_GC_FL;
-            bool large_chunk_found = FALSE;
-            size_t bos = 0;
-            uint8_t* gen0start = generation_plan_allocation_start (youngest_generation);
-            dprintf (3, ("ephemeral_gen_fit_p: gen0 plan start: %zx", (size_t)gen0start));
-            if (gen0start == 0)
-                return FALSE;
-            dprintf (3, ("ephemeral_gen_fit_p: room before free list search %zd, needed: %zd",
-                         room, gen0size));
-            while ((bos < mark_stack_bos) &&
-                   !((room >= gen0size) && large_chunk_found))
-            {
-                uint8_t* plug = pinned_plug (pinned_plug_of (bos));
-                if (in_range_for_segment (plug, ephemeral_heap_segment))
-                {
-                    if (plug >= gen0start)
-                    {
-                        size_t chunk = align_lower_good_size_allocation (pinned_len (pinned_plug_of (bos)));
-                        room += chunk;
-                        if (!large_chunk_found)
-                        {
-                            large_chunk_found = (chunk >= largest_alloc);
-                        }
-                        dprintf (3, ("ephemeral_gen_fit_p: room now %zd, large chunk: %d",
-                                     room, large_chunk_found));
-                    }
-                }
-                bos++;
-            }
-            if (room >= gen0size)
-            {
-                if (large_chunk_found)
-                {
-                    sufficient_gen0_space_p = TRUE;
-                    dprintf (3, ("Enough room"));
-                    return TRUE;
-                }
-                else
-                {
-                    if (end_seg >= end_space_after_gc())
-                    {
-                        dprintf (3, ("Enough room (may need end of seg)"));
-                        return TRUE;
-                    }
-                }
-            }
-            dprintf (3, ("Not enough room"));
-                return FALSE;
-        }
-    }
-    else
-#endif //USE_REGIONS
-    {
-        size_t end_space = 0;
-        dynamic_data* dd = dynamic_data_of (0);
-        if ((tp == tuning_deciding_condemned_gen) ||
-            (tp == tuning_deciding_full_gc))
-        {
-            end_space = max (2*dd_min_size (dd), end_space_after_gc());
-        }
-        else
-        {
-            assert (tp == tuning_deciding_compaction);
-            end_space = approximate_new_allocation();
-        }
-#ifdef USE_REGIONS
-        size_t gen0_end_space = get_gen0_end_space (memory_type_reserved);
-        BOOL can_fit = sufficient_space_regions (gen0_end_space, end_space);
-#else //USE_REGIONS
-        BOOL can_fit = sufficient_space_end_seg (start, heap_segment_committed (ephemeral_heap_segment), heap_segment_reserved (ephemeral_heap_segment), end_space);
-#endif //USE_REGIONS
-        return can_fit;
-    }
-}
-CObjectHeader* gc_heap::allocate_uoh_object (size_t jsize, uint32_t flags, int gen_number, int64_t& alloc_bytes)
-{
-    alloc_context acontext;
-    acontext.init();
-#if HOST_64BIT
-    size_t maxObjectSize = (INT64_MAX - 7 - Align(min_obj_size));
-#else
-    size_t maxObjectSize = (INT32_MAX - 7 - Align(min_obj_size));
-#endif
-    if (jsize >= maxObjectSize)
-    {
-        if (GCConfig::GetBreakOnOOM())
-        {
-            GCToOSInterface::DebugBreak();
-        }
-        return NULL;
-    }
-    size_t size = AlignQword (jsize);
-    int align_const = get_alignment_constant (FALSE);
-    size_t pad = 0;
-#ifdef FEATURE_LOH_COMPACTION
-    if (gen_number == loh_generation)
-    {
-        pad = Align (loh_padding_obj_size, align_const);
-    }
-#endif //FEATURE_LOH_COMPACTION
-    assert (size >= Align (min_obj_size, align_const));
-#ifdef _MSC_VER
-#pragma inline_depth(0)
-#endif //_MSC_VER
-    if (! allocate_more_space (&acontext, (size + pad), flags, gen_number))
-    {
-        return 0;
-    }
-#ifdef _MSC_VER
-#pragma inline_depth(20)
-#endif //_MSC_VER
-#ifdef FEATURE_LOH_COMPACTION
-#endif //FEATURE_LOH_COMPACTION
-    uint8_t*  result = acontext.alloc_ptr;
-    assert ((size_t)(acontext.alloc_limit - acontext.alloc_ptr) == size);
-    alloc_bytes += size;
-    CObjectHeader* obj = (CObjectHeader*)result;
-    assert (obj != 0);
-    assert ((size_t)obj == Align ((size_t)obj, align_const));
-    return obj;
-}
-void gc_heap::reset_memory (uint8_t* o, size_t sizeo)
-{
-    if (gc_heap::use_large_pages_p)
-        return;
-    if (sizeo > 128 * 1024)
-    {
-        size_t size_to_skip = min_free_list - plug_skew;
-        size_t page_start = align_on_page ((size_t)(o + size_to_skip));
-        size_t size = align_lower_page ((size_t)o + sizeo - size_to_skip - plug_skew) - page_start;
-        if (reset_mm_p && gc_heap::dt_high_memory_load_p())
-        {
-#ifdef MULTIPLE_HEAPS
-            bool unlock_p = true;
-#else
-            bool unlock_p = false;
-#endif //MULTIPLE_HEAPS
-            reset_mm_p = GCToOSInterface::VirtualReset((void*)page_start, size, unlock_p);
-        }
-    }
-}
-BOOL gc_heap::uoh_object_marked (uint8_t* o, BOOL clearp)
-{
-    BOOL m = FALSE;
-    if ((o >= lowest_address) && (o < highest_address))
-    {
-        if (marked (o))
-        {
-            if (clearp)
-            {
-                clear_marked (o);
-                if (pinned (o))
-                    clear_pinned(o);
-            }
-            m = TRUE;
-        }
-        else
-            m = FALSE;
-    }
-    else
-        m = TRUE;
-    return m;
-}
-void gc_heap::walk_survivors_relocation (void* profiling_context, record_surv_fn fn)
-{
-    walk_relocation (profiling_context, fn);
-#ifdef FEATURE_LOH_COMPACTION
-    if (loh_compacted_p)
-    {
-        walk_relocation_for_loh (profiling_context, fn);
-    }
-#endif //FEATURE_LOH_COMPACTION
-}
-void gc_heap::walk_survivors_for_uoh (void* profiling_context, record_surv_fn fn, int gen_number)
-{
-    generation* gen        = generation_of (gen_number);
-    heap_segment* seg      = heap_segment_rw (generation_start_segment (gen));;
-    PREFIX_ASSUME(seg != NULL);
-    uint8_t* o                = get_uoh_start_object (seg, gen);
-    uint8_t* plug_end         = o;
-    uint8_t* plug_start       = o;
-    while (1)
-    {
-        if (o >= heap_segment_allocated (seg))
-        {
-            seg = heap_segment_next (seg);
-            if (seg == 0)
-                break;
-            else
-                o = heap_segment_mem (seg);
-        }
-        if (uoh_object_marked(o, FALSE))
-        {
-            plug_start = o;
-            BOOL m = TRUE;
-            while (m)
-            {
-                o = o + AlignQword (size (o));
-                if (o >= heap_segment_allocated (seg))
-                {
-                    break;
-                }
-                m = uoh_object_marked (o, FALSE);
-            }
-            plug_end = o;
-            fn (plug_start, plug_end, 0, profiling_context, false, false);
-        }
-        else
-        {
-            while (o < heap_segment_allocated (seg) && !uoh_object_marked(o, FALSE))
-            {
-                o = o + AlignQword (size (o));
-            }
-        }
-    }
-}
-#ifdef BACKGROUND_GC
-BOOL gc_heap::background_object_marked (uint8_t* o, BOOL clearp)
-{
-    BOOL m = FALSE;
-    if ((o >= background_saved_lowest_address) && (o < background_saved_highest_address))
-    {
-        if (mark_array_marked (o))
-        {
-            if (clearp)
-            {
-                mark_array_clear_marked (o);
-                dprintf (3, ("CM: %p", o));
-            }
-            m = TRUE;
-        }
-        else
-            m = FALSE;
-    }
-    else
-        m = TRUE;
-    dprintf (3, ("o %p(%zu) %s", o, size(o), (m ? "was bm" : "was NOT bm")));
-    return m;
-}
-void gc_heap::background_delay_delete_uoh_segments()
-{
-    for (int i = uoh_start_generation; i < total_generation_count; i++)
-    {
-        generation* gen = generation_of (i);
-        heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-        heap_segment* prev_seg = 0;
-#ifdef USE_REGIONS
-        heap_segment* first_remaining_region = 0;
-#endif //USE_REGIONS
-        while (seg)
-        {
-            heap_segment* next_seg = heap_segment_next (seg);
-            if (seg->flags & heap_segment_flags_uoh_delete)
-            {
-                dprintf (3, ("deleting %zx-%p-%p", (size_t)seg, heap_segment_allocated (seg), heap_segment_reserved (seg)));
-                delete_heap_segment (seg, (GCConfig::GetRetainVM() != 0));
-                heap_segment_next (prev_seg) = next_seg;
-#ifdef USE_REGIONS
-                update_start_tail_regions (gen, seg, prev_seg, next_seg);
-#endif //USE_REGIONS
-            }
-            else
-            {
-#ifdef USE_REGIONS
-                if (!first_remaining_region)
-                    first_remaining_region = seg;
-#endif //USE_REGIONS
-                prev_seg = seg;
-            }
-            seg = next_seg;
-        }
-#ifdef USE_REGIONS
-        assert (heap_segment_rw (generation_start_segment (gen)) == generation_start_segment (gen));
-        if (generation_start_segment (gen) != first_remaining_region)
-        {
-            dprintf (REGIONS_LOG, ("h%d gen%d start %p -> %p",
-                heap_number, gen->gen_num,
-                heap_segment_mem (generation_start_segment (gen)),
-                heap_segment_mem (first_remaining_region)));
-            generation_start_segment (gen) = first_remaining_region;
-        }
-        if (generation_tail_region (gen) != prev_seg)
-        {
-            dprintf (REGIONS_LOG, ("h%d gen%d start %p -> %p",
-                heap_number, gen->gen_num,
-                heap_segment_mem (generation_tail_region (gen)),
-                heap_segment_mem (prev_seg)));
-            generation_tail_region (gen) = prev_seg;
-        }
-#endif //USE_REGIONS
-    }
-}
-uint8_t* gc_heap::background_next_end (heap_segment* seg, BOOL uoh_objects_p)
-{
-    return
-        (uoh_objects_p ? heap_segment_allocated (seg) : heap_segment_background_allocated (seg));
-}
-void gc_heap::set_mem_verify (uint8_t* start, uint8_t* end, uint8_t b)
-{
-#ifdef VERIFY_HEAP
-    if (end > start)
-    {
-        if ((GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC) &&
-           !(GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_NO_MEM_FILL))
-        {
-            dprintf (3, ("setting mem to %c [%p, [%p", b, start, end));
-            memset (start, b, (end - start));
-        }
-    }
-#endif //VERIFY_HEAP
-}
-void gc_heap::generation_delete_heap_segment (generation* gen,
-                                              heap_segment* seg,
-                                              heap_segment* prev_seg,
-                                              heap_segment* next_seg)
-{
-    dprintf (3, ("bgc sweep: deleting seg %zx(%p), next %zx(%p), prev %zx(%p)",
-        (size_t)seg, heap_segment_mem (seg),
-        (size_t)next_seg, (next_seg ? heap_segment_mem (next_seg) : 0),
-        (size_t)prev_seg, (prev_seg ? heap_segment_mem (prev_seg) : 0)));
-    if (gen->gen_num > max_generation)
-    {
-        dprintf (3, ("Preparing empty large segment %zx for deletion", (size_t)seg));
-        seg->flags |= heap_segment_flags_uoh_delete;
-        heap_segment_allocated (seg) = heap_segment_mem (seg);
-    }
-    else
-    {
-        assert (seg != ephemeral_heap_segment);
-#ifdef DOUBLY_LINKED_FL
-        heap_segment_next (prev_seg) = next_seg;
-#else //DOUBLY_LINKED_FL
-        heap_segment_next (next_seg) = prev_seg;
-#endif //DOUBLY_LINKED_FL
-        dprintf (3, ("Preparing empty small segment %zx for deletion", (size_t)seg));
-        heap_segment_next (seg) = freeable_soh_segment;
-        freeable_soh_segment = seg;
-#ifdef USE_REGIONS
-#ifdef DOUBLY_LINKED_FL
-        heap_segment* next_region = next_seg;
-        heap_segment* prev_region = prev_seg;
-#else //DOUBLY_LINKED_FL
-        heap_segment* next_region = prev_seg;
-        heap_segment* prev_region = next_seg;
-#endif //DOUBLY_LINKED_FL
-        update_start_tail_regions (gen, seg, prev_region, next_region);
-#endif //USE_REGIONS
-    }
-    decommit_heap_segment (seg);
-    seg->flags |= heap_segment_flags_decommitted;
-    set_mem_verify (heap_segment_allocated (seg) - plug_skew, heap_segment_used (seg), 0xbb);
-}
-void gc_heap::process_background_segment_end (heap_segment* seg,
-                                              generation* gen,
-                                              uint8_t* last_plug_end,
-                                              heap_segment* start_seg,
-                                              BOOL* delete_p,
-                                              size_t free_obj_size_last_gap)
-{
-    *delete_p = FALSE;
-    uint8_t* allocated = heap_segment_allocated (seg);
-    uint8_t* background_allocated = heap_segment_background_allocated (seg);
-    BOOL uoh_p = heap_segment_uoh_p (seg);
-    dprintf (3, ("EoS [%zx, %p[(%p[), last: %p(%zu)",
-                (size_t)heap_segment_mem (seg), background_allocated, allocated, last_plug_end, free_obj_size_last_gap));
-    if (!uoh_p && (allocated != background_allocated))
-    {
-        assert (gen->gen_num <= max_generation);
-        dprintf (3, ("Make a free object before newly promoted objects [%zx, %p[",
-                    (size_t)last_plug_end, background_allocated));
-        size_t last_gap = background_allocated - last_plug_end;
-        if (last_gap > 0)
-        {
-            thread_gap (last_plug_end, last_gap, generation_of (max_generation));
-            add_gen_free (max_generation, last_gap);
-            fix_brick_to_highest (last_plug_end, background_allocated);
-            fix_brick_to_highest (background_allocated, background_allocated);
-        }
-    }
-    else
-    {
-        if (seg == ephemeral_heap_segment)
-        {
-            FATAL_GC_ERROR();
-        }
-#ifndef USE_REGIONS
-        if (allocated == heap_segment_mem (seg))
-        {
-            assert (gen->gen_num > max_generation);
-        }
-#endif //!USE_REGIONS
-        if (last_plug_end == heap_segment_mem (seg))
-        {
-            if (seg != start_seg)
-            {
-                *delete_p = TRUE;
-            }
-            dprintf (3, ("h%d seg %p %s be deleted", heap_number,
-                        heap_segment_mem (seg), (*delete_p ? "should" : "should not")));
-        }
-        if (!*delete_p)
-        {
-            dprintf (3, ("[h%d] seg %zx alloc %p->%zx",
-                heap_number, (size_t)seg,
-                heap_segment_allocated (seg),
-                (size_t)last_plug_end));
-            heap_segment_allocated (seg) = last_plug_end;
-            set_mem_verify (heap_segment_allocated (seg) - plug_skew, heap_segment_used (seg), 0xbb);
-            decommit_heap_segment_pages (seg, 0);
-        }
-    }
-    if (free_obj_size_last_gap)
-    {
-        generation_free_obj_space (gen) -= free_obj_size_last_gap;
-        dprintf (2, ("[h%d] PS: gen2FO-: %zd->%zd",
-            heap_number, free_obj_size_last_gap, generation_free_obj_space (gen)));
-    }
-    dprintf (3, ("verifying seg %p's mark array was completely cleared", seg));
-    bgc_verify_mark_array_cleared (seg);
-}
-inline
-BOOL gc_heap::fgc_should_consider_object (uint8_t* o,
-                                          heap_segment* seg,
-                                          BOOL consider_bgc_mark_p,
-                                          BOOL check_current_sweep_p,
-                                          BOOL check_saved_sweep_p)
-{
-#ifdef USE_REGIONS
-    assert (!check_saved_sweep_p);
-#endif //USE_REGIONS
-    BOOL no_bgc_mark_p = FALSE;
-    if (consider_bgc_mark_p)
-    {
-        if (check_current_sweep_p && (o < current_sweep_pos))
-        {
-            dprintf (3, ("no bgc mark - o: %p < cs: %p", o, current_sweep_pos));
-            no_bgc_mark_p = TRUE;
-        }
-        if (!no_bgc_mark_p)
-        {
-#ifndef USE_REGIONS
-            if(check_saved_sweep_p && (o >= saved_sweep_ephemeral_start))
-            {
-                dprintf (3, ("no bgc mark - o: %p >= ss: %p", o, saved_sweep_ephemeral_start));
-                no_bgc_mark_p = TRUE;
-            }
-#endif //!USE_REGIONS
-            if (!check_saved_sweep_p)
-            {
-                uint8_t* background_allocated = heap_segment_background_allocated (seg);
-#ifndef USE_REGIONS
-                assert (heap_segment_background_allocated (seg) != saved_sweep_ephemeral_start);
-#endif //!USE_REGIONS
-                if (o >= background_allocated)
-                {
-                    dprintf (3, ("no bgc mark - o: %p >= ba: %p", o, background_allocated));
-                    no_bgc_mark_p = TRUE;
-                }
-            }
-        }
-    }
-    else
-    {
-        no_bgc_mark_p = TRUE;
-    }
-    dprintf (3, ("bgc mark %p: %s (bm: %s)", o, (no_bgc_mark_p ? "no" : "yes"), ((no_bgc_mark_p || background_object_marked (o, FALSE)) ? "yes" : "no")));
-    return (no_bgc_mark_p ? TRUE : background_object_marked (o, FALSE));
-}
-void gc_heap::should_check_bgc_mark (heap_segment* seg,
-                                     BOOL* consider_bgc_mark_p,
-                                     BOOL* check_current_sweep_p,
-                                     BOOL* check_saved_sweep_p)
-{
-    *consider_bgc_mark_p = FALSE;
-    *check_current_sweep_p = FALSE;
-    *check_saved_sweep_p = FALSE;
-    if (current_c_gc_state == c_gc_state_planning)
-    {
-        if ((seg->flags & heap_segment_flags_swept) || (current_sweep_pos == heap_segment_reserved (seg)))
-        {
-            dprintf (3, ("seg %p is already swept by bgc", seg));
-        }
-        else if (heap_segment_background_allocated (seg) == 0)
-        {
-            dprintf (3, ("seg %p newly alloc during bgc", seg));
-        }
-        else
-        {
-            *consider_bgc_mark_p = TRUE;
-            dprintf (3, ("seg %p hasn't been swept by bgc", seg));
-#ifndef USE_REGIONS
-            if (seg == saved_sweep_ephemeral_seg)
-            {
-                dprintf (3, ("seg %p is the saved ephemeral seg", seg));
-                *check_saved_sweep_p = TRUE;
-            }
-#endif //!USE_REGIONS
-            if (in_range_for_segment (current_sweep_pos, seg))
-            {
-                dprintf (3, ("current sweep pos is %p and within seg %p",
-                              current_sweep_pos, seg));
-                *check_current_sweep_p = TRUE;
-            }
-        }
-    }
-}
-void gc_heap::background_ephemeral_sweep()
-{
-    dprintf (3, ("bgc ephemeral sweep"));
-    int align_const = get_alignment_constant (TRUE);
-#ifndef USE_REGIONS
-    saved_sweep_ephemeral_seg = ephemeral_heap_segment;
-    saved_sweep_ephemeral_start = generation_allocation_start (generation_of (max_generation - 1));
-#endif //!USE_REGIONS
-    allocator youngest_free_list;
-    size_t youngest_free_list_space = 0;
-    size_t youngest_free_obj_space = 0;
-    youngest_free_list.clear();
-    for (int i = 0; i <= (max_generation - 1); i++)
-    {
-        generation* gen_to_reset = generation_of (i);
-        assert (generation_free_list_space (gen_to_reset) == 0);
-    }
-    for (int i = (max_generation - 1); i >= 0; i--)
-    {
-        generation* current_gen = generation_of (i);
-#ifdef USE_REGIONS
-        heap_segment* ephemeral_region = heap_segment_rw (generation_start_segment (current_gen));
-        while (ephemeral_region)
-#endif //USE_REGIONS
-        {
-#ifdef USE_REGIONS
-            uint8_t* o = heap_segment_mem (ephemeral_region);
-            uint8_t* end = heap_segment_background_allocated (ephemeral_region);
-            dprintf (3, ("bgc eph: gen%d seg %p(%p-%p)",
-                heap_segment_gen_num (ephemeral_region),
-                heap_segment_mem (ephemeral_region),
-                heap_segment_allocated (ephemeral_region),
-                heap_segment_background_allocated (ephemeral_region)));
-            if (!end)
-            {
-                ephemeral_region->flags |= heap_segment_flags_swept;
-                ephemeral_region = heap_segment_next (ephemeral_region);
-                continue;
-            }
-#else //USE_REGIONS
-            uint8_t* o = generation_allocation_start (current_gen);
-            o = o + Align(size (o), align_const);
-            uint8_t* end = ((i > 0) ?
-                        generation_allocation_start (generation_of (i - 1)) :
-                        heap_segment_allocated (ephemeral_heap_segment));
-#endif //USE_REGIONS
-            uint8_t* plug_end = o;
-            uint8_t* plug_start = o;
-            BOOL marked_p = FALSE;
-            while (o < end)
-            {
-                marked_p = background_object_marked (o, TRUE);
-                if (marked_p)
-                {
-                    plug_start = o;
-                    size_t plug_size = plug_start - plug_end;
-                    if (i >= 1)
-                    {
-                        thread_gap (plug_end, plug_size, current_gen);
-                    }
-                    else
-                    {
-                        if (plug_size > 0)
-                        {
-                            make_unused_array (plug_end, plug_size);
-                            if (plug_size >= min_free_list)
-                            {
-                                youngest_free_list_space += plug_size;
-                                youngest_free_list.thread_item (plug_end, plug_size);
-                            }
-                            else
-                            {
-                                youngest_free_obj_space += plug_size;
-                            }
-                        }
-                    }
-                    fix_brick_to_highest (plug_end, plug_start);
-                    fix_brick_to_highest (plug_start, plug_start);
-                    BOOL m = TRUE;
-                    while (m)
-                    {
-                        o = o + Align (size (o), align_const);
-                        if (o >= end)
-                        {
-                            break;
-                        }
-                        m = background_object_marked (o, TRUE);
-                    }
-                    plug_end = o;
-                    dprintf (3, ("bgs: plug [%zx, %zx[", (size_t)plug_start, (size_t)plug_end));
-                }
-                else
-                {
-                    while ((o < end) && !background_object_marked (o, FALSE))
-                    {
-                        o = o + Align (size (o), align_const);
-                    }
-                }
-            }
-            if (plug_end != end)
-            {
-                if (i >= 1)
-                {
-                    thread_gap (plug_end, end - plug_end, current_gen);
-                }
-                else
-                {
-#ifndef USE_REGIONS
-                    heap_segment_allocated (ephemeral_heap_segment) = plug_end;
-                    heap_segment_saved_bg_allocated (ephemeral_heap_segment) = plug_end;
-#endif //!USE_REGIONS
-                    make_unused_array (plug_end, (end - plug_end));
-                }
-                fix_brick_to_highest (plug_end, end);
-            }
-#ifdef USE_REGIONS
-            ephemeral_region->flags |= heap_segment_flags_swept;
-            heap_segment_background_allocated (ephemeral_region) = 0;
-            ephemeral_region = heap_segment_next (ephemeral_region);
-#endif //USE_REGIONS
-        }
-        dd_fragmentation (dynamic_data_of (i)) =
-            generation_free_list_space (current_gen) + generation_free_obj_space (current_gen);
-    }
-    generation* youngest_gen = generation_of (0);
-    generation_free_list_space (youngest_gen) = youngest_free_list_space;
-    generation_free_obj_space (youngest_gen) = youngest_free_obj_space;
-    dd_fragmentation (dynamic_data_of (0)) = youngest_free_list_space + youngest_free_obj_space;
-    generation_allocator (youngest_gen)->copy_with_no_repair (&youngest_free_list);
-}
-void gc_heap::background_sweep()
-{
-    concurrent_print_time_delta ("Sw");
-    dprintf (2, ("---- (GC%zu)Background Sweep Phase ----", VolatileLoad(&settings.gc_index)));
-    dprintf (3, ("lh state: planning"));
-    for (int i = 0; i <= max_generation; i++)
-    {
-        generation* gen_to_reset = generation_of (i);
-#ifdef DOUBLY_LINKED_FL
-        if (i == max_generation)
-        {
-            dprintf (2, ("h%d: gen2 still has FL: %zd, FO: %zd",
-                heap_number,
-                generation_free_list_space (gen_to_reset),
-                generation_free_obj_space (gen_to_reset)));
-        }
-        else
-#endif //DOUBLY_LINKED_FL
-        {
-            generation_allocator (gen_to_reset)->clear();
-            generation_free_list_space (gen_to_reset) = 0;
-            generation_free_obj_space (gen_to_reset) = 0;
-        }
-        generation_free_list_allocated (gen_to_reset) = 0;
-        generation_end_seg_allocated (gen_to_reset) = 0;
-        generation_condemned_allocated (gen_to_reset) = 0;
-        generation_sweep_allocated (gen_to_reset) = 0;
-        generation_allocation_pointer (gen_to_reset)= 0;
-        generation_allocation_limit (gen_to_reset) = 0;
-        generation_allocation_segment (gen_to_reset) = heap_segment_rw (generation_start_segment (gen_to_reset));
-    }
-    FIRE_EVENT(BGC2ndNonConEnd);
-    uoh_alloc_thread_count = 0;
-    init_free_and_plug();
-    current_bgc_state = bgc_sweep_soh;
-    verify_soh_segment_list();
-#ifdef DOUBLY_LINKED_FL
-    current_sweep_seg = heap_segment_rw (generation_start_segment (generation_of (max_generation)));
-    current_sweep_pos = 0;
-#endif //DOUBLY_LINKED_FL
-#ifdef FEATURE_BASICFREEZE
-    sweep_ro_segments();
-#endif //FEATURE_BASICFREEZE
-    if (current_c_gc_state != c_gc_state_planning)
-    {
-        current_c_gc_state = c_gc_state_planning;
-    }
-    concurrent_print_time_delta ("Swe");
-    for (int i = uoh_start_generation; i < total_generation_count; i++)
-    {
-        heap_segment* uoh_seg = heap_segment_rw (generation_start_segment (generation_of (i)));
-        PREFIX_ASSUME(uoh_seg  != NULL);
-        while (uoh_seg)
-        {
-            uoh_seg->flags &= ~heap_segment_flags_swept;
-            heap_segment_background_allocated (uoh_seg) = heap_segment_allocated (uoh_seg);
-            uoh_seg = heap_segment_next_rw (uoh_seg);
-        }
-    }
-#ifdef MULTIPLE_HEAPS
-    bgc_t_join.join(this, gc_join_restart_ee);
-    if (bgc_t_join.joined())
-    {
-        dprintf(2, ("Starting BGC threads for resuming EE"));
-        bgc_t_join.restart();
-    }
-#endif //MULTIPLE_HEAPS
-    if (heap_number == 0)
-    {
-#ifdef BGC_SERVO_TUNING
-        get_and_reset_loh_alloc_info();
-#endif //BGC_SERVO_TUNING
-        uint64_t suspended_end_ts = GetHighPrecisionTimeStamp();
-        last_bgc_info[last_bgc_info_index].pause_durations[1] = (size_t)(suspended_end_ts - suspended_start_time);
-        total_suspended_time += last_bgc_info[last_bgc_info_index].pause_durations[1];
-        restart_EE ();
-    }
-    FIRE_EVENT(BGC2ndConBegin);
-    background_ephemeral_sweep();
-    concurrent_print_time_delta ("Swe eph");
-#ifdef MULTIPLE_HEAPS
-    bgc_t_join.join(this, gc_join_after_ephemeral_sweep);
-    if (bgc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-#ifdef FEATURE_EVENT_TRACE
-        bgc_heap_walk_for_etw_p = GCEventStatus::IsEnabled(GCEventProvider_Default,
-                                                           GCEventKeyword_GCHeapSurvivalAndMovement,
-                                                           GCEventLevel_Information);
-#endif //FEATURE_EVENT_TRACE
-        leave_spin_lock (&gc_lock);
-#ifdef MULTIPLE_HEAPS
-        dprintf(2, ("Starting BGC threads for BGC sweeping"));
-        bgc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-    disable_preemptive (true);
-    dynamic_data* dd     = dynamic_data_of (max_generation);
-    const int num_objs   = 256;
-    int current_num_objs = 0;
-    for (int i = max_generation; i < total_generation_count; i++)
-    {
-        generation* gen = generation_of (i);
-        heap_segment* gen_start_seg = heap_segment_rw (generation_start_segment(gen));
-        heap_segment* next_seg = 0;
-        heap_segment* prev_seg;
-        heap_segment* start_seg;
-        int align_const = get_alignment_constant (i == max_generation);
-#ifndef DOUBLY_LINKED_FL
-        if (i == max_generation)
-        {
-#ifdef USE_REGIONS
-            start_seg = generation_tail_region (gen);
-#else
-            start_seg = saved_sweep_ephemeral_seg;
-#endif //USE_REGIONS
-            prev_seg = heap_segment_next(start_seg);
-        }
-        else
-#endif //!DOUBLY_LINKED_FL
-        {
-            start_seg = gen_start_seg;
-            prev_seg = NULL;
-            if (i > max_generation)
-            {
-                generation_allocator (gen)->clear();
-                generation_free_list_space (gen) = 0;
-                generation_free_obj_space (gen) = 0;
-                generation_free_list_allocated (gen) = 0;
-                generation_end_seg_allocated (gen) = 0;
-                generation_condemned_allocated (gen) = 0;
-                generation_sweep_allocated (gen) = 0;
-                generation_allocation_pointer (gen)= 0;
-                generation_allocation_limit (gen) = 0;
-                generation_allocation_segment (gen) = heap_segment_rw (generation_start_segment (gen));
-            }
-            else
-            {
-                dprintf (3333, ("h%d: SOH sweep start on seg %zx: total FL: %zd, FO: %zd",
-                    heap_number, (size_t)start_seg,
-                    generation_free_list_space (gen),
-                    generation_free_obj_space (gen)));
-            }
-        }
-        PREFIX_ASSUME(start_seg != NULL);
-        heap_segment* seg = start_seg;
-        dprintf (2, ("bgs: sweeping gen %d seg %p->%p(%p)", gen->gen_num,
-            heap_segment_mem (seg),
-            heap_segment_allocated (seg),
-            heap_segment_background_allocated (seg)));
-        while (seg
-#ifdef DOUBLY_LINKED_FL
-               && !((heap_segment_background_allocated (seg) == 0) && (gen != large_object_generation))
-#endif //DOUBLY_LINKED_FL
-                )
-        {
-            uint8_t* o = heap_segment_mem (seg);
-            if (seg == gen_start_seg)
-            {
-#ifndef USE_REGIONS
-                assert (o == generation_allocation_start (gen));
-                assert (method_table (o) == g_gc_pFreeObjectMethodTable);
-                o = o + Align (size (o), align_const);
-#endif //!USE_REGIONS
-            }
-            uint8_t* plug_end = o;
-            current_sweep_pos = o;
-            next_sweep_obj = o;
-#ifdef DOUBLY_LINKED_FL
-            current_sweep_seg = seg;
-#endif //DOUBLY_LINKED_FL
-            size_t free_obj_size_last_gap = 0;
-            allow_fgc();
-            uint8_t* end = background_next_end (seg, (i > max_generation));
-            dprintf (3333, ("bgs: seg: %zx, [%zx, %zx[%zx", (size_t)seg,
-                            (size_t)heap_segment_mem (seg),
-                            (size_t)heap_segment_allocated (seg),
-                            (size_t)heap_segment_background_allocated (seg)));
-            while (o < end)
-            {
-                if (background_object_marked (o, TRUE))
-                {
-                    uint8_t* plug_start = o;
-                    if (i > max_generation)
-                    {
-                        dprintf (2, ("uoh fr: [%p-%p[(%zd)", plug_end, plug_start, plug_start-plug_end));
-                    }
-                    thread_gap (plug_end, plug_start-plug_end, gen);
-                    if (i == max_generation)
-                    {
-                        add_gen_free (max_generation, plug_start-plug_end);
-#ifdef DOUBLY_LINKED_FL
-                        if (free_obj_size_last_gap)
-                        {
-                            generation_free_obj_space (gen) -= free_obj_size_last_gap;
-                            dprintf (3333, ("[h%d] LG: gen2FO-: %zd->%zd",
-                                heap_number, free_obj_size_last_gap, generation_free_obj_space (gen)));
-                            free_obj_size_last_gap = 0;
-                        }
-#endif //DOUBLY_LINKED_FL
-                        fix_brick_to_highest (plug_end, plug_start);
-                        fix_brick_to_highest (plug_start, plug_start);
-                    }
-                    do
-                    {
-                        next_sweep_obj = o + Align (size (o), align_const);
-                        current_num_objs++;
-                        if (current_num_objs >= num_objs)
-                        {
-                            current_sweep_pos = next_sweep_obj;
-                            allow_fgc();
-                            current_num_objs = 0;
-                        }
-                        o = next_sweep_obj;
-                    } while ((o < end) && background_object_marked(o, TRUE));
-                    plug_end = o;
-                    if (i == max_generation)
-                    {
-                        add_gen_plug (max_generation, plug_end-plug_start);
-                        dd_survived_size (dd) += (plug_end - plug_start);
-                    }
-                    dprintf (3, ("bgs: plug [%zx, %zx[", (size_t)plug_start, (size_t)plug_end));
-                }
-                while ((o < end) && !background_object_marked (o, FALSE))
-                {
-                    size_t size_o = Align(size (o), align_const);
-                    next_sweep_obj = o + size_o;
-#ifdef DOUBLY_LINKED_FL
-                    if (gen != large_object_generation)
-                    {
-                        if (method_table (o) == g_gc_pFreeObjectMethodTable)
-                        {
-                            free_obj_size_last_gap += size_o;
-                            if (is_on_free_list (o, size_o))
-                            {
-#ifdef MULTIPLE_HEAPS
-                                assert (heap_of (o) == this);
-#endif //MULTIPLE_HEAPS
-                                generation_allocator (gen)->unlink_item_no_undo (o, size_o);
-                                generation_free_list_space (gen) -= size_o;
-                                assert ((ptrdiff_t)generation_free_list_space (gen) >= 0);
-                                generation_free_obj_space (gen) += size_o;
-                                dprintf (3333, ("[h%d] gen2F-: %p->%p(%zd) FL: %zd",
-                                    heap_number, o, (o + size_o), size_o,
-                                    generation_free_list_space (gen)));
-                                dprintf (3333, ("h%d: gen2FO+: %p(%zx)->%zd (g: %zd)",
-                                    heap_number, o, size_o,
-                                    generation_free_obj_space (gen),
-                                    free_obj_size_last_gap));
-                                remove_gen_free (max_generation, size_o);
-                            }
-                            else
-                            {
-                                dprintf (3333, ("h%d: gen2FO: %p(%zd)->%zd (g: %zd)",
-                                    heap_number, o, size_o,
-                                    generation_free_obj_space (gen), free_obj_size_last_gap));
-                            }
-                            dprintf (3333, ("h%d: total FO: %p->%p FL: %zd, FO: %zd (g: %zd)",
-                                heap_number, plug_end, next_sweep_obj,
-                                generation_free_list_space (gen),
-                                generation_free_obj_space (gen),
-                                free_obj_size_last_gap));
-                        }
-                    }
-#endif //DOUBLY_LINKED_FL
-                    current_num_objs++;
-                    if (current_num_objs >= num_objs)
-                    {
-                        current_sweep_pos = plug_end;
-                        dprintf (1234, ("f: swept till %p", current_sweep_pos));
-                        allow_fgc();
-                        current_num_objs = 0;
-                    }
-                    o = next_sweep_obj;
-                }
-            }
-#ifdef DOUBLY_LINKED_FL
-            next_seg = heap_segment_next (seg);
-#else //DOUBLY_LINKED_FL
-            if (i > max_generation)
-            {
-                next_seg = heap_segment_next (seg);
-            }
-            else
-            {
-                next_seg = heap_segment_prev (gen_start_seg, seg);
-            }
-#endif //DOUBLY_LINKED_FL
-            BOOL delete_p = FALSE;
-            if (!heap_segment_read_only_p (seg))
-            {
-                if (i > max_generation)
-                {
-                    process_background_segment_end (seg, gen, plug_end,
-                                                    start_seg, &delete_p, 0);
-                }
-                else
-                {
-                    assert (heap_segment_background_allocated (seg) != 0);
-                    process_background_segment_end (seg, gen, plug_end,
-                                                    start_seg, &delete_p, free_obj_size_last_gap);
-#ifndef USE_REGIONS
-                    assert (next_seg || !delete_p);
-#endif //!USE_REGIONS
-                }
-            }
-            heap_segment* saved_prev_seg = prev_seg;
-            if (delete_p)
-            {
-                generation_delete_heap_segment (gen, seg, prev_seg, next_seg);
-            }
-            else
-            {
-                prev_seg = seg;
-                dprintf (2, ("seg %p (%p) has been swept", seg, heap_segment_mem (seg)));
-                seg->flags |= heap_segment_flags_swept;
-                current_sweep_pos = end;
-            }
-            verify_soh_segment_list();
-#ifdef DOUBLY_LINKED_FL
-            while (next_seg && heap_segment_background_allocated (next_seg) == 0)
-            {
-                dprintf (2, ("[h%d] skip new %p ", heap_number, next_seg));
-                next_seg = heap_segment_next (next_seg);
-            }
-#endif //DOUBLY_LINKED_FL
-            dprintf (GTC_LOG, ("seg: %p(%p), next_seg: %p(%p), prev_seg: %p(%p), delete_p %d",
-                seg, (seg ? heap_segment_mem (seg) : 0),
-                next_seg, (next_seg ? heap_segment_mem (next_seg) : 0),
-                saved_prev_seg, (saved_prev_seg ? heap_segment_mem (saved_prev_seg) : 0),
-                (delete_p ? 1 : 0)));
-            seg = next_seg;
-        }
-        generation_allocation_segment (gen) = heap_segment_rw (generation_start_segment (gen));
-        PREFIX_ASSUME(generation_allocation_segment(gen) != NULL);
-        if (i == max_generation)
-        {
-            dprintf (2, ("bgs: sweeping uoh objects"));
-            concurrent_print_time_delta ("Swe SOH");
-            FIRE_EVENT(BGC1stSweepEnd, 0);
-            enter_spin_lock (&more_space_lock_uoh);
-            add_saved_spinlock_info (true, me_acquire, mt_bgc_uoh_sweep, msl_entered);
-            concurrent_print_time_delta ("Swe UOH took msl");
-            int spin_count = yp_spin_count_unit;
-            while (uoh_alloc_thread_count)
-            {
-                spin_and_switch (spin_count, (uoh_alloc_thread_count == 0));
-            }
-            current_bgc_state = bgc_sweep_uoh;
-        }
-    }
-    size_t total_soh_size = generation_sizes (generation_of (max_generation));
-    size_t total_loh_size = generation_size (loh_generation);
-    size_t total_poh_size = generation_size (poh_generation);
-    dprintf (GTC_LOG, ("h%d: S: poh: %zd, loh: %zd, soh: %zd", heap_number, total_poh_size, total_loh_size, total_soh_size));
-    dprintf (GTC_LOG, ("end of bgc sweep: gen2 FL: %zd, FO: %zd",
-        generation_free_list_space (generation_of (max_generation)),
-        generation_free_obj_space (generation_of (max_generation))));
-    dprintf (GTC_LOG, ("h%d: end of bgc sweep: loh FL: %zd, FO: %zd",
-        heap_number,
-        generation_free_list_space (generation_of (loh_generation)),
-        generation_free_obj_space (generation_of (loh_generation))));
-    dprintf (GTC_LOG, ("h%d: end of bgc sweep: poh FL: %zd, FO: %zd",
-        heap_number,
-        generation_free_list_space (generation_of (poh_generation)),
-        generation_free_obj_space (generation_of (poh_generation))));
-    FIRE_EVENT(BGC2ndConEnd);
-    concurrent_print_time_delta ("background sweep");
-    heap_segment* reset_seg = heap_segment_rw (generation_start_segment (generation_of (max_generation)));
-    PREFIX_ASSUME(reset_seg != NULL);
-    while (reset_seg)
-    {
-        heap_segment_saved_bg_allocated (reset_seg) = heap_segment_background_allocated (reset_seg);
-        heap_segment_background_allocated (reset_seg) = 0;
-        reset_seg = heap_segment_next_rw (reset_seg);
-    }
-    compute_new_dynamic_data (max_generation);
-#ifdef DOUBLY_LINKED_FL
-    current_bgc_state = bgc_not_in_process;
-    current_sweep_seg = 0;
-#endif //DOUBLY_LINKED_FL
-    enable_preemptive ();
-#ifdef MULTIPLE_HEAPS
-    bgc_t_join.join(this, gc_join_set_state_free);
-    if (bgc_t_join.joined())
-#endif //MULTIPLE_HEAPS
-    {
-        current_c_gc_state = c_gc_state_free;
-#ifdef BGC_SERVO_TUNING
-        if (bgc_tuning::enable_fl_tuning)
-        {
-            enter_spin_lock (&gc_lock);
-            bgc_tuning::record_and_adjust_bgc_end();
-            leave_spin_lock (&gc_lock);
-        }
-#endif //BGC_SERVO_TUNING
-#ifdef MULTIPLE_HEAPS
-        dprintf(2, ("Starting BGC threads after background sweep phase"));
-        bgc_t_join.restart();
-#endif //MULTIPLE_HEAPS
-    }
-    disable_preemptive (true);
-    add_saved_spinlock_info (true, me_release, mt_bgc_uoh_sweep, msl_entered);
-    leave_spin_lock (&more_space_lock_uoh);
-    dprintf (GTC_LOG, ("---- (GC%zu)ESw ----", VolatileLoad(&settings.gc_index)));
-}
-#endif //BACKGROUND_GC
-void gc_heap::sweep_uoh_objects (int gen_num)
-{
-    generation* gen        = generation_of (gen_num);
-    heap_segment* start_seg = heap_segment_rw (generation_start_segment (gen));
-    PREFIX_ASSUME(start_seg != NULL);
-    heap_segment* seg      = start_seg;
-    heap_segment* prev_seg = 0;
-    uint8_t* o             = get_uoh_start_object (seg, gen);
-    uint8_t* plug_end         = o;
-    uint8_t* plug_start       = o;
-    generation_allocator (gen)->clear();
-    generation_free_list_space (gen) = 0;
-    generation_free_obj_space (gen) = 0;
-    generation_free_list_allocated (gen) = 0;
-    dprintf (3, ("sweeping uoh objects"));
-    dprintf (3, ("seg: %zx, [%zx, %zx[, starting from %p",
-                 (size_t)seg,
-                 (size_t)heap_segment_mem (seg),
-                 (size_t)heap_segment_allocated (seg),
-                 o));
-    while (1)
-    {
-        if (o >= heap_segment_allocated (seg))
-        {
-            heap_segment* next_seg = heap_segment_next (seg);
-            if ((plug_end == heap_segment_mem (seg)) &&
-                (seg != start_seg) && !heap_segment_read_only_p (seg))
-            {
-                dprintf (3, ("Preparing empty large segment %zx", (size_t)seg));
-                assert (prev_seg);
-                heap_segment_next (prev_seg) = next_seg;
-                heap_segment_next (seg) = freeable_uoh_segment;
-                freeable_uoh_segment = seg;
-#ifdef USE_REGIONS
-                update_start_tail_regions (gen, seg, prev_seg, next_seg);
-#endif //USE_REGIONS
-            }
-            else
-            {
-                if (!heap_segment_read_only_p (seg))
-                {
-                    dprintf (3, ("Trimming seg to %zx[", (size_t)plug_end));
-                    heap_segment_allocated (seg) = plug_end;
-                    decommit_heap_segment_pages (seg, 0);
-                }
-                prev_seg = seg;
-            }
-            seg = next_seg;
-            if (seg == 0)
-                break;
-            else
-            {
-                o = heap_segment_mem (seg);
-                plug_end = o;
-                dprintf (3, ("seg: %zx, [%zx, %zx[", (size_t)seg,
-                             (size_t)heap_segment_mem (seg),
-                             (size_t)heap_segment_allocated (seg)));
-#ifdef USE_REGIONS
-                continue;
-#endif //USE_REGIONS
-            }
-        }
-        if (uoh_object_marked(o, TRUE))
-        {
-            plug_start = o;
-            thread_gap (plug_end, plug_start-plug_end, gen);
-            BOOL m = TRUE;
-            while (m)
-            {
-                o = o + AlignQword (size (o));
-                if (o >= heap_segment_allocated (seg))
-                {
-                    break;
-                }
-                m = uoh_object_marked (o, TRUE);
-            }
-            plug_end = o;
-            dprintf (3, ("plug [%zx, %zx[", (size_t)plug_start, (size_t)plug_end));
-        }
-        else
-        {
-            while (o < heap_segment_allocated (seg) && !uoh_object_marked(o, FALSE))
-            {
-                o = o + AlignQword (size (o));
-            }
-        }
-    }
-    generation_allocation_segment (gen) = heap_segment_rw (generation_start_segment (gen));
-    PREFIX_ASSUME(generation_allocation_segment(gen) != NULL);
-}
-void gc_heap::relocate_in_uoh_objects (int gen_num)
-{
-    generation* gen = generation_of (gen_num);
-    heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-    PREFIX_ASSUME(seg != NULL);
-    uint8_t* o = get_uoh_start_object (seg, gen);
-    while (1)
-    {
-        if (o >= heap_segment_allocated (seg))
-        {
-            seg = heap_segment_next_rw (seg);
-            if (seg == 0)
-                break;
-            else
-            {
-                o = heap_segment_mem (seg);
-            }
-        }
-        while (o < heap_segment_allocated (seg))
-        {
-            check_class_object_demotion (o);
-            if (contain_pointers (o))
-            {
-                dprintf(3, ("Relocating through uoh object %zx", (size_t)o));
-                go_through_object_nostart (method_table (o), o, size(o), pval,
-                        {
-                            reloc_survivor_helper (pval);
-                        });
-            }
-            o = o + AlignQword (size (o));
-        }
-    }
-}
-void gc_heap::mark_through_cards_for_uoh_objects (card_fn fn,
-                                                  int gen_num,
-                                                  BOOL relocating
-                                                  CARD_MARKING_STEALING_ARG(gc_heap* hpt))
-{
-#ifdef USE_REGIONS
-    uint8_t*      low               = 0;
-#else
-    uint8_t*      low               = gc_low;
-#endif //USE_REGIONS
-    size_t        end_card          = 0;
-    generation*   oldest_gen        = generation_of (gen_num);
-    heap_segment* seg               = heap_segment_rw (generation_start_segment (oldest_gen));
-    PREFIX_ASSUME(seg != NULL);
-    uint8_t*      beg               = get_uoh_start_object (seg, oldest_gen);
-    uint8_t*      end               = heap_segment_allocated (seg);
-    size_t  cg_pointers_found = 0;
-    size_t  card_word_end = (card_of (align_on_card_word (end)) /
-                             card_word_width);
-    size_t      n_eph             = 0;
-    size_t      n_gen             = 0;
-    size_t      n_card_set        = 0;
-#ifdef USE_REGIONS
-    uint8_t*    next_boundary = 0;
-    uint8_t*    nhigh         = 0;
-#else
-    uint8_t*    next_boundary = (relocating ?
-                              generation_plan_allocation_start (generation_of (max_generation -1)) :
-                              ephemeral_low);
-    uint8_t*    nhigh         = (relocating ?
-                              heap_segment_plan_allocated (ephemeral_heap_segment) :
-                              ephemeral_high);
-#endif //USE_REGIONS
-    BOOL          foundp            = FALSE;
-    uint8_t*      start_address     = 0;
-    uint8_t*      limit             = 0;
-    size_t        card              = card_of (beg);
-    uint8_t*      o                 = beg;
-#ifdef BACKGROUND_GC
-    BOOL consider_bgc_mark_p        = FALSE;
-    BOOL check_current_sweep_p      = FALSE;
-    BOOL check_saved_sweep_p        = FALSE;
-    should_check_bgc_mark (seg, &consider_bgc_mark_p, &check_current_sweep_p, &check_saved_sweep_p);
-#endif //BACKGROUND_GC
-    size_t total_cards_cleared = 0;
-#ifdef FEATURE_CARD_MARKING_STEALING
-    VOLATILE(uint32_t)* chunk_index = (VOLATILE(uint32_t)*) &(gen_num == loh_generation ?
-        card_mark_chunk_index_loh :
-        card_mark_chunk_index_poh);
-    card_marking_enumerator card_mark_enumerator(seg, low, chunk_index);
-    card_word_end = 0;
-#endif // FEATURE_CARD_MARKING_STEALING
-#ifdef USE_REGIONS
-    int condemned_gen = settings.condemned_generation;
-#else
-    int condemned_gen = -1;
-#endif //USE_REGIONS
-    dprintf(3, ("CMl: %zx->%zx", (size_t)beg, (size_t)end));
-    while (1)
-    {
-        if ((o < end) && (card_of(o) > card))
-        {
-            dprintf (3, ("Found %zd cg pointers", cg_pointers_found));
-            if (cg_pointers_found == 0)
-            {
-                uint8_t* last_object_processed = o;
-#ifdef FEATURE_CARD_MARKING_STEALING
-                last_object_processed = min(limit, o);
-#endif // FEATURE_CARD_MARKING_STEALING
-                dprintf (3, (" Clearing cards [%zx, %zx[ ", (size_t)card_address(card), (size_t)last_object_processed));
-                clear_cards (card, card_of((uint8_t*)last_object_processed));
-                total_cards_cleared += (card_of((uint8_t*)last_object_processed) - card);
-            }
-            n_eph +=cg_pointers_found;
-            cg_pointers_found = 0;
-            card = card_of ((uint8_t*)o);
-        }
-        if ((o < end) &&(card >= end_card))
-        {
-#ifdef FEATURE_CARD_MARKING_STEALING
-            foundp = find_next_chunk(card_mark_enumerator, seg, n_card_set, start_address, limit, card, end_card, card_word_end);
-#else // FEATURE_CARD_MARKING_STEALING
-            foundp = find_card (card_table, card, card_word_end, end_card);
-            if (foundp)
-            {
-                n_card_set+= end_card - card;
-                start_address = max (beg, card_address (card));
-            }
-            limit = min (end, card_address (end_card));
-#endif  // FEATURE_CARD_MARKING_STEALING
-        }
-        if ((!foundp) || (o >= end) || (card_address (card) >= end))
-        {
-            if ((foundp) && (cg_pointers_found == 0))
-            {
-                dprintf(3,(" Clearing cards [%zx, %zx[ ", (size_t)card_address(card),
-                           (size_t)card_address(card+1)));
-                clear_cards (card, card+1);
-                total_cards_cleared += 1;
-            }
-            n_eph +=cg_pointers_found;
-            cg_pointers_found = 0;
-#ifdef FEATURE_CARD_MARKING_STEALING
-            card_mark_enumerator.exhaust_segment(seg);
-#endif // FEATURE_CARD_MARKING_STEALING
-            if ((seg = heap_segment_next_rw (seg)) != 0)
-            {
-#ifdef BACKGROUND_GC
-                should_check_bgc_mark (seg, &consider_bgc_mark_p, &check_current_sweep_p, &check_saved_sweep_p);
-#endif //BACKGROUND_GC
-                beg = heap_segment_mem (seg);
-                end = compute_next_end (seg, low);
-#ifdef FEATURE_CARD_MARKING_STEALING
-                card_word_end = 0;
-#else // FEATURE_CARD_MARKING_STEALING
-                card_word_end = card_of (align_on_card_word (end)) / card_word_width;
-#endif // FEATURE_CARD_MARKING_STEALING
-                card = card_of (beg);
-                o  = beg;
-                end_card = 0;
-                continue;
-            }
-            else
-            {
-                break;
-            }
-        }
-        assert (card_set_p (card));
-        {
-            dprintf(3,("card %zx: o: %zx, l: %zx[ ",
-                       card, (size_t)o, (size_t)limit));
-            assert (Align (size (o)) >= Align (min_obj_size));
-            size_t s = size (o);
-            uint8_t* next_o =  o + AlignQword (s);
-            Prefetch (next_o);
-            while (o < limit)
-            {
-                s = size (o);
-                assert (Align (s) >= Align (min_obj_size));
-                next_o =  o + AlignQword (s);
-                Prefetch (next_o);
-                dprintf (4, ("|%zx|", (size_t)o));
-                if (next_o < start_address)
-                {
-                    goto end_object;
-                }
-#ifdef BACKGROUND_GC
-                if (!fgc_should_consider_object (o, seg, consider_bgc_mark_p, check_current_sweep_p, check_saved_sweep_p))
-                {
-                    goto end_object;
-                }
-#endif //BACKGROUND_GC
-#ifdef COLLECTIBLE_CLASS
-                if (is_collectible(o))
-                {
-                    BOOL passed_end_card_p = FALSE;
-                    if (card_of (o) > card)
-                    {
-                        passed_end_card_p = card_transition (o, end, card_word_end,
-                            cg_pointers_found,
-                            n_eph, n_card_set,
-                            card, end_card,
-                            foundp, start_address,
-                            limit, total_cards_cleared
-                            CARD_MARKING_STEALING_ARGS(card_mark_enumerator, seg, card_word_end));
-                    }
-                    if ((!passed_end_card_p || foundp) && (card_of (o) == card))
-                    {
-                        if (fn == &gc_heap::relocate_address)
-                        {
-                            cg_pointers_found++;
-                        }
-                        else
-                        {
-                            uint8_t* class_obj = get_class_object (o);
-                            mark_through_cards_helper (&class_obj, n_gen,
-                                                       cg_pointers_found, fn,
-                                                       nhigh, next_boundary,
-                                                       condemned_gen, max_generation CARD_MARKING_STEALING_ARG(hpt));
-                        }
-                    }
-                    if (passed_end_card_p)
-                    {
-                        if (foundp && (card_address (card) < next_o))
-                        {
-                            goto go_through_refs;
-                        }
-                        else
-                        {
-                            goto end_object;
-                        }
-                    }
-                }
-go_through_refs:
-#endif //COLLECTIBLE_CLASS
-                if (contain_pointers (o))
-                {
-                    dprintf(3,("Going through %zx", (size_t)o));
-                    go_through_object (method_table(o), o, s, poo,
-                                       start_address, use_start, (o + s),
-                       {
-                           if (card_of ((uint8_t*)poo) > card)
-                           {
-                                BOOL passed_end_card_p  = card_transition ((uint8_t*)poo, end,
-                                        card_word_end,
-                                        cg_pointers_found,
-                                        n_eph, n_card_set,
-                                        card, end_card,
-                                        foundp, start_address,
-                                        limit, total_cards_cleared
-                                        CARD_MARKING_STEALING_ARGS(card_mark_enumerator, seg, card_word_end));
-                                if (passed_end_card_p)
-                                {
-                                    if (foundp && (card_address (card) < next_o))
-                                    {
-                                        {
-                                            if (ppstop <= (uint8_t**)start_address)
-                                            {break;}
-                                            else if (poo < (uint8_t**)start_address)
-                                            {poo = (uint8_t**)start_address;}
-                                        }
-                                    }
-                                    else
-                                    {
-                                        goto end_object;
-                                    }
-                                }
-                            }
-                           mark_through_cards_helper (poo, n_gen,
-                                                      cg_pointers_found, fn,
-                                                      nhigh, next_boundary,
-                                                      condemned_gen, max_generation CARD_MARKING_STEALING_ARG(hpt));
-                       }
-                        );
-                }
-            end_object:
-                o = next_o;
-            }
-        }
-    }
-    if (!relocating)
-    {
-#ifdef FEATURE_CARD_MARKING_STEALING
-        Interlocked::ExchangeAddPtr(&n_eph_loh, n_eph);
-        Interlocked::ExchangeAddPtr(&n_gen_loh, n_gen);
-        dprintf (3, ("h%d marking h%d Mloh: cross: %zd, useful: %zd, cards set: %zd, cards cleared: %zd, ratio: %d",
-            hpt->heap_number, heap_number, n_eph, n_gen, n_card_set, total_cards_cleared,
-            (n_eph ? (int)(((float)n_gen / (float)n_eph) * 100) : 0)));
-        dprintf (3, ("h%d marking h%d Mloh: total cross %zd, useful: %zd, running ratio: %d",
-            hpt->heap_number, heap_number, (size_t)n_eph_loh, (size_t)n_gen_loh,
-            (n_eph_loh ? (int)(((float)n_gen_loh / (float)n_eph_loh) * 100) : 0)));
-#else
-        generation_skip_ratio = min (((n_eph > MIN_LOH_CROSS_GEN_REFS) ?
-            (int)(((float)n_gen / (float)n_eph) * 100) : 100),
-            generation_skip_ratio);
-        dprintf (3, ("marking h%d Mloh: cross: %zd, useful: %zd, cards cleared: %zd, cards set: %zd, ratio: %d",
-            heap_number, n_eph, n_gen, total_cards_cleared, n_card_set, generation_skip_ratio));
-#endif //FEATURE_CARD_MARKING_STEALING
-    }
-    else
-    {
-        dprintf (3, ("R: Mloh: cross: %zd, useful: %zd, cards set: %zd, ratio: %d",
-             n_eph, n_gen, n_card_set, generation_skip_ratio));
-    }
-}
-void gc_heap::descr_generations_to_profiler (gen_walk_fn fn, void *context)
-{
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-    {
-        gc_heap* hp = g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = NULL;
-#ifdef _PREFAST_
-        PREFIX_ASSUME(hp != NULL);
-#endif // _PREFAST_
-#endif //MULTIPLE_HEAPS
-        for (int curr_gen_number = total_generation_count-1; curr_gen_number >= 0; curr_gen_number--)
-        {
-            generation* gen = hp->generation_of (curr_gen_number);
-            heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-#ifdef USE_REGIONS
-            while (seg)
-            {
-                fn(context, curr_gen_number, heap_segment_mem (seg),
-                                              heap_segment_allocated (seg),
-                                              heap_segment_reserved (seg));
-                seg = heap_segment_next_rw (seg);
-            }
-#else
-            while (seg && (seg != hp->ephemeral_heap_segment))
-            {
-                assert (curr_gen_number > 0);
-                fn(context, curr_gen_number, heap_segment_mem (seg),
-                                              heap_segment_allocated (seg),
-                                              (curr_gen_number > max_generation) ?
-                                                heap_segment_reserved (seg) : heap_segment_allocated (seg));
-                seg = heap_segment_next_rw (seg);
-            }
-            if (seg)
-            {
-                assert (seg == hp->ephemeral_heap_segment);
-                assert (curr_gen_number <= max_generation);
-                if (curr_gen_number == max_generation)
-                {
-                    if (heap_segment_mem (seg) < generation_allocation_start (hp->generation_of (max_generation-1)))
-                    {
-                        fn(context, curr_gen_number, heap_segment_mem (seg),
-                                                      generation_allocation_start (hp->generation_of (max_generation-1)),
-                                                      generation_allocation_start (hp->generation_of (max_generation-1)) );
-                    }
-                }
-                else if (curr_gen_number != 0)
-                {
-                    fn(context, curr_gen_number, generation_allocation_start (hp->generation_of (curr_gen_number)),
-                                                  generation_allocation_start (hp->generation_of (curr_gen_number-1)),
-                                                  generation_allocation_start (hp->generation_of (curr_gen_number-1)));
-                }
-                else
-                {
-                    fn(context, curr_gen_number, generation_allocation_start (hp->generation_of (curr_gen_number)),
-                                                  heap_segment_allocated (hp->ephemeral_heap_segment),
-                                                  heap_segment_reserved (hp->ephemeral_heap_segment) );
-                }
-            }
-#endif //USE_REGIONS
-        }
-    }
-}
-#ifdef TRACE_GC
-void gc_heap::print_free_list (int gen, heap_segment* seg)
-{
-    UNREFERENCED_PARAMETER(gen);
-    UNREFERENCED_PARAMETER(seg);
-/*
-    if (settings.concurrent == FALSE)
-    {
-        uint8_t* seg_start = heap_segment_mem (seg);
-        uint8_t* seg_end = heap_segment_allocated (seg);
-        dprintf (3, ("Free list in seg %zx:", seg_start));
-        size_t total_free_item = 0;
-        allocator* gen_allocator = generation_allocator (generation_of (gen));
-        for (unsigned int b = 0; b < gen_allocator->number_of_buckets(); b++)
-        {
-            uint8_t* fo = gen_allocator->alloc_list_head_of (b);
-            while (fo)
-            {
-                if (fo >= seg_start && fo < seg_end)
-                {
-                    total_free_item++;
-                    size_t free_item_len = size(fo);
-                    dprintf (3, ("[%zx, %zx[:%zd",
-                                 (size_t)fo,
-                                 (size_t)(fo + free_item_len),
-                                 free_item_len));
-                }
-                fo = free_list_slot (fo);
-            }
-        }
-        dprintf (3, ("total %zd free items", total_free_item));
-    }
-*/
-}
-#endif //TRACE_GC
-void gc_heap::descr_generations (const char* msg)
-{
-#ifndef TRACE_GC
-    UNREFERENCED_PARAMETER(msg);
-#endif //!TRACE_GC
-#ifdef STRESS_LOG
-    if (StressLog::StressLogOn(LF_GC, LL_INFO10))
-    {
-        gc_heap* hp = 0;
-#ifdef MULTIPLE_HEAPS
-        hp= this;
-#endif //MULTIPLE_HEAPS
-        STRESS_LOG1(LF_GC, LL_INFO10, "GC Heap %p\n", hp);
-        for (int n = max_generation; n >= 0; --n)
-        {
-#ifndef USE_REGIONS
-            STRESS_LOG4(LF_GC, LL_INFO10, "    Generation %d [%p, %p] cur = %p\n",
-                    n,
-                    generation_allocation_start(generation_of(n)),
-                    generation_allocation_limit(generation_of(n)),
-                    generation_allocation_pointer(generation_of(n)));
-#endif //USE_REGIONS
-            heap_segment* seg = generation_start_segment(generation_of(n));
-            while (seg)
-            {
-                STRESS_LOG4(LF_GC, LL_INFO10, "        Segment mem %p alloc = %p used %p committed %p\n",
-                        heap_segment_mem(seg),
-                        heap_segment_allocated(seg),
-                        heap_segment_used(seg),
-                        heap_segment_committed(seg));
-                seg = heap_segment_next(seg);
-            }
-        }
-    }
-#endif  // STRESS_LOG
-#ifdef TRACE_GC
-    dprintf (2, ("lowest_address: %zx highest_address: %zx",
-             (size_t) lowest_address, (size_t) highest_address));
-#ifdef BACKGROUND_GC
-    dprintf (2, ("bgc lowest_address: %zx bgc highest_address: %zx",
-             (size_t) background_saved_lowest_address, (size_t) background_saved_highest_address));
-#endif //BACKGROUND_GC
-    if (heap_number == 0)
-    {
-#ifdef USE_REGIONS
-        size_t alloc_size = get_total_heap_size () / 1024 / 1024;
-        size_t commit_size = get_total_committed_size () / 1024 / 1024;
-        size_t frag_size = get_total_fragmentation () / 1024 / 1024;
-        int total_new_gen0_regions_in_plns = get_total_new_gen0_regions_in_plns ();
-        int total_new_regions_in_prr = get_total_new_regions_in_prr ();
-        int total_new_regions_in_threading = get_total_new_regions_in_threading ();
-        uint64_t elapsed_time_so_far = GetHighPrecisionTimeStamp () - process_start_time;
-        size_t idx = VolatileLoadWithoutBarrier (&settings.gc_index);
-        dprintf (REGIONS_LOG, ("[%s] GC#%5Id [%s] heap %Idmb (F: %Idmb %d%%) commit size: %Idmb, %0.3f min, %d,%d new in plan, %d in threading",
-            msg, idx, (settings.promotion ? "PM" : "NPM"), alloc_size, frag_size,
-            (int)((double)frag_size * 100.0 / (double)alloc_size),
-            commit_size,
-            (double)elapsed_time_so_far / (double)1000000 / (double)60,
-            total_new_gen0_regions_in_plns, total_new_regions_in_prr, total_new_regions_in_threading));
-        size_t total_gen_size_mb[loh_generation + 1] = { 0, 0, 0, 0 };
-        size_t total_gen_fragmentation_mb[loh_generation + 1] = { 0, 0, 0, 0 };
-        for (int i = 0; i < (loh_generation + 1); i++)
-        {
-            total_gen_size_mb[i] = get_total_generation_size (i) / 1024 / 1024;
-            total_gen_fragmentation_mb[i] = get_total_gen_fragmentation (i) / 1024 / 1024;
-        }
-        int bgcs = VolatileLoadWithoutBarrier (&current_bgc_state);
-#ifdef SIMPLE_DPRINTF
-        dprintf (REGIONS_LOG, ("[%s] GC#%Id (bgcs: %d, %s) g0: %Idmb (f: %Idmb %d%%), g1: %Idmb (f: %Idmb %d%%), g2: %Idmb (f: %Idmb %d%%), g3: %Idmb (f: %Idmb %d%%)",
-            msg, idx, bgcs, str_bgc_state[bgcs],
-            total_gen_size_mb[0], total_gen_fragmentation_mb[0], (total_gen_size_mb[0] ? (int)((double)total_gen_fragmentation_mb[0] * 100.0 / (double)total_gen_size_mb[0]) : 0),
-            total_gen_size_mb[1], total_gen_fragmentation_mb[1], (total_gen_size_mb[1] ? (int)((double)total_gen_fragmentation_mb[1] * 100.0 / (double)total_gen_size_mb[1]) : 0),
-            total_gen_size_mb[2], total_gen_fragmentation_mb[2], (total_gen_size_mb[2] ? (int)((double)total_gen_fragmentation_mb[2] * 100.0 / (double)total_gen_size_mb[2]) : 0),
-            total_gen_size_mb[3], total_gen_fragmentation_mb[3], (total_gen_size_mb[3] ? (int)((double)total_gen_fragmentation_mb[3] * 100.0 / (double)total_gen_size_mb[3]) : 0)));
-#endif //SIMPLE_DPRINTF
-        if ((idx % 20) == 0)
-        {
-            dprintf (1, ("[%5s] GC#%5Id total heap size: %Idmb (F: %Idmb %d%%) commit size: %Idmb, %0.3f min, %d,%d new in plan, %d in threading\n",
-                msg, idx, alloc_size, frag_size,
-                (int)((double)frag_size * 100.0 / (double)alloc_size),
-                commit_size,
-                (double)elapsed_time_so_far / (double)1000000 / (double)60,
-                total_new_gen0_regions_in_plns, total_new_regions_in_prr, total_new_regions_in_threading));
-        }
-#endif //USE_REGIONS
-    }
-    for (int curr_gen_number = total_generation_count - 1; curr_gen_number >= 0; curr_gen_number--)
-    {
-        size_t total_gen_size = generation_size (curr_gen_number);
-#ifdef SIMPLE_DPRINTF
-        dprintf (GTC_LOG, ("[%s][g%d]gen %d:, size: %zd, frag: %zd(L: %zd, O: %zd), f: %d%% %s %s %s",
-                      msg,
-                      settings.condemned_generation,
-                      curr_gen_number,
-                      total_gen_size,
-                      dd_fragmentation (dynamic_data_of (curr_gen_number)),
-                      generation_free_list_space (generation_of (curr_gen_number)),
-                      generation_free_obj_space (generation_of (curr_gen_number)),
-                      (total_gen_size ?
-                        (int)(((double)dd_fragmentation (dynamic_data_of (curr_gen_number)) / (double)total_gen_size) * 100) :
-                        0),
-                      (settings.compaction ? "(compact)" : "(sweep)"),
-                      (settings.heap_expansion ? "(EX)" : " "),
-                      (settings.promotion ? "Promotion" : "NoPromotion")));
-#else
-        dprintf (2, ( "Generation %d: generation size: %zd, fragmentation: %zd",
-                      curr_gen_number,
-                      total_gen_size,
-                      dd_fragmentation (dynamic_data_of (curr_gen_number))));
-#endif //SIMPLE_DPRINTF
-        generation* gen = generation_of (curr_gen_number);
-        heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-#ifdef USE_REGIONS
-        dprintf (GTC_LOG, ("g%d: start seg: %p alloc seg: %p, tail region: %p",
-            curr_gen_number,
-            heap_segment_mem (seg),
-            heap_segment_mem (generation_allocation_segment (gen)),
-            heap_segment_mem (generation_tail_region (gen))));
-        while (seg)
-        {
-            dprintf (GTC_LOG, ("g%d: (%d:p %d) [%zx %zx(sa: %zx, pa: %zx)[-%zx[ (%zd) (%zd)",
-                               curr_gen_number,
-                               heap_segment_gen_num (seg),
-                               heap_segment_plan_gen_num (seg),
-                               (size_t)heap_segment_mem (seg),
-                               (size_t)heap_segment_allocated (seg),
-                               (size_t)heap_segment_saved_allocated (seg),
-                               (size_t)heap_segment_plan_allocated (seg),
-                               (size_t)heap_segment_committed (seg),
-                               (size_t)(heap_segment_allocated (seg) - heap_segment_mem (seg)),
-                               (size_t)(heap_segment_committed (seg) - heap_segment_allocated (seg))));
-            print_free_list (curr_gen_number, seg);
-            seg = heap_segment_next (seg);
-        }
-#else
-        while (seg && (seg != ephemeral_heap_segment))
-        {
-            dprintf (GTC_LOG, ("g%d: [%zx %zx[-%zx[ (%zd) (%zd)",
-                        curr_gen_number,
-                        (size_t)heap_segment_mem (seg),
-                        (size_t)heap_segment_allocated (seg),
-                        (size_t)heap_segment_committed (seg),
-                        (size_t)(heap_segment_allocated (seg) - heap_segment_mem (seg)),
-                        (size_t)(heap_segment_committed (seg) - heap_segment_allocated (seg))));
-            print_free_list (curr_gen_number, seg);
-            seg = heap_segment_next (seg);
-        }
-        if (seg && (seg != generation_start_segment (gen)))
-        {
-            dprintf (GTC_LOG, ("g%d: [%zx %zx[",
-                         curr_gen_number,
-                         (size_t)heap_segment_mem (seg),
-                         (size_t)generation_allocation_start (generation_of (curr_gen_number-1))));
-            print_free_list (curr_gen_number, seg);
-        }
-        else if (seg)
-        {
-            dprintf (GTC_LOG, ("g%d: [%zx %zx[",
-                         curr_gen_number,
-                         (size_t)generation_allocation_start (generation_of (curr_gen_number)),
-                         (size_t)(((curr_gen_number == 0)) ?
-                                  (heap_segment_allocated
-                                   (generation_start_segment
-                                    (generation_of (curr_gen_number)))) :
-                                  (generation_allocation_start
-                                   (generation_of (curr_gen_number - 1))))
-                         ));
-            print_free_list (curr_gen_number, seg);
-        }
-#endif //USE_REGIONS
-    }
-#endif //TRACE_GC
-}
-VOLATILE(BOOL)    GCHeap::GcInProgress            = FALSE;
-GCEvent           *GCHeap::WaitForGCEvent         = NULL;
-unsigned          GCHeap::GcCondemnedGeneration   = 0;
-size_t            GCHeap::totalSurvivedSize       = 0;
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-CFinalize*        GCHeap::m_Finalize              = 0;
-BOOL              GCHeap::GcCollectClasses        = FALSE;
-VOLATILE(int32_t) GCHeap::m_GCFLock               = 0;
-#ifndef FEATURE_NATIVEAOT // NativeAOT forces relocation a different way
-#ifdef STRESS_HEAP
-#ifndef MULTIPLE_HEAPS
-OBJECTHANDLE      GCHeap::m_StressObjs[NUM_HEAP_STRESS_OBJS];
-int               GCHeap::m_CurStressObj          = 0;
-#endif // !MULTIPLE_HEAPS
-#endif // STRESS_HEAP
-#endif // FEATURE_NATIVEAOT
-#endif //FEATURE_PREMORTEM_FINALIZATION
-class NoGCRegionLockHolder
-{
-public:
-    NoGCRegionLockHolder()
-    {
-        enter_spin_lock_noinstru(&g_no_gc_lock);
-    }
-    ~NoGCRegionLockHolder()
-    {
-        leave_spin_lock_noinstru(&g_no_gc_lock);
-    }
-};
-enable_no_gc_region_callback_status gc_heap::enable_no_gc_callback(NoGCRegionCallbackFinalizerWorkItem* callback, uint64_t callback_threshold)
-{
-    dprintf(1, ("[no_gc_callback] calling enable_no_gc_callback with callback_threshold = %llu\n", callback_threshold));
-    enable_no_gc_region_callback_status status = enable_no_gc_region_callback_status::succeed;
-    suspend_EE();
-    {
-        if (!current_no_gc_region_info.started)
-        {
-            status = enable_no_gc_region_callback_status::not_started;
-        }
-        else if (current_no_gc_region_info.callback != nullptr)
-        {
-            status = enable_no_gc_region_callback_status::already_registered;
-        }
-        else
-        {
-            uint64_t total_original_soh_budget = 0;
-            uint64_t total_original_loh_budget = 0;
-#ifdef MULTIPLE_HEAPS
-            for (int i = 0; i < gc_heap::n_heaps; i++)
-            {
-                gc_heap* hp = gc_heap::g_heaps [i];
-#else
-            {
-                gc_heap* hp = pGenGCHeap;
-#endif
-                total_original_soh_budget += hp->soh_allocation_no_gc;
-                total_original_loh_budget += hp->loh_allocation_no_gc;
-            }
-            uint64_t total_original_budget = total_original_soh_budget + total_original_loh_budget;
-            if (total_original_budget >= callback_threshold)
-            {
-                uint64_t total_withheld = total_original_budget - callback_threshold;
-                float soh_ratio = ((float)total_original_soh_budget)/total_original_budget;
-                float loh_ratio = ((float)total_original_loh_budget)/total_original_budget;
-                size_t soh_withheld_budget = (size_t)(soh_ratio * total_withheld);
-                size_t loh_withheld_budget = (size_t)(loh_ratio * total_withheld);
-#ifdef MULTIPLE_HEAPS
-                soh_withheld_budget = soh_withheld_budget / gc_heap::n_heaps;
-                loh_withheld_budget = loh_withheld_budget / gc_heap::n_heaps;
-#endif
-                soh_withheld_budget = max(soh_withheld_budget, 1);
-                soh_withheld_budget = Align(soh_withheld_budget, get_alignment_constant (TRUE));
-                loh_withheld_budget = Align(loh_withheld_budget, get_alignment_constant (FALSE));
-#ifdef MULTIPLE_HEAPS
-                for (int i = 0; i < gc_heap::n_heaps; i++)
-                {
-                    gc_heap* hp = gc_heap::g_heaps [i];
-#else
-                {
-                    gc_heap* hp = pGenGCHeap;
-#endif
-                    if (dd_new_allocation (hp->dynamic_data_of (soh_gen0)) <= (ptrdiff_t)soh_withheld_budget)
-                    {
-                        dprintf(1, ("[no_gc_callback] failed because of running out of soh budget= %llu\n", soh_withheld_budget));
-                        status = insufficient_budget;
-                    }
-                    if (dd_new_allocation (hp->dynamic_data_of (loh_generation)) <= (ptrdiff_t)loh_withheld_budget)
-                    {
-                        dprintf(1, ("[no_gc_callback] failed because of running out of loh budget= %llu\n", loh_withheld_budget));
-                        status = insufficient_budget;
-                    }
-                }
-                if (status == enable_no_gc_region_callback_status::succeed)
-                {
-                    dprintf(1, ("[no_gc_callback] enabling succeed\n"));
-#ifdef MULTIPLE_HEAPS
-                    for (int i = 0; i < gc_heap::n_heaps; i++)
-                    {
-                        gc_heap* hp = gc_heap::g_heaps [i];
-#else
-                    {
-                        gc_heap* hp = pGenGCHeap;
-#endif
-                        dd_new_allocation (hp->dynamic_data_of (soh_gen0)) -= soh_withheld_budget;
-                        dd_new_allocation (hp->dynamic_data_of (loh_generation)) -= loh_withheld_budget;
-                    }
-                    current_no_gc_region_info.soh_withheld_budget = soh_withheld_budget;
-                    current_no_gc_region_info.loh_withheld_budget = loh_withheld_budget;
-                    current_no_gc_region_info.callback = callback;
-                }
-            }
-            else
-            {
-                status = enable_no_gc_region_callback_status::insufficient_budget;
-            }
-        }
-    }
-    restart_EE();
-    return status;
-}
-BOOL IsValidObject99(uint8_t *pObject)
-{
-#ifdef VERIFY_HEAP
-    if (!((CObjectHeader*)pObject)->IsFree())
-        ((CObjectHeader *) pObject)->Validate();
-#endif //VERIFY_HEAP
-    return(TRUE);
-}
-#ifdef BACKGROUND_GC
-BOOL gc_heap::bgc_mark_array_range (heap_segment* seg,
-                                    BOOL whole_seg_p,
-                                    uint8_t** range_beg,
-                                    uint8_t** range_end)
-{
-    uint8_t* seg_start = heap_segment_mem (seg);
-    uint8_t* seg_end = (whole_seg_p ? heap_segment_reserved (seg) : align_on_mark_word (heap_segment_allocated (seg)));
-    if ((seg_start < background_saved_highest_address) &&
-        (seg_end > background_saved_lowest_address))
-    {
-        *range_beg = max (seg_start, background_saved_lowest_address);
-        *range_end = min (seg_end, background_saved_highest_address);
-        return TRUE;
-    }
-    else
-    {
-        return FALSE;
-    }
-}
-void gc_heap::bgc_verify_mark_array_cleared (heap_segment* seg, bool always_verify_p)
-{
-#ifdef _DEBUG
-    if (gc_heap::background_running_p() || always_verify_p)
-    {
-        uint8_t* range_beg = 0;
-        uint8_t* range_end = 0;
-        if (bgc_mark_array_range (seg, TRUE, &range_beg, &range_end) || always_verify_p)
-        {
-            if (always_verify_p)
-            {
-                range_beg = heap_segment_mem (seg);
-                range_end = heap_segment_reserved (seg);
-            }
-            size_t  markw = mark_word_of (range_beg);
-            size_t  markw_end = mark_word_of (range_end);
-            while (markw < markw_end)
-            {
-                if (mark_array [markw])
-                {
-                    dprintf (1, ("The mark bits at 0x%zx:0x%u(addr: 0x%p) were not cleared",
-                                    markw, mark_array [markw], mark_word_address (markw)));
-                    FATAL_GC_ERROR();
-                }
-                markw++;
-            }
-            uint8_t* p = mark_word_address (markw_end);
-            while (p < range_end)
-            {
-                assert (!(mark_array_marked (p)));
-                p++;
-            }
-        }
-    }
-#endif //_DEBUG
-}
-void gc_heap::verify_mark_bits_cleared (uint8_t* obj, size_t s)
-{
-#ifdef VERIFY_HEAP
-    size_t start_mark_bit = mark_bit_of (obj) + 1;
-    size_t end_mark_bit = mark_bit_of (obj + s);
-    unsigned int startbit = mark_bit_bit (start_mark_bit);
-    unsigned int endbit = mark_bit_bit (end_mark_bit);
-    size_t startwrd = mark_bit_word (start_mark_bit);
-    size_t endwrd = mark_bit_word (end_mark_bit);
-    unsigned int result = 0;
-    unsigned int firstwrd = ~(lowbits (~0, startbit));
-    unsigned int lastwrd = ~(highbits (~0, endbit));
-    if (startwrd == endwrd)
-    {
-        unsigned int wrd = firstwrd & lastwrd;
-        result = mark_array[startwrd] & wrd;
-        if (result)
-        {
-            FATAL_GC_ERROR();
-        }
-        return;
-    }
-    if (startbit)
-    {
-        result = mark_array[startwrd] & firstwrd;
-        if (result)
-        {
-            FATAL_GC_ERROR();
-        }
-        startwrd++;
-    }
-    for (size_t wrdtmp = startwrd; wrdtmp < endwrd; wrdtmp++)
-    {
-        result = mark_array[wrdtmp];
-        if (result)
-        {
-            FATAL_GC_ERROR();
-        }
-    }
-    if (endbit)
-    {
-        result = mark_array[endwrd] & lastwrd;
-        if (result)
-        {
-            FATAL_GC_ERROR();
-        }
-    }
-#endif //VERIFY_HEAP
-}
-void gc_heap::clear_all_mark_array()
-{
-    for (int i = get_start_generation_index(); i < total_generation_count; i++)
-    {
-        generation* gen = generation_of (i);
-        heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-        while (seg)
-        {
-            uint8_t* range_beg = 0;
-            uint8_t* range_end = 0;
-            if (bgc_mark_array_range (seg, (seg == ephemeral_heap_segment), &range_beg, &range_end))
-            {
-                size_t markw = mark_word_of (range_beg);
-                size_t markw_end = mark_word_of (range_end);
-                size_t size_total = (markw_end - markw) * sizeof (uint32_t);
-                size_t size = 0;
-                size_t size_left = 0;
-                assert (((size_t)&mark_array[markw] & (sizeof(PTR_PTR)-1)) == 0);
-                if ((size_total & (sizeof(PTR_PTR) - 1)) != 0)
-                {
-                    size = (size_total & ~(sizeof(PTR_PTR) - 1));
-                    size_left = size_total - size;
-                    assert ((size_left & (sizeof (uint32_t) - 1)) == 0);
-                }
-                else
-                {
-                    size = size_total;
-                }
-                memclr ((uint8_t*)&mark_array[markw], size);
-                if (size_left != 0)
-                {
-                    uint32_t* markw_to_clear = &mark_array[markw + size / sizeof (uint32_t)];
-                    for (size_t i = 0; i < (size_left / sizeof (uint32_t)); i++)
-                    {
-                        *markw_to_clear = 0;
-                        markw_to_clear++;
-                    }
-                }
-            }
-            seg = heap_segment_next_rw (seg);
-        }
-    }
-}
-void gc_heap::verify_mark_array_cleared()
-{
-#ifdef VERIFY_HEAP
-    if (gc_heap::background_running_p() &&
-        (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC))
-    {
-        for (int i = get_start_generation_index(); i < total_generation_count; i++)
-        {
-            generation* gen = generation_of (i);
-            heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-            while (seg)
-            {
-                bgc_verify_mark_array_cleared (seg);
-                seg = heap_segment_next_rw (seg);
-            }
-        }
-    }
-#endif //VERIFY_HEAP
-}
-#endif //BACKGROUND_GC
-void gc_heap::verify_soh_segment_list()
-{
-#ifdef VERIFY_HEAP
-    if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
-    {
-        for (int i = get_start_generation_index(); i <= max_generation; i++)
-        {
-            generation* gen = generation_of (i);
-            heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-            heap_segment* last_seg = 0;
-            while (seg)
-            {
-                last_seg = seg;
-                seg = heap_segment_next_rw (seg);
-            }
-#ifdef USE_REGIONS
-            if (last_seg != generation_tail_region (gen))
-#else
-            if (last_seg != ephemeral_heap_segment)
-#endif //USE_REGIONS
-            {
-                FATAL_GC_ERROR();
-            }
-        }
-    }
-#endif //VERIFY_HEAP
-}
-#ifdef BACKGROUND_GC
-void gc_heap::verify_partial()
-{
-    BOOL mark_missed_p = FALSE;
-    BOOL bad_ref_p = FALSE;
-    BOOL free_ref_p = FALSE;
-    for (int i = get_start_generation_index(); i < total_generation_count; i++)
-    {
-        generation* gen = generation_of (i);
-        int align_const = get_alignment_constant (i == max_generation);
-        heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-        while (seg)
-        {
-            uint8_t* o = heap_segment_mem (seg);
-            uint8_t* end = heap_segment_allocated (seg);
-            while (o < end)
-            {
-                size_t s = size (o);
-                BOOL marked_p = background_object_marked (o, FALSE);
-                if (marked_p)
-                {
-                    go_through_object_cl (method_table (o), o, s, oo,
-                        {
-                            if (*oo)
-                            {
-                                MethodTable *pMT = method_table (*oo);
-                                if (pMT == g_gc_pFreeObjectMethodTable)
-                                {
-                                    free_ref_p = TRUE;
-                                    FATAL_GC_ERROR();
-                                }
-                                if (!pMT->SanityCheck())
-                                {
-                                    bad_ref_p = TRUE;
-                                    dprintf (1, ("Bad member of %zx %zx",
-                                                (size_t)oo, (size_t)*oo));
-                                    FATAL_GC_ERROR();
-                                }
-                                if (current_bgc_state == bgc_final_marking)
-                                {
-                                    if (marked_p && !background_object_marked (*oo, FALSE))
-                                    {
-                                        mark_missed_p = TRUE;
-                                        FATAL_GC_ERROR();
-                                    }
-                                }
-                            }
-                        }
-                    );
-                }
-                o = o + Align(s, align_const);
-            }
-            seg = heap_segment_next_rw (seg);
-        }
-    }
-}
-#endif //BACKGROUND_GC
-#ifdef VERIFY_HEAP
-void
-gc_heap::verify_free_lists ()
-{
-    for (int gen_num = 0; gen_num < total_generation_count; gen_num++)
-    {
-        dprintf (3, ("Verifying free list for gen:%d", gen_num));
-        allocator* gen_alloc = generation_allocator (generation_of (gen_num));
-        size_t sz = gen_alloc->first_bucket_size();
-        bool verify_undo_slot = (gen_num != 0) && (gen_num <= max_generation) && !gen_alloc->discard_if_no_fit_p();
-        for (unsigned int a_l_number = 0; a_l_number < gen_alloc->number_of_buckets(); a_l_number++)
-        {
-            uint8_t* free_list = gen_alloc->alloc_list_head_of (a_l_number);
-            uint8_t* prev = 0;
-            while (free_list)
-            {
-                if (!((CObjectHeader*)free_list)->IsFree())
-                {
-                    dprintf (1, ("Verifiying Heap: curr free list item %zx isn't a free object)",
-                                 (size_t)free_list));
-                    FATAL_GC_ERROR();
-                }
-                if (((a_l_number < (gen_alloc->number_of_buckets()-1))&& (unused_array_size (free_list) >= sz))
-                    || ((a_l_number != 0) && (unused_array_size (free_list) < sz/2)))
-                {
-                    dprintf (1, ("Verifiying Heap: curr free list item %zx isn't in the right bucket",
-                                 (size_t)free_list));
-                    FATAL_GC_ERROR();
-                }
-                if (verify_undo_slot && (free_list_undo (free_list) != UNDO_EMPTY))
-                {
-                    dprintf (1, ("Verifiying Heap: curr free list item %zx has non empty undo slot",
-                                 (size_t)free_list));
-                    FATAL_GC_ERROR();
-                }
-                if ((gen_num <= max_generation) && (object_gennum (free_list)!= gen_num))
-                {
-                    dprintf (1, ("Verifiying Heap: curr free list item %zx is in the wrong generation free list",
-                                 (size_t)free_list));
-                    FATAL_GC_ERROR();
-                }
-#ifdef DOUBLY_LINKED_FL
-                uint8_t* prev_free_item = free_list_prev (free_list);
-                if (gen_num == max_generation)
-                {
-                    if (prev_free_item != prev)
-                    {
-                        dprintf (1, ("%p prev should be: %p, actual: %p", free_list, prev_free_item, prev));
-                        FATAL_GC_ERROR();
-                    }
-                }
-#endif //DOUBLY_LINKED_FL
-#if defined(USE_REGIONS) && defined(MULTIPLE_HEAPS)
-                heap_segment* region = region_of (free_list);
-                if (region->heap != this)
-                {
-                    dprintf (1, ("curr free item %p should be on heap %d, but actually is on heap %d: %d", free_list, this->heap_number, region->heap->heap_number));
-                    FATAL_GC_ERROR();
-                }
-#endif //USE_REGIONS && MULTIPLE_HEAPS
-                prev = free_list;
-                free_list = free_list_slot (free_list);
-            }
-            uint8_t* tail = gen_alloc->alloc_list_tail_of (a_l_number);
-            if (!((tail == 0) || (tail == prev)))
-            {
-                dprintf (1, ("Verifying Heap: tail of free list is not correct, tail %p, prev %p", tail, prev));
-                FATAL_GC_ERROR();
-            }
-            if (tail == 0)
-            {
-                uint8_t* head = gen_alloc->alloc_list_head_of (a_l_number);
-                if ((head != 0) && (free_list_slot (head) != 0))
-                {
-                    dprintf (1, ("Verifying Heap: head of free list is not correct, head %p -> %p",
-                        head, free_list_slot (head)));
-                    FATAL_GC_ERROR();
-                }
-            }
-            sz *=2;
-        }
-    }
-}
-void gc_heap::verify_regions (int gen_number, bool can_verify_gen_num, bool can_verify_tail, size_t* p_total_committed)
-{
-#ifdef USE_REGIONS
-    generation* gen = generation_of (gen_number);
-    int num_regions_in_gen = 0;
-    heap_segment* seg_in_gen = heap_segment_rw (generation_start_segment (gen));
-    heap_segment* prev_region_in_gen = 0;
-    heap_segment* tail_region = generation_tail_region (gen);
-    while (seg_in_gen)
-    {
-        if (p_total_committed)
-        {
-            if (!heap_segment_read_only_p (seg_in_gen))
-            {
-                *p_total_committed += (heap_segment_committed (seg_in_gen) - get_region_start (seg_in_gen));
-            }
-        }
-        if (can_verify_gen_num)
-        {
-            if (heap_segment_gen_num (seg_in_gen) != min (gen_number, max_generation))
-            {
-                dprintf (REGIONS_LOG, ("h%d gen%d region %p(%p) gen is %d!",
-                    heap_number, gen_number, seg_in_gen, heap_segment_mem (seg_in_gen),
-                    heap_segment_gen_num (seg_in_gen)));
-                FATAL_GC_ERROR();
-            }
-            if (heap_segment_gen_num (seg_in_gen) != heap_segment_plan_gen_num (seg_in_gen))
-            {
-                dprintf (REGIONS_LOG, ("h%d gen%d region %p(%p) gen is %d but plan gen is %d!!",
-                    heap_number, gen_number, seg_in_gen, heap_segment_mem (seg_in_gen),
-                    heap_segment_gen_num (seg_in_gen), heap_segment_plan_gen_num (seg_in_gen)));
-                FATAL_GC_ERROR();
-            }
-        }
-        if (heap_segment_allocated (seg_in_gen) > heap_segment_reserved (seg_in_gen))
-        {
-            dprintf (REGIONS_LOG, ("h%d gen%d region %p alloc %p > reserved %p!!",
-                heap_number, gen_number, heap_segment_mem (seg_in_gen),
-                heap_segment_allocated (seg_in_gen), heap_segment_reserved (seg_in_gen)));
-            FATAL_GC_ERROR();
-        }
-        prev_region_in_gen = seg_in_gen;
-        num_regions_in_gen++;
-        heap_segment* next_region = heap_segment_next (seg_in_gen);
-        if (seg_in_gen == next_region)
-        {
-            dprintf (REGIONS_LOG, ("h%d gen%d region %p(%p) pointing to itself!!",
-                heap_number, gen_number, seg_in_gen, heap_segment_mem (seg_in_gen)));
-            FATAL_GC_ERROR();
-        }
-        seg_in_gen = next_region;
-    }
-    if (num_regions_in_gen == 0)
-    {
-        dprintf (REGIONS_LOG, ("h%d gen%d has no regions!!", heap_number, gen_number));
-        FATAL_GC_ERROR();
-    }
-    if (can_verify_tail && (tail_region != prev_region_in_gen))
-    {
-        dprintf (REGIONS_LOG, ("h%d gen%d tail region is %p(%p), diff from last region %p(%p)!!",
-            heap_number, gen_number,
-            tail_region, heap_segment_mem (tail_region),
-            prev_region_in_gen, heap_segment_mem (prev_region_in_gen)));
-        FATAL_GC_ERROR();
-    }
-#endif //USE_REGIONS
-}
-inline bool is_user_alloc_gen (int gen_number)
-{
-    return ((gen_number == soh_gen0) || (gen_number == loh_generation) || (gen_number == poh_generation));
-}
-void gc_heap::verify_regions (bool can_verify_gen_num, bool concurrent_p)
-{
-#ifdef USE_REGIONS
-    size_t total_committed = 0;
-    for (int i = 0; i < total_generation_count; i++)
-    {
-        bool can_verify_tail = (concurrent_p ? !is_user_alloc_gen (i) : true);
-        verify_regions (i, can_verify_gen_num, can_verify_tail, &total_committed);
-        if (can_verify_gen_num &&
-            can_verify_tail &&
-            (i >= max_generation) &&
-            heap_hard_limit)
-        {
-            int oh = i - max_generation;
-#ifdef BACKGROUND_GC
-            if (oh == soh)
-            {
-                heap_segment* freeable = freeable_soh_segment;
-                while (freeable)
-                {
-                    total_committed += (heap_segment_committed (freeable) - get_region_start (freeable));
-                    freeable = heap_segment_next (freeable);
-                }
-            }
-            else
-            {
-                heap_segment* freeable = freeable_uoh_segment;
-                while (freeable)
-                {
-                    if (heap_segment_oh (freeable) == oh)
-                    {
-                        total_committed += (heap_segment_committed (freeable) - get_region_start (freeable));
-                    }
-                    freeable = heap_segment_next (freeable);
-                }
-            }
-#endif //BACKGROUND_GC
-#ifdef MULTIPLE_HEAPS
-#ifdef _DEBUG
-            size_t total_accounted = committed_by_oh_per_heap[i - max_generation];
-#else // _DEBUG
-            size_t total_accounted = total_committed;
-#endif // _DEBUG
-#else // MULTIPLE_HEAPS
-            size_t total_accounted = committed_by_oh[i - max_generation];
-#endif // MULTIPLE_HEAPS
-            if (total_committed != total_accounted)
-            {
-                FATAL_GC_ERROR();
-            }
-            dprintf(3, ("commit-accounting:  checkpoint for %d for heap %d", oh, heap_number));
-            total_committed = 0;
-        }
-    }
-#endif //USE_REGIONS
-}
-BOOL gc_heap::check_need_card (uint8_t* child_obj, int gen_num_for_cards,
-                               uint8_t* low, uint8_t* high)
-{
-#ifdef USE_REGIONS
-    return (is_in_heap_range (child_obj) && (get_region_gen_num (child_obj) < gen_num_for_cards));
-#else
-    return ((child_obj < high) && (child_obj >= low));
-#endif //USE_REGIONS
-}
-void gc_heap::enter_gc_lock_for_verify_heap()
-{
-#ifdef VERIFY_HEAP
-    if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
-    {
-        enter_spin_lock (&gc_heap::gc_lock);
-        dprintf (SPINLOCK_LOG, ("enter gc_lock for verify_heap"));
-    }
-#endif // VERIFY_HEAP
-}
-void gc_heap::leave_gc_lock_for_verify_heap()
-{
-#ifdef VERIFY_HEAP
-    if (GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_GC)
-    {
-        dprintf (SPINLOCK_LOG, ("leave gc_lock taken for verify_heap"));
-        leave_spin_lock (&gc_heap::gc_lock);
-    }
-#endif // VERIFY_HEAP
-}
-void gc_heap::verify_heap (BOOL begin_gc_p)
-{
-    int heap_verify_level = static_cast<int>(GCConfig::GetHeapVerifyLevel());
-#ifdef MULTIPLE_HEAPS
-    t_join* current_join = &gc_t_join;
-#ifdef BACKGROUND_GC
-    if (settings.concurrent && (bgc_thread_id.IsCurrentThread()))
-    {
-        current_join = &bgc_t_join;
-    }
-#endif //BACKGROUND_GC
-#endif //MULTIPLE_HEAPS
-#ifndef TRACE_GC
-    UNREFERENCED_PARAMETER(begin_gc_p);
-#endif //!TRACE_GC
-#ifdef BACKGROUND_GC
-    dprintf (2,("[%s]GC#%zu(%s): Verifying heap - begin",
-        (begin_gc_p ? "BEG" : "END"),
-        VolatileLoad(&settings.gc_index),
-        (settings.concurrent ? "BGC" : (gc_heap::background_running_p() ? "FGC" : "NGC"))));
-#else
-    dprintf (2,("[%s]GC#%zu: Verifying heap - begin",
-                (begin_gc_p ? "BEG" : "END"), VolatileLoad(&settings.gc_index)));
-#endif //BACKGROUND_GC
-#ifndef MULTIPLE_HEAPS
-#ifndef USE_REGIONS
-    if ((ephemeral_low != generation_allocation_start (generation_of (max_generation - 1))) ||
-        (ephemeral_high != heap_segment_reserved (ephemeral_heap_segment)))
-    {
-        FATAL_GC_ERROR();
-    }
-#endif //!USE_REGIONS
-#endif //MULTIPLE_HEAPS
-#ifdef BACKGROUND_GC
-    if (!settings.concurrent)
-#endif //BACKGROUND_GC
-    {
-        if (!(heap_verify_level & GCConfig::HEAPVERIFY_NO_MEM_FILL))
-        {
-            for (int i = get_start_generation_index(); i < total_generation_count; i++)
-            {
-                generation* gen1 = generation_of (i);
-                heap_segment* seg1 = heap_segment_rw (generation_start_segment (gen1));
-                while (seg1)
-                {
-                    uint8_t* clear_start = heap_segment_allocated (seg1) - plug_skew;
-                    if (heap_segment_used (seg1) > clear_start)
-                    {
-                        dprintf (3, ("setting end of seg %p: [%p-[%p to 0xaa",
-                            heap_segment_mem (seg1),
-                            clear_start ,
-                            heap_segment_used (seg1)));
-                        memset (heap_segment_allocated (seg1) - plug_skew, 0xaa,
-                            (heap_segment_used (seg1) - clear_start));
-                    }
-                    seg1 = heap_segment_next_rw (seg1);
-                }
-            }
-        }
-    }
-#ifdef MULTIPLE_HEAPS
-    current_join->join(this, gc_join_verify_copy_table);
-    if (current_join->joined())
-    {
-        for (int i = 0; i < n_heaps; i++)
-        {
-            if (g_gc_card_table != g_heaps[i]->card_table)
-            {
-                g_heaps[i]->copy_brick_card_table();
-            }
-        }
-        current_join->restart();
-    }
-#else
-        if (g_gc_card_table != card_table)
-            copy_brick_card_table();
-#endif //MULTIPLE_HEAPS
-    {
-#ifdef USE_REGIONS
-        verify_regions (true, settings.concurrent);
-#else //USE_REGIONS
-        generation* gen = generation_of (max_generation);
-        assert (generation_allocation_start (gen) ==
-                heap_segment_mem (heap_segment_rw (generation_start_segment (gen))));
-        int gen_num = max_generation-1;
-        generation* prev_gen = gen;
-        while (gen_num >= 0)
-        {
-            gen = generation_of (gen_num);
-            assert (generation_allocation_segment (gen) == ephemeral_heap_segment);
-            assert (generation_allocation_start (gen) >= heap_segment_mem (ephemeral_heap_segment));
-            assert (generation_allocation_start (gen) < heap_segment_allocated (ephemeral_heap_segment));
-            if (generation_start_segment (prev_gen ) ==
-                generation_start_segment (gen))
-            {
-                assert (generation_allocation_start (prev_gen) <
-                        generation_allocation_start (gen));
-            }
-            prev_gen = gen;
-            gen_num--;
-        }
-#endif //USE_REGIONS
-    }
-    size_t          total_objects_verified = 0;
-    size_t          total_objects_verified_deep = 0;
-    BOOL            bCurrentBrickInvalid = FALSE;
-    size_t          last_valid_brick = 0;
-    size_t          curr_brick = 0;
-    size_t          prev_brick = (size_t)-1;
-    int             gen_num_for_cards = 0;
-#ifdef USE_REGIONS
-    int             gen_num_to_stop = 0;
-    uint8_t*        e_high = 0;
-    uint8_t*        next_boundary = 0;
-#else //USE_REGIONS
-    int gen_num_to_stop = max_generation;
-    uint8_t*        e_high = ephemeral_high;
-    uint8_t*        next_boundary = generation_allocation_start (generation_of (max_generation - 1));
-    uint8_t*        begin_youngest = generation_allocation_start(generation_of(0));
-#endif //!USE_REGIONS
-    for (int curr_gen_num = total_generation_count - 1; curr_gen_num >= gen_num_to_stop; curr_gen_num--)
-    {
-        int             align_const = get_alignment_constant (curr_gen_num == max_generation);
-        BOOL            large_brick_p = (curr_gen_num > max_generation);
-#ifdef USE_REGIONS
-        gen_num_for_cards = ((curr_gen_num >= max_generation) ? max_generation : curr_gen_num);
-#endif //USE_REGIONS
-        heap_segment*   seg = heap_segment_in_range (generation_start_segment (generation_of (curr_gen_num) ));
-        while (seg)
-        {
-            uint8_t*        curr_object = heap_segment_mem (seg);
-            uint8_t*        prev_object = 0;
-            bool verify_bricks_p = true;
-#ifdef USE_REGIONS
-            if (heap_segment_read_only_p(seg))
-            {
-                dprintf(1, ("seg %zx is ro! Shouldn't happen with regions", (size_t)seg));
-                FATAL_GC_ERROR();
-            }
-            if (heap_segment_gen_num (seg) != heap_segment_plan_gen_num (seg))
-            {
-                dprintf (1, ("Seg %p, gen num is %d, plan gen num is %d",
-                    heap_segment_mem (seg), heap_segment_gen_num (seg), heap_segment_plan_gen_num (seg)));
-                FATAL_GC_ERROR();
-            }
-#else //USE_REGIONS
-            if (heap_segment_read_only_p(seg))
-            {
-                size_t current_brick = brick_of(max(heap_segment_mem(seg), lowest_address));
-                size_t end_brick = brick_of(min(heap_segment_reserved(seg), highest_address) - 1);
-                while (current_brick <= end_brick)
-                {
-                    if (brick_table[current_brick] != 0)
-                    {
-                        dprintf(1, ("Verifying Heap: %zx brick of a frozen segment is not zeroed", current_brick));
-                        FATAL_GC_ERROR();
-                    }
-                    current_brick++;
-                }
-                verify_bricks_p = false;
-            }
-#endif //USE_REGIONS
-#ifdef BACKGROUND_GC
-            BOOL consider_bgc_mark_p    = FALSE;
-            BOOL check_current_sweep_p  = FALSE;
-            BOOL check_saved_sweep_p    = FALSE;
-            should_check_bgc_mark (seg, &consider_bgc_mark_p, &check_current_sweep_p, &check_saved_sweep_p);
-#endif //BACKGROUND_GC
-            while (curr_object < heap_segment_allocated (seg))
-            {
-                if (is_mark_set (curr_object))
-                {
-                    dprintf (1, ("curr_object: %zx is marked!",(size_t)curr_object));
-                    FATAL_GC_ERROR();
-                }
-                size_t s = size (curr_object);
-                dprintf (3, ("o: %zx, s: %zu", (size_t)curr_object, s));
-                if (s == 0)
-                {
-                    dprintf (1, ("Verifying Heap: size of current object %p == 0", curr_object));
-                    FATAL_GC_ERROR();
-                }
-#ifndef USE_REGIONS
-                if (seg == ephemeral_heap_segment)
-                {
-                    if ((curr_gen_num > 0) && (curr_object >= next_boundary))
-                    {
-                        curr_gen_num--;
-                        if (curr_gen_num > 0)
-                        {
-                            next_boundary = generation_allocation_start (generation_of (curr_gen_num - 1));
-                        }
-                    }
-                }
-#endif //!USE_REGIONS
-#ifdef USE_REGIONS
-                if (verify_bricks_p && curr_gen_num != 0)
-#else
-                if (verify_bricks_p && ((seg != ephemeral_heap_segment) ||
-                     (brick_of(curr_object) < brick_of(begin_youngest))))
-#endif //USE_REGIONS
-                {
-                    curr_brick = brick_of(curr_object);
-                    if (curr_brick != prev_brick)
-                    {
-                        if (bCurrentBrickInvalid &&
-                            (curr_brick != brick_of (heap_segment_mem (seg))) &&
-                            !heap_segment_read_only_p (seg))
-                        {
-                            dprintf (1, ("curr brick %zx invalid", curr_brick));
-                            FATAL_GC_ERROR();
-                        }
-                        if (large_brick_p)
-                        {
-                            if ((heap_segment_reserved (seg) <= highest_address) &&
-                                (heap_segment_mem (seg) >= lowest_address) &&
-                                brick_table [curr_brick] != 0)
-                            {
-                                dprintf (1, ("curr_brick %zx for large object %zx is set to %zx",
-                                    curr_brick, (size_t)curr_object, (size_t)brick_table[curr_brick]));
-                                FATAL_GC_ERROR();
-                            }
-                            else
-                            {
-                                bCurrentBrickInvalid = FALSE;
-                            }
-                        }
-                        else
-                        {
-                            if (brick_table [curr_brick] <= 0)
-                            {
-                                if (brick_table [curr_brick] == 0)
-                                {
-                                    dprintf(1, ("curr_brick %zx for object %zx set to 0",
-                                            curr_brick, (size_t)curr_object));
-                                    FATAL_GC_ERROR();
-                                }
-                                ptrdiff_t i = curr_brick;
-                                while ((i >= ((ptrdiff_t) brick_of (heap_segment_mem (seg)))) &&
-                                       (brick_table[i] < 0))
-                                {
-                                    i = i + brick_table[i];
-                                }
-                                if (i <  ((ptrdiff_t)(brick_of (heap_segment_mem (seg))) - 1))
-                                {
-                                    dprintf (1, ("ptrdiff i: %zx < brick_of (heap_segment_mem (seg)):%zx - 1. curr_brick: %zx",
-                                            i, brick_of (heap_segment_mem (seg)),
-                                            curr_brick));
-                                    FATAL_GC_ERROR();
-                                }
-                                bCurrentBrickInvalid = FALSE;
-                            }
-                            else if (!heap_segment_read_only_p (seg))
-                            {
-                                bCurrentBrickInvalid = TRUE;
-                            }
-                        }
-                    }
-                    if (bCurrentBrickInvalid)
-                    {
-                        if (curr_object == (brick_address(curr_brick) + brick_table[curr_brick] - 1))
-                        {
-                            bCurrentBrickInvalid = FALSE;
-                            last_valid_brick = curr_brick;
-                        }
-                    }
-                }
-                if (*((uint8_t**)curr_object) != (uint8_t *) g_gc_pFreeObjectMethodTable)
-                {
-#ifdef FEATURE_LOH_COMPACTION
-                    if ((curr_gen_num == loh_generation) && (prev_object != 0))
-                    {
-                        assert (method_table (prev_object) == g_gc_pFreeObjectMethodTable);
-                    }
-#endif //FEATURE_LOH_COMPACTION
-                    total_objects_verified++;
-                    BOOL can_verify_deep = TRUE;
-#ifdef BACKGROUND_GC
-                    can_verify_deep = fgc_should_consider_object (curr_object, seg, consider_bgc_mark_p, check_current_sweep_p, check_saved_sweep_p);
-#endif //BACKGROUND_GC
-                    BOOL deep_verify_obj = can_verify_deep;
-                    if ((heap_verify_level & GCConfig::HEAPVERIFY_DEEP_ON_COMPACT) && !settings.compaction)
-                        deep_verify_obj = FALSE;
-                    ((CObjectHeader*)curr_object)->ValidateHeap(deep_verify_obj);
-                    if (can_verify_deep)
-                    {
-                        if (curr_gen_num > 0)
-                        {
-                            BOOL need_card_p = FALSE;
-                            if (contain_pointers_or_collectible (curr_object))
-                            {
-                                dprintf (4, ("curr_object: %zx", (size_t)curr_object));
-                                size_t crd = card_of (curr_object);
-                                BOOL found_card_p = card_set_p (crd);
-#ifdef COLLECTIBLE_CLASS
-                                if (is_collectible(curr_object))
-                                {
-                                    uint8_t* class_obj = get_class_object (curr_object);
-                                    if (check_need_card (class_obj, gen_num_for_cards, next_boundary, e_high))
-                                    {
-                                        if (!found_card_p)
-                                        {
-                                            dprintf (1, ("Card not set, curr_object = [%zx:%zx pointing to class object %p",
-                                                        card_of (curr_object), (size_t)curr_object, class_obj));
-                                            FATAL_GC_ERROR();
-                                        }
-                                    }
-                                }
-#endif //COLLECTIBLE_CLASS
-                                if (contain_pointers(curr_object))
-                                {
-                                    go_through_object_nostart
-                                        (method_table(curr_object), curr_object, s, oo,
-                                        {
-                                            if (crd != card_of ((uint8_t*)oo))
-                                            {
-                                                crd = card_of ((uint8_t*)oo);
-                                                found_card_p = card_set_p (crd);
-                                                need_card_p = FALSE;
-                                            }
-                                            if (*oo && check_need_card (*oo, gen_num_for_cards, next_boundary, e_high))
-                                            {
-                                                need_card_p = TRUE;
-                                            }
-                                            if (need_card_p && !found_card_p)
-                                            {
-                                                dprintf (1, ("Card not set, curr_object = [%zx:%zx, %zx:%zx[",
-                                                            card_of (curr_object), (size_t)curr_object,
-                                                            card_of (curr_object+Align(s, align_const)),
-                                                            (size_t)(curr_object+Align(s, align_const))));
-                                                FATAL_GC_ERROR();
-                                            }
-                                        }
-                                            );
-                                }
-                                if (need_card_p && !found_card_p)
-                                {
-                                    dprintf (1, ("Card not set, curr_object = [%zx:%zx, %zx:%zx[",
-                                        card_of (curr_object), (size_t)curr_object,
-                                        card_of (curr_object + Align(s, align_const)),
-                                        (size_t)(curr_object + Align(s, align_const))));
-                                    FATAL_GC_ERROR();
-                                }
-                            }
-                        }
-                        total_objects_verified_deep++;
-                    }
-                }
-                prev_object = curr_object;
-                prev_brick = curr_brick;
-                curr_object = curr_object + Align(s, align_const);
-                if (curr_object < prev_object)
-                {
-                    dprintf (1, ("overflow because of a bad object size: %p size %zx", prev_object, s));
-                    FATAL_GC_ERROR();
-                }
-            }
-            if (curr_object > heap_segment_allocated(seg))
-            {
-                dprintf (1, ("Verifiying Heap: curr_object: %zx > heap_segment_allocated (seg: %zx) %p",
-                        (size_t)curr_object, (size_t)seg, heap_segment_allocated (seg)));
-                FATAL_GC_ERROR();
-            }
-            seg = heap_segment_next_in_range (seg);
-        }
-    }
-#ifdef BACKGROUND_GC
-    dprintf (2, ("(%s)(%s)(%s) total_objects_verified is %zd, total_objects_verified_deep is %zd",
-                 (settings.concurrent ? "BGC" : (gc_heap::background_running_p () ? "FGC" : "NGC")),
-                 (begin_gc_p ? "BEG" : "END"),
-                 ((current_c_gc_state == c_gc_state_planning) ? "in plan" : "not in plan"),
-                 total_objects_verified, total_objects_verified_deep));
-    if (current_c_gc_state != c_gc_state_planning)
-    {
-        assert (total_objects_verified == total_objects_verified_deep);
-    }
-#endif //BACKGROUND_GC
-    verify_free_lists();
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-    finalize_queue->CheckFinalizerObjects();
-#endif // FEATURE_PREMORTEM_FINALIZATION
-    {
-        ScanContext sc;
-        sc.thread_number = heap_number;
-        sc.thread_count = n_heaps;
-        GCScan::VerifyHandleTable(max_generation, max_generation, &sc);
-    }
-#ifdef MULTIPLE_HEAPS
-    current_join->join(this, gc_join_verify_objects_done);
-    if (current_join->joined())
-#endif //MULTIPLE_HEAPS
-    {
-        GCToEEInterface::VerifySyncTableEntry();
-#ifdef MULTIPLE_HEAPS
-#ifdef USE_REGIONS
-        for (int hn = n_heaps; hn < n_max_heaps; hn++)
-        {
-            gc_heap* hp = g_heaps[hn];
-            hp->check_decommissioned_heap();
-        }
-#endif //USE_REGIONS
-        current_join->restart();
-#endif //MULTIPLE_HEAPS
-    }
-#ifdef BACKGROUND_GC
-    if (settings.concurrent)
-    {
-        verify_mark_array_cleared();
-    }
-    dprintf (2,("GC%zu(%s): Verifying heap - end",
-        VolatileLoad(&settings.gc_index),
-        (settings.concurrent ? "BGC" : (gc_heap::background_running_p() ? "FGC" : "NGC"))));
-#else
-    dprintf (2,("GC#d: Verifying heap - end", VolatileLoad(&settings.gc_index)));
-#endif //BACKGROUND_GC
-}
-#endif  //VERIFY_HEAP
-void GCHeap::ValidateObjectMember (Object* obj)
-{
-#ifdef VERIFY_HEAP
-    size_t s = size (obj);
-    uint8_t* o = (uint8_t*)obj;
-    go_through_object_cl (method_table (obj), o, s, oo,
-        {
-            uint8_t* child_o = *oo;
-            if (child_o)
-            {
-                MethodTable *pMT = method_table (child_o);
-                assert(pMT);
-                if (!pMT->SanityCheck()) {
-                    dprintf (1, ("Bad member of %zx %zx",
-                                (size_t)oo, (size_t)child_o));
-                    FATAL_GC_ERROR();
-                }
-            }
-        } );
-#endif // VERIFY_HEAP
-}
-HRESULT GCHeap::StaticShutdown()
-{
-    deleteGCShadow();
-    GCScan::GcRuntimeStructuresValid (FALSE);
-    uint32_t* ct = &g_gc_card_table[card_word (gcard_of (g_gc_lowest_address))];
-    if (card_table_refcount (ct) == 0)
-    {
-        destroy_card_table (ct);
-        g_gc_card_table = nullptr;
-#ifdef FEATURE_MANUALLY_MANAGED_CARD_BUNDLES
-        g_gc_card_bundle_table = nullptr;
-#endif
-#ifdef FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-        SoftwareWriteWatch::StaticClose();
-#endif // FEATURE_USE_SOFTWARE_WRITE_WATCH_FOR_GC_HEAP
-    }
-#ifndef USE_REGIONS
-    while(gc_heap::segment_standby_list != 0)
-    {
-        heap_segment* next_seg = heap_segment_next (gc_heap::segment_standby_list);
-#ifdef MULTIPLE_HEAPS
-        (gc_heap::g_heaps[0])->delete_heap_segment (gc_heap::segment_standby_list, FALSE);
-#else //MULTIPLE_HEAPS
-        pGenGCHeap->delete_heap_segment (gc_heap::segment_standby_list, FALSE);
-#endif //MULTIPLE_HEAPS
-        gc_heap::segment_standby_list = next_seg;
-    }
-#endif // USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i ++)
-    {
-        gc_heap::destroy_gc_heap (gc_heap::g_heaps[i]);
-    }
-#else
-    gc_heap::destroy_gc_heap (pGenGCHeap);
-#endif //MULTIPLE_HEAPS
-    gc_heap::shutdown_gc();
-    return S_OK;
-}
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-static
-HRESULT AllocateCFinalize(CFinalize **pCFinalize)
-{
-    *pCFinalize = new (nothrow) CFinalize();
-    if (*pCFinalize == NULL || !(*pCFinalize)->Initialize())
-        return E_OUTOFMEMORY;
-    return S_OK;
-}
-#endif // FEATURE_PREMORTEM_FINALIZATION
-HRESULT GCHeap::Init(size_t hn)
-{
-    HRESULT hres = S_OK;
-#ifdef MULTIPLE_HEAPS
-    if ((pGenGCHeap = gc_heap::make_gc_heap(this, (int)hn)) == 0)
-        hres = E_OUTOFMEMORY;
-#else
-    UNREFERENCED_PARAMETER(hn);
-    if (!gc_heap::make_gc_heap())
-        hres = E_OUTOFMEMORY;
-#endif //MULTIPLE_HEAPS
-    return hres;
-}
-HRESULT GCHeap::Initialize()
-{
-#ifndef TRACE_GC
-    STRESS_LOG_VA (1, (ThreadStressLog::gcLoggingIsOffMsg()));
-#endif
-    HRESULT hr = S_OK;
-    qpf = (uint64_t)GCToOSInterface::QueryPerformanceFrequency();
-    qpf_ms = 1000.0 / (double)qpf;
-    qpf_us = 1000.0 * 1000.0 / (double)qpf;
-    g_gc_pFreeObjectMethodTable = GCToEEInterface::GetFreeObjectMethodTable();
-    g_num_processors = GCToOSInterface::GetTotalProcessorCount();
-    assert(g_num_processors != 0);
-    gc_heap::total_physical_mem = (size_t)GCConfig::GetGCTotalPhysicalMemory();
-    if (gc_heap::total_physical_mem != 0)
-    {
-        gc_heap::is_restricted_physical_mem = true;
-#ifdef FEATURE_EVENT_TRACE
-        gc_heap::physical_memory_from_config = (size_t)gc_heap::total_physical_mem;
-#endif //FEATURE_EVENT_TRACE
-    }
-    else
-    {
-        gc_heap::total_physical_mem = GCToOSInterface::GetPhysicalMemoryLimit (&gc_heap::is_restricted_physical_mem);
-    }
-    memset (gc_heap::committed_by_oh, 0, sizeof (gc_heap::committed_by_oh));
-    if (!gc_heap::compute_hard_limit())
-    {
-        return CLR_E_GC_BAD_HARD_LIMIT;
-    }
-    uint32_t nhp = 1;
-    uint32_t nhp_from_config = 0;
-    uint32_t max_nhp_from_config = (uint32_t)GCConfig::GetMaxHeapCount();
-#ifndef MULTIPLE_HEAPS
-    GCConfig::SetServerGC(false);
-#else //!MULTIPLE_HEAPS
-    GCConfig::SetServerGC(true);
-    AffinitySet config_affinity_set;
-    GCConfigStringHolder cpu_index_ranges_holder(GCConfig::GetGCHeapAffinitizeRanges());
-    uintptr_t config_affinity_mask = static_cast<uintptr_t>(GCConfig::GetGCHeapAffinitizeMask());
-    if (!ParseGCHeapAffinitizeRanges(cpu_index_ranges_holder.Get(), &config_affinity_set, config_affinity_mask))
-    {
-        return CLR_E_GC_BAD_AFFINITY_CONFIG_FORMAT;
-    }
-    const AffinitySet* process_affinity_set = GCToOSInterface::SetGCThreadsAffinitySet(config_affinity_mask, &config_affinity_set);
-    GCConfig::SetGCHeapAffinitizeMask(static_cast<int64_t>(config_affinity_mask));
-    if (process_affinity_set->IsEmpty())
-    {
-        return CLR_E_GC_BAD_AFFINITY_CONFIG;
-    }
-    if ((cpu_index_ranges_holder.Get() != nullptr)
-#ifdef TARGET_WINDOWS
-        || (config_affinity_mask != 0)
-#endif
-    )
-    {
-        affinity_config_specified_p = true;
-    }
-    nhp_from_config = static_cast<uint32_t>(GCConfig::GetHeapCount());
-    g_num_active_processors = min (GCToEEInterface::GetCurrentProcessCpuCount(), g_num_processors);
-    if (nhp_from_config)
-    {
-        nhp_from_config = min (nhp_from_config, g_num_active_processors);
-    }
-    nhp = ((nhp_from_config == 0) ? g_num_active_processors : nhp_from_config);
-    nhp = min (nhp, MAX_SUPPORTED_CPUS);
-    gc_heap::gc_thread_no_affinitize_p = (gc_heap::heap_hard_limit ?
-        !affinity_config_specified_p : (GCConfig::GetNoAffinitize() != 0));
-    if (!(gc_heap::gc_thread_no_affinitize_p))
-    {
-        uint32_t num_affinitized_processors = (uint32_t)process_affinity_set->Count();
-        if (num_affinitized_processors != 0)
-        {
-            nhp = min(nhp, num_affinitized_processors);
-        }
-    }
-#endif //!MULTIPLE_HEAPS
-    if (gc_heap::heap_hard_limit)
-    {
-        gc_heap::hard_limit_config_p = true;
-    }
-    size_t seg_size_from_config = 0;
-    bool compute_memory_settings_succeed = gc_heap::compute_memory_settings(true, nhp, nhp_from_config, seg_size_from_config, 0);
-    assert (compute_memory_settings_succeed);
-    if ((!gc_heap::heap_hard_limit) && gc_heap::use_large_pages_p)
-    {
-        return CLR_E_GC_LARGE_PAGE_MISSING_HARD_LIMIT;
-    }
-    GCConfig::SetGCLargePages(gc_heap::use_large_pages_p);
-#ifdef USE_REGIONS
-    gc_heap::regions_range = (size_t)GCConfig::GetGCRegionRange();
-    if (gc_heap::regions_range == 0)
-    {
-        if (gc_heap::heap_hard_limit)
-        {
-            if (gc_heap::heap_hard_limit_oh[soh])
-            {
-                gc_heap::regions_range = gc_heap::heap_hard_limit;
-            }
-            else
-            {
-                gc_heap::regions_range = ((gc_heap::use_large_pages_p) ? (2 * gc_heap::heap_hard_limit)
-                                                                       : (5 * gc_heap::heap_hard_limit));
-            }
-        }
-        else
-        {
-            gc_heap::regions_range = min(GCToOSInterface::GetVirtualMemoryLimit()/2, max((size_t)256 * 1024 * 1024 * 1024, (size_t)(2 * gc_heap::total_physical_mem)));
-        }
-        gc_heap::regions_range = align_on_page(gc_heap::regions_range);
-    }
-    GCConfig::SetGCRegionRange(gc_heap::regions_range);
-#endif //USE_REGIONS
-    size_t seg_size = 0;
-    size_t large_seg_size = 0;
-    size_t pin_seg_size = 0;
-    seg_size = gc_heap::soh_segment_size;
-#ifndef USE_REGIONS
-    if (gc_heap::heap_hard_limit)
-    {
-        if (gc_heap::heap_hard_limit_oh[soh])
-        {
-            large_seg_size = max (gc_heap::adjust_segment_size_hard_limit (gc_heap::heap_hard_limit_oh[loh], nhp), seg_size_from_config);
-            pin_seg_size = max (gc_heap::adjust_segment_size_hard_limit (gc_heap::heap_hard_limit_oh[poh], nhp), seg_size_from_config);
-        }
-        else
-        {
-            large_seg_size = gc_heap::use_large_pages_p ? gc_heap::soh_segment_size : gc_heap::soh_segment_size * 2;
-            pin_seg_size = large_seg_size;
-        }
-        if (gc_heap::use_large_pages_p)
-            gc_heap::min_segment_size = min_segment_size_hard_limit;
-    }
-    else
-    {
-        large_seg_size = get_valid_segment_size (TRUE);
-        pin_seg_size = large_seg_size;
-    }
-    assert (g_theGCHeap->IsValidSegmentSize (seg_size));
-    assert (g_theGCHeap->IsValidSegmentSize (large_seg_size));
-    assert (g_theGCHeap->IsValidSegmentSize (pin_seg_size));
-    dprintf (1, ("%d heaps, soh seg size: %zd mb, loh: %zd mb\n",
-        nhp,
-        (seg_size / (size_t)1024 / 1024),
-        (large_seg_size / 1024 / 1024)));
-    gc_heap::min_uoh_segment_size = min (large_seg_size, pin_seg_size);
-    if (gc_heap::min_segment_size == 0)
-    {
-        gc_heap::min_segment_size = min (seg_size, gc_heap::min_uoh_segment_size);
-    }
-#endif //!USE_REGIONS
-    GCConfig::SetHeapCount(static_cast<int64_t>(nhp));
-#ifdef USE_REGIONS
-    gc_heap::enable_special_regions_p = (bool)GCConfig::GetGCEnableSpecialRegions();
-    size_t gc_region_size = (size_t)GCConfig::GetGCRegionSize();
-    if (gc_region_size >= MAX_REGION_SIZE)
-    {
-        return CLR_E_GC_BAD_REGION_SIZE;
-    }
-    if (gc_region_size == 0)
-    {
-        size_t max_region_size = gc_heap::regions_range / 2 / nhp / min_regions_per_heap;
-        if (max_region_size >= (4 * 1024 * 1024))
-        {
-            gc_region_size = 4 * 1024 * 1024;
-        }
-        else if (max_region_size >= (2 * 1024 * 1024))
-        {
-            gc_region_size = 2 * 1024 * 1024;
-        }
-        else
-        {
-            gc_region_size = 1 * 1024 * 1024;
-        }
-    }
-    if (!power_of_two_p(gc_region_size) || ((gc_region_size * nhp * min_regions_per_heap) > gc_heap::regions_range))
-    {
-        return E_OUTOFMEMORY;
-    }
-    gc_heap::min_segment_size_shr = index_of_highest_set_bit (gc_region_size);
-#else
-    gc_heap::min_segment_size_shr = index_of_highest_set_bit (gc_heap::min_segment_size);
-#endif //USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-    assert (nhp <= g_num_processors);
-    if (max_nhp_from_config)
-    {
-        nhp = min (nhp, max_nhp_from_config);
-    }
-    gc_heap::n_max_heaps = nhp;
-    gc_heap::n_heaps = nhp;
-    hr = gc_heap::initialize_gc (seg_size, large_seg_size, pin_seg_size, nhp);
-#else
-    hr = gc_heap::initialize_gc (seg_size, large_seg_size, pin_seg_size);
-#endif //MULTIPLE_HEAPS
-    GCConfig::SetGCHeapHardLimit(static_cast<int64_t>(gc_heap::heap_hard_limit));
-    GCConfig::SetGCHeapHardLimitSOH(static_cast<int64_t>(gc_heap::heap_hard_limit_oh[soh]));
-    GCConfig::SetGCHeapHardLimitLOH(static_cast<int64_t>(gc_heap::heap_hard_limit_oh[loh]));
-    GCConfig::SetGCHeapHardLimitPOH(static_cast<int64_t>(gc_heap::heap_hard_limit_oh[poh]));
-    if (hr != S_OK)
-        return hr;
-    gc_heap::pm_stress_on = (GCConfig::GetGCProvModeStress() != 0);
-#if defined(HOST_64BIT)
-    gc_heap::youngest_gen_desired_th = gc_heap::mem_one_percent;
-#endif // HOST_64BIT
-    WaitForGCEvent = new (nothrow) GCEvent;
-    if (!WaitForGCEvent)
-    {
-        return E_OUTOFMEMORY;
-    }
-    if (!WaitForGCEvent->CreateManualEventNoThrow(TRUE))
-    {
-        GCToEEInterface::LogErrorToHost("Creation of WaitForGCEvent failed");
-        return E_FAIL;
-    }
-#ifndef FEATURE_NATIVEAOT // NativeAOT forces relocation a different way
-#if defined (STRESS_HEAP) && !defined (MULTIPLE_HEAPS)
-    if (GCStress<cfg_any>::IsEnabled())
-    {
-        for (int i = 0; i < GCHeap::NUM_HEAP_STRESS_OBJS; i++)
-        {
-            m_StressObjs[i] = CreateGlobalHandle(0);
-        }
-        m_CurStressObj = 0;
-    }
-#endif //STRESS_HEAP && !MULTIPLE_HEAPS
-#endif // FEATURE_NATIVEAOT
-    initGCShadow();         // If we are debugging write barriers, initialize heap shadow
-#ifdef USE_REGIONS
-    gc_heap::ephemeral_low = MAX_PTR;
-    gc_heap::ephemeral_high = nullptr;
-#endif //!USE_REGIONS
-#ifdef MULTIPLE_HEAPS
-    for (uint32_t i = 0; i < nhp; i++)
-    {
-        GCHeap* Hp = new (nothrow) GCHeap();
-        if (!Hp)
-            return E_OUTOFMEMORY;
-        if ((hr = Hp->Init (i))!= S_OK)
-        {
-            return hr;
-        }
-    }
-    heap_select::init_numa_node_to_heap_map (nhp);
-    if (g_num_active_processors > nhp)
-        heap_select::distribute_other_procs();
-    gc_heap* hp = gc_heap::g_heaps[0];
-    dynamic_data* gen0_dd = hp->dynamic_data_of (0);
-    gc_heap::min_gen0_balance_delta = (dd_min_size (gen0_dd) >> 6);
-    bool can_use_cpu_groups = GCToOSInterface::CanEnableGCCPUGroups();
-    GCConfig::SetGCCpuGroup(can_use_cpu_groups);
-#ifdef HEAP_BALANCE_INSTRUMENTATION
-    cpu_group_enabled_p = can_use_cpu_groups;
-    if (!GCToOSInterface::GetNumaInfo (&total_numa_nodes_on_machine, &procs_per_numa_node))
-    {
-        total_numa_nodes_on_machine = 1;
-        if (GCToOSInterface::GetCPUGroupInfo (&total_cpu_groups_on_machine, &procs_per_cpu_group))
-            procs_per_numa_node = procs_per_cpu_group + ((total_cpu_groups_on_machine - 1) << 6);
-        else
-            procs_per_numa_node = g_num_processors;
-    }
-    hb_info_numa_nodes = new (nothrow) heap_balance_info_numa[total_numa_nodes_on_machine];
-    dprintf (HEAP_BALANCE_LOG, ("total: %d, numa: %d", g_num_processors, total_numa_nodes_on_machine));
-    int hb_info_size_per_proc = sizeof (heap_balance_info_proc);
-    for (int numa_node_index = 0; numa_node_index < total_numa_nodes_on_machine; numa_node_index++)
-    {
-        int hb_info_size_per_node = hb_info_size_per_proc * procs_per_numa_node;
-        uint8_t* numa_mem = (uint8_t*)GCToOSInterface::VirtualReserve (hb_info_size_per_node, 0, 0, numa_node_index);
-        if (!numa_mem)
-        {
-            GCToEEInterface::LogErrorToHost("Reservation of numa_mem failed");
-            return E_FAIL;
-        }
-        if (!GCToOSInterface::VirtualCommit (numa_mem, hb_info_size_per_node, numa_node_index))
-        {
-            GCToEEInterface::LogErrorToHost("Commit of numa_mem failed");
-            return E_FAIL;
-        }
-        heap_balance_info_proc* hb_info_procs = (heap_balance_info_proc*)numa_mem;
-        hb_info_numa_nodes[numa_node_index].hb_info_procs = hb_info_procs;
-        for (int proc_index = 0; proc_index < (int)procs_per_numa_node; proc_index++)
-        {
-            heap_balance_info_proc* hb_info_proc = &hb_info_procs[proc_index];
-            hb_info_proc->count = default_max_hb_heap_balance_info;
-            hb_info_proc->index = 0;
-        }
-    }
-#endif //HEAP_BALANCE_INSTRUMENTATION
-#else
-    hr = Init (0);
-#endif //MULTIPLE_HEAPS
-#ifdef USE_REGIONS
-    if (initial_regions)
-    {
-        delete[] initial_regions;
-    }
-#endif //USE_REGIONS
-    if (hr == S_OK)
-    {
-#ifdef DYNAMIC_HEAP_COUNT
-        if (gc_heap::dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes)
-        {
-            gc_heap::smoothed_desired_total[0] /= gc_heap::n_heaps;
-            int initial_n_heaps = 1;
-            dprintf (9999, ("gc_heap::n_heaps is %d, initial %d", gc_heap::n_heaps, initial_n_heaps));
-            {
-                if (!gc_heap::prepare_to_change_heap_count (initial_n_heaps))
-                {
-                    return E_FAIL;
-                }
-                gc_heap::dynamic_heap_count_data.new_n_heaps = initial_n_heaps;
-                gc_heap::dynamic_heap_count_data.idle_thread_count = 0;
-                gc_heap::dynamic_heap_count_data.init_only_p = true;
-                int max_threads_to_wake = max (gc_heap::n_heaps, initial_n_heaps);
-                gc_t_join.update_n_threads (max_threads_to_wake);
-                gc_heap::gc_start_event.Set ();
-            }
-            gc_heap::g_heaps[0]->change_heap_count (initial_n_heaps);
-            gc_heap::gc_start_event.Reset ();
-            gc_heap::dynamic_heap_count_data.last_n_heaps = 0;
-        }
-#endif //DYNAMIC_HEAP_COUNT
-        GCScan::GcRuntimeStructuresValid (TRUE);
-        GCToEEInterface::DiagUpdateGenerationBounds();
-#if defined(STRESS_REGIONS) && defined(FEATURE_BASICFREEZE)
-#ifdef MULTIPLE_HEAPS
-        gc_heap* hp = gc_heap::g_heaps[0];
-#else
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        for (int i = 0; i < 2; i++)
-        {
-            size_t ro_seg_size = 1024 * 1024;
-            uint8_t* seg_mem = new (nothrow) uint8_t [ro_seg_size];
-            heap_segment* ro_seg = (heap_segment*) seg_mem;
-            uint8_t* start = seg_mem + gc_heap::segment_info_size;
-            heap_segment_mem (ro_seg) = start;
-            heap_segment_used (ro_seg) = start;
-            heap_segment_reserved (ro_seg) = seg_mem + ro_seg_size;
-            heap_segment_committed (ro_seg) = heap_segment_reserved (ro_seg);
-            gc_heap::init_heap_segment (ro_seg, hp, seg_mem, ro_seg_size, 2);
-            ro_seg->flags = heap_segment_flags_readonly;
-            hp->insert_ro_segment (ro_seg);
-        }
-#endif //STRESS_REGIONS && FEATURE_BASICFREEZE
-    }
-    return hr;
-}
-bool GCHeap::IsPromoted(Object* object)
-{
-    uint8_t* o = (uint8_t*)object;
-    bool is_marked;
-    if (gc_heap::settings.condemned_generation == max_generation)
-    {
-#ifdef MULTIPLE_HEAPS
-        gc_heap* hp = gc_heap::g_heaps[0];
-#else
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-#ifdef BACKGROUND_GC
-        if (gc_heap::settings.concurrent)
-        {
-            is_marked = (!((o < hp->background_saved_highest_address) && (o >= hp->background_saved_lowest_address))||
-                            hp->background_marked (o));
-        }
-        else
-#endif //BACKGROUND_GC
-        {
-            is_marked = (!((o < hp->highest_address) && (o >= hp->lowest_address))
-                        || hp->is_mark_set (o));
-        }
-    }
-    else
-    {
-#ifdef USE_REGIONS
-        is_marked = (gc_heap::is_in_gc_range (o) ? (gc_heap::is_in_condemned_gc (o) ? gc_heap::is_mark_set (o) : true) : true);
-#else
-        gc_heap* hp = gc_heap::heap_of (o);
-        is_marked = (!((o < hp->gc_high) && (o >= hp->gc_low))
-                   || hp->is_mark_set (o));
-#endif //USE_REGIONS
-    }
-#ifdef _DEBUG
-    if (o)
-    {
-        ((CObjectHeader*)o)->Validate(TRUE, TRUE, is_marked);
-        assert(is_marked || !IsInFrozenSegment(object));
-    }
-#endif //_DEBUG
-    return is_marked;
-}
-size_t GCHeap::GetPromotedBytes(int heap_index)
-{
-#ifdef BACKGROUND_GC
-    if (gc_heap::settings.concurrent)
-    {
-        return gc_heap::bpromoted_bytes (heap_index);
-    }
-    else
-#endif //BACKGROUND_GC
-    {
-        gc_heap* hp =
-#ifdef MULTIPLE_HEAPS
-            gc_heap::g_heaps[heap_index];
-#else
-            pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        return hp->get_promoted_bytes();
-    }
-}
-void GCHeap::SetYieldProcessorScalingFactor (float scalingFactor)
-{
-    if (!gc_heap::spin_count_unit_config_p)
-    {
-        assert (yp_spin_count_unit != 0);
-        uint32_t saved_yp_spin_count_unit = yp_spin_count_unit;
-        yp_spin_count_unit = (uint32_t)((float)original_spin_count_unit * scalingFactor / (float)9);
-        if ((yp_spin_count_unit == 0) || (yp_spin_count_unit > MAX_YP_SPIN_COUNT_UNIT))
-        {
-            yp_spin_count_unit = saved_yp_spin_count_unit;
-        }
-    }
-}
-unsigned int GCHeap::WhichGeneration (Object* object)
-{
-    uint8_t* o = (uint8_t*)object;
-#ifdef FEATURE_BASICFREEZE
-    if (!((o < g_gc_highest_address) && (o >= g_gc_lowest_address)))
-    {
-        return INT32_MAX;
-    }
-#ifndef USE_REGIONS
-    if (GCHeap::IsInFrozenSegment (object))
-    {
-        return INT32_MAX;
-    }
-#endif
-#endif //FEATURE_BASICFREEZE
-    gc_heap* hp = gc_heap::heap_of (o);
-    unsigned int g = hp->object_gennum (o);
-    dprintf (3, ("%zx is in gen %d", (size_t)object, g));
-    return g;
-}
-enable_no_gc_region_callback_status GCHeap::EnableNoGCRegionCallback(NoGCRegionCallbackFinalizerWorkItem* callback, uint64_t callback_threshold)
-{
-    return gc_heap::enable_no_gc_callback(callback, callback_threshold);
-}
-FinalizerWorkItem* GCHeap::GetExtraWorkForFinalization()
-{
-    return Interlocked::ExchangePointer(&gc_heap::finalizer_work, nullptr);
-}
-unsigned int GCHeap::GetGenerationWithRange (Object* object, uint8_t** ppStart, uint8_t** ppAllocated, uint8_t** ppReserved)
-{
-    int generation = -1;
-    heap_segment * hs = gc_heap::find_segment ((uint8_t*)object, FALSE);
-#ifdef USE_REGIONS
-    generation = heap_segment_gen_num (hs);
-    if (generation == max_generation)
-    {
-        if (heap_segment_loh_p (hs))
-        {
-            generation = loh_generation;
-        }
-        else if (heap_segment_poh_p (hs))
-        {
-            generation = poh_generation;
-        }
-    }
-    *ppStart = heap_segment_mem (hs);
-    *ppAllocated = heap_segment_allocated (hs);
-    *ppReserved = heap_segment_reserved (hs);
-#else
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hp = heap_segment_heap (hs);
-#else
-    gc_heap* hp = __this;
-#endif //MULTIPLE_HEAPS
-    if (hs == hp->ephemeral_heap_segment)
-    {
-        uint8_t* reserved = heap_segment_reserved (hs);
-        uint8_t* end = heap_segment_allocated(hs);
-        for (int gen = 0; gen < max_generation; gen++)
-        {
-            uint8_t* start = generation_allocation_start (hp->generation_of (gen));
-            if ((uint8_t*)object >= start)
-            {
-                generation = gen;
-                *ppStart = start;
-                *ppAllocated = end;
-                *ppReserved = reserved;
-                break;
-            }
-            end = reserved = start;
-        }
-        if (generation == -1)
-        {
-            generation = max_generation;
-            *ppStart = heap_segment_mem (hs);
-            *ppAllocated = *ppReserved = generation_allocation_start (hp->generation_of (max_generation - 1));
-        }
-    }
-    else
-    {
-        generation = max_generation;
-        if (heap_segment_loh_p (hs))
-        {
-            generation = loh_generation;
-        }
-        else if (heap_segment_poh_p (hs))
-        {
-            generation = poh_generation;
-        }
-        *ppStart = heap_segment_mem (hs);
-        *ppAllocated = heap_segment_allocated (hs);
-        *ppReserved = heap_segment_reserved (hs);
-    }
-#endif //USE_REGIONS
-    return (unsigned int)generation;
-}
-bool GCHeap::IsEphemeral (Object* object)
-{
-    uint8_t* o = (uint8_t*)object;
-#if defined(FEATURE_BASICFREEZE) && defined(USE_REGIONS)
-    if (!is_in_heap_range (o))
-    {
-        return FALSE;
-    }
-#endif
-    gc_heap* hp = gc_heap::heap_of (o);
-    return !!hp->ephemeral_pointer_p (o);
-}
-Object * GCHeap::NextObj (Object * object)
-{
-#ifdef VERIFY_HEAP
-    uint8_t* o = (uint8_t*)object;
-#ifndef FEATURE_BASICFREEZE
-    if (!((o < g_gc_highest_address) && (o >= g_gc_lowest_address)))
-    {
-        return NULL;
-    }
-#endif //!FEATURE_BASICFREEZE
-    heap_segment * hs = gc_heap::find_segment (o, FALSE);
-    if (!hs)
-    {
-        return NULL;
-    }
-    BOOL large_object_p = heap_segment_uoh_p (hs);
-    if (large_object_p)
-        return NULL; //could be racing with another core allocating.
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hp = heap_segment_heap (hs);
-#else //MULTIPLE_HEAPS
-    gc_heap* hp = 0;
-#endif //MULTIPLE_HEAPS
-#ifdef USE_REGIONS
-    unsigned int g = heap_segment_gen_num (hs);
-#else
-    unsigned int g = hp->object_gennum ((uint8_t*)object);
-#endif
-    int align_const = get_alignment_constant (!large_object_p);
-    uint8_t* nextobj = o + Align (size (o), align_const);
-    if (nextobj <= o) // either overflow or 0 sized object.
-    {
-        return NULL;
-    }
-    if (nextobj < heap_segment_mem (hs))
-    {
-        return NULL;
-    }
-    uint8_t* saved_alloc_allocated = hp->alloc_allocated;
-    heap_segment* saved_ephemeral_heap_segment = hp->ephemeral_heap_segment;
-    if ((nextobj >= heap_segment_allocated (hs)) &&
-        ((hs != saved_ephemeral_heap_segment) ||
-         !in_range_for_segment(saved_alloc_allocated, saved_ephemeral_heap_segment) ||
-         (nextobj >= saved_alloc_allocated)))
-    {
-        return NULL;
-    }
-    return (Object *)nextobj;
-#else
-    return nullptr;
-#endif // VERIFY_HEAP
-}
-bool GCHeap::IsHeapPointer (void* vpObject, bool small_heap_only)
-{
-    uint8_t* object = (uint8_t*) vpObject;
-#ifndef FEATURE_BASICFREEZE
-    if (!((object < g_gc_highest_address) && (object >= g_gc_lowest_address)))
-        return FALSE;
-#endif //!FEATURE_BASICFREEZE
-    heap_segment * hs = gc_heap::find_segment (object, small_heap_only);
-    return !!hs;
-}
-void GCHeap::Promote(Object** ppObject, ScanContext* sc, uint32_t flags)
-{
-    THREAD_NUMBER_FROM_CONTEXT;
-#ifndef MULTIPLE_HEAPS
-    const int thread = 0;
-#endif //!MULTIPLE_HEAPS
-    uint8_t* o = (uint8_t*)*ppObject;
-    if (!gc_heap::is_in_find_object_range (o))
-    {
-        return;
-    }
-#ifdef DEBUG_DestroyedHandleValue
-    if (o == (uint8_t*)DEBUG_DestroyedHandleValue)
-        return;
-#endif //DEBUG_DestroyedHandleValue
-    HEAP_FROM_THREAD;
-    gc_heap* hp = gc_heap::heap_of (o);
-#ifdef USE_REGIONS
-    if (!gc_heap::is_in_condemned_gc (o))
-#else //USE_REGIONS
-    if ((o < hp->gc_low) || (o >= hp->gc_high))
-#endif //USE_REGIONS
-    {
-        return;
-    }
-    dprintf (3, ("Promote %zx", (size_t)o));
-    if (flags & GC_CALL_INTERIOR)
-    {
-        if ((o = hp->find_object (o)) == 0)
-        {
-            return;
-        }
-    }
-#ifdef FEATURE_CONSERVATIVE_GC
-    if (GCConfig::GetConservativeGC()
-        && ((CObjectHeader*)o)->IsFree())
-    {
-        return;
-    }
-#endif
-#ifdef _DEBUG
-    ((CObjectHeader*)o)->Validate();
-#else
-    UNREFERENCED_PARAMETER(sc);
-#endif //_DEBUG
-    if (flags & GC_CALL_PINNED)
-        hp->pin_object (o, (uint8_t**) ppObject);
-#ifdef STRESS_PINNING
-    if ((++n_promote % 20) == 1)
-            hp->pin_object (o, (uint8_t**) ppObject);
-#endif //STRESS_PINNING
-    hpt->mark_object_simple (&o THREAD_NUMBER_ARG);
-    STRESS_LOG_ROOT_PROMOTE(ppObject, o, o ? header(o)->GetMethodTable() : NULL);
-}
-void GCHeap::Relocate (Object** ppObject, ScanContext* sc,
-                       uint32_t flags)
-{
-    UNREFERENCED_PARAMETER(sc);
-    uint8_t* object = (uint8_t*)(Object*)(*ppObject);
-    if (!gc_heap::is_in_find_object_range (object))
-    {
-        return;
-    }
-    THREAD_NUMBER_FROM_CONTEXT;
-    dprintf (3, ("R: %zx", (size_t)ppObject));
-    gc_heap* hp = gc_heap::heap_of (object);
-#ifdef _DEBUG
-    if (!(flags & GC_CALL_INTERIOR))
-    {
-#ifdef USE_REGIONS
-        if (!gc_heap::is_in_condemned_gc (object))
-#else //USE_REGIONS
-        if (!((object >= hp->gc_low) && (object < hp->gc_high)))
-#endif //USE_REGIONS
-        {
-            ((CObjectHeader*)object)->Validate(FALSE);
-        }
-    }
-#endif //_DEBUG
-    dprintf (3, ("Relocate %zx\n", (size_t)object));
-    uint8_t* pheader;
-    if ((flags & GC_CALL_INTERIOR) && gc_heap::settings.loh_compaction)
-    {
-#ifdef USE_REGIONS
-        if (!gc_heap::is_in_condemned_gc (object))
-#else //USE_REGIONS
-        if (!((object >= hp->gc_low) && (object < hp->gc_high)))
-#endif //USE_REGIONS
-        {
-            return;
-        }
-        if (gc_heap::loh_object_p (object))
-        {
-            pheader = hp->find_object (object);
-            if (pheader == 0)
-            {
-                return;
-            }
-            ptrdiff_t ref_offset = object - pheader;
-            hp->relocate_address(&pheader THREAD_NUMBER_ARG);
-            *ppObject = (Object*)(pheader + ref_offset);
-            return;
-        }
-    }
-    {
-        pheader = object;
-        hp->relocate_address(&pheader THREAD_NUMBER_ARG);
-        *ppObject = (Object*)pheader;
-    }
-    STRESS_LOG_ROOT_RELOCATE(ppObject, object, pheader, ((!(flags & GC_CALL_INTERIOR)) ? ((Object*)object)->GetGCSafeMethodTable() : 0));
-}
-/*static*/ bool GCHeap::IsLargeObject(Object *pObj)
-{
-    return size( pObj ) >= loh_size_threshold;
-}
-#ifndef FEATURE_NATIVEAOT // NativeAOT forces relocation a different way
-#ifdef STRESS_HEAP
-void StressHeapDummy ();
-int StressRNG(int iMaxValue)
-{
-    static BOOL bisRandInit = FALSE;
-    static int lHoldrand = 1L;
-    if (!bisRandInit)
-    {
-        lHoldrand = (int)time(NULL);
-        bisRandInit = TRUE;
-    }
-    int randValue = (((lHoldrand = lHoldrand * 214013L + 2531011L) >> 16) & 0x7fff);
-    return randValue % iMaxValue;
-}
-#endif // STRESS_HEAP
-#endif // !FEATURE_NATIVEAOT
-bool GCHeap::StressHeap(gc_alloc_context * context)
-{
-#if defined(STRESS_HEAP) && !defined(FEATURE_NATIVEAOT)
-    alloc_context* acontext = static_cast<alloc_context*>(context);
-    assert(context != nullptr);
-    if (!GCStressPolicy::IsEnabled())
-        return FALSE;
-#ifdef _DEBUG
-    if (g_pConfig->FastGCStressLevel() && !GCToEEInterface::GetThread()->StressHeapIsEnabled()) {
-        return FALSE;
-    }
-#endif //_DEBUG
-    if ((g_pConfig->GetGCStressLevel() & EEConfig::GCSTRESS_UNIQUE)
-#ifdef _DEBUG
-        || g_pConfig->FastGCStressLevel() > 1
-#endif //_DEBUG
-        ) {
-        if (!Thread::UniqueStack(&acontext)) {
-            return FALSE;
-        }
-    }
-#ifdef BACKGROUND_GC
-    if (GCToEEInterface::WasCurrentThreadCreatedByGC())
-    {
-        return FALSE;
-    }
-#endif //BACKGROUND_GC
-    if (g_pStringClass == 0)
-    {
-        _ASSERTE(g_fEEInit);
-        return FALSE;
-    }
-#ifndef MULTIPLE_HEAPS
-    static int32_t OneAtATime = -1;
-    if (Interlocked::Increment(&OneAtATime) == 0 &&
-        !TrackAllocations()) // Messing with object sizes can confuse the profiler (see ICorProfilerInfo::GetObjectSize)
-    {
-        StringObject* str;
-        if (HndFetchHandle(m_StressObjs[m_CurStressObj]) == 0)
-        {
-            int i = m_CurStressObj;
-            while(HndFetchHandle(m_StressObjs[i]) == 0)
-            {
-                _ASSERTE(m_StressObjs[i] != 0);
-                unsigned strLen = ((unsigned)loh_size_threshold - 32) / sizeof(WCHAR);
-                unsigned strSize = PtrAlign(StringObject::GetSize(strLen));
-                SetTypeHandleOnThreadForAlloc(TypeHandle(g_pStringClass));
-                str = (StringObject*) pGenGCHeap->allocate (strSize, acontext, /*flags*/ 0);
-                if (str)
-                {
-                    str->SetMethodTable (g_pStringClass);
-                    str->SetStringLength (strLen);
-                    HndAssignHandle(m_StressObjs[i], ObjectToOBJECTREF(str));
-                }
-                i = (i + 1) % NUM_HEAP_STRESS_OBJS;
-                if (i == m_CurStressObj) break;
-            }
-            m_CurStressObj = (m_CurStressObj + 1) % NUM_HEAP_STRESS_OBJS;
-        }
-        str = (StringObject*) OBJECTREFToObject(HndFetchHandle(m_StressObjs[m_CurStressObj]));
-        if (str)
-        {
-            unsigned sizeOfNewObj = (unsigned)Align(min_obj_size * 31);
-            if (str->GetStringLength() > sizeOfNewObj / sizeof(WCHAR))
-            {
-                unsigned sizeToNextObj = (unsigned)Align(size(str));
-                uint8_t* freeObj = ((uint8_t*) str) + sizeToNextObj - sizeOfNewObj;
-                pGenGCHeap->make_unused_array (freeObj, sizeOfNewObj);
-#if !defined(TARGET_AMD64) && !defined(TARGET_X86)
-                MemoryBarrier();
-#endif
-                str->SetStringLength(str->GetStringLength() - (sizeOfNewObj / sizeof(WCHAR)));
-            }
-            else
-            {
-                HndAssignHandle(m_StressObjs[m_CurStressObj], 0);
-            }
-        }
-    }
-    Interlocked::Decrement(&OneAtATime);
-#endif // !MULTIPLE_HEAPS
-    if (IsConcurrentGCEnabled())
-    {
-        int rgen = StressRNG(10);
-        if (rgen >= 8)
-            rgen = 2;
-        else if (rgen >= 4)
-            rgen = 1;
-    else
-            rgen = 0;
-        GarbageCollectTry (rgen, FALSE, collection_gcstress);
-    }
-    else
-    {
-        GarbageCollect(max_generation, FALSE, collection_gcstress);
-    }
-    return TRUE;
-#else
-    UNREFERENCED_PARAMETER(context);
-    return FALSE;
-#endif //STRESS_HEAP && !FEATURE_NATIVEAOT
-}
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-#define REGISTER_FOR_FINALIZATION(_object, _size) \
-    hp->finalize_queue->RegisterForFinalization (0, (_object), (_size))
-#else // FEATURE_PREMORTEM_FINALIZATION
-#define REGISTER_FOR_FINALIZATION(_object, _size) true
-#endif // FEATURE_PREMORTEM_FINALIZATION
-#define CHECK_ALLOC_AND_POSSIBLY_REGISTER_FOR_FINALIZATION(_object, _size, _register) do {  \
-    if ((_object) == NULL || ((_register) && !REGISTER_FOR_FINALIZATION(_object, _size)))   \
-    {                                                                                       \
-        STRESS_LOG_OOM_STACK(_size);                                                        \
-        return NULL;                                                                        \
-    }                                                                                       \
-} while (false)
-#ifdef FEATURE_64BIT_ALIGNMENT
-Object* AllocAlign8(alloc_context* acontext, gc_heap* hp, size_t size, uint32_t flags)
-{
-    CONTRACTL {
-        NOTHROW;
-        GC_TRIGGERS;
-    } CONTRACTL_END;
-    Object* newAlloc = NULL;
-    size_t desiredAlignment = (flags & GC_ALLOC_ALIGN8_BIAS) ? 4 : 0;
-    uint8_t*  result = acontext->alloc_ptr;
-    if ((((size_t)result & 7) == desiredAlignment) && ((result + size) <= acontext->alloc_limit))
-    {
-        newAlloc = (Object*) hp->allocate (size, acontext, flags);
-        ASSERT(((size_t)newAlloc & 7) == desiredAlignment);
-    }
-    else
-    {
-        ASSERT((Align(min_obj_size) & 7) == 4);
-        CObjectHeader *freeobj = (CObjectHeader*) hp->allocate (Align(size) + Align(min_obj_size), acontext, flags);
-        if (freeobj)
-        {
-            if (((size_t)freeobj & 7) == desiredAlignment)
-            {
-                newAlloc = (Object*)freeobj;
-                freeobj = (CObjectHeader*)((uint8_t*)freeobj + Align(size));
-            }
-            else
-            {
-                newAlloc = (Object*)((uint8_t*)freeobj + Align(min_obj_size));
-                ASSERT(((size_t)newAlloc & 7) == desiredAlignment);
-                if (flags & GC_ALLOC_ZEROING_OPTIONAL)
-                {
-                    *(((PTR_PTR)newAlloc)-1) = 0;
-                }
-            }
-            freeobj->SetFree(min_obj_size);
-        }
-    }
-    return newAlloc;
-}
-#endif // FEATURE_64BIT_ALIGNMENT
-Object*
-GCHeap::Alloc(gc_alloc_context* context, size_t size, uint32_t flags REQD_ALIGN_DCL)
-{
-    CONTRACTL {
-        NOTHROW;
-        GC_TRIGGERS;
-    } CONTRACTL_END;
-    TRIGGERSGC();
-    Object* newAlloc = NULL;
-    alloc_context* acontext = static_cast<alloc_context*>(context);
-#ifdef MULTIPLE_HEAPS
-    if (acontext->get_alloc_heap() == 0)
-    {
-        AssignHeap (acontext);
-        assert (acontext->get_alloc_heap());
-    }
-    gc_heap* hp = acontext->get_alloc_heap()->pGenGCHeap;
-#else
-    gc_heap* hp = pGenGCHeap;
-#ifdef _PREFAST_
-    PREFIX_ASSUME(hp != NULL);
-#endif //_PREFAST_
-#endif //MULTIPLE_HEAPS
-    assert(size < loh_size_threshold || (flags & GC_ALLOC_LARGE_OBJECT_HEAP));
-    if (flags & GC_ALLOC_USER_OLD_HEAP)
-    {
-        ASSERT((flags & GC_ALLOC_ALIGN8_BIAS) == 0);
-        ASSERT(65536 < loh_size_threshold);
-        int gen_num = (flags & GC_ALLOC_PINNED_OBJECT_HEAP) ? poh_generation : loh_generation;
-        newAlloc = (Object*) hp->allocate_uoh_object (size + ComputeMaxStructAlignPadLarge(requiredAlignment), flags, gen_num, acontext->alloc_bytes_uoh);
-        ASSERT(((size_t)newAlloc & 7) == 0);
-#ifdef MULTIPLE_HEAPS
-        if (flags & GC_ALLOC_FINALIZE)
-        {
-            hp = gc_heap::heap_of ((uint8_t*)newAlloc);
-        }
-#endif //MULTIPLE_HEAPS
-#ifdef FEATURE_STRUCTALIGN
-        newAlloc = (Object*) hp->pad_for_alignment_large ((uint8_t*) newAlloc, requiredAlignment, size);
-#endif // FEATURE_STRUCTALIGN
-    }
-    else
-    {
-#ifdef FEATURE_64BIT_ALIGNMENT
-        if (flags & GC_ALLOC_ALIGN8)
-        {
-            newAlloc = AllocAlign8 (acontext, hp, size, flags);
-        }
-        else
-#else
-        assert ((flags & GC_ALLOC_ALIGN8) == 0);
-#endif
-        {
-            newAlloc = (Object*) hp->allocate (size + ComputeMaxStructAlignPad(requiredAlignment), acontext, flags);
-        }
-#ifdef MULTIPLE_HEAPS
-        if (flags & GC_ALLOC_FINALIZE)
-        {
-#ifdef DYNAMIC_HEAP_COUNT
-            hp = (newAlloc == nullptr) ? acontext->get_alloc_heap()->pGenGCHeap : gc_heap::heap_of ((uint8_t*)newAlloc);
-#else //DYNAMIC_HEAP_COUNT
-            hp = acontext->get_alloc_heap()->pGenGCHeap;
-            assert ((newAlloc == nullptr) || (hp == gc_heap::heap_of ((uint8_t*)newAlloc)));
-#endif //DYNAMIC_HEAP_COUNT
-        }
-#endif //MULTIPLE_HEAPS
-#ifdef FEATURE_STRUCTALIGN
-        newAlloc = (Object*) hp->pad_for_alignment ((uint8_t*) newAlloc, requiredAlignment, size, acontext);
-#endif // FEATURE_STRUCTALIGN
-    }
-    CHECK_ALLOC_AND_POSSIBLY_REGISTER_FOR_FINALIZATION(newAlloc, size, flags & GC_ALLOC_FINALIZE);
-#ifdef USE_REGIONS
-    assert (IsHeapPointer (newAlloc));
-#endif //USE_REGIONS
-    return newAlloc;
-}
-void
-GCHeap::FixAllocContext (gc_alloc_context* context, void* arg, void *heap)
-{
-    alloc_context* acontext = static_cast<alloc_context*>(context);
-#ifdef MULTIPLE_HEAPS
-    if (arg != 0)
-        acontext->alloc_count = 0;
-    uint8_t * alloc_ptr = acontext->alloc_ptr;
-    if (!alloc_ptr)
-        return;
-    gc_heap* hp = gc_heap::heap_of (alloc_ptr);
-#else
-    gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-    if (heap == NULL || heap == hp)
-    {
-        hp->fix_allocation_context (acontext, ((arg != 0)? TRUE : FALSE), TRUE);
-    }
-}
-Object*
-GCHeap::GetContainingObject (void *pInteriorPtr, bool fCollectedGenOnly)
-{
-    uint8_t *o = (uint8_t*)pInteriorPtr;
-    if (!gc_heap::is_in_find_object_range (o))
-    {
-        return NULL;
-    }
-    gc_heap* hp = gc_heap::heap_of (o);
-#ifdef USE_REGIONS
-    if (fCollectedGenOnly && !gc_heap::is_in_condemned_gc (o))
-    {
-        return NULL;
-    }
-#else //USE_REGIONS
-    uint8_t* lowest = (fCollectedGenOnly ? hp->gc_low : hp->lowest_address);
-    uint8_t* highest = (fCollectedGenOnly ? hp->gc_high : hp->highest_address);
-    if (!((o >= lowest) && (o < highest)))
-    {
-        return NULL;
-    }
-#endif //USE_REGIONS
-    return (Object*)(hp->find_object (o));
-}
-BOOL should_collect_optimized (dynamic_data* dd, BOOL low_memory_p)
-{
-    if (dd_new_allocation (dd) < 0)
-    {
-        return TRUE;
-    }
-    if (((float)(dd_new_allocation (dd)) / (float)dd_desired_allocation (dd)) < (low_memory_p ? 0.7 : 0.3))
-    {
-        return TRUE;
-    }
-    return FALSE;
-}
-HRESULT
-GCHeap::GarbageCollect (int generation, bool low_memory_p, int mode)
-{
-#if defined(HOST_64BIT)
-    if (low_memory_p)
-    {
-        size_t total_allocated = 0;
-        size_t total_desired = 0;
-#ifdef MULTIPLE_HEAPS
-        int hn = 0;
-        for (hn = 0; hn < gc_heap::n_heaps; hn++)
-        {
-            gc_heap* hp = gc_heap::g_heaps [hn];
-            total_desired += dd_desired_allocation (hp->dynamic_data_of (0));
-            total_allocated += dd_desired_allocation (hp->dynamic_data_of (0))-
-                dd_new_allocation (hp->dynamic_data_of (0));
-        }
-#else
-        gc_heap* hp = pGenGCHeap;
-        total_desired = dd_desired_allocation (hp->dynamic_data_of (0));
-        total_allocated = dd_desired_allocation (hp->dynamic_data_of (0))-
-            dd_new_allocation (hp->dynamic_data_of (0));
-#endif //MULTIPLE_HEAPS
-        if ((total_desired > gc_heap::mem_one_percent) && (total_allocated < gc_heap::mem_one_percent))
-        {
-            dprintf (2, ("Async low mem but we've only allocated %zu (< 10%% of physical mem) out of %zu, returning",
-                         total_allocated, total_desired));
-            return S_OK;
-        }
-    }
-#endif // HOST_64BIT
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hpt = gc_heap::g_heaps[0];
-#else
-    gc_heap* hpt = 0;
-#endif //MULTIPLE_HEAPS
-    generation = (generation < 0) ? max_generation : min (generation, max_generation);
-    dynamic_data* dd = hpt->dynamic_data_of (generation);
-#ifdef BACKGROUND_GC
-    if (gc_heap::background_running_p())
-    {
-        if ((mode == collection_optimized) || (mode & collection_non_blocking))
-        {
-            return S_OK;
-        }
-        if (mode & collection_blocking)
-        {
-            pGenGCHeap->background_gc_wait();
-            if (mode & collection_optimized)
-            {
-                return S_OK;
-            }
-        }
-    }
-#endif //BACKGROUND_GC
-    if (mode & collection_optimized)
-    {
-        if (pGenGCHeap->gc_started)
-        {
-            return S_OK;
-        }
-        else
-        {
-            BOOL should_collect = FALSE;
-            BOOL should_check_uoh = (generation == max_generation);
-#ifdef MULTIPLE_HEAPS
-            for (int heap_number = 0; heap_number < gc_heap::n_heaps; heap_number++)
-            {
-                dynamic_data* dd1 = gc_heap::g_heaps [heap_number]->dynamic_data_of (generation);
-                should_collect = should_collect_optimized (dd1, low_memory_p);
-                if (should_check_uoh)
-                {
-                    for (int i = uoh_start_generation; i < total_generation_count && !should_collect; i++)
-                    {
-                        should_collect = should_collect_optimized (gc_heap::g_heaps [heap_number]->dynamic_data_of (i), low_memory_p);
-                    }
-                }
-                if (should_collect)
-                    break;
-            }
-#else
-            should_collect = should_collect_optimized (dd, low_memory_p);
-            if (should_check_uoh)
-            {
-                for (int i = uoh_start_generation; i < total_generation_count && !should_collect; i++)
-                {
-                    should_collect = should_collect_optimized (hpt->dynamic_data_of (i), low_memory_p);
-                }
-            }
-#endif //MULTIPLE_HEAPS
-            if (!should_collect)
-            {
-                return S_OK;
-            }
-        }
-    }
-    size_t CollectionCountAtEntry = dd_collection_count (dd);
-    size_t BlockingCollectionCountAtEntry = gc_heap::full_gc_counts[gc_type_blocking];
-    size_t CurrentCollectionCount = 0;
-retry:
-    CurrentCollectionCount = GarbageCollectTry(generation, low_memory_p, mode);
-    if ((mode & collection_blocking) &&
-        (generation == max_generation) &&
-        (gc_heap::full_gc_counts[gc_type_blocking] == BlockingCollectionCountAtEntry))
-    {
-#ifdef BACKGROUND_GC
-        if (gc_heap::background_running_p())
-        {
-            pGenGCHeap->background_gc_wait();
-        }
-#endif //BACKGROUND_GC
-        goto retry;
-    }
-    if (CollectionCountAtEntry == CurrentCollectionCount)
-    {
-        goto retry;
-    }
-    return S_OK;
-}
-size_t
-GCHeap::GarbageCollectTry (int generation, BOOL low_memory_p, int mode)
-{
-    int gen = (generation < 0) ?
-               max_generation : min (generation, max_generation);
-    gc_reason reason = reason_empty;
-    if (low_memory_p)
-    {
-        if (mode & collection_blocking)
-        {
-            reason = reason_lowmemory_blocking;
-        }
-        else
-        {
-            reason = reason_lowmemory;
-        }
-    }
-    else
-    {
-        reason = reason_induced;
-    }
-    if (reason == reason_induced)
-    {
-        if (mode & collection_aggressive)
-        {
-            reason = reason_induced_aggressive;
-        }
-        else if (mode & collection_compacting)
-        {
-            reason = reason_induced_compacting;
-        }
-        else if (mode & collection_non_blocking)
-        {
-            reason = reason_induced_noforce;
-        }
-#ifdef STRESS_HEAP
-        else if (mode & collection_gcstress)
-        {
-            reason = reason_gcstress;
-        }
-#endif
-    }
-    return GarbageCollectGeneration (gen, reason);
-}
-#ifdef BACKGROUND_GC
-void gc_heap::add_bgc_pause_duration_0()
-{
-    if (settings.concurrent)
-    {
-        uint64_t suspended_end_ts = GetHighPrecisionTimeStamp();
-        size_t pause_duration = (size_t)(suspended_end_ts - suspended_start_time);
-        last_recorded_gc_info* last_gc_info = &(last_bgc_info[last_bgc_info_index]);
-        last_gc_info->pause_durations[0] = pause_duration;
-        if (last_gc_info->index < last_ephemeral_gc_info.index)
-        {
-            last_gc_info->pause_durations[0] -= last_ephemeral_gc_info.pause_durations[0];
-        }
-        total_suspended_time += last_gc_info->pause_durations[0];
-    }
-}
-last_recorded_gc_info* gc_heap::get_completed_bgc_info()
-{
-    int completed_bgc_index = gc_heap::background_running_p() ?
-        (int)(!(gc_heap::last_bgc_info_index)) : (int)gc_heap::last_bgc_info_index;
-    return &gc_heap::last_bgc_info[completed_bgc_index];
-}
-#endif //BACKGROUND_GC
-void gc_heap::do_pre_gc()
-{
-    STRESS_LOG_GC_STACK;
-#ifdef STRESS_LOG
-    STRESS_LOG_GC_START(VolatileLoad(&settings.gc_index),
-                        (uint32_t)settings.condemned_generation,
-                        (uint32_t)settings.reason);
-#endif // STRESS_LOG
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hp = g_heaps[0];
-#else
-    gc_heap* hp = 0;
-#endif //MULTIPLE_HEAPS
-#ifdef BACKGROUND_GC
-    settings.b_state = hp->current_bgc_state;
-    if (settings.concurrent)
-    {
-        last_bgc_info_index = !last_bgc_info_index;
-        last_bgc_info[last_bgc_info_index].index = settings.gc_index;
-    }
-#endif //BACKGROUND_GC
-#ifdef TRACE_GC
-    size_t total_allocated_since_last_gc = get_total_allocated_since_last_gc();
-#ifdef BACKGROUND_GC
-    dprintf (1, (ThreadStressLog::gcDetailedStartMsg(),
-        VolatileLoad(&settings.gc_index),
-        dd_collection_count (hp->dynamic_data_of (0)),
-        settings.condemned_generation,
-        total_allocated_since_last_gc,
-        (settings.concurrent ? "BGC" : (gc_heap::background_running_p() ? "FGC" : "NGC")),
-        settings.b_state));
-#else
-    dprintf (1, ("*GC* %d(gen0:%d)(%d)(alloc: %zd)",
-        VolatileLoad(&settings.gc_index),
-        dd_collection_count(hp->dynamic_data_of(0)),
-        settings.condemned_generation,
-        total_allocated_since_last_gc));
-#endif //BACKGROUND_GC
-    if (heap_hard_limit)
-    {
-        size_t total_heap_committed = get_total_committed_size();
-        size_t total_heap_committed_recorded = current_total_committed - current_total_committed_bookkeeping;
-        dprintf (1, ("(%d)GC commit BEG #%zd: %zd (recorded: %zd = %zd-%zd)",
-            settings.condemned_generation,
-            (size_t)settings.gc_index, total_heap_committed, total_heap_committed_recorded,
-            current_total_committed, current_total_committed_bookkeeping));
-    }
-#endif //TRACE_GC
-    GCHeap::UpdatePreGCCounters();
-    fire_committed_usage_event();
-#if defined(__linux__)
-    GCToEEInterface::UpdateGCEventStatus(static_cast<int>(GCEventStatus::GetEnabledLevel(GCEventProvider_Default)),
-                                         static_cast<int>(GCEventStatus::GetEnabledKeywords(GCEventProvider_Default)),
-                                         static_cast<int>(GCEventStatus::GetEnabledLevel(GCEventProvider_Private)),
-                                         static_cast<int>(GCEventStatus::GetEnabledKeywords(GCEventProvider_Private)));
-#endif // __linux__
-    if (settings.concurrent)
-    {
-#ifdef BACKGROUND_GC
-        full_gc_counts[gc_type_background]++;
-#endif // BACKGROUND_GC
-    }
-    else
-    {
-        if (settings.condemned_generation == max_generation)
-        {
-            full_gc_counts[gc_type_blocking]++;
-        }
-        else
-        {
-#ifdef BACKGROUND_GC
-            if (settings.background_p)
-            {
-                ephemeral_fgc_counts[settings.condemned_generation]++;
-            }
-#endif //BACKGROUND_GC
-        }
-    }
-}
-#ifdef GC_CONFIG_DRIVEN
-void gc_heap::record_interesting_info_per_heap()
-{
-    if (!(settings.concurrent))
-    {
-        for (int i = 0; i < max_idp_count; i++)
-        {
-            interesting_data_per_heap[i] += interesting_data_per_gc[i];
-        }
-    }
-    int compact_reason = get_gc_data_per_heap()->get_mechanism (gc_heap_compact);
-    if (compact_reason >= 0)
-        (compact_reasons_per_heap[compact_reason])++;
-    int expand_mechanism = get_gc_data_per_heap()->get_mechanism (gc_heap_expand);
-    if (expand_mechanism >= 0)
-        (expand_mechanisms_per_heap[expand_mechanism])++;
-    for (int i = 0; i < max_gc_mechanism_bits_count; i++)
-    {
-        if (get_gc_data_per_heap()->is_mechanism_bit_set ((gc_mechanism_bit_per_heap)i))
-            (interesting_mechanism_bits_per_heap[i])++;
-    }
-    cprintf (("%2d | %6d | %1d | %1s | %2s | %2s | %2s | %2s | %2s || %5Id | %5Id | %5Id | %5Id | %5Id | %5Id | %5Id | %5Id | %5Id |",
-            heap_number,
-            (size_t)settings.gc_index,
-            settings.condemned_generation,
-            (settings.compaction ? (((compact_reason >= 0) && gc_heap_compact_reason_mandatory_p[compact_reason]) ? "M" : "W") : ""), // compaction
-            ((expand_mechanism >= 0)? "X" : ""), // EX
-            ((expand_mechanism == expand_reuse_normal) ? "X" : ""), // NF
-            ((expand_mechanism == expand_reuse_bestfit) ? "X" : ""), // BF
-            (get_gc_data_per_heap()->is_mechanism_bit_set (gc_mark_list_bit) ? "X" : ""), // ML
-            (get_gc_data_per_heap()->is_mechanism_bit_set (gc_demotion_bit) ? "X" : ""), // DM
-            interesting_data_per_gc[idp_pre_short],
-            interesting_data_per_gc[idp_post_short],
-            interesting_data_per_gc[idp_merged_pin],
-            interesting_data_per_gc[idp_converted_pin],
-            interesting_data_per_gc[idp_pre_pin],
-            interesting_data_per_gc[idp_post_pin],
-            interesting_data_per_gc[idp_pre_and_post_pin],
-            interesting_data_per_gc[idp_pre_short_padded],
-            interesting_data_per_gc[idp_post_short_padded]));
-}
-void gc_heap::record_global_mechanisms()
-{
-    for (int i = 0; i < max_global_mechanisms_count; i++)
-    {
-        if (gc_data_global.get_mechanism_p ((gc_global_mechanism_p)i))
-        {
-            ::record_global_mechanism (i);
-        }
-    }
-}
-BOOL gc_heap::should_do_sweeping_gc (BOOL compact_p)
-{
-    if (!compact_ratio)
-        return (!compact_p);
-    size_t compact_count = compact_or_sweep_gcs[0];
-    size_t sweep_count = compact_or_sweep_gcs[1];
-    size_t total_count = compact_count + sweep_count;
-    BOOL should_compact = compact_p;
-    if (total_count > 3)
-    {
-        if (compact_p)
-        {
-            int temp_ratio = (int)((compact_count + 1) * 100 / (total_count + 1));
-            if (temp_ratio > compact_ratio)
-            {
-                should_compact = FALSE;
-            }
-        }
-        else
-        {
-            int temp_ratio = (int)((sweep_count + 1) * 100 / (total_count + 1));
-            if (temp_ratio > (100 - compact_ratio))
-            {
-                should_compact = TRUE;
-            }
-        }
-    }
-    return !should_compact;
-}
-#endif //GC_CONFIG_DRIVEN
-#ifdef BGC_SERVO_TUNING
-void gc_heap::check_and_adjust_bgc_tuning (int gen_number, size_t physical_size, ptrdiff_t virtual_fl_size)
-{
-    int min_gen_to_check = ((gen_number == max_generation) ? (max_generation - 1) : 0);
-    if (settings.condemned_generation >= min_gen_to_check)
-    {
-#ifdef MULTIPLE_HEAPS
-        gc_heap* hp = g_heaps[0];
-#else
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        size_t total_gen_size = physical_size;
-        size_t total_generation_fl_size = get_total_generation_fl_size (gen_number);
-        double gen_flr = (double)total_generation_fl_size * 100.0 / (double)total_gen_size;
-        size_t gen1_index = dd_collection_count (hp->dynamic_data_of (max_generation - 1));
-        size_t gen2_index = dd_collection_count (hp->dynamic_data_of (max_generation));
-        bgc_tuning::tuning_calculation* current_gen_calc = &bgc_tuning::gen_calc[gen_number - max_generation];
-        bgc_tuning::tuning_stats* current_gen_stats = &bgc_tuning::gen_stats[gen_number - max_generation];
-        bool gen_size_inc_p = (total_gen_size > current_gen_calc->last_bgc_size);
-        if ((settings.condemned_generation >= min_gen_to_check) &&
-            (settings.condemned_generation != max_generation))
-        {
-            if (gen_size_inc_p)
-            {
-                current_gen_stats->last_gen_increase_flr = gen_flr;
-                dprintf (BGC_TUNING_LOG, ("BTLp[g1: %zd, g2: %zd]: gen%d size inc %s %zd->%zd, flr: %.3f",
-                        gen1_index, gen2_index, gen_number,
-                        (gc_heap::background_running_p() ? "during bgc" : ""),
-                        current_gen_stats->last_bgc_physical_size, total_gen_size, gen_flr));
-            }
-            if (!bgc_tuning::fl_tuning_triggered)
-            {
-                if (bgc_tuning::enable_fl_tuning)
-                {
-                    if (!((gc_heap::background_running_p() || (hp->current_bgc_state == bgc_initialized))))
-                    {
-                        assert (settings.entry_memory_load);
-                        if ((settings.entry_memory_load >= (bgc_tuning::memory_load_goal * 2 / 3)) &&
-                            (full_gc_counts[gc_type_background] >= 2))
-                        {
-                            bgc_tuning::next_bgc_p = true;
-                            current_gen_calc->first_alloc_to_trigger = get_total_servo_alloc (gen_number);
-                            dprintf (BGC_TUNING_LOG, ("BTL[g1: %zd] mem high enough: %d(goal: %d), gen%d fl alloc: %zd, trigger BGC!",
-                                gen1_index, settings.entry_memory_load, bgc_tuning::memory_load_goal,
-                                gen_number, current_gen_calc->first_alloc_to_trigger));
-                        }
-                    }
-                }
-            }
-        }
-        if ((settings.condemned_generation == max_generation) && !(settings.concurrent))
-        {
-            size_t total_survived = get_total_surv_size (gen_number);
-            size_t total_begin = get_total_begin_data_size (gen_number);
-            double current_gc_surv_rate = (double)total_survived * 100.0 / (double)total_begin;
-            double total_virtual_size = (double)physical_size + (double)virtual_fl_size;
-            double total_fl_size = (double)total_generation_fl_size + (double)virtual_fl_size;
-            double new_gen_flr = total_fl_size * 100.0 / total_virtual_size;
-            dprintf (BGC_TUNING_LOG, ("BTL%d NGC2 size %zd->%zd, fl %zd(%.3f)->%zd(%.3f)",
-                gen_number, physical_size, (size_t)total_virtual_size,
-                total_generation_fl_size, gen_flr,
-                (size_t)total_fl_size, new_gen_flr));
-            dprintf (BGC_TUNING_LOG, ("BTL%d* %zd, %.3f, %.3f, %.3f, %.3f, %.3f, %d, %d, %d, %zd",
-                                    gen_number,
-                                    (size_t)total_virtual_size,
-                                    0.0,
-                                    0.0,
-                                    new_gen_flr,
-                                    current_gen_stats->last_gen_increase_flr,
-                                    current_gc_surv_rate,
-                                    0,
-                                    0,
-                                    0,
-                                    current_gen_calc->alloc_to_trigger));
-            bgc_tuning::gen1_index_last_bgc_end = gen1_index;
-            current_gen_calc->last_bgc_size = total_gen_size;
-            current_gen_calc->last_bgc_flr = new_gen_flr;
-            current_gen_calc->last_sweep_above_p = false;
-            current_gen_calc->last_bgc_end_alloc = 0;
-            current_gen_stats->last_alloc_end_to_start = 0;
-            current_gen_stats->last_alloc_start_to_sweep = 0;
-            current_gen_stats->last_alloc_sweep_to_end = 0;
-            current_gen_stats->last_bgc_fl_size = total_generation_fl_size;
-            current_gen_stats->last_bgc_surv_rate = current_gc_surv_rate;
-            current_gen_stats->last_gen_increase_flr = 0;
-        }
-    }
-}
-void gc_heap::get_and_reset_loh_alloc_info()
-{
-    if (!bgc_tuning::enable_fl_tuning)
-        return;
-    total_loh_a_last_bgc = 0;
-    uint64_t total_loh_a_no_bgc = 0;
-    uint64_t total_loh_a_bgc_marking = 0;
-    uint64_t total_loh_a_bgc_planning = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        total_loh_a_no_bgc += hp->loh_a_no_bgc;
-        hp->loh_a_no_bgc = 0;
-        total_loh_a_bgc_marking += hp->loh_a_bgc_marking;
-        hp->loh_a_bgc_marking = 0;
-        total_loh_a_bgc_planning += hp->loh_a_bgc_planning;
-        hp->loh_a_bgc_planning = 0;
-    }
-    dprintf (2, ("LOH alloc: outside bgc: %zd; bm: %zd; bp: %zd",
-        total_loh_a_no_bgc,
-        total_loh_a_bgc_marking,
-        total_loh_a_bgc_planning));
-    total_loh_a_last_bgc = total_loh_a_no_bgc + total_loh_a_bgc_marking + total_loh_a_bgc_planning;
-}
-#endif //BGC_SERVO_TUNING
-bool gc_heap::is_pm_ratio_exceeded()
-{
-    size_t maxgen_frag = 0;
-    size_t maxgen_size = 0;
-    size_t total_heap_size = get_total_heap_size();
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        maxgen_frag += dd_fragmentation (hp->dynamic_data_of (max_generation));
-        maxgen_size += hp->generation_size (max_generation);
-    }
-    double maxgen_ratio = (double)maxgen_size / (double)total_heap_size;
-    double maxgen_frag_ratio = (double)maxgen_frag / (double)maxgen_size;
-    dprintf (GTC_LOG, ("maxgen %zd(%d%% total heap), frag: %zd (%d%% maxgen)",
-        maxgen_size, (int)(maxgen_ratio * 100.0),
-        maxgen_frag, (int)(maxgen_frag_ratio * 100.0)));
-    bool maxgen_highfrag_p = ((maxgen_ratio > 0.5) && (maxgen_frag_ratio > 0.1));
-    if (maxgen_highfrag_p)
-    {
-        settings.should_lock_elevation = FALSE;
-        dprintf (GTC_LOG, ("high frag gen2, turn off elevation"));
-    }
-    return maxgen_highfrag_p;
-}
-void gc_heap::update_recorded_gen_data (last_recorded_gc_info* gc_info)
-{
-    memset (gc_info->gen_info, 0, sizeof (gc_info->gen_info));
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-#else //MULTIPLE_HEAPS
-    {
-        gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        gc_history_per_heap* current_gc_data_per_heap = hp->get_gc_data_per_heap();
-        for (int gen_number = 0; gen_number < total_generation_count; gen_number++)
-        {
-            recorded_generation_info* recorded_info = &(gc_info->gen_info[gen_number]);
-            gc_generation_data* data = &(current_gc_data_per_heap->gen_data[gen_number]);
-            recorded_info->size_before += data->size_before;
-            recorded_info->fragmentation_before += data->free_list_space_before + data->free_obj_space_before;
-            recorded_info->size_after += data->size_after;
-            recorded_info->fragmentation_after += data->free_list_space_after + data->free_obj_space_after;
-        }
-    }
-}
-void gc_heap::do_post_gc()
-{
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hp = g_heaps[0];
-#else
-    gc_heap* hp = 0;
-#endif //MULTIPLE_HEAPS
-    GCToEEInterface::GcDone(settings.condemned_generation);
-    GCToEEInterface::DiagGCEnd(VolatileLoad(&settings.gc_index),
-                         (uint32_t)settings.condemned_generation,
-                         (uint32_t)settings.reason,
-                         !!settings.concurrent);
-    add_to_history();
-    uint32_t current_memory_load = 0;
-#ifdef BGC_SERVO_TUNING
-    if (bgc_tuning::enable_fl_tuning)
-    {
-        uint64_t current_available_physical = 0;
-        size_t gen2_physical_size = 0;
-        size_t gen3_physical_size = 0;
-        ptrdiff_t gen2_virtual_fl_size = 0;
-        ptrdiff_t gen3_virtual_fl_size = 0;
-        ptrdiff_t vfl_from_kp = 0;
-        ptrdiff_t vfl_from_ki = 0;
-        gen2_physical_size = get_total_generation_size (max_generation);
-        gen3_physical_size = get_total_generation_size (loh_generation);
-        get_memory_info (&current_memory_load, &current_available_physical);
-        if ((settings.condemned_generation == max_generation) && !settings.concurrent)
-        {
-            double gen2_size_ratio = (double)gen2_physical_size / ((double)gen2_physical_size + (double)gen3_physical_size);
-            double total_virtual_fl_size = bgc_tuning::calculate_ml_tuning (current_available_physical, true, &vfl_from_kp, &vfl_from_ki);
-            gen2_virtual_fl_size = (ptrdiff_t)(total_virtual_fl_size * gen2_size_ratio);
-            gen3_virtual_fl_size = (ptrdiff_t)(total_virtual_fl_size * (1.0 - gen2_size_ratio));
-#ifdef SIMPLE_DPRINTF
-            dprintf (BGC_TUNING_LOG, ("BTL: ml: %d (g: %d)(%s), a: %zd (g: %zd, elg: %zd+%zd=%zd, %zd+%zd=%zd), vfl: %zd=%zd+%zd(NGC2)",
-                current_memory_load, bgc_tuning::memory_load_goal,
-                ((current_available_physical > bgc_tuning::available_memory_goal) ? "above" : "below"),
-                current_available_physical, bgc_tuning::available_memory_goal,
-                gen2_physical_size, gen2_virtual_fl_size, (gen2_physical_size + gen2_virtual_fl_size),
-                gen3_physical_size, gen3_virtual_fl_size, (gen3_physical_size + gen3_virtual_fl_size),
-                (ptrdiff_t)total_virtual_fl_size, vfl_from_kp, vfl_from_ki));
-#endif //SIMPLE_DPRINTF
-        }
-        check_and_adjust_bgc_tuning (max_generation, gen2_physical_size, gen2_virtual_fl_size);
-        check_and_adjust_bgc_tuning (loh_generation, gen3_physical_size, gen3_virtual_fl_size);
-    }
-#endif //BGC_SERVO_TUNING
-#ifdef BACKGROUND_GC
-    const char* str_gc_type = (settings.concurrent ? "BGC" : (gc_heap::background_running_p () ? "FGC" : "NGC"));
-#else
-    const char* str_gc_type = "NGC";
-#endif //BACKGROUND_GC
-    dprintf (1, (ThreadStressLog::gcDetailedEndMsg(),
-        VolatileLoad (&settings.gc_index),
-        dd_collection_count (hp->dynamic_data_of (0)),
-        (size_t)(GetHighPrecisionTimeStamp () / 1000),
-        settings.condemned_generation,
-        (settings.concurrent ? "BGC" : (gc_heap::background_running_p() ? "FGC" : "NGC")),
-        (settings.compaction ? "C" : "S"),
-        (settings.promotion ? "P" : "S"),
-        settings.entry_memory_load,
-        current_memory_load));
-#if defined(TRACE_GC) && defined(SIMPLE_DPRINTF)
-    flush_gc_log (false);
-#endif //TRACE_GC && SIMPLE_DPRINTF
-    last_recorded_gc_info* last_gc_info = 0;
-#ifdef BACKGROUND_GC
-    if (settings.concurrent)
-    {
-        last_gc_info = &last_bgc_info[last_bgc_info_index];
-        assert (last_gc_info->index == settings.gc_index);
-    }
-    else
-#endif //BACKGROUND_GC
-    {
-        last_gc_info = ((settings.condemned_generation == max_generation) ?
-                        &last_full_blocking_gc_info : &last_ephemeral_gc_info);
-        last_gc_info->index = settings.gc_index;
-    }
-    size_t total_heap_committed = get_total_committed_size();
-    last_gc_info->total_committed = total_heap_committed;
-    last_gc_info->promoted = get_total_promoted();
-    last_gc_info->pinned_objects = get_total_pinned_objects();
-    last_gc_info->finalize_promoted_objects = GCHeap::GetFinalizablePromotedCount();
-    if (!settings.concurrent)
-    {
-        dynamic_data* dd = hp->dynamic_data_of (settings.condemned_generation);
-        uint64_t gc_start_ts = dd_time_clock (dd);
-        size_t pause_duration = (size_t)(end_gc_time - dd_time_clock (dd));
-#ifdef BACKGROUND_GC
-        if ((hp->current_bgc_state != bgc_initialized) && (settings.reason != reason_pm_full_gc))
-        {
-            pause_duration += (size_t)(gc_start_ts - suspended_start_time);
-        }
-#endif //BACKGROUND_GC
-        last_gc_info->pause_durations[0] = pause_duration;
-        total_suspended_time += pause_duration;
-        last_gc_info->pause_durations[1] = 0;
-    }
-    uint64_t total_process_time = end_gc_time - process_start_time;
-    last_gc_info->pause_percentage = (float)(total_process_time ?
-        ((double)total_suspended_time / (double)total_process_time * 100.0) : 0);
-    update_recorded_gen_data (last_gc_info);
-    last_gc_info->heap_size = get_total_heap_size();
-    last_gc_info->fragmentation = get_total_fragmentation();
-    if (settings.exit_memory_load != 0)
-        last_gc_info->memory_load = settings.exit_memory_load;
-    else if (settings.entry_memory_load != 0)
-        last_gc_info->memory_load = settings.entry_memory_load;
-    last_gc_info->condemned_generation = (uint8_t)settings.condemned_generation;
-    last_gc_info->compaction = settings.compaction;
-    last_gc_info->concurrent = settings.concurrent;
-#ifdef BACKGROUND_GC
-    is_last_recorded_bgc = settings.concurrent;
-#endif //BACKGROUND_GC
-#ifdef TRACE_GC
-    if (heap_hard_limit)
-    {
-        size_t total_heap_committed_recorded = current_total_committed - current_total_committed_bookkeeping;
-        dprintf (1, ("(%d)GC commit END #%zd: %zd (recorded: %zd=%zd-%zd), heap %zd, frag: %zd",
-            settings.condemned_generation,
-            (size_t)settings.gc_index, total_heap_committed, total_heap_committed_recorded,
-            current_total_committed, current_total_committed_bookkeeping,
-            last_gc_info->heap_size, last_gc_info->fragmentation));
-    }
-#endif //TRACE_GC
-    if ((settings.condemned_generation == max_generation) && (!settings.concurrent))
-    {
-        if (pm_stress_on)
-        {
-            size_t full_compacting_gc_count = full_gc_counts[gc_type_compacting];
-            if (provisional_mode_triggered)
-            {
-                uint64_t r = gc_rand::get_rand(10);
-                if ((full_compacting_gc_count - provisional_triggered_gc_count) >= r)
-                {
-                    provisional_mode_triggered = false;
-                    provisional_off_gc_count = full_compacting_gc_count;
-                    dprintf (GTC_LOG, ("%zd NGC2s when turned on, %zd NGCs since(%zd)",
-                        provisional_triggered_gc_count, (full_compacting_gc_count - provisional_triggered_gc_count),
-                        num_provisional_triggered));
-                }
-            }
-            else
-            {
-                uint64_t r = gc_rand::get_rand(5);
-                if ((full_compacting_gc_count - provisional_off_gc_count) >= r)
-                {
-                    provisional_mode_triggered = true;
-                    provisional_triggered_gc_count = full_compacting_gc_count;
-                    num_provisional_triggered++;
-                    dprintf (GTC_LOG, ("%zd NGC2s when turned off, %zd NGCs since(%zd)",
-                        provisional_off_gc_count, (full_compacting_gc_count - provisional_off_gc_count),
-                        num_provisional_triggered));
-                }
-            }
-        }
-        else
-        {
-            if (provisional_mode_triggered)
-            {
-                if ((settings.entry_memory_load < high_memory_load_th) ||
-                    !is_pm_ratio_exceeded())
-                {
-                    dprintf (GTC_LOG, ("turning off PM"));
-                    provisional_mode_triggered = false;
-                }
-            }
-            else if ((settings.entry_memory_load >= high_memory_load_th) && is_pm_ratio_exceeded())
-            {
-                dprintf (GTC_LOG, ("highmem && highfrag - turning on PM"));
-                provisional_mode_triggered = true;
-                num_provisional_triggered++;
-            }
-        }
-    }
-    if (!settings.concurrent)
-    {
-        fire_committed_usage_event ();
-    }
-    GCHeap::UpdatePostGCCounters();
-    reinit_pinned_objects();
-#ifdef STRESS_LOG
-    STRESS_LOG_GC_END(VolatileLoad(&settings.gc_index),
-                      (uint32_t)settings.condemned_generation,
-                      (uint32_t)settings.reason);
-#endif // STRESS_LOG
-#ifdef GC_CONFIG_DRIVEN
-    if (!settings.concurrent)
-    {
-        if (settings.compaction)
-            (compact_or_sweep_gcs[0])++;
-        else
-            (compact_or_sweep_gcs[1])++;
-    }
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < n_heaps; i++)
-        g_heaps[i]->record_interesting_info_per_heap();
-#else
-    record_interesting_info_per_heap();
-#endif //MULTIPLE_HEAPS
-    record_global_mechanisms();
-#endif //GC_CONFIG_DRIVEN
-    if (mark_list_overflow)
-    {
-        grow_mark_list();
-        mark_list_overflow = false;
-    }
-}
-unsigned GCHeap::GetGcCount()
-{
-    return (unsigned int)VolatileLoad(&pGenGCHeap->settings.gc_index);
-}
-size_t
-GCHeap::GarbageCollectGeneration (unsigned int gen, gc_reason reason)
-{
-    dprintf (2, ("triggered a GC!"));
-#ifdef COMMITTED_BYTES_SHADOW
-    GCHeap::RefreshMemoryLimit();
-#endif //COMMITTED_BYTES_SHADOW
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hpt = gc_heap::g_heaps[0];
-#else
-    gc_heap* hpt = 0;
-#endif //MULTIPLE_HEAPS
-    bool cooperative_mode = true;
-    dynamic_data* dd = hpt->dynamic_data_of (gen);
-    size_t localCount = dd_collection_count (dd);
-    enter_spin_lock (&gc_heap::gc_lock);
-    dprintf (SPINLOCK_LOG, ("GC Egc"));
-    ASSERT_HOLDING_SPIN_LOCK(&gc_heap::gc_lock);
-    {
-        size_t col_count = dd_collection_count (dd);
-        if (localCount != col_count)
-        {
-#ifdef SYNCHRONIZATION_STATS
-            gc_lock_contended++;
-#endif //SYNCHRONIZATION_STATS
-            dprintf (SPINLOCK_LOG, ("no need GC Lgc"));
-            leave_spin_lock (&gc_heap::gc_lock);
-            return col_count;
-         }
-    }
-    gc_heap::g_low_memory_status = (reason == reason_lowmemory) ||
-                                    (reason == reason_lowmemory_blocking) ||
-                                    (gc_heap::latency_level == latency_level_memory_footprint);
-    gc_trigger_reason = reason;
-#ifdef MULTIPLE_HEAPS
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap::g_heaps[i]->reset_gc_done();
-    }
-#else
-    gc_heap::reset_gc_done();
-#endif //MULTIPLE_HEAPS
-    gc_heap::gc_started = TRUE;
-    {
-        init_sync_log_stats();
-#ifndef MULTIPLE_HEAPS
-        cooperative_mode = gc_heap::enable_preemptive ();
-        dprintf (2, ("Suspending EE"));
-        gc_heap::suspended_start_time = GetHighPrecisionTimeStamp();
-        BEGIN_TIMING(suspend_ee_during_log);
-        GCToEEInterface::SuspendEE(SUSPEND_FOR_GC);
-        END_TIMING(suspend_ee_during_log);
-        gc_heap::proceed_with_gc_p = gc_heap::should_proceed_with_gc();
-        gc_heap::disable_preemptive (cooperative_mode);
-        if (gc_heap::proceed_with_gc_p)
-            pGenGCHeap->settings.init_mechanisms();
-        else
-            gc_heap::update_collection_counts_for_no_gc();
-#endif //!MULTIPLE_HEAPS
-    }
-    unsigned int condemned_generation_number = gen;
-    FIRE_EVENT(GCTriggered, static_cast<uint32_t>(reason));
-#ifdef MULTIPLE_HEAPS
-    GcCondemnedGeneration = condemned_generation_number;
-    cooperative_mode = gc_heap::enable_preemptive ();
-    BEGIN_TIMING(gc_during_log);
-    gc_heap::ee_suspend_event.Set();
-    gc_heap::wait_for_gc_done();
-    END_TIMING(gc_during_log);
-    gc_heap::disable_preemptive (cooperative_mode);
-    condemned_generation_number = GcCondemnedGeneration;
-#else
-    if (gc_heap::proceed_with_gc_p)
-    {
-        BEGIN_TIMING(gc_during_log);
-        pGenGCHeap->garbage_collect (condemned_generation_number);
-        if (gc_heap::pm_trigger_full_gc)
-        {
-            pGenGCHeap->garbage_collect_pm_full_gc();
-        }
-        END_TIMING(gc_during_log);
-    }
-#endif //MULTIPLE_HEAPS
-#ifndef MULTIPLE_HEAPS
-#ifdef BACKGROUND_GC
-    if (!gc_heap::dont_restart_ee_p)
-#endif //BACKGROUND_GC
-    {
-#ifdef BACKGROUND_GC
-        gc_heap::add_bgc_pause_duration_0();
-#endif //BACKGROUND_GC
-        BEGIN_TIMING(restart_ee_during_log);
-        GCToEEInterface::RestartEE(TRUE);
-        END_TIMING(restart_ee_during_log);
-    }
-#endif //!MULTIPLE_HEAPS
-#ifndef MULTIPLE_HEAPS
-    process_sync_log_stats();
-    gc_heap::gc_started = FALSE;
-    gc_heap::set_gc_done();
-    dprintf (SPINLOCK_LOG, ("GC Lgc"));
-    leave_spin_lock (&gc_heap::gc_lock);
-#endif //!MULTIPLE_HEAPS
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-    GCToEEInterface::EnableFinalization(!pGenGCHeap->settings.concurrent && pGenGCHeap->settings.found_finalizers);
-#endif // FEATURE_PREMORTEM_FINALIZATION
-    return dd_collection_count (dd);
-}
-size_t      GCHeap::GetTotalBytesInUse ()
-{
-    enter_spin_lock (&pGenGCHeap->gc_lock);
-#ifdef MULTIPLE_HEAPS
-    size_t tot_size = 0;
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        GCHeap* Hp = gc_heap::g_heaps [i]->vm_heap;
-        tot_size += Hp->ApproxTotalBytesInUse();
-    }
-#else
-    size_t tot_size = ApproxTotalBytesInUse();
-#endif //MULTIPLE_HEAPS
-    leave_spin_lock (&pGenGCHeap->gc_lock);
-    return tot_size;
-}
-uint64_t GCHeap::GetTotalAllocatedBytes()
-{
-#ifdef MULTIPLE_HEAPS
-    uint64_t total_alloc_bytes = 0;
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        gc_heap* hp = gc_heap::g_heaps[i];
-        total_alloc_bytes += hp->total_alloc_bytes_soh;
-        total_alloc_bytes += hp->total_alloc_bytes_uoh;
-    }
-    return total_alloc_bytes;
-#else
-    return (pGenGCHeap->total_alloc_bytes_soh +  pGenGCHeap->total_alloc_bytes_uoh);
-#endif //MULTIPLE_HEAPS
-}
-int GCHeap::CollectionCount (int generation, int get_bgc_fgc_count)
-{
-    if (get_bgc_fgc_count != 0)
-    {
-#ifdef BACKGROUND_GC
-        if (generation == max_generation)
-        {
-            return (int)(gc_heap::full_gc_counts[gc_type_background]);
-        }
-        else
-        {
-            return (int)(gc_heap::ephemeral_fgc_counts[generation]);
-        }
-#else
-        return 0;
-#endif //BACKGROUND_GC
-    }
-#ifdef MULTIPLE_HEAPS
-    gc_heap* hp = gc_heap::g_heaps [0];
-#else  //MULTIPLE_HEAPS
-    gc_heap* hp = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-    if (generation > max_generation)
-        return 0;
-    else
-        return (int)dd_collection_count (hp->dynamic_data_of (generation));
-}
-size_t GCHeap::ApproxTotalBytesInUse(BOOL small_heap_only)
-{
-    size_t totsize = 0;
-    generation* gen = pGenGCHeap->generation_of (0);
-    size_t gen0_frag = generation_free_list_space (gen) + generation_free_obj_space (gen);
-    uint8_t* current_alloc_allocated = pGenGCHeap->alloc_allocated;
-    heap_segment* current_eph_seg = pGenGCHeap->ephemeral_heap_segment;
-    size_t gen0_size = 0;
-#ifdef USE_REGIONS
-    heap_segment* gen0_seg = generation_start_segment (gen);
-    while (gen0_seg)
-    {
-        uint8_t* end = in_range_for_segment (current_alloc_allocated, gen0_seg) ?
-                       current_alloc_allocated : heap_segment_allocated (gen0_seg);
-        gen0_size += end - heap_segment_mem (gen0_seg);
-        if (gen0_seg == current_eph_seg)
-        {
-            break;
-        }
-        gen0_seg = heap_segment_next (gen0_seg);
-    }
-#else //USE_REGIONS
-    gen0_size = current_alloc_allocated - heap_segment_mem (current_eph_seg);
-#endif //USE_REGIONS
-    totsize = gen0_size - gen0_frag;
-    int stop_gen_index = max_generation;
-#ifdef BACKGROUND_GC
-    if (gc_heap::current_c_gc_state == c_gc_state_planning)
-    {
-        generation* oldest_gen = pGenGCHeap->generation_of (max_generation);
-        totsize = pGenGCHeap->background_soh_size_end_mark - generation_free_list_space (oldest_gen) - generation_free_obj_space (oldest_gen);
-        stop_gen_index--;
-    }
-#endif //BACKGROUND_GC
-    for (int i = (max_generation - 1); i <= stop_gen_index; i++)
-    {
-        generation* gen = pGenGCHeap->generation_of (i);
-        totsize += pGenGCHeap->generation_size (i) - generation_free_list_space (gen) - generation_free_obj_space (gen);
-    }
-    if (!small_heap_only)
-    {
-        for (int i = uoh_start_generation; i < total_generation_count; i++)
-        {
-            generation* gen = pGenGCHeap->generation_of (i);
-            totsize += pGenGCHeap->generation_size (i) - generation_free_list_space (gen) - generation_free_obj_space (gen);
-        }
-    }
-    return totsize;
-}
-#ifdef MULTIPLE_HEAPS
-void GCHeap::AssignHeap (alloc_context* acontext)
-{
-    acontext->set_alloc_heap(GetHeap(heap_select::select_heap(acontext)));
-    acontext->set_home_heap(acontext->get_alloc_heap());
-}
-GCHeap* GCHeap::GetHeap (int n)
-{
-    assert (n < gc_heap::n_heaps);
-    return gc_heap::g_heaps[n]->vm_heap;
-}
-#endif //MULTIPLE_HEAPS
-bool GCHeap::IsThreadUsingAllocationContextHeap(gc_alloc_context* context, int thread_number)
-{
-    alloc_context* acontext = static_cast<alloc_context*>(context);
-#ifdef MULTIPLE_HEAPS
-    assert (thread_number < gc_heap::n_heaps);
-    assert ((acontext->get_home_heap() == 0) ||
-            (acontext->get_home_heap()->pGenGCHeap->heap_number < gc_heap::n_heaps));
-    return ((acontext->get_home_heap() == GetHeap(thread_number)) ||
-            ((acontext->get_home_heap() == 0) && (thread_number == 0)));
-#else
-    UNREFERENCED_PARAMETER(acontext);
-    UNREFERENCED_PARAMETER(thread_number);
-    return true;
-#endif //MULTIPLE_HEAPS
-}
-int GCHeap::GetNumberOfHeaps ()
-{
-#ifdef MULTIPLE_HEAPS
-    return gc_heap::n_heaps;
-#else
-    return 1;
-#endif //MULTIPLE_HEAPS
-}
-/*
-  in this way we spend extra time cycling through all the heaps while create the handle
-  it ought to be changed by keeping alloc_context.home_heap as number (equals heap_number)
-*/
-int GCHeap::GetHomeHeapNumber ()
-{
-#ifdef MULTIPLE_HEAPS
-    gc_alloc_context* ctx = GCToEEInterface::GetAllocContext();
-    if (!ctx)
-    {
-        return 0;
-    }
-    GCHeap *hp = static_cast<alloc_context*>(ctx)->get_home_heap();
-    return (hp ? hp->pGenGCHeap->heap_number : 0);
-#else
-    return 0;
-#endif //MULTIPLE_HEAPS
-}
-unsigned int GCHeap::GetCondemnedGeneration()
-{
-    return gc_heap::settings.condemned_generation;
-}
-void GCHeap::GetMemoryInfo(uint64_t* highMemLoadThresholdBytes,
-                           uint64_t* totalAvailableMemoryBytes,
-                           uint64_t* lastRecordedMemLoadBytes,
-                           uint64_t* lastRecordedHeapSizeBytes,
-                           uint64_t* lastRecordedFragmentationBytes,
-                           uint64_t* totalCommittedBytes,
-                           uint64_t* promotedBytes,
-                           uint64_t* pinnedObjectCount,
-                           uint64_t* finalizationPendingCount,
-                           uint64_t* index,
-                           uint32_t* generation,
-                           uint32_t* pauseTimePct,
-                           bool* isCompaction,
-                           bool* isConcurrent,
-                           uint64_t* genInfoRaw,
-                           uint64_t* pauseInfoRaw,
-                           int kind)
-{
-    last_recorded_gc_info* last_gc_info = 0;
-    if ((gc_kind)kind == gc_kind_ephemeral)
-    {
-        last_gc_info = &gc_heap::last_ephemeral_gc_info;
-    }
-    else if ((gc_kind)kind == gc_kind_full_blocking)
-    {
-        last_gc_info = &gc_heap::last_full_blocking_gc_info;
-    }
-#ifdef BACKGROUND_GC
-    else if ((gc_kind)kind == gc_kind_background)
-    {
-        last_gc_info = gc_heap::get_completed_bgc_info();
-    }
-#endif //BACKGROUND_GC
-    else
-    {
-        assert ((gc_kind)kind == gc_kind_any);
-#ifdef BACKGROUND_GC
-        if (gc_heap::is_last_recorded_bgc)
-        {
-            last_gc_info = gc_heap::get_completed_bgc_info();
-        }
-        else
-#endif //BACKGROUND_GC
-        {
-            last_gc_info = ((gc_heap::last_ephemeral_gc_info.index > gc_heap::last_full_blocking_gc_info.index) ?
-                &gc_heap::last_ephemeral_gc_info : &gc_heap::last_full_blocking_gc_info);
-        }
-    }
-    *highMemLoadThresholdBytes = (uint64_t) (((double)(gc_heap::high_memory_load_th)) / 100 * gc_heap::total_physical_mem);
-    *totalAvailableMemoryBytes = gc_heap::heap_hard_limit != 0 ? gc_heap::heap_hard_limit : gc_heap::total_physical_mem;
-    *lastRecordedMemLoadBytes = (uint64_t) (((double)(last_gc_info->memory_load)) / 100 * gc_heap::total_physical_mem);
-    *lastRecordedHeapSizeBytes = last_gc_info->heap_size;
-    *lastRecordedFragmentationBytes = last_gc_info->fragmentation;
-    *totalCommittedBytes = last_gc_info->total_committed;
-    *promotedBytes = last_gc_info->promoted;
-    *pinnedObjectCount = last_gc_info->pinned_objects;
-    *finalizationPendingCount = last_gc_info->finalize_promoted_objects;
-    *index = last_gc_info->index;
-    *generation = last_gc_info->condemned_generation;
-    *pauseTimePct = (int)(last_gc_info->pause_percentage * 100);
-    *isCompaction = last_gc_info->compaction;
-    *isConcurrent = last_gc_info->concurrent;
-    int genInfoIndex = 0;
-    for (int i = 0; i < total_generation_count; i++)
-    {
-        genInfoRaw[genInfoIndex++] = last_gc_info->gen_info[i].size_before;
-        genInfoRaw[genInfoIndex++] = last_gc_info->gen_info[i].fragmentation_before;
-        genInfoRaw[genInfoIndex++] = last_gc_info->gen_info[i].size_after;
-        genInfoRaw[genInfoIndex++] = last_gc_info->gen_info[i].fragmentation_after;
-    }
-    for (int i = 0; i < 2; i++)
-    {
-        pauseInfoRaw[i] = (uint64_t)(last_gc_info->pause_durations[i]) * 10;
-    }
-#ifdef _DEBUG
-    if (VolatileLoadWithoutBarrier (&last_gc_info->index) != 0)
-    {
-        if ((gc_kind)kind == gc_kind_ephemeral)
-        {
-            assert (last_gc_info->condemned_generation < max_generation);
-        }
-        else if ((gc_kind)kind == gc_kind_full_blocking)
-        {
-            assert (last_gc_info->condemned_generation == max_generation);
-            assert (last_gc_info->concurrent == false);
-        }
-#ifdef BACKGROUND_GC
-        else if ((gc_kind)kind == gc_kind_background)
-        {
-            assert (last_gc_info->condemned_generation == max_generation);
-            assert (last_gc_info->concurrent == true);
-        }
-#endif //BACKGROUND_GC
-    }
-#endif //_DEBUG
-}
-int64_t GCHeap::GetTotalPauseDuration()
-{
-    return (int64_t)(gc_heap::total_suspended_time * 10);
-}
-void GCHeap::EnumerateConfigurationValues(void* context, ConfigurationValueFunc configurationValueFunc)
-{
-    GCConfig::EnumerateConfigurationValues(context, configurationValueFunc);
-}
-uint32_t GCHeap::GetMemoryLoad()
-{
-    uint32_t memory_load = 0;
-    if (gc_heap::settings.exit_memory_load != 0)
-        memory_load = gc_heap::settings.exit_memory_load;
-    else if (gc_heap::settings.entry_memory_load != 0)
-        memory_load = gc_heap::settings.entry_memory_load;
-    return memory_load;
-}
-int GCHeap::GetGcLatencyMode()
-{
-    return (int)(pGenGCHeap->settings.pause_mode);
-}
-int GCHeap::SetGcLatencyMode (int newLatencyMode)
-{
-    if (gc_heap::settings.pause_mode == pause_no_gc)
-        return (int)set_pause_mode_no_gc;
-    gc_pause_mode new_mode = (gc_pause_mode)newLatencyMode;
-    if (new_mode == pause_low_latency)
-    {
-#ifndef MULTIPLE_HEAPS
-        pGenGCHeap->settings.pause_mode = new_mode;
-#endif //!MULTIPLE_HEAPS
-    }
-    else if (new_mode == pause_sustained_low_latency)
-    {
-#ifdef BACKGROUND_GC
-        if (gc_heap::gc_can_use_concurrent)
-        {
-            pGenGCHeap->settings.pause_mode = new_mode;
-        }
-#endif //BACKGROUND_GC
-    }
-    else
-    {
-        pGenGCHeap->settings.pause_mode = new_mode;
-    }
-#ifdef BACKGROUND_GC
-    if (gc_heap::background_running_p())
-    {
-        if (gc_heap::saved_bgc_settings.pause_mode != new_mode)
-        {
-            gc_heap::saved_bgc_settings.pause_mode = new_mode;
-        }
-    }
-#endif //BACKGROUND_GC
-    return (int)set_pause_mode_success;
-}
-int GCHeap::GetLOHCompactionMode()
-{
-#ifdef FEATURE_LOH_COMPACTION
-    return pGenGCHeap->loh_compaction_mode;
-#else
-    return loh_compaction_default;
-#endif //FEATURE_LOH_COMPACTION
-}
-void GCHeap::SetLOHCompactionMode (int newLOHCompactionMode)
-{
-#ifdef FEATURE_LOH_COMPACTION
-    pGenGCHeap->loh_compaction_mode = (gc_loh_compaction_mode)newLOHCompactionMode;
-#endif //FEATURE_LOH_COMPACTION
-}
-bool GCHeap::RegisterForFullGCNotification(uint32_t gen2Percentage,
-                                           uint32_t lohPercentage)
-{
-#ifdef MULTIPLE_HEAPS
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps [hn];
-        hp->fgn_last_alloc = dd_new_allocation (hp->dynamic_data_of (0));
-        hp->fgn_maxgen_percent = gen2Percentage;
-    }
-#else //MULTIPLE_HEAPS
-    pGenGCHeap->fgn_last_alloc = dd_new_allocation (pGenGCHeap->dynamic_data_of (0));
-    pGenGCHeap->fgn_maxgen_percent = gen2Percentage;
-#endif //MULTIPLE_HEAPS
-    pGenGCHeap->full_gc_approach_event.Reset();
-    pGenGCHeap->full_gc_end_event.Reset();
-    pGenGCHeap->full_gc_approach_event_set = false;
-    pGenGCHeap->fgn_loh_percent = lohPercentage;
-    return TRUE;
-}
-bool GCHeap::CancelFullGCNotification()
-{
-#ifdef MULTIPLE_HEAPS
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps [hn];
-        hp->fgn_maxgen_percent = 0;
-    }
-#else //MULTIPLE_HEAPS
-    pGenGCHeap->fgn_maxgen_percent = 0;
-#endif //MULTIPLE_HEAPS
-    pGenGCHeap->fgn_loh_percent = 0;
-    pGenGCHeap->full_gc_approach_event.Set();
-    pGenGCHeap->full_gc_end_event.Set();
-    return TRUE;
-}
-int GCHeap::WaitForFullGCApproach(int millisecondsTimeout)
-{
-    dprintf (2, ("WFGA: Begin wait"));
-    int result = gc_heap::full_gc_wait (&(pGenGCHeap->full_gc_approach_event), millisecondsTimeout);
-    dprintf (2, ("WFGA: End wait"));
-    return result;
-}
-int GCHeap::WaitForFullGCComplete(int millisecondsTimeout)
-{
-    dprintf (2, ("WFGE: Begin wait"));
-    int result = gc_heap::full_gc_wait (&(pGenGCHeap->full_gc_end_event), millisecondsTimeout);
-    dprintf (2, ("WFGE: End wait"));
-    return result;
-}
-int GCHeap::StartNoGCRegion(uint64_t totalSize, bool lohSizeKnown, uint64_t lohSize, bool disallowFullBlockingGC)
-{
-    NoGCRegionLockHolder lh;
-    dprintf (1, ("begin no gc called"));
-    start_no_gc_region_status status = gc_heap::prepare_for_no_gc_region (totalSize, lohSizeKnown, lohSize, disallowFullBlockingGC);
-    if (status == start_no_gc_success)
-    {
-        GarbageCollect (max_generation);
-        status = gc_heap::get_start_no_gc_region_status();
-    }
-    if (status != start_no_gc_success)
-        gc_heap::handle_failure_for_no_gc();
-    return (int)status;
-}
-int GCHeap::EndNoGCRegion()
-{
-    NoGCRegionLockHolder lh;
-    return (int)gc_heap::end_no_gc_region();
-}
-void GCHeap::PublishObject (uint8_t* Obj)
-{
-#ifdef BACKGROUND_GC
-    gc_heap* hp = gc_heap::heap_of (Obj);
-    hp->bgc_alloc_lock->uoh_alloc_done (Obj);
-    hp->bgc_untrack_uoh_alloc();
-#endif //BACKGROUND_GC
-}
-size_t GCHeap::ApproxFreeBytes()
-{
-    enter_spin_lock (&pGenGCHeap->gc_lock);
-    generation* gen = pGenGCHeap->generation_of (0);
-    size_t res = generation_allocation_limit (gen) - generation_allocation_pointer (gen);
-    leave_spin_lock (&pGenGCHeap->gc_lock);
-    return res;
-}
-HRESULT GCHeap::GetGcCounters(int gen, gc_counters* counters)
-{
-    if ((gen < 0) || (gen > max_generation))
-        return E_FAIL;
-#ifdef MULTIPLE_HEAPS
-    counters->current_size = 0;
-    counters->promoted_size = 0;
-    counters->collection_count = 0;
-    for (int i = 0; i < gc_heap::n_heaps; i++)
-    {
-        dynamic_data* dd = gc_heap::g_heaps [i]->dynamic_data_of (gen);
-        counters->current_size += dd_current_size (dd);
-        counters->promoted_size += dd_promoted_size (dd);
-        if (i == 0)
-        counters->collection_count += dd_collection_count (dd);
-    }
-#else
-    dynamic_data* dd = pGenGCHeap->dynamic_data_of (gen);
-    counters->current_size = dd_current_size (dd);
-    counters->promoted_size = dd_promoted_size (dd);
-    counters->collection_count = dd_collection_count (dd);
-#endif //MULTIPLE_HEAPS
-    return S_OK;
-}
-size_t GCHeap::GetValidSegmentSize(bool large_seg)
-{
-#ifdef USE_REGIONS
-    return (large_seg ? global_region_allocator.get_large_region_alignment() :
-                        global_region_allocator.get_region_alignment());
-#else
-    return (large_seg ? gc_heap::min_uoh_segment_size : gc_heap::soh_segment_size);
-#endif //USE_REGIONS
-}
-size_t gc_heap::get_gen0_min_size()
-{
-    size_t gen0size = static_cast<size_t>(GCConfig::GetGen0Size());
-    bool is_config_invalid = ((gen0size == 0) || !g_theGCHeap->IsValidGen0MaxSize(gen0size));
-    if (is_config_invalid)
-    {
-#ifdef SERVER_GC
-        gen0size = max(GCToOSInterface::GetCacheSizePerLogicalCpu(FALSE),(256*1024));
-        size_t trueSize = max(GCToOSInterface::GetCacheSizePerLogicalCpu(TRUE),(256*1024));
-        dprintf (1, ("cache: %zd-%zd",
-            GCToOSInterface::GetCacheSizePerLogicalCpu(FALSE),
-            GCToOSInterface::GetCacheSizePerLogicalCpu(TRUE)));
-        int n_heaps = gc_heap::n_heaps;
-#else //SERVER_GC
-        size_t trueSize = GCToOSInterface::GetCacheSizePerLogicalCpu(TRUE);
-        gen0size = max((4*trueSize/5),(256*1024));
-        trueSize = max(trueSize, (256*1024));
-        int n_heaps = 1;
-#endif //SERVER_GC
-#ifdef DYNAMIC_HEAP_COUNT
-        if (dynamic_adaptation_mode == dynamic_adaptation_to_application_sizes)
-        {
-            gen0size = min (gen0size, (4*1024*1024));
-        }
-#endif //DYNAMIC_HEAP_COUNT
-        dprintf (1, ("gen0size: %zd * %d = %zd, physical mem: %zd / 6 = %zd",
-                gen0size, n_heaps, (gen0size * n_heaps),
-                gc_heap::total_physical_mem,
-                gc_heap::total_physical_mem / 6));
-        while ((gen0size * n_heaps) > (gc_heap::total_physical_mem / 6))
-        {
-            gen0size = gen0size / 2;
-            if (gen0size <= trueSize)
-            {
-                gen0size = trueSize;
-                break;
-            }
-        }
-    }
-#ifdef FEATURE_EVENT_TRACE
-    else
-    {
-        gen0_min_budget_from_config = gen0size;
-    }
-#endif //FEATURE_EVENT_TRACE
-    size_t seg_size = gc_heap::soh_segment_size;
-    assert (seg_size);
-    if (gen0size >= (seg_size / 2))
-        gen0size = seg_size / 2;
-    if (is_config_invalid)
-    {
-        if (heap_hard_limit)
-        {
-            size_t gen0size_seg = seg_size / 8;
-            if (gen0size >= gen0size_seg)
-            {
-                dprintf (1, ("gen0 limited by seg size %zd->%zd", gen0size, gen0size_seg));
-                gen0size = gen0size_seg;
-            }
-        }
-        gen0size = gen0size / 8 * 5;
-    }
-#ifdef USE_REGIONS
-#ifdef STRESS_REGIONS
-    gen0size = ((size_t)1 << min_segment_size_shr) * 3;
-#endif //STRESS_REGIONS
-#endif //USE_REGIONS
-    gen0size = Align (gen0size);
-    return gen0size;
-}
-void GCHeap::SetReservedVMLimit (size_t vmlimit)
-{
-    gc_heap::reserved_memory_limit = vmlimit;
-}
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-Object* GCHeap::GetNextFinalizableObject()
-{
-#ifdef MULTIPLE_HEAPS
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps [hn];
-        Object* O = hp->finalize_queue->GetNextFinalizableObject(TRUE);
-        if (O)
-            return O;
-    }
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps [hn];
-        Object* O = hp->finalize_queue->GetNextFinalizableObject(FALSE);
-        if (O)
-            return O;
-    }
-    return 0;
-#else //MULTIPLE_HEAPS
-    return pGenGCHeap->finalize_queue->GetNextFinalizableObject();
-#endif //MULTIPLE_HEAPS
-}
-size_t GCHeap::GetNumberFinalizableObjects()
-{
-#ifdef MULTIPLE_HEAPS
-    size_t cnt = 0;
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps [hn];
-        cnt += hp->finalize_queue->GetNumberFinalizableObjects();
-    }
-    return cnt;
-#else //MULTIPLE_HEAPS
-    return pGenGCHeap->finalize_queue->GetNumberFinalizableObjects();
-#endif //MULTIPLE_HEAPS
-}
-size_t GCHeap::GetFinalizablePromotedCount()
-{
-#ifdef MULTIPLE_HEAPS
-    size_t cnt = 0;
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps [hn];
-        cnt += hp->finalize_queue->GetPromotedCount();
-    }
-    return cnt;
-#else //MULTIPLE_HEAPS
-    return pGenGCHeap->finalize_queue->GetPromotedCount();
-#endif //MULTIPLE_HEAPS
-}
-bool GCHeap::RegisterForFinalization (int gen, Object* obj)
-{
-    if (gen == -1)
-        gen = 0;
-    if (((((CObjectHeader*)obj)->GetHeader()->GetBits()) & BIT_SBLK_FINALIZER_RUN))
-    {
-        ((CObjectHeader*)obj)->GetHeader()->ClrBit(BIT_SBLK_FINALIZER_RUN);
-        return true;
-    }
-    else
-    {
-        gc_heap* hp = gc_heap::heap_of ((uint8_t*)obj);
-        return hp->finalize_queue->RegisterForFinalization (gen, obj);
-    }
-}
-void GCHeap::SetFinalizationRun (Object* obj)
-{
-    ((CObjectHeader*)obj)->GetHeader()->SetBit(BIT_SBLK_FINALIZER_RUN);
-}
-inline
-unsigned int gen_segment (int gen)
-{
-    assert (((signed)total_generation_count - gen - 1)>=0);
-    return (total_generation_count - gen - 1);
-}
-bool CFinalize::Initialize()
-{
-    CONTRACTL {
-        NOTHROW;
-        GC_NOTRIGGER;
-    } CONTRACTL_END;
-    m_Array = new (nothrow)(Object*[100]);
-    if (!m_Array)
-    {
-        ASSERT (m_Array);
-        STRESS_LOG_OOM_STACK(sizeof(Object*[100]));
-        if (GCConfig::GetBreakOnOOM())
-        {
-            GCToOSInterface::DebugBreak();
-        }
-        return false;
-    }
-    m_EndArray = &m_Array[100];
-    for (int i =0; i < FreeList; i++)
-    {
-        SegQueueLimit (i) = m_Array;
-    }
-    m_PromotedCount = 0;
-    lock = -1;
-#ifdef _DEBUG
-    lockowner_threadid.Clear();
-#endif // _DEBUG
-    return true;
-}
-CFinalize::~CFinalize()
-{
-    delete m_Array;
-}
-size_t CFinalize::GetPromotedCount ()
-{
-    return m_PromotedCount;
-}
-inline
-void CFinalize::EnterFinalizeLock()
-{
-    _ASSERTE(dbgOnly_IsSpecialEEThread() ||
-             GCToEEInterface::GetThread() == 0 ||
-             GCToEEInterface::IsPreemptiveGCDisabled());
-retry:
-    if (Interlocked::CompareExchange(&lock, 0, -1) >= 0)
-    {
-        unsigned int i = 0;
-        while (lock >= 0)
-        {
-            if (g_num_processors > 1)
-            {
-                int spin_count = 128 * yp_spin_count_unit;
-                for (int j = 0; j < spin_count; j++)
-                {
-                    if (lock < 0)
-                        break;
-                    YieldProcessor ();           // indicate to the processor that we are spinning
-                }
-            }
-            if (lock < 0)
-                break;
-            if (++i & 7)
-                GCToOSInterface::YieldThread (0);
-            else
-                GCToOSInterface::Sleep (5);
-        }
-        goto retry;
-    }
-#ifdef _DEBUG
-    lockowner_threadid.SetToCurrentThread();
-#endif // _DEBUG
-}
-inline
-void CFinalize::LeaveFinalizeLock()
-{
-    _ASSERTE(dbgOnly_IsSpecialEEThread() ||
-             GCToEEInterface::GetThread() == 0 ||
-             GCToEEInterface::IsPreemptiveGCDisabled());
-#ifdef _DEBUG
-    lockowner_threadid.Clear();
-#endif // _DEBUG
-    lock = -1;
-}
-bool
-CFinalize::RegisterForFinalization (int gen, Object* obj, size_t size)
-{
-    CONTRACTL {
-        NOTHROW;
-        GC_NOTRIGGER;
-    } CONTRACTL_END;
-    EnterFinalizeLock();
-    unsigned int dest = gen_segment (gen);
-    Object*** s_i = &SegQueue (FreeList);
-    if ((*s_i) == m_EndArray)
-    {
-        if (!GrowArray())
-        {
-            LeaveFinalizeLock();
-            if (method_table(obj) == NULL)
-            {
-                assert (size >= Align (min_obj_size));
-                dprintf (3, (ThreadStressLog::gcMakeUnusedArrayMsg(), (size_t)obj, (size_t)(obj+size)));
-                ((CObjectHeader*)obj)->SetFree(size);
-            }
-            STRESS_LOG_OOM_STACK(0);
-            if (GCConfig::GetBreakOnOOM())
-            {
-                GCToOSInterface::DebugBreak();
-            }
-            return false;
-        }
-    }
-    Object*** end_si = &SegQueueLimit (dest);
-    do
-    {
-        if (!(*s_i == *(s_i-1)))
-        {
-            *(*s_i) = *(*(s_i-1));
-        }
-        (*s_i)++;
-        s_i--;
-    } while (s_i > end_si);
-    **s_i = obj;
-    (*s_i)++;
-    LeaveFinalizeLock();
-    return true;
-}
-Object*
-CFinalize::GetNextFinalizableObject (BOOL only_non_critical)
-{
-    Object* obj = 0;
-    EnterFinalizeLock();
-    if (!IsSegEmpty(FinalizerListSeg))
-    {
-        obj =  *(--SegQueueLimit (FinalizerListSeg));
-    }
-    else if (!only_non_critical && !IsSegEmpty(CriticalFinalizerListSeg))
-    {
-        obj =  *(--SegQueueLimit (CriticalFinalizerListSeg));
-        --SegQueueLimit (FinalizerListSeg);
-    }
-    if (obj)
-    {
-        dprintf (3, ("running finalizer for %p (mt: %p)", obj, method_table (obj)));
-    }
-    LeaveFinalizeLock();
-    return obj;
-}
-size_t
-CFinalize::GetNumberFinalizableObjects()
-{
-    return SegQueueLimit(FinalizerListSeg) - SegQueue(FinalizerListSeg);
-}
-void
-CFinalize::MoveItem (Object** fromIndex,
-                     unsigned int fromSeg,
-                     unsigned int toSeg)
-{
-    int step;
-    ASSERT (fromSeg != toSeg);
-    if (fromSeg > toSeg)
-        step = -1;
-    else
-        step = +1;
-    Object** srcIndex = fromIndex;
-    for (unsigned int i = fromSeg; i != toSeg; i+= step)
-    {
-        Object**& destFill = m_FillPointers[i+(step - 1 )/2];
-        Object** destIndex = destFill - (step + 1)/2;
-        if (srcIndex != destIndex)
-        {
-            Object* tmp = *srcIndex;
-            *srcIndex = *destIndex;
-            *destIndex = tmp;
-        }
-        destFill -= step;
-        srcIndex = destIndex;
-    }
-}
-void
-CFinalize::GcScanRoots (promote_func* fn, int hn, ScanContext *pSC)
-{
-    ScanContext sc;
-    if (pSC == 0)
-        pSC = &sc;
-    pSC->thread_number = hn;
-    Object** startIndex  = SegQueue (CriticalFinalizerListSeg);
-    Object** stopIndex  = SegQueueLimit (FinalizerListSeg);
-    for (Object** po = startIndex; po < stopIndex; po++)
-    {
-        Object* o = *po;
-        dprintf (3, ("scan f %zx", (size_t)o));
-        (*fn)(po, pSC, 0);
-    }
-}
-void CFinalize::WalkFReachableObjects (fq_walk_fn fn)
-{
-    Object** startIndex = SegQueue (CriticalFinalizerListSeg);
-    Object** stopCriticalIndex = SegQueueLimit (CriticalFinalizerListSeg);
-    Object** stopIndex  = SegQueueLimit (FinalizerListSeg);
-    for (Object** po = startIndex; po < stopIndex; po++)
-    {
-        fn(po < stopCriticalIndex, *po);
-    }
-}
-BOOL
-CFinalize::ScanForFinalization (promote_func* pfn, int gen, BOOL mark_only_p,
-                                gc_heap* hp)
-{
-    ScanContext sc;
-    sc.promotion = TRUE;
-#ifdef MULTIPLE_HEAPS
-    sc.thread_number = hp->heap_number;
-    sc.thread_count = gc_heap::n_heaps;
-#else
-    UNREFERENCED_PARAMETER(hp);
-    sc.thread_count = 1;
-#endif //MULTIPLE_HEAPS
-    BOOL finalizedFound = FALSE;
-    unsigned int startSeg = gen_segment (gen);
-    {
-        m_PromotedCount = 0;
-        for (unsigned int Seg = startSeg; Seg <= gen_segment(0); Seg++)
-        {
-            Object** endIndex = SegQueue (Seg);
-            for (Object** i = SegQueueLimit (Seg)-1; i >= endIndex ;i--)
-            {
-                CObjectHeader* obj = (CObjectHeader*)*i;
-                dprintf (3, ("scanning: %zx", (size_t)obj));
-                if (!g_theGCHeap->IsPromoted (obj))
-                {
-                    dprintf (3, ("freacheable: %zx", (size_t)obj));
-                    assert (method_table(obj)->HasFinalizer());
-                    if (GCToEEInterface::EagerFinalized(obj))
-                    {
-                        MoveItem (i, Seg, FreeList);
-                    }
-                    else if ((obj->GetHeader()->GetBits()) & BIT_SBLK_FINALIZER_RUN)
-                    {
-                        MoveItem (i, Seg, FreeList);
-                        obj->GetHeader()->ClrBit (BIT_SBLK_FINALIZER_RUN);
-                    }
-                    else
-                    {
-                        m_PromotedCount++;
-                        if (method_table(obj)->HasCriticalFinalizer())
-                        {
-                            MoveItem (i, Seg, CriticalFinalizerListSeg);
-                        }
-                        else
-                        {
-                            MoveItem (i, Seg, FinalizerListSeg);
-                        }
-                    }
-                }
-#ifdef BACKGROUND_GC
-                else
-                {
-                    if ((gen == max_generation) && (gc_heap::background_running_p()))
-                    {
-                        dprintf (3, ("%zx is marked", (size_t)obj));
-                    }
-                }
-#endif //BACKGROUND_GC
-            }
-        }
-    }
-    finalizedFound = !IsSegEmpty(FinalizerListSeg) ||
-                     !IsSegEmpty(CriticalFinalizerListSeg);
-    if (finalizedFound)
-    {
-        GcScanRoots (pfn,
-#ifdef MULTIPLE_HEAPS
-                     hp->heap_number
-#else
-                     0
-#endif //MULTIPLE_HEAPS
-                     , 0);
-        hp->settings.found_finalizers = TRUE;
-#ifdef BACKGROUND_GC
-        if (hp->settings.concurrent)
-        {
-            hp->settings.found_finalizers = !(IsSegEmpty(FinalizerListSeg) && IsSegEmpty(CriticalFinalizerListSeg));
-        }
-#endif //BACKGROUND_GC
-        if (hp->settings.concurrent && hp->settings.found_finalizers)
-        {
-            if (!mark_only_p)
-                GCToEEInterface::EnableFinalization(true);
-        }
-    }
-    return finalizedFound;
-}
-void
-CFinalize::RelocateFinalizationData (int gen, gc_heap* hp)
-{
-    ScanContext sc;
-    sc.promotion = FALSE;
-#ifdef MULTIPLE_HEAPS
-    sc.thread_number = hp->heap_number;
-    sc.thread_count = gc_heap::n_heaps;
-#else
-    UNREFERENCED_PARAMETER(hp);
-    sc.thread_count = 1;
-#endif //MULTIPLE_HEAPS
-    unsigned int Seg = gen_segment (gen);
-    Object** startIndex = SegQueue (Seg);
-    dprintf (3, ("RelocateFinalizationData gen=%d, [%p,%p[", gen, startIndex, SegQueue (FreeList)));
-    for (Object** po = startIndex; po < SegQueue (FreeList);po++)
-    {
-        GCHeap::Relocate (po, &sc);
-    }
-}
-void
-CFinalize::UpdatePromotedGenerations (int gen, BOOL gen_0_empty_p)
-{
-    dprintf(3, ("UpdatePromotedGenerations gen=%d, gen_0_empty_p=%d", gen, gen_0_empty_p));
-    if (gen_0_empty_p)
-    {
-        for (int i = min (gen+1, max_generation); i > 0; i--)
-        {
-            m_FillPointers [gen_segment(i)] = m_FillPointers [gen_segment(i-1)];
-        }
-    }
-    else
-    {
-        for (int i = gen; i >= 0; i--)
-        {
-            unsigned int Seg = gen_segment (i);
-            Object** startIndex = SegQueue (Seg);
-            for (Object** po = startIndex;
-                 po < SegQueueLimit (gen_segment(i)); po++)
-            {
-                int new_gen = g_theGCHeap->WhichGeneration (*po);
-                if (new_gen != i)
-                {
-                    assert (new_gen <= max_generation);
-                    dprintf (3, ("Moving object %p->%p from gen %d to gen %d", po, *po, i, new_gen));
-                    if (new_gen > i)
-                    {
-                        MoveItem (po, gen_segment (i), gen_segment (new_gen));
-                    }
-                    else
-                    {
-                        MoveItem (po, gen_segment (i), gen_segment (new_gen));
-                        po--;
-                    }
-                }
-            }
-        }
-    }
-}
-BOOL
-CFinalize::GrowArray()
-{
-    size_t oldArraySize = (m_EndArray - m_Array);
-    size_t newArraySize =  (size_t)(((float)oldArraySize / 10) * 12);
-    Object** newArray = new (nothrow) Object*[newArraySize];
-    if (!newArray)
-    {
-        return FALSE;
-    }
-    memcpy (newArray, m_Array, oldArraySize*sizeof(Object*));
-    dprintf (3, ("Grow finalizer array [%p,%p[ -> [%p,%p[", m_Array, m_EndArray, newArray, &m_Array[newArraySize]));
-    for (int i = 0; i < FreeList; i++)
-    {
-        m_FillPointers [i] += (newArray - m_Array);
-    }
-    delete[] m_Array;
-    m_Array = newArray;
-    m_EndArray = &m_Array [newArraySize];
-    return TRUE;
-}
-bool CFinalize::MergeFinalizationData (CFinalize* other_fq)
-{
-    size_t otherNeededArraySize = other_fq->SegQueue (FreeList) - other_fq->m_Array;
-    if (otherNeededArraySize == 0)
-    {
-        return true;
-    }
-    size_t thisArraySize = (m_EndArray - m_Array);
-    size_t thisNeededArraySize = SegQueue (FreeList) - m_Array;
-    size_t neededArraySize = thisNeededArraySize + otherNeededArraySize;
-    Object ** newArray = m_Array;
-    if (thisArraySize < neededArraySize)
-    {
-        newArray = new (nothrow) Object*[neededArraySize];
-        if (!newArray)
-        {
-            dprintf (3, ("ran out of space merging finalization data"));
-            return false;
-        }
-    }
-    for (int i = FreeList - 1; i >= 0; i--)
-    {
-        size_t thisIndex = SegQueue (i) - m_Array;
-        size_t otherIndex = other_fq->SegQueue (i) - other_fq->m_Array;
-        size_t thisLimit = SegQueueLimit (i) - m_Array;
-        size_t otherLimit = other_fq->SegQueueLimit (i) - other_fq->m_Array;
-        size_t thisSize = thisLimit - thisIndex;
-        size_t otherSize = otherLimit - otherIndex;
-        memmove (&newArray[thisIndex + otherIndex],           &m_Array[thisIndex ], sizeof(newArray[0])*thisSize );
-        memmove (&newArray[thisLimit + otherIndex], &other_fq->m_Array[otherIndex], sizeof(newArray[0])*otherSize);
-    }
-    for (int i = FreeList - 1; i >= 0; i--)
-    {
-        size_t thisLimit = SegQueueLimit (i) - m_Array;
-        size_t otherLimit = other_fq->SegQueueLimit (i) - other_fq->m_Array;
-        SegQueueLimit (i) = &newArray[thisLimit + otherLimit];
-        other_fq->SegQueueLimit (i) = other_fq->m_Array;
-    }
-    if (m_Array != newArray)
-    {
-        delete[] m_Array;
-        m_Array = newArray;
-        m_EndArray = &m_Array [neededArraySize];
-    }
-    return true;
-}
-bool CFinalize::SplitFinalizationData (CFinalize* other_fq)
-{
-    size_t otherCurrentArraySize = other_fq->SegQueue (FreeList) - other_fq->m_Array;
-    assert (otherCurrentArraySize == 0);
-    size_t thisCurrentArraySize = SegQueue (FreeList) - m_Array;
-    if (thisCurrentArraySize == 0)
-    {
-        return true;
-    }
-    size_t otherNeededArraySize = thisCurrentArraySize / 2;
-    size_t otherArraySize = other_fq->m_EndArray - other_fq->m_Array;
-    if (otherArraySize < otherNeededArraySize)
-    {
-        Object ** newArray = new (nothrow) Object*[otherNeededArraySize];
-        if (!newArray)
-        {
-            return false;
-        }
-        delete[] other_fq->m_Array;
-        other_fq->m_Array = newArray;
-        other_fq->m_EndArray = &other_fq->m_Array[otherNeededArraySize];
-    }
-    PTR_PTR_Object newFillPointers[FreeList];
-    PTR_PTR_Object segQueue = m_Array;
-    for (int i = 0; i < FreeList; i++)
-    {
-        size_t thisIndex = SegQueue (i) - m_Array;
-        size_t thisLimit = SegQueueLimit (i) - m_Array;
-        size_t thisSize = thisLimit - thisIndex;
-        size_t otherSize = thisSize / 2;
-        size_t otherIndex = other_fq->SegQueue (i) - other_fq->m_Array;
-        size_t thisNewSize = thisSize - otherSize;
-        memmove (&other_fq->m_Array[otherIndex], &m_Array[thisIndex + thisNewSize], sizeof(other_fq->m_Array[0])*otherSize);
-        other_fq->SegQueueLimit (i) = &other_fq->m_Array[otherIndex + otherSize];
-        memmove (segQueue, &m_Array[thisIndex], sizeof(m_Array[0])*thisNewSize);
-        segQueue += thisNewSize;
-        newFillPointers[i] = segQueue;
-    }
-    for (int i = 0; i < FreeList; i++)
-    {
-        m_FillPointers[i] = newFillPointers[i];
-    }
-    return true;
-}
-#ifdef VERIFY_HEAP
-void CFinalize::CheckFinalizerObjects()
-{
-    for (int i = 0; i <= max_generation; i++)
-    {
-        Object **startIndex = SegQueue (gen_segment (i));
-        Object **stopIndex  = SegQueueLimit (gen_segment (i));
-        for (Object **po = startIndex; po < stopIndex; po++)
-        {
-            if ((int)g_theGCHeap->WhichGeneration (*po) < i)
-                FATAL_GC_ERROR ();
-            ((CObjectHeader*)*po)->Validate();
-        }
-    }
-}
-#endif //VERIFY_HEAP
-#endif // FEATURE_PREMORTEM_FINALIZATION
-void gc_heap::walk_heap_per_heap (walk_fn fn, void* context, int gen_number, BOOL walk_large_object_heap_p)
-{
-    generation* gen = gc_heap::generation_of (gen_number);
-    heap_segment*    seg = generation_start_segment (gen);
-    uint8_t* x = ((gen_number == max_generation) ? heap_segment_mem (seg) : get_soh_start_object (seg, gen));
-    uint8_t*       end = heap_segment_allocated (seg);
-    int align_const = get_alignment_constant (TRUE);
-    BOOL walk_pinned_object_heap = walk_large_object_heap_p;
-    while (1)
-    {
-        if (x >= end)
-        {
-            if ((seg = heap_segment_next (seg)) != 0)
-            {
-                x = heap_segment_mem (seg);
-                end = heap_segment_allocated (seg);
-                continue;
-            }
-#ifdef USE_REGIONS
-            else if (gen_number > 0)
-            {
-                gen_number--;
-                gen = gc_heap::generation_of (gen_number);
-                seg = generation_start_segment (gen);
-                x = heap_segment_mem (seg);
-                end = heap_segment_allocated (seg);
-                continue;
-            }
-#endif // USE_REGIONS
-            else
-            {
-                if (walk_large_object_heap_p)
-                {
-                    walk_large_object_heap_p = FALSE;
-                    seg = generation_start_segment (large_object_generation);
-                }
-                else if (walk_pinned_object_heap)
-                {
-                    walk_pinned_object_heap = FALSE;
-                    seg = generation_start_segment (pinned_object_generation);
-                }
-                else
-                {
-                    break;
-                }
-                align_const = get_alignment_constant (FALSE);
-                x = heap_segment_mem (seg);
-                end = heap_segment_allocated (seg);
-                continue;
-            }
-        }
-        size_t s = size (x);
-        CObjectHeader* o = (CObjectHeader*)x;
-        if (!o->IsFree())
-        {
-            _ASSERTE(((size_t)o & 0x3) == 0); // Last two bits should never be set at this point
-            if (!fn (o->GetObjectBase(), context))
-                return;
-        }
-        x = x + Align (s, align_const);
-    }
-}
-void gc_heap::walk_finalize_queue (fq_walk_fn fn)
-{
-#ifdef FEATURE_PREMORTEM_FINALIZATION
-    finalize_queue->WalkFReachableObjects (fn);
-#endif //FEATURE_PREMORTEM_FINALIZATION
-}
-void gc_heap::walk_heap (walk_fn fn, void* context, int gen_number, BOOL walk_large_object_heap_p)
-{
-#ifdef MULTIPLE_HEAPS
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps [hn];
-        hp->walk_heap_per_heap (fn, context, gen_number, walk_large_object_heap_p);
-    }
-#else
-    walk_heap_per_heap(fn, context, gen_number, walk_large_object_heap_p);
-#endif //MULTIPLE_HEAPS
-}
-void GCHeap::DiagWalkObject (Object* obj, walk_fn fn, void* context)
-{
-    uint8_t* o = (uint8_t*)obj;
-    if (o)
-    {
-        go_through_object_cl (method_table (o), o, size(o), oo,
-                                    {
-                                        if (*oo)
-                                        {
-                                            Object *oh = (Object*)*oo;
-                                            if (!fn (oh, context))
-                                                return;
-                                        }
-                                    }
-            );
-    }
-}
-void GCHeap::DiagWalkObject2 (Object* obj, walk_fn2 fn, void* context)
-{
-    uint8_t* o = (uint8_t*)obj;
-    if (o)
-    {
-        go_through_object_cl (method_table (o), o, size(o), oo,
-                                    {
-                                        if (*oo)
-                                        {
-                                            if (!fn (obj, oo, context))
-                                                return;
-                                        }
-                                    }
-            );
-    }
-}
-void GCHeap::DiagWalkSurvivorsWithType (void* gc_context, record_surv_fn fn, void* diag_context, walk_surv_type type, int gen_number)
-{
-    gc_heap* hp = (gc_heap*)gc_context;
-    if (type == walk_for_uoh)
-    {
-        hp->walk_survivors_for_uoh (diag_context, fn, gen_number);
-    }
-    else
-    {
-        hp->walk_survivors (fn, diag_context, type);
-    }
-}
-void GCHeap::DiagWalkHeap (walk_fn fn, void* context, int gen_number, bool walk_large_object_heap_p)
-{
-    gc_heap::walk_heap (fn, context, gen_number, walk_large_object_heap_p);
-}
-void GCHeap::DiagWalkFinalizeQueue (void* gc_context, fq_walk_fn fn)
-{
-    gc_heap* hp = (gc_heap*)gc_context;
-    hp->walk_finalize_queue (fn);
-}
-void GCHeap::DiagScanFinalizeQueue (fq_scan_fn fn, ScanContext* sc)
-{
-#ifdef MULTIPLE_HEAPS
-    for (int hn = 0; hn < gc_heap::n_heaps; hn++)
-    {
-        gc_heap* hp = gc_heap::g_heaps [hn];
-        hp->finalize_queue->GcScanRoots(fn, hn, sc);
-    }
-#else
-        pGenGCHeap->finalize_queue->GcScanRoots(fn, 0, sc);
-#endif //MULTIPLE_HEAPS
-}
-void GCHeap::DiagScanHandles (handle_scan_fn fn, int gen_number, ScanContext* context)
-{
-    GCScan::GcScanHandlesForProfilerAndETW (gen_number, context, fn);
-}
-void GCHeap::DiagScanDependentHandles (handle_scan_fn fn, int gen_number, ScanContext* context)
-{
-    GCScan::GcScanDependentHandlesForProfilerAndETW (gen_number, context, fn);
-}
-void GCHeap::DiagGetGCSettings(EtwGCSettingsInfo* etw_settings)
-{
-#ifdef FEATURE_EVENT_TRACE
-    etw_settings->heap_hard_limit = gc_heap::heap_hard_limit;
-    etw_settings->loh_threshold = loh_size_threshold;
-    etw_settings->physical_memory_from_config = gc_heap::physical_memory_from_config;
-    etw_settings->gen0_min_budget_from_config = gc_heap::gen0_min_budget_from_config;
-    etw_settings->gen0_max_budget_from_config = gc_heap::gen0_max_budget_from_config;
-    etw_settings->high_mem_percent_from_config = gc_heap::high_mem_percent_from_config;
-#ifdef BACKGROUND_GC
-    etw_settings->concurrent_gc_p = gc_heap::gc_can_use_concurrent;
-#else
-    etw_settings->concurrent_gc_p = false;
-#endif //BACKGROUND_GC
-    etw_settings->use_large_pages_p = gc_heap::use_large_pages_p;
-    etw_settings->use_frozen_segments_p = gc_heap::use_frozen_segments_p;
-    etw_settings->hard_limit_config_p = gc_heap::hard_limit_config_p;
-    etw_settings->no_affinitize_p =
-#ifdef MULTIPLE_HEAPS
-        gc_heap::gc_thread_no_affinitize_p;
-#else
-        true;
-#endif //MULTIPLE_HEAPS
-#endif //FEATURE_EVENT_TRACE
-}
-#if defined(WRITE_BARRIER_CHECK) && !defined (SERVER_GC)
-void deleteGCShadow()
-{
-    if (g_GCShadow != 0)
-        GCToOSInterface::VirtualRelease (g_GCShadow, g_GCShadowEnd - g_GCShadow);
-    g_GCShadow = 0;
-    g_GCShadowEnd = 0;
-}
-void initGCShadow()
-{
-    if (!(GCConfig::GetHeapVerifyLevel() & GCConfig::HEAPVERIFY_BARRIERCHECK))
-        return;
-    uint8_t* highest = nullptr;
-#ifdef USE_REGIONS
-    highest = global_region_allocator.get_left_used_unsafe();
-#else
-    highest = g_gc_highest_address;
-#endif
-    size_t len = g_gc_highest_address - g_gc_lowest_address;
-    size_t commit_len = highest - g_gc_lowest_address;
-    if (len > (size_t)(g_GCShadowEnd - g_GCShadow))
-    {
-        deleteGCShadow();
-        g_GCShadowEnd = g_GCShadow = (uint8_t *)GCToOSInterface::VirtualReserve (len, 0, VirtualReserveFlags::None);
-        if (g_GCShadow == NULL || !GCToOSInterface::VirtualCommit (g_GCShadow, commit_len))
-        {
-            _ASSERTE(!"Not enough memory to run HeapVerify level 2");
-            deleteGCShadow();
-            return;
-        }
-        g_GCShadowEnd += commit_len;
-    }
-    g_shadow_lowest_address = g_gc_lowest_address;
-    for (int i = get_start_generation_index(); i < total_generation_count; i++)
-    {
-        generation* gen = gc_heap::generation_of (i);
-        heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-        ptrdiff_t delta = g_GCShadow - g_gc_lowest_address;
-        while (seg)
-        {
-            uint8_t* start = heap_segment_mem (seg);
-            uint8_t* end = heap_segment_allocated (seg);
-            memcpy (start + delta, start, end - start);
-            seg = heap_segment_next_rw (seg);
-        }
-    }
-}
-#define INVALIDGCVALUE (void*)((size_t)0xcccccccd)
-inline void testGCShadow(Object** ptr)
-{
-    Object** shadow = (Object**) &g_GCShadow[((uint8_t*) ptr - g_gc_lowest_address)];
-    if (*ptr != 0 && (uint8_t*) shadow < g_GCShadowEnd && *ptr != *shadow)
-    {
-        if(*shadow!=INVALIDGCVALUE)
-        {
-#ifdef FEATURE_BASICFREEZE
-            if (!g_theGCHeap->IsInFrozenSegment (*ptr))
-#endif // FEATURE_BASICFREEZE
-            {
-                _ASSERTE(!"Pointer updated without using write barrier");
-            }
-        }
-        /*
-        else
-        {
-             printf("saw a INVALIDGCVALUE. (just to let you know)\n");
-        }
-        */
-    }
-}
-void testGCShadowHelper (uint8_t* x)
-{
-    size_t s = size (x);
-    if (contain_pointers (x))
-    {
-        go_through_object_nostart (method_table(x), x, s, oo,
-                           { testGCShadow((Object**) oo); });
-    }
-}
-void checkGCWriteBarrier()
-{
-    if (g_GCShadowEnd <= g_GCShadow || g_shadow_lowest_address != g_gc_lowest_address)
-    {
-        return;
-    }
-    for (int i = get_start_generation_index(); i < total_generation_count; i++)
-    {
-        int alignment = get_alignment_constant(i <= max_generation);
-        {
-            generation* gen = gc_heap::generation_of (i);
-            heap_segment* seg = heap_segment_rw (generation_start_segment (gen));
-            PREFIX_ASSUME(seg != NULL);
-            while(seg)
-            {
-                uint8_t* x = heap_segment_mem (seg);
-                while (x < heap_segment_allocated (seg))
-                {
-                    size_t s = size (x);
-                    testGCShadowHelper (x);
-                    x = x + Align (s, alignment);
-                }
-                seg = heap_segment_next_rw (seg);
-            }
-        }
-    }
-}
-#endif //WRITE_BARRIER_CHECK && !SERVER_GC
-#ifdef FEATURE_BASICFREEZE
-void gc_heap::walk_read_only_segment(heap_segment *seg, void *pvContext, object_callback_func pfnMethodTable, object_callback_func pfnObjRef)
-{
-    uint8_t *o = heap_segment_mem(seg);
-    int alignment = get_alignment_constant(TRUE);
-    while (o < heap_segment_allocated(seg))
-    {
-        pfnMethodTable(pvContext, o);
-        if (contain_pointers (o))
-        {
-            go_through_object_nostart (method_table (o), o, size(o), oo,
-                   {
-                       if (*oo)
-                           pfnObjRef(pvContext, oo);
-                   }
-            );
-        }
-        o += Align(size(o), alignment);
-    }
-}
-#endif // FEATURE_BASICFREEZE
-HRESULT GCHeap::WaitUntilConcurrentGCCompleteAsync(int millisecondsTimeout)
-{
-#ifdef BACKGROUND_GC
-    if (gc_heap::background_running_p())
-    {
-        uint32_t dwRet = pGenGCHeap->background_gc_wait(awr_ignored, millisecondsTimeout);
-        if (dwRet == WAIT_OBJECT_0)
-            return S_OK;
-        else if (dwRet == WAIT_TIMEOUT)
-            return HRESULT_FROM_WIN32(ERROR_TIMEOUT);
-        else
-            return E_FAIL;      // It is not clear if what the last error would be if the wait failed,
-    }
-#endif
-    return S_OK;
-}
-void GCHeap::TemporaryEnableConcurrentGC()
-{
-#ifdef BACKGROUND_GC
-    gc_heap::temp_disable_concurrent_p = false;
-#endif //BACKGROUND_GC
-}
-void GCHeap::TemporaryDisableConcurrentGC()
-{
-#ifdef BACKGROUND_GC
-    gc_heap::temp_disable_concurrent_p = true;
-#endif //BACKGROUND_GC
-}
-bool GCHeap::IsConcurrentGCEnabled()
-{
-#ifdef BACKGROUND_GC
-    return (gc_heap::gc_can_use_concurrent && !(gc_heap::temp_disable_concurrent_p));
-#else
-    return FALSE;
-#endif //BACKGROUND_GC
-}
-void PopulateDacVars(GcDacVars *gcDacVars)
-{
-    bool v2 = gcDacVars->minor_version_number >= 2;
-#define DEFINE_FIELD(field_name, field_type) offsetof(CLASS_NAME, field_name),
-#define DEFINE_DPTR_FIELD(field_name, field_type) offsetof(CLASS_NAME, field_name),
-#define DEFINE_ARRAY_FIELD(field_name, field_type, array_length) offsetof(CLASS_NAME, field_name),
-#define DEFINE_MISSING_FIELD(field_name) -1,
-#ifdef MULTIPLE_HEAPS
-    static int gc_heap_field_offsets[] = {
-#define CLASS_NAME gc_heap
-#include "dac_gcheap_fields.h"
-#undef CLASS_NAME
-    };
-#endif //MULTIPLE_HEAPS
-    static int generation_field_offsets[] = {
-#define CLASS_NAME generation
-#include "dac_generation_fields.h"
-#undef CLASS_NAME
-#undef DEFINE_MISSING_FIELD
-#undef DEFINE_ARRAY_FIELD
-#undef DEFINE_DPTR_FIELD
-#undef DEFINE_FIELD
-    };
-    assert(gcDacVars != nullptr);
-    gcDacVars->major_version_number = 2;
-    gcDacVars->minor_version_number = 0;
-    if (v2)
-    {
-        gcDacVars->total_bookkeeping_elements = total_bookkeeping_elements;
-        gcDacVars->card_table_info_size = sizeof(card_table_info);
-    }
-#ifdef USE_REGIONS
-    gcDacVars->minor_version_number |= 1;
-    if (v2)
-    {
-        gcDacVars->count_free_region_kinds = count_free_region_kinds;
-        gcDacVars->global_regions_to_decommit = reinterpret_cast<dac_region_free_list**>(&gc_heap::global_regions_to_decommit);
-        gcDacVars->global_free_huge_regions = reinterpret_cast<dac_region_free_list**>(&gc_heap::global_free_huge_regions);
-    }
-#endif //USE_REGIONS
-#ifndef BACKGROUND_GC
-    gcDacVars->minor_version_number |= 2;
-#endif //!BACKGROUND_GC
-    gcDacVars->built_with_svr = &g_built_with_svr_gc;
-    gcDacVars->build_variant = &g_build_variant;
-    gcDacVars->gc_structures_invalid_cnt = const_cast<int32_t*>(&GCScan::m_GcStructuresInvalidCnt);
-    gcDacVars->generation_size = sizeof(generation);
-    gcDacVars->total_generation_count = total_generation_count;
-    gcDacVars->max_gen = &g_max_generation;
-#ifdef BACKGROUND_GC
-    gcDacVars->current_c_gc_state = const_cast<c_gc_state*>(&gc_heap::current_c_gc_state);
-#else //BACKGROUND_GC
-    gcDacVars->current_c_gc_state = 0;
-#endif //BACKGROUND_GC
-#ifndef MULTIPLE_HEAPS
-    gcDacVars->ephemeral_heap_segment = reinterpret_cast<dac_heap_segment**>(&gc_heap::ephemeral_heap_segment);
-#ifdef USE_REGIONS
-    if (v2)
-    {
-        gcDacVars->free_regions = reinterpret_cast<dac_region_free_list**>(&gc_heap::free_regions);
-    }
-#endif
-#ifdef BACKGROUND_GC
-    gcDacVars->mark_array = &gc_heap::mark_array;
-    gcDacVars->background_saved_lowest_address = &gc_heap::background_saved_lowest_address;
-    gcDacVars->background_saved_highest_address = &gc_heap::background_saved_highest_address;
-    if (v2)
-    {
-        gcDacVars->freeable_soh_segment = reinterpret_cast<dac_heap_segment**>(&gc_heap::freeable_soh_segment);
-        gcDacVars->freeable_uoh_segment = reinterpret_cast<dac_heap_segment**>(&gc_heap::freeable_uoh_segment);
-    }
-    gcDacVars->next_sweep_obj = &gc_heap::next_sweep_obj;
-#ifdef USE_REGIONS
-    gcDacVars->saved_sweep_ephemeral_seg = 0;
-    gcDacVars->saved_sweep_ephemeral_start = 0;
-#else
-    gcDacVars->saved_sweep_ephemeral_seg = reinterpret_cast<dac_heap_segment**>(&gc_heap::saved_sweep_ephemeral_seg);
-    gcDacVars->saved_sweep_ephemeral_start = &gc_heap::saved_sweep_ephemeral_start;
-#endif //USE_REGIONS
-#else //BACKGROUND_GC
-    gcDacVars->mark_array = 0;
-    gcDacVars->background_saved_lowest_address = 0;
-    gcDacVars->background_saved_highest_address = 0;
-    if (v2)
-    {
-        gcDacVars->freeable_soh_segment = 0;
-        gcDacVars->freeable_uoh_segment = 0;
-    }
-    gcDacVars->next_sweep_obj = 0;
-    gcDacVars->saved_sweep_ephemeral_seg = 0;
-    gcDacVars->saved_sweep_ephemeral_start = 0;
-#endif //BACKGROUND_GC
-    gcDacVars->alloc_allocated = &gc_heap::alloc_allocated;
-    gcDacVars->oom_info = &gc_heap::oom_info;
-    gcDacVars->finalize_queue = reinterpret_cast<dac_finalize_queue**>(&gc_heap::finalize_queue);
-    gcDacVars->generation_table = reinterpret_cast<unused_generation**>(&gc_heap::generation_table);
-#ifdef GC_CONFIG_DRIVEN
-    gcDacVars->gc_global_mechanisms = reinterpret_cast<size_t**>(&gc_global_mechanisms);
-    gcDacVars->interesting_data_per_heap = reinterpret_cast<size_t**>(&gc_heap::interesting_data_per_heap);
-    gcDacVars->compact_reasons_per_heap = reinterpret_cast<size_t**>(&gc_heap::compact_reasons_per_heap);
-    gcDacVars->expand_mechanisms_per_heap = reinterpret_cast<size_t**>(&gc_heap::expand_mechanisms_per_heap);
-    gcDacVars->interesting_mechanism_bits_per_heap = reinterpret_cast<size_t**>(&gc_heap::interesting_mechanism_bits_per_heap);
-#endif // GC_CONFIG_DRIVEN
-#ifdef HEAP_ANALYZE
-    gcDacVars->internal_root_array = &gc_heap::internal_root_array;
-    gcDacVars->internal_root_array_index = &gc_heap::internal_root_array_index;
-    gcDacVars->heap_analyze_success = &gc_heap::heap_analyze_success;
-#endif // HEAP_ANALYZE
-#else
-    gcDacVars->n_heaps = &gc_heap::n_heaps;
-    gcDacVars->g_heaps = reinterpret_cast<unused_gc_heap***>(&gc_heap::g_heaps);
-    gcDacVars->gc_heap_field_offsets = reinterpret_cast<int**>(&gc_heap_field_offsets);
-#endif // MULTIPLE_HEAPS
-    gcDacVars->generation_field_offsets = reinterpret_cast<int**>(&generation_field_offsets);
-    if (v2)
-    {
-        gcDacVars->bookkeeping_start = &gc_heap::bookkeeping_start;
-    }
-}
-int GCHeap::RefreshMemoryLimit()
-{
-    return gc_heap::refresh_memory_limit();
-}
-bool gc_heap::compute_hard_limit()
-{
-    heap_hard_limit_oh[soh] = 0;
-#ifdef HOST_64BIT
-    heap_hard_limit = (size_t)GCConfig::GetGCHeapHardLimit();
-    heap_hard_limit_oh[soh] = (size_t)GCConfig::GetGCHeapHardLimitSOH();
-    heap_hard_limit_oh[loh] = (size_t)GCConfig::GetGCHeapHardLimitLOH();
-    heap_hard_limit_oh[poh] = (size_t)GCConfig::GetGCHeapHardLimitPOH();
-    use_large_pages_p = GCConfig::GetGCLargePages();
-    if (heap_hard_limit_oh[soh] || heap_hard_limit_oh[loh] || heap_hard_limit_oh[poh])
-    {
-        if (!heap_hard_limit_oh[soh])
-        {
-            return false;
-        }
-        if (!heap_hard_limit_oh[loh])
-        {
-            return false;
-        }
-        heap_hard_limit = heap_hard_limit_oh[soh] +
-            heap_hard_limit_oh[loh] + heap_hard_limit_oh[poh];
-    }
-    else
-    {
-        uint32_t percent_of_mem_soh = (uint32_t)GCConfig::GetGCHeapHardLimitSOHPercent();
-        uint32_t percent_of_mem_loh = (uint32_t)GCConfig::GetGCHeapHardLimitLOHPercent();
-        uint32_t percent_of_mem_poh = (uint32_t)GCConfig::GetGCHeapHardLimitPOHPercent();
-        if (percent_of_mem_soh || percent_of_mem_loh || percent_of_mem_poh)
-        {
-            if ((percent_of_mem_soh <= 0) || (percent_of_mem_soh >= 100))
-            {
-                return false;
-            }
-            if ((percent_of_mem_loh <= 0) || (percent_of_mem_loh >= 100))
-            {
-                return false;
-            }
-            else if ((percent_of_mem_poh < 0) || (percent_of_mem_poh >= 100))
-            {
-                return false;
-            }
-            if ((percent_of_mem_soh + percent_of_mem_loh + percent_of_mem_poh) >= 100)
-            {
-                return false;
-            }
-            heap_hard_limit_oh[soh] = (size_t)(total_physical_mem * (uint64_t)percent_of_mem_soh / (uint64_t)100);
-            heap_hard_limit_oh[loh] = (size_t)(total_physical_mem * (uint64_t)percent_of_mem_loh / (uint64_t)100);
-            heap_hard_limit_oh[poh] = (size_t)(total_physical_mem * (uint64_t)percent_of_mem_poh / (uint64_t)100);
-            heap_hard_limit = heap_hard_limit_oh[soh] +
-                heap_hard_limit_oh[loh] + heap_hard_limit_oh[poh];
-        }
-    }
-    if (heap_hard_limit_oh[soh] && (!heap_hard_limit_oh[poh]) && (!use_large_pages_p))
-    {
-        return false;
-    }
-    if (!(heap_hard_limit))
-    {
-        uint32_t percent_of_mem = (uint32_t)GCConfig::GetGCHeapHardLimitPercent();
-        if ((percent_of_mem > 0) && (percent_of_mem < 100))
-        {
-            heap_hard_limit = (size_t)(total_physical_mem * (uint64_t)percent_of_mem / (uint64_t)100);
-        }
-    }
-#endif //HOST_64BIT
-    return true;
-}
-bool gc_heap::compute_memory_settings(bool is_initialization, uint32_t& nhp, uint32_t nhp_from_config, size_t& seg_size_from_config, size_t new_current_total_committed)
-{
-#ifdef HOST_64BIT
-    if (!hard_limit_config_p)
-    {
-        if (is_restricted_physical_mem)
-        {
-            uint64_t physical_mem_for_gc = total_physical_mem * (uint64_t)75 / (uint64_t)100;
-#ifndef USE_REGIONS
-            if (is_initialization)
-#endif //USE_REGIONS
-            {
-                heap_hard_limit = (size_t)max ((20 * 1024 * 1024), physical_mem_for_gc);
-            }
-        }
-    }
-    if (heap_hard_limit && (heap_hard_limit < new_current_total_committed))
-    {
-        return false;
-    }
-#endif //HOST_64BIT
-#ifdef USE_REGIONS
-    {
-#else
-    if (is_initialization)
-    {
-#endif //USE_REGIONS
-        if (heap_hard_limit)
-        {
-            if (is_initialization && (!nhp_from_config))
-            {
-                nhp = adjust_heaps_hard_limit (nhp);
-            }
-            seg_size_from_config = (size_t)GCConfig::GetSegmentSize();
-            if (seg_size_from_config)
-            {
-                seg_size_from_config = adjust_segment_size_hard_limit_va (seg_size_from_config);
-            }
-            size_t limit_to_check = (heap_hard_limit_oh[soh] ? heap_hard_limit_oh[soh] : heap_hard_limit);
-            soh_segment_size = max (adjust_segment_size_hard_limit (limit_to_check, nhp), seg_size_from_config);
-        }
-        else
-        {
-            soh_segment_size = get_valid_segment_size();
-        }
-    }
-    mem_one_percent = total_physical_mem / 100;
-#ifndef MULTIPLE_HEAPS
-    mem_one_percent /= g_num_processors;
-#endif //!MULTIPLE_HEAPS
-    uint32_t highmem_th_from_config = (uint32_t)GCConfig::GetGCHighMemPercent();
-    if (highmem_th_from_config)
-    {
-        high_memory_load_th = min (99, highmem_th_from_config);
-        v_high_memory_load_th = min (99, (highmem_th_from_config + 7));
-#ifdef FEATURE_EVENT_TRACE
-        high_mem_percent_from_config = highmem_th_from_config;
-#endif //FEATURE_EVENT_TRACE
-    }
-    else
-    {
-        int available_mem_th = 10;
-        if (total_physical_mem >= ((uint64_t)80 * 1024 * 1024 * 1024))
-        {
-            int adjusted_available_mem_th = 3 + (int)((float)47 / (float)g_num_processors);
-            available_mem_th = min (available_mem_th, adjusted_available_mem_th);
-        }
-        high_memory_load_th = 100 - available_mem_th;
-        v_high_memory_load_th = 97;
-    }
-    m_high_memory_load_th = min ((high_memory_load_th + 5), v_high_memory_load_th);
-    return true;
-}
-#ifdef USE_REGIONS
-void gc_heap::compute_committed_bytes(size_t& total_committed, size_t& committed_decommit, size_t& committed_free,
-                                      size_t& committed_bookkeeping, size_t& new_current_total_committed, size_t& new_current_total_committed_bookkeeping,
-                                      size_t* new_committed_by_oh)
-{
-    for (int oh = soh; oh < total_oh_count; oh++)
-    {
-        int start_generation = (oh == 0) ? 0 : oh + max_generation;
-        int end_generation = oh + max_generation;
-        size_t total_committed_per_oh = 0;
-#ifdef MULTIPLE_HEAPS
-        for (int h = 0; h < n_heaps; h++)
-        {
-            gc_heap* heap = g_heaps[h];
-#else
-        {
-            gc_heap* heap = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-            size_t total_committed_per_heap = 0;
-            for (int gen = start_generation; gen <= end_generation; gen++)
-            {
-                heap->accumulate_committed_bytes ( generation_start_segment (heap->generation_of (gen)), total_committed_per_heap, committed_bookkeeping);
-            }
-            if (oh == soh)
-            {
-                heap->accumulate_committed_bytes (heap->freeable_soh_segment, total_committed_per_heap, committed_bookkeeping);
-            }
-            else
-            {
-                heap->accumulate_committed_bytes (heap->freeable_uoh_segment, total_committed_per_heap, committed_bookkeeping, (gc_oh_num)oh);
-            }
-#if defined(MULTIPLE_HEAPS) && defined(_DEBUG)
-            heap->committed_by_oh_per_heap_refresh[oh] = total_committed_per_heap;
-#endif //MULTIPLE_HEAPS && _DEBUG
-            total_committed_per_oh += total_committed_per_heap;
-        }
-        new_committed_by_oh[oh] = total_committed_per_oh;
-        total_committed += total_committed_per_oh;
-    }
-    size_t committed_old_free = 0;
-    committed_free = 0;
-#ifdef MULTIPLE_HEAPS
-    for (int h = 0; h < n_heaps; h++)
-    {
-        gc_heap* heap = g_heaps[h];
-#else
-    {
-        gc_heap* heap = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        for (int i = 0; i < count_free_region_kinds; i++)
-        {
-            heap_segment* seg = heap->free_regions[i].get_first_free_region();
-            heap->accumulate_committed_bytes (seg, committed_free, committed_bookkeeping);
-        }
-    }
-    committed_old_free += committed_free;
-    committed_decommit = 0;
-    for (int i = 0; i < count_free_region_kinds; i++)
-    {
-        heap_segment* seg = global_regions_to_decommit[i].get_first_free_region();
-#ifdef MULTIPLE_HEAPS
-        gc_heap* heap = g_heaps[0];
-#else
-        gc_heap* heap = nullptr;
-#endif //MULTIPLE_HEAPS
-        heap->accumulate_committed_bytes (seg, committed_decommit, committed_bookkeeping);
-    }
-    committed_old_free += committed_decommit;
-    {
-        heap_segment* seg = global_free_huge_regions.get_first_free_region();
-#ifdef MULTIPLE_HEAPS
-        gc_heap* heap = g_heaps[0];
-#else
-        gc_heap* heap = pGenGCHeap;
-#endif //MULTIPLE_HEAPS
-        heap->accumulate_committed_bytes (seg, committed_old_free, committed_bookkeeping);
-    }
-    new_committed_by_oh[recorded_committed_free_bucket] = committed_old_free;
-    total_committed += committed_old_free;
-    uint8_t* commit_begins[total_bookkeeping_elements];
-    size_t commit_sizes[total_bookkeeping_elements];
-    size_t new_sizes[total_bookkeeping_elements];
-    bool get_card_table_commit_layout_result = get_card_table_commit_layout(g_gc_lowest_address, bookkeeping_covered_committed, commit_begins, commit_sizes, new_sizes);
-    assert (get_card_table_commit_layout_result);
-    for (int i = card_table_element; i <= seg_mapping_table_element; i++)
-    {
-        assert (commit_sizes[i] >= 0);
-        committed_bookkeeping += commit_sizes[i];
-    }
-    new_current_total_committed_bookkeeping = committed_bookkeeping;
-    new_committed_by_oh[recorded_committed_bookkeeping_bucket] = committed_bookkeeping;
-    total_committed += committed_bookkeeping;
-    new_current_total_committed = total_committed;
-}
-#endif //USE_REGIONS
-int gc_heap::refresh_memory_limit()
-{
-    refresh_memory_limit_status status = refresh_success;
-    if (GCConfig::GetGCTotalPhysicalMemory() != 0)
-    {
-        return (int)status;
-    }
-    GCToEEInterface::SuspendEE(SUSPEND_FOR_GC);
-#ifdef USE_REGIONS
-    decommit_lock.Enter();
-    size_t total_committed = 0;
-    size_t committed_decommit; // unused
-    size_t committed_free; // unused
-    size_t committed_bookkeeping = 0;
-    size_t new_current_total_committed;
-    size_t new_current_total_committed_bookkeeping;
-    size_t new_committed_by_oh[recorded_committed_bucket_counts];
-    compute_committed_bytes(total_committed, committed_decommit, committed_free,
-                            committed_bookkeeping, new_current_total_committed, new_current_total_committed_bookkeeping,
-                            new_committed_by_oh);
-#endif //USE_REGIONS
-    uint32_t nhp_from_config = static_cast<uint32_t>(GCConfig::GetHeapCount());
-#ifdef MULTIPLE_HEAPS
-    uint32_t nhp = n_heaps;
-#else
-    uint32_t nhp = 1;
-#endif //MULTIPLE_HEAPS
-    size_t seg_size_from_config;
-    bool old_is_restricted_physical_mem = is_restricted_physical_mem;
-    uint64_t old_total_physical_mem = total_physical_mem;
-    size_t old_heap_hard_limit = heap_hard_limit;
-    size_t old_heap_hard_limit_soh = heap_hard_limit_oh[soh];
-    size_t old_heap_hard_limit_loh = heap_hard_limit_oh[loh];
-    size_t old_heap_hard_limit_poh = heap_hard_limit_oh[poh];
-    bool old_hard_limit_config_p = hard_limit_config_p;
-    total_physical_mem = GCToOSInterface::GetPhysicalMemoryLimit (&is_restricted_physical_mem, true);
-    bool succeed = true;
-#ifdef USE_REGIONS
-    GCConfig::RefreshHeapHardLimitSettings();
-    if (!compute_hard_limit())
-    {
-        succeed = false;
-        status = refresh_hard_limit_invalid;
-    }
-    hard_limit_config_p = heap_hard_limit != 0;
-#else
-    size_t new_current_total_committed = 0;
-#endif //USE_REGIONS
-    if (succeed && !compute_memory_settings(false, nhp, nhp_from_config, seg_size_from_config, new_current_total_committed))
-    {
-        succeed = false;
-        status = refresh_hard_limit_too_low;
-    }
-    if (!succeed)
-    {
-        is_restricted_physical_mem = old_is_restricted_physical_mem;
-        total_physical_mem = old_total_physical_mem;
-        heap_hard_limit = old_heap_hard_limit;
-        heap_hard_limit_oh[soh] = old_heap_hard_limit_soh;
-        heap_hard_limit_oh[loh] = old_heap_hard_limit_loh;
-        heap_hard_limit_oh[poh] = old_heap_hard_limit_poh;
-        hard_limit_config_p = old_hard_limit_config_p;
-    }
-    else
-#ifndef COMMITTED_BYTES_SHADOW
-    if (!old_heap_hard_limit && heap_hard_limit)
-#endif //COMMITTED_BYTES_SHADOW
-    {
-#ifdef USE_REGIONS
-        check_commit_cs.Initialize();
-#ifdef COMMITTED_BYTES_SHADOW
-        assert (new_current_total_committed == current_total_committed);
-        assert (new_current_total_committed_bookkeeping == current_total_committed_bookkeeping);
-#else
-        current_total_committed = new_current_total_committed;
-        current_total_committed_bookkeeping = new_current_total_committed_bookkeeping;
-#endif
-        for (int i = 0; i < recorded_committed_bucket_counts; i++)
-        {
-#ifdef COMMITTED_BYTES_SHADOW
-            assert (new_committed_by_oh[i] == committed_by_oh[i]);
-#else
-            committed_by_oh[i] = new_committed_by_oh[i];
-#endif
-        }
-#ifdef MULTIPLE_HEAPS
-#ifdef _DEBUG
-        for (int h = 0; h < n_heaps; h++)
-        {
-            for (int oh = soh; oh < total_oh_count; oh++)
-            {
-#ifdef COMMITTED_BYTES_SHADOW
-                assert (g_heaps[h]->committed_by_oh_per_heap[oh] == g_heaps[h]->committed_by_oh_per_heap_refresh[oh]);
-#else
-                g_heaps[h]->committed_by_oh_per_heap[oh] = g_heaps[h]->committed_by_oh_per_heap_refresh[oh];
-#endif
-            }
-        }
-#endif //_DEBUG
-#endif //MULTIPLE_HEAPS
-#else
-        assert (!"NYI - Segments");
-#endif //USE_REGIONS
-    }
-#ifdef USE_REGIONS
-    decommit_lock.Leave();
-#endif
-    GCToEEInterface::RestartEE(TRUE);
-    return (int)status;
-}
-#ifdef USE_REGIONS
-void gc_heap::accumulate_committed_bytes(heap_segment* seg, size_t& committed_bytes, size_t& mark_array_committed_bytes, gc_oh_num oh)
-{
-    seg = heap_segment_rw (seg);
-    while (seg)
-    {
-        if ((oh == unknown) || (heap_segment_oh (seg) == oh))
-        {
-            mark_array_committed_bytes += get_mark_array_size (seg);
-            committed_bytes += (heap_segment_committed (seg) - get_region_start (seg));
-        }
-        seg = heap_segment_next_rw (seg);
-    }
-}
-size_t gc_heap::get_mark_array_size (heap_segment* seg)
-{
-    if (seg->flags & heap_segment_flags_ma_committed)
-    {
-        uint32_t* mark_array_addr = mark_array;
-        uint8_t* begin = get_start_address (seg);
-        uint8_t* end = heap_segment_reserved (seg);
-        size_t beg_word = mark_word_of (begin);
-        size_t end_word = mark_word_of (align_on_mark_word (end));
-        uint8_t* commit_start = align_lower_page ((uint8_t*)&mark_array_addr[beg_word]);
-        uint8_t* commit_end = align_on_page ((uint8_t*)&mark_array_addr[end_word]);
-        return (size_t)(commit_end - commit_start);
-    }
-    return 0;
-}
-#endif //USE_REGIONS

--- a/src/coreclr/gc/gcconfig.h
+++ b//dev/null
@@ -1,147 +0,0 @@
-#ifndef __GCCONFIG_H__
-#define __GCCONFIG_H__
-class GCConfigStringHolder
-{
-private:
-    const char* m_str;
-public:
-    explicit GCConfigStringHolder(const char* str)
-      : m_str(str) {}
-    GCConfigStringHolder(const GCConfigStringHolder&) = delete;
-    GCConfigStringHolder& operator=(const GCConfigStringHolder&) = delete;
-    GCConfigStringHolder(GCConfigStringHolder&&) = default;
-    ~GCConfigStringHolder()
-    {
-        if (m_str)
-        {
-            GCToEEInterface::FreeStringConfigValue(m_str);
-        }
-        m_str = nullptr;
-    }
-    const char* Get() const { return m_str; }
-};
-#define GC_CONFIGURATION_KEYS \
-    BOOL_CONFIG  (ServerGC,                  "gcServer",                  "System.GC.Server",                  false,              "Whether we should be using Server GC")                                                   \
-    BOOL_CONFIG  (ConcurrentGC,              "gcConcurrent",              "System.GC.Concurrent",              true,               "Whether we should be using Concurrent GC")                                                \
-    BOOL_CONFIG  (ConservativeGC,            "gcConservative",            NULL,                                false,              "Enables/Disables conservative GC")                                                       \
-    BOOL_CONFIG  (ForceCompact,              "gcForceCompact",            NULL,                                false,              "When set to true, always do compacting GC")                                              \
-    BOOL_CONFIG  (RetainVM,                  "GCRetainVM",                "System.GC.RetainVM",                false,              "When set we put the segments that should be deleted on a standby list (instead of "       \
-                                                                                                                                          "releasing them back to the OS) which will be considered to satisfy new segment requests" \
-                                                                                                                                          " (note that the same thing can be specified via API which is the supported way)")        \
-    BOOL_CONFIG  (BreakOnOOM,                "GCBreakOnOOM",              NULL,                                false,              "Does a DebugBreak at the soonest time we detect an OOM")                                 \
-    BOOL_CONFIG  (NoAffinitize,              "GCNoAffinitize",            "System.GC.NoAffinitize",            false,              "If set, do not affinitize server GC threads")                                             \
-    BOOL_CONFIG  (LogEnabled,                "GCLogEnabled",              NULL,                                false,              "Specifies if you want to turn on logging in GC")                                         \
-    BOOL_CONFIG  (ConfigLogEnabled,          "GCConfigLogEnabled",        NULL,                                false,              "Specifies the name of the GC config log file")                                           \
-    BOOL_CONFIG  (GCNumaAware,               "GCNumaAware",               NULL,                                true,               "Enables numa allocations in the GC")                                                     \
-    BOOL_CONFIG  (GCCpuGroup,                "GCCpuGroup",                "System.GC.CpuGroup",                false,              "Enables CPU groups in the GC")                                                            \
-    BOOL_CONFIG  (GCLargePages,              "GCLargePages",              "System.GC.LargePages",              false,              "Enables using Large Pages in the GC")                                                     \
-    INT_CONFIG   (HeapVerifyLevel,           "HeapVerify",                NULL,                                HEAPVERIFY_NONE,    "When set verifies the integrity of the managed heap on entry and exit of each GC")       \
-    INT_CONFIG   (LOHCompactionMode,         "GCLOHCompact",              NULL,                                0,                  "Specifies the LOH compaction mode")                                                      \
-    INT_CONFIG   (LOHThreshold,              "GCLOHThreshold",            NULL,                                LARGE_OBJECT_SIZE,  "Specifies the size that will make objects go on LOH")                                     \
-    INT_CONFIG   (BGCSpinCount,              "BGCSpinCount",              NULL,                                140,                "Specifies the bgc spin count")                                                           \
-    INT_CONFIG   (BGCSpin,                   "BGCSpin",                   NULL,                                2,                  "Specifies the bgc spin time")                                                            \
-    INT_CONFIG   (HeapCount,                 "GCHeapCount",               "System.GC.HeapCount",               0,                  "Specifies the number of server GC heaps")                                                 \
-    INT_CONFIG   (MaxHeapCount,              "GCMaxHeapCount",            "System.GC.MaxHeapCount",            0,                  "Specifies the max number of server GC heaps to adjust to")                                                 \
-    INT_CONFIG   (Gen0Size,                  "GCgen0size",                NULL,                                0,                  "Specifies the smallest gen0 budget")                                                     \
-    INT_CONFIG   (SegmentSize,               "GCSegmentSize",             NULL,                                0,                  "Specifies the managed heap segment size")                                                \
-    INT_CONFIG   (LatencyMode,               "GCLatencyMode",             NULL,                                -1,                 "Specifies the GC latency mode - batch, interactive or low latency (note that the same "   \
-                                                                                                                                           "thing can be specified via API which is the supported way")                             \
-    INT_CONFIG   (LatencyLevel,              "GCLatencyLevel",            NULL,                                1,                  "Specifies the GC latency level that you want to optimize for. Must be a number from 0 "  \
-                                                                                                                                          "to 3. See documentation for more details on each level.")                                \
-    INT_CONFIG   (LogFileSize,               "GCLogFileSize",             NULL,                                0,                  "Specifies the GC log file size")                                                         \
-    INT_CONFIG   (CompactRatio,              "GCCompactRatio",            NULL,                                0,                  "Specifies the ratio compacting GCs vs sweeping")                                         \
-    INT_CONFIG   (GCHeapAffinitizeMask,      "GCHeapAffinitizeMask",      "System.GC.HeapAffinitizeMask",      0,                  "Specifies processor mask for Server GC threads")                                          \
-    STRING_CONFIG(GCHeapAffinitizeRanges,    "GCHeapAffinitizeRanges",    "System.GC.HeapAffinitizeRanges",                        "Specifies list of processors for Server GC threads. The format is a comma separated "     \
-                                                                                                                                          "list of processor numbers or ranges of processor numbers. On Windows, each entry is "    \
-                                                                                                                                          "prefixed by the CPU group number. Example: Unix - 1,3,5,7-9,12, Windows - 0:1,1:7-9")    \
-    INT_CONFIG   (GCHighMemPercent,          "GCHighMemPercent",          "System.GC.HighMemoryPercent",       0,                  "The percent for GC to consider as high memory")                                           \
-    INT_CONFIG   (GCProvModeStress,          "GCProvModeStress",          NULL,                                0,                  "Stress the provisional modes")                                                           \
-    INT_CONFIG   (GCGen0MaxBudget,           "GCGen0MaxBudget",           NULL,                                0,                  "Specifies the largest gen0 allocation budget")                                           \
-    INT_CONFIG   (GCGen1MaxBudget,           "GCGen1MaxBudget",           NULL,                                0,                  "Specifies the largest gen1 allocation budget")                                           \
-    INT_CONFIG   (GCLowSkipRatio,            "GCLowSkipRatio",            NULL,                                30,                 "Specifies the low generation skip ratio")                                                \
-    INT_CONFIG   (GCHeapHardLimit,           "GCHeapHardLimit",           "System.GC.HeapHardLimit",           0,                  "Specifies a hard limit for the GC heap")                                                 \
-    INT_CONFIG   (GCHeapHardLimitPercent,    "GCHeapHardLimitPercent",    "System.GC.HeapHardLimitPercent",    0,                  "Specifies the GC heap usage as a percentage of the total memory")                        \
-    INT_CONFIG   (GCTotalPhysicalMemory,     "GCTotalPhysicalMemory",     NULL,                                0,                  "Specifies what the GC should consider to be total physical memory")                      \
-    INT_CONFIG   (GCRegionRange,             "GCRegionRange",             NULL,                                0,                  "Specifies the range for the GC heap")                                                    \
-    INT_CONFIG   (GCRegionSize,              "GCRegionSize",              NULL,                                0,                  "Specifies the size for a basic GC region")                                               \
-    INT_CONFIG   (GCEnableSpecialRegions,    "GCEnableSpecialRegions",    NULL,                                0,                  "Specifies to enable special handling some regions like SIP")                             \
-    STRING_CONFIG(LogFile,                   "GCLogFile",                 NULL,                                                    "Specifies the name of the GC log file")                                                  \
-    STRING_CONFIG(ConfigLogFile,             "GCConfigLogFile",           NULL,                                                    "Specifies the name of the GC config log file")                                           \
-    INT_CONFIG   (BGCFLTuningEnabled,        "BGCFLTuningEnabled",        NULL,                                0,                  "Enables FL tuning")                                                                      \
-    INT_CONFIG   (BGCMemGoal,                "BGCMemGoal",                NULL,                                75,                 "Specifies the physical memory load goal")                                                \
-    INT_CONFIG   (BGCMemGoalSlack,           "BGCMemGoalSlack",           NULL,                                10,                 "Specifies comfort zone of going above goal")                                             \
-    INT_CONFIG   (BGCFLSweepGoal,            "BGCFLSweepGoal",            NULL,                                0,                  "Specifies the gen2 sweep FL ratio goal")                                                 \
-    INT_CONFIG   (BGCFLSweepGoalLOH,         "BGCFLSweepGoalLOH",         NULL,                                0,                  "Specifies the LOH sweep FL ratio goal")                                                  \
-    INT_CONFIG   (BGCFLkp,                   "BGCFLkp",                   NULL,                                6000,               "Specifies kp for above goal tuning")                                                     \
-    INT_CONFIG   (BGCFLki,                   "BGCFLki",                   NULL,                                1000,               "Specifies ki for above goal tuning")                                                     \
-    INT_CONFIG   (BGCFLkd,                   "BGCFLkd",                   NULL,                                11,                 "Specifies kd for above goal tuning")                                                     \
-    INT_CONFIG   (BGCFLff,                   "BGCFLff",                   NULL,                                100,                "Specifies ff ratio")                                                                     \
-    INT_CONFIG   (BGCFLSmoothFactor,         "BGCFLSmoothFactor",         NULL,                                150,                "Smoothing over these")                                                                   \
-    INT_CONFIG   (BGCFLGradualD,             "BGCFLGradualD",             NULL,                                0,                  "Enable gradual D instead of cutting off at the value")                                   \
-    INT_CONFIG   (BGCMLkp,                   "BGCMLkp",                   NULL,                                1000,               "Specifies kp for ML tuning")                                                             \
-    INT_CONFIG   (BGCMLki,                   "BGCMLki",                   NULL,                                16,                 "Specifies ki for ML tuning")                                                             \
-    INT_CONFIG   (BGCFLEnableKi,             "BGCFLEnableKi",             NULL,                                1,                  "Enables ki for above goal tuning")                                                       \
-    INT_CONFIG   (BGCFLEnableKd,             "BGCFLEnableKd",             NULL,                                0,                  "Enables kd for above goal tuning")                                                       \
-    INT_CONFIG   (BGCFLEnableSmooth,         "BGCFLEnableSmooth",         NULL,                                0,                  "Enables smoothing")                                                                      \
-    INT_CONFIG   (BGCFLEnableTBH,            "BGCFLEnableTBH",            NULL,                                0,                  "Enables TBH")                                                                            \
-    INT_CONFIG   (BGCFLEnableFF,             "BGCFLEnableFF",             NULL,                                0,                  "Enables FF")                                                                             \
-    INT_CONFIG   (BGCG2RatioStep,            "BGCG2RatioStep",            NULL,                                5,                  "Ratio correction factor for ML loop")                                                    \
-    INT_CONFIG   (GCHeapHardLimitSOH,        "GCHeapHardLimitSOH",        "System.GC.HeapHardLimitSOH",        0,                  "Specifies a hard limit for the GC heap SOH")                                             \
-    INT_CONFIG   (GCHeapHardLimitLOH,        "GCHeapHardLimitLOH",        "System.GC.HeapHardLimitLOH",        0,                  "Specifies a hard limit for the GC heap LOH")                                             \
-    INT_CONFIG   (GCHeapHardLimitPOH,        "GCHeapHardLimitPOH",        "System.GC.HeapHardLimitPOH",        0,                  "Specifies a hard limit for the GC heap POH")                                             \
-    INT_CONFIG   (GCHeapHardLimitSOHPercent, "GCHeapHardLimitSOHPercent", "System.GC.HeapHardLimitSOHPercent", 0,                  "Specifies the GC heap SOH usage as a percentage of the total memory")                    \
-    INT_CONFIG   (GCHeapHardLimitLOHPercent, "GCHeapHardLimitLOHPercent", "System.GC.HeapHardLimitLOHPercent", 0,                  "Specifies the GC heap LOH usage as a percentage of the total memory")                    \
-    INT_CONFIG   (GCHeapHardLimitPOHPercent, "GCHeapHardLimitPOHPercent", "System.GC.HeapHardLimitPOHPercent", 0,                  "Specifies the GC heap POH usage as a percentage of the total memory")                    \
-    INT_CONFIG   (GCEnabledInstructionSets,  "GCEnabledInstructionSets",  NULL,                                -1,                 "Specifies whether GC can use AVX2 or AVX512F - 0 for neither, 1 for AVX2, 3 for AVX512F")\
-    INT_CONFIG   (GCConserveMem,             "GCConserveMemory",          "System.GC.ConserveMemory",          0,                  "Specifies how hard GC should try to conserve memory - values 0-9")                       \
-    INT_CONFIG   (GCWriteBarrier,            "GCWriteBarrier",            NULL,                                0,                  "Specifies whether GC should use more precise but slower write barrier")                  \
-    STRING_CONFIG(GCName,                    "GCName",                    "System.GC.Name",                                        "Specifies the path of the standalone GC implementation.")                                \
-    INT_CONFIG   (GCSpinCountUnit,           "GCSpinCountUnit",           0,                                   0,                  "Specifies the spin count unit used by the GC.")                                          \
-    INT_CONFIG   (GCDynamicAdaptationMode,   "GCDynamicAdaptationMode",   "System.GC.DynamicAdaptationMode",   0,                  "Enable the GC to dynamically adapt to application sizes.")                               \
-    BOOL_CONFIG  (GCCacheSizeFromSysConf,    "GCCacheSizeFromSysConf",    NULL,                                false,              "Specifies using sysconf to retrieve the last level cache size for Unix.")
-class GCConfig
-{
-#define BOOL_CONFIG(name, unused_private_key, unused_public_key, unused_default, unused_doc) \
-  public: static bool Get##name();                                \
-  public: static bool Get##name(bool defaultValue);               \
-  public: static void Set##name(bool value);                      \
-  private: static bool s_##name;                                  \
-  private: static bool s_##name##Provided;                        \
-  private: static bool s_Updated##name;
-#define INT_CONFIG(name, unused_private_key, unused_public_key, unused_default, unused_doc) \
-  public: static int64_t Get##name();                            \
-  public: static int64_t Get##name(int64_t defaultValue);        \
-  public: static void Set##name(int64_t value);                  \
-  private: static int64_t s_##name;                              \
-  private: static bool s_##name##Provided;                       \
-  private: static int64_t s_Updated##name;                       \
-#define STRING_CONFIG(name, unused_private_key, unused_public_key, unused_doc) \
-  public: static GCConfigStringHolder Get##name();
-GC_CONFIGURATION_KEYS
-#undef BOOL_CONFIG
-#undef INT_CONFIG
-#undef STRING_CONFIG
-public:
-  static void RefreshHeapHardLimitSettings();
-  static void EnumerateConfigurationValues(void* context, ConfigurationValueFunc configurationValueFunc);
-enum HeapVerifyFlags {
-    HEAPVERIFY_NONE             = 0,
-    HEAPVERIFY_GC               = 1,   // Verify the heap at beginning and end of GC
-    HEAPVERIFY_BARRIERCHECK     = 2,   // Verify the brick table
-    HEAPVERIFY_SYNCBLK          = 4,   // Verify sync block scanning
-    HEAPVERIFY_NO_RANGE_CHECKS  = 0x10,   // Excludes checking if an OBJECTREF is within the bounds of the managed heap
-    HEAPVERIFY_NO_MEM_FILL      = 0x20,   // Excludes filling unused segment portions with fill pattern
-    HEAPVERIFY_POST_GC_ONLY     = 0x40,   // Performs heap verification post-GCs only (instead of before and after each GC)
-    HEAPVERIFY_DEEP_ON_COMPACT  = 0x80    // Performs deep object verfication only on compacting GCs.
-};
-enum WriteBarrierFlavor
-{
-    WRITE_BARRIER_DEFAULT = 0,
-    WRITE_BARRIER_REGION_BIT = 1,
-    WRITE_BARRIER_REGION_BYTE = 2,
-    WRITE_BARRIER_SERVER = 3,
-};
-static void Initialize();
-};
-bool ParseGCHeapAffinitizeRanges(const char* cpu_index_ranges, AffinitySet* config_affinity_set, uintptr_t& config_affinity_mask);
-#endif // __GCCONFIG_H__

--- a/src/coreclr/gc/unix/gcenv.unix.cpp
+++ b//dev/null
@@ -1,1051 +0,0 @@
-#define _WITH_GETLINE
-#include <cstdint>
-#include <cstddef>
-#include <cstdio>
-#include <cassert>
-#define __STDC_FORMAT_MACROS
-#include <cinttypes>
-#include <memory>
-#include <pthread.h>
-#include <signal.h>
-#include "config.gc.h"
-#include "common.h"
-#include "gcenv.structs.h"
-#include "gcenv.base.h"
-#include "gcenv.os.h"
-#include "gcenv.ee.h"
-#include "gcenv.unix.inl"
-#include "volatile.h"
-#include "gcconfig.h"
-#include "numasupport.h"
-#if HAVE_SWAPCTL
-#include <sys/swap.h>
-#endif
-#undef min
-#undef max
-#ifndef __has_cpp_attribute
-#define __has_cpp_attribute(x) (0)
-#endif
-#if __has_cpp_attribute(fallthrough)
-#define FALLTHROUGH [[fallthrough]]
-#else
-#define FALLTHROUGH
-#endif
-#include <algorithm>
-#if HAVE_SYS_TIME_H
- #include <sys/time.h>
-#else
- #error "sys/time.h required by GC PAL for the time being"
-#endif
-#if HAVE_SYS_MMAN_H
- #include <sys/mman.h>
-#else
- #error "sys/mman.h required by GC PAL"
-#endif
-#if HAVE_SYSCTLBYNAME
-#include <sys/types.h>
-#include <sys/sysctl.h>
-#endif
-#if HAVE_SYSINFO
-#include <sys/sysinfo.h>
-#endif
-#if HAVE_XSWDEV
-#include <vm/vm_param.h>
-#endif // HAVE_XSWDEV
-#ifdef __APPLE__
-#include <mach/vm_types.h>
-#include <mach/vm_param.h>
-#include <mach/mach_port.h>
-#include <mach/mach_host.h>
-#include <mach/task.h>
-#include <mach/vm_map.h>
-extern "C"
-{
-#  include <mach/thread_state.h>
-}
-#define CHECK_MACH(_msg, machret) do {                                      \
-        if (machret != KERN_SUCCESS)                                        \
-        {                                                                   \
-            char _szError[1024];                                            \
-            snprintf(_szError, ARRAY_SIZE(_szError), "%s: %u: %s", __FUNCTION__, __LINE__, _msg);  \
-            mach_error(_szError, machret);                                  \
-            abort();                                                        \
-        }                                                                   \
-    } while (false)
-#endif // __APPLE__
-#ifdef __linux__
-#include <sys/syscall.h> // __NR_membarrier
-# if !defined(__NR_membarrier)
-#  if defined(__amd64__)
-#   define __NR_membarrier  324
-#  elif defined(__i386__)
-#   define __NR_membarrier  375
-#  elif defined(__arm__)
-#   define __NR_membarrier  389
-#  elif defined(__aarch64__)
-#   define __NR_membarrier  283
-#  elif defined(__loongarch64)
-#   define __NR_membarrier  283
-#  else
-#   error Unknown architecture
-#  endif
-# endif
-#endif
-#if HAVE_PTHREAD_NP_H
-#include <pthread_np.h>
-#endif
-#if HAVE_CPUSET_T
-typedef cpuset_t cpu_set_t;
-#endif
-#include <time.h> // nanosleep
-#include <sched.h> // sched_yield
-#include <errno.h>
-#include <unistd.h> // sysconf
-#include "globals.h"
-#include "cgroup.h"
-#ifndef __APPLE__
-#if HAVE_SYSCONF && HAVE__SC_AVPHYS_PAGES
-#define SYSCONF_PAGES _SC_AVPHYS_PAGES
-#elif HAVE_SYSCONF && HAVE__SC_PHYS_PAGES
-#define SYSCONF_PAGES _SC_PHYS_PAGES
-#else
-#error Dont know how to get page-size on this architecture!
-#endif
-#endif // __APPLE__
-#if defined(HOST_ARM) || defined(HOST_ARM64) || defined(HOST_LOONGARCH64) || defined(HOST_RISCV64)
-#define SYSCONF_GET_NUMPROCS _SC_NPROCESSORS_CONF
-#else
-#define SYSCONF_GET_NUMPROCS _SC_NPROCESSORS_ONLN
-#endif
-static uint32_t g_totalCpuCount = 0;
-#ifdef __NR_membarrier
-# define membarrier(...)  syscall(__NR_membarrier, __VA_ARGS__)
-#else
-# define membarrier(...)  -ENOSYS
-#endif
-enum membarrier_cmd
-{
-    MEMBARRIER_CMD_QUERY                                 = 0,
-    MEMBARRIER_CMD_GLOBAL                                = (1 << 0),
-    MEMBARRIER_CMD_GLOBAL_EXPEDITED                      = (1 << 1),
-    MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED             = (1 << 2),
-    MEMBARRIER_CMD_PRIVATE_EXPEDITED                     = (1 << 3),
-    MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED            = (1 << 4),
-    MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE           = (1 << 5),
-    MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE  = (1 << 6)
-};
-bool CanFlushUsingMembarrier()
-{
-#ifdef TARGET_ANDROID
-    int apiLevel = android_get_device_api_level();
-    if (apiLevel < __ANDROID_API_Q__)
-    {
-        return false;
-    }
-#endif
-    int mask = membarrier(MEMBARRIER_CMD_QUERY, 0);
-    if (mask >= 0 &&
-        mask & MEMBARRIER_CMD_PRIVATE_EXPEDITED &&
-        membarrier(MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED, 0) == 0)
-    {
-        return true;
-    }
-    return false;
-}
-static int s_flushUsingMemBarrier = 0;
-static uint8_t* g_helperPage = 0;
-static pthread_mutex_t g_flushProcessWriteBuffersMutex;
-size_t GetRestrictedPhysicalMemoryLimit();
-bool GetPhysicalMemoryUsed(size_t* val);
-static size_t g_RestrictedPhysicalMemoryLimit = 0;
-uint32_t g_pageSizeUnixInl = 0;
-AffinitySet g_processAffinitySet;
-extern "C" int g_highestNumaNode;
-extern "C" bool g_numaAvailable;
-bool GCToOSInterface::Initialize()
-{
-    int pageSize = sysconf( _SC_PAGE_SIZE );
-    g_pageSizeUnixInl = uint32_t((pageSize > 0) ? pageSize : 0x1000);
-    int cpuCount = sysconf(SYSCONF_GET_NUMPROCS);
-    if (cpuCount == -1)
-    {
-        return false;
-    }
-    g_totalCpuCount = cpuCount;
-    assert(s_flushUsingMemBarrier == 0);
-    if (CanFlushUsingMembarrier())
-    {
-        s_flushUsingMemBarrier = TRUE;
-    }
-#ifndef TARGET_APPLE
-    else
-    {
-        assert(g_helperPage == 0);
-        g_helperPage = static_cast<uint8_t*>(mmap(0, OS_PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_PRIVATE, -1, 0));
-        if (g_helperPage == MAP_FAILED)
-        {
-            return false;
-        }
-        assert((((size_t)g_helperPage) & (OS_PAGE_SIZE - 1)) == 0);
-        int status = mlock(g_helperPage, OS_PAGE_SIZE);
-        if (status != 0)
-        {
-            return false;
-        }
-        status = pthread_mutex_init(&g_flushProcessWriteBuffersMutex, NULL);
-        if (status != 0)
-        {
-            munlock(g_helperPage, OS_PAGE_SIZE);
-            return false;
-        }
-    }
-#endif // !TARGET_APPLE
-    InitializeCGroup();
-#if HAVE_SCHED_GETAFFINITY
-    cpu_set_t cpuSet;
-    int st = sched_getaffinity(getpid(), sizeof(cpu_set_t), &cpuSet);
-    if (st == 0)
-    {
-        for (size_t i = 0; i < CPU_SETSIZE; i++)
-        {
-            if (CPU_ISSET(i, &cpuSet))
-            {
-                g_processAffinitySet.Add(i);
-            }
-        }
-    }
-    else
-    {
-        assert(false);
-    }
-#else // HAVE_SCHED_GETAFFINITY
-    for (size_t i = 0; i < g_totalCpuCount; i++)
-    {
-        g_processAffinitySet.Add(i);
-    }
-#endif // HAVE_SCHED_GETAFFINITY
-    NUMASupportInitialize();
-    return true;
-}
-void GCToOSInterface::Shutdown()
-{
-    int ret = munlock(g_helperPage, OS_PAGE_SIZE);
-    assert(ret == 0);
-    ret = pthread_mutex_destroy(&g_flushProcessWriteBuffersMutex);
-    assert(ret == 0);
-    munmap(g_helperPage, OS_PAGE_SIZE);
-    CleanupCGroup();
-}
-uint64_t GCToOSInterface::GetCurrentThreadIdForLogging()
-{
-#if defined(__linux__)
-    return (uint64_t)syscall(SYS_gettid);
-#elif HAVE_PTHREAD_GETTHREADID_NP
-    return (uint64_t)pthread_getthreadid_np();
-#elif HAVE_PTHREAD_THREADID_NP
-    unsigned long long tid;
-    pthread_threadid_np(pthread_self(), &tid);
-    return (uint64_t)tid;
-#else
-    return (uint64_t)pthread_self();
-#endif
-}
-uint32_t GCToOSInterface::GetCurrentProcessId()
-{
-    return getpid();
-}
-bool GCToOSInterface::SetCurrentThreadIdealAffinity(uint16_t srcProcNo, uint16_t dstProcNo)
-{
-    return true;
-}
-uint32_t GCToOSInterface::GetCurrentProcessorNumber()
-{
-#if HAVE_SCHED_GETCPU
-    int processorNumber = sched_getcpu();
-    assert(processorNumber != -1);
-    return processorNumber;
-#else
-    assert(false); // This method is expected to be called only if CanGetCurrentProcessorNumber is true
-    return 0;
-#endif
-}
-bool GCToOSInterface::CanGetCurrentProcessorNumber()
-{
-    return HAVE_SCHED_GETCPU;
-}
-void GCToOSInterface::FlushProcessWriteBuffers()
-{
-    if (s_flushUsingMemBarrier)
-    {
-        int status = membarrier(MEMBARRIER_CMD_PRIVATE_EXPEDITED, 0);
-        assert(status == 0 && "Failed to flush using membarrier");
-    }
-    else if (g_helperPage != 0)
-    {
-        int status = pthread_mutex_lock(&g_flushProcessWriteBuffersMutex);
-        assert(status == 0 && "Failed to lock the flushProcessWriteBuffersMutex lock");
-        status = mprotect(g_helperPage, OS_PAGE_SIZE, PROT_READ | PROT_WRITE);
-        assert(status == 0 && "Failed to change helper page protection to read / write");
-        __sync_add_and_fetch((size_t*)g_helperPage, 1);
-        status = mprotect(g_helperPage, OS_PAGE_SIZE, PROT_NONE);
-        assert(status == 0 && "Failed to change helper page protection to no access");
-        status = pthread_mutex_unlock(&g_flushProcessWriteBuffersMutex);
-        assert(status == 0 && "Failed to unlock the flushProcessWriteBuffersMutex lock");
-    }
-#ifdef TARGET_APPLE
-    else
-    {
-        mach_msg_type_number_t cThreads;
-        thread_act_t *pThreads;
-        kern_return_t machret = task_threads(mach_task_self(), &pThreads, &cThreads);
-        CHECK_MACH("task_threads()", machret);
-        uintptr_t sp;
-        uintptr_t registerValues[128];
-        for (mach_msg_type_number_t i = 0; i < cThreads; i++)
-        {
-            if (__builtin_available (macOS 10.14, iOS 12, tvOS 9, *))
-            {
-                size_t registers = 128;
-                machret = thread_get_register_pointer_values(pThreads[i], &sp, &registers, registerValues);
-            }
-            else
-            {
-#if defined(HOST_AMD64)
-                x86_thread_state64_t threadState;
-                mach_msg_type_number_t count = x86_THREAD_STATE64_COUNT;
-                machret = thread_get_state(pThreads[i], x86_THREAD_STATE64, (thread_state_t)&threadState, &count);
-#elif defined(HOST_ARM64)
-                arm_thread_state64_t threadState;
-                mach_msg_type_number_t count = ARM_THREAD_STATE64_COUNT;
-                machret = thread_get_state(pThreads[i], ARM_THREAD_STATE64, (thread_state_t)&threadState, &count);
-#else
-                #error Unexpected architecture
-#endif
-            }
-            if (machret == KERN_INSUFFICIENT_BUFFER_SIZE)
-            {
-                CHECK_MACH("thread_get_register_pointer_values()", machret);
-            }
-            machret = mach_port_deallocate(mach_task_self(), pThreads[i]);
-            CHECK_MACH("mach_port_deallocate()", machret);
-        }
-        machret = vm_deallocate(mach_task_self(), (vm_address_t)pThreads, cThreads * sizeof(thread_act_t));
-        CHECK_MACH("vm_deallocate()", machret);
-    }
-#endif // TARGET_APPLE
-}
-void GCToOSInterface::DebugBreak()
-{
-#if __has_builtin(__builtin_debugtrap)
-    __builtin_debugtrap();
-#else
-    raise(SIGTRAP);
-#endif
-}
-void GCToOSInterface::Sleep(uint32_t sleepMSec)
-{
-    if (sleepMSec == 0)
-    {
-        return;
-    }
-    timespec requested;
-    requested.tv_sec = sleepMSec / tccSecondsToMilliSeconds;
-    requested.tv_nsec = (sleepMSec - requested.tv_sec * tccSecondsToMilliSeconds) * tccMilliSecondsToNanoSeconds;
-    timespec remaining;
-    while (nanosleep(&requested, &remaining) == EINTR)
-    {
-        requested = remaining;
-    }
-}
-void GCToOSInterface::YieldThread(uint32_t switchCount)
-{
-    int ret = sched_yield();
-    assert(ret == 0);
-}
-static void* VirtualReserveInner(size_t size, size_t alignment, uint32_t flags, uint32_t hugePagesFlag = 0)
-{
-    assert(!(flags & VirtualReserveFlags::WriteWatch) && "WriteWatch not supported on Unix");
-    if (alignment < OS_PAGE_SIZE)
-    {
-        alignment = OS_PAGE_SIZE;
-    }
-    size_t alignedSize = size + (alignment - OS_PAGE_SIZE);
-    void * pRetVal = mmap(nullptr, alignedSize, PROT_NONE, MAP_ANON | MAP_PRIVATE | hugePagesFlag, -1, 0);
-    if (pRetVal != MAP_FAILED)
-    {
-        void * pAlignedRetVal = (void *)(((size_t)pRetVal + (alignment - 1)) & ~(alignment - 1));
-        size_t startPadding = (size_t)pAlignedRetVal - (size_t)pRetVal;
-        if (startPadding != 0)
-        {
-            int ret = munmap(pRetVal, startPadding);
-            assert(ret == 0);
-        }
-        size_t endPadding = alignedSize - (startPadding + size);
-        if (endPadding != 0)
-        {
-            int ret = munmap((void *)((size_t)pAlignedRetVal + size), endPadding);
-            assert(ret == 0);
-        }
-        pRetVal = pAlignedRetVal;
-#ifdef MADV_DONTDUMP
-        madvise(pRetVal, size, MADV_DONTDUMP);
-#endif
-        return pRetVal;
-    }
-    return NULL; // return NULL if mmap failed
-}
-void* GCToOSInterface::VirtualReserve(size_t size, size_t alignment, uint32_t flags, uint16_t node)
-{
-    return VirtualReserveInner(size, alignment, flags);
-}
-bool GCToOSInterface::VirtualRelease(void* address, size_t size)
-{
-    int ret = munmap(address, size);
-    return (ret == 0);
-}
-void* GCToOSInterface::VirtualReserveAndCommitLargePages(size_t size, uint16_t node)
-{
-#if HAVE_MAP_HUGETLB
-    uint32_t largePagesFlag = MAP_HUGETLB;
-#elif HAVE_VM_FLAGS_SUPERPAGE_SIZE_ANY
-    uint32_t largePagesFlag = VM_FLAGS_SUPERPAGE_SIZE_ANY;
-#else
-    uint32_t largePagesFlag = 0;
-#endif
-    void* pRetVal = VirtualReserveInner(size, OS_PAGE_SIZE, 0, largePagesFlag);
-    if (VirtualCommit(pRetVal, size, node))
-    {
-        return pRetVal;
-    }
-    return nullptr;
-}
-bool GCToOSInterface::VirtualCommit(void* address, size_t size, uint16_t node)
-{
-    bool success = mprotect(address, size, PROT_WRITE | PROT_READ) == 0;
-#ifdef MADV_DODUMP
-    if (success)
-    {
-        madvise(address, size, MADV_DODUMP);
-    }
-#endif
-#ifdef TARGET_LINUX
-    if (success && g_numaAvailable && (node != NUMA_NODE_UNDEFINED))
-    {
-        if ((int)node <= g_highestNumaNode)
-        {
-            int usedNodeMaskBits = g_highestNumaNode + 1;
-            int nodeMaskLength = (usedNodeMaskBits + sizeof(unsigned long) - 1) / sizeof(unsigned long);
-            unsigned long nodeMask[nodeMaskLength];
-            memset(nodeMask, 0, sizeof(nodeMask));
-            int index = node / sizeof(unsigned long);
-            nodeMask[index] = ((unsigned long)1) << (node & (sizeof(unsigned long) - 1));
-            int st = BindMemoryPolicy(address, size, nodeMask, usedNodeMaskBits);
-            assert(st == 0);
-        }
-    }
-#endif // TARGET_LINUX
-    return success;
-}
-bool GCToOSInterface::VirtualDecommit(void* address, size_t size)
-{
-    bool bRetVal = mmap(address, size, PROT_NONE, MAP_FIXED | MAP_ANON | MAP_PRIVATE, -1, 0) != MAP_FAILED;
-#ifdef MADV_DONTDUMP
-    if (bRetVal)
-    {
-        madvise(address, size, MADV_DONTDUMP);
-    }
-#endif
-    return  bRetVal;
-}
-bool GCToOSInterface::VirtualReset(void * address, size_t size, bool unlock)
-{
-    int st;
-#if HAVE_MADV_FREE
-    st = madvise(address, size, MADV_FREE);
-    if (st != 0)
-#endif
-    {
-#if HAVE_POSIX_MADVISE
-        st = posix_madvise(address, size, MADV_DONTNEED);
-#else
-        st = EINVAL;
-#endif
-    }
-#ifdef MADV_DONTDUMP
-    if (st == 0)
-    {
-        madvise(address, size, MADV_DONTDUMP);
-    }
-#endif
-    return (st == 0);
-}
-bool GCToOSInterface::SupportsWriteWatch()
-{
-    return false;
-}
-void GCToOSInterface::ResetWriteWatch(void* address, size_t size)
-{
-    assert(!"should never call ResetWriteWatch on Unix");
-}
-bool GCToOSInterface::GetWriteWatch(bool resetState, void* address, size_t size, void** pageAddresses, uintptr_t* pageAddressesCount)
-{
-    assert(!"should never call GetWriteWatch on Unix");
-    return false;
-}
-bool ReadMemoryValueFromFile(const char* filename, uint64_t* val)
-{
-    bool result = false;
-    char* line = nullptr;
-    size_t lineLen = 0;
-    char* endptr = nullptr;
-    uint64_t num = 0, l, multiplier;
-    FILE* file = nullptr;
-    if (val == nullptr)
-        goto done;
-    file = fopen(filename, "r");
-    if (file == nullptr)
-        goto done;
-    if (getline(&line, &lineLen, file) == -1)
-        goto done;
-    errno = 0;
-    num = strtoull(line, &endptr, 0);
-    if (line == endptr || errno != 0)
-        goto done;
-    multiplier = 1;
-    switch (*endptr)
-    {
-    case 'g':
-    case 'G': multiplier = 1024;
-              FALLTHROUGH;
-    case 'm':
-    case 'M': multiplier = multiplier * 1024;
-              FALLTHROUGH;
-    case 'k':
-    case 'K': multiplier = multiplier * 1024;
-    }
-    *val = num * multiplier;
-    result = true;
-    if (*val / multiplier != num)
-        result = false;
-done:
-    if (file)
-        fclose(file);
-    free(line);
-    return result;
-}
-static void GetLogicalProcessorCacheSizeFromSysConf(size_t* cacheLevel, size_t* cacheSize)
-{
-    assert (cacheLevel != nullptr);
-    assert (cacheSize != nullptr);
-#if defined(_SC_LEVEL1_DCACHE_SIZE) || defined(_SC_LEVEL2_CACHE_SIZE) || defined(_SC_LEVEL3_CACHE_SIZE) || defined(_SC_LEVEL4_CACHE_SIZE)
-    const int cacheLevelNames[] =
-    {
-        _SC_LEVEL1_DCACHE_SIZE,
-        _SC_LEVEL2_CACHE_SIZE,
-        _SC_LEVEL3_CACHE_SIZE,
-        _SC_LEVEL4_CACHE_SIZE,
-    };
-    for (int i = ARRAY_SIZE(cacheLevelNames) - 1; i >= 0; i--)
-    {
-        long size = sysconf(cacheLevelNames[i]);
-        if (size > 0)
-        {
-            *cacheSize = (size_t)size;
-            *cacheLevel = i + 1;
-            break;
-        }
-    }
-#endif
-}
-static void GetLogicalProcessorCacheSizeFromSysFs(size_t* cacheLevel, size_t* cacheSize)
-{
-    assert (cacheLevel != nullptr);
-    assert (cacheSize != nullptr);
-#if defined(TARGET_LINUX) && !defined(HOST_ARM) && !defined(HOST_X86)
-    size_t level;
-    char path_to_size_file[] =  "/sys/devices/system/cpu/cpu0/cache/index-/size";
-    char path_to_level_file[] =  "/sys/devices/system/cpu/cpu0/cache/index-/level";
-    int index = 40;
-    assert(path_to_size_file[index] == '-');
-    assert(path_to_level_file[index] == '-');
-    for (int i = 0; i < 5; i++)
-    {
-        path_to_size_file[index] = (char)(48 + i);
-        uint64_t cache_size_from_sys_file = 0;
-        if (ReadMemoryValueFromFile(path_to_size_file, &cache_size_from_sys_file))
-        {
-            *cacheSize = std::max(*cacheSize, (size_t)cache_size_from_sys_file);
-            path_to_level_file[index] = (char)(48 + i);
-            if (ReadMemoryValueFromFile(path_to_level_file, &level))
-            {
-                *cacheLevel = level;
-            }
-        }
-    }
-#endif 
-}
-static void GetLogicalProcessorCacheSizeFromHeuristic(size_t* cacheLevel, size_t* cacheSize)
-{
-    assert (cacheLevel != nullptr);
-    assert (cacheSize != nullptr);
-#if (defined(TARGET_LINUX) && !defined(TARGET_APPLE))
-    {
-        DWORD logicalCPUs = g_processAffinitySet.Count();
-        if (logicalCPUs < 5)
-        {
-            *cacheSize = 4;
-        }
-        else if (logicalCPUs < 17)
-        {
-            *cacheSize = 8;
-        }
-        else if (logicalCPUs < 65)
-        {
-            *cacheSize = 16;
-        }
-        else
-        {
-            *cacheSize = 32;
-        }
-        *cacheSize *= (1024 * 1024);
-    }
-#endif
-}
-static size_t GetLogicalProcessorCacheSizeFromOS()
-{
-    size_t cacheLevel = 0;
-    size_t cacheSize = 0;
-    if (GCConfig::GetGCCacheSizeFromSysConf())
-    {
-        GetLogicalProcessorCacheSizeFromSysConf(&cacheLevel, &cacheSize);
-    }
-    if (cacheSize == 0) 
-    {
-        GetLogicalProcessorCacheSizeFromSysFs(&cacheLevel, &cacheSize);
-        if (cacheSize == 0)
-        {
-            GetLogicalProcessorCacheSizeFromHeuristic(&cacheLevel, &cacheSize);
-        }
-    }
-#if HAVE_SYSCTLBYNAME
-    if (cacheSize == 0)
-    {
-        int64_t cacheSizeFromSysctl = 0;
-        size_t sz = sizeof(cacheSizeFromSysctl);
-        const bool success = false
-            || sysctlbyname("hw.perflevel0.l3cachesize", &cacheSizeFromSysctl, &sz, nullptr, 0) == 0
-            || sysctlbyname("hw.perflevel0.l2cachesize", &cacheSizeFromSysctl, &sz, nullptr, 0) == 0
-            || sysctlbyname("hw.l3cachesize", &cacheSizeFromSysctl, &sz, nullptr, 0) == 0
-            || sysctlbyname("hw.l2cachesize", &cacheSizeFromSysctl, &sz, nullptr, 0) == 0
-            || sysctlbyname("hw.l1dcachesize", &cacheSizeFromSysctl, &sz, nullptr, 0) == 0;
-        if (success)
-        {
-            assert(cacheSizeFromSysctl > 0);
-            cacheSize = (size_t) cacheSizeFromSysctl;
-        }
-    }
-#endif
-#if (defined(HOST_ARM64) || defined(HOST_LOONGARCH64)) && !defined(TARGET_APPLE)
-    if (cacheLevel != 3)
-    {
-        GetLogicalProcessorCacheSizeFromHeuristic(&cacheLevel, &cacheSize);
-    }
-#endif
-    return cacheSize;
-}
-static uint64_t GetMemorySizeMultiplier(char units)
-{
-    switch(units)
-    {
-        case 'g':
-        case 'G': return 1024 * 1024 * 1024;
-        case 'm':
-        case 'M': return 1024 * 1024;
-        case 'k':
-        case 'K': return 1024;
-    }
-    return 1;
-}
-#ifndef __APPLE__
-static bool ReadMemAvailable(uint64_t* memAvailable)
-{
-    bool foundMemAvailable = false;
-    FILE* memInfoFile = fopen("/proc/meminfo", "r");
-    if (memInfoFile != NULL)
-    {
-        char *line = nullptr;
-        size_t lineLen = 0;
-        while (getline(&line, &lineLen, memInfoFile) != -1)
-        {
-            char units = '\0';
-            uint64_t available;
-            int fieldsParsed = sscanf(line, "MemAvailable: %" SCNu64 " %cB", &available, &units);
-            if (fieldsParsed >= 1)
-            {
-                uint64_t multiplier = GetMemorySizeMultiplier(units);
-                *memAvailable = available * multiplier;
-                foundMemAvailable = true;
-                break;
-            }
-        }
-        free(line);
-        fclose(memInfoFile);
-    }
-    return foundMemAvailable;
-}
-#endif // __APPLE__
-size_t GCToOSInterface::GetCacheSizePerLogicalCpu(bool trueSize)
-{
-    static volatile size_t s_maxSize;
-    static volatile size_t s_maxTrueSize;
-    size_t size = trueSize ? s_maxTrueSize : s_maxSize;
-    if (size != 0)
-        return size;
-    size_t maxSize, maxTrueSize;
-    maxSize = maxTrueSize = GetLogicalProcessorCacheSizeFromOS(); // Returns the size of the highest level processor cache
-    s_maxSize = maxSize;
-    s_maxTrueSize = maxTrueSize;
-    return trueSize ? maxTrueSize : maxSize;
-}
-bool GCToOSInterface::SetThreadAffinity(uint16_t procNo)
-{
-#if HAVE_SCHED_SETAFFINITY || HAVE_PTHREAD_SETAFFINITY_NP
-    cpu_set_t cpuSet;
-    CPU_ZERO(&cpuSet);
-    CPU_SET((int)procNo, &cpuSet);
-#if HAVE_SCHED_SETAFFINITY
-    int st = sched_setaffinity(0, sizeof(cpu_set_t), &cpuSet);
-#else
-    int st = pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuSet);
-#endif
-    return (st == 0);
-#else  // !(HAVE_SCHED_SETAFFINITY || HAVE_PTHREAD_SETAFFINITY_NP)
-    return false;
-#endif // HAVE_SCHED_SETAFFINITY || HAVE_PTHREAD_SETAFFINITY_NP
-}
-bool GCToOSInterface::BoostThreadPriority()
-{
-    return false;
-}
-const AffinitySet* GCToOSInterface::SetGCThreadsAffinitySet(uintptr_t configAffinityMask, const AffinitySet* configAffinitySet)
-{
-    if (!configAffinitySet->IsEmpty())
-    {
-        for (size_t i = 0; i < MAX_SUPPORTED_CPUS; i++)
-        {
-            if (g_processAffinitySet.Contains(i) && !configAffinitySet->Contains(i))
-            {
-                g_processAffinitySet.Remove(i);
-            }
-        }
-    }
-    return &g_processAffinitySet;
-}
-size_t GCToOSInterface::GetVirtualMemoryLimit()
-{
-#ifdef HOST_64BIT
-#ifndef TARGET_RISCV64
-    static const uint64_t _128TB = (1ull << 47);
-    return _128TB;
-#else // TARGET_RISCV64
-    static const uint64_t _256GB = (1ull << 38);
-    return _256GB;
-#endif // TARGET_RISCV64
-#else
-    return (size_t)-1;
-#endif
-}
-uint64_t GCToOSInterface::GetPhysicalMemoryLimit(bool* is_restricted, bool refresh)
-{
-    size_t restricted_limit;
-    if (is_restricted)
-        *is_restricted = false;
-    if (g_RestrictedPhysicalMemoryLimit == 0 || refresh)
-    {
-        restricted_limit = GetRestrictedPhysicalMemoryLimit();
-        VolatileStore(&g_RestrictedPhysicalMemoryLimit, restricted_limit);
-    }
-    restricted_limit = g_RestrictedPhysicalMemoryLimit;
-    if (restricted_limit != 0 && restricted_limit != SIZE_T_MAX)
-    {
-        if (is_restricted)
-            *is_restricted = true;
-        return restricted_limit;
-    }
-#if HAVE_SYSCONF && HAVE__SC_PHYS_PAGES
-    long pages = sysconf(_SC_PHYS_PAGES);
-    if (pages == -1)
-    {
-        return 0;
-    }
-    long pageSize = sysconf(_SC_PAGE_SIZE);
-    if (pageSize == -1)
-    {
-        return 0;
-    }
-    return (uint64_t)pages * (uint64_t)pageSize;
-#elif HAVE_SYSCTL
-    int mib[3];
-    mib[0] = CTL_HW;
-    mib[1] = HW_MEMSIZE;
-    int64_t physical_memory = 0;
-    size_t length = sizeof(INT64);
-    int rc = sysctl(mib, 2, &physical_memory, &length, NULL, 0);
-    assert(rc == 0);
-    return physical_memory;
-#else // HAVE_SYSCTL
-#error "Don't know how to get total physical memory on this platform"
-#endif // HAVE_SYSCTL
-}
-uint64_t GetAvailablePhysicalMemory()
-{
-    uint64_t available = 0;
-#ifndef __APPLE__
-    static volatile bool tryReadMemInfo = true;
-    if (tryReadMemInfo)
-    {
-        tryReadMemInfo = ReadMemAvailable(&available);
-    }
-    if (!tryReadMemInfo)
-    {
-        available = sysconf(SYSCONF_PAGES) * sysconf(_SC_PAGE_SIZE);
-    }
-#else // __APPLE__
-    vm_size_t page_size;
-    mach_port_t mach_port;
-    mach_msg_type_number_t count;
-    vm_statistics_data_t vm_stats;
-    mach_port = mach_host_self();
-    count = sizeof(vm_stats) / sizeof(natural_t);
-    if (KERN_SUCCESS == host_page_size(mach_port, &page_size))
-    {
-        if (KERN_SUCCESS == host_statistics(mach_port, HOST_VM_INFO, (host_info_t)&vm_stats, &count))
-        {
-            available = (int64_t)vm_stats.free_count * (int64_t)page_size;
-        }
-    }
-    mach_port_deallocate(mach_task_self(), mach_port);
-#endif // __APPLE__
-    return available;
-}
-uint64_t GetAvailablePageFile()
-{
-    uint64_t available = 0;
-    int mib[3];
-    int rc;
-#if HAVE_XSW_USAGE
-    struct xsw_usage xsu;
-    mib[0] = CTL_VM;
-    mib[1] = VM_SWAPUSAGE;
-    size_t length = sizeof(xsu);
-    rc = sysctl(mib, 2, &xsu, &length, NULL, 0);
-    if (rc == 0)
-    {
-        available = xsu.xsu_avail;
-    }
-#elif HAVE_XSWDEV
-    struct xswdev xsw;
-    size_t length = 2;
-    rc = sysctlnametomib("vm.swap_info", mib, &length);
-    if (rc == 0)
-    {
-        int pagesize = getpagesize();
-        for (mib[2] = 0; ; mib[2]++)
-        {
-            length = sizeof(xsw);
-            rc = sysctl(mib, 3, &xsw, &length, NULL, 0);
-            if ((rc < 0) || (xsw.xsw_version != XSWDEV_VERSION))
-            {
-                break;
-            }
-            uint64_t avail = xsw.xsw_nblks - xsw.xsw_used;
-            available += avail * pagesize;
-        }
-    }
-#elif HAVE_SWAPCTL
-    struct anoninfo ai;
-    if (swapctl(SC_AINFO, &ai) != -1)
-    {
-        int pagesize = getpagesize();
-        available = ai.ani_free * pagesize;
-    }
-#elif HAVE_SYSINFO
-    struct sysinfo info;
-    rc = sysinfo(&info);
-    if (rc == 0)
-    {
-        available = info.freeswap;
-#if HAVE_SYSINFO_WITH_MEM_UNIT
-        available *= info.mem_unit;
-#endif // HAVE_SYSINFO_WITH_MEM_UNIT
-    }
-#endif // HAVE_SYSINFO
-    return available;
-}
-void GCToOSInterface::GetMemoryStatus(uint64_t restricted_limit, uint32_t* memory_load, uint64_t* available_physical, uint64_t* available_page_file)
-{
-    uint64_t available = 0;
-    uint32_t load = 0;
-    if (memory_load != nullptr || available_physical != nullptr)
-    {
-        size_t used;
-        if (restricted_limit != 0)
-        {
-            if (GetPhysicalMemoryUsed(&used))
-            {
-                available = restricted_limit > used ? restricted_limit - used : 0;
-                load = (uint32_t)(((float)used * 100) / (float)restricted_limit);
-            }
-        }
-        else
-        {
-            available = GetAvailablePhysicalMemory();
-            if (memory_load != NULL)
-            {
-                bool isRestricted;
-                uint64_t total = GetPhysicalMemoryLimit(&isRestricted);
-                if (total > available)
-                {
-                    used = total - available;
-                    load = (uint32_t)(((float)used * 100) / (float)total);
-                }
-            }
-        }
-    }
-    if (available_physical != NULL)
-        *available_physical = available;
-    if (memory_load != nullptr)
-        *memory_load = load;
-    if (available_page_file != nullptr)
-        *available_page_file = GetAvailablePageFile();
-}
-int64_t GCToOSInterface::QueryPerformanceCounter()
-{
-#if HAVE_CLOCK_GETTIME_NSEC_NP
-    return (int64_t)clock_gettime_nsec_np(CLOCK_UPTIME_RAW);
-#elif HAVE_CLOCK_MONOTONIC
-    struct timespec ts;
-    int result = clock_gettime(CLOCK_MONOTONIC, &ts);
-    if (result != 0)
-    {
-        assert(!"clock_gettime(CLOCK_MONOTONIC) failed");
-        __UNREACHABLE();
-    }
-    return ((int64_t)(ts.tv_sec) * (int64_t)(tccSecondsToNanoSeconds)) + (int64_t)(ts.tv_nsec);
-#else
-#error " clock_gettime(CLOCK_MONOTONIC) or clock_gettime_nsec_np() must be supported."
-#endif
-}
-int64_t GCToOSInterface::QueryPerformanceFrequency()
-{
-    return tccSecondsToNanoSeconds;
-}
-uint64_t GCToOSInterface::GetLowPrecisionTimeStamp()
-{
-    uint64_t retval = 0;
-#if HAVE_CLOCK_GETTIME_NSEC_NP
-    retval = clock_gettime_nsec_np(CLOCK_UPTIME_RAW) / tccMilliSecondsToNanoSeconds;
-#elif HAVE_CLOCK_MONOTONIC
-    struct timespec ts;
-#if HAVE_CLOCK_MONOTONIC_COARSE
-    clockid_t clockType = CLOCK_MONOTONIC_COARSE; // good enough resolution, fastest speed
-#else
-    clockid_t clockType = CLOCK_MONOTONIC;
-#endif
-    if (clock_gettime(clockType, &ts) != 0)
-    {
-#if HAVE_CLOCK_MONOTONIC_COARSE
-        assert(!"clock_gettime(HAVE_CLOCK_MONOTONIC_COARSE) failed\n");
-#else
-        assert(!"clock_gettime(CLOCK_MONOTONIC) failed\n");
-#endif
-    }
-    retval = (ts.tv_sec * tccSecondsToMilliSeconds) + (ts.tv_nsec / tccMilliSecondsToNanoSeconds);
-#else
-    struct timeval tv;
-    if (gettimeofday(&tv, NULL) == 0)
-    {
-        retval = (tv.tv_sec * tccSecondsToMilliSeconds) + (tv.tv_usec / tccMilliSecondsToMicroSeconds);
-    }
-    else
-    {
-        assert(!"gettimeofday() failed\n");
-    }
-#endif
-    return retval;
-}
-uint32_t GCToOSInterface::GetTotalProcessorCount()
-{
-    return g_totalCpuCount;
-}
-bool GCToOSInterface::CanEnableGCNumaAware()
-{
-    return g_numaAvailable;
-}
-bool GCToOSInterface::CanEnableGCCPUGroups()
-{
-    return false;
-}
-bool GCToOSInterface::GetProcessorForHeap(uint16_t heap_number, uint16_t* proc_no, uint16_t* node_no)
-{
-    bool success = false;
-    uint16_t availableProcNumber = 0;
-    for (size_t procNumber = 0; procNumber < MAX_SUPPORTED_CPUS; procNumber++)
-    {
-        if (g_processAffinitySet.Contains(procNumber))
-        {
-            if (availableProcNumber == heap_number)
-            {
-                *proc_no = procNumber;
-#ifdef TARGET_LINUX
-                if (GCToOSInterface::CanEnableGCNumaAware())
-                {
-                    int result = GetNumaNodeNumByCpu(procNumber);
-                    *node_no = (result >= 0) ? (uint16_t)result : NUMA_NODE_UNDEFINED;
-                }
-                else
-#endif // TARGET_LINUX
-                {
-                    *node_no = NUMA_NODE_UNDEFINED;
-                }
-                success = true;
-                break;
-            }
-            availableProcNumber++;
-        }
-    }
-    return success;
-}
-bool GCToOSInterface::ParseGCHeapAffinitizeRangesEntry(const char** config_string, size_t* start_index, size_t* end_index)
-{
-    return ParseIndexOrRange(config_string, start_index, end_index);
-}
-bool CLRCriticalSection::Initialize()
-{
-    pthread_mutexattr_t mutexAttributes;
-    int st = pthread_mutexattr_init(&mutexAttributes);
-    if (st != 0)
-    {
-        return false;
-    }
-    st = pthread_mutexattr_settype(&mutexAttributes, PTHREAD_MUTEX_RECURSIVE);
-    if (st == 0)
-    {
-        st = pthread_mutex_init(&m_cs.mutex, &mutexAttributes);
-    }
-    pthread_mutexattr_destroy(&mutexAttributes);
-    return (st == 0);
-}
-void CLRCriticalSection::Destroy()
-{
-    int st = pthread_mutex_destroy(&m_cs.mutex);
-    assert(st == 0);
-}
-void CLRCriticalSection::Enter()
-{
-    pthread_mutex_lock(&m_cs.mutex);
-}
-void CLRCriticalSection::Leave()
-{
-    pthread_mutex_unlock(&m_cs.mutex);
-}

--- a/src/coreclr/utilcode/collections.cpp
+++ b//dev/null
@@ -1,623 +0,0 @@
-#include "stdafx.h"
-#include "utilcode.h"
-#include "ex.h"
-#ifndef DACCESS_COMPILE
-HRESULT CHashTable::NewInit(            // Return status.
-    BYTE        *pcEntries,             // Array of structs we are managing.
-    ULONG      iEntrySize)             // Size of the entries.
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    _ASSERTE(iEntrySize >= sizeof(FREEHASHENTRY));
-    if ((m_piBuckets = new (nothrow) ULONG [m_iBuckets]) == NULL)
-        return (OutOfMemory());
-    memset(m_piBuckets, 0xff, m_iBuckets * sizeof(ULONG));
-    m_pcEntries = (TADDR)pcEntries;
-    m_iEntrySize = iEntrySize;
-    return (S_OK);
-}
-BYTE *CHashTable::Add(                  // New entry.
-    ULONG      iHash,                  // Hash value of entry to add.
-    ULONG      iIndex)                 // Index of struct in m_pcEntries.
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    HASHENTRY   *psEntry;               // The struct we are adding.
-    psEntry = EntryPtr(iIndex);
-    iHash %= m_iBuckets;
-    _ASSERTE(m_piBuckets[iHash] != iIndex &&
-        (m_piBuckets[iHash] == UINT32_MAX || EntryPtr(m_piBuckets[iHash])->iPrev != iIndex));
-    psEntry->iPrev = UINT32_MAX;
-    psEntry->iNext = m_piBuckets[iHash];
-    if (m_piBuckets[iHash] != UINT32_MAX)
-        EntryPtr(m_piBuckets[iHash])->iPrev = iIndex;
-    m_piBuckets[iHash] = iIndex;
-    return ((BYTE *) psEntry);
-}
-void CHashTable::Delete(
-    ULONG      iHash,                  // Hash value of entry to delete.
-    ULONG      iIndex)                 // Index of struct in m_pcEntries.
-{
-    WRAPPER_NO_CONTRACT;
-    HASHENTRY   *psEntry;               // Struct to delete.
-    psEntry = EntryPtr(iIndex);
-    Delete(iHash, psEntry);
-}
-void CHashTable::Delete(
-    ULONG      iHash,                  // Hash value of entry to delete.
-    HASHENTRY   *psEntry)               // The struct to delete.
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    iHash %= m_iBuckets;
-    _ASSERTE(psEntry->iPrev != psEntry->iNext || psEntry->iPrev == UINT32_MAX);
-    if (psEntry->iPrev == UINT32_MAX)
-        m_piBuckets[iHash] = psEntry->iNext;
-    else
-        EntryPtr(psEntry->iPrev)->iNext = psEntry->iNext;
-    if (psEntry->iNext != UINT32_MAX)
-        EntryPtr(psEntry->iNext)->iPrev = psEntry->iPrev;
-}
-void CHashTable::Move(
-    ULONG      iHash,                  // Hash value for the item.
-    ULONG      iNew)                   // New location.
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    HASHENTRY   *psEntry;               // The struct we are deleting.
-    psEntry = EntryPtr(iNew);
-    _ASSERTE(psEntry->iPrev != iNew && psEntry->iNext != iNew);
-    if (psEntry->iPrev != UINT32_MAX)
-        EntryPtr(psEntry->iPrev)->iNext = iNew;
-    else
-        m_piBuckets[iHash % m_iBuckets] = iNew;
-    if (psEntry->iNext != UINT32_MAX)
-        EntryPtr(psEntry->iNext)->iPrev = iNew;
-}
-#endif // !DACCESS_COMPILE
-BYTE *CHashTable::Find(                 // Index of struct in m_pcEntries.
-    ULONG      iHash,                  // Hash value of the item.
-    SIZE_T     key)                    // The key to match.
-{
-    CONTRACTL
-    {
-        NOTHROW;
-        GC_NOTRIGGER;
-        SUPPORTS_DAC;
-    }
-    CONTRACTL_END;
-    ULONG      iNext;                  // Used to traverse the chains.
-    HASHENTRY   *psEntry;               // Used to traverse the chains.
-    iNext = m_piBuckets[iHash % m_iBuckets];
-#ifdef _DEBUG
-    unsigned count = 0;
-#endif
-    while (iNext != UINT32_MAX)
-    {
-        psEntry = EntryPtr(iNext);
-#ifdef _DEBUG
-        count++;
-#endif
-        if (!Cmp(key, psEntry))
-        {
-#ifdef _DEBUG
-            if (count > m_maxSearch)
-                m_maxSearch = count;
-#endif
-            return ((BYTE *) psEntry);
-        }
-        iNext = psEntry->iNext;
-    }
-    return (0);
-}
-ULONG CHashTable::FindNext(            // Index of struct in m_pcEntries.
-    SIZE_T     key,                    // The key to match.
-    ULONG      iIndex)                 // Index of previous match.
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    ULONG      iNext;                  // Used to traverse the chains.
-    HASHENTRY   *psEntry;               // Used to traverse the chains.
-    iNext = EntryPtr(iIndex)->iNext;
-    while (iNext != UINT32_MAX)
-    {
-        psEntry = EntryPtr(iNext);
-        if (!Cmp(key, psEntry))
-            return (iNext);
-        iNext = psEntry->iNext;
-    }
-    return (UINT32_MAX);
-}
-BYTE *CHashTable::FindNextEntry(        // The next entry, or0 for end of list.
-    HASHFIND    *psSrch)                // Search object.
-{
-    CONTRACTL
-    {
-        NOTHROW;
-        GC_NOTRIGGER;
-        SUPPORTS_DAC;
-    }
-    CONTRACTL_END;
-    HASHENTRY   *psEntry;               // Used to traverse the chains.
-    for (;;)
-    {
-        if (psSrch->iNext != UINT32_MAX)
-        {
-            psEntry = EntryPtr(psSrch->iNext);
-#if DACCESS_COMPILE
-            if (psEntry->iNext == psSrch->iNext)
-                return NULL;
-#endif
-            psSrch->iNext = psEntry->iNext;
-            return ((BYTE *) psEntry);
-        }
-        if (psSrch->iBucket < m_iBuckets)
-            psSrch->iNext = m_piBuckets[psSrch->iBucket++];
-        else
-            break;
-    }
-    return (0);
-}
-#ifdef DACCESS_COMPILE
-void
-CHashTable::EnumMemoryRegions(CLRDataEnumMemoryFlags flags,
-                              ULONG numEntries)
-{
-    SUPPORTS_DAC;
-    DacEnumMemoryRegion(m_pcEntries,
-                        (ULONG)numEntries * m_iEntrySize);
-    DacEnumMemoryRegion(dac_cast<TADDR>(m_piBuckets),
-                        (ULONG)m_iBuckets * sizeof(ULONG));
-}
-#endif // #ifdef DACCESS_COMPILE
-void CClosedHashBase::Delete(
-    void        *pData)                 // Key value to delete.
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    BYTE        *ptr;
-    if ((ptr = Find(pData)) == 0)
-    {
-        _ASSERTE(0);
-        return;
-    }
-    if (m_bPerfect)
-    {
-        SetStatus(ptr, FREE);
-        --m_iCount;
-        return;
-    }
-    SetStatus(ptr, DELETED);
-    BYTE        *pnext;
-    if ((pnext = ptr + m_iEntrySize) > EntryPtr(m_iSize - 1))
-        pnext = &m_rgData[0];
-    if (Status(pnext) != FREE)
-        return;
-    while (Status(ptr) == DELETED)
-    {
-        SetStatus(ptr, FREE);
-        --m_iCount;
-        if ((ptr -= m_iEntrySize) < &m_rgData[0])
-            ptr = EntryPtr(m_iSize - 1);
-    }
-}
-void CClosedHashBase::DeleteLoop(
-    DELETELOOPFUNC pDeleteLoopFunc,     // Decides whether to delete item
-    void *pCustomizer)                  // Extra value passed to deletefunc.
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    int i;
-    if (m_rgData == 0)
-    {
-        return;
-    }
-    for (i = 0; i < m_iSize; i++)
-    {
-        BYTE *pEntry = EntryPtr(i);
-        if (Status(pEntry) == USED)
-        {
-            if (pDeleteLoopFunc(pEntry, pCustomizer))
-            {
-                if (m_bPerfect)
-                {
-                    SetStatus(pEntry, FREE);
-                    --m_iCount;
-                }
-                else
-                {
-                    SetStatus(pEntry, DELETED);
-                }
-            }
-        }
-    }
-    if (!m_bPerfect)
-    {
-        for (i = 0; i < m_iSize; i++)
-        {
-            if (Status(EntryPtr(i)) == FREE)
-            {
-                break;
-            }
-        }
-        if (i != m_iSize)
-        {
-            int iFirstFree = i;
-            do
-            {
-                if (i-- == 0)
-                {
-                    i = m_iSize - 1;
-                }
-                while (Status(EntryPtr(i)) == DELETED)
-                {
-                    SetStatus(EntryPtr(i), FREE);
-                    --m_iCount;
-                    if (i-- == 0)
-                    {
-                        i = m_iSize - 1;
-                    }
-                }
-                while (Status(EntryPtr(i)) != FREE)
-                {
-                    if (i-- == 0)
-                    {
-                        i = m_iSize - 1;
-                    }
-                }
-            }
-            while (i != iFirstFree);
-        }
-    }
-}
-BYTE *CClosedHashBase::Find(            // The item if found, 0 if not.
-    void        *pData)                 // The key to lookup.
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    unsigned int iHash;                // Hash value for this data.
-    int         iBucket;                // Which bucke to start at.
-    int         i;                      // Loop control.
-    if (!m_rgData || m_iCount == 0)
-        return (0);
-    iHash = Hash(pData);
-    iBucket = iHash % m_iBuckets;
-    if (m_bPerfect)
-    {
-        if (Status(EntryPtr(iBucket)) != FREE)
-            return (EntryPtr(iBucket));
-        return (0);
-    }
-    for (i=iBucket;  Status(EntryPtr(i)) != FREE;  )
-    {
-        if (Status(EntryPtr(i)) == DELETED)
-        {
-            if (++i >= m_iSize)
-                i = 0;
-            continue;
-        }
-        if (Compare(pData, EntryPtr(i)) == 0)
-            return (EntryPtr(i));
-        if (!m_iCollisions)
-            return (0);
-        if (++i >= m_iSize)
-            i = 0;
-    }
-    return (0);
-}
-BYTE *CClosedHashBase::FindOrAdd(       // The item if found, 0 if not.
-    void        *pData,                 // The key to lookup.
-    bool        &bNew)                  // true if created.
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    unsigned int iHash;                // Hash value for this data.
-    int         iBucket;                // Which bucke to start at.
-    int         i;                      // Loop control.
-    if (!m_rgData || ((m_iCount + 1) > (m_iSize * 3 / 4) && !m_bPerfect))
-    {
-        if (!ReHash())
-            return (0);
-    }
-    bNew = false;
-    iHash = Hash(pData);
-    iBucket = iHash % m_iBuckets;
-    if (m_bPerfect)
-    {
-        if (Status(EntryPtr(iBucket)) != FREE)
-            return (EntryPtr(iBucket));
-        i = iBucket;
-    }
-    else
-    {
-        for (i=iBucket;  Status(EntryPtr(i)) != FREE;  )
-        {
-            if (Status(EntryPtr(i)) == DELETED)
-            {
-                if (++i >= m_iSize)
-                    i = 0;
-                continue;
-            }
-            if (Compare(pData, EntryPtr(i)) == 0)
-                return (EntryPtr(i));
-            ++m_iCollisions;
-            if (++i >= m_iSize)
-                i = 0;
-        }
-    }
-    _ASSERTE(Status(EntryPtr(i)) == FREE);
-    bNew = true;
-    ++m_iCount;
-    return (EntryPtr(i));
-}
-BYTE *CClosedHashBase::DoAdd(void *pData, BYTE *rgData, int &iBuckets, int iSize,
-            int &iCollisions, int &iCount)
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    unsigned int iHash;                // Hash value for this data.
-    int         iBucket;                // Which bucke to start at.
-    int         i;                      // Loop control.
-    iHash = Hash(pData);
-    iBucket = iHash % iBuckets;
-    if (m_bPerfect)
-    {
-        i = iBucket;
-        _ASSERTE(Status(EntryPtr(i, rgData)) == FREE);
-    }
-    else
-    {
-        for (i=iBucket;  Status(EntryPtr(i, rgData)) != FREE;  )
-        {
-            if (++i >= iSize)
-                i = 0;
-            ++iCollisions;
-        }
-    }
-    ++iCount;
-    return (EntryPtr(i, rgData));
-}
-bool CClosedHashBase::ReHash()          // true if successful.
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    if (!m_rgData)
-    {
-        if ((m_rgData = new (nothrow) BYTE [m_iSize * m_iEntrySize]) == 0)
-            return (false);
-        InitFree(&m_rgData[0], m_iSize);
-        return (true);
-    }
-    BYTE        *rgTemp, *p;
-    int         iBuckets = m_iBuckets * 2 - 1;
-    int         iSize = iBuckets + 7;
-    int         iCollisions = 0;
-    int         iCount = 0;
-    if ((rgTemp = new (nothrow) BYTE [iSize * m_iEntrySize]) == 0)
-        return (false);
-    InitFree(&rgTemp[0], iSize);
-    m_bPerfect = false;
-    for (int i=0;  i<m_iSize;  i++)
-    {
-        if (Status(EntryPtr(i)) != USED)
-            continue;
-        VERIFY((p = DoAdd(GetKey(EntryPtr(i)), rgTemp, iBuckets,
-                iSize, iCollisions, iCount)));
-        memmove(p, EntryPtr(i), m_iEntrySize);
-    }
-    delete [] m_rgData;
-    m_rgData = rgTemp;
-    m_iBuckets = iBuckets;
-    m_iSize = iSize;
-    m_iCollisions = iCollisions;
-    m_iCount = iCount;
-    return (true);
-}
-void *CStructArray::InsertThrowing(
-    int         iIndex)
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    _ASSERTE(iIndex >= 0);
-    if (iIndex > m_iCount)
-        return (NULL);
-    Grow(1);
-    BYTE *pcList = m_pList + iIndex * m_iElemSize;
-    if (iIndex < m_iCount)
-        memmove(pcList + m_iElemSize, pcList, (m_iCount - iIndex) * m_iElemSize);
-    ++m_iCount;
-    return(pcList);
-}
-void *CStructArray::Insert(int iIndex)
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    void *result = NULL;
-    EX_TRY
-    {
-        result = InsertThrowing(iIndex);
-    }
-    EX_CATCH
-    {
-    }
-    EX_END_CATCH(SwallowAllExceptions);
-    return result;
-}
-void *CStructArray::AppendThrowing()
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    Grow(1);
-    return (m_pList + m_iCount++ * m_iElemSize);
-}
-void *CStructArray::Append()
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    void *result = NULL;
-    EX_TRY
-    {
-        result = AppendThrowing();
-    }
-    EX_CATCH
-    {
-    }
-    EX_END_CATCH(SwallowAllExceptions);
-    return result;
-}
-void CStructArray::AllocateBlockThrowing(int iCount)
-{
-    CONTRACTL
-    {
-        THROWS;
-    }
-    CONTRACTL_END;
-    if (m_iSize < m_iCount+iCount)
-        Grow(iCount);
-    m_iCount += iCount;
-}
-int CStructArray::AllocateBlock(int iCount)
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    int result = FALSE;
-    EX_TRY
-    {
-        AllocateBlockThrowing(iCount);
-        result = TRUE;
-    }
-    EX_CATCH
-    {
-    }
-    EX_END_CATCH(SwallowAllExceptions);
-    return result;
-}
-void CStructArray::Delete(
-    int         iIndex)
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    _ASSERTE(iIndex >= 0);
-    if (iIndex < --m_iCount)
-    {
-        BYTE *pcList = m_pList + iIndex * m_iElemSize;
-        memmove(pcList, pcList + m_iElemSize, (m_iCount - iIndex) * m_iElemSize);
-    }
-}
-void CStructArray::Grow(
-    int         iCount)
-{
-    CONTRACTL {
-        THROWS;
-    } CONTRACTL_END;
-    BYTE        *pTemp;                 // temporary pointer used in realloc.
-    int         iGrow;
-    if (m_iSize < m_iCount+iCount)
-    {
-        if (m_pList == NULL)
-        {
-            iGrow = max(m_iGrowInc, iCount);
-            S_SIZE_T newSize = S_SIZE_T(iGrow) * S_SIZE_T(m_iElemSize);
-            if(newSize.IsOverflow())
-                ThrowOutOfMemory();
-            else
-            {
-                m_pList = new BYTE[newSize.Value()];
-                m_iSize = iGrow;
-                m_bFree = true;
-            }
-        }
-        else
-        {
-            if (m_iSize / m_iGrowInc >= 3)
-            {   // Don't overflow and go negative.
-                int newinc = m_iGrowInc * 2;
-                if (newinc > m_iGrowInc)
-                    m_iGrowInc = newinc;
-            }
-            iGrow = max(m_iGrowInc, iCount);
-            S_SIZE_T allocSize = (S_SIZE_T(m_iSize) + S_SIZE_T(iGrow)) * S_SIZE_T(m_iElemSize);
-            S_SIZE_T copyBytes = S_SIZE_T(m_iSize) * S_SIZE_T(m_iElemSize);
-            if(allocSize.IsOverflow() || copyBytes.IsOverflow())
-                ThrowOutOfMemory();
-            if (m_bFree)
-            {   // We already own memory.
-                pTemp = new BYTE[allocSize.Value()];
-                memcpy (pTemp, m_pList, copyBytes.Value());
-                delete [] m_pList;
-            }
-            else
-            {   // We don't own memory; get our own.
-                pTemp = new BYTE[allocSize.Value()];
-                memcpy(pTemp, m_pList, copyBytes.Value());
-                m_bFree = true;
-            }
-            m_pList = pTemp;
-            m_iSize += iGrow;
-        }
-    }
-}
-void CStructArray::Clear()
-{
-    CONTRACTL
-    {
-        NOTHROW;
-    }
-    CONTRACTL_END;
-    if (m_bFree && m_pList != NULL)
-        delete [] m_pList;
-    m_pList = NULL;
-    m_iSize = 0;
-    m_iCount = 0;
-}

--- a/src/libraries/Common/src/Microsoft/Win32/SafeHandles/SafeCertContextHandleWithKeyContainerDeletion.cs
+++ b//dev/null
@@ -1,65 +0,0 @@
-using System;
-using System.Diagnostics;
-using System.Runtime.InteropServices;
-using System.Security.Cryptography;
-namespace Microsoft.Win32.SafeHandles
-{
-    internal sealed class SafeCertContextHandleWithKeyContainerDeletion : SafeCertContextHandle
-    {
-        protected sealed override bool ReleaseHandle()
-        {
-            using (SafeCertContextHandle certContext = Interop.Crypt32.CertDuplicateCertificateContext(handle))
-            {
-                DeleteKeyContainer(certContext);
-            }
-            base.ReleaseHandle();
-            return true;
-        }
-        internal static void DeleteKeyContainer(SafeCertContextHandle pCertContext)
-        {
-            if (pCertContext.IsInvalid)
-                return;
-            int cb = 0;
-            bool containsPrivateKey = Interop.Crypt32.CertGetCertificateContextProperty(pCertContext, Interop.Crypt32.CertContextPropId.CERT_KEY_PROV_INFO_PROP_ID, null, ref cb);
-            if (!containsPrivateKey)
-                return;
-            byte[] provInfoAsBytes = new byte[cb];
-            if (!Interop.Crypt32.CertGetCertificateContextProperty(pCertContext, Interop.Crypt32.CertContextPropId.CERT_KEY_PROV_INFO_PROP_ID, provInfoAsBytes, ref cb))
-                return;
-            unsafe
-            {
-                fixed (byte* pProvInfoAsBytes = provInfoAsBytes)
-                {
-                    Interop.Crypt32.CRYPT_KEY_PROV_INFO* pProvInfo = (Interop.Crypt32.CRYPT_KEY_PROV_INFO*)pProvInfoAsBytes;
-                    if (pProvInfo->dwProvType == 0)
-                    {
-                        string providerName = Marshal.PtrToStringUni((IntPtr)(pProvInfo->pwszProvName))!;
-                        string keyContainerName = Marshal.PtrToStringUni((IntPtr)(pProvInfo->pwszContainerName))!;
-                        CngKeyOpenOptions openOpts = CngKeyOpenOptions.None;
-                        if ((pProvInfo->dwFlags & Interop.Crypt32.CryptAcquireContextFlags.CRYPT_MACHINE_KEYSET) != 0)
-                        {
-                            openOpts = CngKeyOpenOptions.MachineKey;
-                        }
-                        try
-                        {
-                            using (CngKey cngKey = CngKey.Open(keyContainerName, new CngProvider(providerName), openOpts))
-                            {
-                                cngKey.Delete();
-                            }
-                        }
-                        catch (CryptographicException)
-                        {
-                        }
-                    }
-                    else
-                    {
-                        Interop.Crypt32.CryptAcquireContextFlags flags = (pProvInfo->dwFlags & Interop.Crypt32.CryptAcquireContextFlags.CRYPT_MACHINE_KEYSET) | Interop.Crypt32.CryptAcquireContextFlags.CRYPT_DELETEKEYSET;
-                        IntPtr hProv;
-                        _ = Interop.Advapi32.CryptAcquireContext(out hProv, pProvInfo->pwszContainerName, pProvInfo->pwszProvName, pProvInfo->dwProvType, flags);
-                        Debug.Assert(hProv == IntPtr.Zero);
-                    }
-                }
-            }
-        }
-    }
-}

--- a/src/libraries/System.Net.Mail/src/System/Net/Mail/SmtpNegotiateAuthenticationModule.cs
+++ b//dev/null
@@ -1,104 +0,0 @@
-using System.Buffers;
-using System.Collections.Generic;
-using System.ComponentModel;
-using System.Net.Security;
-using System.Security.Authentication.ExtendedProtection;
-namespace System.Net.Mail
-{
-    internal sealed class SmtpNegotiateAuthenticationModule : ISmtpAuthenticationModule
-    {
-        private static readonly byte[] s_saslNoSecurtyLayerToken = new byte[] { 1, 0, 0, 0 };
-        private readonly Dictionary<object, NegotiateAuthentication> _sessions = new Dictionary<object, NegotiateAuthentication>();
-        internal SmtpNegotiateAuthenticationModule()
-        {
-        }
-        public Authorization? Authenticate(string? challenge, NetworkCredential? credential, object sessionCookie, string? spn, ChannelBinding? channelBindingToken)
-        {
-            lock (_sessions)
-            {
-                NegotiateAuthentication? clientContext;
-                if (!_sessions.TryGetValue(sessionCookie, out clientContext))
-                {
-                    if (credential == null)
-                    {
-                        return null;
-                    }
-                    ProtectionLevel protectionLevel = ProtectionLevel.Sign;
-                    if (OperatingSystem.IsLinux())
-                    {
-                        protectionLevel = ProtectionLevel.EncryptAndSign;
-                    }
-                    _sessions[sessionCookie] = clientContext =
-                        new NegotiateAuthentication(
-                            new NegotiateAuthenticationClientOptions
-                            {
-                                Credential = credential,
-                                TargetName = spn,
-                                RequiredProtectionLevel = protectionLevel,
-                                Binding = channelBindingToken
-                            });
-                }
-                string? resp = null;
-                NegotiateAuthenticationStatusCode statusCode;
-                if (!clientContext.IsAuthenticated)
-                {
-                    resp = clientContext.GetOutgoingBlob(challenge, out statusCode);
-                    if (statusCode != NegotiateAuthenticationStatusCode.Completed &&
-                        statusCode != NegotiateAuthenticationStatusCode.ContinueNeeded)
-                    {
-                        return null;
-                    }
-                }
-                else
-                {
-                    resp = GetSecurityLayerOutgoingBlob(challenge, clientContext);
-                }
-                return new Authorization(resp, clientContext.IsAuthenticated);
-            }
-        }
-        public string AuthenticationType
-        {
-            get
-            {
-                return "gssapi";
-            }
-        }
-        public void CloseContext(object sessionCookie)
-        {
-            NegotiateAuthentication? clientContext = null;
-            lock (_sessions)
-            {
-                if (_sessions.TryGetValue(sessionCookie, out clientContext))
-                {
-                    _sessions.Remove(sessionCookie);
-                }
-            }
-            clientContext?.Dispose();
-        }
-        private static string? GetSecurityLayerOutgoingBlob(string? challenge, NegotiateAuthentication clientContext)
-        {
-            if (challenge == null)
-                return null;
-            byte[] input = Convert.FromBase64String(challenge);
-            Span<byte> unwrappedChallenge;
-            NegotiateAuthenticationStatusCode statusCode;
-            statusCode = clientContext.UnwrapInPlace(input, out int newOffset, out int newLength, out _);
-            if (statusCode != NegotiateAuthenticationStatusCode.Completed)
-            {
-                return null;
-            }
-            unwrappedChallenge = input.AsSpan(newOffset, newLength);
-            if (unwrappedChallenge.Length != 4 || (unwrappedChallenge[0] & 1) != 1)
-            {
-                return null;
-            }
-            ArrayBufferWriter<byte> outputWriter = new ArrayBufferWriter<byte>();
-            statusCode = clientContext.Wrap(s_saslNoSecurtyLayerToken, outputWriter, false, out _);
-            if (statusCode != NegotiateAuthenticationStatusCode.Completed)
-            {
-                return null;
-            }
-            return Convert.ToBase64String(outputWriter.WrittenSpan);
-        }
-    }
-}

--- a/src/libraries/System.Net.Quic/src/System/Net/Quic/Internal/MsQuicApi.cs
+++ b//dev/null
@@ -1,213 +0,0 @@
-using System.Diagnostics;
-using System.Diagnostics.CodeAnalysis;
-using System.Net.Sockets;
-using System.Runtime.InteropServices;
-using Microsoft.Quic;
-using static Microsoft.Quic.MsQuic;
-#if TARGET_WINDOWS
-using Microsoft.Win32;
-#endif
-namespace System.Net.Quic;
-internal sealed unsafe partial class MsQuicApi
-{
-    private static readonly Version s_minWindowsVersion = new Version(10, 0, 20145, 1000);
-    private static readonly Version s_minMsQuicVersion = new Version(2, 2, 2);
-    private static readonly delegate* unmanaged[Cdecl]<uint, QUIC_API_TABLE**, int> MsQuicOpenVersion;
-    private static readonly delegate* unmanaged[Cdecl]<QUIC_API_TABLE*, void> MsQuicClose;
-    public MsQuicSafeHandle Registration { get; }
-    public QUIC_API_TABLE* ApiTable { get; }
-    [DynamicDependency(DynamicallyAccessedMemberTypes.PublicConstructors, typeof(MsQuicSafeHandle))]
-    [DynamicDependency(DynamicallyAccessedMemberTypes.PublicConstructors, typeof(MsQuicContextSafeHandle))]
-    private MsQuicApi(QUIC_API_TABLE* apiTable)
-    {
-        ApiTable = apiTable;
-        fixed (byte* pAppName = "System.Net.Quic"u8)
-        {
-            var cfg = new QUIC_REGISTRATION_CONFIG
-            {
-                AppName = (sbyte*)pAppName,
-                ExecutionProfile = QUIC_EXECUTION_PROFILE.LOW_LATENCY
-            };
-            QUIC_HANDLE* handle;
-            ThrowHelper.ThrowIfMsQuicError(ApiTable->RegistrationOpen(&cfg, &handle), "RegistrationOpen failed");
-            Registration = new MsQuicSafeHandle(handle, apiTable->RegistrationClose, SafeHandleType.Registration);
-        }
-    }
-    private static readonly Lazy<MsQuicApi> _api = new Lazy<MsQuicApi>(AllocateMsQuicApi);
-    internal static MsQuicApi Api => _api.Value;
-    internal static bool IsQuicSupported { get; }
-    internal static string MsQuicLibraryVersion { get; } = "unknown";
-    internal static string? NotSupportedReason { get; }
-    internal static bool UsesSChannelBackend { get; }
-    internal static bool Tls13ServerMayBeDisabled { get; }
-    internal static bool Tls13ClientMayBeDisabled { get; }
-#pragma warning disable CA1810 // Initialize all static fields in 'MsQuicApi' when those fields are declared and remove the explicit static constructor
-    [UnconditionalSuppressMessage("SingleFile", "IL3000: Avoid accessing Assembly file path when publishing as a single file",
-        Justification = "The code handles the Assembly.Location being null/empty by falling back to AppContext.BaseDirectory")]
-    static MsQuicApi()
-    {
-        bool loaded = false;
-        IntPtr msQuicHandle;
-        if (!Socket.OSSupportsIPv6)
-        {
-            NotSupportedReason = "OS does not support dual mode sockets.";
-            if (NetEventSource.Log.IsEnabled())
-            {
-                NetEventSource.Info(null, NotSupportedReason);
-            }
-            return;
-        }
-        if (OperatingSystem.IsWindows())
-        {
-#pragma warning disable IL3000 // Avoid accessing Assembly file path when publishing as a single file
-            string path = typeof(MsQuicApi).Assembly.Location is string assemblyLocation && !string.IsNullOrEmpty(assemblyLocation)
-                ? System.IO.Path.GetDirectoryName(assemblyLocation)!
-                : AppContext.BaseDirectory;
-#pragma warning restore IL3000
-            path = System.IO.Path.Combine(path, Interop.Libraries.MsQuic);
-            if (NetEventSource.Log.IsEnabled())
-            {
-                NetEventSource.Info(null, $"Attempting to load MsQuic from {path}");
-            }
-            loaded = NativeLibrary.TryLoad(path, typeof(MsQuicApi).Assembly, DllImportSearchPath.LegacyBehavior, out msQuicHandle);
-        }
-        else
-        {
-            loaded = NativeLibrary.TryLoad($"{Interop.Libraries.MsQuic}.{s_minMsQuicVersion.Major}", typeof(MsQuicApi).Assembly, null, out msQuicHandle) ||
-                     NativeLibrary.TryLoad(Interop.Libraries.MsQuic, typeof(MsQuicApi).Assembly, null, out msQuicHandle);
-        }
-        if (!loaded)
-        {
-            NotSupportedReason = $"Unable to load MsQuic library version '{s_minMsQuicVersion.Major}'.";
-            if (NetEventSource.Log.IsEnabled())
-            {
-                NetEventSource.Info(null, NotSupportedReason);
-            }
-            return;
-        }
-        MsQuicOpenVersion = (delegate* unmanaged[Cdecl]<uint, QUIC_API_TABLE**, int>)NativeLibrary.GetExport(msQuicHandle, nameof(MsQuicOpenVersion));
-        MsQuicClose = (delegate* unmanaged[Cdecl]<QUIC_API_TABLE*, void>)NativeLibrary.GetExport(msQuicHandle, nameof(MsQuicClose));
-        if (!TryOpenMsQuic(out QUIC_API_TABLE* apiTable, out int openStatus))
-        {
-            NotSupportedReason = $"MsQuicOpenVersion for version {s_minMsQuicVersion.Major} returned {openStatus} status code.";
-            if (NetEventSource.Log.IsEnabled())
-            {
-                NetEventSource.Info(null, NotSupportedReason);
-            }
-            return;
-        }
-        try
-        {
-            uint paramSize;
-            int status;
-            paramSize = 4 * sizeof(uint);
-            uint* libVersion = stackalloc uint[4];
-            status = apiTable->GetParam(null, QUIC_PARAM_GLOBAL_LIBRARY_VERSION, &paramSize, libVersion);
-            if (StatusFailed(status))
-            {
-                if (NetEventSource.Log.IsEnabled())
-                {
-                    NetEventSource.Error(null, $"Cannot retrieve {nameof(QUIC_PARAM_GLOBAL_LIBRARY_VERSION)} from MsQuic library: '{status}'.");
-                }
-                return;
-            }
-            Version version = new Version((int)libVersion[0], (int)libVersion[1], (int)libVersion[2], (int)libVersion[3]);
-            paramSize = 64 * sizeof(sbyte);
-            sbyte* libGitHash = stackalloc sbyte[64];
-            status = apiTable->GetParam(null, QUIC_PARAM_GLOBAL_LIBRARY_GIT_HASH, &paramSize, libGitHash);
-            if (StatusFailed(status))
-            {
-                if (NetEventSource.Log.IsEnabled())
-                {
-                    NetEventSource.Error(null, $"Cannot retrieve {nameof(QUIC_PARAM_GLOBAL_LIBRARY_GIT_HASH)} from MsQuic library: '{status}'.");
-                }
-                return;
-            }
-            string? gitHash = Marshal.PtrToStringUTF8((IntPtr)libGitHash);
-            MsQuicLibraryVersion = $"{Interop.Libraries.MsQuic} {version} ({gitHash})";
-            if (version < s_minMsQuicVersion)
-            {
-                NotSupportedReason = $"Incompatible MsQuic library version '{version}', expecting higher than '{s_minMsQuicVersion}'.";
-                if (NetEventSource.Log.IsEnabled())
-                {
-                    NetEventSource.Info(null, NotSupportedReason);
-                }
-                return;
-            }
-            if (NetEventSource.Log.IsEnabled())
-            {
-                NetEventSource.Info(null, $"Loaded MsQuic library '{MsQuicLibraryVersion}'.");
-            }
-            QUIC_TLS_PROVIDER provider = OperatingSystem.IsWindows() ? QUIC_TLS_PROVIDER.SCHANNEL : QUIC_TLS_PROVIDER.OPENSSL;
-            paramSize = sizeof(QUIC_TLS_PROVIDER);
-            apiTable->GetParam(null, QUIC_PARAM_GLOBAL_TLS_PROVIDER, &paramSize, &provider);
-            UsesSChannelBackend = provider == QUIC_TLS_PROVIDER.SCHANNEL;
-            if (UsesSChannelBackend)
-            {
-                if (!IsWindowsVersionSupported())
-                {
-                    NotSupportedReason = $"Current Windows version ({Environment.OSVersion}) is not supported by QUIC. Minimal supported version is {s_minWindowsVersion}.";
-                    if (NetEventSource.Log.IsEnabled())
-                    {
-                        NetEventSource.Info(null, NotSupportedReason);
-                    }
-                    return;
-                }
-                Tls13ServerMayBeDisabled = IsTls13Disabled(isServer: true);
-                Tls13ClientMayBeDisabled = IsTls13Disabled(isServer: false);
-            }
-            IsQuicSupported = true;
-        }
-        finally
-        {
-            MsQuicClose(apiTable);
-        }
-    }
-#pragma warning restore CA1810
-    private static MsQuicApi AllocateMsQuicApi()
-    {
-        Debug.Assert(IsQuicSupported);
-        if (!TryOpenMsQuic(out QUIC_API_TABLE* apiTable, out int openStatus))
-        {
-            throw ThrowHelper.GetExceptionForMsQuicStatus(openStatus);
-        }
-        return new MsQuicApi(apiTable);
-    }
-    private static bool TryOpenMsQuic(out QUIC_API_TABLE* apiTable, out int openStatus)
-    {
-        Debug.Assert(MsQuicOpenVersion != null);
-        QUIC_API_TABLE* table = null;
-        openStatus = MsQuicOpenVersion((uint)s_minMsQuicVersion.Major, &table);
-        if (StatusFailed(openStatus))
-        {
-            apiTable = null;
-            return false;
-        }
-        apiTable = table;
-        return true;
-    }
-    private static bool IsWindowsVersionSupported() => OperatingSystem.IsWindowsVersionAtLeast(s_minWindowsVersion.Major,
-        s_minWindowsVersion.Minor, s_minWindowsVersion.Build, s_minWindowsVersion.Revision);
-    private static bool IsTls13Disabled(bool isServer)
-    {
-#if TARGET_WINDOWS
-        string SChannelTls13RegistryKey = isServer
-            ? @"SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.3\Server"
-            : @"SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.3\Client";
-        using var regKey = Registry.LocalMachine.OpenSubKey(SChannelTls13RegistryKey);
-        if (regKey is null)
-        {
-            return false;
-        }
-        if (regKey.GetValue("Enabled") is int enabled && enabled == 0)
-        {
-            return true;
-        }
-        if (regKey.GetValue("DisabledByDefault") is int disabled && disabled == 1)
-        {
-            return true;
-        }
-#endif
-        return false;
-    }
-}

--- a/src/libraries/System.Net.Security/src/System/Net/Security/SslStreamPal.Windows.cs
+++ b//dev/null
@@ -1,570 +0,0 @@
-using System.Collections.Generic;
-using System.ComponentModel;
-using System.Diagnostics;
-using System.Runtime.InteropServices;
-using System.Security.Authentication;
-using System.Security.Authentication.ExtendedProtection;
-using System.Security.Cryptography.X509Certificates;
-using System.Security.Principal;
-using Microsoft.Win32.SafeHandles;
-namespace System.Net.Security
-{
-    internal static class SslStreamPal
-    {
-        private static readonly byte[] s_http1 = Interop.Sec_Application_Protocols.ToByteArray(new List<SslApplicationProtocol> { SslApplicationProtocol.Http11 });
-        private static readonly byte[] s_http2 = Interop.Sec_Application_Protocols.ToByteArray(new List<SslApplicationProtocol> { SslApplicationProtocol.Http2 });
-        private static readonly byte[] s_http12 = Interop.Sec_Application_Protocols.ToByteArray(new List<SslApplicationProtocol> { SslApplicationProtocol.Http11, SslApplicationProtocol.Http2 });
-        private static readonly byte[] s_http21 = Interop.Sec_Application_Protocols.ToByteArray(new List<SslApplicationProtocol> { SslApplicationProtocol.Http2, SslApplicationProtocol.Http11 });
-        private static readonly bool UseNewCryptoApi =
-            Environment.OSVersion.Version.Major >= 10 && Environment.OSVersion.Version.Build >= 18836;
-        private const string SecurityPackage = "Microsoft Unified Security Protocol Provider";
-        private const Interop.SspiCli.ContextFlags RequiredFlags =
-            Interop.SspiCli.ContextFlags.ReplayDetect |
-            Interop.SspiCli.ContextFlags.SequenceDetect |
-            Interop.SspiCli.ContextFlags.Confidentiality |
-            Interop.SspiCli.ContextFlags.AllocateMemory;
-        private const Interop.SspiCli.ContextFlags ServerRequiredFlags =
-            RequiredFlags | Interop.SspiCli.ContextFlags.AcceptStream | Interop.SspiCli.ContextFlags.AcceptExtendedError;
-        public static Exception GetException(SecurityStatusPal status)
-        {
-            int win32Code = (int)SecurityStatusAdapterPal.GetInteropFromSecurityStatusPal(status);
-            return new Win32Exception(win32Code);
-        }
-        internal const bool StartMutualAuthAsAnonymous = true;
-        internal const bool CanEncryptEmptyMessage = true;
-        private static readonly byte[] s_sessionTokenBuffer = InitSessionTokenBuffer();
-        private static byte[] InitSessionTokenBuffer()
-        {
-            var schannelSessionToken = new Interop.SChannel.SCHANNEL_SESSION_TOKEN() {
-                dwTokenType = Interop.SChannel.SCHANNEL_SESSION,
-                dwFlags = Interop.SChannel.SSL_SESSION_DISABLE_RECONNECTS,
-            };
-            return MemoryMarshal.AsBytes(new ReadOnlySpan<Interop.SChannel.SCHANNEL_SESSION_TOKEN>(in schannelSessionToken)).ToArray();
-        }
-        public static void VerifyPackageInfo()
-        {
-            SSPIWrapper.GetVerifyPackageInfo(GlobalSSPI.SSPISecureChannel, SecurityPackage, true);
-        }
-        private static unsafe void SetAlpn(ref InputSecurityBuffers inputBuffers, List<SslApplicationProtocol> alpn, Span<byte> localBuffer)
-        {
-            if (alpn.Count == 1 && alpn[0] == SslApplicationProtocol.Http11)
-            {
-                inputBuffers.SetNextBuffer(new InputSecurityBuffer(s_http1, SecurityBufferType.SECBUFFER_APPLICATION_PROTOCOLS));
-            }
-            else if (alpn.Count == 1 && alpn[0] == SslApplicationProtocol.Http2)
-            {
-                inputBuffers.SetNextBuffer(new InputSecurityBuffer(s_http2, SecurityBufferType.SECBUFFER_APPLICATION_PROTOCOLS));
-            }
-            else if (alpn.Count == 2 && alpn[0] == SslApplicationProtocol.Http11 && alpn[1] == SslApplicationProtocol.Http2)
-            {
-                inputBuffers.SetNextBuffer(new InputSecurityBuffer(s_http12, SecurityBufferType.SECBUFFER_APPLICATION_PROTOCOLS));
-            }
-            else if (alpn.Count == 2 && alpn[0] == SslApplicationProtocol.Http2 && alpn[1] == SslApplicationProtocol.Http11)
-            {
-                inputBuffers.SetNextBuffer(new InputSecurityBuffer(s_http21, SecurityBufferType.SECBUFFER_APPLICATION_PROTOCOLS));
-            }
-            else
-            {
-                int protocolLength = Interop.Sec_Application_Protocols.GetProtocolLength(alpn);
-                int bufferLength = sizeof(Interop.Sec_Application_Protocols) + protocolLength;
-                Span<byte> alpnBuffer = bufferLength <= localBuffer.Length ? localBuffer : new byte[bufferLength];
-                Interop.Sec_Application_Protocols.SetProtocols(alpnBuffer, alpn, protocolLength);
-                inputBuffers.SetNextBuffer(new InputSecurityBuffer(alpnBuffer, SecurityBufferType.SECBUFFER_APPLICATION_PROTOCOLS));
-            }
-        }
-        public static SecurityStatusPal SelectApplicationProtocol(
-            SafeFreeCredentials? credentialsHandle,
-            SafeDeleteSslContext? context,
-            SslAuthenticationOptions sslAuthenticationOptions,
-            ReadOnlySpan<byte> clientProtocols)
-        {
-            throw new PlatformNotSupportedException(nameof(SelectApplicationProtocol));
-        }
-        public static unsafe SecurityStatusPal AcceptSecurityContext(
-            ref SafeFreeCredentials? credentialsHandle,
-            ref SafeDeleteSslContext? context,
-            ReadOnlySpan<byte> inputBuffer,
-            ref byte[]? outputBuffer,
-            SslAuthenticationOptions sslAuthenticationOptions)
-        {
-            Interop.SspiCli.ContextFlags unusedAttributes = default;
-            scoped InputSecurityBuffers inputBuffers = default;
-            inputBuffers.SetNextBuffer(new InputSecurityBuffer(inputBuffer, SecurityBufferType.SECBUFFER_TOKEN));
-            inputBuffers.SetNextBuffer(new InputSecurityBuffer(default, SecurityBufferType.SECBUFFER_EMPTY));
-            if (context == null && sslAuthenticationOptions.ApplicationProtocols != null && sslAuthenticationOptions.ApplicationProtocols.Count != 0)
-            {
-                Span<byte> localBuffer = stackalloc byte[64];
-                SetAlpn(ref inputBuffers, sslAuthenticationOptions.ApplicationProtocols, localBuffer);
-            }
-            var resultBuffer = new SecurityBuffer(outputBuffer, SecurityBufferType.SECBUFFER_TOKEN);
-            int errorCode = SSPIWrapper.AcceptSecurityContext(
-                GlobalSSPI.SSPISecureChannel,
-                credentialsHandle,
-                ref context,
-                ServerRequiredFlags | (sslAuthenticationOptions.RemoteCertRequired ? Interop.SspiCli.ContextFlags.MutualAuth : Interop.SspiCli.ContextFlags.Zero),
-                Interop.SspiCli.Endianness.SECURITY_NATIVE_DREP,
-                inputBuffers,
-                ref resultBuffer,
-                ref unusedAttributes);
-            outputBuffer = resultBuffer.token;
-            return SecurityStatusAdapterPal.GetSecurityStatusPalFromNativeInt(errorCode);
-        }
-        public static bool TryUpdateClintCertificate(
-            SafeFreeCredentials? _1,
-            SafeDeleteSslContext? _2,
-            SslAuthenticationOptions _3)
-        {
-            return false;
-        }
-        public static unsafe SecurityStatusPal InitializeSecurityContext(
-            ref SafeFreeCredentials? credentialsHandle,
-            ref SafeDeleteSslContext? context,
-            string? targetName,
-            ReadOnlySpan<byte> inputBuffer,
-            ref byte[]? outputBuffer,
-            SslAuthenticationOptions sslAuthenticationOptions)
-        {
-            bool newContext = context == null;
-            Interop.SspiCli.ContextFlags unusedAttributes = default;
-            scoped InputSecurityBuffers inputBuffers = default;
-            inputBuffers.SetNextBuffer(new InputSecurityBuffer(inputBuffer, SecurityBufferType.SECBUFFER_TOKEN));
-            inputBuffers.SetNextBuffer(new InputSecurityBuffer(default, SecurityBufferType.SECBUFFER_EMPTY));
-            if (context == null && sslAuthenticationOptions.ApplicationProtocols != null && sslAuthenticationOptions.ApplicationProtocols.Count != 0)
-            {
-                Span<byte> localBuffer = stackalloc byte[64];
-                SetAlpn(ref inputBuffers, sslAuthenticationOptions.ApplicationProtocols, localBuffer);
-            }
-            var resultBuffer = new SecurityBuffer(outputBuffer, SecurityBufferType.SECBUFFER_TOKEN);
-            int errorCode = SSPIWrapper.InitializeSecurityContext(
-                            GlobalSSPI.SSPISecureChannel,
-                            ref credentialsHandle,
-                            ref context,
-                            targetName,
-                            RequiredFlags | Interop.SspiCli.ContextFlags.InitManualCredValidation,
-                            Interop.SspiCli.Endianness.SECURITY_NATIVE_DREP,
-                            inputBuffers,
-                            ref resultBuffer,
-                            ref unusedAttributes);
-            if (!sslAuthenticationOptions.AllowTlsResume && newContext && context != null)
-            {
-                var securityBuffer = new SecurityBuffer(s_sessionTokenBuffer, SecurityBufferType.SECBUFFER_TOKEN);
-                SecurityStatusPal result = SecurityStatusAdapterPal.GetSecurityStatusPalFromNativeInt(SSPIWrapper.ApplyControlToken(
-                    GlobalSSPI.SSPISecureChannel,
-                    ref context,
-                    in securityBuffer));
-                if (result.ErrorCode != SecurityStatusPalErrorCode.OK)
-                {
-                    return result;
-                }
-            }
-            outputBuffer = resultBuffer.token;
-            return SecurityStatusAdapterPal.GetSecurityStatusPalFromNativeInt(errorCode);
-        }
-        public static SecurityStatusPal Renegotiate(
-            ref SafeFreeCredentials? credentialsHandle,
-            ref SafeDeleteSslContext? context,
-            SslAuthenticationOptions sslAuthenticationOptions,
-            out byte[]? outputBuffer)
-        {
-            byte[]? output = Array.Empty<byte>();
-            SecurityStatusPal status = AcceptSecurityContext(ref credentialsHandle, ref context, Span<byte>.Empty, ref output, sslAuthenticationOptions);
-            outputBuffer = output;
-            return status;
-        }
-        public static SafeFreeCredentials AcquireCredentialsHandle(SslAuthenticationOptions sslAuthenticationOptions, bool newCredentialsRequested)
-        {
-            SslStreamCertificateContext? certificateContext = sslAuthenticationOptions.CertificateContext;
-            try
-            {
-                EncryptionPolicy policy = sslAuthenticationOptions.EncryptionPolicy;
-#pragma warning disable SYSLIB0040 // NoEncryption and AllowNoEncryption are obsolete
-                SafeFreeCredentials cred = !UseNewCryptoApi || policy == EncryptionPolicy.NoEncryption ?
-                    AcquireCredentialsHandleSchannelCred(sslAuthenticationOptions) :
-                    AcquireCredentialsHandleSchCredentials(sslAuthenticationOptions);
-#pragma warning restore SYSLIB0040
-                if (certificateContext != null && certificateContext.Trust != null && certificateContext.Trust._sendTrustInHandshake)
-                {
-                    AttachCertificateStore(cred, certificateContext.Trust._store!);
-                }
-                if (newCredentialsRequested && sslAuthenticationOptions.CertificateContext != null)
-                {
-                    SafeFreeCredential_SECURITY handle = (SafeFreeCredential_SECURITY)cred;
-                    handle.HasLocalCertificate = true;
-                }
-                return cred;
-            }
-            catch (Win32Exception e) when (e.NativeErrorCode == (int)Interop.SECURITY_STATUS.NoCredentials && certificateContext != null)
-            {
-                Debug.Assert(certificateContext.TargetCertificate.HasPrivateKey);
-                using SafeCertContextHandle safeCertContextHandle = Interop.Crypt32.CertDuplicateCertificateContext(certificateContext.TargetCertificate.Handle);
-                throw new AuthenticationException(safeCertContextHandle.HasEphemeralPrivateKey ? SR.net_auth_ephemeral : SR.net_auth_SSPI, e);
-            }
-            catch (Win32Exception e)
-            {
-                throw new AuthenticationException(SR.net_auth_SSPI, e);
-            }
-        }
-        private static unsafe void AttachCertificateStore(SafeFreeCredentials cred, X509Store store)
-        {
-            Interop.SspiCli.SecPkgCred_ClientCertPolicy clientCertPolicy = default;
-            fixed (char* ptr = store.Name)
-            {
-                clientCertPolicy.pwszSslCtlStoreName = ptr;
-                Interop.SECURITY_STATUS errorCode = Interop.SspiCli.SetCredentialsAttributesW(
-                            cred._handle,
-                            (long)Interop.SspiCli.ContextAttribute.SECPKG_ATTR_CLIENT_CERT_POLICY,
-                            clientCertPolicy,
-                            sizeof(Interop.SspiCli.SecPkgCred_ClientCertPolicy));
-                if (errorCode != Interop.SECURITY_STATUS.OK)
-                {
-                    throw new Win32Exception((int)errorCode);
-                }
-            }
-            return;
-        }
-        public static unsafe SafeFreeCredentials AcquireCredentialsHandleSchannelCred(SslAuthenticationOptions authOptions)
-        {
-            X509Certificate2? certificate = authOptions.CertificateContext?.TargetCertificate;
-            bool isServer = authOptions.IsServer;
-            int protocolFlags = GetProtocolFlagsFromSslProtocols(authOptions.EnabledSslProtocols, isServer);
-            Interop.SspiCli.SCHANNEL_CRED.Flags flags;
-            Interop.SspiCli.CredentialUse direction;
-            if (!isServer)
-            {
-                direction = Interop.SspiCli.CredentialUse.SECPKG_CRED_OUTBOUND;
-                flags =
-                    Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_MANUAL_CRED_VALIDATION |
-                    Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_NO_DEFAULT_CREDS |
-                    Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_SEND_AUX_RECORD;
-                if (authOptions.CertificateRevocationCheckMode != X509RevocationMode.NoCheck)
-                {
-                    flags |=
-                        Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_REVOCATION_CHECK_END_CERT |
-                        Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_IGNORE_NO_REVOCATION_CHECK |
-                        Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_IGNORE_REVOCATION_OFFLINE;
-                }
-            }
-            else
-            {
-                direction = Interop.SspiCli.CredentialUse.SECPKG_CRED_INBOUND;
-                flags =
-                    Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_SEND_AUX_RECORD |
-                    Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_NO_SYSTEM_MAPPER;
-                if (!authOptions.AllowTlsResume)
-                {
-                    flags |= Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_CRED_DISABLE_RECONNECTS;
-                }
-            }
-            EncryptionPolicy policy = authOptions.EncryptionPolicy;
-#pragma warning disable SYSLIB0040 // NoEncryption and AllowNoEncryption are obsolete
-            if (((protocolFlags == 0) ||
-                    (protocolFlags & ~(Interop.SChannel.SP_PROT_SSL2 | Interop.SChannel.SP_PROT_SSL3)) != 0)
-                    && (policy != EncryptionPolicy.AllowNoEncryption) && (policy != EncryptionPolicy.NoEncryption))
-            {
-                flags |= Interop.SspiCli.SCHANNEL_CRED.Flags.SCH_USE_STRONG_CRYPTO;
-            }
-#pragma warning restore SYSLIB0040
-            if (NetEventSource.Log.IsEnabled()) NetEventSource.Info($"flags=({flags}), ProtocolFlags=({protocolFlags}), EncryptionPolicy={policy}");
-            Interop.SspiCli.SCHANNEL_CRED secureCredential = CreateSecureCredential(
-                flags,
-                protocolFlags,
-                policy);
-            if (!isServer && !authOptions.AllowTlsResume)
-            {
-                secureCredential.dwSessionLifespan = -1;
-            }
-            if (certificate != null)
-            {
-                secureCredential.cCreds = 1;
-                Interop.Crypt32.CERT_CONTEXT* certificateHandle = (Interop.Crypt32.CERT_CONTEXT*)certificate.Handle;
-                secureCredential.paCred = &certificateHandle;
-            }
-            return AcquireCredentialsHandle(direction, &secureCredential);
-        }
-        public static unsafe SafeFreeCredentials AcquireCredentialsHandleSchCredentials(SslAuthenticationOptions authOptions)
-        {
-            X509Certificate2? certificate = authOptions.CertificateContext?.TargetCertificate;
-            bool isServer = authOptions.IsServer;
-            int protocolFlags = GetProtocolFlagsFromSslProtocols(authOptions.EnabledSslProtocols, isServer);
-            Interop.SspiCli.SCH_CREDENTIALS.Flags flags;
-            Interop.SspiCli.CredentialUse direction;
-            if (isServer)
-            {
-                direction = Interop.SspiCli.CredentialUse.SECPKG_CRED_INBOUND;
-                flags =
-                    Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_SEND_AUX_RECORD |
-                    Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_NO_SYSTEM_MAPPER;
-                if (!authOptions.AllowTlsResume)
-                {
-                    flags |= Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_DISABLE_RECONNECTS;
-                }
-            }
-            else
-            {
-                direction = Interop.SspiCli.CredentialUse.SECPKG_CRED_OUTBOUND;
-                flags =
-                    Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_MANUAL_CRED_VALIDATION |
-                    Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_NO_DEFAULT_CREDS |
-                    Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_SEND_AUX_RECORD;
-                if (authOptions.CertificateRevocationCheckMode != X509RevocationMode.NoCheck)
-                {
-                    flags |=
-                        Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_REVOCATION_CHECK_END_CERT |
-                        Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_IGNORE_NO_REVOCATION_CHECK |
-                        Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_CRED_IGNORE_REVOCATION_OFFLINE;
-                }
-            }
-            EncryptionPolicy policy = authOptions.EncryptionPolicy;
-            if (policy == EncryptionPolicy.RequireEncryption)
-            {
-                if ((protocolFlags & Interop.SChannel.SP_PROT_SSL3) == 0)
-                {
-                    flags |= Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_USE_STRONG_CRYPTO;
-                }
-            }
-#pragma warning disable SYSLIB0040 // NoEncryption and AllowNoEncryption are obsolete
-            else if (policy == EncryptionPolicy.AllowNoEncryption)
-            {
-                flags |= Interop.SspiCli.SCH_CREDENTIALS.Flags.SCH_ALLOW_NULL_ENCRYPTION;
-            }
-#pragma warning restore SYSLIB0040
-            else
-            {
-                throw new ArgumentException(SR.Format(SR.net_invalid_enum, "EncryptionPolicy"), nameof(policy));
-            }
-            Interop.SspiCli.SCH_CREDENTIALS credential = default;
-            credential.dwVersion = Interop.SspiCli.SCH_CREDENTIALS.CurrentVersion;
-            credential.dwFlags = flags;
-            if (!isServer && !authOptions.AllowTlsResume)
-            {
-                credential.dwSessionLifespan = -1;
-            }
-            if (certificate != null)
-            {
-                credential.cCreds = 1;
-                Interop.Crypt32.CERT_CONTEXT* certificateHandle = (Interop.Crypt32.CERT_CONTEXT*)certificate.Handle;
-                credential.paCred = &certificateHandle;
-            }
-            if (NetEventSource.Log.IsEnabled()) NetEventSource.Info($"flags=({flags}), ProtocolFlags=({protocolFlags}), EncryptionPolicy={policy}");
-            if (protocolFlags != 0)
-            {
-                Interop.SspiCli.TLS_PARAMETERS tlsParameters = default;
-                tlsParameters.grbitDisabledProtocols = (uint)protocolFlags ^ uint.MaxValue;
-                credential.cTlsParameters = 1;
-                credential.pTlsParameters = &tlsParameters;
-            }
-            return AcquireCredentialsHandle(direction, &credential);
-        }
-        public static unsafe SecurityStatusPal EncryptMessage(SafeDeleteSslContext securityContext, ReadOnlyMemory<byte> input, int headerSize, int trailerSize, ref byte[] output, out int resultSize)
-        {
-            int bufferSizeNeeded = checked(input.Length + headerSize + trailerSize);
-            if (output == null || output.Length < bufferSizeNeeded)
-            {
-                output = new byte[bufferSizeNeeded];
-            }
-            input.Span.CopyTo(new Span<byte>(output, headerSize, input.Length));
-            const int NumSecBuffers = 4; // header + data + trailer + empty
-            Interop.SspiCli.SecBuffer* unmanagedBuffer = stackalloc Interop.SspiCli.SecBuffer[NumSecBuffers];
-            Interop.SspiCli.SecBufferDesc sdcInOut = new Interop.SspiCli.SecBufferDesc(NumSecBuffers)
-            {
-                pBuffers = unmanagedBuffer
-            };
-            fixed (byte* outputPtr = output)
-            {
-                Interop.SspiCli.SecBuffer* headerSecBuffer = &unmanagedBuffer[0];
-                headerSecBuffer->BufferType = SecurityBufferType.SECBUFFER_STREAM_HEADER;
-                headerSecBuffer->pvBuffer = (IntPtr)outputPtr;
-                headerSecBuffer->cbBuffer = headerSize;
-                Interop.SspiCli.SecBuffer* dataSecBuffer = &unmanagedBuffer[1];
-                dataSecBuffer->BufferType = SecurityBufferType.SECBUFFER_DATA;
-                dataSecBuffer->pvBuffer = (IntPtr)(outputPtr + headerSize);
-                dataSecBuffer->cbBuffer = input.Length;
-                Interop.SspiCli.SecBuffer* trailerSecBuffer = &unmanagedBuffer[2];
-                trailerSecBuffer->BufferType = SecurityBufferType.SECBUFFER_STREAM_TRAILER;
-                trailerSecBuffer->pvBuffer = (IntPtr)(outputPtr + headerSize + input.Length);
-                trailerSecBuffer->cbBuffer = trailerSize;
-                Interop.SspiCli.SecBuffer* emptySecBuffer = &unmanagedBuffer[3];
-                emptySecBuffer->BufferType = SecurityBufferType.SECBUFFER_EMPTY;
-                emptySecBuffer->cbBuffer = 0;
-                emptySecBuffer->pvBuffer = IntPtr.Zero;
-                int errorCode = GlobalSSPI.SSPISecureChannel.EncryptMessage(securityContext, ref sdcInOut, 0);
-                if (errorCode != 0)
-                {
-                    if (NetEventSource.Log.IsEnabled())
-                        NetEventSource.Info(securityContext, $"Encrypt ERROR {errorCode:X}");
-                    resultSize = 0;
-                    return SecurityStatusAdapterPal.GetSecurityStatusPalFromNativeInt(errorCode);
-                }
-                Debug.Assert(headerSecBuffer->cbBuffer >= 0 && dataSecBuffer->cbBuffer >= 0 && trailerSecBuffer->cbBuffer >= 0);
-                Debug.Assert(checked(headerSecBuffer->cbBuffer + dataSecBuffer->cbBuffer + trailerSecBuffer->cbBuffer) <= output.Length);
-                resultSize = checked(headerSecBuffer->cbBuffer + dataSecBuffer->cbBuffer + trailerSecBuffer->cbBuffer);
-                return new SecurityStatusPal(SecurityStatusPalErrorCode.OK);
-            }
-        }
-        public static unsafe SecurityStatusPal DecryptMessage(SafeDeleteSslContext? securityContext, Span<byte> buffer, out int offset, out int count)
-        {
-            const int NumSecBuffers = 4; // data + empty + empty + empty
-            fixed (byte* bufferPtr = buffer)
-            {
-                Interop.SspiCli.SecBuffer* unmanagedBuffer = stackalloc Interop.SspiCli.SecBuffer[NumSecBuffers];
-                Interop.SspiCli.SecBuffer* dataBuffer = &unmanagedBuffer[0];
-                dataBuffer->BufferType = SecurityBufferType.SECBUFFER_DATA;
-                dataBuffer->pvBuffer = (IntPtr)bufferPtr;
-                dataBuffer->cbBuffer = buffer.Length;
-                for (int i = 1; i < NumSecBuffers; i++)
-                {
-                    Interop.SspiCli.SecBuffer* emptyBuffer = &unmanagedBuffer[i];
-                    emptyBuffer->BufferType = SecurityBufferType.SECBUFFER_EMPTY;
-                    emptyBuffer->pvBuffer = IntPtr.Zero;
-                    emptyBuffer->cbBuffer = 0;
-                }
-                Interop.SspiCli.SecBufferDesc sdcInOut = new Interop.SspiCli.SecBufferDesc(NumSecBuffers)
-                {
-                    pBuffers = unmanagedBuffer
-                };
-                Interop.SECURITY_STATUS errorCode = (Interop.SECURITY_STATUS)GlobalSSPI.SSPISecureChannel.DecryptMessage(securityContext!, ref sdcInOut, out _);
-                count = 0;
-                offset = 0;
-                for (int i = 0; i < NumSecBuffers; i++)
-                {
-                    if ((errorCode == Interop.SECURITY_STATUS.OK && unmanagedBuffer[i].BufferType == SecurityBufferType.SECBUFFER_DATA)
-                        || (errorCode != Interop.SECURITY_STATUS.OK && unmanagedBuffer[i].BufferType == SecurityBufferType.SECBUFFER_EXTRA))
-                    {
-                        offset = (int)((byte*)unmanagedBuffer[i].pvBuffer - bufferPtr);
-                        count = unmanagedBuffer[i].cbBuffer;
-                        Debug.Assert(offset >= 0 && count >= 0, $"Expected offset and count greater than 0, got {offset} and {count}");
-                        Debug.Assert(checked(offset + count) <= buffer.Length, $"Expected offset+count <= buffer.Length, got {offset}+{count}>={buffer.Length}");
-                        break;
-                    }
-                }
-                return SecurityStatusAdapterPal.GetSecurityStatusPalFromInterop(errorCode);
-            }
-        }
-        public static SecurityStatusPal ApplyAlertToken(SafeDeleteSslContext? securityContext, TlsAlertType alertType, TlsAlertMessage alertMessage)
-        {
-            var alertToken = new Interop.SChannel.SCHANNEL_ALERT_TOKEN
-            {
-                dwTokenType = Interop.SChannel.SCHANNEL_ALERT,
-                dwAlertType = (uint)alertType,
-                dwAlertNumber = (uint)alertMessage
-            };
-            byte[] buffer = MemoryMarshal.AsBytes(new ReadOnlySpan<Interop.SChannel.SCHANNEL_ALERT_TOKEN>(in alertToken)).ToArray();
-            var securityBuffer = new SecurityBuffer(buffer, SecurityBufferType.SECBUFFER_TOKEN);
-            var errorCode = (Interop.SECURITY_STATUS)SSPIWrapper.ApplyControlToken(
-                GlobalSSPI.SSPISecureChannel,
-                ref securityContext,
-                in securityBuffer);
-            return SecurityStatusAdapterPal.GetSecurityStatusPalFromInterop(errorCode, attachException: true);
-        }
-        private static readonly byte[] s_schannelShutdownBytes = BitConverter.GetBytes(Interop.SChannel.SCHANNEL_SHUTDOWN);
-        public static SecurityStatusPal ApplyShutdownToken(SafeDeleteSslContext? securityContext)
-        {
-            var securityBuffer = new SecurityBuffer(s_schannelShutdownBytes, SecurityBufferType.SECBUFFER_TOKEN);
-            var errorCode = (Interop.SECURITY_STATUS)SSPIWrapper.ApplyControlToken(
-                GlobalSSPI.SSPISecureChannel,
-                ref securityContext,
-                in securityBuffer);
-            return SecurityStatusAdapterPal.GetSecurityStatusPalFromInterop(errorCode, attachException: true);
-        }
-        public static SafeFreeContextBufferChannelBinding? QueryContextChannelBinding(SafeDeleteContext securityContext, ChannelBindingKind attribute)
-        {
-            return SSPIWrapper.QueryContextChannelBinding(GlobalSSPI.SSPISecureChannel, securityContext, (Interop.SspiCli.ContextAttribute)attribute);
-        }
-        public static void QueryContextStreamSizes(SafeDeleteContext securityContext, out StreamSizes streamSizes)
-        {
-            SecPkgContext_StreamSizes interopStreamSizes = default;
-            bool success = SSPIWrapper.QueryBlittableContextAttributes(GlobalSSPI.SSPISecureChannel, securityContext, Interop.SspiCli.ContextAttribute.SECPKG_ATTR_STREAM_SIZES, ref interopStreamSizes);
-            Debug.Assert(success);
-            streamSizes = new StreamSizes(interopStreamSizes);
-        }
-        public static void QueryContextConnectionInfo(SafeDeleteContext securityContext, ref SslConnectionInfo connectionInfo)
-        {
-            connectionInfo.UpdateSslConnectionInfo(securityContext);
-        }
-        private static int GetProtocolFlagsFromSslProtocols(SslProtocols protocols, bool isServer)
-        {
-            int protocolFlags = (int)protocols;
-            if (isServer)
-            {
-                protocolFlags &= Interop.SChannel.ServerProtocolMask;
-            }
-            else
-            {
-                protocolFlags &= Interop.SChannel.ClientProtocolMask;
-            }
-            return protocolFlags;
-        }
-        private static Interop.SspiCli.SCHANNEL_CRED CreateSecureCredential(
-            Interop.SspiCli.SCHANNEL_CRED.Flags flags,
-            int protocols, EncryptionPolicy policy)
-        {
-            var credential = new Interop.SspiCli.SCHANNEL_CRED()
-            {
-                hRootStore = IntPtr.Zero,
-                aphMappers = IntPtr.Zero,
-                palgSupportedAlgs = IntPtr.Zero,
-                paCred = null,
-                cCreds = 0,
-                cMappers = 0,
-                cSupportedAlgs = 0,
-                dwSessionLifespan = 0,
-                reserved = 0,
-                dwVersion = Interop.SspiCli.SCHANNEL_CRED.CurrentVersion
-            };
-            if (policy == EncryptionPolicy.RequireEncryption)
-            {
-                credential.dwMinimumCipherStrength = 0;
-                credential.dwMaximumCipherStrength = 0;
-            }
-#pragma warning disable SYSLIB0040 // NoEncryption and AllowNoEncryption are obsolete
-            else if (policy == EncryptionPolicy.AllowNoEncryption)
-            {
-                credential.dwMinimumCipherStrength = -1;
-                credential.dwMaximumCipherStrength = 0;
-            }
-            else if (policy == EncryptionPolicy.NoEncryption)
-            {
-                credential.dwMinimumCipherStrength = -1;
-                credential.dwMaximumCipherStrength = -1;
-            }
-#pragma warning restore SYSLIB0040
-            else
-            {
-                throw new ArgumentException(SR.Format(SR.net_invalid_enum, "EncryptionPolicy"), nameof(policy));
-            }
-            credential.dwFlags = flags;
-            credential.grbitEnabledProtocols = protocols;
-            return credential;
-        }
-        private static unsafe SafeFreeCredentials AcquireCredentialsHandle(Interop.SspiCli.CredentialUse credUsage, Interop.SspiCli.SCHANNEL_CRED* secureCredential)
-        {
-            try
-            {
-                using SafeAccessTokenHandle invalidHandle = SafeAccessTokenHandle.InvalidHandle;
-                return WindowsIdentity.RunImpersonated<SafeFreeCredentials>(invalidHandle, () =>
-                {
-                    return SSPIWrapper.AcquireCredentialsHandle(GlobalSSPI.SSPISecureChannel, SecurityPackage, credUsage, secureCredential);
-                });
-            }
-            catch
-            {
-                return SSPIWrapper.AcquireCredentialsHandle(GlobalSSPI.SSPISecureChannel, SecurityPackage, credUsage, secureCredential);
-            }
-        }
-        private static unsafe SafeFreeCredentials AcquireCredentialsHandle(Interop.SspiCli.CredentialUse credUsage, Interop.SspiCli.SCH_CREDENTIALS* secureCredential)
-        {
-            try
-            {
-                using SafeAccessTokenHandle invalidHandle = SafeAccessTokenHandle.InvalidHandle;
-                return WindowsIdentity.RunImpersonated<SafeFreeCredentials>(invalidHandle, () =>
-                {
-                    return SSPIWrapper.AcquireCredentialsHandle(GlobalSSPI.SSPISecureChannel, SecurityPackage, credUsage, secureCredential);
-                });
-            }
-            catch
-            {
-                return SSPIWrapper.AcquireCredentialsHandle(GlobalSSPI.SSPISecureChannel, SecurityPackage, credUsage, secureCredential);
-            }
-        }
-    }
-}

--- a/src/libraries/System.Private.CoreLib/src/System/Threading/Tasks/Task.cs
+++ b//dev/null
@@ -1,3414 +0,0 @@
-using System.Collections.Generic;
-using System.Diagnostics;
-using System.Diagnostics.CodeAnalysis;
-using System.Diagnostics.Tracing;
-using System.Runtime.CompilerServices;
-using System.Runtime.ExceptionServices;
-using System.Runtime.InteropServices;
-using System.Runtime.Versioning;
-namespace System.Threading.Tasks
-{
-    public enum TaskStatus
-    {
-        Created,
-        WaitingForActivation,
-        WaitingToRun,
-        Running,
-        WaitingForChildrenToComplete,
-        RanToCompletion,
-        Canceled,
-        Faulted
-    }
-    [DebuggerTypeProxy(typeof(SystemThreadingTasks_TaskDebugView))]
-    [DebuggerDisplay("Id = {Id}, Status = {Status}, Method = {DebuggerDisplayMethodDescription}")]
-    public class Task : IAsyncResult, IDisposable
-    {
-        [ThreadStatic]
-        internal static Task? t_currentTask;  // The currently executing task.
-        private static int s_taskIdCounter; // static counter used to generate unique task IDs
-        private int m_taskId; // this task's unique ID. initialized only if it is ever requested
-        internal Delegate? m_action;
-        private protected object? m_stateObject; // A state object that can be optionally supplied, passed to action.
-        internal TaskScheduler? m_taskScheduler; // The task scheduler this task runs under.
-        internal volatile int m_stateFlags; // SOS DumpAsync command depends on this name
-        private Task? ParentForDebugger => m_contingentProperties?.m_parent; // Private property used by a debugger to access this Task's parent
-        private int StateFlagsForDebugger => m_stateFlags; // Private property used by a debugger to access this Task's state flags
-        private TaskStateFlags StateFlags => (TaskStateFlags)(m_stateFlags & ~(int)TaskStateFlags.OptionsMask); // Private property used to help with debugging
-        [Flags]
-        internal enum TaskStateFlags
-        {
-            Started = 0x10000,                       // bin: 0000 0000 0000 0001 0000 0000 0000 0000
-            DelegateInvoked = 0x20000,               // bin: 0000 0000 0000 0010 0000 0000 0000 0000
-            Disposed = 0x40000,                      // bin: 0000 0000 0000 0100 0000 0000 0000 0000
-            ExceptionObservedByParent = 0x80000,     // bin: 0000 0000 0000 1000 0000 0000 0000 0000
-            CancellationAcknowledged = 0x100000,     // bin: 0000 0000 0001 0000 0000 0000 0000 0000
-            Faulted = 0x200000,                      // bin: 0000 0000 0010 0000 0000 0000 0000 0000
-            Canceled = 0x400000,                     // bin: 0000 0000 0100 0000 0000 0000 0000 0000
-            WaitingOnChildren = 0x800000,            // bin: 0000 0000 1000 0000 0000 0000 0000 0000
-            RanToCompletion = 0x1000000,             // bin: 0000 0001 0000 0000 0000 0000 0000 0000
-            WaitingForActivation = 0x2000000,        // bin: 0000 0010 0000 0000 0000 0000 0000 0000
-            CompletionReserved = 0x4000000,          // bin: 0000 0100 0000 0000 0000 0000 0000 0000
-            WaitCompletionNotification = 0x10000000, // bin: 0001 0000 0000 0000 0000 0000 0000 0000
-            ExecutionContextIsNull = 0x20000000,     // bin: 0010 0000 0000 0000 0000 0000 0000 0000
-            TaskScheduledWasFired = 0x40000000,      // bin: 0100 0000 0000 0000 0000 0000 0000 0000
-            CompletedMask = Canceled | Faulted | RanToCompletion, // A mask for all of the final states a task may be in. SOS DumpAsync command depends on these values.
-            OptionsMask = 0xFFFF,                    // signifies the Options portion of m_stateFlags bin: 0000 0000 0000 0000 1111 1111 1111 1111
-        }
-        private const int CANCELLATION_REQUESTED = 0x1;
-        private volatile object? m_continuationObject; // SOS DumpAsync command depends on this name
-        private static readonly object s_taskCompletionSentinel = new object();
-        internal static bool s_asyncDebuggingEnabled; // false by default
-        private static Dictionary<int, Task>? s_currentActiveTasks;
-        internal static bool AddToActiveTasks(Task task)
-        {
-            Debug.Assert(task != null, "Null Task objects can't be added to the ActiveTasks collection");
-            Dictionary<int, Task> activeTasks =
-                Volatile.Read(ref s_currentActiveTasks) ??
-                Interlocked.CompareExchange(ref s_currentActiveTasks, new Dictionary<int, Task>(), null) ??
-                s_currentActiveTasks;
-            int taskId = task.Id;
-            lock (activeTasks)
-            {
-                activeTasks[taskId] = task;
-            }
-            return true;
-        }
-        internal static void RemoveFromActiveTasks(Task task)
-        {
-            Dictionary<int, Task>? activeTasks = s_currentActiveTasks;
-            if (activeTasks is null)
-                return;
-            int taskId = task.Id;
-            lock (activeTasks)
-            {
-                activeTasks.Remove(taskId);
-            }
-        }
-        internal sealed class ContingentProperties
-        {
-            internal ExecutionContext? m_capturedContext; // The execution context to run the task within, if any. Only set from non-concurrent contexts.
-            internal volatile ManualResetEventSlim? m_completionEvent; // Lazily created if waiting is required.
-            internal volatile TaskExceptionHolder? m_exceptionsHolder; // Tracks exceptions, if any have occurred
-            internal CancellationToken m_cancellationToken; // Task's cancellation token, if it has one
-            internal StrongBox<CancellationTokenRegistration>? m_cancellationRegistration; // Task's registration with the cancellation token
-            internal volatile int m_internalCancellationRequested; // Its own field because multiple threads legally try to set it.
-            internal volatile int m_completionCountdown = 1;
-            internal volatile List<Task>? m_exceptionalChildren;
-            internal Task? m_parent;
-            internal void SetCompleted()
-            {
-                ManualResetEventSlim? mres = m_completionEvent;
-                if (mres != null) SetEvent(mres);
-            }
-            internal static void SetEvent(ManualResetEventSlim mres)
-            {
-                try
-                {
-                    mres.Set();
-                }
-                catch (ObjectDisposedException)
-                {
-                }
-            }
-            internal void UnregisterCancellationCallback()
-            {
-                if (m_cancellationRegistration != null)
-                {
-                    try { m_cancellationRegistration.Value.Dispose(); }
-                    catch (ObjectDisposedException) { }
-                    m_cancellationRegistration = null;
-                }
-            }
-        }
-        internal ContingentProperties? m_contingentProperties;
-        internal Task(bool canceled, TaskCreationOptions creationOptions, CancellationToken ct)
-        {
-            int optionFlags = (int)creationOptions;
-            if (canceled)
-            {
-                m_stateFlags = (int)TaskStateFlags.Canceled | (int)TaskStateFlags.CancellationAcknowledged | optionFlags;
-                m_contingentProperties = new ContingentProperties() // can't have children, so just instantiate directly
-                {
-                    m_cancellationToken = ct,
-                    m_internalCancellationRequested = CANCELLATION_REQUESTED,
-                };
-            }
-            else
-            {
-                m_stateFlags = (int)TaskStateFlags.RanToCompletion | optionFlags;
-            }
-        }
-        internal Task()
-        {
-            m_stateFlags = (int)TaskStateFlags.WaitingForActivation | (int)InternalTaskOptions.PromiseTask;
-        }
-        internal Task(object? state, TaskCreationOptions creationOptions, bool promiseStyle)
-        {
-            Debug.Assert(promiseStyle, "Promise CTOR: promiseStyle was false");
-            if ((creationOptions & ~(TaskCreationOptions.AttachedToParent | TaskCreationOptions.RunContinuationsAsynchronously)) != 0)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.creationOptions);
-            }
-            if ((creationOptions & TaskCreationOptions.AttachedToParent) != 0)
-            {
-                Task? parent = InternalCurrent;
-                if (parent != null)
-                {
-                    EnsureContingentPropertiesInitializedUnsafe().m_parent = parent;
-                }
-            }
-            TaskConstructorCore(null, state, default, creationOptions, InternalTaskOptions.PromiseTask, null);
-        }
-        public Task(Action action)
-            : this(action, null, null, default, TaskCreationOptions.None, InternalTaskOptions.None, null)
-        {
-        }
-        public Task(Action action, CancellationToken cancellationToken)
-            : this(action, null, null, cancellationToken, TaskCreationOptions.None, InternalTaskOptions.None, null)
-        {
-        }
-        public Task(Action action, TaskCreationOptions creationOptions)
-            : this(action, null, InternalCurrentIfAttached(creationOptions), default, creationOptions, InternalTaskOptions.None, null)
-        {
-        }
-        public Task(Action action, CancellationToken cancellationToken, TaskCreationOptions creationOptions)
-            : this(action, null, InternalCurrentIfAttached(creationOptions), cancellationToken, creationOptions, InternalTaskOptions.None, null)
-        {
-        }
-        public Task(Action<object?> action, object? state)
-            : this(action, state, null, default, TaskCreationOptions.None, InternalTaskOptions.None, null)
-        {
-        }
-        public Task(Action<object?> action, object? state, CancellationToken cancellationToken)
-            : this(action, state, null, cancellationToken, TaskCreationOptions.None, InternalTaskOptions.None, null)
-        {
-        }
-        public Task(Action<object?> action, object? state, TaskCreationOptions creationOptions)
-            : this(action, state, InternalCurrentIfAttached(creationOptions), default, creationOptions, InternalTaskOptions.None, null)
-        {
-        }
-        public Task(Action<object?> action, object? state, CancellationToken cancellationToken, TaskCreationOptions creationOptions)
-            : this(action, state, InternalCurrentIfAttached(creationOptions), cancellationToken, creationOptions, InternalTaskOptions.None, null)
-        {
-        }
-        internal Task(Delegate action, object? state, Task? parent, CancellationToken cancellationToken,
-            TaskCreationOptions creationOptions, InternalTaskOptions internalOptions, TaskScheduler? scheduler)
-        {
-            if (action == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.action);
-            }
-            if (parent != null && (creationOptions & TaskCreationOptions.AttachedToParent) != 0)
-            {
-                EnsureContingentPropertiesInitializedUnsafe().m_parent = parent;
-            }
-            TaskConstructorCore(action, state, cancellationToken, creationOptions, internalOptions, scheduler);
-            Debug.Assert(m_contingentProperties == null || m_contingentProperties.m_capturedContext == null,
-                "Captured an ExecutionContext when one was already captured.");
-            CapturedContext = ExecutionContext.Capture();
-        }
-        internal void TaskConstructorCore(Delegate? action, object? state, CancellationToken cancellationToken,
-            TaskCreationOptions creationOptions, InternalTaskOptions internalOptions, TaskScheduler? scheduler)
-        {
-            m_action = action;
-            m_stateObject = state;
-            m_taskScheduler = scheduler;
-            if ((creationOptions &
-                    ~(TaskCreationOptions.AttachedToParent |
-                      TaskCreationOptions.LongRunning |
-                      TaskCreationOptions.DenyChildAttach |
-                      TaskCreationOptions.HideScheduler |
-                      TaskCreationOptions.PreferFairness |
-                      TaskCreationOptions.RunContinuationsAsynchronously)) != 0)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.creationOptions);
-            }
-#if DEBUG
-            int illegalInternalOptions =
-                    (int)(internalOptions &
-                            ~(InternalTaskOptions.PromiseTask |
-                              InternalTaskOptions.HiddenState |
-                              InternalTaskOptions.ContinuationTask |
-                              InternalTaskOptions.LazyCancellation |
-                              InternalTaskOptions.QueuedByRuntime));
-            Debug.Assert(illegalInternalOptions == 0, "TaskConstructorCore: Illegal internal options");
-#endif
-            Debug.Assert(m_stateFlags == 0, "TaskConstructorCore: non-zero m_stateFlags");
-            Debug.Assert((((int)creationOptions) | (int)TaskStateFlags.OptionsMask) == (int)TaskStateFlags.OptionsMask, "TaskConstructorCore: options take too many bits");
-            int tmpFlags = (int)creationOptions | (int)internalOptions; // one write to the volatile m_stateFlags instead of two when setting the above options
-            m_stateFlags = m_action == null || (internalOptions & InternalTaskOptions.ContinuationTask) != 0 ?
-                tmpFlags | (int)TaskStateFlags.WaitingForActivation :
-                tmpFlags;
-            ContingentProperties? props = m_contingentProperties;
-            if (props != null)
-            {
-                Task? parent = props.m_parent;
-                if (parent != null
-                    && ((creationOptions & TaskCreationOptions.AttachedToParent) != 0)
-                    && ((parent.CreationOptions & TaskCreationOptions.DenyChildAttach) == 0))
-                {
-                    parent.AddNewChild();
-                }
-            }
-            if (cancellationToken.CanBeCanceled)
-            {
-                Debug.Assert((internalOptions & InternalTaskOptions.ContinuationTask) == 0, "TaskConstructorCore: Did not expect to see cancelable token for continuation task.");
-                AssignCancellationToken(cancellationToken, null, null);
-            }
-        }
-        private void AssignCancellationToken(CancellationToken cancellationToken, Task? antecedent, TaskContinuation? continuation)
-        {
-            ContingentProperties props = EnsureContingentPropertiesInitializedUnsafe();
-            props.m_cancellationToken = cancellationToken;
-            try
-            {
-                if (((InternalTaskOptions)Options &
-                    (InternalTaskOptions.QueuedByRuntime | InternalTaskOptions.PromiseTask | InternalTaskOptions.LazyCancellation)) == 0)
-                {
-                    if (cancellationToken.IsCancellationRequested)
-                    {
-                        InternalCancel();
-                    }
-                    else
-                    {
-                        CancellationTokenRegistration ctr;
-                        if (antecedent == null)
-                        {
-                            ctr = cancellationToken.UnsafeRegister(static t => ((Task)t!).InternalCancel(), this);
-                        }
-                        else
-                        {
-                            Debug.Assert(continuation != null);
-                            ctr = cancellationToken.UnsafeRegister(static t =>
-                            {
-                                var tuple = (TupleSlim<Task, Task, TaskContinuation>)t!;
-                                Task targetTask = tuple.Item1;
-                                Task antecedentTask = tuple.Item2;
-                                antecedentTask.RemoveContinuation(tuple.Item3);
-                                targetTask.InternalCancel();
-                            }, new TupleSlim<Task, Task, TaskContinuation>(this, antecedent, continuation));
-                        }
-                        props.m_cancellationRegistration = new StrongBox<CancellationTokenRegistration>(ctr);
-                    }
-                }
-            }
-            catch
-            {
-                Task? parent = m_contingentProperties?.m_parent;
-                if ((parent != null) &&
-                    ((Options & TaskCreationOptions.AttachedToParent) != 0)
-                     && ((parent.Options & TaskCreationOptions.DenyChildAttach) == 0))
-                {
-                    parent.DisregardChild();
-                }
-                throw;
-            }
-        }
-        private string DebuggerDisplayMethodDescription => m_action?.Method.ToString() ?? "{null}";
-        internal TaskCreationOptions Options => OptionsMethod(m_stateFlags);
-        internal static TaskCreationOptions OptionsMethod(int flags)
-        {
-            Debug.Assert(((int)TaskStateFlags.OptionsMask & 1) == 1, "OptionsMask needs a shift in Options.get");
-            return (TaskCreationOptions)(flags & (int)TaskStateFlags.OptionsMask);
-        }
-        internal bool AtomicStateUpdate(int newBits, int illegalBits)
-        {
-            int oldFlags = m_stateFlags;
-            return
-                (oldFlags & illegalBits) == 0 &&
-                (Interlocked.CompareExchange(ref m_stateFlags, oldFlags | newBits, oldFlags) == oldFlags ||
-                 AtomicStateUpdateSlow(newBits, illegalBits));
-        }
-        private bool AtomicStateUpdateSlow(int newBits, int illegalBits)
-        {
-            int flags = m_stateFlags;
-            while (true)
-            {
-                if ((flags & illegalBits) != 0) return false;
-                int oldFlags = Interlocked.CompareExchange(ref m_stateFlags, flags | newBits, flags);
-                if (oldFlags == flags)
-                {
-                    return true;
-                }
-                flags = oldFlags;
-            }
-        }
-        internal bool AtomicStateUpdate(int newBits, int illegalBits, ref int oldFlags)
-        {
-            int flags = oldFlags = m_stateFlags;
-            while (true)
-            {
-                if ((flags & illegalBits) != 0) return false;
-                oldFlags = Interlocked.CompareExchange(ref m_stateFlags, flags | newBits, flags);
-                if (oldFlags == flags)
-                {
-                    return true;
-                }
-                flags = oldFlags;
-            }
-        }
-        internal void SetNotificationForWaitCompletion(bool enabled)
-        {
-            Debug.Assert((Options & (TaskCreationOptions)InternalTaskOptions.PromiseTask) != 0,
-                "Should only be used for promise-style tasks"); // hasn't been vetted on other kinds as there hasn't been a need
-            if (enabled)
-            {
-                bool success = AtomicStateUpdate((int)TaskStateFlags.WaitCompletionNotification,
-                                  (int)TaskStateFlags.CompletedMask | (int)TaskStateFlags.CompletionReserved);
-                Debug.Assert(success, "Tried to set enabled on completed Task");
-            }
-            else
-            {
-                Interlocked.And(ref m_stateFlags, ~(int)TaskStateFlags.WaitCompletionNotification);
-            }
-        }
-        internal bool NotifyDebuggerOfWaitCompletionIfNecessary()
-        {
-            if (IsWaitNotificationEnabled && ShouldNotifyDebuggerOfWaitCompletion)
-            {
-                NotifyDebuggerOfWaitCompletion();
-                return true;
-            }
-            return false;
-        }
-        internal static bool AnyTaskRequiresNotifyDebuggerOfWaitCompletion(Task?[] tasks)
-        {
-            Debug.Assert(tasks != null, "Expected non-null array of tasks");
-            foreach (Task? task in tasks)
-            {
-                if (task != null &&
-                    task.IsWaitNotificationEnabled &&
-                    task.ShouldNotifyDebuggerOfWaitCompletion) // potential recursion
-                {
-                    return true;
-                }
-            }
-            return false;
-        }
-        internal bool IsWaitNotificationEnabledOrNotRanToCompletion
-        {
-            [MethodImpl(MethodImplOptions.AggressiveInlining)]
-            get => (m_stateFlags & ((int)TaskStateFlags.WaitCompletionNotification | (int)TaskStateFlags.RanToCompletion))
-                        != (int)TaskStateFlags.RanToCompletion;
-        }
-        private protected virtual bool ShouldNotifyDebuggerOfWaitCompletion
-        {
-            get
-            {
-                bool isWaitNotificationEnabled = IsWaitNotificationEnabled;
-                Debug.Assert(isWaitNotificationEnabled, "Should only be called if the wait completion bit is set.");
-                return isWaitNotificationEnabled;
-            }
-        }
-        internal bool IsWaitNotificationEnabled => // internal only to enable unit tests; would otherwise be private
-            (m_stateFlags & (int)TaskStateFlags.WaitCompletionNotification) != 0;
-        [MethodImpl(MethodImplOptions.NoOptimization | MethodImplOptions.NoInlining)]
-        private void NotifyDebuggerOfWaitCompletion()
-        {
-            Debug.Assert(IsWaitNotificationEnabled, "Should only be called if the wait completion bit is set.");
-            SetNotificationForWaitCompletion(enabled: false);
-        }
-        internal bool MarkStarted()
-        {
-            return AtomicStateUpdate((int)TaskStateFlags.Started, (int)TaskStateFlags.Canceled | (int)TaskStateFlags.Started);
-        }
-        internal void FireTaskScheduledIfNeeded(TaskScheduler ts)
-        {
-            if ((m_stateFlags & (int)TaskStateFlags.TaskScheduledWasFired) == 0)
-            {
-                m_stateFlags |= (int)TaskStateFlags.TaskScheduledWasFired;
-                if (TplEventSource.Log.IsEnabled())
-                {
-                    Task? currentTask = InternalCurrent;
-                    Task? parentTask = m_contingentProperties?.m_parent;
-                    TplEventSource.Log.TaskScheduled(ts.Id, currentTask == null ? 0 : currentTask.Id,
-                                        this.Id, parentTask == null ? 0 : parentTask.Id, (int)this.Options);
-                }
-            }
-        }
-        internal void AddNewChild()
-        {
-            Debug.Assert(InternalCurrent == this, "Task.AddNewChild(): Called from an external context");
-            ContingentProperties props = EnsureContingentPropertiesInitialized();
-            if (props.m_completionCountdown == 1)
-            {
-                props.m_completionCountdown++;
-            }
-            else
-            {
-                Interlocked.Increment(ref props.m_completionCountdown);
-            }
-        }
-        internal void DisregardChild()
-        {
-            Debug.Assert(InternalCurrent == this, "Task.DisregardChild(): Called from an external context");
-            ContingentProperties props = EnsureContingentPropertiesInitialized();
-            Debug.Assert(props.m_completionCountdown >= 2, "Task.DisregardChild(): Expected parent count to be >= 2");
-            Interlocked.Decrement(ref props.m_completionCountdown);
-        }
-        public void Start()
-        {
-            Start(TaskScheduler.Current);
-        }
-        public void Start(TaskScheduler scheduler)
-        {
-            int flags = m_stateFlags;
-            if (IsCompletedMethod(flags))
-            {
-                ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_Start_TaskCompleted);
-            }
-            if (scheduler == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
-            }
-            TaskCreationOptions options = OptionsMethod(flags);
-            if ((options & (TaskCreationOptions)InternalTaskOptions.PromiseTask) != 0)
-            {
-                ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_Start_Promise);
-            }
-            if ((options & (TaskCreationOptions)InternalTaskOptions.ContinuationTask) != 0)
-            {
-                ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_Start_ContinuationTask);
-            }
-            if (Interlocked.CompareExchange(ref m_taskScheduler, scheduler, null) != null)
-            {
-                ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_Start_AlreadyStarted);
-            }
-            ScheduleAndStart(true);
-        }
-        public void RunSynchronously()
-        {
-            InternalRunSynchronously(TaskScheduler.Current, waitForCompletion: true);
-        }
-        public void RunSynchronously(TaskScheduler scheduler)
-        {
-            if (scheduler == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
-            }
-            InternalRunSynchronously(scheduler, waitForCompletion: true);
-        }
-        internal void InternalRunSynchronously(TaskScheduler scheduler, bool waitForCompletion)
-        {
-            Debug.Assert(scheduler != null, "Task.InternalRunSynchronously(): null TaskScheduler");
-            int flags = m_stateFlags;
-            TaskCreationOptions options = OptionsMethod(flags);
-            if ((options & (TaskCreationOptions)InternalTaskOptions.ContinuationTask) != 0)
-            {
-                ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_RunSynchronously_Continuation);
-            }
-            if ((options & (TaskCreationOptions)InternalTaskOptions.PromiseTask) != 0)
-            {
-                ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_RunSynchronously_Promise);
-            }
-            if (IsCompletedMethod(flags))
-            {
-                ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_RunSynchronously_TaskCompleted);
-            }
-            if (Interlocked.CompareExchange(ref m_taskScheduler, scheduler, null) != null)
-            {
-                ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_RunSynchronously_AlreadyStarted);
-            }
-            if (MarkStarted())
-            {
-                bool taskQueued = false;
-                try
-                {
-                    if (!scheduler.TryRunInline(this, false))
-                    {
-                        scheduler.InternalQueueTask(this);
-                        taskQueued = true; // only mark this after successfully queuing the task.
-                    }
-                    if (waitForCompletion && !IsCompleted)
-                    {
-                        SpinThenBlockingWait(Timeout.Infinite, default);
-                    }
-                }
-                catch (Exception e)
-                {
-                    if (!taskQueued)
-                    {
-                        TaskSchedulerException tse = new TaskSchedulerException(e);
-                        AddException(tse);
-                        Finish(false);
-                        Debug.Assert(
-                            (m_contingentProperties != null) &&
-                            (m_contingentProperties.m_exceptionsHolder != null) &&
-                            (m_contingentProperties.m_exceptionsHolder.ContainsFaultList),
-                            "Task.InternalRunSynchronously(): Expected m_contingentProperties.m_exceptionsHolder to exist " +
-                            "and to have faults recorded.");
-                        m_contingentProperties.m_exceptionsHolder.MarkAsHandled(false);
-                        throw tse;
-                    }
-                    else throw;
-                }
-            }
-            else
-            {
-                Debug.Assert((m_stateFlags & (int)TaskStateFlags.Canceled) != 0, "Task.RunSynchronously: expected TaskStateFlags.Canceled to be set");
-                ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_RunSynchronously_TaskCompleted);
-            }
-        }
-        internal static Task InternalStartNew(
-            Task? creatingTask, Delegate action, object? state, CancellationToken cancellationToken, TaskScheduler scheduler,
-            TaskCreationOptions options, InternalTaskOptions internalOptions)
-        {
-            if (scheduler == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
-            }
-            Task t = new Task(action, state, creatingTask, cancellationToken, options, internalOptions | InternalTaskOptions.QueuedByRuntime, scheduler);
-            t.ScheduleAndStart(false);
-            return t;
-        }
-        internal static int NewId()
-        {
-            int newId;
-            do
-            {
-                newId = Interlocked.Increment(ref s_taskIdCounter);
-            }
-            while (newId == 0);
-            if (TplEventSource.Log.IsEnabled())
-                TplEventSource.Log.NewID(newId);
-            return newId;
-        }
-        public int Id
-        {
-            get
-            {
-                if (Volatile.Read(ref m_taskId) == 0)
-                {
-                    int newId = NewId();
-                    Interlocked.CompareExchange(ref m_taskId, newId, 0);
-                }
-                return m_taskId;
-            }
-        }
-        public static int? CurrentId
-        {
-            get
-            {
-                Task? currentTask = InternalCurrent;
-                if (currentTask != null)
-                    return currentTask.Id;
-                else
-                    return null;
-            }
-        }
-        internal static Task? InternalCurrent => t_currentTask;
-        internal static Task? InternalCurrentIfAttached(TaskCreationOptions creationOptions)
-        {
-            return (creationOptions & TaskCreationOptions.AttachedToParent) != 0 ? InternalCurrent : null;
-        }
-        public AggregateException? Exception
-        {
-            get
-            {
-                AggregateException? e = null;
-                if (IsFaulted) e = GetExceptions(false);
-                Debug.Assert((e == null) || IsFaulted, "Task.Exception_get(): returning non-null value when not Faulted");
-                return e;
-            }
-        }
-        public TaskStatus Status
-        {
-            get
-            {
-                TaskStatus rval;
-                int sf = m_stateFlags;
-                if ((sf & (int)TaskStateFlags.Faulted) != 0) rval = TaskStatus.Faulted;
-                else if ((sf & (int)TaskStateFlags.Canceled) != 0) rval = TaskStatus.Canceled;
-                else if ((sf & (int)TaskStateFlags.RanToCompletion) != 0) rval = TaskStatus.RanToCompletion;
-                else if ((sf & (int)TaskStateFlags.WaitingOnChildren) != 0) rval = TaskStatus.WaitingForChildrenToComplete;
-                else if ((sf & (int)TaskStateFlags.DelegateInvoked) != 0) rval = TaskStatus.Running;
-                else if ((sf & (int)TaskStateFlags.Started) != 0) rval = TaskStatus.WaitingToRun;
-                else if ((sf & (int)TaskStateFlags.WaitingForActivation) != 0) rval = TaskStatus.WaitingForActivation;
-                else rval = TaskStatus.Created;
-                return rval;
-            }
-        }
-        public bool IsCanceled =>
-                (m_stateFlags & ((int)TaskStateFlags.Canceled | (int)TaskStateFlags.Faulted)) == (int)TaskStateFlags.Canceled;
-        internal bool IsCancellationRequested
-        {
-            get
-            {
-                ContingentProperties? props = Volatile.Read(ref m_contingentProperties);
-                return props != null &&
-                    (props.m_internalCancellationRequested == CANCELLATION_REQUESTED ||
-                     props.m_cancellationToken.IsCancellationRequested);
-            }
-        }
-        internal ContingentProperties EnsureContingentPropertiesInitialized()
-        {
-            return Volatile.Read(ref m_contingentProperties) ?? InitializeContingentProperties();
-            ContingentProperties InitializeContingentProperties()
-            {
-                Interlocked.CompareExchange(ref m_contingentProperties, new ContingentProperties(), null);
-                return m_contingentProperties;
-            }
-        }
-        internal ContingentProperties EnsureContingentPropertiesInitializedUnsafe() =>
-            m_contingentProperties ??= new ContingentProperties();
-        internal CancellationToken CancellationToken
-        {
-            get
-            {
-                ContingentProperties? props = Volatile.Read(ref m_contingentProperties);
-                return (props == null) ? default : props.m_cancellationToken;
-            }
-        }
-        internal bool IsCancellationAcknowledged => (m_stateFlags & (int)TaskStateFlags.CancellationAcknowledged) != 0;
-        public bool IsCompleted
-        {
-            get
-            {
-                int stateFlags = m_stateFlags; // enable inlining of IsCompletedMethod by "cast"ing away the volatility
-                return IsCompletedMethod(stateFlags);
-            }
-        }
-        private static bool IsCompletedMethod(int flags)
-        {
-            return (flags & (int)TaskStateFlags.CompletedMask) != 0;
-        }
-        public bool IsCompletedSuccessfully => (m_stateFlags & (int)TaskStateFlags.CompletedMask) == (int)TaskStateFlags.RanToCompletion;
-        public TaskCreationOptions CreationOptions => Options & (TaskCreationOptions)(~InternalTaskOptions.InternalOptionsMask);
-        internal void SpinUntilCompleted()
-        {
-            SpinWait sw = default;
-            while (!IsCompleted)
-            {
-                sw.SpinOnce();
-            }
-        }
-        WaitHandle IAsyncResult.AsyncWaitHandle
-        {
-            get
-            {
-                bool isDisposed = (m_stateFlags & (int)TaskStateFlags.Disposed) != 0;
-                if (isDisposed)
-                {
-                    ThrowHelper.ThrowObjectDisposedException(ExceptionResource.Task_ThrowIfDisposed);
-                }
-                return CompletedEvent.WaitHandle;
-            }
-        }
-        public object? AsyncState => (m_stateFlags & (int)InternalTaskOptions.HiddenState) == 0 ? m_stateObject : null;
-        bool IAsyncResult.CompletedSynchronously => false;
-        internal TaskScheduler? ExecutingTaskScheduler => m_taskScheduler;
-        public static TaskFactory Factory { get; } = new TaskFactory();
-        internal static readonly Task<VoidTaskResult> s_cachedCompleted = new Task<VoidTaskResult>(false, default, (TaskCreationOptions)InternalTaskOptions.DoNotDispose, default);
-        public static Task CompletedTask => s_cachedCompleted;
-        internal ManualResetEventSlim CompletedEvent
-        {
-            get
-            {
-                ContingentProperties contingentProps = EnsureContingentPropertiesInitialized();
-                if (contingentProps.m_completionEvent == null)
-                {
-                    bool wasCompleted = IsCompleted;
-                    ManualResetEventSlim newMre = new ManualResetEventSlim(wasCompleted);
-                    if (Interlocked.CompareExchange(ref contingentProps.m_completionEvent, newMre, null) != null)
-                    {
-                        newMre.Dispose();
-                    }
-                    else if (!wasCompleted && IsCompleted)
-                    {
-                        ContingentProperties.SetEvent(newMre);
-                    }
-                }
-                return contingentProps.m_completionEvent;
-            }
-        }
-        internal bool ExceptionRecorded
-        {
-            get
-            {
-                ContingentProperties? props = Volatile.Read(ref m_contingentProperties);
-                return (props != null) && (props.m_exceptionsHolder != null) && (props.m_exceptionsHolder.ContainsFaultList);
-            }
-        }
-        [MemberNotNullWhen(true, nameof(Exception))]
-        public bool IsFaulted =>
-            (m_stateFlags & (int)TaskStateFlags.Faulted) != 0;
-        internal ExecutionContext? CapturedContext
-        {
-            get
-            {
-                if ((m_stateFlags & (int)TaskStateFlags.ExecutionContextIsNull) == (int)TaskStateFlags.ExecutionContextIsNull)
-                {
-                    return null;
-                }
-                else
-                {
-                    return m_contingentProperties?.m_capturedContext ?? ExecutionContext.Default;
-                }
-            }
-            set
-            {
-                if (value == null)
-                {
-                    m_stateFlags |= (int)TaskStateFlags.ExecutionContextIsNull;
-                }
-                else if (value != ExecutionContext.Default) // not the default context, then inflate the contingent properties and set it
-                {
-                    EnsureContingentPropertiesInitializedUnsafe().m_capturedContext = value;
-                }
-            }
-        }
-        public void Dispose()
-        {
-            Dispose(true);
-            GC.SuppressFinalize(this);
-        }
-        protected virtual void Dispose(bool disposing)
-        {
-            if (disposing)
-            {
-                if ((Options & (TaskCreationOptions)InternalTaskOptions.DoNotDispose) != 0)
-                {
-                    return;
-                }
-                if (!IsCompleted)
-                {
-                    ThrowHelper.ThrowInvalidOperationException(ExceptionResource.Task_Dispose_NotCompleted);
-                }
-                ContingentProperties? cp = Volatile.Read(ref m_contingentProperties);
-                if (cp != null)
-                {
-                    ManualResetEventSlim? ev = cp.m_completionEvent;
-                    if (ev != null)
-                    {
-                        cp.m_completionEvent = null;
-                        ContingentProperties.SetEvent(ev);
-                        ev.Dispose();
-                    }
-                }
-            }
-            m_stateFlags |= (int)TaskStateFlags.Disposed;
-        }
-        internal void ScheduleAndStart(bool needsProtection)
-        {
-            Debug.Assert(m_taskScheduler != null, "expected a task scheduler to have been selected");
-            Debug.Assert((m_stateFlags & (int)TaskStateFlags.Started) == 0, "task has already started");
-            if (needsProtection)
-            {
-                if (!MarkStarted())
-                {
-                    return;
-                }
-            }
-            else
-            {
-                m_stateFlags |= (int)TaskStateFlags.Started;
-            }
-            if (s_asyncDebuggingEnabled)
-                AddToActiveTasks(this);
-            if (TplEventSource.Log.IsEnabled() && (Options & (TaskCreationOptions)InternalTaskOptions.ContinuationTask) == 0)
-            {
-                Debug.Assert(m_action != null, "Must have a delegate to be in ScheduleAndStart");
-                TplEventSource.Log.TraceOperationBegin(this.Id, "Task: " + m_action.Method.Name, 0);
-            }
-            try
-            {
-                m_taskScheduler.InternalQueueTask(this);
-            }
-            catch (Exception e)
-            {
-                TaskSchedulerException tse = new TaskSchedulerException(e);
-                AddException(tse);
-                Finish(false);
-                if ((Options & (TaskCreationOptions)InternalTaskOptions.ContinuationTask) == 0)
-                {
-                    Debug.Assert(
-                        (m_contingentProperties != null) &&
-                        (m_contingentProperties.m_exceptionsHolder != null) &&
-                        (m_contingentProperties.m_exceptionsHolder.ContainsFaultList),
-                            "Task.ScheduleAndStart(): Expected m_contingentProperties.m_exceptionsHolder to exist " +
-                            "and to have faults recorded.");
-                    m_contingentProperties.m_exceptionsHolder.MarkAsHandled(false);
-                }
-                throw tse;
-            }
-        }
-        internal void AddException(object exceptionObject)
-        {
-            Debug.Assert(exceptionObject != null, "Task.AddException: Expected a non-null exception object");
-            AddException(exceptionObject, representsCancellation: false);
-        }
-        internal void AddException(object exceptionObject, bool representsCancellation)
-        {
-            Debug.Assert(exceptionObject != null, "Task.AddException: Expected a non-null exception object");
-#if DEBUG
-            var eoAsException = exceptionObject as Exception;
-            var eoAsEnumerableException = exceptionObject as IEnumerable<Exception>;
-            var eoAsEdi = exceptionObject as ExceptionDispatchInfo;
-            var eoAsEnumerableEdi = exceptionObject as IEnumerable<ExceptionDispatchInfo>;
-            Debug.Assert(
-                eoAsException != null || eoAsEnumerableException != null || eoAsEdi != null || eoAsEnumerableEdi != null,
-                "Task.AddException: Expected an Exception, ExceptionDispatchInfo, or an IEnumerable<> of one of those");
-            var eoAsOce = exceptionObject as OperationCanceledException;
-            Debug.Assert(
-                !representsCancellation ||
-                eoAsOce != null ||
-                (eoAsEdi != null && eoAsEdi.SourceException is OperationCanceledException),
-                "representsCancellation should be true only if an OCE was provided.");
-#endif
-            ContingentProperties props = EnsureContingentPropertiesInitialized();
-            if (props.m_exceptionsHolder == null)
-            {
-                TaskExceptionHolder holder = new TaskExceptionHolder(this);
-                if (Interlocked.CompareExchange(ref props.m_exceptionsHolder, holder, null) != null)
-                {
-                    holder.MarkAsHandled(false);
-                }
-            }
-            lock (props)
-            {
-                props.m_exceptionsHolder.Add(exceptionObject, representsCancellation);
-            }
-        }
-        private AggregateException? GetExceptions(bool includeTaskCanceledExceptions)
-        {
-            Exception? canceledException = null;
-            if (includeTaskCanceledExceptions && IsCanceled)
-            {
-                canceledException = new TaskCanceledException(this);
-                canceledException.SetCurrentStackTrace();
-            }
-            if (ExceptionRecorded)
-            {
-                Debug.Assert(m_contingentProperties != null && m_contingentProperties.m_exceptionsHolder != null, "ExceptionRecorded should imply this");
-                return m_contingentProperties.m_exceptionsHolder.CreateExceptionObject(false, canceledException);
-            }
-            else if (canceledException != null)
-            {
-                return new AggregateException(canceledException);
-            }
-            return null;
-        }
-        internal List<ExceptionDispatchInfo> GetExceptionDispatchInfos()
-        {
-            Debug.Assert(IsFaulted && ExceptionRecorded, "Must only be used when the task has faulted with exceptions.");
-            return m_contingentProperties!.m_exceptionsHolder!.GetExceptionDispatchInfos();
-        }
-        internal ExceptionDispatchInfo? GetCancellationExceptionDispatchInfo()
-        {
-            Debug.Assert(IsCanceled, "Must only be used when the task has canceled.");
-            return Volatile.Read(ref m_contingentProperties)?.m_exceptionsHolder?.GetCancellationExceptionDispatchInfo(); // may be null
-        }
-        internal void MarkExceptionsAsHandled()
-        {
-            Volatile.Read(ref m_contingentProperties)?.m_exceptionsHolder?.MarkAsHandled(calledFromFinalizer: false);
-        }
-        internal void ThrowIfExceptional(bool includeTaskCanceledExceptions)
-        {
-            Debug.Assert(IsCompleted, "ThrowIfExceptional(): Expected IsCompleted == true");
-            Exception? exception = GetExceptions(includeTaskCanceledExceptions);
-            if (exception != null)
-            {
-                UpdateExceptionObservedStatus();
-                throw exception;
-            }
-        }
-        internal static void ThrowAsync(Exception exception, SynchronizationContext? targetContext)
-        {
-            var edi = ExceptionDispatchInfo.Capture(exception);
-            if (targetContext != null)
-            {
-                try
-                {
-                    targetContext.Post(static state => ((ExceptionDispatchInfo)state!).Throw(), edi);
-                    return;
-                }
-                catch (Exception postException)
-                {
-                    edi = ExceptionDispatchInfo.Capture(new AggregateException(exception, postException));
-                }
-            }
-#if NATIVEAOT
-            RuntimeExceptionHelpers.ReportUnhandledException(edi.SourceException);
-#else
-            ThreadPool.QueueUserWorkItem(static state => ((ExceptionDispatchInfo)state!).Throw(), edi);
-#endif // NATIVEAOT
-        }
-        internal void UpdateExceptionObservedStatus()
-        {
-            Task? parent = m_contingentProperties?.m_parent;
-            if ((parent != null)
-                && ((Options & TaskCreationOptions.AttachedToParent) != 0)
-                && ((parent.CreationOptions & TaskCreationOptions.DenyChildAttach) == 0)
-                && InternalCurrent == parent)
-            {
-                m_stateFlags |= (int)TaskStateFlags.ExceptionObservedByParent;
-            }
-        }
-        internal bool IsExceptionObservedByParent => (m_stateFlags & (int)TaskStateFlags.ExceptionObservedByParent) != 0;
-        internal bool IsDelegateInvoked => (m_stateFlags & (int)TaskStateFlags.DelegateInvoked) != 0;
-        internal void Finish(bool userDelegateExecute)
-        {
-            if (m_contingentProperties == null)
-            {
-                FinishStageTwo();
-            }
-            else
-            {
-                FinishSlow(userDelegateExecute);
-            }
-        }
-        private void FinishSlow(bool userDelegateExecute)
-        {
-            Debug.Assert(userDelegateExecute || m_contingentProperties != null);
-            if (!userDelegateExecute)
-            {
-                FinishStageTwo();
-            }
-            else
-            {
-                ContingentProperties props = m_contingentProperties!;
-                if ((props.m_completionCountdown == 1) ||
-                    Interlocked.Decrement(ref props.m_completionCountdown) == 0) // Reaching this sub clause means there may be remaining active children,
-                {
-                    FinishStageTwo();
-                }
-                else
-                {
-                    AtomicStateUpdate((int)TaskStateFlags.WaitingOnChildren, (int)TaskStateFlags.Faulted | (int)TaskStateFlags.Canceled | (int)TaskStateFlags.RanToCompletion);
-                }
-                List<Task>? exceptionalChildren = props.m_exceptionalChildren;
-                if (exceptionalChildren != null)
-                {
-                    lock (exceptionalChildren)
-                    {
-                        exceptionalChildren.RemoveAll(t => t.IsExceptionObservedByParent); // RemoveAll has better performance than doing it ourselves
-                    }
-                }
-            }
-        }
-        private void FinishStageTwo()
-        {
-            ContingentProperties? cp = Volatile.Read(ref m_contingentProperties);
-            if (cp != null)
-            {
-                AddExceptionsFromChildren(cp);
-            }
-            int completionState;
-            if (ExceptionRecorded)
-            {
-                completionState = (int)TaskStateFlags.Faulted;
-                if (TplEventSource.Log.IsEnabled())
-                    TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Error);
-                if (s_asyncDebuggingEnabled)
-                    RemoveFromActiveTasks(this);
-            }
-            else if (IsCancellationRequested && IsCancellationAcknowledged)
-            {
-                completionState = (int)TaskStateFlags.Canceled;
-                if (TplEventSource.Log.IsEnabled())
-                    TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Canceled);
-                if (s_asyncDebuggingEnabled)
-                    RemoveFromActiveTasks(this);
-            }
-            else
-            {
-                completionState = (int)TaskStateFlags.RanToCompletion;
-                if (TplEventSource.Log.IsEnabled())
-                    TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Completed);
-                if (s_asyncDebuggingEnabled)
-                    RemoveFromActiveTasks(this);
-            }
-            Interlocked.Exchange(ref m_stateFlags, m_stateFlags | completionState);
-            cp = Volatile.Read(ref m_contingentProperties); // need to re-read after updating state
-            if (cp != null)
-            {
-                cp.SetCompleted();
-                cp.UnregisterCancellationCallback();
-            }
-            FinishStageThree();
-        }
-        internal void FinishStageThree()
-        {
-            m_action = null;
-            ContingentProperties? cp = m_contingentProperties;
-            if (cp != null)
-            {
-                cp.m_capturedContext = null;
-                NotifyParentIfPotentiallyAttachedTask();
-            }
-            FinishContinuations();
-        }
-        internal void NotifyParentIfPotentiallyAttachedTask()
-        {
-            Task? parent = m_contingentProperties?.m_parent;
-            if (parent != null
-                 && ((parent.CreationOptions & TaskCreationOptions.DenyChildAttach) == 0)
-                 && (((TaskCreationOptions)(m_stateFlags & (int)TaskStateFlags.OptionsMask)) & TaskCreationOptions.AttachedToParent) != 0)
-            {
-                parent.ProcessChildCompletion(this);
-            }
-        }
-        internal void ProcessChildCompletion(Task childTask)
-        {
-            Debug.Assert(childTask != null);
-            Debug.Assert(childTask.IsCompleted, "ProcessChildCompletion was called for an uncompleted task");
-            Debug.Assert(childTask.m_contingentProperties?.m_parent == this, "ProcessChildCompletion should only be called for a child of this task");
-            ContingentProperties? props = Volatile.Read(ref m_contingentProperties);
-            if (childTask.IsFaulted && !childTask.IsExceptionObservedByParent)
-            {
-                if (props!.m_exceptionalChildren == null)
-                {
-                    Interlocked.CompareExchange(ref props.m_exceptionalChildren, new List<Task>(), null);
-                }
-                List<Task>? tmp = props.m_exceptionalChildren;
-                if (tmp != null)
-                {
-                    lock (tmp)
-                    {
-                        tmp.Add(childTask);
-                    }
-                }
-            }
-            if (Interlocked.Decrement(ref props!.m_completionCountdown) == 0)
-            {
-                FinishStageTwo();
-            }
-        }
-        internal void AddExceptionsFromChildren(ContingentProperties props)
-        {
-            Debug.Assert(props != null);
-            List<Task>? exceptionalChildren = props.m_exceptionalChildren;
-            if (exceptionalChildren != null)
-            {
-                lock (exceptionalChildren)
-                {
-                    foreach (Task task in exceptionalChildren)
-                    {
-                        Debug.Assert(task.IsCompleted, "Expected all tasks in list to be completed");
-                        if (task.IsFaulted && !task.IsExceptionObservedByParent)
-                        {
-                            TaskExceptionHolder? exceptionHolder = Volatile.Read(ref task.m_contingentProperties)!.m_exceptionsHolder;
-                            Debug.Assert(exceptionHolder != null);
-                            AddException(exceptionHolder.CreateExceptionObject(false, null));
-                        }
-                    }
-                }
-                props.m_exceptionalChildren = null;
-            }
-        }
-        internal bool ExecuteEntry()
-        {
-            int previousState = 0;
-            if (!AtomicStateUpdate((int)TaskStateFlags.DelegateInvoked,
-                                    (int)TaskStateFlags.DelegateInvoked | (int)TaskStateFlags.CompletedMask,
-                                    ref previousState) && (previousState & (int)TaskStateFlags.Canceled) == 0)
-            {
-                return false;
-            }
-            if (!IsCancellationRequested & !IsCanceled)
-            {
-                ExecuteWithThreadLocal(ref t_currentTask);
-            }
-            else
-            {
-                ExecuteEntryCancellationRequestedOrCanceled();
-            }
-            return true;
-        }
-        internal virtual void ExecuteFromThreadPool(Thread threadPoolThread) => ExecuteEntryUnsafe(threadPoolThread);
-        internal void ExecuteEntryUnsafe(Thread? threadPoolThread) // used instead of ExecuteEntry() when we don't have to worry about double-execution prevent
-        {
-            m_stateFlags |= (int)TaskStateFlags.DelegateInvoked;
-            if (!IsCancellationRequested & !IsCanceled)
-            {
-                ExecuteWithThreadLocal(ref t_currentTask, threadPoolThread);
-            }
-            else
-            {
-                ExecuteEntryCancellationRequestedOrCanceled();
-            }
-        }
-        internal void ExecuteEntryCancellationRequestedOrCanceled()
-        {
-            if (!IsCanceled)
-            {
-                int prevState = Interlocked.Exchange(ref m_stateFlags, m_stateFlags | (int)TaskStateFlags.Canceled);
-                if ((prevState & (int)TaskStateFlags.Canceled) == 0)
-                {
-                    CancellationCleanupLogic();
-                }
-            }
-        }
-        private void ExecuteWithThreadLocal(ref Task? currentTaskSlot, Thread? threadPoolThread = null)
-        {
-            Task? previousTask = currentTaskSlot;
-            TplEventSource log = TplEventSource.Log;
-            Guid savedActivityID = default;
-            bool etwIsEnabled = log.IsEnabled();
-            if (etwIsEnabled)
-            {
-                if (log.TasksSetActivityIds)
-                    EventSource.SetCurrentThreadActivityId(TplEventSource.CreateGuidForTaskID(this.Id), out savedActivityID);
-                if (previousTask != null)
-                    log.TaskStarted(previousTask.m_taskScheduler!.Id, previousTask.Id, this.Id);
-                else
-                    log.TaskStarted(TaskScheduler.Current.Id, 0, this.Id);
-                log.TraceSynchronousWorkBegin(this.Id, CausalitySynchronousWork.Execution);
-            }
-            try
-            {
-                currentTaskSlot = this;
-                try
-                {
-                    ExecutionContext? ec = CapturedContext;
-                    if (ec == null)
-                    {
-                        InnerInvoke();
-                    }
-                    else
-                    {
-                        if (threadPoolThread is null)
-                        {
-                            ExecutionContext.RunInternal(ec, s_ecCallback, this);
-                        }
-                        else
-                        {
-                            ExecutionContext.RunFromThreadPoolDispatchLoop(threadPoolThread, ec, s_ecCallback, this);
-                        }
-                    }
-                }
-                catch (Exception exn)
-                {
-                    HandleException(exn);
-                }
-                if (etwIsEnabled)
-                    log.TraceSynchronousWorkEnd(CausalitySynchronousWork.Execution);
-                Finish(true);
-            }
-            finally
-            {
-                currentTaskSlot = previousTask;
-                if (etwIsEnabled)
-                {
-                    if (previousTask != null)
-                        log.TaskCompleted(previousTask.m_taskScheduler!.Id, previousTask.Id, this.Id, IsFaulted);
-                    else
-                        log.TaskCompleted(TaskScheduler.Current.Id, 0, this.Id, IsFaulted);
-                    if (log.TasksSetActivityIds)
-                        EventSource.SetCurrentThreadActivityId(savedActivityID);
-                }
-            }
-        }
-        private static readonly ContextCallback s_ecCallback = obj =>
-        {
-            Debug.Assert(obj is Task);
-            Unsafe.As<Task>(obj).InnerInvoke();
-        };
-        internal virtual void InnerInvoke()
-        {
-            Debug.Assert(m_action != null, "Null action in InnerInvoke()");
-            if (m_action is Action action)
-            {
-                action();
-                return;
-            }
-            if (m_action is Action<object?> actionWithState)
-            {
-                actionWithState(m_stateObject);
-                return;
-            }
-            Debug.Fail("Invalid m_action in Task");
-        }
-        private void HandleException(Exception unhandledException)
-        {
-            Debug.Assert(unhandledException != null);
-            if (unhandledException is OperationCanceledException exceptionAsOce && IsCancellationRequested &&
-                m_contingentProperties!.m_cancellationToken == exceptionAsOce.CancellationToken)
-            {
-                SetCancellationAcknowledged();
-                AddException(exceptionAsOce, representsCancellation: true);
-            }
-            else
-            {
-                AddException(unhandledException);
-            }
-        }
-        #region Await Support
-        public TaskAwaiter GetAwaiter()
-        {
-            return new TaskAwaiter(this);
-        }
-        public ConfiguredTaskAwaitable ConfigureAwait(bool continueOnCapturedContext)
-        {
-            return new ConfiguredTaskAwaitable(this, continueOnCapturedContext ? ConfigureAwaitOptions.ContinueOnCapturedContext : ConfigureAwaitOptions.None);
-        }
-        public ConfiguredTaskAwaitable ConfigureAwait(ConfigureAwaitOptions options)
-        {
-            if ((options & ~(ConfigureAwaitOptions.ContinueOnCapturedContext |
-                             ConfigureAwaitOptions.SuppressThrowing |
-                             ConfigureAwaitOptions.ForceYielding)) != 0)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.options);
-            }
-            return new ConfiguredTaskAwaitable(this, options);
-        }
-        internal void SetContinuationForAwait(
-            Action continuationAction, bool continueOnCapturedContext, bool flowExecutionContext)
-        {
-            Debug.Assert(continuationAction != null);
-            TaskContinuation? tc;
-            if (continueOnCapturedContext)
-            {
-                if (SynchronizationContext.Current is SynchronizationContext syncCtx && syncCtx.GetType() != typeof(SynchronizationContext))
-                {
-                    tc = new SynchronizationContextAwaitTaskContinuation(syncCtx, continuationAction, flowExecutionContext);
-                    goto HaveTaskContinuation;
-                }
-                if (TaskScheduler.InternalCurrent is TaskScheduler scheduler && scheduler != TaskScheduler.Default)
-                {
-                    tc = new TaskSchedulerAwaitTaskContinuation(scheduler, continuationAction, flowExecutionContext);
-                    goto HaveTaskContinuation;
-                }
-            }
-            if (flowExecutionContext)
-            {
-                tc = new AwaitTaskContinuation(continuationAction, flowExecutionContext: true);
-                goto HaveTaskContinuation;
-            }
-            Debug.Assert(!flowExecutionContext, "We already determined we're not required to flow context.");
-            if (!AddTaskContinuation(continuationAction, addBeforeOthers: false))
-            {
-                AwaitTaskContinuation.UnsafeScheduleAction(continuationAction, this);
-            }
-            return;
-            HaveTaskContinuation:
-            if (!AddTaskContinuation(tc, addBeforeOthers: false))
-            {
-                tc.Run(this, canInlineContinuationTask: false);
-            }
-        }
-        internal void UnsafeSetContinuationForAwait(IAsyncStateMachineBox stateMachineBox, bool continueOnCapturedContext)
-        {
-            Debug.Assert(stateMachineBox != null);
-            TaskContinuation? tc;
-            if (continueOnCapturedContext)
-            {
-                if (SynchronizationContext.Current is SynchronizationContext syncCtx && syncCtx.GetType() != typeof(SynchronizationContext))
-                {
-                    tc = new SynchronizationContextAwaitTaskContinuation(syncCtx, stateMachineBox.MoveNextAction, flowExecutionContext: false);
-                    goto HaveTaskContinuation;
-                }
-                if (TaskScheduler.InternalCurrent is TaskScheduler scheduler && scheduler != TaskScheduler.Default)
-                {
-                    tc = new TaskSchedulerAwaitTaskContinuation(scheduler, stateMachineBox.MoveNextAction, flowExecutionContext: false);
-                    goto HaveTaskContinuation;
-                }
-            }
-            if (!AddTaskContinuation(stateMachineBox, addBeforeOthers: false))
-            {
-                ThreadPool.UnsafeQueueUserWorkItemInternal(stateMachineBox, preferLocal: true);
-            }
-            return;
-            HaveTaskContinuation:
-            if (!AddTaskContinuation(tc, addBeforeOthers: false))
-            {
-                tc.Run(this, canInlineContinuationTask: false);
-            }
-        }
-        public static YieldAwaitable Yield()
-        {
-            return default;
-        }
-        #endregion
-        public void Wait()
-        {
-#if DEBUG
-            bool waitResult =
-#endif
-            Wait(Timeout.Infinite, default);
-#if DEBUG
-            Debug.Assert(waitResult, "expected wait to succeed");
-#endif
-        }
-        public bool Wait(TimeSpan timeout) => Wait(timeout, default);
-        public bool Wait(TimeSpan timeout, CancellationToken cancellationToken)
-        {
-            long totalMilliseconds = (long)timeout.TotalMilliseconds;
-            if (totalMilliseconds < -1 || totalMilliseconds > int.MaxValue)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.timeout);
-            }
-            cancellationToken.ThrowIfCancellationRequested();
-            return Wait((int)totalMilliseconds, cancellationToken);
-        }
-        public void Wait(CancellationToken cancellationToken)
-        {
-            Wait(Timeout.Infinite, cancellationToken);
-        }
-        public bool Wait(int millisecondsTimeout)
-        {
-            return Wait(millisecondsTimeout, default);
-        }
-        public bool Wait(int millisecondsTimeout, CancellationToken cancellationToken)
-        {
-            if (millisecondsTimeout < -1)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.millisecondsTimeout);
-            }
-            if (!IsWaitNotificationEnabledOrNotRanToCompletion) // (!DebuggerBitSet && RanToCompletion)
-                return true;
-            if (!InternalWait(millisecondsTimeout, cancellationToken))
-                return false;
-            if (IsWaitNotificationEnabledOrNotRanToCompletion) // avoid a few unnecessary volatile reads if we completed successfully
-            {
-                NotifyDebuggerOfWaitCompletionIfNecessary();
-                if (IsCanceled) cancellationToken.ThrowIfCancellationRequested();
-                ThrowIfExceptional(true);
-            }
-            Debug.Assert((m_stateFlags & (int)TaskStateFlags.Faulted) == 0, "Task.Wait() completing when in Faulted state.");
-            return true;
-        }
-        public Task WaitAsync(CancellationToken cancellationToken) => WaitAsync(Timeout.UnsignedInfinite, TimeProvider.System, cancellationToken);
-        public Task WaitAsync(TimeSpan timeout) => WaitAsync(ValidateTimeout(timeout, ExceptionArgument.timeout), TimeProvider.System, default);
-        public Task WaitAsync(TimeSpan timeout, TimeProvider timeProvider)
-        {
-            ArgumentNullException.ThrowIfNull(timeProvider);
-            return WaitAsync(ValidateTimeout(timeout, ExceptionArgument.timeout), timeProvider, default);
-        }
-        public Task WaitAsync(TimeSpan timeout, CancellationToken cancellationToken) =>
-            WaitAsync(ValidateTimeout(timeout, ExceptionArgument.timeout), TimeProvider.System, cancellationToken);
-        public Task WaitAsync(TimeSpan timeout, TimeProvider timeProvider, CancellationToken cancellationToken)
-        {
-            ArgumentNullException.ThrowIfNull(timeProvider);
-            return WaitAsync(ValidateTimeout(timeout, ExceptionArgument.timeout), timeProvider, cancellationToken);
-        }
-        private Task WaitAsync(uint millisecondsTimeout, TimeProvider timeProvider, CancellationToken cancellationToken)
-        {
-            if (IsCompleted || (!cancellationToken.CanBeCanceled && millisecondsTimeout == Timeout.UnsignedInfinite))
-            {
-                return this;
-            }
-            if (cancellationToken.IsCancellationRequested)
-            {
-                return FromCanceled(cancellationToken);
-            }
-            if (millisecondsTimeout == 0)
-            {
-                return FromException(new TimeoutException());
-            }
-            return new CancellationPromise<VoidTaskResult>(this, millisecondsTimeout, timeProvider, cancellationToken);
-        }
-        private protected sealed class CancellationPromise<TResult> : Task<TResult>, ITaskCompletionAction
-        {
-            private readonly Task _task;
-            private readonly CancellationTokenRegistration _registration;
-            private readonly ITimer? _timer;
-            internal CancellationPromise(Task source, uint millisecondsDelay, TimeProvider timeProvider, CancellationToken token)
-            {
-                Debug.Assert(source != null);
-                Debug.Assert(millisecondsDelay != 0);
-                _task = source;
-                source.AddCompletionAction(this);
-                if (millisecondsDelay != Timeout.UnsignedInfinite)
-                {
-                    TimerCallback callback = static state =>
-                    {
-                        var thisRef = (CancellationPromise<TResult>)state!;
-                        if (thisRef.TrySetException(new TimeoutException()))
-                        {
-                            thisRef.Cleanup();
-                        }
-                    };
-                    if (timeProvider == TimeProvider.System)
-                    {
-                        _timer = new TimerQueueTimer(callback, this, millisecondsDelay, Timeout.UnsignedInfinite, flowExecutionContext: false);
-                    }
-                    else
-                    {
-                        using (ExecutionContext.SuppressFlow())
-                        {
-                            _timer = timeProvider.CreateTimer(callback, this, TimeSpan.FromMilliseconds(millisecondsDelay), Timeout.InfiniteTimeSpan);
-                        }
-                    }
-                }
-                _registration = token.UnsafeRegister(static (state, cancellationToken) =>
-                {
-                    var thisRef = (CancellationPromise<TResult>)state!;
-                    if (thisRef.TrySetCanceled(cancellationToken))
-                    {
-                        thisRef.Cleanup();
-                    }
-                }, this);
-                if (IsCompleted)
-                {
-                    Cleanup();
-                }
-            }
-            bool ITaskCompletionAction.InvokeMayRunArbitraryCode => true;
-            void ITaskCompletionAction.Invoke(Task completingTask)
-            {
-                Debug.Assert(completingTask.IsCompleted);
-                bool set = completingTask.Status switch
-                {
-                    TaskStatus.Canceled => TrySetCanceled(completingTask.CancellationToken, completingTask.GetCancellationExceptionDispatchInfo()),
-                    TaskStatus.Faulted => TrySetException(completingTask.GetExceptionDispatchInfos()),
-                    _ => completingTask is Task<TResult> taskTResult ? TrySetResult(taskTResult.Result) : TrySetResult(),
-                };
-                if (set)
-                {
-                    Cleanup();
-                }
-            }
-            private void Cleanup()
-            {
-                _registration.Dispose();
-                _timer?.Dispose();
-                _task.RemoveContinuation(this);
-            }
-        }
-        private bool WrappedTryRunInline()
-        {
-            if (m_taskScheduler == null)
-                return false;
-            try
-            {
-                return m_taskScheduler.TryRunInline(this, true);
-            }
-            catch (Exception e)
-            {
-                throw new TaskSchedulerException(e);
-            }
-        }
-        [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
-        internal bool InternalWait(int millisecondsTimeout, CancellationToken cancellationToken) =>
-            InternalWaitCore(millisecondsTimeout, cancellationToken);
-        private bool InternalWaitCore(int millisecondsTimeout, CancellationToken cancellationToken)
-        {
-            bool returnValue = IsCompleted;
-            if (returnValue)
-            {
-                return true;
-            }
-            TplEventSource log = TplEventSource.Log;
-            bool etwIsEnabled = log.IsEnabled();
-            if (etwIsEnabled)
-            {
-                Task? currentTask = InternalCurrent;
-                log.TaskWaitBegin(
-                    currentTask != null ? currentTask.m_taskScheduler!.Id : TaskScheduler.Default.Id, currentTask != null ? currentTask.Id : 0,
-                    this.Id, TplEventSource.TaskWaitBehavior.Synchronous, 0);
-            }
-            Debugger.NotifyOfCrossThreadDependency();
-            if (millisecondsTimeout == Timeout.Infinite && !cancellationToken.CanBeCanceled &&
-                WrappedTryRunInline() && IsCompleted) // TryRunInline doesn't guarantee completion, as there may be unfinished children.
-            {
-                returnValue = true;
-            }
-            else
-            {
-                returnValue = SpinThenBlockingWait(millisecondsTimeout, cancellationToken);
-            }
-            Debug.Assert(IsCompleted || millisecondsTimeout != Timeout.Infinite);
-            if (etwIsEnabled)
-            {
-                Task? currentTask = InternalCurrent;
-                if (currentTask != null)
-                {
-                    log.TaskWaitEnd(currentTask.m_taskScheduler!.Id, currentTask.Id, this.Id);
-                }
-                else
-                {
-                    log.TaskWaitEnd(TaskScheduler.Default.Id, 0, this.Id);
-                }
-                log.TaskWaitContinuationComplete(this.Id);
-            }
-            return returnValue;
-        }
-        private sealed class SetOnInvokeMres : ManualResetEventSlim, ITaskCompletionAction
-        {
-            internal SetOnInvokeMres() : base(false, 0) { }
-            public void Invoke(Task completingTask) { Set(); }
-            public bool InvokeMayRunArbitraryCode => false;
-        }
-        private bool SpinThenBlockingWait(int millisecondsTimeout, CancellationToken cancellationToken)
-        {
-            bool infiniteWait = millisecondsTimeout == Timeout.Infinite;
-            uint startTimeTicks = infiniteWait ? 0 : (uint)Environment.TickCount;
-            bool returnValue = SpinWait(millisecondsTimeout);
-            if (!returnValue)
-            {
-#if CORECLR
-                if (ThreadPoolWorkQueue.s_prioritizationExperiment)
-                {
-                    ThreadPoolWorkQueue.TransferAllLocalWorkItemsToHighPriorityGlobalQueue();
-                }
-#endif
-                var mres = new SetOnInvokeMres();
-                try
-                {
-                    AddCompletionAction(mres, addBeforeOthers: true);
-#pragma warning disable CA1416 // Validate platform compatibility, issue: https://github.com/dotnet/runtime/issues/44622
-                    if (infiniteWait)
-                    {
-                        bool notifyWhenUnblocked = ThreadPool.NotifyThreadBlocked();
-                        try
-                        {
-                            returnValue = mres.Wait(Timeout.Infinite, cancellationToken);
-                        }
-                        finally
-                        {
-                            if (notifyWhenUnblocked)
-                            {
-                                ThreadPool.NotifyThreadUnblocked();
-                            }
-                        }
-                    }
-                    else
-                    {
-                        uint elapsedTimeTicks = ((uint)Environment.TickCount) - startTimeTicks;
-                        if (elapsedTimeTicks < millisecondsTimeout)
-                        {
-                            bool notifyWhenUnblocked = ThreadPool.NotifyThreadBlocked();
-                            try
-                            {
-                                returnValue = mres.Wait((int)(millisecondsTimeout - elapsedTimeTicks), cancellationToken);
-                            }
-                            finally
-                            {
-                                if (notifyWhenUnblocked)
-                                {
-                                    ThreadPool.NotifyThreadUnblocked();
-                                }
-                            }
-                        }
-                    }
-#pragma warning restore CA1416
-                }
-                finally
-                {
-                    if (!IsCompleted) RemoveContinuation(mres);
-                }
-            }
-            return returnValue;
-        }
-        private bool SpinWait(int millisecondsTimeout)
-        {
-            if (IsCompleted) return true;
-            if (millisecondsTimeout == 0)
-            {
-                return false;
-            }
-            int spinCount = Threading.SpinWait.SpinCountforSpinBeforeWait;
-            SpinWait spinner = default;
-            while (spinner.Count < spinCount)
-            {
-                spinner.SpinOnce(sleep1Threshold: -1);
-                if (IsCompleted)
-                {
-                    return true;
-                }
-            }
-            return false;
-        }
-        internal void InternalCancel()
-        {
-            Debug.Assert((Options & (TaskCreationOptions)InternalTaskOptions.PromiseTask) == 0, "Task.InternalCancel() did not expect promise-style task");
-            TaskSchedulerException? tse = null;
-            bool popped = false;
-            if ((m_stateFlags & (int)TaskStateFlags.Started) != 0)
-            {
-                TaskScheduler? ts = m_taskScheduler;
-                try
-                {
-                    popped = (ts != null) && ts.TryDequeue(this);
-                }
-                catch (Exception e)
-                {
-                    tse = new TaskSchedulerException(e);
-                }
-            }
-            RecordInternalCancellationRequest();
-            bool mustCleanup = false;
-            if (popped)
-            {
-                mustCleanup = AtomicStateUpdate((int)TaskStateFlags.Canceled, (int)TaskStateFlags.Canceled | (int)TaskStateFlags.DelegateInvoked);
-            }
-            else if ((m_stateFlags & (int)TaskStateFlags.Started) == 0)
-            {
-                mustCleanup = AtomicStateUpdate((int)TaskStateFlags.Canceled,
-                    (int)TaskStateFlags.Canceled | (int)TaskStateFlags.Started | (int)TaskStateFlags.RanToCompletion |
-                    (int)TaskStateFlags.Faulted | (int)TaskStateFlags.DelegateInvoked);
-            }
-            if (mustCleanup)
-            {
-                CancellationCleanupLogic();
-            }
-            if (tse != null)
-            {
-                throw tse;
-            }
-        }
-        internal void InternalCancelContinueWithInitialState()
-        {
-            const int IllegalFlags = (int)TaskStateFlags.Started | (int)TaskStateFlags.CompletedMask | (int)TaskStateFlags.DelegateInvoked;
-            Debug.Assert((m_stateFlags & IllegalFlags) == 0, "The continuation was in an invalid state.");
-            Debug.Assert((m_stateFlags & (int)TaskStateFlags.WaitingForActivation) != 0, "Expected continuation to be waiting for activation");
-            Debug.Assert(m_contingentProperties is null || m_contingentProperties.m_cancellationToken == default);
-            m_stateFlags |= (int)TaskStateFlags.Canceled; // no synchronization necessary, per above comment
-            CancellationCleanupLogic();
-        }
-        internal void RecordInternalCancellationRequest()
-        {
-            EnsureContingentPropertiesInitialized().m_internalCancellationRequested = CANCELLATION_REQUESTED;
-        }
-        internal void RecordInternalCancellationRequest(CancellationToken tokenToRecord, object? cancellationException)
-        {
-            Debug.Assert((Options & (TaskCreationOptions)InternalTaskOptions.PromiseTask) != 0, "Task.RecordInternalCancellationRequest(CancellationToken) only valid for promise-style task");
-            RecordInternalCancellationRequest();
-            Debug.Assert(m_contingentProperties!.m_cancellationToken == default);
-            if (tokenToRecord != default)
-            {
-                m_contingentProperties.m_cancellationToken = tokenToRecord;
-            }
-            if (cancellationException != null)
-            {
-#if DEBUG
-                var oce = cancellationException as OperationCanceledException;
-                if (oce == null)
-                {
-                    var edi = cancellationException as ExceptionDispatchInfo;
-                    Debug.Assert(edi != null, "Expected either an OCE or an EDI");
-                    oce = edi.SourceException as OperationCanceledException;
-                    Debug.Assert(oce != null, "Expected EDI to contain an OCE");
-                }
-                Debug.Assert(oce.CancellationToken == tokenToRecord,
-                                "Expected OCE's token to match the provided token.");
-#endif
-                AddException(cancellationException, representsCancellation: true);
-            }
-        }
-        internal void CancellationCleanupLogic()
-        {
-            Debug.Assert((m_stateFlags & ((int)TaskStateFlags.Canceled | (int)TaskStateFlags.CompletionReserved)) != 0, "Task.CancellationCleanupLogic(): Task not canceled or reserved.");
-            Interlocked.Exchange(ref m_stateFlags, m_stateFlags | (int)TaskStateFlags.Canceled);
-            ContingentProperties? cp = Volatile.Read(ref m_contingentProperties);
-            if (cp != null)
-            {
-                cp.SetCompleted();
-                cp.UnregisterCancellationCallback();
-            }
-            if (TplEventSource.Log.IsEnabled())
-                TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Canceled);
-            if (s_asyncDebuggingEnabled)
-                RemoveFromActiveTasks(this);
-            FinishStageThree();
-        }
-        private void SetCancellationAcknowledged()
-        {
-            Debug.Assert(this == InternalCurrent, "SetCancellationAcknowledged() should only be called while this is still the current task");
-            Debug.Assert(IsCancellationRequested, "SetCancellationAcknowledged() should not be called if the task's CT wasn't signaled");
-            m_stateFlags |= (int)TaskStateFlags.CancellationAcknowledged;
-        }
-        internal bool TrySetResult()
-        {
-            if (AtomicStateUpdate(
-                (int)TaskStateFlags.CompletionReserved | (int)TaskStateFlags.RanToCompletion,
-                (int)TaskStateFlags.CompletionReserved | (int)TaskStateFlags.RanToCompletion | (int)TaskStateFlags.Faulted | (int)TaskStateFlags.Canceled))
-            {
-                ContingentProperties? props = m_contingentProperties;
-                if (props != null)
-                {
-                    NotifyParentIfPotentiallyAttachedTask();
-                    props.SetCompleted();
-                }
-                FinishContinuations();
-                return true;
-            }
-            return false;
-        }
-        internal bool TrySetException(object exceptionObject)
-        {
-            Debug.Assert(exceptionObject != null, "Expected non-null exceptionObject argument");
-            Debug.Assert(
-                (exceptionObject is Exception) || (exceptionObject is IEnumerable<Exception>) ||
-                (exceptionObject is ExceptionDispatchInfo) || (exceptionObject is IEnumerable<ExceptionDispatchInfo>),
-                "Expected exceptionObject to be either Exception, ExceptionDispatchInfo, or IEnumerable<> of one of those");
-            bool returnValue = false;
-            EnsureContingentPropertiesInitialized();
-            if (AtomicStateUpdate(
-                (int)TaskStateFlags.CompletionReserved,
-                (int)TaskStateFlags.CompletionReserved | (int)TaskStateFlags.RanToCompletion | (int)TaskStateFlags.Faulted | (int)TaskStateFlags.Canceled))
-            {
-                AddException(exceptionObject); // handles singleton exception or exception collection
-                Finish(false);
-                returnValue = true;
-            }
-            return returnValue;
-        }
-        internal bool TrySetCanceled(CancellationToken tokenToRecord)
-        {
-            return TrySetCanceled(tokenToRecord, null);
-        }
-        internal bool TrySetCanceled(CancellationToken tokenToRecord, object? cancellationException)
-        {
-            Debug.Assert(
-                cancellationException == null ||
-                cancellationException is OperationCanceledException ||
-                (cancellationException as ExceptionDispatchInfo)?.SourceException is OperationCanceledException,
-                "Expected null or an OperationCanceledException");
-            bool returnValue = false;
-            if (AtomicStateUpdate(
-                (int)TaskStateFlags.CompletionReserved,
-                (int)TaskStateFlags.CompletionReserved | (int)TaskStateFlags.Canceled | (int)TaskStateFlags.Faulted | (int)TaskStateFlags.RanToCompletion))
-            {
-                RecordInternalCancellationRequest(tokenToRecord, cancellationException);
-                CancellationCleanupLogic(); // perform cancellation cleanup actions
-                returnValue = true;
-            }
-            return returnValue;
-        }
-        internal void FinishContinuations()
-        {
-            object? continuationObject = Interlocked.Exchange(ref m_continuationObject, s_taskCompletionSentinel);
-            if (continuationObject != null)
-            {
-                RunContinuations(continuationObject);
-            }
-        }
-        private void RunContinuations(object continuationObject) // separated out of FinishContinuations to enable it to be inlined
-        {
-            Debug.Assert(continuationObject != null);
-            TplEventSource log = TplEventSource.Log;
-            bool etwIsEnabled = log.IsEnabled();
-            if (etwIsEnabled)
-                log.TraceSynchronousWorkBegin(this.Id, CausalitySynchronousWork.CompletionNotification);
-            bool canInlineContinuations =
-                (m_stateFlags & (int)TaskCreationOptions.RunContinuationsAsynchronously) == 0 &&
-                RuntimeHelpers.TryEnsureSufficientExecutionStack();
-            switch (continuationObject)
-            {
-                case IAsyncStateMachineBox stateMachineBox:
-                    AwaitTaskContinuation.RunOrScheduleAction(stateMachineBox, canInlineContinuations);
-                    LogFinishCompletionNotification();
-                    return;
-                case Action action:
-                    AwaitTaskContinuation.RunOrScheduleAction(action, canInlineContinuations);
-                    LogFinishCompletionNotification();
-                    return;
-                case TaskContinuation tc:
-                    tc.Run(this, canInlineContinuations);
-                    LogFinishCompletionNotification();
-                    return;
-                case ITaskCompletionAction completionAction:
-                    RunOrQueueCompletionAction(completionAction, canInlineContinuations);
-                    LogFinishCompletionNotification();
-                    return;
-            }
-            List<object?> continuations = (List<object?>)continuationObject;
-            lock (continuations) { }
-            int continuationCount = continuations.Count;
-            if (canInlineContinuations)
-            {
-                bool forceContinuationsAsync = false;
-                for (int i = 0; i < continuationCount; i++)
-                {
-                    object? currentContinuation = continuations[i];
-                    if (currentContinuation == null)
-                    {
-                        continue;
-                    }
-                    else if (currentContinuation is ContinueWithTaskContinuation stc)
-                    {
-                        if ((stc.m_options & TaskContinuationOptions.ExecuteSynchronously) == 0)
-                        {
-                            continuations[i] = null; // so that we can skip this later
-                            if (etwIsEnabled)
-                                log.RunningContinuationList(Id, i, stc);
-                            stc.Run(this, canInlineContinuationTask: false);
-                        }
-                    }
-                    else if (!(currentContinuation is ITaskCompletionAction))
-                    {
-                        if (forceContinuationsAsync)
-                        {
-                            continuations[i] = null;
-                            if (etwIsEnabled)
-                                log.RunningContinuationList(Id, i, currentContinuation);
-                            switch (currentContinuation)
-                            {
-                                case IAsyncStateMachineBox stateMachineBox:
-                                    AwaitTaskContinuation.RunOrScheduleAction(stateMachineBox, allowInlining: false);
-                                    break;
-                                case Action action:
-                                    AwaitTaskContinuation.RunOrScheduleAction(action, allowInlining: false);
-                                    break;
-                                default:
-                                    Debug.Assert(currentContinuation is TaskContinuation);
-                                    ((TaskContinuation)currentContinuation).Run(this, canInlineContinuationTask: false);
-                                    break;
-                            }
-                        }
-                        forceContinuationsAsync = true;
-                    }
-                }
-            }
-            for (int i = 0; i < continuationCount; i++)
-            {
-                object? currentContinuation = continuations[i];
-                if (currentContinuation == null)
-                {
-                    continue;
-                }
-                continuations[i] = null; // to enable free'ing up memory earlier
-                if (etwIsEnabled)
-                   log.RunningContinuationList(Id, i, currentContinuation);
-                switch (currentContinuation)
-                {
-                    case IAsyncStateMachineBox stateMachineBox:
-                        AwaitTaskContinuation.RunOrScheduleAction(stateMachineBox, canInlineContinuations);
-                        break;
-                    case Action action:
-                        AwaitTaskContinuation.RunOrScheduleAction(action, canInlineContinuations);
-                        break;
-                    case TaskContinuation tc:
-                        tc.Run(this, canInlineContinuations);
-                        break;
-                    default:
-                        Debug.Assert(currentContinuation is ITaskCompletionAction);
-                        RunOrQueueCompletionAction((ITaskCompletionAction)currentContinuation, canInlineContinuations);
-                        break;
-                }
-            }
-            LogFinishCompletionNotification();
-        }
-        private void RunOrQueueCompletionAction(ITaskCompletionAction completionAction, bool allowInlining)
-        {
-            if (allowInlining || !completionAction.InvokeMayRunArbitraryCode)
-            {
-                completionAction.Invoke(this);
-            }
-            else
-            {
-                ThreadPool.UnsafeQueueUserWorkItemInternal(new CompletionActionInvoker(completionAction, this), preferLocal: true);
-            }
-        }
-        private static void LogFinishCompletionNotification()
-        {
-            if (TplEventSource.Log.IsEnabled())
-                TplEventSource.Log.TraceSynchronousWorkEnd(CausalitySynchronousWork.CompletionNotification);
-        }
-        #region Continuation methods
-        #region Action<Task> continuation
-        public Task ContinueWith(Action<Task> continuationAction)
-        {
-            return ContinueWith(continuationAction, TaskScheduler.Current, default, TaskContinuationOptions.None);
-        }
-        public Task ContinueWith(Action<Task> continuationAction, CancellationToken cancellationToken)
-        {
-            return ContinueWith(continuationAction, TaskScheduler.Current, cancellationToken, TaskContinuationOptions.None);
-        }
-        public Task ContinueWith(Action<Task> continuationAction, TaskScheduler scheduler)
-        {
-            return ContinueWith(continuationAction, scheduler, default, TaskContinuationOptions.None);
-        }
-        public Task ContinueWith(Action<Task> continuationAction, TaskContinuationOptions continuationOptions)
-        {
-            return ContinueWith(continuationAction, TaskScheduler.Current, default, continuationOptions);
-        }
-        public Task ContinueWith(Action<Task> continuationAction, CancellationToken cancellationToken,
-                                 TaskContinuationOptions continuationOptions, TaskScheduler scheduler)
-        {
-            return ContinueWith(continuationAction, scheduler, cancellationToken, continuationOptions);
-        }
-        private Task ContinueWith(Action<Task> continuationAction, TaskScheduler scheduler,
-            CancellationToken cancellationToken, TaskContinuationOptions continuationOptions)
-        {
-            if (continuationAction == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.continuationAction);
-            }
-            if (scheduler == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
-            }
-            CreationOptionsFromContinuationOptions(continuationOptions, out TaskCreationOptions creationOptions, out InternalTaskOptions internalOptions);
-            Task continuationTask = new ContinuationTaskFromTask(
-                this, continuationAction, null,
-                creationOptions, internalOptions
-            );
-            ContinueWithCore(continuationTask, scheduler, cancellationToken, continuationOptions);
-            return continuationTask;
-        }
-        #endregion
-        #region Action<Task, Object> continuation
-        public Task ContinueWith(Action<Task, object?> continuationAction, object? state)
-        {
-            return ContinueWith(continuationAction, state, TaskScheduler.Current, default, TaskContinuationOptions.None);
-        }
-        public Task ContinueWith(Action<Task, object?> continuationAction, object? state, CancellationToken cancellationToken)
-        {
-            return ContinueWith(continuationAction, state, TaskScheduler.Current, cancellationToken, TaskContinuationOptions.None);
-        }
-        public Task ContinueWith(Action<Task, object?> continuationAction, object? state, TaskScheduler scheduler)
-        {
-            return ContinueWith(continuationAction, state, scheduler, default, TaskContinuationOptions.None);
-        }
-        public Task ContinueWith(Action<Task, object?> continuationAction, object? state, TaskContinuationOptions continuationOptions)
-        {
-            return ContinueWith(continuationAction, state, TaskScheduler.Current, default, continuationOptions);
-        }
-        public Task ContinueWith(Action<Task, object?> continuationAction, object? state, CancellationToken cancellationToken,
-                                 TaskContinuationOptions continuationOptions, TaskScheduler scheduler)
-        {
-            return ContinueWith(continuationAction, state, scheduler, cancellationToken, continuationOptions);
-        }
-        private Task ContinueWith(Action<Task, object?> continuationAction, object? state, TaskScheduler scheduler,
-            CancellationToken cancellationToken, TaskContinuationOptions continuationOptions)
-        {
-            if (continuationAction == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.continuationAction);
-            }
-            if (scheduler == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
-            }
-            CreationOptionsFromContinuationOptions(continuationOptions, out TaskCreationOptions creationOptions, out InternalTaskOptions internalOptions);
-            Task continuationTask = new ContinuationTaskFromTask(
-                this, continuationAction, state,
-                creationOptions, internalOptions
-            );
-            ContinueWithCore(continuationTask, scheduler, cancellationToken, continuationOptions);
-            return continuationTask;
-        }
-        #endregion
-        #region Func<Task, TResult> continuation
-        public Task<TResult> ContinueWith<TResult>(Func<Task, TResult> continuationFunction)
-        {
-            return ContinueWith(continuationFunction, TaskScheduler.Current, default,
-                TaskContinuationOptions.None);
-        }
-        public Task<TResult> ContinueWith<TResult>(Func<Task, TResult> continuationFunction, CancellationToken cancellationToken)
-        {
-            return ContinueWith(continuationFunction, TaskScheduler.Current, cancellationToken, TaskContinuationOptions.None);
-        }
-        public Task<TResult> ContinueWith<TResult>(Func<Task, TResult> continuationFunction, TaskScheduler scheduler)
-        {
-            return ContinueWith(continuationFunction, scheduler, default, TaskContinuationOptions.None);
-        }
-        public Task<TResult> ContinueWith<TResult>(Func<Task, TResult> continuationFunction, TaskContinuationOptions continuationOptions)
-        {
-            return ContinueWith(continuationFunction, TaskScheduler.Current, default, continuationOptions);
-        }
-        public Task<TResult> ContinueWith<TResult>(Func<Task, TResult> continuationFunction, CancellationToken cancellationToken,
-                                                   TaskContinuationOptions continuationOptions, TaskScheduler scheduler)
-        {
-            return ContinueWith(continuationFunction, scheduler, cancellationToken, continuationOptions);
-        }
-        private Task<TResult> ContinueWith<TResult>(Func<Task, TResult> continuationFunction, TaskScheduler scheduler,
-            CancellationToken cancellationToken, TaskContinuationOptions continuationOptions)
-        {
-            if (continuationFunction == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.continuationFunction);
-            }
-            if (scheduler == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
-            }
-            CreationOptionsFromContinuationOptions(continuationOptions, out TaskCreationOptions creationOptions, out InternalTaskOptions internalOptions);
-            Task<TResult> continuationTask = new ContinuationResultTaskFromTask<TResult>(
-                this, continuationFunction, null,
-                creationOptions, internalOptions
-            );
-            ContinueWithCore(continuationTask, scheduler, cancellationToken, continuationOptions);
-            return continuationTask;
-        }
-        #endregion
-        #region Func<Task, Object, TResult> continuation
-        public Task<TResult> ContinueWith<TResult>(Func<Task, object?, TResult> continuationFunction, object? state)
-        {
-            return ContinueWith(continuationFunction, state, TaskScheduler.Current, default,
-                TaskContinuationOptions.None);
-        }
-        public Task<TResult> ContinueWith<TResult>(Func<Task, object?, TResult> continuationFunction, object? state, CancellationToken cancellationToken)
-        {
-            return ContinueWith(continuationFunction, state, TaskScheduler.Current, cancellationToken, TaskContinuationOptions.None);
-        }
-        public Task<TResult> ContinueWith<TResult>(Func<Task, object?, TResult> continuationFunction, object? state, TaskScheduler scheduler)
-        {
-            return ContinueWith(continuationFunction, state, scheduler, default, TaskContinuationOptions.None);
-        }
-        public Task<TResult> ContinueWith<TResult>(Func<Task, object?, TResult> continuationFunction, object? state, TaskContinuationOptions continuationOptions)
-        {
-            return ContinueWith(continuationFunction, state, TaskScheduler.Current, default, continuationOptions);
-        }
-        public Task<TResult> ContinueWith<TResult>(Func<Task, object?, TResult> continuationFunction, object? state, CancellationToken cancellationToken,
-                                                   TaskContinuationOptions continuationOptions, TaskScheduler scheduler)
-        {
-            return ContinueWith(continuationFunction, state, scheduler, cancellationToken, continuationOptions);
-        }
-        private Task<TResult> ContinueWith<TResult>(Func<Task, object?, TResult> continuationFunction, object? state, TaskScheduler scheduler,
-            CancellationToken cancellationToken, TaskContinuationOptions continuationOptions)
-        {
-            if (continuationFunction == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.continuationFunction);
-            }
-            if (scheduler == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.scheduler);
-            }
-            CreationOptionsFromContinuationOptions(continuationOptions, out TaskCreationOptions creationOptions, out InternalTaskOptions internalOptions);
-            Task<TResult> continuationTask = new ContinuationResultTaskFromTask<TResult>(
-                this, continuationFunction, state,
-                creationOptions, internalOptions
-            );
-            ContinueWithCore(continuationTask, scheduler, cancellationToken, continuationOptions);
-            return continuationTask;
-        }
-        #endregion
-        internal static void CreationOptionsFromContinuationOptions(
-            TaskContinuationOptions continuationOptions,
-            out TaskCreationOptions creationOptions,
-            out InternalTaskOptions internalOptions)
-        {
-            const TaskContinuationOptions NotOnAnything =
-                TaskContinuationOptions.NotOnCanceled |
-                TaskContinuationOptions.NotOnFaulted |
-                TaskContinuationOptions.NotOnRanToCompletion;
-            const TaskContinuationOptions CreationOptionsMask =
-                TaskContinuationOptions.PreferFairness |
-                TaskContinuationOptions.LongRunning |
-                TaskContinuationOptions.DenyChildAttach |
-                TaskContinuationOptions.HideScheduler |
-                TaskContinuationOptions.AttachedToParent |
-                TaskContinuationOptions.RunContinuationsAsynchronously;
-            const TaskContinuationOptions IllegalMask = TaskContinuationOptions.ExecuteSynchronously | TaskContinuationOptions.LongRunning;
-            if ((continuationOptions & IllegalMask) == IllegalMask)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.continuationOptions, ExceptionResource.Task_ContinueWith_ESandLR);
-            }
-            if ((continuationOptions &
-                ~(CreationOptionsMask | NotOnAnything |
-                    TaskContinuationOptions.LazyCancellation | TaskContinuationOptions.ExecuteSynchronously)) != 0)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.continuationOptions);
-            }
-            if ((continuationOptions & NotOnAnything) == NotOnAnything)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.continuationOptions, ExceptionResource.Task_ContinueWith_NotOnAnything);
-            }
-            creationOptions = (TaskCreationOptions)(continuationOptions & CreationOptionsMask);
-            internalOptions = (continuationOptions & TaskContinuationOptions.LazyCancellation) != 0 ?
-                InternalTaskOptions.ContinuationTask | InternalTaskOptions.LazyCancellation :
-                InternalTaskOptions.ContinuationTask;
-        }
-        internal void ContinueWithCore(Task continuationTask,
-                                       TaskScheduler scheduler,
-                                       CancellationToken cancellationToken,
-                                       TaskContinuationOptions options)
-        {
-            Debug.Assert(continuationTask != null, "Task.ContinueWithCore(): null continuationTask");
-            Debug.Assert(scheduler != null, "Task.ContinueWithCore(): null scheduler");
-            Debug.Assert(!continuationTask.IsCompleted, "Did not expect continuationTask to be completed");
-            var continuation = new ContinueWithTaskContinuation(continuationTask, options, scheduler);
-            if (cancellationToken.CanBeCanceled)
-            {
-                if (IsCompleted || cancellationToken.IsCancellationRequested)
-                {
-                    continuationTask.AssignCancellationToken(cancellationToken, null, null);
-                }
-                else
-                {
-                    continuationTask.AssignCancellationToken(cancellationToken, this, continuation);
-                }
-            }
-            if (!continuationTask.IsCompleted)
-            {
-                if ((this.Options & (TaskCreationOptions)InternalTaskOptions.PromiseTask) != 0 &&
-                    !(this is ITaskCompletionAction))
-                {
-                    TplEventSource log = TplEventSource.Log;
-                    if (log.IsEnabled())
-                    {
-                        log.AwaitTaskContinuationScheduled(TaskScheduler.Current.Id, CurrentId ?? 0, continuationTask.Id);
-                    }
-                }
-                bool continuationQueued = AddTaskContinuation(continuation, addBeforeOthers: false);
-                if (!continuationQueued) continuation.Run(this, canInlineContinuationTask: true);
-            }
-        }
-        #endregion
-        internal void AddCompletionAction(ITaskCompletionAction action, bool addBeforeOthers = false)
-        {
-            if (!AddTaskContinuation(action, addBeforeOthers))
-                action.Invoke(this); // run the action directly if we failed to queue the continuation (i.e., the task completed)
-        }
-        private bool AddTaskContinuationComplex(object tc, bool addBeforeOthers)
-        {
-            Debug.Assert(tc != null, "Expected non-null tc object in AddTaskContinuationComplex");
-            object? oldValue = m_continuationObject;
-            if ((oldValue != s_taskCompletionSentinel) && (!(oldValue is List<object?>)))
-            {
-                Interlocked.CompareExchange(ref m_continuationObject, new List<object?> { oldValue }, oldValue);
-            }
-            List<object?>? list = m_continuationObject as List<object?>;
-            Debug.Assert((list != null) || (m_continuationObject == s_taskCompletionSentinel),
-                "Expected m_continuationObject to be list or sentinel");
-            if (list != null)
-            {
-                lock (list)
-                {
-                    if (m_continuationObject != s_taskCompletionSentinel)
-                    {
-                        if (list.Count == list.Capacity)
-                        {
-                            list.RemoveAll(l => l == null);
-                        }
-                        if (addBeforeOthers)
-                            list.Insert(0, tc);
-                        else
-                            list.Add(tc);
-                        return true; // continuation successfully queued, so return true.
-                    }
-                }
-            }
-            return false;
-        }
-        private bool AddTaskContinuation(object tc, bool addBeforeOthers)
-        {
-            Debug.Assert(tc != null);
-            if (IsCompleted) return false;
-            if ((m_continuationObject != null) || (Interlocked.CompareExchange(ref m_continuationObject, tc, null) != null))
-            {
-                return AddTaskContinuationComplex(tc, addBeforeOthers);
-            }
-            else return true;
-        }
-        internal void RemoveContinuation(object continuationObject) // could be TaskContinuation or Action<Task>
-        {
-            object? continuationsLocalRef = m_continuationObject;
-            if (continuationsLocalRef == s_taskCompletionSentinel) return;
-            List<object?>? continuationsLocalListRef = continuationsLocalRef as List<object?>;
-            if (continuationsLocalListRef is null)
-            {
-                if (Interlocked.CompareExchange(ref m_continuationObject, new List<object?>(), continuationObject) != continuationObject)
-                {
-                    continuationsLocalListRef = m_continuationObject as List<object?>;
-                }
-                else
-                {
-                    return;
-                }
-            }
-            if (continuationsLocalListRef != null)
-            {
-                lock (continuationsLocalListRef)
-                {
-                    if (m_continuationObject == s_taskCompletionSentinel) return;
-                    int index = continuationsLocalListRef.IndexOf(continuationObject);
-                    if (index >= 0)
-                    {
-                        continuationsLocalListRef[index] = null;
-                    }
-                }
-            }
-        }
-        [UnsupportedOSPlatform("browser")]
-        [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
-        public static void WaitAll(params Task[] tasks)
-        {
-#if DEBUG
-            bool waitResult =
-#endif
-            WaitAllCore(tasks, Timeout.Infinite, default);
-#if DEBUG
-            Debug.Assert(waitResult, "expected wait to succeed");
-#endif
-        }
-        [UnsupportedOSPlatform("browser")]
-        [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
-        public static bool WaitAll(Task[] tasks, TimeSpan timeout)
-        {
-            long totalMilliseconds = (long)timeout.TotalMilliseconds;
-            if (totalMilliseconds < -1 || totalMilliseconds > int.MaxValue)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.timeout);
-            }
-            return WaitAllCore(tasks, (int)totalMilliseconds, default);
-        }
-        [UnsupportedOSPlatform("browser")]
-        [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
-        public static bool WaitAll(Task[] tasks, int millisecondsTimeout)
-        {
-            return WaitAllCore(tasks, millisecondsTimeout, default);
-        }
-        [UnsupportedOSPlatform("browser")]
-        [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
-        public static void WaitAll(Task[] tasks, CancellationToken cancellationToken)
-        {
-            WaitAllCore(tasks, Timeout.Infinite, cancellationToken);
-        }
-        [UnsupportedOSPlatform("browser")]
-        [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
-        public static bool WaitAll(Task[] tasks, int millisecondsTimeout, CancellationToken cancellationToken) =>
-            WaitAllCore(tasks, millisecondsTimeout, cancellationToken);
-        [UnsupportedOSPlatform("browser")]
-        private static bool WaitAllCore(Task[] tasks, int millisecondsTimeout, CancellationToken cancellationToken)
-        {
-            if (tasks == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
-            }
-            if (millisecondsTimeout < -1)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.millisecondsTimeout);
-            }
-            cancellationToken.ThrowIfCancellationRequested(); // early check before we make any allocations
-            List<Exception>? exceptions = null;
-            List<Task>? waitedOnTaskList = null;
-            List<Task>? notificationTasks = null;
-            bool exceptionSeen = false, cancellationSeen = false;
-            bool returnValue = true;
-            for (int i = tasks.Length - 1; i >= 0; i--)
-            {
-                Task task = tasks[i];
-                if (task == null)
-                {
-                    ThrowHelper.ThrowArgumentException(ExceptionResource.Task_WaitMulti_NullTask, ExceptionArgument.tasks);
-                }
-                bool taskIsCompleted = task.IsCompleted;
-                if (!taskIsCompleted)
-                {
-                    if (millisecondsTimeout != Timeout.Infinite || cancellationToken.CanBeCanceled)
-                    {
-                        AddToList(task, ref waitedOnTaskList, initSize: tasks.Length);
-                    }
-                    else
-                    {
-                        taskIsCompleted = task.WrappedTryRunInline() && task.IsCompleted; // A successful TryRunInline doesn't guarantee completion
-                        if (!taskIsCompleted) AddToList(task, ref waitedOnTaskList, initSize: tasks.Length);
-                    }
-                }
-                if (taskIsCompleted)
-                {
-                    if (task.IsFaulted) exceptionSeen = true;
-                    else if (task.IsCanceled) cancellationSeen = true;
-                    if (task.IsWaitNotificationEnabled) AddToList(task, ref notificationTasks, initSize: 1);
-                }
-            }
-            if (waitedOnTaskList != null)
-            {
-                returnValue = WaitAllBlockingCore(waitedOnTaskList, millisecondsTimeout, cancellationToken);
-                if (returnValue)
-                {
-                    foreach (Task task in waitedOnTaskList)
-                    {
-                        if (task.IsFaulted) exceptionSeen = true;
-                        else if (task.IsCanceled) cancellationSeen = true;
-                        if (task.IsWaitNotificationEnabled) AddToList(task, ref notificationTasks, initSize: 1);
-                    }
-                }
-                GC.KeepAlive(tasks);
-            }
-            if (returnValue && notificationTasks != null)
-            {
-                foreach (Task task in notificationTasks)
-                {
-                    if (task.NotifyDebuggerOfWaitCompletionIfNecessary()) break;
-                }
-            }
-            if (returnValue && (exceptionSeen || cancellationSeen))
-            {
-                if (!exceptionSeen) cancellationToken.ThrowIfCancellationRequested();
-                foreach (Task task in tasks) AddExceptionsForCompletedTask(ref exceptions, task);
-                Debug.Assert(exceptions != null, "Should have seen at least one exception");
-                ThrowHelper.ThrowAggregateException(exceptions);
-            }
-            return returnValue;
-        }
-        private static void AddToList<T>(T item, ref List<T>? list, int initSize)
-        {
-            list ??= new List<T>(initSize);
-            list.Add(item);
-        }
-        [UnsupportedOSPlatform("browser")]
-        private static bool WaitAllBlockingCore(List<Task> tasks, int millisecondsTimeout, CancellationToken cancellationToken)
-        {
-            Debug.Assert(tasks != null, "Expected a non-null list of tasks");
-            Debug.Assert(tasks.Count > 0, "Expected at least one task");
-            bool waitCompleted = false;
-            var mres = new SetOnCountdownMres(tasks.Count);
-            try
-            {
-                foreach (Task task in tasks)
-                {
-                    task.AddCompletionAction(mres, addBeforeOthers: true);
-                }
-                waitCompleted = mres.Wait(millisecondsTimeout, cancellationToken);
-            }
-            finally
-            {
-                if (!waitCompleted)
-                {
-                    foreach (Task task in tasks)
-                    {
-                        if (!task.IsCompleted) task.RemoveContinuation(mres);
-                    }
-                }
-            }
-            return waitCompleted;
-        }
-        private sealed class SetOnCountdownMres : ManualResetEventSlim, ITaskCompletionAction
-        {
-            private int _count;
-            internal SetOnCountdownMres(int count)
-            {
-                Debug.Assert(count > 0, "Expected count > 0");
-                _count = count;
-            }
-            public void Invoke(Task completingTask)
-            {
-                if (Interlocked.Decrement(ref _count) == 0) Set();
-                Debug.Assert(_count >= 0, "Count should never go below 0");
-            }
-            public bool InvokeMayRunArbitraryCode => false;
-        }
-        internal static void AddExceptionsForCompletedTask(ref List<Exception>? exceptions, Task t)
-        {
-            AggregateException? ex = t.GetExceptions(true);
-            if (ex != null)
-            {
-                t.UpdateExceptionObservedStatus();
-                exceptions ??= new List<Exception>(ex.InnerExceptionCount);
-                exceptions.AddRange(ex.InternalInnerExceptions);
-            }
-        }
-        [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
-        public static int WaitAny(params Task[] tasks)
-        {
-            int waitResult = WaitAnyCore(tasks, Timeout.Infinite, default);
-            Debug.Assert(tasks.Length == 0 || waitResult != -1, "expected wait to succeed");
-            return waitResult;
-        }
-        [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
-        public static int WaitAny(Task[] tasks, TimeSpan timeout)
-        {
-            long totalMilliseconds = (long)timeout.TotalMilliseconds;
-            if (totalMilliseconds < -1 || totalMilliseconds > int.MaxValue)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.timeout);
-            }
-            return WaitAnyCore(tasks, (int)totalMilliseconds, default);
-        }
-        [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
-        public static int WaitAny(Task[] tasks, CancellationToken cancellationToken)
-        {
-            return WaitAnyCore(tasks, Timeout.Infinite, cancellationToken);
-        }
-        [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
-        public static int WaitAny(Task[] tasks, int millisecondsTimeout)
-        {
-            return WaitAnyCore(tasks, millisecondsTimeout, default);
-        }
-        [MethodImpl(MethodImplOptions.NoOptimization)]  // this is needed for the parallel debugger
-        public static int WaitAny(Task[] tasks, int millisecondsTimeout, CancellationToken cancellationToken) =>
-            WaitAnyCore(tasks, millisecondsTimeout, cancellationToken);
-        private static int WaitAnyCore(Task[] tasks, int millisecondsTimeout, CancellationToken cancellationToken)
-        {
-            if (tasks == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
-            }
-            if (millisecondsTimeout < -1)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.millisecondsTimeout);
-            }
-            cancellationToken.ThrowIfCancellationRequested(); // early check before we make any allocations
-            int signaledTaskIndex = -1;
-            for (int taskIndex = 0; taskIndex < tasks.Length; taskIndex++)
-            {
-                Task task = tasks[taskIndex];
-                if (task == null)
-                {
-                    ThrowHelper.ThrowArgumentException(ExceptionResource.Task_WaitMulti_NullTask, ExceptionArgument.tasks);
-                }
-                if (signaledTaskIndex == -1 && task.IsCompleted)
-                {
-                    signaledTaskIndex = taskIndex;
-                }
-            }
-            if (signaledTaskIndex == -1 && tasks.Length != 0)
-            {
-                Task<Task> firstCompleted = TaskFactory.CommonCWAnyLogic(tasks, isSyncBlocking: true);
-                bool waitCompleted = firstCompleted.Wait(millisecondsTimeout, cancellationToken);
-                if (waitCompleted)
-                {
-                    Debug.Assert(firstCompleted.Status == TaskStatus.RanToCompletion);
-                    signaledTaskIndex = Array.IndexOf(tasks, firstCompleted.Result);
-                    Debug.Assert(signaledTaskIndex >= 0);
-                }
-                else
-                {
-                    TaskFactory.CommonCWAnyLogicCleanup(firstCompleted);
-                }
-            }
-            GC.KeepAlive(tasks);
-            return signaledTaskIndex;
-        }
-        #region FromResult / FromException / FromCanceled
-        [MethodImpl(MethodImplOptions.AggressiveInlining)] // method looks long, but for a given TResult it results in a relatively small amount of asm
-        public static unsafe Task<TResult> FromResult<TResult>(TResult result)
-        {
-#pragma warning disable 8500 // address of / sizeof of managed types
-            if (result is null)
-            {
-                return Task<TResult>.s_defaultResultTask;
-            }
-            if (typeof(TResult) == typeof(bool)) // only the relevant branches are kept for each value-type generic instantiation
-            {
-                Task<bool> task = *(bool*)&result ? TaskCache.s_trueTask : TaskCache.s_falseTask;
-                return *(Task<TResult>*)&task;
-            }
-            if (typeof(TResult) == typeof(int))
-            {
-                int value = *(int*)&result;
-                if ((uint)(value - TaskCache.InclusiveInt32Min) < (TaskCache.ExclusiveInt32Max - TaskCache.InclusiveInt32Min))
-                {
-                    Task<int> task = TaskCache.s_int32Tasks[value - TaskCache.InclusiveInt32Min];
-                    return *(Task<TResult>*)&task;
-                }
-            }
-            else if (!RuntimeHelpers.IsReferenceOrContainsReferences<TResult>())
-            {
-                if ((sizeof(TResult) == sizeof(byte) && *(byte*)&result == default(byte)) ||
-                    (sizeof(TResult) == sizeof(ushort) && *(ushort*)&result == default(ushort)) ||
-                    (sizeof(TResult) == sizeof(uint) && *(uint*)&result == default) ||
-                    (sizeof(TResult) == sizeof(ulong) && *(ulong*)&result == default) ||
-                    (sizeof(TResult) == sizeof(UInt128) && *(UInt128*)&result == default))
-                {
-                    return Task<TResult>.s_defaultResultTask;
-                }
-            }
-            return new Task<TResult>(result);
-#pragma warning restore 8500
-        }
-        public static Task FromException(Exception exception)
-        {
-            if (exception == null) ThrowHelper.ThrowArgumentNullException(ExceptionArgument.exception);
-            var task = new Task();
-            bool succeeded = task.TrySetException(exception);
-            Debug.Assert(succeeded, "This should always succeed on a new task.");
-            return task;
-        }
-        public static Task<TResult> FromException<TResult>(Exception exception)
-        {
-            if (exception == null) ThrowHelper.ThrowArgumentNullException(ExceptionArgument.exception);
-            var task = new Task<TResult>();
-            bool succeeded = task.TrySetException(exception);
-            Debug.Assert(succeeded, "This should always succeed on a new task.");
-            return task;
-        }
-        public static Task FromCanceled(CancellationToken cancellationToken)
-        {
-            if (!cancellationToken.IsCancellationRequested)
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.cancellationToken);
-            return new Task(true, TaskCreationOptions.None, cancellationToken);
-        }
-        public static Task<TResult> FromCanceled<TResult>(CancellationToken cancellationToken)
-        {
-            if (!cancellationToken.IsCancellationRequested)
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.cancellationToken);
-            return new Task<TResult>(true, default, TaskCreationOptions.None, cancellationToken);
-        }
-        internal static Task FromCanceled(OperationCanceledException exception)
-        {
-            Debug.Assert(exception != null);
-            var task = new Task();
-            bool succeeded = task.TrySetCanceled(exception.CancellationToken, exception);
-            Debug.Assert(succeeded, "This should always succeed on a new task.");
-            return task;
-        }
-        internal static Task<TResult> FromCanceled<TResult>(OperationCanceledException exception)
-        {
-            Debug.Assert(exception != null);
-            var task = new Task<TResult>();
-            bool succeeded = task.TrySetCanceled(exception.CancellationToken, exception);
-            Debug.Assert(succeeded, "This should always succeed on a new task.");
-            return task;
-        }
-        #endregion
-        #region Run methods
-        public static Task Run(Action action)
-        {
-            return InternalStartNew(null, action, null, default, TaskScheduler.Default,
-                TaskCreationOptions.DenyChildAttach, InternalTaskOptions.None);
-        }
-        public static Task Run(Action action, CancellationToken cancellationToken)
-        {
-            return InternalStartNew(null, action, null, cancellationToken, TaskScheduler.Default,
-                TaskCreationOptions.DenyChildAttach, InternalTaskOptions.None);
-        }
-        public static Task<TResult> Run<TResult>(Func<TResult> function)
-        {
-            return Task<TResult>.StartNew(null, function, default,
-                TaskCreationOptions.DenyChildAttach, InternalTaskOptions.None, TaskScheduler.Default);
-        }
-        public static Task<TResult> Run<TResult>(Func<TResult> function, CancellationToken cancellationToken)
-        {
-            return Task<TResult>.StartNew(null, function, cancellationToken,
-                TaskCreationOptions.DenyChildAttach, InternalTaskOptions.None, TaskScheduler.Default);
-        }
-        public static Task Run(Func<Task?> function)
-        {
-            return Run(function, default);
-        }
-        public static Task Run(Func<Task?> function, CancellationToken cancellationToken)
-        {
-            if (function == null) ThrowHelper.ThrowArgumentNullException(ExceptionArgument.function);
-            if (cancellationToken.IsCancellationRequested)
-                return FromCanceled(cancellationToken);
-            Task<Task?> task1 = Task<Task?>.Factory.StartNew(function, cancellationToken, TaskCreationOptions.DenyChildAttach, TaskScheduler.Default);
-            UnwrapPromise<VoidTaskResult> promise = new UnwrapPromise<VoidTaskResult>(task1, lookForOce: true);
-            return promise;
-        }
-        public static Task<TResult> Run<TResult>(Func<Task<TResult>?> function)
-        {
-            return Run(function, default);
-        }
-        public static Task<TResult> Run<TResult>(Func<Task<TResult>?> function, CancellationToken cancellationToken)
-        {
-            if (function == null) ThrowHelper.ThrowArgumentNullException(ExceptionArgument.function);
-            if (cancellationToken.IsCancellationRequested)
-                return FromCanceled<TResult>(cancellationToken);
-            Task<Task<TResult>?> task1 = Task<Task<TResult>?>.Factory.StartNew(function, cancellationToken, TaskCreationOptions.DenyChildAttach, TaskScheduler.Default);
-            UnwrapPromise<TResult> promise = new UnwrapPromise<TResult>(task1, lookForOce: true);
-            return promise;
-        }
-        #endregion
-        #region Delay methods
-        public static Task Delay(TimeSpan delay) => Delay(delay, TimeProvider.System, default);
-        public static Task Delay(TimeSpan delay, TimeProvider timeProvider) => Delay(delay, timeProvider, default);
-        public static Task Delay(TimeSpan delay, CancellationToken cancellationToken) =>
-            Delay(delay, TimeProvider.System, cancellationToken);
-        public static Task Delay(TimeSpan delay, TimeProvider timeProvider, CancellationToken cancellationToken)
-        {
-            ArgumentNullException.ThrowIfNull(timeProvider);
-            return Delay(ValidateTimeout(delay, ExceptionArgument.delay), timeProvider, cancellationToken);
-        }
-        public static Task Delay(int millisecondsDelay) => Delay(millisecondsDelay, default);
-        public static Task Delay(int millisecondsDelay, CancellationToken cancellationToken)
-        {
-            if (millisecondsDelay < -1)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.millisecondsDelay, ExceptionResource.Task_Delay_InvalidMillisecondsDelay);
-            }
-            return Delay((uint)millisecondsDelay, TimeProvider.System, cancellationToken);
-        }
-        private static Task Delay(uint millisecondsDelay, TimeProvider timeProvider, CancellationToken cancellationToken) =>
-            cancellationToken.IsCancellationRequested ? FromCanceled(cancellationToken) :
-            millisecondsDelay == 0 ? CompletedTask :
-            cancellationToken.CanBeCanceled ? new DelayPromiseWithCancellation(millisecondsDelay, timeProvider, cancellationToken) :
-            new DelayPromise(millisecondsDelay, timeProvider);
-        internal static uint ValidateTimeout(TimeSpan timeout, ExceptionArgument argument)
-        {
-            long totalMilliseconds = (long)timeout.TotalMilliseconds;
-            if (totalMilliseconds < -1 || totalMilliseconds > Timer.MaxSupportedTimeout)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(argument, ExceptionResource.Task_InvalidTimerTimeSpan);
-            }
-            return (uint)totalMilliseconds;
-        }
-        private class DelayPromise : Task
-        {
-            private static readonly TimerCallback s_timerCallback = TimerCallback;
-            private readonly ITimer? _timer;
-            internal DelayPromise(uint millisecondsDelay, TimeProvider timeProvider)
-            {
-                Debug.Assert(millisecondsDelay != 0);
-                if (TplEventSource.Log.IsEnabled())
-                    TplEventSource.Log.TraceOperationBegin(this.Id, "Task.Delay", 0);
-                if (s_asyncDebuggingEnabled)
-                    AddToActiveTasks(this);
-                if (millisecondsDelay != Timeout.UnsignedInfinite) // no need to create the timer if it's an infinite timeout
-                {
-                    if (timeProvider == TimeProvider.System)
-                    {
-                        _timer = new TimerQueueTimer(s_timerCallback, this, millisecondsDelay, Timeout.UnsignedInfinite, flowExecutionContext: false);
-                    }
-                    else
-                    {
-                        using (ExecutionContext.SuppressFlow())
-                        {
-                            _timer = timeProvider.CreateTimer(s_timerCallback, this, TimeSpan.FromMilliseconds(millisecondsDelay), Timeout.InfiniteTimeSpan);
-                        }
-                    }
-                    if (IsCompleted)
-                    {
-                        _timer.Dispose();
-                    }
-                }
-            }
-            private static void TimerCallback(object? state) => ((DelayPromise)state!).CompleteTimedOut();
-            private void CompleteTimedOut()
-            {
-                if (TrySetResult())
-                {
-                    Cleanup();
-                    if (s_asyncDebuggingEnabled)
-                        RemoveFromActiveTasks(this);
-                    if (TplEventSource.Log.IsEnabled())
-                        TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Completed);
-                }
-            }
-            protected virtual void Cleanup() => _timer?.Dispose();
-        }
-        private sealed class DelayPromiseWithCancellation : DelayPromise
-        {
-            private readonly CancellationTokenRegistration _registration;
-            internal DelayPromiseWithCancellation(uint millisecondsDelay, TimeProvider timeProvider, CancellationToken token) : base(millisecondsDelay, timeProvider)
-            {
-                Debug.Assert(token.CanBeCanceled);
-                _registration = token.UnsafeRegister(static (state, cancellationToken) =>
-                {
-                    var thisRef = (DelayPromiseWithCancellation)state!;
-                    thisRef.AtomicStateUpdate((int)TaskCreationOptions.RunContinuationsAsynchronously, 0);
-                    if (thisRef.TrySetCanceled(cancellationToken))
-                    {
-                        thisRef.Cleanup();
-                    }
-                }, this);
-                if (IsCompleted)
-                {
-                    _registration.Dispose();
-                }
-            }
-            protected override void Cleanup()
-            {
-                _registration.Dispose();
-                base.Cleanup();
-            }
-        }
-        #endregion
-        #region WhenAll
-        public static Task WhenAll(IEnumerable<Task> tasks)
-        {
-            if (tasks is null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
-            }
-            if (tasks is ICollection<Task> taskCollection)
-            {
-                if (tasks is Task[] taskArray)
-                {
-                    return WhenAll((ReadOnlySpan<Task>)taskArray);
-                }
-                if (tasks is List<Task> taskList)
-                {
-                    return WhenAll(CollectionsMarshal.AsSpan(taskList));
-                }
-                taskArray = new Task[taskCollection.Count];
-                taskCollection.CopyTo(taskArray, 0);
-                return WhenAll((ReadOnlySpan<Task>)taskArray);
-            }
-            else
-            {
-                var taskList = new List<Task>();
-                foreach (Task task in tasks)
-                {
-                    taskList.Add(task);
-                }
-                return WhenAll(CollectionsMarshal.AsSpan(taskList));
-            }
-        }
-        public static Task WhenAll(params Task[] tasks)
-        {
-            if (tasks is null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
-            }
-            return WhenAll((ReadOnlySpan<Task>)tasks);
-        }
-        internal static Task WhenAll(ReadOnlySpan<Task> tasks) => // TODO https://github.com/dotnet/runtime/issues/77873: Make this public.
-            tasks.Length != 0 ? new WhenAllPromise(tasks) : CompletedTask;
-        private sealed class WhenAllPromise : Task, ITaskCompletionAction
-        {
-            private object? _failedOrCanceled;
-            private int _remainingToComplete;
-            internal WhenAllPromise(ReadOnlySpan<Task> tasks)
-            {
-                Debug.Assert(tasks.Length != 0, "Expected a non-zero length task array");
-                foreach (Task task in tasks)
-                {
-                    if (task is null)
-                    {
-                        ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
-                    }
-                }
-                if (TplEventSource.Log.IsEnabled())
-                {
-                    TplEventSource.Log.TraceOperationBegin(Id, "Task.WhenAll", 0);
-                }
-                if (s_asyncDebuggingEnabled)
-                {
-                    AddToActiveTasks(this);
-                }
-                _remainingToComplete = tasks.Length;
-                foreach (Task task in tasks)
-                {
-                    if (task is null || task.IsCompleted)
-                    {
-                        Invoke(task); // short-circuit the completion action, if possible
-                    }
-                    else
-                    {
-                        task.AddCompletionAction(this); // simple completion action
-                    }
-                }
-            }
-            public void Invoke(Task? completedTask)
-            {
-                if (TplEventSource.Log.IsEnabled())
-                {
-                    TplEventSource.Log.TraceOperationRelation(Id, CausalityRelation.Join);
-                }
-                if (completedTask is not null)
-                {
-                    if (completedTask.IsWaitNotificationEnabled)
-                    {
-                        SetNotificationForWaitCompletion(enabled: true);
-                    }
-                    if (!completedTask.IsCompletedSuccessfully)
-                    {
-                        if (Interlocked.CompareExchange(ref _failedOrCanceled, completedTask, null) != null)
-                        {
-                            while (true)
-                            {
-                                object? failedOrCanceled = _failedOrCanceled;
-                                Debug.Assert(failedOrCanceled is not null);
-                                if (_failedOrCanceled is List<Task> list)
-                                {
-                                    lock (list)
-                                    {
-                                        list.Add(completedTask);
-                                    }
-                                    break;
-                                }
-                                Debug.Assert(failedOrCanceled is Task, $"Expected Task, got {failedOrCanceled}");
-                                if (Interlocked.CompareExchange(ref _failedOrCanceled, new List<Task> { (Task)failedOrCanceled, completedTask }, failedOrCanceled) == failedOrCanceled)
-                                {
-                                    break;
-                                }
-                                Debug.Assert(_failedOrCanceled is List<Task>);
-                            }
-                        }
-                    }
-                }
-                if (Interlocked.Decrement(ref _remainingToComplete) == 0)
-                {
-                    object? failedOrCanceled = _failedOrCanceled;
-                    if (failedOrCanceled is null)
-                    {
-                        if (TplEventSource.Log.IsEnabled())
-                        {
-                            TplEventSource.Log.TraceOperationEnd(Id, AsyncCausalityStatus.Completed);
-                        }
-                        if (s_asyncDebuggingEnabled)
-                        {
-                            RemoveFromActiveTasks(this);
-                        }
-                        bool completed = TrySetResult();
-                        Debug.Assert(completed);
-                    }
-                    else
-                    {
-                        List<ExceptionDispatchInfo>? observedExceptions = null;
-                        Task? canceledTask = null;
-                        void HandleTask(Task task)
-                        {
-                            if (task.IsFaulted)
-                            {
-                                (observedExceptions ??= new()).AddRange(task.GetExceptionDispatchInfos());
-                            }
-                            else if (task.IsCanceled)
-                            {
-                                canceledTask ??= task; // use the first task that's canceled
-                            }
-                        }
-                        if (failedOrCanceled is List<Task> list)
-                        {
-                            foreach (Task task in list)
-                            {
-                                HandleTask(task);
-                            }
-                        }
-                        else
-                        {
-                            Debug.Assert(failedOrCanceled is Task);
-                            HandleTask((Task)failedOrCanceled);
-                        }
-                        if (observedExceptions != null)
-                        {
-                            Debug.Assert(observedExceptions.Count > 0, "Expected at least one exception");
-                            TrySetException(observedExceptions);
-                        }
-                        else if (canceledTask != null)
-                        {
-                            TrySetCanceled(canceledTask.CancellationToken, canceledTask.GetCancellationExceptionDispatchInfo());
-                        }
-                    }
-                    Debug.Assert(IsCompleted);
-                }
-                Debug.Assert(_remainingToComplete >= 0, "Count should never go below 0");
-            }
-            public bool InvokeMayRunArbitraryCode => true;
-        }
-        public static Task<TResult[]> WhenAll<TResult>(IEnumerable<Task<TResult>> tasks)
-        {
-            if (tasks is Task<TResult>[] taskArray)
-            {
-                return WhenAll(taskArray);
-            }
-            if (tasks is ICollection<Task<TResult>> taskCollection)
-            {
-                taskArray = new Task<TResult>[taskCollection.Count];
-                taskCollection.CopyTo(taskArray, 0);
-                foreach (Task<TResult> task in taskArray)
-                {
-                    if (task is null)
-                    {
-                        ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
-                    }
-                }
-                return InternalWhenAll(taskArray);
-            }
-            if (tasks == null) ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
-            List<Task<TResult>> taskList = new List<Task<TResult>>();
-            foreach (Task<TResult> task in tasks)
-            {
-                if (task == null) ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
-                taskList.Add(task);
-            }
-            return InternalWhenAll(taskList.ToArray());
-        }
-        public static Task<TResult[]> WhenAll<TResult>(params Task<TResult>[] tasks)
-        {
-            if (tasks == null) ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
-            int taskCount = tasks.Length;
-            if (taskCount == 0) return InternalWhenAll(tasks); // small optimization in the case of an empty task array
-            Task<TResult>[] tasksCopy = (Task<TResult>[])tasks.Clone();
-            foreach (Task<TResult> task in tasksCopy)
-            {
-                if (task is null)
-                {
-                    ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
-                }
-            }
-            return InternalWhenAll(tasksCopy);
-        }
-        private static Task<TResult[]> InternalWhenAll<TResult>(Task<TResult>[] tasks)
-        {
-            Debug.Assert(tasks != null, "Expected a non-null tasks array");
-            return (tasks.Length == 0) ? // take shortcut if there are no tasks upon which to wait
-                new Task<TResult[]>(false, Array.Empty<TResult>(), TaskCreationOptions.None, default) :
-                new WhenAllPromise<TResult>(tasks);
-        }
-        private sealed class WhenAllPromise<T> : Task<T[]>, ITaskCompletionAction
-        {
-            private readonly Task<T>?[] m_tasks;
-            private int m_count;
-            internal WhenAllPromise(Task<T>[] tasks)
-            {
-                Debug.Assert(tasks != null, "Expected a non-null task array");
-                Debug.Assert(tasks.Length > 0, "Expected a non-zero length task array");
-                m_tasks = tasks;
-                m_count = tasks.Length;
-                if (TplEventSource.Log.IsEnabled())
-                    TplEventSource.Log.TraceOperationBegin(this.Id, "Task.WhenAll", 0);
-                if (s_asyncDebuggingEnabled)
-                    AddToActiveTasks(this);
-                foreach (Task<T> task in tasks)
-                {
-                    if (task.IsCompleted) this.Invoke(task); // short-circuit the completion action, if possible
-                    else task.AddCompletionAction(this); // simple completion action
-                }
-            }
-            public void Invoke(Task ignored)
-            {
-                if (TplEventSource.Log.IsEnabled())
-                    TplEventSource.Log.TraceOperationRelation(this.Id, CausalityRelation.Join);
-                if (Interlocked.Decrement(ref m_count) == 0)
-                {
-                    T[] results = new T[m_tasks.Length];
-                    List<ExceptionDispatchInfo>? observedExceptions = null;
-                    Task? canceledTask = null;
-                    for (int i = 0; i < m_tasks.Length; i++)
-                    {
-                        Task<T>? task = m_tasks[i];
-                        Debug.Assert(task != null, "Constituent task in WhenAll should never be null");
-                        if (task.IsFaulted)
-                        {
-                            observedExceptions ??= new List<ExceptionDispatchInfo>();
-                            observedExceptions.AddRange(task.GetExceptionDispatchInfos());
-                        }
-                        else if (task.IsCanceled)
-                        {
-                            canceledTask ??= task; // use the first task that's canceled
-                        }
-                        else
-                        {
-                            Debug.Assert(task.Status == TaskStatus.RanToCompletion);
-                            results[i] = task.GetResultCore(waitCompletionNotification: false); // avoid Result, which would triggering debug notification
-                        }
-                        if (task.IsWaitNotificationEnabled) this.SetNotificationForWaitCompletion(enabled: true);
-                        else m_tasks[i] = null; // avoid holding onto tasks unnecessarily
-                    }
-                    if (observedExceptions != null)
-                    {
-                        Debug.Assert(observedExceptions.Count > 0, "Expected at least one exception");
-                        TrySetException(observedExceptions);
-                    }
-                    else if (canceledTask != null)
-                    {
-                        TrySetCanceled(canceledTask.CancellationToken, canceledTask.GetCancellationExceptionDispatchInfo());
-                    }
-                    else
-                    {
-                        if (TplEventSource.Log.IsEnabled())
-                            TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Completed);
-                        if (s_asyncDebuggingEnabled)
-                            RemoveFromActiveTasks(this);
-                        TrySetResult(results);
-                    }
-                }
-                Debug.Assert(m_count >= 0, "Count should never go below 0");
-            }
-            public bool InvokeMayRunArbitraryCode => true;
-            private protected override bool ShouldNotifyDebuggerOfWaitCompletion =>
-                base.ShouldNotifyDebuggerOfWaitCompletion &&
-                AnyTaskRequiresNotifyDebuggerOfWaitCompletion(m_tasks);
-        }
-        #endregion
-        #region WhenAny
-        public static Task<Task> WhenAny(params Task[] tasks)
-        {
-            ArgumentNullException.ThrowIfNull(tasks);
-            return WhenAny((ReadOnlySpan<Task>)tasks);
-        }
-        private static Task<TTask> WhenAny<TTask>(ReadOnlySpan<TTask> tasks) where TTask : Task
-        {
-            if (tasks.Length == 2)
-            {
-                return WhenAny(tasks[0], tasks[1]);
-            }
-            if (tasks.IsEmpty)
-            {
-                ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_EmptyTaskList, ExceptionArgument.tasks);
-            }
-            TTask[] tasksCopy = tasks.ToArray();
-            foreach (TTask task in tasksCopy)
-            {
-                if (task is null)
-                {
-                    ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
-                }
-            }
-            return TaskFactory.CommonCWAnyLogic(tasksCopy);
-        }
-        public static Task<Task> WhenAny(Task task1, Task task2) =>
-            WhenAny<Task>(task1, task2);
-        private static Task<TTask> WhenAny<TTask>(TTask task1, TTask task2) where TTask : Task
-        {
-            ArgumentNullException.ThrowIfNull(task1);
-            ArgumentNullException.ThrowIfNull(task2);
-            return
-                task1.IsCompleted ? FromResult(task1) :
-                task2.IsCompleted ? FromResult(task2) :
-                new TwoTaskWhenAnyPromise<TTask>(task1, task2);
-        }
-        private sealed class TwoTaskWhenAnyPromise<TTask> : Task<TTask>, ITaskCompletionAction where TTask : Task
-        {
-            private TTask? _task1, _task2;
-            public TwoTaskWhenAnyPromise(TTask task1, TTask task2)
-            {
-                Debug.Assert(task1 != null && task2 != null);
-                _task1 = task1;
-                _task2 = task2;
-                if (TplEventSource.Log.IsEnabled())
-                {
-                    TplEventSource.Log.TraceOperationBegin(this.Id, "Task.WhenAny", 0);
-                }
-                if (s_asyncDebuggingEnabled)
-                {
-                    AddToActiveTasks(this);
-                }
-                task1.AddCompletionAction(this);
-                task2.AddCompletionAction(this);
-                if (task1.IsCompleted)
-                {
-                    task2.RemoveContinuation(this);
-                }
-            }
-            public void Invoke(Task completingTask)
-            {
-                Task? task1;
-                if ((task1 = Interlocked.Exchange(ref _task1, null)) != null)
-                {
-                    Task? task2 = _task2;
-                    _task2 = null;
-                    Debug.Assert(task1 != null && task2 != null);
-                    Debug.Assert(task1.IsCompleted || task2.IsCompleted);
-                    if (TplEventSource.Log.IsEnabled())
-                    {
-                        TplEventSource.Log.TraceOperationRelation(this.Id, CausalityRelation.Choice);
-                        TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Completed);
-                    }
-                    if (s_asyncDebuggingEnabled)
-                    {
-                        RemoveFromActiveTasks(this);
-                    }
-                    if (!task1.IsCompleted)
-                    {
-                        task1.RemoveContinuation(this);
-                    }
-                    else
-                    {
-                        task2.RemoveContinuation(this);
-                    }
-                    bool success = TrySetResult((TTask)completingTask);
-                    Debug.Assert(success, "Only one task should have gotten to this point, and thus this must be successful.");
-                }
-            }
-            public bool InvokeMayRunArbitraryCode => true;
-        }
-        public static Task<Task> WhenAny(IEnumerable<Task> tasks) =>
-            WhenAny<Task>(tasks);
-        private static Task<TTask> WhenAny<TTask>(IEnumerable<TTask> tasks) where TTask : Task
-        {
-            if (tasks is ICollection<TTask> tasksAsCollection)
-            {
-                if (tasks is List<TTask> tasksAsList)
-                {
-                    return WhenAny((ReadOnlySpan<TTask>)CollectionsMarshal.AsSpan(tasksAsList));
-                }
-                if (tasks is TTask[] tasksAsArray)
-                {
-                    return WhenAny((ReadOnlySpan<TTask>)tasksAsArray);
-                }
-                int count = tasksAsCollection.Count;
-                if (count <= 0)
-                {
-                    ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_EmptyTaskList, ExceptionArgument.tasks);
-                }
-                var taskArray = new TTask[count];
-                tasksAsCollection.CopyTo(taskArray, 0);
-                foreach (TTask task in taskArray)
-                {
-                    if (task is null)
-                    {
-                        ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
-                    }
-                }
-                return TaskFactory.CommonCWAnyLogic(taskArray);
-            }
-            if (tasks is null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.tasks);
-            }
-            var taskList = new List<TTask>();
-            foreach (TTask task in tasks)
-            {
-                if (task is null)
-                {
-                    ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_NullTask, ExceptionArgument.tasks);
-                }
-                taskList.Add(task);
-            }
-            if (taskList.Count == 0)
-            {
-                ThrowHelper.ThrowArgumentException(ExceptionResource.Task_MultiTaskContinuation_EmptyTaskList, ExceptionArgument.tasks);
-            }
-            return TaskFactory.CommonCWAnyLogic(taskList);
-        }
-        public static Task<Task<TResult>> WhenAny<TResult>(params Task<TResult>[] tasks)
-        {
-            ArgumentNullException.ThrowIfNull(tasks);
-            return WhenAny((ReadOnlySpan<Task<TResult>>)tasks);
-        }
-        public static Task<Task<TResult>> WhenAny<TResult>(Task<TResult> task1, Task<TResult> task2) =>
-            WhenAny<Task<TResult>>(task1, task2);
-        public static Task<Task<TResult>> WhenAny<TResult>(IEnumerable<Task<TResult>> tasks) =>
-            WhenAny<Task<TResult>>(tasks);
-        #endregion
-        internal static Task<TResult> CreateUnwrapPromise<TResult>(Task outerTask, bool lookForOce)
-        {
-            Debug.Assert(outerTask != null);
-            return new UnwrapPromise<TResult>(outerTask, lookForOce);
-        }
-        internal virtual Delegate[]? GetDelegateContinuationsForDebugger()
-        {
-            if (m_continuationObject != this)
-                return GetDelegatesFromContinuationObject(m_continuationObject);
-            else
-                return null;
-        }
-        private static Delegate[]? GetDelegatesFromContinuationObject(object? continuationObject)
-        {
-            if (continuationObject != null)
-            {
-                if (continuationObject is Action singleAction)
-                {
-                    return new Delegate[] { AsyncMethodBuilderCore.TryGetStateMachineForDebugger(singleAction) };
-                }
-                if (continuationObject is TaskContinuation taskContinuation)
-                {
-                    return taskContinuation.GetDelegateContinuationsForDebugger();
-                }
-                if (continuationObject is Task continuationTask)
-                {
-                    Delegate[]? delegates = continuationTask.GetDelegateContinuationsForDebugger();
-                    if (delegates != null)
-                        return delegates;
-                }
-                if (continuationObject is ITaskCompletionAction singleCompletionAction)
-                {
-                    return new Delegate[] { new Action<Task>(singleCompletionAction.Invoke) };
-                }
-                if (continuationObject is List<object?> continuationList)
-                {
-                    List<Delegate> result = new List<Delegate>();
-                    foreach (object? obj in continuationList)
-                    {
-                        Delegate[]? innerDelegates = GetDelegatesFromContinuationObject(obj);
-                        if (innerDelegates != null)
-                        {
-                            foreach (Delegate del in innerDelegates)
-                            {
-                                if (del != null)
-                                    result.Add(del);
-                            }
-                        }
-                    }
-                    return result.ToArray();
-                }
-            }
-            return null;
-        }
-        private static Task? GetActiveTaskFromId(int taskId)
-        {
-            Task? task = null;
-            s_currentActiveTasks?.TryGetValue(taskId, out task);
-            return task;
-        }
-    }
-    internal sealed class CompletionActionInvoker : IThreadPoolWorkItem
-    {
-        private readonly ITaskCompletionAction m_action;
-        private readonly Task m_completingTask;
-        internal CompletionActionInvoker(ITaskCompletionAction action, Task completingTask)
-        {
-            m_action = action;
-            m_completingTask = completingTask;
-        }
-        void IThreadPoolWorkItem.Execute()
-        {
-            m_action.Invoke(m_completingTask);
-        }
-    }
-    internal sealed class SystemThreadingTasks_TaskDebugView
-    {
-        private readonly Task m_task;
-        public SystemThreadingTasks_TaskDebugView(Task task)
-        {
-            m_task = task;
-        }
-        public object? AsyncState => m_task.AsyncState;
-        public TaskCreationOptions CreationOptions => m_task.CreationOptions;
-        public Exception? Exception => m_task.Exception;
-        public int Id => m_task.Id;
-        public bool CancellationPending => (m_task.Status == TaskStatus.WaitingToRun) && m_task.CancellationToken.IsCancellationRequested;
-        public TaskStatus Status => m_task.Status;
-    }
-    [Flags]
-    public enum TaskCreationOptions
-    {
-        None = 0x0,
-        PreferFairness = 0x01,
-        LongRunning = 0x02,
-        AttachedToParent = 0x04,
-        DenyChildAttach = 0x08,
-        HideScheduler = 0x10,
-        RunContinuationsAsynchronously = 0x40
-    }
-    [Flags]
-    internal enum InternalTaskOptions
-    {
-        None,
-        InternalOptionsMask = 0x0000FF00,
-        ContinuationTask = 0x0200,
-        PromiseTask = 0x0400,
-        HiddenState = 0x0800,
-        LazyCancellation = 0x1000,
-        QueuedByRuntime = 0x2000,
-        DoNotDispose = 0x4000
-    }
-    [Flags]
-    public enum TaskContinuationOptions
-    {
-        None = 0,
-        PreferFairness = 0x01,
-        LongRunning = 0x02,
-        AttachedToParent = 0x04,
-        DenyChildAttach = 0x08,
-        HideScheduler = 0x10,
-        LazyCancellation = 0x20,
-        RunContinuationsAsynchronously = 0x40,
-        NotOnRanToCompletion = 0x10000,
-        NotOnFaulted = 0x20000,
-        NotOnCanceled = 0x40000,
-        OnlyOnRanToCompletion = NotOnFaulted | NotOnCanceled,
-        OnlyOnFaulted = NotOnRanToCompletion | NotOnCanceled,
-        OnlyOnCanceled = NotOnRanToCompletion | NotOnFaulted,
-        ExecuteSynchronously = 0x80000
-    }
-    internal readonly struct VoidTaskResult { }
-    internal interface ITaskCompletionAction
-    {
-        void Invoke(Task completingTask);
-        bool InvokeMayRunArbitraryCode { get; }
-    }
-    internal sealed class UnwrapPromise<TResult> : Task<TResult>, ITaskCompletionAction
-    {
-        private const byte STATE_WAITING_ON_OUTER_TASK = 0; // Invoke() means "process completed outer task"
-        private const byte STATE_WAITING_ON_INNER_TASK = 1; // Invoke() means "process completed inner task"
-        private const byte STATE_DONE = 2;                  // Invoke() means "something went wrong and we are hosed!"
-        private byte _state;
-        private readonly bool _lookForOce;
-        public UnwrapPromise(Task outerTask, bool lookForOce)
-            : base((object?)null, outerTask.CreationOptions & TaskCreationOptions.AttachedToParent)
-        {
-            Debug.Assert(outerTask != null, "Expected non-null outerTask");
-            _lookForOce = lookForOce;
-            if (TplEventSource.Log.IsEnabled())
-                TplEventSource.Log.TraceOperationBegin(this.Id, "Task.Unwrap", 0);
-            if (s_asyncDebuggingEnabled)
-                AddToActiveTasks(this);
-            if (outerTask.IsCompleted)
-            {
-                ProcessCompletedOuterTask(outerTask);
-            }
-            else // Otherwise, process its completion asynchronously.
-            {
-                outerTask.AddCompletionAction(this);
-            }
-        }
-        public void Invoke(Task completingTask)
-        {
-            if (RuntimeHelpers.TryEnsureSufficientExecutionStack())
-            {
-                InvokeCore(completingTask);
-            }
-            else
-            {
-                InvokeCoreAsync(completingTask);
-            }
-        }
-        private void InvokeCore(Task completingTask)
-        {
-            switch (_state)
-            {
-                case STATE_WAITING_ON_OUTER_TASK:
-                    ProcessCompletedOuterTask(completingTask);
-                    break;
-                case STATE_WAITING_ON_INNER_TASK:
-                    bool result = TrySetFromTask(completingTask, lookForOce: false);
-                    _state = STATE_DONE; // bump the state
-                    Debug.Assert(result, "Expected TrySetFromTask from inner task to succeed");
-                    break;
-                default:
-                    Debug.Fail("UnwrapPromise in illegal state");
-                    break;
-            }
-        }
-        private void InvokeCoreAsync(Task completingTask)
-        {
-            ThreadPool.UnsafeQueueUserWorkItem(static state =>
-            {
-                var tuple = (TupleSlim<UnwrapPromise<TResult>, Task>)state!;
-                tuple.Item1.InvokeCore(tuple.Item2);
-            }, new TupleSlim<UnwrapPromise<TResult>, Task>(this, completingTask));
-        }
-        private void ProcessCompletedOuterTask(Task task)
-        {
-            Debug.Assert(task != null && task.IsCompleted, "Expected non-null, completed outer task");
-            Debug.Assert(_state == STATE_WAITING_ON_OUTER_TASK, "We're in the wrong state!");
-            _state = STATE_WAITING_ON_INNER_TASK;
-            switch (task.Status)
-            {
-                case TaskStatus.Canceled:
-                case TaskStatus.Faulted:
-                    bool result = TrySetFromTask(task, _lookForOce);
-                    Debug.Assert(result, "Expected TrySetFromTask from outer task to succeed");
-                    break;
-                case TaskStatus.RanToCompletion:
-                    ProcessInnerTask(task is Task<Task<TResult>> taskOfTaskOfTResult ? // it's either a Task<Task> or Task<Task<TResult>>
-                        taskOfTaskOfTResult.Result : ((Task<Task>)task).Result);
-                    break;
-            }
-        }
-        private bool TrySetFromTask(Task task, bool lookForOce)
-        {
-            Debug.Assert(task != null && task.IsCompleted, "TrySetFromTask: Expected task to have completed.");
-            if (TplEventSource.Log.IsEnabled())
-                TplEventSource.Log.TraceOperationRelation(this.Id, CausalityRelation.Join);
-            bool result = false;
-            switch (task.Status)
-            {
-                case TaskStatus.Canceled:
-                    result = TrySetCanceled(task.CancellationToken, task.GetCancellationExceptionDispatchInfo());
-                    break;
-                case TaskStatus.Faulted:
-                    List<ExceptionDispatchInfo> edis = task.GetExceptionDispatchInfos();
-                    ExceptionDispatchInfo oceEdi;
-                    if (lookForOce && edis.Count > 0 &&
-                        (oceEdi = edis[0]) != null &&
-                        oceEdi.SourceException is OperationCanceledException oce)
-                    {
-                        result = TrySetCanceled(oce.CancellationToken, oceEdi);
-                    }
-                    else
-                    {
-                        result = TrySetException(edis);
-                    }
-                    break;
-                case TaskStatus.RanToCompletion:
-                    if (TplEventSource.Log.IsEnabled())
-                        TplEventSource.Log.TraceOperationEnd(this.Id, AsyncCausalityStatus.Completed);
-                    if (s_asyncDebuggingEnabled)
-                        RemoveFromActiveTasks(this);
-                    result = TrySetResult(task is Task<TResult> taskTResult ? taskTResult.Result : default);
-                    break;
-            }
-            return result;
-        }
-        private void ProcessInnerTask(Task? task)
-        {
-            if (task == null)
-            {
-                TrySetCanceled(default);
-                _state = STATE_DONE; // ... and record that we are done
-            }
-            else if (task.IsCompleted)
-            {
-                TrySetFromTask(task, lookForOce: false);
-                _state = STATE_DONE; // ... and record that we are done
-            }
-            else
-            {
-                task.AddCompletionAction(this);
-            }
-        }
-        public bool InvokeMayRunArbitraryCode => true;
-    }
-}

--- a/src/libraries/System.Private.CoreLib/src/System/Threading/ThreadPoolWorkQueue.cs
+++ b//dev/null
@@ -1,1320 +0,0 @@
-using System.Collections.Concurrent;
-using System.Collections.Generic;
-using System.Diagnostics;
-using System.Diagnostics.CodeAnalysis;
-using System.Diagnostics.Tracing;
-using System.Runtime.CompilerServices;
-using System.Runtime.InteropServices;
-using System.Runtime.Versioning;
-using System.Threading.Tasks;
-namespace System.Threading
-{
-    internal sealed partial class ThreadPoolWorkQueue
-    {
-        internal static class WorkStealingQueueList
-        {
-#pragma warning disable CA1825 // avoid the extra generic instantiation for Array.Empty<T>(); this is the only place we'll ever create this array
-            private static volatile WorkStealingQueue[] _queues = new WorkStealingQueue[0];
-#pragma warning restore CA1825
-            public static WorkStealingQueue[] Queues => _queues;
-            public static void Add(WorkStealingQueue queue)
-            {
-                Debug.Assert(queue != null);
-                while (true)
-                {
-                    WorkStealingQueue[] oldQueues = _queues;
-                    Debug.Assert(Array.IndexOf(oldQueues, queue) < 0);
-                    var newQueues = new WorkStealingQueue[oldQueues.Length + 1];
-                    Array.Copy(oldQueues, newQueues, oldQueues.Length);
-                    newQueues[^1] = queue;
-                    if (Interlocked.CompareExchange(ref _queues, newQueues, oldQueues) == oldQueues)
-                    {
-                        break;
-                    }
-                }
-            }
-            public static void Remove(WorkStealingQueue queue)
-            {
-                Debug.Assert(queue != null);
-                while (true)
-                {
-                    WorkStealingQueue[] oldQueues = _queues;
-                    if (oldQueues.Length == 0)
-                    {
-                        return;
-                    }
-                    int pos = Array.IndexOf(oldQueues, queue);
-                    if (pos < 0)
-                    {
-                        Debug.Fail("Should have found the queue");
-                        return;
-                    }
-                    var newQueues = new WorkStealingQueue[oldQueues.Length - 1];
-                    if (pos == 0)
-                    {
-                        Array.Copy(oldQueues, 1, newQueues, 0, newQueues.Length);
-                    }
-                    else if (pos == oldQueues.Length - 1)
-                    {
-                        Array.Copy(oldQueues, newQueues, newQueues.Length);
-                    }
-                    else
-                    {
-                        Array.Copy(oldQueues, newQueues, pos);
-                        Array.Copy(oldQueues, pos + 1, newQueues, pos, newQueues.Length - pos);
-                    }
-                    if (Interlocked.CompareExchange(ref _queues, newQueues, oldQueues) == oldQueues)
-                    {
-                        break;
-                    }
-                }
-            }
-        }
-        internal sealed class WorkStealingQueue
-        {
-            private const int INITIAL_SIZE = 32;
-            internal volatile object?[] m_array = new object[INITIAL_SIZE]; // SOS's ThreadPool command depends on this name
-            private volatile int m_mask = INITIAL_SIZE - 1;
-#if DEBUG
-            private const int START_INDEX = int.MaxValue;
-#else
-            private const int START_INDEX = 0;
-#endif
-            private volatile int m_headIndex = START_INDEX;
-            private volatile int m_tailIndex = START_INDEX;
-            private SpinLock m_foreignLock = new SpinLock(enableThreadOwnerTracking: false);
-            public void LocalPush(object obj)
-            {
-                int tail = m_tailIndex;
-                if (tail == int.MaxValue)
-                {
-                    tail = LocalPush_HandleTailOverflow();
-                }
-                if (tail < m_headIndex + m_mask)
-                {
-                    Volatile.Write(ref m_array[tail & m_mask], obj);
-                    m_tailIndex = tail + 1;
-                }
-                else
-                {
-                    bool lockTaken = false;
-                    try
-                    {
-                        m_foreignLock.Enter(ref lockTaken);
-                        int head = m_headIndex;
-                        int count = m_tailIndex - m_headIndex;
-                        if (count >= m_mask)
-                        {
-                            var newArray = new object?[m_array.Length << 1];
-                            for (int i = 0; i < m_array.Length; i++)
-                                newArray[i] = m_array[(i + head) & m_mask];
-                            m_array = newArray;
-                            m_headIndex = 0;
-                            m_tailIndex = tail = count;
-                            m_mask = (m_mask << 1) | 1;
-                        }
-                        Volatile.Write(ref m_array[tail & m_mask], obj);
-                        m_tailIndex = tail + 1;
-                    }
-                    finally
-                    {
-                        if (lockTaken)
-                            m_foreignLock.Exit(useMemoryBarrier: false);
-                    }
-                }
-            }
-            [MethodImpl(MethodImplOptions.NoInlining)]
-            private int LocalPush_HandleTailOverflow()
-            {
-                bool lockTaken = false;
-                try
-                {
-                    m_foreignLock.Enter(ref lockTaken);
-                    int tail = m_tailIndex;
-                    if (tail == int.MaxValue)
-                    {
-                        m_headIndex &= m_mask;
-                        m_tailIndex = tail = m_tailIndex & m_mask;
-                        Debug.Assert(m_headIndex <= m_tailIndex);
-                    }
-                    return tail;
-                }
-                finally
-                {
-                    if (lockTaken)
-                        m_foreignLock.Exit(useMemoryBarrier: true);
-                }
-            }
-            public bool LocalFindAndPop(object obj)
-            {
-                if (m_array[(m_tailIndex - 1) & m_mask] == obj)
-                {
-                    object? unused = LocalPop();
-                    Debug.Assert(unused == null || unused == obj);
-                    return unused != null;
-                }
-                for (int i = m_tailIndex - 2; i >= m_headIndex; i--)
-                {
-                    if (m_array[i & m_mask] == obj)
-                    {
-                        bool lockTaken = false;
-                        try
-                        {
-                            m_foreignLock.Enter(ref lockTaken);
-                            if (m_array[i & m_mask] == null)
-                                return false;
-                            Volatile.Write(ref m_array[i & m_mask], null);
-                            if (i == m_tailIndex)
-                                m_tailIndex--;
-                            else if (i == m_headIndex)
-                                m_headIndex++;
-                            return true;
-                        }
-                        finally
-                        {
-                            if (lockTaken)
-                                m_foreignLock.Exit(useMemoryBarrier: false);
-                        }
-                    }
-                }
-                return false;
-            }
-            public object? LocalPop() => m_headIndex < m_tailIndex ? LocalPopCore() : null;
-            private object? LocalPopCore()
-            {
-                while (true)
-                {
-                    int tail = m_tailIndex;
-                    if (m_headIndex >= tail)
-                    {
-                        return null;
-                    }
-                    tail--;
-                    Interlocked.Exchange(ref m_tailIndex, tail);
-                    if (m_headIndex <= tail)
-                    {
-                        int idx = tail & m_mask;
-                        object? obj = Volatile.Read(ref m_array[idx]);
-                        if (obj == null) continue;
-                        m_array[idx] = null;
-                        return obj;
-                    }
-                    else
-                    {
-                        bool lockTaken = false;
-                        try
-                        {
-                            m_foreignLock.Enter(ref lockTaken);
-                            if (m_headIndex <= tail)
-                            {
-                                int idx = tail & m_mask;
-                                object? obj = Volatile.Read(ref m_array[idx]);
-                                if (obj == null) continue;
-                                m_array[idx] = null;
-                                return obj;
-                            }
-                            else
-                            {
-                                m_tailIndex = tail + 1;
-                                return null;
-                            }
-                        }
-                        finally
-                        {
-                            if (lockTaken)
-                                m_foreignLock.Exit(useMemoryBarrier: false);
-                        }
-                    }
-                }
-            }
-            public bool CanSteal => m_headIndex < m_tailIndex;
-            public object? TrySteal(ref bool missedSteal)
-            {
-                while (true)
-                {
-                    if (CanSteal)
-                    {
-                        bool taken = false;
-                        try
-                        {
-                            m_foreignLock.TryEnter(ref taken);
-                            if (taken)
-                            {
-                                int head = m_headIndex;
-                                Interlocked.Exchange(ref m_headIndex, head + 1);
-                                if (head < m_tailIndex)
-                                {
-                                    int idx = head & m_mask;
-                                    object? obj = Volatile.Read(ref m_array[idx]);
-                                    if (obj == null) continue;
-                                    m_array[idx] = null;
-                                    return obj;
-                                }
-                                else
-                                {
-                                    m_headIndex = head;
-                                }
-                            }
-                        }
-                        finally
-                        {
-                            if (taken)
-                                m_foreignLock.Exit(useMemoryBarrier: false);
-                        }
-                        missedSteal = true;
-                    }
-                    return null;
-                }
-            }
-            public int Count
-            {
-                get
-                {
-                    bool lockTaken = false;
-                    try
-                    {
-                        m_foreignLock.Enter(ref lockTaken);
-                        return Math.Max(0, m_tailIndex - m_headIndex);
-                    }
-                    finally
-                    {
-                        if (lockTaken)
-                        {
-                            m_foreignLock.Exit(useMemoryBarrier: false);
-                        }
-                    }
-                }
-            }
-        }
-#if CORECLR
-        internal static readonly bool s_prioritizationExperiment =
-            AppContextConfigHelper.GetBooleanConfig(
-                "System.Threading.ThreadPool.PrioritizationExperiment",
-                "DOTNET_ThreadPool_PrioritizationExperiment",
-                defaultValue: false);
-#endif
-        private const int ProcessorsPerAssignableWorkItemQueue = 16;
-        private static readonly int s_assignableWorkItemQueueCount =
-            Environment.ProcessorCount <= 32 ? 0 :
-                (Environment.ProcessorCount + (ProcessorsPerAssignableWorkItemQueue - 1)) / ProcessorsPerAssignableWorkItemQueue;
-        private bool _loggingEnabled;
-        private bool _dispatchNormalPriorityWorkFirst;
-        private int _mayHaveHighPriorityWorkItems;
-        internal readonly ConcurrentQueue<object> workItems = new ConcurrentQueue<object>();
-        internal readonly ConcurrentQueue<object> highPriorityWorkItems = new ConcurrentQueue<object>();
-#if CORECLR
-        internal readonly ConcurrentQueue<object> lowPriorityWorkItems =
-            s_prioritizationExperiment ? new ConcurrentQueue<object>() : null!;
-#endif
-        internal readonly ConcurrentQueue<object>[] _assignableWorkItemQueues =
-            new ConcurrentQueue<object>[s_assignableWorkItemQueueCount];
-        private readonly LowLevelLock _queueAssignmentLock = new();
-        private readonly int[] _assignedWorkItemQueueThreadCounts =
-            s_assignableWorkItemQueueCount > 0 ? new int[s_assignableWorkItemQueueCount] : Array.Empty<int>();
-        [StructLayout(LayoutKind.Sequential)]
-        private struct CacheLineSeparated
-        {
-            private readonly Internal.PaddingFor32 pad1;
-            public int hasOutstandingThreadRequest;
-            private readonly Internal.PaddingFor32 pad2;
-        }
-        private CacheLineSeparated _separated;
-        public ThreadPoolWorkQueue()
-        {
-            for (int i = 0; i < s_assignableWorkItemQueueCount; i++)
-            {
-                _assignableWorkItemQueues[i] = new ConcurrentQueue<object>();
-            }
-            RefreshLoggingEnabled();
-        }
-        private void AssignWorkItemQueue(ThreadPoolWorkQueueThreadLocals tl)
-        {
-            Debug.Assert(s_assignableWorkItemQueueCount > 0);
-            _queueAssignmentLock.Acquire();
-            int queueIndex = -1;
-            int minCount = int.MaxValue;
-            int minCountQueueIndex = 0;
-            for (int i = 0; i < s_assignableWorkItemQueueCount; i++)
-            {
-                int count = _assignedWorkItemQueueThreadCounts[i];
-                Debug.Assert(count >= 0);
-                if (count < ProcessorsPerAssignableWorkItemQueue)
-                {
-                    queueIndex = i;
-                    _assignedWorkItemQueueThreadCounts[queueIndex] = count + 1;
-                    break;
-                }
-                if (count < minCount)
-                {
-                    minCount = count;
-                    minCountQueueIndex = i;
-                }
-            }
-            if (queueIndex < 0)
-            {
-                queueIndex = minCountQueueIndex;
-                _assignedWorkItemQueueThreadCounts[queueIndex]++;
-            }
-            _queueAssignmentLock.Release();
-            tl.queueIndex = queueIndex;
-            tl.assignedGlobalWorkItemQueue = _assignableWorkItemQueues[queueIndex];
-        }
-        private void TryReassignWorkItemQueue(ThreadPoolWorkQueueThreadLocals tl)
-        {
-            Debug.Assert(s_assignableWorkItemQueueCount > 0);
-            int queueIndex = tl.queueIndex;
-            if (queueIndex == 0)
-            {
-                return;
-            }
-            if (!_queueAssignmentLock.TryAcquire())
-            {
-                return;
-            }
-            Debug.Assert(_assignedWorkItemQueueThreadCounts[queueIndex] >= 0);
-            if (_assignedWorkItemQueueThreadCounts[queueIndex] > 1)
-            {
-                for (int i = 0; i < queueIndex; i++)
-                {
-                    if (_assignedWorkItemQueueThreadCounts[i] < ProcessorsPerAssignableWorkItemQueue)
-                    {
-                        _assignedWorkItemQueueThreadCounts[queueIndex]--;
-                        queueIndex = i;
-                        _assignedWorkItemQueueThreadCounts[queueIndex]++;
-                        break;
-                    }
-                }
-            }
-            _queueAssignmentLock.Release();
-            tl.queueIndex = queueIndex;
-            tl.assignedGlobalWorkItemQueue = _assignableWorkItemQueues[queueIndex];
-        }
-        private void UnassignWorkItemQueue(ThreadPoolWorkQueueThreadLocals tl)
-        {
-            Debug.Assert(s_assignableWorkItemQueueCount > 0);
-            int queueIndex = tl.queueIndex;
-            _queueAssignmentLock.Acquire();
-            int newCount = --_assignedWorkItemQueueThreadCounts[queueIndex];
-            _queueAssignmentLock.Release();
-            Debug.Assert(newCount >= 0);
-            if (newCount > 0)
-            {
-                return;
-            }
-            bool movedWorkItem = false;
-            ConcurrentQueue<object> queue = tl.assignedGlobalWorkItemQueue;
-            while (_assignedWorkItemQueueThreadCounts[queueIndex] <= 0 && queue.TryDequeue(out object? workItem))
-            {
-                workItems.Enqueue(workItem);
-                movedWorkItem = true;
-            }
-            if (movedWorkItem)
-            {
-                EnsureThreadRequested();
-            }
-        }
-        public ThreadPoolWorkQueueThreadLocals GetOrCreateThreadLocals() =>
-            ThreadPoolWorkQueueThreadLocals.threadLocals ?? CreateThreadLocals();
-        [MethodImpl(MethodImplOptions.NoInlining)]
-        private ThreadPoolWorkQueueThreadLocals CreateThreadLocals()
-        {
-            Debug.Assert(ThreadPoolWorkQueueThreadLocals.threadLocals == null);
-            return ThreadPoolWorkQueueThreadLocals.threadLocals = new ThreadPoolWorkQueueThreadLocals(this);
-        }
-        [MethodImpl(MethodImplOptions.AggressiveInlining)]
-        public void RefreshLoggingEnabled()
-        {
-            if (!FrameworkEventSource.Log.IsEnabled())
-            {
-                if (_loggingEnabled)
-                {
-                    _loggingEnabled = false;
-                }
-                return;
-            }
-            RefreshLoggingEnabledFull();
-        }
-        [MethodImpl(MethodImplOptions.NoInlining)]
-        public void RefreshLoggingEnabledFull()
-        {
-            _loggingEnabled = FrameworkEventSource.Log.IsEnabled(EventLevel.Verbose, FrameworkEventSource.Keywords.ThreadPool | FrameworkEventSource.Keywords.ThreadTransfer);
-        }
-        [MethodImpl(MethodImplOptions.AggressiveInlining)]
-        internal void EnsureThreadRequested()
-        {
-            if (Interlocked.CompareExchange(ref _separated.hasOutstandingThreadRequest, 1, 0) == 0)
-            {
-                ThreadPool.RequestWorkerThread();
-            }
-        }
-        [MethodImpl(MethodImplOptions.AggressiveInlining)]
-        internal void MarkThreadRequestSatisfied()
-        {
-            _separated.hasOutstandingThreadRequest = 0;
-            Interlocked.MemoryBarrier();
-        }
-        public void Enqueue(object callback, bool forceGlobal)
-        {
-            Debug.Assert((callback is IThreadPoolWorkItem) ^ (callback is Task));
-            if (_loggingEnabled && FrameworkEventSource.Log.IsEnabled())
-                FrameworkEventSource.Log.ThreadPoolEnqueueWorkObject(callback);
-#if CORECLR
-            if (s_prioritizationExperiment)
-            {
-                EnqueueForPrioritizationExperiment(callback, forceGlobal);
-            }
-            else
-#endif
-            {
-                ThreadPoolWorkQueueThreadLocals? tl;
-                if (!forceGlobal && (tl = ThreadPoolWorkQueueThreadLocals.threadLocals) != null)
-                {
-                    tl.workStealingQueue.LocalPush(callback);
-                }
-                else
-                {
-                    ConcurrentQueue<object> queue =
-                        s_assignableWorkItemQueueCount > 0 && (tl = ThreadPoolWorkQueueThreadLocals.threadLocals) != null
-                            ? tl.assignedGlobalWorkItemQueue
-                            : workItems;
-                    queue.Enqueue(callback);
-                }
-            }
-            EnsureThreadRequested();
-        }
-#if CORECLR
-        [MethodImpl(MethodImplOptions.NoInlining)]
-        private void EnqueueForPrioritizationExperiment(object callback, bool forceGlobal)
-        {
-            ThreadPoolWorkQueueThreadLocals? tl = ThreadPoolWorkQueueThreadLocals.threadLocals;
-            if (!forceGlobal && tl != null)
-            {
-                tl.workStealingQueue.LocalPush(callback);
-                return;
-            }
-            ConcurrentQueue<object> queue;
-            if (tl == null && callback is QueueUserWorkItemCallbackBase)
-            {
-                queue = lowPriorityWorkItems;
-            }
-            else if (s_assignableWorkItemQueueCount > 0 && tl != null)
-            {
-                queue = tl.assignedGlobalWorkItemQueue;
-            }
-            else
-            {
-                queue = workItems;
-            }
-            queue.Enqueue(callback);
-        }
-#endif
-        public void EnqueueAtHighPriority(object workItem)
-        {
-            Debug.Assert((workItem is IThreadPoolWorkItem) ^ (workItem is Task));
-            if (_loggingEnabled && FrameworkEventSource.Log.IsEnabled())
-                FrameworkEventSource.Log.ThreadPoolEnqueueWorkObject(workItem);
-            highPriorityWorkItems.Enqueue(workItem);
-            Volatile.Write(ref _mayHaveHighPriorityWorkItems, 1);
-            EnsureThreadRequested();
-        }
-        internal static void TransferAllLocalWorkItemsToHighPriorityGlobalQueue()
-        {
-            if (ThreadPoolWorkQueueThreadLocals.threadLocals is not ThreadPoolWorkQueueThreadLocals tl)
-            {
-                return;
-            }
-            ThreadPoolWorkQueue queue = ThreadPool.s_workQueue;
-            while (tl.workStealingQueue.LocalPop() is object workItem)
-            {
-                queue.highPriorityWorkItems.Enqueue(workItem);
-            }
-            Volatile.Write(ref queue._mayHaveHighPriorityWorkItems, 1);
-            queue.EnsureThreadRequested();
-        }
-        internal static bool LocalFindAndPop(object callback)
-        {
-            ThreadPoolWorkQueueThreadLocals? tl = ThreadPoolWorkQueueThreadLocals.threadLocals;
-            return tl != null && tl.workStealingQueue.LocalFindAndPop(callback);
-        }
-        public object? Dequeue(ThreadPoolWorkQueueThreadLocals tl, ref bool missedSteal)
-        {
-            object? workItem = tl.workStealingQueue.LocalPop();
-            if (workItem != null)
-            {
-                return workItem;
-            }
-            if (tl.isProcessingHighPriorityWorkItems)
-            {
-                if (highPriorityWorkItems.TryDequeue(out workItem))
-                {
-                    return workItem;
-                }
-                tl.isProcessingHighPriorityWorkItems = false;
-            }
-            else if (
-                _mayHaveHighPriorityWorkItems != 0 &&
-                Interlocked.CompareExchange(ref _mayHaveHighPriorityWorkItems, 0, 1) != 0 &&
-                TryStartProcessingHighPriorityWorkItemsAndDequeue(tl, out workItem))
-            {
-                return workItem;
-            }
-            if (s_assignableWorkItemQueueCount > 0 && tl.assignedGlobalWorkItemQueue.TryDequeue(out workItem))
-            {
-                return workItem;
-            }
-            if (workItems.TryDequeue(out workItem))
-            {
-                return workItem;
-            }
-            uint randomValue = tl.random.NextUInt32();
-            if (s_assignableWorkItemQueueCount > 0)
-            {
-                int queueIndex = tl.queueIndex;
-                int c = s_assignableWorkItemQueueCount;
-                int maxIndex = c - 1;
-                for (int i = (int)(randomValue % (uint)c); c > 0; i = i < maxIndex ? i + 1 : 0, c--)
-                {
-                    if (i != queueIndex && _assignableWorkItemQueues[i].TryDequeue(out workItem))
-                    {
-                        return workItem;
-                    }
-                }
-            }
-#if CORECLR
-            if (s_prioritizationExperiment && lowPriorityWorkItems.TryDequeue(out workItem))
-            {
-                return workItem;
-            }
-#endif
-            {
-                WorkStealingQueue localWsq = tl.workStealingQueue;
-                WorkStealingQueue[] queues = WorkStealingQueueList.Queues;
-                int c = queues.Length;
-                Debug.Assert(c > 0, "There must at least be a queue for this thread.");
-                int maxIndex = c - 1;
-                for (int i = (int)(randomValue % (uint)c); c > 0; i = i < maxIndex ? i + 1 : 0, c--)
-                {
-                    WorkStealingQueue otherQueue = queues[i];
-                    if (otherQueue != localWsq && otherQueue.CanSteal)
-                    {
-                        workItem = otherQueue.TrySteal(ref missedSteal);
-                        if (workItem != null)
-                        {
-                            return workItem;
-                        }
-                    }
-                }
-            }
-            return null;
-        }
-        [MethodImpl(MethodImplOptions.NoInlining)]
-        private bool TryStartProcessingHighPriorityWorkItemsAndDequeue(
-            ThreadPoolWorkQueueThreadLocals tl,
-            [MaybeNullWhen(false)] out object workItem)
-        {
-            Debug.Assert(!tl.isProcessingHighPriorityWorkItems);
-            if (!highPriorityWorkItems.TryDequeue(out workItem))
-            {
-                return false;
-            }
-            tl.isProcessingHighPriorityWorkItems = true;
-            _mayHaveHighPriorityWorkItems = 1;
-            return true;
-        }
-        public static long LocalCount
-        {
-            get
-            {
-                long count = 0;
-                foreach (WorkStealingQueue workStealingQueue in WorkStealingQueueList.Queues)
-                {
-                    count += workStealingQueue.Count;
-                }
-                return count;
-            }
-        }
-        public long GlobalCount
-        {
-            get
-            {
-                long count = (long)highPriorityWorkItems.Count + workItems.Count;
-#if CORECLR
-                if (s_prioritizationExperiment)
-                {
-                    count += lowPriorityWorkItems.Count;
-                }
-#endif
-                for (int i = 0; i < s_assignableWorkItemQueueCount; i++)
-                {
-                    count += _assignableWorkItemQueues[i].Count;
-                }
-                return count;
-            }
-        }
-        public const uint DispatchQuantumMs = 30;
-        internal static bool Dispatch()
-        {
-            ThreadPoolWorkQueue workQueue = ThreadPool.s_workQueue;
-            ThreadPoolWorkQueueThreadLocals tl = workQueue.GetOrCreateThreadLocals();
-            if (s_assignableWorkItemQueueCount > 0)
-            {
-                workQueue.AssignWorkItemQueue(tl);
-            }
-            workQueue.MarkThreadRequestSatisfied();
-            object? workItem = null;
-            {
-                bool dispatchNormalPriorityWorkFirst = workQueue._dispatchNormalPriorityWorkFirst;
-                if (dispatchNormalPriorityWorkFirst && !tl.workStealingQueue.CanSteal)
-                {
-                    workQueue._dispatchNormalPriorityWorkFirst = !dispatchNormalPriorityWorkFirst;
-                    ConcurrentQueue<object> queue =
-                        s_assignableWorkItemQueueCount > 0 ? tl.assignedGlobalWorkItemQueue : workQueue.workItems;
-                    if (!queue.TryDequeue(out workItem) && s_assignableWorkItemQueueCount > 0)
-                    {
-                        workQueue.workItems.TryDequeue(out workItem);
-                    }
-                }
-                if (workItem == null)
-                {
-                    bool missedSteal = false;
-                    workItem = workQueue.Dequeue(tl, ref missedSteal);
-                    if (workItem == null)
-                    {
-                        if (s_assignableWorkItemQueueCount > 0)
-                        {
-                            workQueue.UnassignWorkItemQueue(tl);
-                        }
-                        if (missedSteal)
-                        {
-                            workQueue.EnsureThreadRequested();
-                        }
-                        return true;
-                    }
-                }
-                workQueue.EnsureThreadRequested();
-            }
-            workQueue.RefreshLoggingEnabled();
-            object? threadLocalCompletionCountObject = tl.threadLocalCompletionCountObject;
-            Thread currentThread = tl.currentThread;
-            currentThread._executionContext = null;
-            currentThread._synchronizationContext = null;
-            int startTickCount = Environment.TickCount;
-            while (true)
-            {
-                if (workItem == null)
-                {
-                    bool missedSteal = false;
-                    workItem = workQueue.Dequeue(tl, ref missedSteal);
-                    if (workItem == null)
-                    {
-                        if (s_assignableWorkItemQueueCount > 0)
-                        {
-                            workQueue.UnassignWorkItemQueue(tl);
-                        }
-                        if (missedSteal)
-                        {
-                            workQueue.EnsureThreadRequested();
-                        }
-                        return true;
-                    }
-                }
-                if (workQueue._loggingEnabled && FrameworkEventSource.Log.IsEnabled())
-                {
-                    FrameworkEventSource.Log.ThreadPoolDequeueWorkObject(workItem);
-                }
-#if FEATURE_OBJCMARSHAL
-                if (AutoreleasePool.EnableAutoreleasePool)
-                {
-                    DispatchItemWithAutoreleasePool(workItem, currentThread);
-                }
-                else
-#endif
-#pragma warning disable CS0162 // Unreachable code detected. EnableWorkerTracking may be a constant in some runtimes.
-                if (ThreadPool.EnableWorkerTracking)
-                {
-                    DispatchWorkItemWithWorkerTracking(workItem, currentThread);
-                }
-                else
-                {
-                    DispatchWorkItem(workItem, currentThread);
-                }
-#pragma warning restore CS0162
-                workItem = null;
-                ExecutionContext.ResetThreadPoolThread(currentThread);
-                currentThread.ResetThreadPoolThread();
-                int currentTickCount = Environment.TickCount;
-                if (!ThreadPool.NotifyWorkItemComplete(threadLocalCompletionCountObject!, currentTickCount))
-                {
-                    tl.TransferLocalWork();
-                    tl.isProcessingHighPriorityWorkItems = false;
-                    if (s_assignableWorkItemQueueCount > 0)
-                    {
-                        workQueue.UnassignWorkItemQueue(tl);
-                    }
-                    return false;
-                }
-                if ((uint)(currentTickCount - startTickCount) < DispatchQuantumMs)
-                {
-                    continue;
-                }
-                if (ThreadPool.YieldFromDispatchLoop)
-                {
-                    tl.isProcessingHighPriorityWorkItems = false;
-                    if (s_assignableWorkItemQueueCount > 0)
-                    {
-                        workQueue.UnassignWorkItemQueue(tl);
-                    }
-                    return true;
-                }
-                if (s_assignableWorkItemQueueCount > 0)
-                {
-                    workQueue.TryReassignWorkItemQueue(tl);
-                }
-                startTickCount = currentTickCount;
-                workQueue.RefreshLoggingEnabled();
-            }
-        }
-        [MethodImpl(MethodImplOptions.NoInlining)]
-        private static void DispatchWorkItemWithWorkerTracking(object workItem, Thread currentThread)
-        {
-            Debug.Assert(ThreadPool.EnableWorkerTracking);
-            Debug.Assert(currentThread == Thread.CurrentThread);
-            bool reportedStatus = false;
-            try
-            {
-                ThreadPool.ReportThreadStatus(isWorking: true);
-                reportedStatus = true;
-                DispatchWorkItem(workItem, currentThread);
-            }
-            finally
-            {
-                if (reportedStatus)
-                    ThreadPool.ReportThreadStatus(isWorking: false);
-            }
-        }
-        [MethodImpl(MethodImplOptions.AggressiveInlining)]
-        private static void DispatchWorkItem(object workItem, Thread currentThread)
-        {
-            if (workItem is Task task)
-            {
-                task.ExecuteFromThreadPool(currentThread);
-            }
-            else
-            {
-                Debug.Assert(workItem is IThreadPoolWorkItem);
-                Unsafe.As<IThreadPoolWorkItem>(workItem).Execute();
-            }
-        }
-    }
-    internal sealed class ThreadPoolWorkQueueThreadLocals
-    {
-        [ThreadStatic]
-        public static ThreadPoolWorkQueueThreadLocals? threadLocals;
-        public bool isProcessingHighPriorityWorkItems;
-        public int queueIndex;
-        public ConcurrentQueue<object> assignedGlobalWorkItemQueue;
-        public readonly ThreadPoolWorkQueue workQueue;
-        public readonly ThreadPoolWorkQueue.WorkStealingQueue workStealingQueue;
-        public readonly Thread currentThread;
-        public readonly object? threadLocalCompletionCountObject;
-        public readonly Random.XoshiroImpl random = new Random.XoshiroImpl();
-        public ThreadPoolWorkQueueThreadLocals(ThreadPoolWorkQueue tpq)
-        {
-            assignedGlobalWorkItemQueue = tpq.workItems;
-            workQueue = tpq;
-            workStealingQueue = new ThreadPoolWorkQueue.WorkStealingQueue();
-            ThreadPoolWorkQueue.WorkStealingQueueList.Add(workStealingQueue);
-            currentThread = Thread.CurrentThread;
-            threadLocalCompletionCountObject = ThreadPool.GetOrCreateThreadLocalCompletionCountObject();
-        }
-        public void TransferLocalWork()
-        {
-            while (workStealingQueue.LocalPop() is object cb)
-            {
-                workQueue.Enqueue(cb, forceGlobal: true);
-            }
-        }
-        ~ThreadPoolWorkQueueThreadLocals()
-        {
-            if (null != workStealingQueue)
-            {
-                TransferLocalWork();
-                ThreadPoolWorkQueue.WorkStealingQueueList.Remove(workStealingQueue);
-            }
-        }
-    }
-    internal interface IThreadPoolTypedWorkItemQueueCallback<T>
-    {
-        static abstract void Invoke(T item);
-    }
-    internal sealed class ThreadPoolTypedWorkItemQueue<T, TCallback> : IThreadPoolWorkItem
-        where T : struct
-        where TCallback : struct, IThreadPoolTypedWorkItemQueueCallback<T>
-    {
-        private int _isScheduledForProcessing;
-        private readonly ConcurrentQueue<T> _workItems = new ConcurrentQueue<T>();
-        public int Count => _workItems.Count;
-        public void Enqueue(T workItem)
-        {
-            BatchEnqueue(workItem);
-            CompleteBatchEnqueue();
-        }
-        public void BatchEnqueue(T workItem) => _workItems.Enqueue(workItem);
-        public void CompleteBatchEnqueue() => ScheduleForProcessing();
-        private void ScheduleForProcessing()
-        {
-            if (Interlocked.CompareExchange(ref _isScheduledForProcessing, 1, 0) == 0)
-            {
-                ThreadPool.UnsafeQueueHighPriorityWorkItemInternal(this);
-            }
-        }
-        void IThreadPoolWorkItem.Execute()
-        {
-            Debug.Assert(_isScheduledForProcessing != 0);
-            _isScheduledForProcessing = 0;
-            Interlocked.MemoryBarrier();
-            if (!_workItems.TryDequeue(out T workItem))
-            {
-                return;
-            }
-            ScheduleForProcessing();
-            ThreadPoolWorkQueueThreadLocals tl = ThreadPoolWorkQueueThreadLocals.threadLocals!;
-            Debug.Assert(tl != null);
-            Thread currentThread = tl.currentThread;
-            Debug.Assert(currentThread == Thread.CurrentThread);
-            uint completedCount = 0;
-            int startTimeMs = Environment.TickCount;
-            while (true)
-            {
-                TCallback.Invoke(workItem);
-                if (++completedCount == uint.MaxValue ||
-                    tl.workStealingQueue.CanSteal ||
-                    (uint)(Environment.TickCount - startTimeMs) >= ThreadPoolWorkQueue.DispatchQuantumMs / 2 ||
-                    !_workItems.TryDequeue(out workItem))
-                {
-                    break;
-                }
-                ExecutionContext.ResetThreadPoolThread(currentThread);
-                currentThread.ResetThreadPoolThread();
-            }
-            ThreadInt64PersistentCounter.Add(tl.threadLocalCompletionCountObject!, completedCount);
-        }
-    }
-    public delegate void WaitCallback(object? state);
-    public delegate void WaitOrTimerCallback(object? state, bool timedOut);  // signaled or timed out
-    internal abstract class QueueUserWorkItemCallbackBase : IThreadPoolWorkItem
-    {
-#if DEBUG
-        private int executed;
-        ~QueueUserWorkItemCallbackBase()
-        {
-            Interlocked.MemoryBarrier(); // ensure that an old cached value is not read below
-            Debug.Assert(
-                executed != 0, "A QueueUserWorkItemCallback was never called!");
-        }
-#endif
-        public virtual void Execute()
-        {
-#if DEBUG
-            GC.SuppressFinalize(this);
-            Debug.Assert(
-                0 == Interlocked.Exchange(ref executed, 1),
-                "A QueueUserWorkItemCallback was called twice!");
-#endif
-        }
-    }
-    internal sealed class QueueUserWorkItemCallback : QueueUserWorkItemCallbackBase
-    {
-        private WaitCallback? _callback; // SOS's ThreadPool command depends on this name
-        private readonly object? _state;
-        private readonly ExecutionContext _context;
-        private static readonly Action<QueueUserWorkItemCallback> s_executionContextShim = quwi =>
-        {
-            Debug.Assert(quwi._callback != null);
-            WaitCallback callback = quwi._callback;
-            quwi._callback = null;
-            callback(quwi._state);
-        };
-        internal QueueUserWorkItemCallback(WaitCallback callback, object? state, ExecutionContext context)
-        {
-            Debug.Assert(context != null);
-            _callback = callback;
-            _state = state;
-            _context = context;
-        }
-        public override void Execute()
-        {
-            base.Execute();
-            ExecutionContext.RunForThreadPoolUnsafe(_context, s_executionContextShim, this);
-        }
-    }
-    internal sealed class QueueUserWorkItemCallback<TState> : QueueUserWorkItemCallbackBase
-    {
-        private Action<TState>? _callback; // SOS's ThreadPool command depends on this name
-        private readonly TState _state;
-        private readonly ExecutionContext _context;
-        internal QueueUserWorkItemCallback(Action<TState> callback, TState state, ExecutionContext context)
-        {
-            Debug.Assert(callback != null);
-            _callback = callback;
-            _state = state;
-            _context = context;
-        }
-        public override void Execute()
-        {
-            base.Execute();
-            Debug.Assert(_callback != null);
-            Action<TState> callback = _callback;
-            _callback = null;
-            ExecutionContext.RunForThreadPoolUnsafe(_context, callback, in _state);
-        }
-    }
-    internal sealed class QueueUserWorkItemCallbackDefaultContext : QueueUserWorkItemCallbackBase
-    {
-        private WaitCallback? _callback; // SOS's ThreadPool command depends on this name
-        private readonly object? _state;
-        internal QueueUserWorkItemCallbackDefaultContext(WaitCallback callback, object? state)
-        {
-            Debug.Assert(callback != null);
-            _callback = callback;
-            _state = state;
-        }
-        public override void Execute()
-        {
-            ExecutionContext.CheckThreadPoolAndContextsAreDefault();
-            base.Execute();
-            Debug.Assert(_callback != null);
-            WaitCallback callback = _callback;
-            _callback = null;
-            callback(_state);
-        }
-    }
-    internal sealed class QueueUserWorkItemCallbackDefaultContext<TState> : QueueUserWorkItemCallbackBase
-    {
-        private Action<TState>? _callback; // SOS's ThreadPool command depends on this name
-        private readonly TState _state;
-        internal QueueUserWorkItemCallbackDefaultContext(Action<TState> callback, TState state)
-        {
-            Debug.Assert(callback != null);
-            _callback = callback;
-            _state = state;
-        }
-        public override void Execute()
-        {
-            ExecutionContext.CheckThreadPoolAndContextsAreDefault();
-            base.Execute();
-            Debug.Assert(_callback != null);
-            Action<TState> callback = _callback;
-            _callback = null;
-            callback(_state);
-        }
-    }
-    internal sealed class _ThreadPoolWaitOrTimerCallback
-    {
-        private readonly WaitOrTimerCallback _waitOrTimerCallback;
-        private readonly ExecutionContext? _executionContext;
-        private readonly object? _state;
-        private static readonly ContextCallback _ccbt = new ContextCallback(WaitOrTimerCallback_Context_t);
-        private static readonly ContextCallback _ccbf = new ContextCallback(WaitOrTimerCallback_Context_f);
-        internal _ThreadPoolWaitOrTimerCallback(WaitOrTimerCallback waitOrTimerCallback, object? state, bool flowExecutionContext)
-        {
-            _waitOrTimerCallback = waitOrTimerCallback;
-            _state = state;
-            if (flowExecutionContext)
-            {
-                _executionContext = ExecutionContext.Capture();
-            }
-        }
-        private static void WaitOrTimerCallback_Context_t(object? state) =>
-            WaitOrTimerCallback_Context(state, timedOut: true);
-        private static void WaitOrTimerCallback_Context_f(object? state) =>
-            WaitOrTimerCallback_Context(state, timedOut: false);
-        private static void WaitOrTimerCallback_Context(object? state, bool timedOut)
-        {
-            _ThreadPoolWaitOrTimerCallback helper = (_ThreadPoolWaitOrTimerCallback)state!;
-            helper._waitOrTimerCallback(helper._state, timedOut);
-        }
-        internal static void PerformWaitOrTimerCallback(_ThreadPoolWaitOrTimerCallback helper, bool timedOut)
-        {
-            Debug.Assert(helper != null, "Null state passed to PerformWaitOrTimerCallback!");
-            ExecutionContext? context = helper._executionContext;
-            if (context == null)
-            {
-                WaitOrTimerCallback callback = helper._waitOrTimerCallback;
-                callback(helper._state, timedOut);
-            }
-            else
-            {
-                ExecutionContext.Run(context, timedOut ? _ccbt : _ccbf, helper);
-            }
-        }
-    }
-    public static partial class ThreadPool
-    {
-        internal const string WorkerThreadName = ".NET TP Worker";
-        internal static readonly ThreadPoolWorkQueue s_workQueue = new ThreadPoolWorkQueue();
-        internal static readonly Action<object?> s_invokeAsyncStateMachineBox = static state =>
-        {
-            if (state is IAsyncStateMachineBox box)
-            {
-                box.MoveNext();
-            }
-            else
-            {
-                ThrowHelper.ThrowUnexpectedStateForKnownCallback(state);
-            }
-        };
-        internal static bool EnableWorkerTracking => IsWorkerTrackingEnabledInConfig && EventSource.IsSupported;
-#if !FEATURE_WASM_THREADS
-        [UnsupportedOSPlatform("browser")]
-#endif
-        [CLSCompliant(false)]
-        public static RegisteredWaitHandle RegisterWaitForSingleObject(
-             WaitHandle waitObject,
-             WaitOrTimerCallback callBack,
-             object? state,
-             uint millisecondsTimeOutInterval,
-             bool executeOnlyOnce    // NOTE: we do not allow other options that allow the callback to be queued as an APC
-             )
-        {
-            if (millisecondsTimeOutInterval > (uint)int.MaxValue && millisecondsTimeOutInterval != uint.MaxValue)
-                throw new ArgumentOutOfRangeException(nameof(millisecondsTimeOutInterval), SR.ArgumentOutOfRange_LessEqualToIntegerMaxVal);
-            return RegisterWaitForSingleObject(waitObject, callBack, state, millisecondsTimeOutInterval, executeOnlyOnce, true);
-        }
-#if !FEATURE_WASM_THREADS
-        [UnsupportedOSPlatform("browser")]
-#endif
-        [CLSCompliant(false)]
-        public static RegisteredWaitHandle UnsafeRegisterWaitForSingleObject(
-             WaitHandle waitObject,
-             WaitOrTimerCallback callBack,
-             object? state,
-             uint millisecondsTimeOutInterval,
-             bool executeOnlyOnce    // NOTE: we do not allow other options that allow the callback to be queued as an APC
-             )
-        {
-            if (millisecondsTimeOutInterval > (uint)int.MaxValue && millisecondsTimeOutInterval != uint.MaxValue)
-                throw new ArgumentOutOfRangeException(nameof(millisecondsTimeOutInterval), SR.ArgumentOutOfRange_NeedNonNegOrNegative1);
-            return RegisterWaitForSingleObject(waitObject, callBack, state, millisecondsTimeOutInterval, executeOnlyOnce, false);
-        }
-#if !FEATURE_WASM_THREADS
-        [UnsupportedOSPlatform("browser")]
-#endif
-        public static RegisteredWaitHandle RegisterWaitForSingleObject(
-             WaitHandle waitObject,
-             WaitOrTimerCallback callBack,
-             object? state,
-             int millisecondsTimeOutInterval,
-             bool executeOnlyOnce    // NOTE: we do not allow other options that allow the callback to be queued as an APC
-             )
-        {
-            ArgumentOutOfRangeException.ThrowIfLessThan(millisecondsTimeOutInterval, -1);
-            return RegisterWaitForSingleObject(waitObject, callBack, state, (uint)millisecondsTimeOutInterval, executeOnlyOnce, true);
-        }
-#if !FEATURE_WASM_THREADS
-        [UnsupportedOSPlatform("browser")]
-#endif
-        public static RegisteredWaitHandle UnsafeRegisterWaitForSingleObject(
-             WaitHandle waitObject,
-             WaitOrTimerCallback callBack,
-             object? state,
-             int millisecondsTimeOutInterval,
-             bool executeOnlyOnce    // NOTE: we do not allow other options that allow the callback to be queued as an APC
-             )
-        {
-            ArgumentOutOfRangeException.ThrowIfLessThan(millisecondsTimeOutInterval, -1);
-            return RegisterWaitForSingleObject(waitObject, callBack, state, (uint)millisecondsTimeOutInterval, executeOnlyOnce, false);
-        }
-#if !FEATURE_WASM_THREADS
-        [UnsupportedOSPlatform("browser")]
-#endif
-        public static RegisteredWaitHandle RegisterWaitForSingleObject(
-            WaitHandle waitObject,
-            WaitOrTimerCallback callBack,
-            object? state,
-            long millisecondsTimeOutInterval,
-            bool executeOnlyOnce    // NOTE: we do not allow other options that allow the callback to be queued as an APC
-        )
-        {
-            ArgumentOutOfRangeException.ThrowIfLessThan(millisecondsTimeOutInterval, -1);
-            ArgumentOutOfRangeException.ThrowIfGreaterThan(millisecondsTimeOutInterval, int.MaxValue);
-            return RegisterWaitForSingleObject(waitObject, callBack, state, (uint)millisecondsTimeOutInterval, executeOnlyOnce, true);
-        }
-#if !FEATURE_WASM_THREADS
-        [UnsupportedOSPlatform("browser")]
-#endif
-        public static RegisteredWaitHandle UnsafeRegisterWaitForSingleObject(
-            WaitHandle waitObject,
-            WaitOrTimerCallback callBack,
-            object? state,
-            long millisecondsTimeOutInterval,
-            bool executeOnlyOnce    // NOTE: we do not allow other options that allow the callback to be queued as an APC
-        )
-        {
-            ArgumentOutOfRangeException.ThrowIfLessThan(millisecondsTimeOutInterval, -1);
-            ArgumentOutOfRangeException.ThrowIfGreaterThan(millisecondsTimeOutInterval, int.MaxValue);
-            return RegisterWaitForSingleObject(waitObject, callBack, state, (uint)millisecondsTimeOutInterval, executeOnlyOnce, false);
-        }
-#if !FEATURE_WASM_THREADS
-        [UnsupportedOSPlatform("browser")]
-#endif
-        public static RegisteredWaitHandle RegisterWaitForSingleObject(
-                          WaitHandle waitObject,
-                          WaitOrTimerCallback callBack,
-                          object? state,
-                          TimeSpan timeout,
-                          bool executeOnlyOnce
-                          )
-        {
-            long tm = (long)timeout.TotalMilliseconds;
-            ArgumentOutOfRangeException.ThrowIfLessThan(tm, -1, nameof(timeout));
-            ArgumentOutOfRangeException.ThrowIfGreaterThan(tm, int.MaxValue, nameof(timeout));
-            return RegisterWaitForSingleObject(waitObject, callBack, state, (uint)tm, executeOnlyOnce, true);
-        }
-#if !FEATURE_WASM_THREADS
-        [UnsupportedOSPlatform("browser")]
-#endif
-        public static RegisteredWaitHandle UnsafeRegisterWaitForSingleObject(
-                          WaitHandle waitObject,
-                          WaitOrTimerCallback callBack,
-                          object? state,
-                          TimeSpan timeout,
-                          bool executeOnlyOnce
-                          )
-        {
-            long tm = (long)timeout.TotalMilliseconds;
-            ArgumentOutOfRangeException.ThrowIfLessThan(tm, -1, nameof(timeout));
-            ArgumentOutOfRangeException.ThrowIfGreaterThan(tm, int.MaxValue, nameof(timeout));
-            return RegisterWaitForSingleObject(waitObject, callBack, state, (uint)tm, executeOnlyOnce, false);
-        }
-        public static bool QueueUserWorkItem(WaitCallback callBack) =>
-            QueueUserWorkItem(callBack, null);
-        public static bool QueueUserWorkItem(WaitCallback callBack, object? state)
-        {
-            if (callBack == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.callBack);
-            }
-            ExecutionContext? context = ExecutionContext.Capture();
-            object tpcallBack = (context == null || context.IsDefault) ?
-                new QueueUserWorkItemCallbackDefaultContext(callBack!, state) :
-                (object)new QueueUserWorkItemCallback(callBack!, state, context);
-            s_workQueue.Enqueue(tpcallBack, forceGlobal: true);
-            return true;
-        }
-        public static bool QueueUserWorkItem<TState>(Action<TState> callBack, TState state, bool preferLocal)
-        {
-            if (callBack == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.callBack);
-            }
-            ExecutionContext? context = ExecutionContext.Capture();
-            object tpcallBack = (context == null || context.IsDefault) ?
-                new QueueUserWorkItemCallbackDefaultContext<TState>(callBack!, state) :
-                (object)new QueueUserWorkItemCallback<TState>(callBack!, state, context);
-            s_workQueue.Enqueue(tpcallBack, forceGlobal: !preferLocal);
-            return true;
-        }
-        public static bool UnsafeQueueUserWorkItem<TState>(Action<TState> callBack, TState state, bool preferLocal)
-        {
-            if (callBack == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.callBack);
-            }
-            if (ReferenceEquals(callBack, s_invokeAsyncStateMachineBox))
-            {
-                if (!(state is IAsyncStateMachineBox))
-                {
-                    ThrowHelper.ThrowUnexpectedStateForKnownCallback(state);
-                }
-                UnsafeQueueUserWorkItemInternal((object)state!, preferLocal);
-                return true;
-            }
-            s_workQueue.Enqueue(
-                new QueueUserWorkItemCallbackDefaultContext<TState>(callBack!, state), forceGlobal: !preferLocal);
-            return true;
-        }
-        public static bool UnsafeQueueUserWorkItem(WaitCallback callBack, object? state)
-        {
-            if (callBack == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.callBack);
-            }
-            object tpcallBack = new QueueUserWorkItemCallbackDefaultContext(callBack!, state);
-            s_workQueue.Enqueue(tpcallBack, forceGlobal: true);
-            return true;
-        }
-        public static bool UnsafeQueueUserWorkItem(IThreadPoolWorkItem callBack, bool preferLocal)
-        {
-            if (callBack == null)
-            {
-                ThrowHelper.ThrowArgumentNullException(ExceptionArgument.callBack);
-            }
-            if (callBack is Task)
-            {
-                ThrowHelper.ThrowArgumentOutOfRangeException(ExceptionArgument.callBack);
-            }
-            UnsafeQueueUserWorkItemInternal(callBack!, preferLocal);
-            return true;
-        }
-        internal static void UnsafeQueueUserWorkItemInternal(object callBack, bool preferLocal) =>
-            s_workQueue.Enqueue(callBack, forceGlobal: !preferLocal);
-        internal static void UnsafeQueueHighPriorityWorkItemInternal(IThreadPoolWorkItem callBack) =>
-            s_workQueue.EnqueueAtHighPriority(callBack);
-        internal static bool TryPopCustomWorkItem(object workItem)
-        {
-            Debug.Assert(null != workItem);
-            return ThreadPoolWorkQueue.LocalFindAndPop(workItem);
-        }
-        internal static IEnumerable<object> GetQueuedWorkItems()
-        {
-            foreach (object workItem in s_workQueue.highPriorityWorkItems)
-            {
-                yield return workItem;
-            }
-            foreach (ConcurrentQueue<object> queue in s_workQueue._assignableWorkItemQueues)
-            {
-                foreach (object workItem in queue)
-                {
-                    yield return workItem;
-                }
-            }
-            foreach (object workItem in s_workQueue.workItems)
-            {
-                yield return workItem;
-            }
-#if CORECLR
-            if (ThreadPoolWorkQueue.s_prioritizationExperiment)
-            {
-                foreach (object workItem in s_workQueue.lowPriorityWorkItems)
-                {
-                    yield return workItem;
-                }
-            }
-#endif
-            foreach (ThreadPoolWorkQueue.WorkStealingQueue wsq in ThreadPoolWorkQueue.WorkStealingQueueList.Queues)
-            {
-                if (wsq != null && wsq.m_array != null)
-                {
-                    object?[] items = wsq.m_array;
-                    for (int i = 0; i < items.Length; i++)
-                    {
-                        object? item = items[i];
-                        if (item != null)
-                        {
-                            yield return item;
-                        }
-                    }
-                }
-            }
-        }
-        public static long PendingWorkItemCount
-        {
-            get
-            {
-                ThreadPoolWorkQueue workQueue = s_workQueue;
-                return ThreadPoolWorkQueue.LocalCount + workQueue.GlobalCount;
-            }
-        }
-    }
-}

--- a/src/mono/mono/component/debugger-engine.c
+++ b//dev/null
@@ -1,1442 +0,0 @@
-/**
- * \file
- * Debugger Engine shared code.
- *
- * Author:
- *   Zoltan Varga (vargaz@gmail.com)
- *   Rodrigo Kumpera (kumpera@gmail.com)
- *
- * Licensed under the MIT license. See LICENSE file in the project root for full license information.
- */
-#include <config.h>
-#include <mono/mini/mini-runtime.h>
-#if !defined (DISABLE_SDB) || defined(TARGET_WASM)
-#include <glib.h>
-#include <mono/mini/seq-points.h>
-#include <mono/mini/aot-runtime.h>
-#include "debugger-engine.h"
-#include "debugger-state-machine.h"
-#include <mono/metadata/debug-internals.h>
-static void mono_de_ss_start (SingleStepReq *ss_req, SingleStepArgs *ss_args);
-static gboolean mono_de_ss_update (SingleStepReq *req, MonoJitInfo *ji, SeqPoint *sp, void *tls, MonoContext *ctx, MonoMethod* method);
-static gpointer get_this_addr(DbgEngineStackFrame* the_frame);
-static MonoMethod* get_set_notification_method(MonoClass* async_builder_class);
-static DebuggerEngineCallbacks rt_callbacks;
-/*
- * Logging support
- */
-static int log_level;
-static FILE *log_file;
-static bool using_icordbg = FALSE;
-/*
- * Locking
- */
-#define dbg_lock() mono_coop_mutex_lock (&debug_mutex)
-#define dbg_unlock() mono_coop_mutex_unlock (&debug_mutex)
-static MonoCoopMutex debug_mutex;
-void
-mono_de_lock (void)
-{
-	dbg_lock ();
-}
-void
-mono_de_unlock (void)
-{
-	dbg_unlock ();
-}
-/*
- * Domain support
- */
-/* A hash table containing all active domains */
-/* Protected by the loader lock */
-static GHashTable *domains;
-static void
-domains_init (void)
-{
-	domains = g_hash_table_new (mono_aligned_addr_hash, NULL);
-}
-static void
-domains_cleanup (void)
-{
-}
-/*
- * mono_de_foreach_domain:
- *
- * Iterate over all domains under debugging. Caller must take the loader lock.
- *
- * FIXME can we move the locking to here? Callers in sdb must be properly audited.
- */
-void
-mono_de_foreach_domain (GHFunc func, gpointer user_data)
-{
-	g_hash_table_foreach (domains, func, user_data);
-}
-/*
- * LOCKING: Takes the loader lock
- */
-void
-mono_de_domain_add (MonoDomain *domain)
-{
-	mono_loader_lock ();
-	g_hash_table_insert (domains, domain, domain);
-	mono_loader_unlock ();
-}
-/*
- * BREAKPOINTS
- */
-/* List of breakpoints */
-/* Protected by the loader lock */
-static GPtrArray *breakpoints;
-/* Maps breakpoint locations to the number of breakpoints at that location */
-static GHashTable *bp_locs;
-static void
-breakpoints_init (void)
-{
-	breakpoints = g_ptr_array_new ();
-	bp_locs = g_hash_table_new (NULL, NULL);
-}
-/*
- * insert_breakpoint:
- *
- *   Insert the breakpoint described by BP into the method described by
- * JI.
- */
-static void
-insert_breakpoint (MonoSeqPointInfo *seq_points, MonoDomain *domain, MonoJitInfo *ji, MonoBreakpoint *bp, MonoError *error)
-{
-	int count;
-	BreakpointInstance *inst;
-	SeqPointIterator it;
-	gboolean it_has_sp = FALSE;
-	if (error)
-		error_init (error);
-	mono_seq_point_iterator_init (&it, seq_points);
-	while (mono_seq_point_iterator_next (&it)) {
-		if (it.seq_point.il_offset == bp->il_offset) {
-			it_has_sp = TRUE;
-			break;
-		}
-	}
-	if (!it_has_sp) {
-		/*
-		 * The set of IL offsets with seq points doesn't completely match the
-		 * info returned by CMD_METHOD_GET_DEBUG_INFO (#407).
-		 */
-		mono_seq_point_iterator_init (&it, seq_points);
-		while (mono_seq_point_iterator_next (&it)) {
-			if (it.seq_point.il_offset != METHOD_ENTRY_IL_OFFSET &&
-				it.seq_point.il_offset != METHOD_EXIT_IL_OFFSET &&
-				it.seq_point.il_offset + 1 == bp->il_offset) {
-				it_has_sp = TRUE;
-				break;
-			}
-		}
-	}
-	if (!it_has_sp && using_icordbg)
-	{
-		mono_seq_point_iterator_init (&it, seq_points);
-		while (mono_seq_point_iterator_next (&it)) {
-			if (it.seq_point.il_offset != METHOD_ENTRY_IL_OFFSET &&
-				it.seq_point.il_offset != METHOD_EXIT_IL_OFFSET &&
-				it.seq_point.il_offset > bp->il_offset) {
-				it_has_sp = TRUE;
-				break;
-			}
-		}
-	}
-	if (!it_has_sp) {
-		char *s = g_strdup_printf ("Unable to insert breakpoint at %s:%ld", mono_method_full_name (jinfo_get_method (ji), TRUE), bp->il_offset);
-		mono_seq_point_iterator_init (&it, seq_points);
-		while (mono_seq_point_iterator_next (&it))
-			PRINT_DEBUG_MSG (1, "%d\n", it.seq_point.il_offset);
-		if (error) {
-			mono_error_set_error (error, MONO_ERROR_GENERIC, "%s", s);
-			g_warning ("%s", s);
-			g_free (s);
-			return;
-		} else {
-			g_warning ("%s", s);
-			g_free (s);
-			return;
-		}
-	}
-	inst = g_new0 (BreakpointInstance, 1);
-	inst->il_offset = it.seq_point.il_offset;
-	inst->native_offset = it.seq_point.native_offset;
-	inst->ip = (guint8*)ji->code_start + it.seq_point.native_offset;
-	inst->ji = ji;
-	inst->domain = domain;
-	mono_loader_lock ();
-	g_ptr_array_add (bp->children, inst);
-	mono_loader_unlock ();
-	dbg_lock ();
-	count = GPOINTER_TO_INT (g_hash_table_lookup (bp_locs, inst->ip));
-	g_hash_table_insert (bp_locs, inst->ip, GINT_TO_POINTER (count + 1));
-	dbg_unlock ();
-	if (it.seq_point.native_offset == SEQ_POINT_NATIVE_OFFSET_DEAD_CODE) {
-		PRINT_DEBUG_MSG (1, "[dbg] Attempting to insert seq point at dead IL offset %d, ignoring.\n", (int)bp->il_offset);
-	} else if (count == 0) {
-		if (ji->is_interp) {
-			mini_get_interp_callbacks_api ()->set_breakpoint (ji, inst->ip);
-		} else {
-#ifdef MONO_ARCH_SOFT_DEBUG_SUPPORTED
-			mono_arch_set_breakpoint (ji, inst->ip);
-#else
-			NOT_IMPLEMENTED;
-#endif
-		}
-	}
-	PRINT_DEBUG_MSG (1, "[dbg] Inserted breakpoint at %s:[il=0x%x,native=0x%x] [%p](%d).\n", mono_method_full_name (jinfo_get_method (ji), TRUE), (int)it.seq_point.il_offset, (int)it.seq_point.native_offset, inst->ip, count);
-}
-static void
-remove_breakpoint (BreakpointInstance *inst)
-{
-	int count;
-	MonoJitInfo *ji = inst->ji;
-	guint8 *ip = inst->ip;
-	dbg_lock ();
-	count = GPOINTER_TO_INT (g_hash_table_lookup (bp_locs, ip));
-	g_hash_table_insert (bp_locs, ip, GINT_TO_POINTER (count - 1));
-	dbg_unlock ();
-	g_assert (count > 0);
-	if (count == 1 && inst->native_offset != SEQ_POINT_NATIVE_OFFSET_DEAD_CODE) {
-		if (ji->is_interp) {
-			mini_get_interp_callbacks_api ()->clear_breakpoint (ji, ip);
-		} else {
-#ifdef MONO_ARCH_SOFT_DEBUG_SUPPORTED
-			mono_arch_clear_breakpoint (ji, ip);
-#else
-			NOT_IMPLEMENTED;
-#endif
-		}
-		PRINT_DEBUG_MSG (1, "[dbg] Clear breakpoint at %s [%p].\n", mono_method_full_name (jinfo_get_method (ji), TRUE), ip);
-	}
-}
-/*
- * This doesn't take any locks.
- */
-static gboolean
-bp_matches_method (MonoBreakpoint *bp, MonoMethod *method)
-{
-	if (!bp->method)
-		return TRUE;
-	if (method == bp->method)
-		return TRUE;
-	if (method->is_inflated && ((MonoMethodInflated*)method)->declaring == bp->method)
-		return TRUE;
-	if (bp->method->is_inflated && method->is_inflated) {
-		MonoMethodInflated *bpimethod = (MonoMethodInflated*)bp->method;
-		MonoMethodInflated *imethod = (MonoMethodInflated*)method;
-		/* Open generic methods should match closed generic methods of the same class */
-		if (bpimethod->declaring == imethod->declaring && bpimethod->context.class_inst == imethod->context.class_inst && bpimethod->context.method_inst && bpimethod->context.method_inst->is_open) {
-			for (guint i = 0; i < bpimethod->context.method_inst->type_argc; ++i) {
-				MonoType *t1 = bpimethod->context.method_inst->type_argv [i];
-				/* FIXME: Handle !mvar */
-				if (t1->type != MONO_TYPE_MVAR)
-					return FALSE;
-			}
-			return TRUE;
-		}
-	}
-	return FALSE;
-}
-/*
- * mono_de_add_pending_breakpoints:
- *
- *   Insert pending breakpoints into the newly JITted method METHOD.
- */
-void
-mono_de_add_pending_breakpoints (MonoMethod *method, MonoJitInfo *ji)
-{
-	MonoSeqPointInfo *seq_points;
-	MonoDomain *domain;
-	if (!breakpoints)
-		return;
-	domain = mono_domain_get ();
-	mono_loader_lock ();
-	for (guint i = 0; i < breakpoints->len; ++i) {
-		MonoBreakpoint *bp = (MonoBreakpoint *)g_ptr_array_index (breakpoints, i);
-		gboolean found = FALSE;
-		if (!bp_matches_method (bp, method))
-			continue;
-		for (guint j = 0; j < bp->children->len; ++j) {
-			BreakpointInstance *inst = (BreakpointInstance *)g_ptr_array_index (bp->children, j);
-			if (inst->ji == ji)
-				found = TRUE;
-		}
-		if (!found) {
-			seq_points = (MonoSeqPointInfo *) ji->seq_points;
-			if (!seq_points) {
-				MonoMethod *jmethod = jinfo_get_method (ji);
-				if (jmethod->is_inflated) {
-					MonoJitInfo *seq_ji;
-					MonoMethod *declaring = mono_method_get_declaring_generic_method (jmethod);
-					mono_jit_search_all_backends_for_jit_info (declaring, &seq_ji);
-					seq_points = (MonoSeqPointInfo *) seq_ji->seq_points;
-				}
-			}
-			if (!seq_points)
-				/* Could be AOT code, or above "search_all_backends" call could have failed */
-				continue;
-			insert_breakpoint (seq_points, domain, ji, bp, NULL);
-		}
-	}
-	mono_loader_unlock ();
-}
-static void
-set_bp_in_method (MonoDomain *domain, MonoMethod *method, MonoSeqPointInfo *seq_points, MonoBreakpoint *bp, MonoError *error)
-{
-	MonoJitInfo *ji;
-	if (error)
-		error_init (error);
-	(void)mono_jit_search_all_backends_for_jit_info (method, &ji);
-	g_assert (ji);
-	insert_breakpoint (seq_points, domain, ji, bp, error);
-}
-typedef struct {
-	MonoBreakpoint *bp;
-	GPtrArray *methods;
-	GPtrArray *method_domains;
-	GPtrArray *method_seq_points;
-} CollectDomainData;
-static void
-collect_domain_bp (gpointer key, gpointer value, gpointer user_data)
-{
-	GHashTableIter iter;
-	MonoSeqPointInfo *seq_points;
-	MonoDomain *domain = (MonoDomain*)key;
-	CollectDomainData *ud = (CollectDomainData*)user_data;
-	MonoMethod *m;
-	MonoJitMemoryManager *jit_mm = get_default_jit_mm ();
-	jit_mm_lock (jit_mm);
-	g_hash_table_iter_init (&iter, jit_mm->seq_points);
-	while (g_hash_table_iter_next (&iter, (void**)&m, (void**)&seq_points)) {
-		if (bp_matches_method (ud->bp, m)) {
-			/* Save the info locally to simplify the code inside the domain lock */
-			g_ptr_array_add (ud->methods, m);
-			g_ptr_array_add (ud->method_domains, domain);
-			g_ptr_array_add (ud->method_seq_points, seq_points);
-		}
-	}
-	jit_mm_unlock (jit_mm);
-}
-/*
- * mono_de_set_breakpoint:
- *
- *   Set a breakpoint at IL_OFFSET in METHOD.
- * METHOD can be NULL, in which case a breakpoint is placed in all methods.
- * METHOD can also be a generic method definition, in which case a breakpoint
- * is placed in all instances of the method.
- * If ERROR is non-NULL, then it is set and NULL is returnd if some breakpoints couldn't be
- * inserted.
- */
-MonoBreakpoint*
-mono_de_set_breakpoint (MonoMethod *method, long il_offset, EventRequest *req, MonoError *error)
-{
-	MonoBreakpoint *bp;
-	MonoDomain *domain;
-	MonoMethod *m;
-	MonoSeqPointInfo *seq_points;
-	GPtrArray *methods;
-	GPtrArray *method_domains;
-	GPtrArray *method_seq_points;
-	if (error)
-		error_init (error);
-	bp = g_new0 (MonoBreakpoint, 1);
-	bp->method = method;
-	bp->il_offset = il_offset;
-	bp->req = req;
-	bp->children = g_ptr_array_new ();
-	PRINT_DEBUG_MSG  (1, "[dbg] Setting %sbreakpoint at %s:0x%x.\n", (req->event_kind == EVENT_KIND_STEP) ? "single step " : "", method ? mono_method_full_name (method, TRUE) : "<all>", (int)il_offset);
-	methods = g_ptr_array_new ();
-	method_domains = g_ptr_array_new ();
-	method_seq_points = g_ptr_array_new ();
-	mono_loader_lock ();
-	CollectDomainData user_data;
-	memset (&user_data, 0, sizeof (user_data));
-	user_data.bp = bp;
-	user_data.methods = methods;
-	user_data.method_domains = method_domains;
-	user_data.method_seq_points = method_seq_points;
-	mono_de_foreach_domain (collect_domain_bp, &user_data);
-	for (guint i = 0; i < methods->len; ++i) {
-		m = (MonoMethod *)g_ptr_array_index (methods, i);
-		domain = (MonoDomain *)g_ptr_array_index (method_domains, i);
-		seq_points = (MonoSeqPointInfo *)g_ptr_array_index (method_seq_points, i);
-		set_bp_in_method (domain, m, seq_points, bp, error);
-	}
-	if (methods->len == 0) 
-	{
-		MonoJitInfo *ji;
-		(void)mono_jit_search_all_backends_for_jit_info (method, &ji);
-		if (ji && ji->seq_points)
-			set_bp_in_method (mono_get_root_domain (), method, ji->seq_points, bp, error);
-	}
-	g_ptr_array_add (breakpoints, bp);
-	mono_debugger_log_add_bp (bp, bp->method, bp->il_offset);
-	mono_loader_unlock ();
-	g_ptr_array_free (methods, TRUE);
-	g_ptr_array_free (method_domains, TRUE);
-	g_ptr_array_free (method_seq_points, TRUE);
-	if (error && !is_ok (error)) {
-		mono_de_clear_breakpoint (bp);
-		return NULL;
-	}
-	return bp;
-}
-void
-mono_de_clear_breakpoint (MonoBreakpoint *bp)
-{
-	for (guint i = 0; i < bp->children->len; ++i) {
-		BreakpointInstance *inst = (BreakpointInstance *)g_ptr_array_index (bp->children, i);
-		remove_breakpoint (inst);
-		g_free (inst);
-	}
-	mono_loader_lock ();
-	mono_debugger_log_remove_bp (bp, bp->method, bp->il_offset);
-	g_ptr_array_remove (breakpoints, bp);
-	mono_loader_unlock ();
-	g_ptr_array_free (bp->children, TRUE);
-	g_free (bp);
-}
-void
-mono_de_collect_breakpoints_by_sp (SeqPoint *sp, MonoJitInfo *ji, GPtrArray *ss_reqs, GPtrArray *bp_reqs)
-{
-	for (guint i = 0; i < breakpoints->len; ++i) {
-		MonoBreakpoint *bp = (MonoBreakpoint *)g_ptr_array_index (breakpoints, i);
-		if (!bp->method)
-			continue;
-		for (guint j = 0; j < bp->children->len; ++j) {
-			BreakpointInstance *inst = (BreakpointInstance *)g_ptr_array_index (bp->children, j);
-			if (inst->ji == ji && inst->il_offset == sp->il_offset && inst->native_offset == sp->native_offset) {
-				if (bp->req->event_kind == EVENT_KIND_STEP) {
-					if (ss_reqs)
-						g_ptr_array_add (ss_reqs, bp->req);
-				} else {
-					if (bp_reqs)
-						g_ptr_array_add (bp_reqs, bp->req);
-				}
-			}
-		}
-		}
-}
-static void
-breakpoints_cleanup (void)
-{
-	mono_loader_lock ();
-	for (guint i = 0; i < breakpoints->len; ++i)
-		g_free (g_ptr_array_index (breakpoints, i));
-	g_ptr_array_free (breakpoints, TRUE);
-	g_hash_table_destroy (bp_locs);
-	breakpoints = NULL;
-	bp_locs = NULL;
-	mono_loader_unlock ();
-}
-/*
- * mono_de_clear_breakpoints_for_domain:
- *
- *   Clear breakpoint instances which reference DOMAIN.
- */
-void
-mono_de_clear_breakpoints_for_domain (MonoDomain *domain)
-{
-	/* This could be called after shutdown */
-	if (!breakpoints)
-		return;
-	mono_loader_lock ();
-	for (guint i = 0; i < breakpoints->len; ++i) {
-		MonoBreakpoint *bp = (MonoBreakpoint *)g_ptr_array_index (breakpoints, i);
-		guint j = 0;
-		while (j < bp->children->len) {
-			BreakpointInstance *inst = (BreakpointInstance *)g_ptr_array_index (bp->children, j);
-			if (inst->domain == domain) {
-				remove_breakpoint (inst);
-				g_free (inst);
-				g_ptr_array_remove_index_fast (bp->children, j);
-			} else {
-				j ++;
-			}
-		}
-	}
-	mono_loader_unlock ();
-}
-/* Single stepping engine */
-/* Number of single stepping operations in progress */
-static int ss_count;
-/* The single step request instances */
-static GPtrArray *the_ss_reqs;
-static void
-ss_req_init (void)
-{
-	the_ss_reqs = g_ptr_array_new ();
-}
-static void
-ss_req_cleanup (void)
-{
-	dbg_lock ();
-	g_ptr_array_free (the_ss_reqs, TRUE);
-	the_ss_reqs = NULL;
-	dbg_unlock ();
-}
-/*
- * mono_de_start_single_stepping:
- *
- *   Turn on single stepping. Can be called multiple times, for example,
- * by a single step event request + a suspend.
- */
-void
-mono_de_start_single_stepping (void)
-{
-		int val = mono_atomic_inc_i32 (&ss_count);
-	if (val == 1) {
-		#ifdef MONO_ARCH_SOFT_DEBUG_SUPPORTED
-		mono_arch_start_single_stepping ();
-#endif
-		mini_get_interp_callbacks_api ()->start_single_stepping ();
-	}
-}
-void
-mono_de_stop_single_stepping (void)
-{
-		int val = mono_atomic_dec_i32 (&ss_count);
-	if (val == 0) {
-		#ifdef MONO_ARCH_SOFT_DEBUG_SUPPORTED
-		mono_arch_stop_single_stepping ();
-#endif
-		mini_get_interp_callbacks_api ()->stop_single_stepping ();
-	}
-}
-static MonoJitInfo*
-get_top_method_ji (gpointer ip, MonoDomain **domain, gpointer *out_ip)
-{
-	MonoJitInfo *ji;
-	if (out_ip)
-		*out_ip = ip;
-	if (domain)
-		*domain = mono_get_root_domain ();
-	ji = mini_jit_info_table_find (ip);
-	if (!ji) {
-		/* Could be an interpreter method */
-		MonoLMF *lmf = mono_get_lmf ();
-		MonoInterpFrameHandle *frame;
-		g_assert (((gsize)lmf->previous_lmf) & 2);
-		MonoLMFExt *ext = (MonoLMFExt*)lmf;
-		g_assert (ext->kind == MONO_LMFEXT_INTERP_EXIT || ext->kind == MONO_LMFEXT_INTERP_EXIT_WITH_CTX);
-		frame = (MonoInterpFrameHandle*)ext->interp_exit_data;
-		ji = mini_get_interp_callbacks_api ()->frame_get_jit_info (frame);
-		if (domain)
-			*domain = mono_domain_get ();
-		if (out_ip)
-			*out_ip = mini_get_interp_callbacks_api ()->frame_get_ip (frame);
-	}
-	return ji;
-}
-static void
-no_seq_points_found (MonoMethod *method, int offset)
-{
-	/*
-	 * This can happen in full-aot mode with assemblies AOTed without the 'soft-debug' option to save space.
-	 */
-	PRINT_MSG ("Unable to find seq points for method '%s', offset 0x%x.\n", mono_method_full_name (method, TRUE), offset);
-}
-static const char*
-ss_depth_to_string (StepDepth depth)
-{
-	switch (depth) {
-	case STEP_DEPTH_OVER:
-		return "over";
-	case STEP_DEPTH_OUT:
-		return "out";
-	case STEP_DEPTH_INTO:
-		return "into";
-	default:
-		g_assert_not_reached ();
-		return NULL;
-	}
-}
-/*
- * ss_stop:
- *
- *   Stop the single stepping operation given by SS_REQ.
- */
-static void
-ss_stop (SingleStepReq *ss_req)
-{
-	if (ss_req->bps) {
-		GSList *l;
-		for (l = ss_req->bps; l; l = l->next) {
-			mono_de_clear_breakpoint ((MonoBreakpoint *)l->data);
-		}
-		g_slist_free (ss_req->bps);
-		ss_req->bps = NULL;
-	}
-	ss_req->async_id = 0;
-	ss_req->async_stepout_method = NULL;
-	if (ss_req->global) {
-		mono_de_stop_single_stepping ();
-		ss_req->global = FALSE;
-	}
-}
-static void
-ss_destroy (SingleStepReq *req)
-{
-	PRINT_DEBUG_MSG (1, "[dbg] ss_destroy.\n");
-	ss_stop (req);
-	g_free (req);
-}
-static SingleStepReq*
-ss_req_acquire (MonoInternalThread *thread)
-{
-	SingleStepReq *req = NULL;
-	dbg_lock ();
-	for (guint i = 0; i < the_ss_reqs->len; ++i) {
-		SingleStepReq *current_req = (SingleStepReq *)g_ptr_array_index (the_ss_reqs, i);
-		if (current_req->thread == thread) {
-			current_req->refcount ++;
-			req = current_req;
-		}
-	}
-	dbg_unlock ();
-	return req;
-}
-static int
-ss_req_count (void)
-{
-	return the_ss_reqs->len;
-}
-static void
-mono_de_ss_req_release (SingleStepReq *req)
-{
-	gboolean free = FALSE;
-	dbg_lock ();
-	g_assert (req->refcount);
-	req->refcount --;
-	if (req->refcount == 0)
-		free = TRUE;
-	if (free) {
-		g_ptr_array_remove (the_ss_reqs, req);
-		ss_destroy (req);
-	}
-	dbg_unlock ();
-}
-void
-mono_de_cancel_ss (SingleStepReq *req)
-{
-	if (the_ss_reqs) {
-		mono_de_ss_req_release (req);
-	}
-}
-void
-mono_de_cancel_all_ss (void)
-{
-	for (guint i = 0; i < the_ss_reqs->len; ++i) {
-		SingleStepReq *current_req = (SingleStepReq *)g_ptr_array_index (the_ss_reqs, i);
-		mono_de_ss_req_release (current_req);
-	}
-}
-void
-mono_de_process_single_step (void *tls, gboolean from_signal)
-{
-	MonoJitInfo *ji;
-	guint8 *ip;
-	GPtrArray *reqs;
-	int il_offset;
-	MonoDomain *domain;
-	MonoContext *ctx = rt_callbacks.tls_get_restore_state (tls);
-	MonoMethod *method;
-	SeqPoint sp;
-	MonoSeqPointInfo *info;
-	SingleStepReq *ss_req;
-	/* Skip the instruction causing the single step */
-	rt_callbacks.begin_single_step_processing (ctx, from_signal);
-	if (rt_callbacks.try_process_suspend (tls, ctx, FALSE))
-		return;
-	/*
-	 * This can run concurrently with a clear_event_request () call, so needs locking/reference counts.
-	 */
-	ss_req = ss_req_acquire (mono_thread_internal_current ());
-	if (!ss_req)
-		return;
-	ip = (guint8 *)MONO_CONTEXT_GET_IP (ctx);
-	ji = get_top_method_ji (ip, &domain, (gpointer*)&ip);
-	g_assert (ji && !ji->is_trampoline);
-	if (log_level > 0) {
-		PRINT_DEBUG_MSG (1, "[%p] Single step event (depth=%s) at %s (%p)[0x%x], sp %p, last sp %p\n", (gpointer) (gsize) mono_native_thread_id_get (), ss_depth_to_string (ss_req->depth), mono_method_full_name (jinfo_get_method (ji), TRUE), MONO_CONTEXT_GET_IP (ctx), (int)((guint8*)MONO_CONTEXT_GET_IP (ctx) - (guint8*)ji->code_start), MONO_CONTEXT_GET_SP (ctx), ss_req->last_sp);
-	}
-	method = jinfo_get_method (ji);
-	g_assert (method);
-	if (method->wrapper_type && method->wrapper_type != MONO_WRAPPER_DYNAMIC_METHOD)
-		goto exit;
-	/*
-	 * FIXME:
-	 * Stopping in memset makes half-initialized vtypes visible.
-	 * Stopping in memcpy makes half-copied vtypes visible.
-	 */
-	if (method->klass == mono_get_string_class () && (!strcmp (method->name, "memset") || strstr (method->name, "memcpy")))
-		goto exit;
-	/*
-	 * This could be in mono_de_ss_update method, but mono_find_next_seq_point_for_native_offset is pretty expensive method,
-	 * hence we prefer this check here.
-	 */
-	if (ss_req->user_assemblies) {
-		gboolean found = FALSE;
-		for (int k = 0; ss_req->user_assemblies[k]; k++)
-			if (ss_req->user_assemblies[k] == m_class_get_image (method->klass)->assembly) {
-				found = TRUE;
-				break;
-			}
-		if (!found)
-			goto exit;
-	}
-	/*
-	 * The ip points to the instruction causing the single step event, which is before
-	 * the offset recorded in the seq point map, so find the next seq point after ip.
-	 */
-	if (!mono_find_next_seq_point_for_native_offset (method, GPTRDIFF_TO_INT32 ((guint8*)ip - (guint8*)ji->code_start), &info, &sp)) {
-		g_assert_not_reached ();
-		goto exit;
-	}
-	il_offset = sp.il_offset;
-	if (!mono_de_ss_update (ss_req, ji, &sp, tls, ctx, method))
-		goto exit;
-	/* Start single stepping again from the current sequence point */
-	SingleStepArgs args;
-	memset (&args, 0, sizeof (args));
-	args.method = method;
-	args.ctx = ctx;
-	args.tls = tls;
-	args.step_to_catch = FALSE;
-	args.sp = sp;
-	args.info = info;
-	args.frames = NULL;
-	args.nframes = 0;
-	mono_de_ss_start (ss_req, &args);
-	if ((ss_req->filter & STEP_FILTER_STATIC_CTOR) &&
-		(method->flags & METHOD_ATTRIBUTE_SPECIAL_NAME) &&
-		!strcmp (method->name, ".cctor"))
-		goto exit;
-	reqs = g_ptr_array_new ();
-	mono_loader_lock ();
-	g_ptr_array_add (reqs, ss_req->req);
-	void *bp_events;
-	bp_events = mono_dbg_create_breakpoint_events (reqs, NULL, ji, EVENT_KIND_BREAKPOINT);
-	g_ptr_array_free (reqs, TRUE);
-	mono_loader_unlock ();
-	mono_dbg_process_breakpoint_events (bp_events, method, ctx, il_offset);
- exit:
-	mono_de_ss_req_release (ss_req);
-}
-/*
- * mono_de_ss_update:
- *
- * Return FALSE if single stepping needs to continue.
- */
-static gboolean
-mono_de_ss_update (SingleStepReq *req, MonoJitInfo *ji, SeqPoint *sp, void *tls, MonoContext *ctx, MonoMethod* method)
-{
-	MonoDebugMethodInfo *minfo;
-	MonoDebugSourceLocation *loc = NULL;
-	gboolean hit = TRUE;
-	if ((req->filter & STEP_FILTER_STATIC_CTOR)) {
-		DbgEngineStackFrame **frames;
-		int nframes;
-		rt_callbacks.ss_calculate_framecount (tls, ctx, TRUE, &frames, &nframes);
-		gboolean ret = FALSE;
-		gboolean method_in_stack = FALSE;
-		for (int i = 0; i < nframes; i++) {
-			MonoMethod *external_method = frames [i]->method;
-			if (method == external_method)
-				method_in_stack = TRUE;
-			if (!ret) {
-				ret = (external_method->flags & METHOD_ATTRIBUTE_SPECIAL_NAME);
-				ret = ret && !strcmp (external_method->name, ".cctor");
-				ret = ret && (external_method != req->start_method);
-			}
-		}
-		if (!method_in_stack) {
-			PRINT_ERROR_MSG ("[%p] The instruction pointer of the currently executing method(%s) is not on the recorded stack. This is likely due to a runtime bug. The %d frames are as follow: \n", (gpointer)(gsize)mono_native_thread_id_get (), mono_method_full_name (method, TRUE), nframes);
-			/*PRINT_DEBUG_MSG (1, "[%p] The instruction pointer of the currently executing method(%s) is not on the recorded stack. This is likely due to a runtime bug. The %d frames are as follow: \n", (gpointer)(gsize)mono_native_thread_id_get (), mono_method_full_name (method, TRUE), tls->frame_count);*/
-			for (int i=0; i < nframes; i++)
-				PRINT_ERROR_MSG ("\t [%p] Frame (%d / %d): %s\n", (gpointer)(gsize)mono_native_thread_id_get (), i, nframes, mono_method_full_name (frames [i]->method, TRUE));
-		}
-		rt_callbacks.ss_discard_frame_context (tls);
-		if (ret)
-			return FALSE;
-	}
-	if (req->async_stepout_method == method) {
-		PRINT_DEBUG_MSG (1, "[%p] Breakpoint hit during async step-out at %s hit, continuing stepping out.\n", (gpointer)(gsize)mono_native_thread_id_get (), method->name);
-		return FALSE;
-	}
-	if (req->depth == STEP_DEPTH_OVER && (sp->flags & MONO_SEQ_POINT_FLAG_NONEMPTY_STACK) && !(sp->flags & MONO_SEQ_POINT_FLAG_NESTED_CALL) && req->start_method == method) {
-		/*
-		 * These seq points are inserted by the JIT after calls, step over needs to skip them.
-		 */
-		PRINT_DEBUG_MSG (1, "[%p] Seq point at nonempty stack %x while stepping over, continuing single stepping.\n", (gpointer) (gsize) mono_native_thread_id_get (), sp->il_offset);
-		return FALSE;
-	}
-	if ((req->depth == STEP_DEPTH_OVER || req->depth == STEP_DEPTH_OUT) && hit && !req->async_stepout_method) {
-		gboolean is_step_out = req->depth == STEP_DEPTH_OUT;
-		int nframes;
-		rt_callbacks.ss_calculate_framecount (tls, ctx, FALSE, NULL, &nframes);
-		int target_frames = req->nframes + (is_step_out ? -1 : 0);
-		if (req->nframes > 0 && nframes > 0 && nframes > target_frames) {
-			/* Hit the breakpoint in a recursive call, don't halt */
-			PRINT_DEBUG_MSG (1, "[%p] Breakpoint at lower frame while stepping %s, continuing single stepping.\n", (gpointer) (gsize) mono_native_thread_id_get (), is_step_out ? "out" : "over");
-			return FALSE;
-		}
-	}
-	if (req->depth == STEP_DEPTH_INTO && req->size == STEP_SIZE_MIN && (sp->flags & MONO_SEQ_POINT_FLAG_NONEMPTY_STACK) && req->start_method) {
-		int nframes;
-		rt_callbacks.ss_calculate_framecount (tls, ctx, FALSE, NULL, &nframes);
-		if (req->start_method == method && req->nframes && nframes == req->nframes) { //Check also frame count(could be recursion)
-			PRINT_DEBUG_MSG (1, "[%p] Seq point at nonempty stack %x while stepping in, continuing single stepping.\n", (gpointer) (gsize) mono_native_thread_id_get (), sp->il_offset);
-			return FALSE;
-		}
-	}
-	MonoDebugMethodAsyncInfo* async_method = mono_debug_lookup_method_async_debug_info (method);
-	if (async_method) {
-		for (int i = 0; i < async_method->num_awaits; i++) {
-			if (async_method->yield_offsets[i] == sp->il_offset || async_method->resume_offsets[i] == sp->il_offset) {
-				mono_debug_free_method_async_debug_info (async_method);
-				return FALSE;
-			}
-		}
-		mono_debug_free_method_async_debug_info (async_method);
-	}
-	if (req->size != STEP_SIZE_LINE_COLUMN)
-		return TRUE;
-	/* Have to check whenever a different source line was reached */
-	minfo = mono_debug_lookup_method (method);
-	if (minfo)
-		loc = mono_debug_method_lookup_location (minfo, sp->il_offset);
-	if (!loc) { //we should not continue single stepping because the client side can have symbols loaded dynamically
-		PRINT_DEBUG_MSG (1, "[%p] No line number info for il offset %x, don't know if it's in the same line single stepping.\n", (gpointer) (gsize) mono_native_thread_id_get (), sp->il_offset);
-		req->last_method = method;
-		req->last_line = -1;
-		req->last_column = -1;
-		return hit;
-	} else if (loc && method == req->last_method && loc->row == req->last_line && loc->column == req->last_column) {
-		int nframes;
-		rt_callbacks.ss_calculate_framecount (tls, ctx, FALSE, NULL, &nframes);
-		if (nframes == req->nframes) { // If the frame has changed we're clearly not on the same source line.
-			PRINT_DEBUG_MSG (1, "[%p] Same source line (%d), continuing single stepping.\n", (gpointer) (gsize) mono_native_thread_id_get (), loc->row);
-			hit = FALSE;
-		}
-	}
-	if (loc) {
-		req->last_method = method;
-		req->last_line = loc->row;
-		req->last_column = loc->column;
-		mono_debug_free_source_location (loc);
-	}
-	return hit;
-}
-void
-mono_de_process_breakpoint (void *void_tls, gboolean from_signal)
-{
-	DebuggerTlsData *tls = (DebuggerTlsData*)void_tls;
-	MonoJitInfo *ji;
-	guint8 *ip;
-	guint32 native_offset;
-	GPtrArray *bp_reqs, *ss_reqs_orig, *ss_reqs;
-	EventKind kind = EVENT_KIND_BREAKPOINT;
-	MonoContext *ctx = rt_callbacks.tls_get_restore_state (tls);
-	MonoMethod *method;
-	MonoSeqPointInfo *info;
-	SeqPoint sp;
-	gboolean found_sp;
-	if (rt_callbacks.try_process_suspend (tls, ctx, TRUE))
-		return;
-	ip = (guint8 *)MONO_CONTEXT_GET_IP (ctx);
-	ji = get_top_method_ji (ip, NULL, (gpointer*)&ip);
-	g_assert (ji && !ji->is_trampoline);
-	method = jinfo_get_method (ji);
-	/* Compute the native offset of the breakpoint from the ip */
-	native_offset = GPTRDIFF_TO_UINT32 (ip - (guint8*)ji->code_start);
-	if (!rt_callbacks.begin_breakpoint_processing (tls, ctx, ji, from_signal))
-		return;
-	if (method->wrapper_type)
-		return;
-	bp_reqs = g_ptr_array_new ();
-	ss_reqs = g_ptr_array_new ();
-	ss_reqs_orig = g_ptr_array_new ();
-	mono_loader_lock ();
-	/*
-	 * The ip points to the instruction causing the breakpoint event, which is after
-	 * the offset recorded in the seq point map, so find the prev seq point before ip.
-	 */
-	found_sp = mono_find_prev_seq_point_for_native_offset (method, native_offset, &info, &sp);
-	if (!found_sp)
-		no_seq_points_found (method, native_offset);
-	g_assert (found_sp);
-	PRINT_DEBUG_MSG (1, "[%p] Breakpoint hit, method=%s, ip=%p, [il=0x%x,native=0x%x].\n", (gpointer) (gsize) mono_native_thread_id_get (), method->name, ip, sp.il_offset, native_offset);
-	mono_debugger_log_bp_hit (tls, method, sp.il_offset);
-	mono_de_collect_breakpoints_by_sp (&sp, ji, ss_reqs_orig, bp_reqs);
-	if (bp_reqs->len == 0 && ss_reqs_orig->len == 0) {
-		/* Maybe a method entry/exit event */
-		if (sp.il_offset == METHOD_ENTRY_IL_OFFSET)
-			kind = EVENT_KIND_METHOD_ENTRY;
-		else if (sp.il_offset == METHOD_EXIT_IL_OFFSET)
-			kind = EVENT_KIND_METHOD_EXIT;
-	}
-	/* Process single step requests */
-	for (guint i = 0; i < ss_reqs_orig->len; ++i) {
-		EventRequest *req = (EventRequest *)g_ptr_array_index (ss_reqs_orig, i);
-		SingleStepReq *ss_req = (SingleStepReq *)req->info;
-		gboolean hit;
-		if ((ss_req->async_stepout_method != method) && (ss_req->async_id || mono_thread_internal_current () != ss_req->thread)) {
-			DbgEngineStackFrame **frames;
-			int nframes;
-			if (ss_req->async_id == 0)
-				continue;
-			rt_callbacks.ss_discard_frame_context (tls);
-			rt_callbacks.ss_calculate_framecount (tls, ctx, FALSE, &frames, &nframes);
-			if (nframes == 0 || !rt_callbacks.ensure_jit (frames [0]))
-				continue;
-			MonoDebugMethodAsyncInfo* asyncMethod = mono_debug_lookup_method_async_debug_info (method);
-			if (!asyncMethod)
-				continue;
-			else
-				mono_debug_free_method_async_debug_info (asyncMethod);
-			if (ss_req->async_id != mono_de_frame_async_id (frames [0]))
-				continue;
-		}
-		if (ss_req->async_stepout_method || ss_req->async_id) {
-			int nframes;
-			rt_callbacks.ss_discard_frame_context (tls);
-			rt_callbacks.ss_calculate_framecount (tls, ctx, FALSE, NULL, &nframes);
-			ss_req->thread = mono_thread_internal_current ();
-			ss_req->nframes = nframes;
-		}
-		hit = mono_de_ss_update (ss_req, ji, &sp, tls, ctx, method);
-		if (hit)
-			g_ptr_array_add (ss_reqs, req);
-		SingleStepArgs args;
-		memset (&args, 0, sizeof (args));
-		args.method = method;
-		args.ctx = ctx;
-		args.tls = tls;
-		args.step_to_catch = FALSE;
-		args.sp = sp;
-		args.info = info;
-		args.frames = NULL;
-		args.nframes = 0;
-		mono_de_ss_start (ss_req, &args);
-	}
-	void *bp_events = mono_dbg_create_breakpoint_events (ss_reqs, bp_reqs, ji, kind);
-	mono_loader_unlock ();
-	g_ptr_array_free (bp_reqs, TRUE);
-	g_ptr_array_free (ss_reqs, TRUE);
-	mono_dbg_process_breakpoint_events (bp_events, method, ctx, sp.il_offset);
-}
-/*
- * ss_bp_is_unique:
- *
- * Reject breakpoint if it is a duplicate of one already in list or hash table.
- */
-static gboolean
-ss_bp_is_unique (GSList *bps, GHashTable *ss_req_bp_cache, MonoMethod *method, guint32 il_offset)
-{
-	if (ss_req_bp_cache) {
-		MonoBreakpoint dummy = {method, (long)il_offset, NULL, NULL};
-		return !g_hash_table_lookup (ss_req_bp_cache, &dummy);
-	}
-	for (GSList *l = bps; l; l = l->next) {
-		MonoBreakpoint *bp = (MonoBreakpoint *)l->data;
-		if (bp->method == method && bp->il_offset == il_offset)
-			return FALSE;
-	}
-	return TRUE;
-}
-/*
- * ss_bp_eq:
- *
- * GHashTable equality for a MonoBreakpoint (only care about method and il_offset fields)
- */
-static gint
-ss_bp_eq (gconstpointer ka, gconstpointer kb)
-{
-	const MonoBreakpoint *s1 = (const MonoBreakpoint *)ka;
-	const MonoBreakpoint *s2 = (const MonoBreakpoint *)kb;
-	return (s1->method == s2->method && s1->il_offset == s2->il_offset) ? 1 : 0;
-}
-/*
- * ss_bp_eq:
- *
- * GHashTable hash for a MonoBreakpoint (only care about method and il_offset fields)
- */
-static guint
-ss_bp_hash (gconstpointer data)
-{
-	const MonoBreakpoint *s = (const MonoBreakpoint *)data;
-	guint hash = (guint) (uintptr_t) s->method;
-	hash ^= ((guint)s->il_offset) << 16; // Assume low bits are more interesting
-	hash ^= ((guint)s->il_offset) >> 16;
-	return hash;
-}
-#define MAX_LINEAR_SCAN_BPS 7
-/*
- * ss_bp_add_one:
- *
- * Create a new breakpoint and add it to a step request.
- * Will adjust the bp count and cache used by mono_de_ss_start.
- */
-static void
-ss_bp_add_one (SingleStepReq *ss_req, int *ss_req_bp_count, GHashTable **ss_req_bp_cache,
-	          MonoMethod *method, guint32 il_offset)
-{
-	if (!*ss_req_bp_cache && *ss_req_bp_count > MAX_LINEAR_SCAN_BPS) {
-		*ss_req_bp_cache = g_hash_table_new (ss_bp_hash, ss_bp_eq);
-		for (GSList *l = ss_req->bps; l; l = l->next)
-			g_hash_table_insert (*ss_req_bp_cache, l->data, l->data);
-	}
-	if (ss_bp_is_unique (ss_req->bps, *ss_req_bp_cache, method, il_offset)) {
-		MonoBreakpoint *bp = mono_de_set_breakpoint (method, il_offset, ss_req->req, NULL);
-		ss_req->bps = g_slist_append (ss_req->bps, bp);
-		if (*ss_req_bp_cache)
-			g_hash_table_insert (*ss_req_bp_cache, bp, bp);
-		(*ss_req_bp_count)++;
-	} else {
-		PRINT_DEBUG_MSG (1, "[dbg] Candidate breakpoint at %s:[il=0x%x] is a duplicate for this step request, will not add.\n", mono_method_full_name (method, TRUE), (int)il_offset);
-	}
-}
-static gboolean
-is_last_non_empty (SeqPoint* sp, MonoSeqPointInfo *info)
-{
-	if (!sp->next_len)
-		return TRUE;
-	SeqPoint* next = g_new (SeqPoint, sp->next_len);
-	mono_seq_point_init_next (info, *sp, next);
-	for (int i = 0; i < sp->next_len; i++) {
-		if (next [i].flags & MONO_SEQ_POINT_FLAG_NONEMPTY_STACK && !(next [i].flags & MONO_SEQ_POINT_FLAG_NESTED_CALL)) {
-			if (!is_last_non_empty (&next [i], info)) {
-				g_free (next);
-				return FALSE;
-			}
-		} else {
-			g_free (next);
-			return FALSE;
-		}
-	}
-	g_free (next);
-	return TRUE;
-}
-/*
- * mono_de_ss_start:
- *
- *   Start the single stepping operation given by SS_REQ from the sequence point SP.
- * If CTX is not set, then this can target any thread. If CTX is set, then TLS should
- * belong to the same thread as CTX.
- * If FRAMES is not-null, use that instead of tls->frames for placing breakpoints etc.
- */
-static void
-mono_de_ss_start (SingleStepReq *ss_req, SingleStepArgs *ss_args)
-{
-	int frame_index;
-	SeqPoint *next_sp, *parent_sp = NULL;
-	SeqPoint local_sp, local_parent_sp;
-	gboolean found_sp;
-	MonoSeqPointInfo *parent_info = NULL;
-	MonoMethod *parent_sp_method = NULL;
-	gboolean enable_global = FALSE;
-	int ss_req_bp_count = g_slist_length (ss_req->bps);
-	GHashTable *ss_req_bp_cache = NULL;
-	/* Stop the previous operation */
-	ss_stop (ss_req);
-	gboolean locked = FALSE;
-	void *tls = ss_args->tls;
-	MonoMethod *method = ss_args->method;
-	DbgEngineStackFrame **frames = ss_args->frames;
-	int nframes = ss_args->nframes;
-	SeqPoint *sp = &ss_args->sp;
-	/* this can happen on a single step in a exception on android (Mono_UnhandledException_internal) and on IOS */
-	if (!method)
-		return;
-	/*
-	 * Implement single stepping using breakpoints if possible.
-	 */
-	if (ss_args->step_to_catch) {
-		ss_bp_add_one (ss_req, &ss_req_bp_count, &ss_req_bp_cache, method, sp->il_offset);
-	} else {
-		frame_index = 1;
-#ifndef TARGET_WASM
-		if (ss_args->ctx && !frames) {
-#else
-		if (!frames) {
-#endif
-			mono_loader_lock ();
-			locked = TRUE;
-			/* Need parent frames */
-			rt_callbacks.ss_calculate_framecount (tls, ss_args->ctx, FALSE, &frames, &nframes);
-		}
-		MonoDebugMethodAsyncInfo* asyncMethod = mono_debug_lookup_method_async_debug_info (method);
-		/* Need to stop in catch clauses as well */
-		for (int i = ss_req->depth == STEP_DEPTH_OUT ? 1 : 0; i < nframes; ++i) {
-			DbgEngineStackFrame *frame = frames [i];
-			if (frame->ji) {
-				MonoJitInfo *jinfo = frame->ji;
-				for (guint32 j = 0; j < jinfo->num_clauses; ++j) {
-					if (asyncMethod && asyncMethod->num_awaits && i == 0 && j + 1 == jinfo->num_clauses)
-						break;
-					MonoJitExceptionInfo *ei = &jinfo->clauses [j];
-					if (mono_find_next_seq_point_for_native_offset (frame->method, GPTRDIFF_TO_INT32 ((char*)ei->handler_start - (char*)jinfo->code_start), NULL, &local_sp))
-						ss_bp_add_one (ss_req, &ss_req_bp_count, &ss_req_bp_cache, frame->method, local_sp.il_offset);
-				}
-			}
-		}
-		if (asyncMethod && asyncMethod->num_awaits && nframes && rt_callbacks.ensure_jit (frames [0])) {
-			for (int i = 0; i < asyncMethod->num_awaits; i++) {
-				if (sp->il_offset == asyncMethod->yield_offsets [i]) {
-					ss_req->async_id = mono_de_frame_async_id (frames [0]);
-					ss_bp_add_one (ss_req, &ss_req_bp_count, &ss_req_bp_cache, method, asyncMethod->resume_offsets [i]);
-					g_hash_table_destroy (ss_req_bp_cache);
-					mono_debug_free_method_async_debug_info (asyncMethod);
-					if (locked)
-						mono_loader_unlock ();
-					goto cleanup;
-				}
-			}
-			if (is_last_non_empty (sp, ss_args->info)) {
-				ss_req->depth = STEP_DEPTH_OUT;//setting depth to step-out is important, don't inline IF, because code later depends on this
-			}
-			if (ss_req->depth == STEP_DEPTH_OUT) {
-				if (set_set_notification_for_wait_completion_flag (frames [0])) {
-					ss_req->async_id = mono_de_frame_async_id (frames [0]);
-					ss_req->async_stepout_method = get_notify_debugger_of_wait_completion_method ();
-					ss_bp_add_one (ss_req, &ss_req_bp_count, &ss_req_bp_cache, ss_req->async_stepout_method, 0);
-					g_hash_table_destroy (ss_req_bp_cache);
-					mono_debug_free_method_async_debug_info (asyncMethod);
-					if (locked)
-						mono_loader_unlock ();
-					goto cleanup;
-				}
-			}
-		}
-		if (asyncMethod)
-			mono_debug_free_method_async_debug_info (asyncMethod);
-		/*
-		* Find the first sequence point in the current or in a previous frame which
-		* is not the last in its method.
-		*/
-		if (ss_req->depth == STEP_DEPTH_OUT) {
-			/* Ignore seq points in current method */
-			while (frame_index < nframes) {
-				DbgEngineStackFrame *frame = frames [frame_index];
-				method = frame->method;
-				found_sp = mono_find_prev_seq_point_for_native_offset (frame->method, frame->native_offset, &ss_args->info, &local_sp);
-				sp = (found_sp)? &local_sp : NULL;
-				frame_index ++;
-				if (sp && sp->next_len != 0)
-					break;
-			}
-		} else {
-			if (sp && sp->next_len == 0) {
-				sp = NULL;
-				while (frame_index < nframes) {
-					DbgEngineStackFrame *frame = frames [frame_index];
-					method = frame->method;
-					found_sp = mono_find_prev_seq_point_for_native_offset (frame->method, frame->native_offset, &ss_args->info, &local_sp);
-					sp = (found_sp)? &local_sp : NULL;
-					if (sp && sp->next_len != 0)
-						break;
-					sp = NULL;
-					frame_index ++;
-				}
-			} else {
-				/* Have to put a breakpoint into a parent frame since the seq points might not cover all control flow out of the method */
-				while (frame_index < nframes) {
-					DbgEngineStackFrame *frame = frames [frame_index];
-					parent_sp_method = frame->method;
-					found_sp = mono_find_prev_seq_point_for_native_offset (frame->method, frame->native_offset, &parent_info, &local_parent_sp);
-					parent_sp = found_sp ? &local_parent_sp : NULL;
-					if (found_sp && parent_sp->next_len != 0)
-						break;
-					parent_sp = NULL;
-					frame_index ++;
-				}
-			}
-		}
-		if (sp && sp->next_len > 0) {
-			SeqPoint* next = g_new(SeqPoint, sp->next_len);
-			mono_seq_point_init_next (ss_args->info, *sp, next);
-			for (int i = 0; i < sp->next_len; i++) {
-				next_sp = &next[i];
-				ss_bp_add_one (ss_req, &ss_req_bp_count, &ss_req_bp_cache, method, next_sp->il_offset);
-			}
-			g_free (next);
-		}
-		if (parent_sp) {
-			SeqPoint* next = g_new(SeqPoint, parent_sp->next_len);
-			mono_seq_point_init_next (parent_info, *parent_sp, next);
-			for (int i = 0; i < parent_sp->next_len; i++) {
-				next_sp = &next[i];
-				ss_bp_add_one (ss_req, &ss_req_bp_count, &ss_req_bp_cache, parent_sp_method, next_sp->il_offset);
-			}
-			g_free (next);
-		}
-		if (ss_req->nframes == 0)
-			ss_req->nframes = nframes;
-		if ((ss_req->depth == STEP_DEPTH_OVER) && (!sp && !parent_sp)) {
-			PRINT_DEBUG_MSG (1, "[dbg] No parent frame for step over, transition to step into.\n");
-			/*
-			 * This is needed since if we leave managed code, and later return to it, step over
-			 * is not going to stop.
-			 * This approach is a bit ugly, since we change the step depth, but it only affects
-			 * clients who reuse the same step request, and only in this special case.
-			 */
-			ss_req->depth = STEP_DEPTH_INTO;
-		}
-		if (ss_req->depth == STEP_DEPTH_INTO) {
-			/* Enable global stepping so we stop at method entry too */
-			enable_global = TRUE;
-		}
-		/*
-		 * The ctx/frame info computed above will become invalid when we continue.
-		 */
-		rt_callbacks.ss_discard_frame_context (tls);
-	}
-	if (enable_global) {
-		PRINT_DEBUG_MSG (1, "[dbg] Turning on global single stepping.\n");
-		ss_req->global = TRUE;
-		mono_de_start_single_stepping ();
-	} else if (!ss_req->bps) {
-		PRINT_DEBUG_MSG (1, "[dbg] Turning on global single stepping.\n");
-		ss_req->global = TRUE;
-		mono_de_start_single_stepping ();
-	} else {
-		ss_req->global = FALSE;
-	}
-	g_hash_table_destroy (ss_req_bp_cache);
-	if (locked)
-		mono_loader_unlock ();
-cleanup:
-	mono_ss_args_destroy (ss_args);
-}
-/*
- * Start single stepping of thread THREAD
- */
-DbgEngineErrorCode
-mono_de_ss_create (MonoInternalThread *thread, StepSize size, StepDepth depth, StepFilter filter, EventRequest *req)
-{
-	int err = rt_callbacks.ensure_runtime_is_suspended ();
-	if (err)
-		return err;
-	if (ss_req_count () > 1) {
-		err = rt_callbacks.handle_multiple_ss_requests ();
-		if (err == DE_ERR_NOT_IMPLEMENTED) {
-			PRINT_DEBUG_MSG (0, "Received a single step request while the previous one was still active.\n");
-			return DE_ERR_NOT_IMPLEMENTED;
-		}
-	}
-	PRINT_DEBUG_MSG (1, "[dbg] Starting single step of thread %p (depth=%s).\n", thread, ss_depth_to_string (depth));
-	SingleStepReq *ss_req = g_new0 (SingleStepReq, 1);
-	ss_req->req = req;
-	ss_req->thread = thread;
-	ss_req->size = size;
-	ss_req->depth = depth;
-	ss_req->filter = filter;
-	ss_req->refcount = 1;
-	req->info = ss_req;
-	for (int i = 0; i < req->nmodifiers; i++) {
-		if (req->modifiers[i].kind == MOD_KIND_ASSEMBLY_ONLY) {
-			ss_req->user_assemblies = req->modifiers[i].data.assemblies;
-			break;
-		}
-	}
-	SingleStepArgs args;
-	err = mono_ss_create_init_args (ss_req, &args);
-	if (err)
-		return err;
-	g_ptr_array_add (the_ss_reqs, ss_req);
-	mono_de_ss_start (ss_req, &args);
-	return DE_ERR_NONE;
-}
-/*
- * mono_de_set_log_level:
- *
- * Configures logging level and output file. Must be called together with mono_de_init.
- */
-void
-mono_de_set_log_level (int level, FILE *file)
-{
-	log_level = level;
-	log_file = file;
-}
-void
-mono_de_set_using_icordbg (void)
-{
-	using_icordbg = TRUE;
-}
-/*
- * mono_de_init:
- *
- * Inits the shared debugger engine. Not reentrant.
- */
-void
-mono_de_init (DebuggerEngineCallbacks *cbs)
-{
-	rt_callbacks = *cbs;
-	mono_coop_mutex_init_recursive (&debug_mutex);
-	domains_init ();
-	breakpoints_init ();
-	ss_req_init ();
-	mono_debugger_log_init ();
-}
-void
-mono_de_cleanup (void)
-{
-	breakpoints_cleanup ();
-	domains_cleanup ();
-	ss_req_cleanup ();
-}
-void
-mono_debugger_free_objref (gpointer value)
-{
-	ObjRef *o = (ObjRef *)value;
-	mono_gchandle_free_internal (o->handle);
-	g_free (o);
-}
-MonoClass *
-get_class_to_get_builder_field (DbgEngineStackFrame *frame)
-{
-	ERROR_DECL (error);
-	StackFrame *the_frame = (StackFrame *)frame;
-	gpointer this_addr = get_this_addr (frame);
-	MonoClass *original_class = frame->method->klass;
-	MonoClass *ret;
-	if (mono_class_is_open_constructed_type (m_class_get_byval_arg (original_class))) {
-		MonoObject *this_obj = *(MonoObject**)this_addr;
-		MonoGenericContext context;
-		MonoType *inflated_type;
-		if (!this_obj)
-			return NULL;
-		context = mono_get_generic_context_from_stack_frame (frame->ji, mono_get_generic_info_from_stack_frame (frame->ji, &the_frame->ctx));
-		inflated_type = mono_class_inflate_generic_type_checked (m_class_get_byval_arg (original_class), &context, error);
-		mono_error_assert_ok (error); /* FIXME don't swallow the error */
-		ret = mono_class_from_mono_type_internal (inflated_type);
-		mono_metadata_free_type (inflated_type);
-		return ret;
-	}
-	return original_class;
-}
-gboolean
-set_set_notification_for_wait_completion_flag (DbgEngineStackFrame *frame)
-{
-	MonoClassField *builder_field = mono_class_get_field_from_name_full (get_class_to_get_builder_field(frame), "<>t__builder", NULL);
-	if (!builder_field)
-		return FALSE;
-	gpointer builder = get_async_method_builder (frame);
-	if (!builder)
-		return FALSE;
-	MonoMethod* method = get_set_notification_method (mono_class_from_mono_type_internal (builder_field->type));
-	if (method == NULL)
-		return FALSE;
-	gboolean arg = TRUE;
-	ERROR_DECL (error);
-	void *args [ ] = { &arg };
-	mono_runtime_invoke_checked (method, builder, args, error);
-	mono_error_assert_ok (error);
-	return TRUE;
-}
-MonoMethod*
-get_object_id_for_debugger_method (MonoClass* async_builder_class)
-{
-	ERROR_DECL (error);
-	GPtrArray *array = mono_class_get_methods_by_name (async_builder_class, "get_ObjectIdForDebugger", 0x24, 1, FALSE, error);
-	mono_error_assert_ok (error);
-	if (array->len != 1) {
-		g_ptr_array_free (array, TRUE);
-		MonoProperty *prop = mono_class_get_property_from_name_internal (async_builder_class, "Task");
-		if (!prop) {
-			PRINT_DEBUG_MSG (1, "Impossible to debug async methods.\n");
-			return NULL;
-		}
-		return prop->get;
-	}
-	MonoMethod *method = (MonoMethod *)g_ptr_array_index (array, 0);
-	g_ptr_array_free (array, TRUE);
-	return method;
-}
-static gpointer
-get_this_addr (DbgEngineStackFrame *the_frame)
-{
-	StackFrame *frame = (StackFrame *)the_frame;
-	if (frame->de.ji->is_interp)
-		return mini_get_interp_callbacks_api ()->frame_get_this (frame->interp_frame);
-	MonoDebugVarInfo *var = frame->jit->this_var;
-	if ((var->index & MONO_DEBUG_VAR_ADDRESS_MODE_FLAGS) != MONO_DEBUG_VAR_ADDRESS_MODE_REGOFFSET)
-		return NULL;
-	guint8 *addr = (guint8 *)mono_arch_context_get_int_reg (&frame->ctx, var->index & ~MONO_DEBUG_VAR_ADDRESS_MODE_FLAGS);
-	addr += (gint32)var->offset;
-	return addr;
-}
-/* Return the address of the AsyncMethodBuilder struct belonging to the state machine method pointed to by FRAME */
-gpointer
-get_async_method_builder (DbgEngineStackFrame *frame)
-{
-	MonoObject *this_obj;
-	MonoClassField *builder_field;
-	gpointer builder;
-	gpointer this_addr;
-	MonoClass* klass = frame->method->klass;
-	klass = get_class_to_get_builder_field(frame);
-	builder_field = mono_class_get_field_from_name_full (klass, "<>t__builder", NULL);
-	if (!builder_field)
-		return NULL;
-	this_addr = get_this_addr (frame);
-	if (!this_addr)
-		return NULL;
-	if (m_class_is_valuetype (klass)) {
-		builder = mono_vtype_get_field_addr (*(guint8**)this_addr, builder_field);
-	} else {
-		this_obj = *(MonoObject**)this_addr;
-		builder = (char*)this_obj + m_field_get_offset (builder_field);
-	}
-	return builder;
-}
-static MonoMethod*
-get_set_notification_method (MonoClass* async_builder_class)
-{
-	ERROR_DECL (error);
-	GPtrArray* array = mono_class_get_methods_by_name (async_builder_class, "SetNotificationForWaitCompletion", 0x24, 1, FALSE, error);
-	mono_error_assert_ok (error);
-	if (array->len == 0) {
-		g_ptr_array_free (array, TRUE);
-		return NULL;
-	}
-	MonoMethod* set_notification_method = (MonoMethod *)g_ptr_array_index (array, 0);
-	g_ptr_array_free (array, TRUE);
-	return set_notification_method;
-}
-static MonoMethod* notify_debugger_of_wait_completion_method_cache;
-MonoMethod*
-get_notify_debugger_of_wait_completion_method (void)
-{
-	if (notify_debugger_of_wait_completion_method_cache != NULL)
-		return notify_debugger_of_wait_completion_method_cache;
-	ERROR_DECL (error);
-	MonoClass* task_class = mono_class_load_from_name (mono_get_corlib (), "System.Threading.Tasks", "Task");
-	GPtrArray* array = mono_class_get_methods_by_name (task_class, "NotifyDebuggerOfWaitCompletion", 0x24, 1, FALSE, error);
-	mono_error_assert_ok (error);
-	g_assert (array->len == 1);
-	notify_debugger_of_wait_completion_method_cache = (MonoMethod *)g_ptr_array_index (array, 0);
-	g_ptr_array_free (array, TRUE);
-	return notify_debugger_of_wait_completion_method_cache;
-}
-DbgEngineErrorCode
-mono_de_set_interp_var (MonoType *t, gpointer addr, guint8 *val_buf)
-{
-	int size;
-	if (m_type_is_byref (t)) {
-		addr = *(gpointer*)addr;
-		if (!addr)
-			return ERR_INVALID_OBJECT;
-	}
-	if (MONO_TYPE_IS_REFERENCE (t))
-		size = sizeof (gpointer);
-	else
-		size = mono_class_value_size (mono_class_from_mono_type_internal (t), NULL);
-	memcpy (addr, val_buf, size);
-	return ERR_NONE;
-}
-#endif

--- a/src/mono/mono/metadata/class-internals.h
+++ b//dev/null
@@ -1,1279 +0,0 @@
-/**
- * \file
- * Copyright 2012 Xamarin Inc
- * Licensed under the MIT license. See LICENSE file in the project root for full license information.
- */
-#ifndef __MONO_METADATA_CLASS_INTERNALS_H__
-#define __MONO_METADATA_CLASS_INTERNALS_H__
-#include <mono/metadata/class.h>
-#include <mono/metadata/object.h>
-#include <mono/metadata/mempool.h>
-#include <mono/metadata/metadata-internals.h>
-#include <mono/metadata/property-bag.h>
-#include "mono/utils/mono-compiler.h"
-#include "mono/utils/mono-error.h"
-#include "mono/sgen/gc-internal-agnostic.h"
-#include "mono/utils/mono-error-internals.h"
-#include "mono/utils/mono-memory-model.h"
-#include "mono/utils/mono-compiler.h"
-#define MONO_CLASS_IS_ARRAY(c) (m_class_get_rank (c))
-#define MONO_CLASS_HAS_STATIC_METADATA(klass) (m_class_get_type_token (klass) && !m_class_get_image (klass)->dynamic && !mono_class_is_ginst (klass))
-#define MONO_DEFAULT_SUPERTABLE_SIZE 6
-extern gboolean mono_print_vtable;
-extern gboolean mono_align_small_structs;
-extern gint32 mono_simd_register_size;
-typedef struct _MonoMethodWrapper MonoMethodWrapper;
-typedef struct _MonoMethodInflated MonoMethodInflated;
-typedef struct _MonoMethodPInvoke MonoMethodPInvoke;
-typedef struct _MonoDynamicMethod MonoDynamicMethod;
-/* Properties that applies to a group of structs should better use a higher number
- * to avoid colision with type specific properties.
- *
- * This prop applies to class, method, property, event, assembly and image.
- */
-#define MONO_PROP_DYNAMIC_CATTR 0x1000
-typedef enum {
-#define WRAPPER(e,n) MONO_WRAPPER_ ## e,
-#include "wrapper-types.h"
-#undef WRAPPER
-	MONO_WRAPPER_NUM
-} MonoWrapperType;
-#define MONO_METHOD_PROP_GENERIC_CONTAINER 0
-/* verification success bit, protected by the image lock */
-#define MONO_METHOD_PROP_VERIFICATION_SUCCESS 1
-/* infrequent vtable layout bits protected by the loader lock */
-#define MONO_METHOD_PROP_INFREQUENT_BITS 2
-/* Infrequently accessed bits of method definitions stored in the image properties.
- * The method must not be inflated.
- *
- * LOCKING: Reading the bits acquires the image lock.  Writing the bits assumes
- * the loader lock is held.
- */
-typedef struct _MonoMethodDefInfrequentBits {
-	unsigned int is_reabstracted:1;  /* whenever this is a reabstraction of another interface */
-	unsigned int is_covariant_override_impl:1; /* whether this is an override with a signature different from its declared method */
-} MonoMethodDefInfrequentBits;
-struct _MonoMethod {
-	guint16 flags;  /* method flags */
-	guint16 iflags; /* method implementation flags */
-	guint32 token;
-	MonoClass *klass; /* To what class does this method belong */
-	MonoMethodSignature *signature;
-	/* name is useful mostly for debugging */
-	const char *name;
-	/* this is used by the inlining algorithm */
-	unsigned int inline_info:1;
-	unsigned int inline_failure:1;
-	unsigned int wrapper_type:5;
-	unsigned int string_ctor:1;
-	unsigned int save_lmf:1;
-	unsigned int dynamic:1; /* created & destroyed during runtime */
-	unsigned int sre_method:1; /* created at runtime using Reflection.Emit */
-	unsigned int is_generic:1; /* whenever this is a generic method definition */
-	unsigned int is_inflated:1; /* whether we're a MonoMethodInflated */
-	unsigned int skip_visibility:1; /* whenever to skip JIT visibility checks */
-	unsigned int _unused : 2; /* unused */
-	signed int slot : 16;
-	/*
-	 * If is_generic is TRUE, the generic_container is stored in image->property_hash,
-	 * using the key MONO_METHOD_PROP_GENERIC_CONTAINER.
-	 */
-};
-struct _MonoMethodWrapper {
-	MonoMethod method;
-	MonoMethodHeader *header;
-	MonoMemoryManager *mem_manager;
-	void *method_data;
-};
-struct _MonoDynamicMethod {
-	MonoMethodWrapper method;
-	MonoAssembly *assembly;
-	MonoMemPool *mp;
-};
-struct _MonoMethodPInvoke {
-	MonoMethod method;
-	gpointer addr;
-	/* add marshal info */
-	union {
-		guint16 piflags;  /* pinvoke flags */
-		guint16 icflags;  /* icall flags */
-	};
-	guint32 implmap_idx;  /* index into IMPLMAP */
-};
-/*
- * Stores the default value / RVA of fields.
- * This information is rarely needed, so it is stored separately from
- * MonoClassField.
- */
-typedef struct MonoFieldDefaultValue {
-	/*
-	 * If the field is constant, pointer to the metadata constant
-	 * value.
-	 * If the field has an RVA flag, pointer to the data.
-	 * Else, invalid.
-	 */
-	const char      *data;
-	/* If the field is constant, the type of the constant. */
-	MonoTypeEnum     def_type;
-} MonoFieldDefaultValue;
-/*
- * MonoClassField is just a runtime representation of the metadata for
- * field, it doesn't contain the data directly.  Static fields are
- * stored in MonoVTable->data.  Instance fields are allocated in the
- * objects after the object header.
- */
-struct _MonoClassField {
-	/* Type of the field */
-	MonoType        *type;
-	const char      *name;
-	/* Type where the field was defined */
-	/* Do not access directly, use m_field_get_parent */
-	/* We use the lowest bits of the pointer to store some flags, see m_field_get_meta_flags */
-	uintptr_t	parent_and_flags;
-	/*
-	 * Offset where this field is stored; if it is an instance
-	 * field, it's the offset from the start of the object, if
-	 * it's static, it's from the start of the memory chunk
-	 * allocated for statics for the class.
-	 * -1 means its a special static field.
-	 * -2 means its a collectible static field.
-	 */
-	int              offset;
-};
-/* a field is ignored if it's named "_Deleted" and it has the specialname and rtspecialname flags set */
-#define mono_field_is_deleted(field) (((field)->type->attrs & (FIELD_ATTRIBUTE_SPECIAL_NAME | FIELD_ATTRIBUTE_RT_SPECIAL_NAME)) \
-				      && (strcmp (mono_field_get_name (field), "_Deleted") == 0))
-/* a field is ignored if it's named "_Deleted" and it has the specialname and rtspecialname flags set */
-/* Try to avoid loading the field's type */
-#define mono_field_is_deleted_with_flags(field, flags) (((flags) & (FIELD_ATTRIBUTE_SPECIAL_NAME | FIELD_ATTRIBUTE_RT_SPECIAL_NAME)) \
-				      && (strcmp (mono_field_get_name (field), "_Deleted") == 0))
-typedef struct {
-	MonoClassField *field;
-	guint32 offset;
-	MonoMarshalSpec *mspec;
-} MonoMarshalField;
-typedef struct {
-	MonoPropertyBagItem head;
-	guint32 native_size, min_align;
-	guint32 num_fields;
-	MonoMethod *ptr_to_str;
-	MonoMethod *str_to_ptr;
-	MonoMarshalField fields [MONO_ZERO_LEN_ARRAY];
-} MonoMarshalType;
-#define MONO_SIZEOF_MARSHAL_TYPE (offsetof (MonoMarshalType, fields))
-struct _MonoProperty {
-	MonoClass *parent;
-	const char *name;
-	MonoMethod *get;
-	MonoMethod *set;
-	guint32 attrs; /* upper bits store non-ECMA flags */
-};
-/* non-ECMA flags for the MonoProperty attrs field */
-enum {
-	/* added by metadata-update after class was created;
-	 * not in MonoClassPropertyInfo array - don't do ptr arithmetic */
-	MONO_PROPERTY_META_FLAG_FROM_UPDATE = 0x00010000,
-	MONO_PROPERTY_META_FLAG_MASK = 0x00010000,
-};
-struct _MonoEvent {
-	MonoClass *parent;
-	const char *name;
-	MonoMethod *add;
-	MonoMethod *remove;
-	MonoMethod *raise;
-#ifndef MONO_SMALL_CONFIG
-	MonoMethod **other;
-#endif
-	guint32 attrs;  /* upper bits store non-ECMA flags */
-};
-/* non-ECMA flags for the MonoEvent attrs field */
-enum {
-	/* added by metadata-update after class was created;
-	 * not in MonoClassEventInfo array - don't do ptr arithmetic */
-	MONO_EVENT_META_FLAG_FROM_UPDATE = 0x00010000,
-	MONO_EVENT_META_FLAG_MASK = 0x00010000,
-};
-/* type of exception being "on hold" for later processing (see exception_type) */
-typedef enum {
-	MONO_EXCEPTION_NONE = 0,
-	MONO_EXCEPTION_INVALID_PROGRAM = 3,
-	MONO_EXCEPTION_UNVERIFIABLE_IL = 4,
-	MONO_EXCEPTION_MISSING_METHOD = 5,
-	MONO_EXCEPTION_MISSING_FIELD = 6,
-	MONO_EXCEPTION_TYPE_LOAD = 7,
-	MONO_EXCEPTION_FILE_NOT_FOUND = 8,
-	MONO_EXCEPTION_METHOD_ACCESS = 9,
-	MONO_EXCEPTION_FIELD_ACCESS = 10,
-	MONO_EXCEPTION_GENERIC_SHARING_FAILED = 11,
-	MONO_EXCEPTION_BAD_IMAGE = 12,
-	MONO_EXCEPTION_OBJECT_SUPPLIED = 13, /*The exception object is already created.*/
-	MONO_EXCEPTION_OUT_OF_MEMORY = 14,
-	MONO_EXCEPTION_INLINE_FAILED = 15,
-	MONO_EXCEPTION_MONO_ERROR = 16,
-	/* add other exception type */
-} MonoExceptionType;
-typedef struct {
-	MonoPropertyBagItem head;
-	MonoProperty *properties;
-	guint32 first, count;
-	MonoFieldDefaultValue *def_values;
-} MonoClassPropertyInfo;
-typedef struct {
-	MonoPropertyBagItem head;
-	/* Initialized by a call to mono_class_setup_events () */
-	MonoEvent *events;
-	guint32 first, count;
-} MonoClassEventInfo;
-typedef enum {
-	MONO_CLASS_DEF = 1, /* non-generic type */
-	MONO_CLASS_GTD, /* generic type definition */
-	MONO_CLASS_GINST, /* generic instantiation */
-	MONO_CLASS_GPARAM, /* generic parameter */
-	MONO_CLASS_ARRAY, /* vector or array, bounded or not */
-	MONO_CLASS_POINTER, /* pointer or function pointer*/
-	MONO_CLASS_GC_FILLER = 0xAC /* not a real class kind - used for sgen nursery filler arrays */
-} MonoTypeKind;
-typedef struct _MonoClassDef MonoClassDef;
-typedef struct _MonoClassGtd MonoClassGtd;
-typedef struct _MonoClassGenericInst MonoClassGenericInst;
-typedef struct _MonoClassGenericParam MonoClassGenericParam;
-typedef struct _MonoClassArray MonoClassArray;
-typedef struct _MonoClassPointer MonoClassPointer;
-union _MonoClassSizes {
-		int class_size; /* size of area for static fields */
-		int element_size; /* for array types */
-		int generic_param_token; /* for generic param types, both var and mvar */
-};
-/* enabled only with small config for now: we might want to do it unconditionally */
-#ifdef MONO_SMALL_CONFIG
-#define COMPRESSED_INTERFACE_BITMAP 1
-#endif
-#ifdef ENABLE_CHECKED_BUILD_PRIVATE_TYPES
-#define MONO_CLASS_DEF_PRIVATE 1
-#endif
-/* Hide _MonoClass definition in checked build mode to ensure that
- * it is only accessed via getter and setter methods.
- */
-#ifndef MONO_CLASS_DEF_PRIVATE
-#include "class-private-definition.h"
-#endif
-/* If MonoClass definition is hidden, just declare the getters.
- * Otherwise, define them as static inline functions.
- *
- * In-tree profilers are allowed to use the getters.  So if we're compiling
- * with --enable-checked-build=private_types, mark the symbols with
- * MONO_PROFILER_API
- */
-#ifdef MONO_CLASS_DEF_PRIVATE
-#define MONO_CLASS_GETTER(funcname, rettype, optref, argtype, fieldname) MONO_PROFILER_API rettype funcname (argtype *klass);
-#else
-#define MONO_CLASS_GETTER(funcname, rettype, optref, argtype, fieldname) static inline rettype funcname (argtype *klass) { return optref klass-> fieldname ; }
-#endif
-#define MONO_CLASS_OFFSET(funcname, argtype, fieldname) /*nothing*/
-#include "class-getters.h"
-#undef MONO_CLASS_GETTER
-#undef MONO_CLASS_OFFSET
-#ifdef COMPRESSED_INTERFACE_BITMAP
-int mono_compress_bitmap (uint8_t *dest, const uint8_t *bitmap, int size);
-int mono_class_interface_match (const uint8_t *bitmap, int id);
-#else
-#define mono_class_interface_match(bmap,uiid) ((bmap) [(uiid) >> 3] & (1 << ((uiid)&7)))
-#endif
-#define MONO_CLASS_IMPLEMENTS_INTERFACE(k,uiid) (((uiid) <= m_class_get_max_interface_id (k)) && mono_class_interface_match (m_class_get_interface_bitmap (k), (uiid)))
-#define MONO_VTABLE_AVAILABLE_GC_BITS 4
-#ifdef DISABLE_COM
-#define mono_class_is_com_object(klass) (FALSE)
-#else
-#define mono_class_is_com_object(klass) (m_class_is_com_object (klass))
-#endif
-MONO_API int mono_class_interface_offset (MonoClass *klass, MonoClass *itf);
-MONO_COMPONENT_API int mono_class_interface_offset_with_variance (MonoClass *klass, MonoClass *itf, gboolean *non_exact_match);
-typedef gpointer MonoRuntimeGenericContext;
-typedef enum {
-	/* array or string */
-	MONO_VT_FLAG_ARRAY_OR_STRING = (1 << 0),
-	MONO_VT_FLAG_HAS_REFERENCES = (1 << 1),
-	MONO_VT_FLAG_ARRAY_IS_PRIMITIVE = (1 << 2),
-} MonoVTableFlags;
-/* the interface_offsets array is stored in memory before this struct */
-struct MonoVTable {
-	MonoClass  *klass;
-	 /*
-	 * According to comments in gc_gcj.h, this should be the second word in
-	 * the vtable.
-	 */
-	MonoGCDescriptor gc_descr;
-	MonoDomain *domain;  /* each object/vtable belongs to exactly one domain */
-	gpointer    type; /* System.Type type for klass */
-	guint8     *interface_bitmap;
-	MonoGCHandle loader_alloc; /* LoaderAllocator object for objects in collectible alcs */
-	guint32     max_interface_id;
-	guint8      rank;
-	/* Keep this a guint8, the jit depends on it */
-	guint8      initialized; /* cctor has been run */
-	/* Keep this a guint8, the jit depends on it */
-	guint8      flags; /* MonoVTableFlags */
-	guint init_failed     : 1; /* cctor execution failed */
-	guint has_static_fields : 1; /* pointer to the data stored at the end of the vtable array */
-	guint gc_bits         : MONO_VTABLE_AVAILABLE_GC_BITS; /* Those bits are reserved for the usaged of the GC */
-	guint32     imt_collisions_bitmap;
-	MonoRuntimeGenericContext *runtime_generic_context;
-	/* Maintained by the Execution Engine */
-	gpointer ee_data;
-	/* do not add any fields after vtable, the structure is dynamically extended */
-	/* vtable contains function pointers to methods or their trampolines, at the
-	 end there may be a slot containing the pointer to the static fields */
-	gpointer    vtable [MONO_ZERO_LEN_ARRAY];
-};
-#define MONO_SIZEOF_VTABLE (sizeof (MonoVTable) - MONO_ZERO_LEN_ARRAY * SIZEOF_VOID_P)
-#define MONO_VTABLE_IMPLEMENTS_INTERFACE(vt,uiid) (((uiid) <= (vt)->max_interface_id) && mono_class_interface_match ((vt)->interface_bitmap, (uiid)))
-/*
- * Generic instantiation data type encoding.
- */
-/*
- * A particular generic instantiation:
- *
- * All instantiations are cached and we don't distinguish between class and method
- * instantiations here.
- */
-struct _MonoGenericInst {
-#ifndef MONO_SMALL_CONFIG
-	gint32 id;			/* unique ID for debugging */
-#endif
-	guint type_argc    : 22;	/* number of type arguments */
-	guint is_open      :  1;	/* if this is an open type */
-	MonoType *type_argv [MONO_ZERO_LEN_ARRAY];
-};
-#define MONO_SIZEOF_GENERIC_INST (sizeof (MonoGenericInst) - MONO_ZERO_LEN_ARRAY * SIZEOF_VOID_P)
-/*
- * The generic context: an instantiation of a set of class and method generic parameters.
- *
- * NOTE: Never allocate this directly on the heap.  It have to be either allocated on the stack,
- *	 or embedded within other objects.  Don't store pointers to this, because it may be on the stack.
- *	 If you really have to, ensure you store a pointer to the embedding object along with it.
- */
-struct _MonoGenericContext {
-	/* The instantiation corresponding to the class generic parameters */
-	MonoGenericInst *class_inst;
-	/* The instantiation corresponding to the method generic parameters */
-	MonoGenericInst *method_inst;
-};
-/*
- * Inflated generic method.
- */
-struct _MonoMethodInflated {
-	union {
-		MonoMethod method;
-		MonoMethodPInvoke pinvoke;
-		MonoMethodWrapper wrapper;
-	} method;
-	MonoMethod *declaring;		/* the generic method definition. */
-	MonoGenericContext context;	/* The current instantiation */
-	MonoMemoryManager *owner; /* The mem manager that the inflated method belongs to. */
-};
-/*
- * A particular instantiation of a generic type.
- */
-struct _MonoGenericClass {
-	MonoClass *container_class;	/* the generic type definition */
-	MonoGenericContext context;	/* a context that contains the type instantiation doesn't contain any method instantiation */ /* FIXME: Only the class_inst member of "context" is ever used, so this field could be replaced with just a monogenericinst */
-	guint is_dynamic  : 1;		/* Contains dynamic types */
-	guint is_tb_open  : 1;		/* This is the fully open instantiation for a type_builder. Quite ugly, but it's temporary.*/
-	MonoClass *cached_class;	/* if present, the MonoClass corresponding to the instantiation.  */
-	/* The mem manager which owns this generic class. */
-	MonoMemoryManager *owner;
-};
-/* Additional details about a MonoGenericParam */
-/* Keep in sync with managed Mono.RuntimeStructs.GenericParamInfo */
-typedef struct {
-	MonoClass *pklass;		/* The corresponding `MonoClass'. */
-	const char *name;
-	guint16 flags;
-	guint32 token;
-	MonoClass** constraints; /* NULL means end of list */
-} MonoGenericParamInfo;
-/*
- * A type parameter.
- */
-struct _MonoGenericParam {
-	/*
-	 * Type or method this parameter was defined in.
-	 */
-	MonoGenericContainer *owner;
-	guint16 num;
-	/*
-	 * If != NULL, this is a generated generic param used by the JIT to implement generic
-	 * sharing.
-	 */
-	MonoType *gshared_constraint;
-	MonoGenericParamInfo info;
-};
-typedef MonoGenericParam MonoGenericParamFull;
-/*
- * The generic container.
- *
- * Stores the type parameters of a generic type definition or a generic method definition.
- */
-struct _MonoGenericContainer {
-	MonoGenericContext context;
-	/* If we're a generic method definition in a generic type definition,
-	   the generic container of the containing class. */
-	MonoGenericContainer *parent;
-	/* the generic type definition or the generic method definition corresponding to this container */
-	/* Union rules: If is_anonymous, image field is valid; else if is_method, method field is valid; else klass is valid. */
-	union {
-		MonoClass *klass;
-		MonoMethod *method;
-		MonoImage *image;
-	} owner;
-	int type_argc    : 29; // Per the ECMA spec, this value is capped at 16 bits
-	/* If true, we're a generic method, otherwise a generic type definition. */
-	/* Invariant: parent != NULL => is_method */
-	guint is_method     : 1;
-	/* If true, this container has no associated class/method and only the image is known. This can happen:
-	   1. For the special anonymous containers kept by MonoImage.
-	   2. When user code creates a generic parameter via SRE, but has not yet set an owner. */
-	guint is_anonymous : 1;
-	/* Our type parameters. If this is a special anonymous container (case 1, above), this field is not valid, use mono_metadata_create_anon_gparam ()  */
-	MonoGenericParamFull *type_params;
-};
-static inline MonoGenericParam *
-mono_generic_container_get_param (MonoGenericContainer *gc, int i)
-{
-	return (MonoGenericParam *) &gc->type_params [i];
-}
-static inline MonoGenericParamInfo *
-mono_generic_container_get_param_info (MonoGenericContainer *gc, int i)
-{
-	return &gc->type_params [i].info;
-}
-static inline MonoGenericContainer *
-mono_generic_param_owner (MonoGenericParam *p)
-{
-	return p->owner;
-}
-static inline guint16
-mono_generic_param_num (MonoGenericParam *p)
-{
-	return p->num;
-}
-static inline MonoGenericParamInfo *
-mono_generic_param_info (MonoGenericParam *p)
-{
-	return &((MonoGenericParamFull *) p)->info;
-}
-static inline const char *
-mono_generic_param_name (MonoGenericParam *p)
-{
-	return ((MonoGenericParamFull *) p)->info.name;
-}
-static inline MonoGenericContainer *
-mono_type_get_generic_param_owner (MonoType *t)
-{
-	return mono_generic_param_owner (t->data.generic_param);
-}
-static inline guint16
-mono_type_get_generic_param_num (MonoType *t)
-{
-	return mono_generic_param_num (t->data.generic_param);
-}
-/*
- * Class information which might be cached by the runtime in the AOT file for
- * example. Caching this allows us to avoid computing a generic vtable
- * (class->vtable) in most cases, saving time and avoiding creation of lots of
- * MonoMethod structures.
- */
-typedef struct MonoCachedClassInfo {
-	guint32 vtable_size;
-	guint has_finalize : 1;
-	guint ghcimpl : 1;
-	guint has_cctor : 1;
-	guint has_nested_classes : 1;
-	guint blittable : 1;
-	guint has_references : 1;
-	guint has_static_refs : 1;
-	guint no_special_static_fields : 1;
-	guint is_generic_container : 1;
-	guint has_weak_fields : 1;
-	guint has_deferred_failure : 1;
-	guint32 cctor_token;
-	MonoImage *finalize_image;
-	guint32 finalize_token;
-	guint32 instance_size;
-	guint32 class_size;
-	guint32 packing_size;
-	guint32 min_align;
-} MonoCachedClassInfo;
-typedef struct {
-	const char *name;
-	gconstpointer func;
-	gconstpointer wrapper;
-	gconstpointer trampoline;
-	MonoMethodSignature *sig;
-	const char *c_symbol;
-	MonoMethod *wrapper_method;
-} MonoJitICallInfo;
-MONO_COMPONENT_API void
-mono_class_setup_supertypes (MonoClass *klass);
-/* WARNING
- * Only call this function if you can ensure both @klass and @parent
- * have supertype information initialized.
- * This can be accomplished by mono_class_setup_supertypes or mono_class_init.
- * If unsure, use mono_class_has_parent.
- */
-static inline gboolean
-mono_class_has_parent_fast (MonoClass *klass, MonoClass *parent)
-{
-	return (m_class_get_idepth (klass) >= m_class_get_idepth (parent)) && (m_class_get_supertypes (klass) [m_class_get_idepth (parent) - 1] == parent);
-}
-static inline gboolean
-mono_class_has_parent (MonoClass *klass, MonoClass *parent)
-{
-	if (G_UNLIKELY (!m_class_get_supertypes (klass)))
-		mono_class_setup_supertypes (klass);
-	if (G_UNLIKELY (!m_class_get_supertypes (parent)))
-		mono_class_setup_supertypes (parent);
-	return mono_class_has_parent_fast (klass, parent);
-}
-typedef struct {
-	MonoVTable *default_vtable;
-	MonoVTable *xdomain_vtable;
-	MonoClass *proxy_class;
-	char* proxy_class_name;
-	uint32_t interface_count;
-	MonoClass *interfaces [MONO_ZERO_LEN_ARRAY];
-} MonoRemoteClass;
-#define MONO_SIZEOF_REMOTE_CLASS (sizeof (MonoRemoteClass) - MONO_ZERO_LEN_ARRAY * SIZEOF_VOID_P)
-typedef struct {
-	gint32 initialized_class_count;
-	gint32 generic_vtable_count;
-	gint32 used_class_count;
-	gint32 method_count;
-	gint32 class_vtable_size;
-	gint32 class_static_data_size;
-	gint32 generic_class_count;
-	gint32 inflated_method_count;
-	gint32 inflated_type_count;
-	gint32 delegate_creations;
-	gint32 imt_tables_size;
-	gint32 imt_number_of_tables;
-	gint32 imt_number_of_methods;
-	gint32 imt_used_slots;
-	gint32 imt_slots_with_collisions;
-	gint32 imt_max_collisions_in_slot;
-	gint32 imt_method_count_when_max_collisions;
-	gint32 imt_trampolines_size;
-	gint32 jit_info_table_insert_count;
-	gint32 jit_info_table_remove_count;
-	gint32 jit_info_table_lookup_count;
-	gint32 generics_sharable_methods;
-	gint32 generics_unsharable_methods;
-	gint32 generics_shared_methods;
-	gint32 gsharedvt_methods;
-	gboolean enabled;
-} MonoStats;
-/*
- * The definition of the first field in SafeHandle,
- * Keep in sync with SafeHandle.cs, this is only used
- * to access the `handle' parameter.
- */
-typedef struct {
-	MonoObject  base;
-	void       *handle;
-} MonoSafeHandle;
-/*
- * Keep in sync with HandleRef.cs
- */
-typedef struct {
-	MonoObject *wrapper;
-	void       *handle;
-} MonoHandleRef;
-extern MonoStats mono_stats;
-static inline gboolean
-method_is_dynamic (MonoMethod *method)
-{
-#ifdef DISABLE_REFLECTION_EMIT
-	return FALSE;
-#else
-	return method->dynamic;
-#endif
-}
-MonoMethod*
-mono_class_get_method_by_index (MonoClass *klass, int index);
-MonoMethod*
-mono_class_get_inflated_method (MonoClass *klass, MonoMethod *method, MonoError *error);
-MonoMethod*
-mono_class_get_vtable_entry (MonoClass *klass, int offset);
-GPtrArray*
-mono_class_get_implemented_interfaces (MonoClass *klass, MonoError *error);
-int
-mono_class_get_vtable_size (MonoClass *klass);
-MONO_COMPONENT_API gboolean
-mono_class_is_open_constructed_type (MonoType *t);
-void
-mono_class_get_overrides_full (MonoImage *image, guint32 type_token, MonoMethod ***overrides, gint32 *num_overrides, MonoGenericContext *generic_context, MonoError *error);
-MonoMethod*
-mono_class_get_cctor (MonoClass *klass);
-MonoMethod*
-mono_class_get_finalizer (MonoClass *klass);
-gboolean
-mono_class_needs_cctor_run (MonoClass *klass, MonoMethod *caller);
-MONO_COMPONENT_API gboolean
-mono_class_field_is_special_static (MonoClassField *field);
-MONO_COMPONENT_API guint32
-mono_class_field_get_special_static_type (MonoClassField *field);
-gboolean
-mono_class_has_special_static_fields (MonoClass *klass);
-const char*
-mono_class_get_field_default_value (MonoClassField *field, MonoTypeEnum *def_type);
-MONO_COMPONENT_API MonoProperty*
-mono_class_get_property_from_name_internal (MonoClass *klass, const char *name);
-const char*
-mono_class_get_property_default_value (MonoProperty *property, MonoTypeEnum *def_type);
-gpointer
-mono_lookup_dynamic_token (MonoImage *image, guint32 token, MonoGenericContext *context, MonoError *error);
-gpointer
-mono_lookup_dynamic_token_class (MonoImage *image, guint32 token, gboolean check_token, MonoClass **handle_class, MonoGenericContext *context, MonoError *error);
-MONO_PROFILER_API MonoGenericContext*
-mono_class_get_context (MonoClass *klass);
-MONO_PROFILER_API MonoMethodSignature*
-mono_method_signature_checked_slow (MonoMethod *m, MonoError *err);
-MONO_PROFILER_API MonoMethodSignature*
-mono_method_signature_internal_slow (MonoMethod *m);
-/**
- * mono_method_signature_checked:
- *
- * Return the signature of the method M. On failure, returns NULL, and ERR is set.
- */
-static inline MonoMethodSignature*
-mono_method_signature_checked (MonoMethod *m, MonoError *error)
-{
-	error_init (error);
-	MonoMethodSignature* sig = m->signature;
-	return sig ? sig : mono_method_signature_checked_slow (m, error);
-}
-/**
- * mono_method_signature_internal:
- * \returns the signature of the method \p m. On failure, returns NULL.
- */
-static inline MonoMethodSignature*
-mono_method_signature_internal (MonoMethod *m)
-{
-	MonoMethodSignature* sig = m->signature;
-	return sig ? sig : mono_method_signature_internal_slow (m);
-}
-MonoGenericContext*
-mono_method_get_context_general (MonoMethod *method, gboolean uninflated);
-MONO_PROFILER_API MonoGenericContext*
-mono_method_get_context (MonoMethod *method);
-/* Used by monodis, thus cannot be MONO_INTERNAL */
-MONO_API MonoGenericContainer*
-mono_method_get_generic_container (MonoMethod *method);
-MonoGenericContext*
-mono_generic_class_get_context (MonoGenericClass *gclass);
-void
-mono_method_set_generic_container (MonoMethod *method, MonoGenericContainer* container);
-void
-mono_method_set_verification_success (MonoMethod *method);
-gboolean
-mono_method_get_verification_success (MonoMethod *method);
-const MonoMethodDefInfrequentBits *
-mono_method_lookup_infrequent_bits (MonoMethod *methoddef);
-MonoMethodDefInfrequentBits *
-mono_method_get_infrequent_bits (MonoMethod *methoddef);
-gboolean
-mono_method_get_is_reabstracted (MonoMethod *method);
-void
-mono_method_set_is_reabstracted (MonoMethod *methoddef);
-gboolean
-mono_method_get_is_covariant_override_impl (MonoMethod *method);
-void
-mono_method_set_is_covariant_override_impl (MonoMethod *methoddef);
-MONO_COMPONENT_API MonoMethod*
-mono_class_inflate_generic_method_full_checked (MonoMethod *method, MonoClass *klass_hint, MonoGenericContext *context, MonoError *error);
-MONO_COMPONENT_API MonoMethod *
-mono_class_inflate_generic_method_checked (MonoMethod *method, MonoGenericContext *context, MonoError *error);
-MonoMemoryManager *
-mono_metadata_get_mem_manager_for_type (MonoType *type);
-MonoMemoryManager *
-mono_metadata_get_mem_manager_for_class (MonoClass *klass);
-MonoMemoryManager*
-mono_metadata_get_mem_manager_for_method (MonoMethodInflated *method);
-MONO_API MonoMethodSignature *
-mono_metadata_get_inflated_signature (MonoMethodSignature *sig, MonoGenericContext *context);
-MonoType*
-mono_class_inflate_generic_type_with_mempool (MonoImage *image, MonoType *type, MonoGenericContext *context, MonoError *error);
-MONO_COMPONENT_API MonoType*
-mono_class_inflate_generic_type_checked (MonoType *type, MonoGenericContext *context, MonoError *error);
-MONO_API void
-mono_metadata_free_inflated_signature (MonoMethodSignature *sig);
-MonoMethodSignature*
-mono_inflate_generic_signature (MonoMethodSignature *sig, MonoGenericContext *context, MonoError *error);
-MonoClass*
-mono_generic_param_get_base_type (MonoClass *klass);
-typedef struct {
-	MonoImage *corlib;
-	MonoClass *object_class;
-	MonoClass *object_class_array; // used via token pasting in mono_array_class_get_cached
-	MonoClass *byte_class;
-	MonoClass *void_class;
-	MonoClass *boolean_class;
-	MonoClass *sbyte_class;
-	MonoClass *int16_class;
-	MonoClass *uint16_class;
-	MonoClass *int32_class;
-	MonoClass *uint32_class;
-	MonoClass *int_class;
-	MonoClass *uint_class;
-	MonoClass *int64_class;
-	MonoClass *uint64_class;
-	MonoClass *single_class;
-	MonoClass *double_class;
-	MonoClass *char_class;
-	MonoClass *string_class;
-	MonoClass *enum_class;
-	MonoClass *array_class;
-	MonoClass *delegate_class;
-	MonoClass *multicastdelegate_class;
-	MonoClass *manualresetevent_class;
-	MonoClass *typehandle_class;
-	MonoClass *fieldhandle_class;
-	MonoClass *methodhandle_class;
-	MonoClass *systemtype_class;
-	MonoClass *runtimetype_class;
-	MonoClass *runtimetype_class_array; // used via token pasting in mono_array_class_get_cached
-	MonoClass *exception_class;
-	MonoClass *threadabortexception_class;
-	MonoClass *thread_class;
-	MonoClass *internal_thread_class;
-	MonoClass *autoreleasepool_class;
-	MonoClass *mono_method_message_class;
-	MonoClass *field_info_class;
-	MonoClass *method_info_class;
-	MonoClass *stack_frame_class;
-	MonoClass *marshal_class;
-	MonoClass *typed_reference_class;
-	MonoClass *argumenthandle_class;
-	MonoClass *monitor_class;
-	MonoClass *generic_ilist_class;
-	MonoClass *generic_nullable_class;
-	MonoClass *attribute_class;
-	MonoClass *attribute_class_array; // used via token pasting in mono_array_class_get_cached
-	MonoClass *critical_finalizer_object; /* MAYBE NULL */
-	MonoClass *generic_ireadonlylist_class;
-	MonoClass *generic_ienumerator_class;
-	MonoClass *alc_class;
-	MonoClass *appcontext_class;
-} MonoDefaults;
-/* If you need a MonoType, use one of the mono_get_*_type () functions in class-inlines.h */
-extern MonoDefaults mono_defaults;
-MONO_COMPONENT_API
-MonoDefaults *
-mono_get_defaults (void);
-#define GENERATE_GET_CLASS_WITH_CACHE_DECL(shortname) \
-MonoClass* mono_class_get_##shortname##_class (void);
-#define GENERATE_TRY_GET_CLASS_WITH_CACHE_DECL(shortname) \
-MonoClass* mono_class_try_get_##shortname##_class (void);
-static inline MonoImage *
-mono_class_generate_get_corlib_impl (void)
-{
-#ifdef COMPILING_COMPONENT_DYNAMIC
-  return mono_get_corlib ();
-#else
-  return mono_defaults.corlib;
-#endif
-}
-#define GENERATE_GET_CLASS_WITH_CACHE(shortname,name_space,name) \
-MonoClass*	\
-mono_class_get_##shortname##_class (void)	\
-{	\
-	static MonoClass *tmp_class;	\
-	MonoClass *klass = tmp_class;	\
-	if (!klass) {	\
-	  klass = mono_class_load_from_name (mono_class_generate_get_corlib_impl (), name_space, name); \
-		mono_memory_barrier ();	/* FIXME excessive? */ \
-		tmp_class = klass;	\
-	}	\
-	return klass;	\
-}
-#define GENERATE_TRY_GET_CLASS_WITH_CACHE(shortname,name_space,name) \
-MonoClass*	\
-mono_class_try_get_##shortname##_class (void)	\
-{	\
-	static volatile MonoClass *tmp_class;	\
-	static volatile gboolean inited;	\
-	MonoClass *klass = (MonoClass *)tmp_class;	\
-	mono_memory_barrier ();	\
-	if (!inited) {	\
-		klass = mono_class_try_load_from_name (mono_class_generate_get_corlib_impl (), name_space, name);	\
-		tmp_class = klass;	\
-		mono_memory_barrier ();	\
-		inited = TRUE;	\
-	}	\
-	return klass;	\
-}
-GENERATE_TRY_GET_CLASS_WITH_CACHE_DECL (safehandle)
-#ifndef DISABLE_COM
-GENERATE_GET_CLASS_WITH_CACHE_DECL (interop_proxy)
-GENERATE_GET_CLASS_WITH_CACHE_DECL (idispatch)
-GENERATE_GET_CLASS_WITH_CACHE_DECL (iunknown)
-GENERATE_GET_CLASS_WITH_CACHE_DECL (com_object)
-GENERATE_GET_CLASS_WITH_CACHE_DECL (variant)
-#endif
-MonoClass* mono_class_get_appdomain_class (void);
-GENERATE_GET_CLASS_WITH_CACHE_DECL (appdomain_unloaded_exception)
-GENERATE_TRY_GET_CLASS_WITH_CACHE_DECL (appdomain_unloaded_exception)
-GENERATE_GET_CLASS_WITH_CACHE_DECL (valuetype)
-GENERATE_TRY_GET_CLASS_WITH_CACHE_DECL(handleref)
-GENERATE_GET_CLASS_WITH_CACHE_DECL (assembly_load_context)
-GENERATE_GET_CLASS_WITH_CACHE_DECL (native_library)
-void
-mono_loader_init           (void);
-void
-mono_loader_cleanup        (void);
-MONO_COMPONENT_API void
-mono_loader_lock           (void);
-MONO_COMPONENT_API void
-mono_loader_unlock         (void);
-MONO_COMPONENT_API void
-mono_loader_lock_track_ownership (gboolean track);
-MONO_COMPONENT_API gboolean
-mono_loader_lock_is_owned_by_self (void);
-void
-mono_loader_lock_if_inited (void);
-void
-mono_loader_unlock_if_inited (void);
-void
-mono_reflection_init       (void);
-void
-mono_icall_init            (void);
-MONO_COMPONENT_API gpointer
-mono_method_get_wrapper_data (MonoMethod *method, guint32 id);
-gboolean
-mono_metadata_has_generic_params (MonoImage *image, guint32 token);
-MONO_API MonoGenericContainer *
-mono_metadata_load_generic_params (MonoImage *image, guint32 token,
-				   MonoGenericContainer *parent_container,
-				   gpointer real_owner);
-MONO_API gboolean
-mono_metadata_load_generic_param_constraints_checked (MonoImage *image, guint32 token,
-					      MonoGenericContainer *container, MonoError *error);
-void
-mono_register_jit_icall_info (MonoJitICallInfo *info, gconstpointer func, const char *name,
-			      MonoMethodSignature *sig, gboolean no_wrapper, const char *c_symbol);
-#ifdef __cplusplus
-template <typename T>
-inline void
-mono_register_jit_icall_info (MonoJitICallInfo *info, T func, const char *name, MonoMethodSignature *sig, gboolean no_wrapper, const char *c_symbol)
-{
-	mono_register_jit_icall_info (info, (gconstpointer)func, name, sig, no_wrapper, c_symbol);
-}
-#endif // __cplusplus
-#define mono_register_jit_icall(func, sig, no_wrapper) (mono_register_jit_icall_info (&mono_get_jit_icall_info ()->func, func, #func, (sig), (no_wrapper), NULL))
-MonoException*
-mono_class_get_exception_for_failure (MonoClass *klass);
-char*
-mono_identifier_escape_type_name_chars (const char* identifier);
-char*
-mono_type_get_full_name (MonoClass *klass);
-MONO_COMPONENT_API
-char *
-mono_method_get_name_full (MonoMethod *method, gboolean signature, gboolean ret, MonoTypeNameFormat format);
-MONO_PROFILER_API char *
-mono_method_get_full_name (MonoMethod *method);
-const char*
-mono_wrapper_type_to_str (guint32 wrapper_type);
-MonoArrayType *mono_dup_array_type (MonoImage *image, MonoArrayType *a);
-MonoMethodSignature *mono_metadata_signature_deep_dup (MonoImage *image, MonoMethodSignature *sig);
-MONO_API void
-mono_image_init_name_cache (MonoImage *image);
-MonoClass*
-mono_class_get_nullable_param_internal (MonoClass *klass);
-/* object debugging functions, for use inside gdb */
-MONO_API void mono_object_describe        (MonoObject *obj);
-MONO_API void mono_object_describe_fields (MonoObject *obj);
-MONO_API void mono_value_describe_fields  (MonoClass* klass, const char* addr);
-MONO_API void mono_class_describe_statics (MonoClass* klass);
-/* method debugging functions, for use inside gdb */
-MONO_API void mono_method_print_code (MonoMethod *method);
-MONO_PROFILER_API char *mono_signature_full_name (MonoMethodSignature *sig);
-/*Enum validation related functions*/
-MONO_API gboolean
-mono_type_is_valid_enum_basetype (MonoType * type);
-MONO_API gboolean
-mono_class_is_valid_enum (MonoClass *klass);
-MONO_PROFILER_API gboolean
-mono_type_is_primitive (MonoType *type);
-MonoType *
-mono_type_get_checked        (MonoImage *image, guint32 type_token, MonoGenericContext *context, MonoError *error);
-gboolean
-mono_generic_class_is_generic_type_definition (MonoGenericClass *gklass);
-MonoType*
-mono_type_get_basic_type_from_generic (MonoType *type);
-gboolean
-mono_method_can_access_method_full (MonoMethod *method, MonoMethod *called, MonoClass *context_klass);
-gboolean
-mono_method_can_access_field_full (MonoMethod *method, MonoClassField *field, MonoClass *context_klass);
-gboolean
-mono_class_can_access_class (MonoClass *access_class, MonoClass *target_class);
-MONO_COMPONENT_API MonoClass *
-mono_class_get_generic_type_definition (MonoClass *klass);
-gboolean
-mono_class_has_parent_and_ignore_generics (MonoClass *klass, MonoClass *parent);
-int
-mono_method_get_vtable_slot (MonoMethod *method);
-int
-mono_method_get_vtable_index (MonoMethod *method);
-MonoMethod*
-mono_method_get_base_method (MonoMethod *method, gboolean definition, MonoError *error);
-MonoMethod*
-mono_method_search_in_array_class (MonoClass *klass, const char *name, MonoMethodSignature *sig);
-void
-mono_class_setup_interface_id (MonoClass *klass);
-MONO_COMPONENT_API MonoGenericContainer*
-mono_class_get_generic_container (MonoClass *klass);
-gpointer
-mono_class_alloc (MonoClass *klass, int size);
-MONO_COMPONENT_API gpointer
-mono_class_alloc0 (MonoClass *klass, int size);
-#define mono_class_alloc0(klass, size) (g_cast (mono_class_alloc0 ((klass), (size))))
-MONO_COMPONENT_API void
-mono_class_setup_interfaces (MonoClass *klass, MonoError *error);
-MONO_COMPONENT_API MonoClassField*
-mono_class_get_field_from_name_full (MonoClass *klass, const char *name, MonoType *type);
-MONO_COMPONENT_API MonoVTable*
-mono_class_vtable_checked (MonoClass *klass, MonoError *error);
-void
-mono_class_is_assignable_from_checked (MonoClass *klass, MonoClass *oklass, gboolean *result, MonoError *error);
-void
-mono_class_signature_is_assignable_from (MonoClass *klass, MonoClass *oklass, gboolean *result, MonoError *error);
-gboolean
-mono_class_is_assignable_from_slow (MonoClass *target, MonoClass *candidate);
-gboolean
-mono_class_has_variant_generic_params (MonoClass *klass);
-gboolean
-mono_class_is_variant_compatible (MonoClass *klass, MonoClass *oklass, gboolean check_for_reference_conv);
-gboolean
-mono_class_is_subclass_of_internal (MonoClass *klass, MonoClass *klassc, gboolean check_interfaces);
-MONO_COMPONENT_API mono_bool
-mono_class_is_assignable_from_internal (MonoClass *klass, MonoClass *oklass);
-gboolean
-mono_byref_type_is_assignable_from (MonoType *type, MonoType *ctype, gboolean signature_assignment);
-gboolean mono_is_corlib_image (MonoImage *image);
-MonoType*
-mono_field_get_type_checked (MonoClassField *field, MonoError *error);
-MonoType*
-mono_field_get_type_internal (MonoClassField *field);
-MONO_COMPONENT_API MonoClassField*
-mono_class_get_fields_internal (MonoClass* klass, gpointer *iter);
-MonoClassField*
-mono_class_get_fields_lazy (MonoClass* klass, gpointer *iter);
-gboolean
-mono_class_check_vtable_constraints (MonoClass *klass, GList *in_setup);
-MONO_COMPONENT_API gboolean
-mono_class_has_finalizer (MonoClass *klass);
-void
-mono_unload_interface_id (MonoClass *klass);
-MONO_COMPONENT_API GPtrArray*
-mono_class_get_methods_by_name (MonoClass *klass, const char *name, guint32 bflags, guint32 mlisttype, gboolean allow_ctors, MonoError *error);
-char*
-mono_class_full_name (MonoClass *klass);
-MonoClass*
-mono_class_inflate_generic_class_checked (MonoClass *gklass, MonoGenericContext *context, MonoError *error);
-MONO_PROFILER_API MonoClass *
-mono_class_get_checked (MonoImage *image, guint32 type_token, MonoError *error);
-MonoClass *
-mono_class_get_and_inflate_typespec_checked (MonoImage *image, guint32 type_token, MonoGenericContext *context, MonoError *error);
-MONO_COMPONENT_API MonoClass *
-mono_class_from_name_checked (MonoImage *image, const char* name_space, const char *name, MonoError *error);
-MonoClass *
-mono_class_from_name_case_checked (MonoImage *image, const char* name_space, const char *name, MonoError *error);
-MONO_PROFILER_API MonoClass *
-mono_class_from_mono_type_internal (MonoType *type);
-MONO_COMPONENT_API MonoClassField*
-mono_field_from_token_checked (MonoImage *image, uint32_t token, MonoClass **retklass, MonoGenericContext *context, MonoError *error);
-MONO_COMPONENT_API gpointer
-mono_ldtoken_checked (MonoImage *image, guint32 token, MonoClass **handle_class, MonoGenericContext *context, MonoError *error);
-MonoImage *
-mono_get_image_for_generic_param (MonoGenericParam *param);
-char *
-mono_make_generic_name_string (MonoImage *image, int num);
-MONO_COMPONENT_API MonoClass *
-mono_class_load_from_name (MonoImage *image, const char* name_space, const char *name);
-MONO_COMPONENT_API MonoClass*
-mono_class_try_load_from_name (MonoImage *image, const char* name_space, const char *name);
-void
-mono_error_set_for_class_failure (MonoError *orerror, const MonoClass *klass);
-gboolean
-mono_class_has_failure (const MonoClass *klass);
-gboolean
-mono_class_has_deferred_failure (const MonoClass *klass);
-/* Kind specific accessors */
-MONO_COMPONENT_API MonoGenericClass*
-mono_class_get_generic_class (MonoClass *klass);
-MonoGenericClass*
-mono_class_try_get_generic_class (MonoClass *klass);
-void
-mono_class_set_flags (MonoClass *klass, guint32 flags);
-MonoGenericContainer*
-mono_class_try_get_generic_container (MonoClass *klass);
-void
-mono_class_set_generic_container (MonoClass *klass, MonoGenericContainer *container);
-MonoType*
-mono_class_gtd_get_canonical_inst (MonoClass *klass);
-guint32
-mono_class_get_first_method_idx (MonoClass *klass);
-void
-mono_class_set_first_method_idx (MonoClass *klass, guint32 idx);
-guint32
-mono_class_get_first_field_idx (MonoClass *klass);
-void
-mono_class_set_first_field_idx (MonoClass *klass, guint32 idx);
-MONO_COMPONENT_API
-guint32
-mono_class_get_method_count (MonoClass *klass);
-void
-mono_class_set_method_count (MonoClass *klass, guint32 count);
-MONO_COMPONENT_API
-guint32
-mono_class_get_field_count (MonoClass *klass);
-void
-mono_class_set_field_count (MonoClass *klass, guint32 count);
-MonoMarshalType*
-mono_class_get_marshal_info (MonoClass *klass);
-void
-mono_class_set_marshal_info (MonoClass *klass, MonoMarshalType *marshal_info);
-MonoGCHandle
-mono_class_get_ref_info_handle (MonoClass *klass);
-MonoGCHandle
-mono_class_set_ref_info_handle (MonoClass *klass, gpointer value);
-MonoErrorBoxed*
-mono_class_get_exception_data (MonoClass *klass);
-void
-mono_class_set_exception_data (MonoClass *klass, MonoErrorBoxed *value);
-MONO_COMPONENT_API
-GList*
-mono_class_get_nested_classes_property (MonoClass *klass);
-MONO_COMPONENT_API
-void
-mono_class_set_nested_classes_property (MonoClass *klass, GList *value);
-MONO_COMPONENT_API MonoClassPropertyInfo*
-mono_class_get_property_info (MonoClass *klass);
-void
-mono_class_set_property_info (MonoClass *klass, MonoClassPropertyInfo *info);
-MONO_COMPONENT_API MonoClassEventInfo*
-mono_class_get_event_info (MonoClass *klass);
-void
-mono_class_set_event_info (MonoClass *klass, MonoClassEventInfo *info);
-MonoFieldDefaultValue*
-mono_class_get_field_def_values (MonoClass *klass);
-MonoFieldDefaultValue*
-mono_class_get_field_def_values_with_swizzle (MonoClass *klass, int swizzle);
-void
-mono_class_set_field_def_values (MonoClass *klass, MonoFieldDefaultValue *values);
-void
-mono_class_set_field_def_values_with_swizzle (MonoClass *klass, MonoFieldDefaultValue *values, int swizzle);
-guint32
-mono_class_get_declsec_flags (MonoClass *klass);
-void
-mono_class_set_declsec_flags (MonoClass *klass, guint32 value);
-void
-mono_class_set_is_com_object (MonoClass *klass);
-void
-mono_class_set_weak_bitmap (MonoClass *klass, int nbits, gsize *bits);
-gsize*
-mono_class_get_weak_bitmap (MonoClass *klass, int *nbits);
-gboolean
-mono_class_has_dim_conflicts (MonoClass *klass);
-gboolean
-mono_class_is_method_ambiguous (MonoClass *klass, MonoMethod *method);
-void
-mono_class_set_dim_conflicts (MonoClass *klass, GSList *conflicts);
-GSList*
-mono_class_get_dim_conflicts (MonoClass *klass);
-/* opaque struct of class specific hot reload info */
-typedef struct _MonoClassMetadataUpdateInfo MonoClassMetadataUpdateInfo;
-MONO_COMPONENT_API gboolean
-mono_class_has_metadata_update_info (MonoClass *klass);
-MONO_COMPONENT_API MonoClassMetadataUpdateInfo *
-mono_class_get_metadata_update_info (MonoClass *klass);
-MONO_COMPONENT_API void
-mono_class_set_metadata_update_info (MonoClass *klass, MonoClassMetadataUpdateInfo *value);
-MONO_COMPONENT_API MonoMethod *
-mono_class_get_method_from_name_checked (MonoClass *klass, const char *name, int param_count, int flags, MonoError *error);
-void
-mono_class_set_is_simd_type (MonoClass *klass, gboolean is_simd);
-MONO_COMPONENT_API gboolean
-mono_method_has_no_body (MonoMethod *method);
-MONO_COMPONENT_API MonoMethodHeader*
-mono_method_get_header_internal (MonoMethod *method, MonoError *error);
-gboolean
-mono_method_metadata_has_header (MonoMethod *method);
-MONO_COMPONENT_API void
-mono_method_get_param_names_internal (MonoMethod *method, const char **names);
-MonoType*
-mono_class_find_enum_basetype (MonoClass *klass, MonoError *error);
-gboolean
-mono_class_set_failure (MonoClass *klass, MonoErrorBoxed *boxed_error);
-void
-mono_class_set_deferred_failure (MonoClass *klass);
-gboolean
-mono_class_set_type_load_failure_causedby_class (MonoClass *klass, const MonoClass *caused_by, const gchar* msg);
-gboolean mono_class_get_cached_class_info (MonoClass *klass, MonoCachedClassInfo *res);
-MonoMethod* mono_find_method_in_metadata (MonoClass *klass, const char *name, int param_count, int flags);
-int
-mono_class_get_object_finalize_slot (void);
-MonoMethod *
-mono_class_get_default_finalize_method (void);
-MONO_COMPONENT_API const char *
-mono_field_get_rva (MonoClassField *field, int swizzle);
-MONO_COMPONENT_API void
-mono_field_resolve_type (MonoClassField *field, MonoError *error);
-gboolean
-mono_type_has_exceptions (MonoType *type);
-void
-mono_class_set_nonblittable (MonoClass *klass);
-gboolean
-mono_class_publish_gc_descriptor (MonoClass *klass, MonoGCDescriptor gc_descr);
-void
-mono_class_compute_gc_descriptor (MonoClass *klass);
-gboolean
-mono_class_init_checked (MonoClass *klass, MonoError *error);
-MonoType*
-mono_class_enum_basetype_internal (MonoClass *klass);
-gboolean
-mono_method_is_constructor (MonoMethod *method);
-gboolean
-mono_class_has_default_constructor (MonoClass *klass, gboolean public_only);
-gboolean
-mono_method_has_unmanaged_callers_only_attribute (MonoMethod *method);
-#define MONO_STATIC_POINTER_INIT(type, name)					\
-	static type *static_ ## name;						\
-	type *name; 								\
-	name = static_ ## name;							\
-	if (!name) {								\
-		/* Custom code here to initialize name */
-#define MONO_STATIC_POINTER_INIT_END(type, name)				\
-		if (name) {							\
-			/* Success, commit to static. */			\
-			mono_atomic_store_seq (&static_ ## name, name);		\
-		}								\
-	}									\
-/* Metadata flags for MonoClassField.  These are stored in the lowest bits of a pointer, so there
- * can't be too many. */
-enum {
-	/* This MonoClassField was added by EnC metadata update, it's not part of the
-	 * MonoClass:fields array, and at runtime it is not stored like ordinary instance or static
-	 * fields. */
-	MONO_CLASS_FIELD_META_FLAG_FROM_UPDATE = 0x01u,
-	/* Lowest 2 bits of a pointer reserved for flags */
-	MONO_CLASS_FIELD_META_FLAG_MASK = 0x03u,
-};
-static inline MonoClass *
-m_field_get_parent (MonoClassField *field)
-{
-	return (MonoClass*)(field->parent_and_flags & ~MONO_CLASS_FIELD_META_FLAG_MASK);
-}
-static inline unsigned int
-m_field_get_meta_flags (MonoClassField *field)
-{
-	return (unsigned int)(field->parent_and_flags & MONO_CLASS_FIELD_META_FLAG_MASK);
-}
-static inline int
-m_field_get_offset (MonoClassField *field)
-{
-	g_assert (m_class_is_fields_inited (m_field_get_parent (field)));
-	return field->offset;
-}
-static inline gboolean
-m_field_is_from_update (MonoClassField *field)
-{
-	return (m_field_get_meta_flags (field) & MONO_CLASS_FIELD_META_FLAG_FROM_UPDATE) != 0;
-}
-static inline gboolean
-m_property_is_from_update (MonoProperty *prop)
-{
-	return (prop->attrs & MONO_PROPERTY_META_FLAG_FROM_UPDATE) != 0;
-}
-static inline gboolean
-m_event_is_from_update (MonoEvent *evt)
-{
-	return (evt->attrs & MONO_EVENT_META_FLAG_FROM_UPDATE) != 0;
-}
-/*
- * Memory allocation for images/classes/methods
- *
- *   These should be used to allocate memory whose lifetime is equal to
- * the lifetime of the image/class/method.
- */
-static inline MonoMemoryManager*
-mono_mem_manager_get_ambient (void)
-{
-	return (MonoMemoryManager *)mono_alc_get_default ()->memory_manager;
-}
-static inline MonoMemoryManager*
-m_image_get_mem_manager (MonoImage *image)
-{
-	MonoAssemblyLoadContext *alc = mono_image_get_alc (image);
-	if (!alc)
-		alc = mono_alc_get_default ();
-	return alc->memory_manager;
-}
-static inline void *
-m_image_alloc (MonoImage *image, guint size)
-{
-	return mono_mem_manager_alloc (m_image_get_mem_manager (image), size);
-}
-static inline void *
-m_image_alloc0 (MonoImage *image, guint size)
-{
-	return mono_mem_manager_alloc0 (m_image_get_mem_manager (image), size);
-}
-static inline MonoMemoryManager*
-m_class_get_mem_manager (MonoClass *klass)
-{
-	if (m_class_get_class_kind (klass) == MONO_CLASS_GINST)
-		return mono_class_get_generic_class (klass)->owner;
-	if (m_class_get_rank (klass))
-		return m_class_get_mem_manager (m_class_get_element_class (klass));
-	MonoAssemblyLoadContext *alc = mono_image_get_alc (m_class_get_image (klass));
-	if (alc)
-		return alc->memory_manager;
-	else
-		/* Dynamic assemblies */
-		return mono_mem_manager_get_ambient ();
-}
-static inline void *
-m_class_alloc (MonoClass *klass, guint size)
-{
-	return mono_mem_manager_alloc (m_class_get_mem_manager (klass), size);
-}
-static inline void *
-m_class_alloc0 (MonoClass *klass, guint size)
-{
-	return mono_mem_manager_alloc0 (m_class_get_mem_manager (klass), size);
-}
-static inline MonoMemoryManager*
-m_method_get_mem_manager (MonoMethod *method)
-{
-	if (method->is_inflated)
-		return ((MonoMethodInflated*)method)->owner;
-	else if (method->wrapper_type && ((MonoMethodWrapper*)method)->mem_manager)
-		return ((MonoMethodWrapper*)method)->mem_manager;
-	else
-		return m_class_get_mem_manager (method->klass);
-}
-static inline void *
-m_method_alloc (MonoMethod *method, guint size)
-{
-	return mono_mem_manager_alloc (m_method_get_mem_manager (method), size);
-}
-static inline void *
-m_method_alloc0 (MonoMethod *method, guint size)
-{
-	return mono_mem_manager_alloc0 (m_method_get_mem_manager (method), size);
-}
-#include "jit-icall-reg.h"
-/*Now that everything has been defined, let's include the inline functions */
-#include <mono/metadata/class-inlines.h>
-#endif /* __MONO_METADATA_CLASS_INTERNALS_H__ */

--- a/src/mono/mono/mini/method-to-ir.c
+++ b//dev/null
@@ -1,11711 +0,0 @@
-/**
- * \file
- * Convert CIL to the JIT internal representation
- *
- * Author:
- *   Paolo Molaro (lupus@ximian.com)
- *   Dietmar Maurer (dietmar@ximian.com)
- *
- * (C) 2002 Ximian, Inc.
- * Copyright 2003-2010 Novell, Inc (http://www.novell.com)
- * Copyright 2011 Xamarin, Inc (http://www.xamarin.com)
- * Licensed under the MIT license. See LICENSE file in the project root for full license information.
- */
-#include <config.h>
-#include <glib.h>
-#include <mono/utils/mono-compiler.h>
-#include "mini.h"
-#ifndef DISABLE_JIT
-#include <signal.h>
-#ifdef HAVE_UNISTD_H
-#include <unistd.h>
-#endif
-#include <math.h>
-#include <string.h>
-#include <ctype.h>
-#ifdef HAVE_SYS_TIME_H
-#include <sys/time.h>
-#endif
-#ifdef HAVE_ALLOCA_H
-#include <alloca.h>
-#endif
-#include <mono/utils/memcheck.h>
-#include <mono/metadata/abi-details.h>
-#include <mono/metadata/assembly.h>
-#include <mono/metadata/assembly-internals.h>
-#include <mono/metadata/attrdefs.h>
-#include <mono/metadata/loader.h>
-#include <mono/metadata/tabledefs.h>
-#include <mono/metadata/class.h>
-#include <mono/metadata/class-abi-details.h>
-#include <mono/metadata/object.h>
-#include <mono/metadata/exception.h>
-#include <mono/metadata/exception-internals.h>
-#include <mono/metadata/opcodes.h>
-#include <mono/metadata/mono-endian.h>
-#include <mono/metadata/tokentype.h>
-#include <mono/metadata/tabledefs.h>
-#include <mono/metadata/marshal.h>
-#include <mono/metadata/debug-helpers.h>
-#include <mono/metadata/debug-internals.h>
-#include <mono/metadata/gc-internals.h>
-#include <mono/metadata/threads-types.h>
-#include <mono/metadata/profiler-private.h>
-#include <mono/metadata/profiler.h>
-#include <mono/metadata/monitor.h>
-#include <mono/utils/mono-memory-model.h>
-#include <mono/utils/mono-error-internals.h>
-#include <mono/metadata/mono-basic-block.h>
-#include <mono/metadata/reflection-internals.h>
-#include <mono/utils/mono-threads-coop.h>
-#include <mono/utils/mono-utils-debug.h>
-#include <mono/utils/mono-logger-internals.h>
-#include <mono/metadata/verify-internals.h>
-#include <mono/metadata/icall-decl.h>
-#include "mono/metadata/icall-signatures.h"
-#include "trace.h"
-#include "ir-emit.h"
-#include "jit-icalls.h"
-#include <mono/jit/jit.h>
-#include "seq-points.h"
-#include "aot-compiler.h"
-#include "mini-llvm.h"
-#include "mini-runtime.h"
-#include "llvmonly-runtime.h"
-#include "mono/utils/mono-tls-inline.h"
-MONO_DISABLE_WARNING(4127) /* conditional expression is constant */
-#define BRANCH_COST 10
-#define CALL_COST 10
-/* Used for the JIT */
-#define INLINE_LENGTH_LIMIT 20
-/*
- * The aot and jit inline limits should be different,
- * since aot sees the whole program so we can let opt inline methods for us,
- * while the jit only sees one method, so we have to inline things ourselves.
- */
-/* Used by LLVM AOT */
-#define LLVM_AOT_INLINE_LENGTH_LIMIT 30
-/* Used to LLVM JIT */
-#define LLVM_JIT_INLINE_LENGTH_LIMIT 100
-static const gboolean debug_tailcall = FALSE;               // logging
-static const gboolean debug_tailcall_try_all = FALSE;       // consider any call followed by ret
-gboolean
-mono_tailcall_print_enabled (void)
-{
-	return debug_tailcall || MONO_TRACE_IS_TRACED (G_LOG_LEVEL_DEBUG, MONO_TRACE_TAILCALL);
-}
-void
-mono_tailcall_print (const char *format, ...)
-{
-	if (!mono_tailcall_print_enabled ())
-		return;
-	va_list args;
-	va_start (args, format);
-	g_printv (format, args);
-	va_end (args);
-}
-/* These have 'cfg' as an implicit argument */
-#define INLINE_FAILURE(msg) do {									\
-	if ((cfg->method != cfg->current_method) && (cfg->current_method->wrapper_type == MONO_WRAPPER_NONE)) { \
-		inline_failure (cfg, msg);										\
-		goto exception_exit;											\
-	} \
-	} while (0)
-#define CHECK_CFG_EXCEPTION do {\
-		if (cfg->exception_type != MONO_EXCEPTION_NONE)	\
-			goto exception_exit;						\
-	} while (0)
-#define FIELD_ACCESS_FAILURE(method, field) do {					\
-		field_access_failure ((cfg), (method), (field));			\
-		goto exception_exit;	\
-	} while (0)
-#define GENERIC_SHARING_FAILURE(opcode) do {		\
-		if (cfg->gshared) {									\
-			gshared_failure (cfg, opcode, __FILE__, __LINE__);	\
-			goto exception_exit;	\
-		}			\
-	} while (0)
-#define GSHAREDVT_FAILURE(opcode) do {		\
-	if (cfg->gsharedvt) {												\
-		gsharedvt_failure (cfg, opcode, __FILE__, __LINE__);			\
-		goto exception_exit;											\
-	}																	\
-	} while (0)
-#define OUT_OF_MEMORY_FAILURE do {	\
-		mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);		\
-		mono_error_set_out_of_memory (cfg->error, "");					\
-		goto exception_exit;	\
-	} while (0)
-#define DISABLE_AOT(cfg) do { \
-		if ((cfg)->verbose_level >= 2)						  \
-			printf ("AOT disabled: %s:%d\n", __FILE__, __LINE__);	\
-		(cfg)->disable_aot = TRUE;							  \
-	} while (0)
-#define LOAD_ERROR do { \
-		break_on_unverified ();								\
-		mono_cfg_set_exception (cfg, MONO_EXCEPTION_TYPE_LOAD); \
-		goto exception_exit;									\
-	} while (0)
-#define TYPE_LOAD_ERROR(klass) do { \
-		cfg->exception_ptr = klass; \
-		LOAD_ERROR;					\
-	} while (0)
-#define CHECK_CFG_ERROR do {\
-		if (!is_ok (cfg->error)) { \
-			mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);	\
-			goto mono_error_exit; \
-		} \
-	} while (0)
-int mono_op_to_op_imm (int opcode);
-int mono_op_to_op_imm_noemul (int opcode);
-static int inline_method (MonoCompile *cfg, MonoMethod *cmethod, MonoMethodSignature *fsig, MonoInst **sp,
-						  guchar *ip, guint real_offset, gboolean inline_always, gboolean *is_empty);
-static MonoInst*
-convert_value (MonoCompile *cfg, MonoType *type, MonoInst *ins);
-/*
- * Instruction metadata
- */
-#ifdef MINI_OP
-#undef MINI_OP
-#endif
-#ifdef MINI_OP3
-#undef MINI_OP3
-#endif
-#define MINI_OP(a,b,dest,src1,src2) dest, src1, src2, ' ',
-#define MINI_OP3(a,b,dest,src1,src2,src3) dest, src1, src2, src3,
-#define NONE ' '
-#define IREG 'i'
-#define FREG 'f'
-#define VREG 'v'
-#define XREG 'x'
-#if SIZEOF_REGISTER == 8 && SIZEOF_REGISTER == TARGET_SIZEOF_VOID_P
-#define LREG IREG
-#else
-#define LREG 'l'
-#endif
-/* keep in sync with the enum in mini.h */
-const char
-mini_ins_info[] = {
-#include "mini-ops.h"
-};
-#undef MINI_OP
-#undef MINI_OP3
-#define MINI_OP(a,b,dest,src1,src2) ((src2) != NONE ? 2 : ((src1) != NONE ? 1 : 0)),
-#define MINI_OP3(a,b,dest,src1,src2,src3) ((src3) != NONE ? 3 : ((src2) != NONE ? 2 : ((src1) != NONE ? 1 : 0))),
-/*
- * This should contain the index of the last sreg + 1. This is not the same
- * as the number of sregs for opcodes like IA64_CMP_EQ_IMM.
- */
-const gint8 mini_ins_sreg_counts[] = {
-#include "mini-ops.h"
-};
-#undef MINI_OP
-#undef MINI_OP3
-guint32
-mono_alloc_ireg (MonoCompile *cfg)
-{
-	return alloc_ireg (cfg);
-}
-guint32
-mono_alloc_lreg (MonoCompile *cfg)
-{
-	return alloc_lreg (cfg);
-}
-guint32
-mono_alloc_freg (MonoCompile *cfg)
-{
-	return alloc_freg (cfg);
-}
-guint32
-mono_alloc_preg (MonoCompile *cfg)
-{
-	return alloc_preg (cfg);
-}
-guint32
-mono_alloc_dreg (MonoCompile *cfg, MonoStackType stack_type)
-{
-	return alloc_dreg (cfg, stack_type);
-}
-/*
- * mono_alloc_ireg_ref:
- *
- *   Allocate an IREG, and mark it as holding a GC ref.
- */
-guint32
-mono_alloc_ireg_ref (MonoCompile *cfg)
-{
-	return alloc_ireg_ref (cfg);
-}
-/*
- * mono_alloc_ireg_mp:
- *
- *   Allocate an IREG, and mark it as holding a managed pointer.
- */
-guint32
-mono_alloc_ireg_mp (MonoCompile *cfg)
-{
-	return alloc_ireg_mp (cfg);
-}
-/*
- * mono_alloc_ireg_copy:
- *
- *   Allocate an IREG with the same GC type as VREG.
- */
-guint32
-mono_alloc_ireg_copy (MonoCompile *cfg, guint32 vreg)
-{
-	if (vreg_is_ref (cfg, vreg))
-		return alloc_ireg_ref (cfg);
-	else if (vreg_is_mp (cfg, vreg))
-		return alloc_ireg_mp (cfg);
-	else
-		return alloc_ireg (cfg);
-}
-guint
-mono_type_to_regmove (MonoCompile *cfg, MonoType *type)
-{
-	if (m_type_is_byref (type))
-		return OP_MOVE;
-	type = mini_get_underlying_type (type);
-handle_enum:
-	switch (type->type) {
-	case MONO_TYPE_I1:
-	case MONO_TYPE_U1:
-		return OP_MOVE;
-	case MONO_TYPE_I2:
-	case MONO_TYPE_U2:
-		return OP_MOVE;
-	case MONO_TYPE_I4:
-	case MONO_TYPE_U4:
-		return OP_MOVE;
-	case MONO_TYPE_I:
-	case MONO_TYPE_U:
-	case MONO_TYPE_PTR:
-	case MONO_TYPE_FNPTR:
-		return OP_MOVE;
-	case MONO_TYPE_CLASS:
-	case MONO_TYPE_STRING:
-	case MONO_TYPE_OBJECT:
-	case MONO_TYPE_SZARRAY:
-	case MONO_TYPE_ARRAY:
-		return OP_MOVE;
-	case MONO_TYPE_I8:
-	case MONO_TYPE_U8:
-#if SIZEOF_REGISTER == 8
-		return OP_MOVE;
-#else
-		return OP_LMOVE;
-#endif
-	case MONO_TYPE_R4:
-		return cfg->r4fp ? OP_RMOVE : OP_FMOVE;
-	case MONO_TYPE_R8:
-		return OP_FMOVE;
-	case MONO_TYPE_VALUETYPE:
-		if (m_class_is_enumtype (type->data.klass)) {
-			type = mono_class_enum_basetype_internal (type->data.klass);
-			goto handle_enum;
-		}
-		if (mini_class_is_simd (cfg, mono_class_from_mono_type_internal (type)))
-			return OP_XMOVE;
-		return OP_VMOVE;
-	case MONO_TYPE_TYPEDBYREF:
-		return OP_VMOVE;
-	case MONO_TYPE_GENERICINST:
-		if (mini_class_is_simd (cfg, mono_class_from_mono_type_internal (type)))
-			return OP_XMOVE;
-		type = m_class_get_byval_arg (type->data.generic_class->container_class);
-		goto handle_enum;
-	case MONO_TYPE_VAR:
-	case MONO_TYPE_MVAR:
-		g_assert (cfg->gshared);
-		if (mini_type_var_is_vt (type))
-			return OP_VMOVE;
-		else
-			return mono_type_to_regmove (cfg, mini_get_underlying_type (type));
-	default:
-		g_error ("unknown type 0x%02x in type_to_regstore", type->type);
-	}
-	return -1;
-}
-void
-mono_print_bb (MonoBasicBlock *bb, const char *msg)
-{
-	int i;
-	MonoInst *tree;
-	GString *str = g_string_new ("");
-	g_string_append_printf (str, "%s %d: [IN: ", msg, bb->block_num);
-	for (i = 0; i < bb->in_count; ++i)
-		g_string_append_printf (str, " BB%d(%d)", bb->in_bb [i]->block_num, bb->in_bb [i]->dfn);
-	g_string_append_printf (str, ", OUT: ");
-	for (i = 0; i < bb->out_count; ++i)
-		g_string_append_printf (str, " BB%d(%d)", bb->out_bb [i]->block_num, bb->out_bb [i]->dfn);
-	g_string_append_printf (str, " ]\n");
-	g_print ("%s", str->str);
-	g_string_free (str, TRUE);
-	for (tree = bb->code; tree; tree = tree->next)
-		mono_print_ins_index (-1, tree);
-}
-static MONO_NEVER_INLINE gboolean
-break_on_unverified (void)
-{
-	if (mini_debug_options.break_on_unverified) {
-		G_BREAKPOINT ();
-		return TRUE;
-	}
-	return FALSE;
-}
-static void
-clear_cfg_error (MonoCompile *cfg)
-{
-	mono_error_cleanup (cfg->error);
-	error_init (cfg->error);
-}
-static MONO_NEVER_INLINE void
-field_access_failure (MonoCompile *cfg, MonoMethod *method, MonoClassField *field)
-{
-	char *method_fname = mono_method_full_name (method, TRUE);
-	char *field_fname = mono_field_full_name (field);
-	mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
-	mono_error_set_generic_error (cfg->error, "System", "FieldAccessException", "Field `%s' is inaccessible from method `%s'\n", field_fname, method_fname);
-	g_free (method_fname);
-	g_free (field_fname);
-}
-static MONO_NEVER_INLINE void
-inline_failure (MonoCompile *cfg, const char *msg)
-{
-	if (cfg->verbose_level >= 2)
-		printf ("inline failed: %s\n", msg);
-	mono_cfg_set_exception (cfg, MONO_EXCEPTION_INLINE_FAILED);
-}
-static MONO_NEVER_INLINE void
-gshared_failure (MonoCompile *cfg, int opcode, const char *file, int line)
-{
-	if (cfg->verbose_level > 2)
-		printf ("sharing failed for method %s.%s.%s/%d opcode %s line %d\n", m_class_get_name_space (cfg->current_method->klass), m_class_get_name (cfg->current_method->klass), cfg->current_method->name, cfg->current_method->signature->param_count, mono_opcode_name (opcode), line);
-	mono_cfg_set_exception (cfg, MONO_EXCEPTION_GENERIC_SHARING_FAILED);
-}
-static MONO_NEVER_INLINE void
-gsharedvt_failure (MonoCompile *cfg, int opcode, const char *file, int line)
-{
-	cfg->exception_message = g_strdup_printf ("gsharedvt failed for method %s.%s.%s/%d opcode %s %s:%d", m_class_get_name_space (cfg->current_method->klass), m_class_get_name (cfg->current_method->klass), cfg->current_method->name, cfg->current_method->signature->param_count, mono_opcode_name ((opcode)), file, line);
-	if (cfg->verbose_level >= 2)
-		printf ("%s\n", cfg->exception_message);
-	mono_cfg_set_exception (cfg, MONO_EXCEPTION_GENERIC_SHARING_FAILED);
-}
-void
-mini_set_inline_failure (MonoCompile *cfg, const char *msg)
-{
-	if (cfg->verbose_level >= 2)
-		printf ("inline failed: %s\n", msg);
-	mono_cfg_set_exception (cfg, MONO_EXCEPTION_INLINE_FAILED);
-}
-/*
- * When using gsharedvt, some instatiations might be verifiable, and some might be not. i.e.
- * foo<T> (int i) { ldarg.0; box T; }
- */
-#define UNVERIFIED do { \
-	if (cfg->gsharedvt) { \
-		if (cfg->verbose_level > 2) \
-			printf ("gsharedvt method failed to verify, falling back to instantiation.\n"); \
-		mono_cfg_set_exception (cfg, MONO_EXCEPTION_GENERIC_SHARING_FAILED); \
-		goto exception_exit; \
-	} \
-	break_on_unverified (); \
-	goto unverified; \
-} while (0)
-#define GET_BBLOCK(cfg,tblock,ip) do { \
-		if ((ip) >= end || (ip) < header->code) { UNVERIFIED; }	\
-		(tblock) = cfg->cil_offset_to_bb [(ip) - cfg->cil_start]; \
-		if (!(tblock)) { \
-			NEW_BBLOCK (cfg, (tblock)); \
-			(tblock)->cil_code = (ip); \
-			ADD_BBLOCK (cfg, (tblock)); \
-		} \
-	} while (0)
-/* Emit conversions so both operands of a binary opcode are of the same type */
-static void
-add_widen_op (MonoCompile *cfg, MonoInst *ins, MonoInst **arg1_ref, MonoInst **arg2_ref)
-{
-	MonoInst *arg1 = *arg1_ref;
-	MonoInst *arg2 = *arg2_ref;
-	if (cfg->r4fp &&
-		((arg1->type == STACK_R4 && arg2->type == STACK_R8) ||
-		 (arg1->type == STACK_R8 && arg2->type == STACK_R4))) {
-		MonoInst *conv;
-		/* Mixing r4/r8 is allowed by the spec */
-		if (arg1->type == STACK_R4) {
-			int dreg = alloc_freg (cfg);
-			EMIT_NEW_UNALU (cfg, conv, OP_RCONV_TO_R8, dreg, arg1->dreg);
-			conv->type = STACK_R8;
-			ins->sreg1 = dreg;
-			*arg1_ref = conv;
-		}
-		if (arg2->type == STACK_R4) {
-			int dreg = alloc_freg (cfg);
-			EMIT_NEW_UNALU (cfg, conv, OP_RCONV_TO_R8, dreg, arg2->dreg);
-			conv->type = STACK_R8;
-			ins->sreg2 = dreg;
-			*arg2_ref = conv;
-		}
-	}
-#if SIZEOF_REGISTER == 8
-	/* FIXME: Need to add many more cases */
-	if ((arg1)->type == STACK_PTR && (arg2)->type == STACK_I4) {
-		MonoInst *widen;
-		int dr = alloc_preg (cfg);
-		EMIT_NEW_UNALU (cfg, widen, OP_SEXT_I4, dr, (arg2)->dreg);
-		(ins)->sreg2 = widen->dreg;
-	}
-#endif
-}
-#define ADD_UNOP(op) do { \
-		MONO_INST_NEW (cfg, ins, (op)); \
-		sp--; \
-		ins->sreg1 = sp [0]->dreg; \
-		type_from_op (cfg, ins, sp [0], NULL); \
-		CHECK_TYPE (ins); \
-		(ins)->dreg = alloc_dreg ((cfg), (MonoStackType)(ins)->type); \
-		MONO_ADD_INS ((cfg)->cbb, (ins)); \
-		*sp++ = mono_decompose_opcode (cfg, ins); \
-	} while (0)
-#define ADD_BINCOND(next_block) do { \
-		MonoInst *cmp; \
-		sp -= 2; \
-		MONO_INST_NEW(cfg, cmp, OP_COMPARE); \
-		cmp->sreg1 = sp [0]->dreg; \
-		cmp->sreg2 = sp [1]->dreg; \
-		add_widen_op (cfg, cmp, &sp [0], &sp [1]); \
-		type_from_op (cfg, cmp, sp [0], sp [1]); \
-		CHECK_TYPE (cmp); \
-		type_from_op (cfg, ins, sp [0], sp [1]); \
-		ins->inst_many_bb = (MonoBasicBlock **)mono_mempool_alloc (cfg->mempool, sizeof(gpointer)*2); \
-		GET_BBLOCK (cfg, tblock, target); \
-		link_bblock (cfg, cfg->cbb, tblock); \
-		ins->inst_true_bb = tblock; \
-		MONO_DISABLE_WARNING(4127) \
-		if ((next_block)) { \
-			link_bblock (cfg, cfg->cbb, (next_block)); \
-			ins->inst_false_bb = (next_block); \
-			start_new_bblock = 1; \
-		} else { \
-			GET_BBLOCK (cfg, tblock, next_ip); \
-			link_bblock (cfg, cfg->cbb, tblock); \
-			ins->inst_false_bb = tblock; \
-			start_new_bblock = 2; \
-		} \
-		MONO_RESTORE_WARNING \
-		if (sp != stack_start) { \
-			handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start)); \
-			CHECK_UNVERIFIABLE (cfg); \
-		} \
-		MONO_ADD_INS (cfg->cbb, cmp); \
-		MONO_ADD_INS (cfg->cbb, ins); \
-	} while (0)
-/* *
- * link_bblock: Links two basic blocks
- *
- * links two basic blocks in the control flow graph, the 'from'
- * argument is the starting block and the 'to' argument is the block
- * the control flow ends to after 'from'.
- */
-static void
-link_bblock (MonoCompile *cfg, MonoBasicBlock *from, MonoBasicBlock* to)
-{
-	MonoBasicBlock **newa;
-	int i, found;
-#if 0
-	if (from->cil_code) {
-		if (to->cil_code)
-			printf ("edge from IL%04x to IL_%04x\n", from->cil_code - cfg->cil_code, to->cil_code - cfg->cil_code);
-		else
-			printf ("edge from IL%04x to exit\n", from->cil_code - cfg->cil_code);
-	} else {
-		if (to->cil_code)
-			printf ("edge from entry to IL_%04x\n", to->cil_code - cfg->cil_code);
-		else
-			printf ("edge from entry to exit\n");
-	}
-#endif
-	found = FALSE;
-	for (i = 0; i < from->out_count; ++i) {
-		if (to == from->out_bb [i]) {
-			found = TRUE;
-			break;
-		}
-	}
-	if (!found) {
-		newa = (MonoBasicBlock **)mono_mempool_alloc (cfg->mempool, sizeof (gpointer) * (from->out_count + 1));
-		for (i = 0; i < from->out_count; ++i) {
-			newa [i] = from->out_bb [i];
-		}
-		newa [i] = to;
-		from->out_count++;
-		from->out_bb = newa;
-	}
-	found = FALSE;
-	for (i = 0; i < to->in_count; ++i) {
-		if (from == to->in_bb [i]) {
-			found = TRUE;
-			break;
-		}
-	}
-	if (!found) {
-		newa = (MonoBasicBlock **)mono_mempool_alloc (cfg->mempool, sizeof (gpointer) * (to->in_count + 1));
-		for (i = 0; i < to->in_count; ++i) {
-			newa [i] = to->in_bb [i];
-		}
-		newa [i] = from;
-		to->in_count++;
-		to->in_bb = newa;
-	}
-}
-void
-mono_link_bblock (MonoCompile *cfg, MonoBasicBlock *from, MonoBasicBlock* to)
-{
-	link_bblock (cfg, from, to);
-}
-static void
-mono_create_spvar_for_region (MonoCompile *cfg, int region);
-static void
-mark_bb_in_region (MonoCompile *cfg, guint region, uint32_t start, uint32_t end)
-{
-	MonoBasicBlock *bb = cfg->cil_offset_to_bb [start];
-	g_assert (bb);
-	if (cfg->verbose_level > 1)
-		g_print ("FIRST BB for %d is BB_%d\n", start, bb->block_num);
-	for (; bb && bb->real_offset < end; bb = bb->next_bb) {
-		if (bb->region == -1) {
-			bb->region = region;
-			continue;
-		}
-		if ((bb->region & (0xf << 4)) != MONO_REGION_TRY) {
-			continue;
-		}
-		if ((region & (0xf << 4)) != MONO_REGION_TRY) {
-			bb->region = region;
-		}
-	}
-	if (cfg->spvars)
-		mono_create_spvar_for_region (cfg, region);
-}
-static void
-compute_bb_regions (MonoCompile *cfg)
-{
-	MonoMethodHeader *header = cfg->header;
-	for (MonoBasicBlock *bb = cfg->bb_entry; bb; bb = bb->next_bb)
-		bb->region = -1;
-	for (guint i = 0; i < header->num_clauses; ++i) {
-		MonoExceptionClause *clause = &header->clauses [i];
-		if (clause->flags == MONO_EXCEPTION_CLAUSE_FILTER)
-			mark_bb_in_region (cfg, ((i + 1) << 8) | MONO_REGION_FILTER | clause->flags, clause->data.filter_offset, clause->handler_offset);
-		guint handler_region;
-		if (clause->flags == MONO_EXCEPTION_CLAUSE_FINALLY)
-			handler_region = ((i + 1) << 8) | MONO_REGION_FINALLY | clause->flags;
-		else if (clause->flags == MONO_EXCEPTION_CLAUSE_FAULT)
-			handler_region = ((i + 1) << 8) | MONO_REGION_FAULT | clause->flags;
-		else
-			handler_region = ((i + 1) << 8) | MONO_REGION_CATCH | clause->flags;
-		mark_bb_in_region (cfg, handler_region, clause->handler_offset, clause->handler_offset + clause->handler_len);
-		mark_bb_in_region (cfg, ((i + 1) << 8) | clause->flags, clause->try_offset, clause->try_offset + clause->try_len);
-	}
-	if (cfg->verbose_level > 2) {
-		for (MonoBasicBlock *bb = cfg->bb_entry; bb; bb = bb->next_bb)
-			g_print ("REGION BB%d IL_%04x ID_%08X\n", bb->block_num, bb->real_offset, bb->region);
-	}
-}
-static gboolean
-ip_in_finally_clause (MonoCompile *cfg, int offset)
-{
-	MonoMethodHeader *header = cfg->header;
-	MonoExceptionClause *clause;
-	for (guint i = 0; i < header->num_clauses; ++i) {
-		clause = &header->clauses [i];
-		if (clause->flags != MONO_EXCEPTION_CLAUSE_FINALLY && clause->flags != MONO_EXCEPTION_CLAUSE_FAULT)
-			continue;
-		if (MONO_OFFSET_IN_HANDLER (clause, GINT_TO_UINT32(offset)))
-			return TRUE;
-	}
-	return FALSE;
-}
-/* Find clauses between ip and target, from inner to outer */
-static GList*
-mono_find_leave_clauses (MonoCompile *cfg, guchar *ip, guchar *target)
-{
-	MonoMethodHeader *header = cfg->header;
-	MonoExceptionClause *clause;
-	GList *res = NULL;
-	for (guint i = 0; i < header->num_clauses; ++i) {
-		clause = &header->clauses [i];
-		if (MONO_OFFSET_IN_CLAUSE (clause, GPTRDIFF_TO_UINT32(ip - header->code)) &&
-		    (!MONO_OFFSET_IN_CLAUSE (clause, GPTRDIFF_TO_UINT32(target - header->code)))) {
-			MonoLeaveClause *leave = mono_mempool_alloc0 (cfg->mempool, sizeof (MonoLeaveClause));
-			leave->index = i;
-			leave->clause = clause;
-			res = g_list_append_mempool (cfg->mempool, res, leave);
-		}
-	}
-	return res;
-}
-static void
-mono_create_spvar_for_region (MonoCompile *cfg, int region)
-{
-	MonoInst *var;
-	var = (MonoInst *)g_hash_table_lookup (cfg->spvars, GINT_TO_POINTER (region));
-	if (var)
-		return;
-	var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
-	/* prevent it from being register allocated */
-	var->flags |= MONO_INST_VOLATILE;
-	g_hash_table_insert (cfg->spvars, GINT_TO_POINTER (region), var);
-}
-MonoInst *
-mono_find_exvar_for_offset (MonoCompile *cfg, int offset)
-{
-	return (MonoInst *)g_hash_table_lookup (cfg->exvars, GINT_TO_POINTER (offset));
-}
-static MonoInst*
-mono_create_exvar_for_offset (MonoCompile *cfg, int offset)
-{
-	MonoInst *var;
-	var = (MonoInst *)g_hash_table_lookup (cfg->exvars, GINT_TO_POINTER (offset));
-	if (var)
-		return var;
-	var = mono_compile_create_var (cfg, mono_get_object_type (), OP_LOCAL);
-	/* prevent it from being register allocated */
-	var->flags |= MONO_INST_VOLATILE;
-	g_hash_table_insert (cfg->exvars, GINT_TO_POINTER (offset), var);
-	return var;
-}
-/*
- * Returns the type used in the eval stack when @type is loaded.
- * FIXME: return a MonoType/MonoClass for the byref and VALUETYPE cases.
- */
-void
-mini_type_to_eval_stack_type (MonoCompile *cfg, MonoType *type, MonoInst *inst)
-{
-	MonoClass *klass;
-	type = mini_get_underlying_type (type);
-	inst->klass = klass = mono_class_from_mono_type_internal (type);
-	if (m_type_is_byref (type)) {
-		inst->type = STACK_MP;
-		return;
-	}
-handle_enum:
-	switch (type->type) {
-	case MONO_TYPE_VOID:
-		inst->type = STACK_INV;
-		return;
-	case MONO_TYPE_I1:
-	case MONO_TYPE_U1:
-	case MONO_TYPE_I2:
-	case MONO_TYPE_U2:
-	case MONO_TYPE_I4:
-	case MONO_TYPE_U4:
-		inst->type = STACK_I4;
-		return;
-	case MONO_TYPE_I:
-	case MONO_TYPE_U:
-	case MONO_TYPE_PTR:
-	case MONO_TYPE_FNPTR:
-		inst->type = STACK_PTR;
-		return;
-	case MONO_TYPE_CLASS:
-	case MONO_TYPE_STRING:
-	case MONO_TYPE_OBJECT:
-	case MONO_TYPE_SZARRAY:
-	case MONO_TYPE_ARRAY:
-		inst->type = STACK_OBJ;
-		return;
-	case MONO_TYPE_I8:
-	case MONO_TYPE_U8:
-		inst->type = STACK_I8;
-		return;
-	case MONO_TYPE_R4:
-		inst->type = GINT_TO_UINT8 (cfg->r4_stack_type);
-		break;
-	case MONO_TYPE_R8:
-		inst->type = STACK_R8;
-		return;
-	case MONO_TYPE_VALUETYPE:
-		if (m_class_is_enumtype (type->data.klass)) {
-			type = mono_class_enum_basetype_internal (type->data.klass);
-			goto handle_enum;
-		} else {
-			inst->klass = klass;
-			inst->type = STACK_VTYPE;
-			return;
-		}
-	case MONO_TYPE_TYPEDBYREF:
-		inst->klass = mono_defaults.typed_reference_class;
-		inst->type = STACK_VTYPE;
-		return;
-	case MONO_TYPE_GENERICINST:
-		type = m_class_get_byval_arg (type->data.generic_class->container_class);
-		goto handle_enum;
-	case MONO_TYPE_VAR:
-	case MONO_TYPE_MVAR:
-		g_assert (cfg->gshared);
-		if (mini_is_gsharedvt_type (type)) {
-			g_assert (cfg->gsharedvt);
-			inst->type = STACK_VTYPE;
-		} else {
-			mini_type_to_eval_stack_type (cfg, mini_get_underlying_type (type), inst);
-		}
-		return;
-	default:
-		g_error ("unknown type 0x%02x in eval stack type", type->type);
-	}
-}
-/*
- * The following tables are used to quickly validate the IL code in type_from_op ().
- */
-#define IF_P8(v) (SIZEOF_VOID_P == 8 ? v : STACK_INV)
-#define IF_P8_I8 IF_P8(STACK_I8)
-#define IF_P8_PTR IF_P8(STACK_PTR)
-static const char
-bin_num_table [STACK_MAX] [STACK_MAX] = {
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_I4,  IF_P8_I8,  STACK_PTR, STACK_INV, STACK_MP,  STACK_INV, STACK_INV},
-	{STACK_INV, IF_P8_I8,  STACK_I8,  IF_P8_PTR, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_PTR, IF_P8_PTR, STACK_PTR, STACK_INV, STACK_MP,  STACK_INV, STACK_INV},
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_R8,  STACK_INV, STACK_INV, STACK_INV, STACK_R8},
-	{STACK_INV, STACK_MP,  STACK_INV, STACK_MP,  STACK_INV, STACK_PTR, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_R8, STACK_INV, STACK_INV, STACK_INV, STACK_R4}
-};
-static const char
-neg_table [] = {
-	STACK_INV, STACK_I4, STACK_I8, STACK_PTR, STACK_R8, STACK_INV, STACK_INV, STACK_INV, STACK_R4
-};
-/* reduce the size of this table */
-static const char
-bin_int_table [STACK_MAX] [STACK_MAX] = {
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_I4,  IF_P8_I8,  STACK_PTR, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, IF_P8_I8,  STACK_I8,  IF_P8_PTR, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_PTR, IF_P8_PTR, STACK_PTR, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV}
-};
-#define P1 (SIZEOF_VOID_P == 8)
-static const char
-bin_comp_table [STACK_MAX] [STACK_MAX] = {
-/*	Inv i  L  p  F  &  O  vt r4 */
-	{0},
-	{0, 1, 0, 1, 0, 0, 0, 0}, /* i, int32 */
-	{0, 0, 1,P1, 0, 0, 0, 0}, /* L, int64 */
-	{0, 1,P1, 1, 0, 2, 4, 0}, /* p, ptr */
-	{0, 0, 0, 0, 1, 0, 0, 0, 1}, /* F, R8 */
-	{0, 0, 0, 2, 0, 1, 0, 0}, /* &, managed pointer */
-	{0, 0, 0, 4, 0, 0, 3, 0}, /* O, reference */
-	{0, 0, 0, 0, 0, 0, 0, 0}, /* vt value type */
-	{0, 0, 0, 0, 1, 0, 0, 0, 1}, /* r, r4 */
-};
-#undef P1
-/* reduce the size of this table */
-static const char
-shift_table [STACK_MAX] [STACK_MAX] = {
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_I4,  STACK_INV, STACK_I4,  STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_I8,  STACK_INV, STACK_I8,  STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_PTR, STACK_INV, STACK_PTR, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV},
-	{STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV, STACK_INV}
-};
-/*
- * Tables to map from the non-specific opcode to the matching
- * type-specific opcode.
- */
-/* handles from CEE_ADD to CEE_SHR_UN (CEE_REM_UN for floats) */
-static const guint16
-binops_op_map [STACK_MAX] = {
-	0, OP_IADD-CEE_ADD, OP_LADD-CEE_ADD, OP_PADD-CEE_ADD, OP_FADD-CEE_ADD, OP_PADD-CEE_ADD, 0, 0, OP_RADD-CEE_ADD
-};
-/* handles from CEE_NEG to CEE_CONV_U8 */
-static const guint16
-unops_op_map [STACK_MAX] = {
-	0, OP_INEG-CEE_NEG, OP_LNEG-CEE_NEG, OP_PNEG-CEE_NEG, OP_FNEG-CEE_NEG, OP_PNEG-CEE_NEG, 0, 0, OP_RNEG-CEE_NEG
-};
-/* handles from CEE_CONV_U2 to CEE_SUB_OVF_UN */
-static const guint16
-ovfops_op_map [STACK_MAX] = {
-	0, OP_ICONV_TO_U2-CEE_CONV_U2, OP_LCONV_TO_U2-CEE_CONV_U2, OP_PCONV_TO_U2-CEE_CONV_U2, OP_FCONV_TO_U2-CEE_CONV_U2, OP_PCONV_TO_U2-CEE_CONV_U2, OP_PCONV_TO_U2-CEE_CONV_U2, 0, OP_RCONV_TO_U2-CEE_CONV_U2
-};
-/* handles from CEE_CONV_OVF_I1_UN to CEE_CONV_OVF_U_UN */
-static const guint16
-ovf2ops_op_map [STACK_MAX] = {
-	0, OP_ICONV_TO_OVF_I1_UN-CEE_CONV_OVF_I1_UN, OP_LCONV_TO_OVF_I1_UN-CEE_CONV_OVF_I1_UN, OP_PCONV_TO_OVF_I1_UN-CEE_CONV_OVF_I1_UN, OP_FCONV_TO_OVF_I1_UN-CEE_CONV_OVF_I1_UN, OP_PCONV_TO_OVF_I1_UN-CEE_CONV_OVF_I1_UN, 0, 0, OP_RCONV_TO_OVF_I1_UN-CEE_CONV_OVF_I1_UN
-};
-/* handles from CEE_CONV_OVF_I1 to CEE_CONV_OVF_U8 */
-static const guint16
-ovf3ops_op_map [STACK_MAX] = {
-	0, OP_ICONV_TO_OVF_I1-CEE_CONV_OVF_I1, OP_LCONV_TO_OVF_I1-CEE_CONV_OVF_I1, OP_PCONV_TO_OVF_I1-CEE_CONV_OVF_I1, OP_FCONV_TO_OVF_I1-CEE_CONV_OVF_I1, OP_PCONV_TO_OVF_I1-CEE_CONV_OVF_I1, 0, 0, OP_RCONV_TO_OVF_I1-CEE_CONV_OVF_I1
-};
-/* handles from CEE_BEQ to CEE_BLT_UN */
-static const guint16
-beqops_op_map [STACK_MAX] = {
-	0, OP_IBEQ-CEE_BEQ, OP_LBEQ-CEE_BEQ, OP_PBEQ-CEE_BEQ, OP_FBEQ-CEE_BEQ, OP_PBEQ-CEE_BEQ, OP_PBEQ-CEE_BEQ, 0, OP_FBEQ-CEE_BEQ
-};
-/* handles from CEE_CEQ to CEE_CLT_UN */
-static const guint16
-ceqops_op_map [STACK_MAX] = {
-	0, OP_ICEQ-OP_CEQ, OP_LCEQ-OP_CEQ, OP_PCEQ-OP_CEQ, OP_FCEQ-OP_CEQ, OP_PCEQ-OP_CEQ, OP_PCEQ-OP_CEQ, 0, OP_RCEQ-OP_CEQ
-};
-/*
- * Sets ins->type (the type on the eval stack) according to the
- * type of the opcode and the arguments to it.
- * Invalid IL code is marked by setting ins->type to the invalid value STACK_INV.
- *
- * FIXME: this function sets ins->type unconditionally in some cases, but
- * it should set it to invalid for some types (a conv.x on an object)
- */
-static void
-type_from_op (MonoCompile *cfg, MonoInst *ins, MonoInst *src1, MonoInst *src2)
-{
-	switch (ins->opcode) {
-	/* binops */
-	case MONO_CEE_ADD:
-	case MONO_CEE_SUB:
-	case MONO_CEE_MUL:
-	case MONO_CEE_DIV:
-	case MONO_CEE_REM:
-		/* FIXME: check unverifiable args for STACK_MP */
-		ins->type = bin_num_table [src1->type] [src2->type];
-		ins->opcode += binops_op_map [ins->type];
-		break;
-	case MONO_CEE_DIV_UN:
-	case MONO_CEE_REM_UN:
-	case MONO_CEE_AND:
-	case MONO_CEE_OR:
-	case MONO_CEE_XOR:
-		ins->type = bin_int_table [src1->type] [src2->type];
-		ins->opcode += binops_op_map [ins->type];
-		break;
-	case MONO_CEE_SHL:
-	case MONO_CEE_SHR:
-	case MONO_CEE_SHR_UN:
-		ins->type = shift_table [src1->type] [src2->type];
-		ins->opcode += binops_op_map [ins->type];
-		break;
-	case OP_COMPARE:
-	case OP_LCOMPARE:
-	case OP_ICOMPARE:
-		ins->type = bin_comp_table [src1->type] [src2->type] ? STACK_I4: STACK_INV;
-		if ((src1->type == STACK_I8) || ((TARGET_SIZEOF_VOID_P == 8) && ((src1->type == STACK_PTR) || (src1->type == STACK_OBJ) || (src1->type == STACK_MP))))
-			ins->opcode = OP_LCOMPARE;
-		else if (src1->type == STACK_R4)
-			ins->opcode = OP_RCOMPARE;
-		else if (src1->type == STACK_R8)
-			ins->opcode = OP_FCOMPARE;
-		else
-			ins->opcode = OP_ICOMPARE;
-		break;
-	case OP_ICOMPARE_IMM:
-		ins->type = bin_comp_table [src1->type] [src1->type] ? STACK_I4 : STACK_INV;
-		if ((src1->type == STACK_I8) || ((TARGET_SIZEOF_VOID_P == 8) && ((src1->type == STACK_PTR) || (src1->type == STACK_OBJ) || (src1->type == STACK_MP))))
-			ins->opcode = OP_LCOMPARE_IMM;
-		break;
-	case MONO_CEE_BEQ:
-	case MONO_CEE_BGE:
-	case MONO_CEE_BGT:
-	case MONO_CEE_BLE:
-	case MONO_CEE_BLT:
-	case MONO_CEE_BNE_UN:
-	case MONO_CEE_BGE_UN:
-	case MONO_CEE_BGT_UN:
-	case MONO_CEE_BLE_UN:
-	case MONO_CEE_BLT_UN:
-		ins->opcode += beqops_op_map [src1->type];
-		break;
-	case OP_CEQ:
-		ins->type = bin_comp_table [src1->type] [src2->type] ? STACK_I4: STACK_INV;
-		ins->opcode += ceqops_op_map [src1->type];
-		break;
-	case OP_CGT:
-	case OP_CGT_UN:
-	case OP_CLT:
-	case OP_CLT_UN:
-		ins->type = (bin_comp_table [src1->type] [src2->type] & 1) ? STACK_I4: STACK_INV;
-		ins->opcode += ceqops_op_map [src1->type];
-		break;
-	/* unops */
-	case MONO_CEE_NEG:
-		ins->type = neg_table [src1->type];
-		ins->opcode += unops_op_map [ins->type];
-		break;
-	case MONO_CEE_NOT:
-		if (src1->type >= STACK_I4 && src1->type <= STACK_PTR)
-			ins->type = src1->type;
-		else
-			ins->type = STACK_INV;
-		ins->opcode += unops_op_map [ins->type];
-		break;
-	case MONO_CEE_CONV_I1:
-	case MONO_CEE_CONV_I2:
-	case MONO_CEE_CONV_I4:
-	case MONO_CEE_CONV_U4:
-		ins->type = STACK_I4;
-		ins->opcode += unops_op_map [src1->type];
-		break;
-	case MONO_CEE_CONV_R_UN:
-		ins->type = STACK_R8;
-		switch (src1->type) {
-		case STACK_I4:
-#if TARGET_SIZEOF_VOID_P == 4
-		case STACK_PTR:
-#endif
-			ins->opcode = OP_ICONV_TO_R_UN;
-			break;
-		case STACK_I8:
-#if TARGET_SIZEOF_VOID_P == 8
-		case STACK_PTR:
-#endif
-			ins->opcode = OP_LCONV_TO_R_UN;
-			break;
-		case STACK_R4:
-			ins->opcode = OP_RCONV_TO_R8;
-			break;
-		case STACK_R8:
-			ins->opcode = OP_FMOVE;
-			break;
-		}
-		break;
-	case MONO_CEE_CONV_OVF_I1:
-	case MONO_CEE_CONV_OVF_U1:
-	case MONO_CEE_CONV_OVF_I2:
-	case MONO_CEE_CONV_OVF_U2:
-	case MONO_CEE_CONV_OVF_I4:
-	case MONO_CEE_CONV_OVF_U4:
-		ins->type = STACK_I4;
-		ins->opcode += ovf3ops_op_map [src1->type];
-		break;
-	case MONO_CEE_CONV_OVF_I_UN:
-	case MONO_CEE_CONV_OVF_U_UN:
-		ins->type = STACK_PTR;
-		ins->opcode += ovf2ops_op_map [src1->type];
-		break;
-	case MONO_CEE_CONV_OVF_I1_UN:
-	case MONO_CEE_CONV_OVF_I2_UN:
-	case MONO_CEE_CONV_OVF_I4_UN:
-	case MONO_CEE_CONV_OVF_U1_UN:
-	case MONO_CEE_CONV_OVF_U2_UN:
-	case MONO_CEE_CONV_OVF_U4_UN:
-		ins->type = STACK_I4;
-		ins->opcode += ovf2ops_op_map [src1->type];
-		break;
-	case MONO_CEE_CONV_U:
-		ins->type = STACK_PTR;
-		switch (src1->type) {
-		case STACK_I4:
-			ins->opcode = OP_ICONV_TO_U;
-			break;
-		case STACK_PTR:
-		case STACK_MP:
-		case STACK_OBJ:
-#if TARGET_SIZEOF_VOID_P == 8
-			ins->opcode = OP_LCONV_TO_U;
-#else
-			ins->opcode = OP_MOVE;
-#endif
-			break;
-		case STACK_I8:
-			ins->opcode = OP_LCONV_TO_U;
-			break;
-		case STACK_R8:
-			if (TARGET_SIZEOF_VOID_P == 8)
-				ins->opcode = OP_FCONV_TO_U8;
-			else
-				ins->opcode = OP_FCONV_TO_U4;
-			break;
-		case STACK_R4:
-			if (TARGET_SIZEOF_VOID_P == 8)
-				ins->opcode = OP_RCONV_TO_U8;
-			else
-				ins->opcode = OP_RCONV_TO_U4;
-			break;
-		}
-		break;
-	case MONO_CEE_CONV_I8:
-	case MONO_CEE_CONV_U8:
-		ins->type = STACK_I8;
-		ins->opcode += unops_op_map [src1->type];
-		break;
-	case MONO_CEE_CONV_OVF_I8:
-	case MONO_CEE_CONV_OVF_U8:
-		ins->type = STACK_I8;
-		ins->opcode += ovf3ops_op_map [src1->type];
-		break;
-	case MONO_CEE_CONV_OVF_U8_UN:
-	case MONO_CEE_CONV_OVF_I8_UN:
-		ins->type = STACK_I8;
-		ins->opcode += ovf2ops_op_map [src1->type];
-		break;
-	case MONO_CEE_CONV_R4:
-		ins->type = GINT_TO_UINT8 (cfg->r4_stack_type);
-		ins->opcode += unops_op_map [src1->type];
-		break;
-	case MONO_CEE_CONV_R8:
-		ins->type = STACK_R8;
-		ins->opcode += unops_op_map [src1->type];
-		break;
-	case OP_CKFINITE:
-		ins->type = STACK_R8;
-		break;
-	case MONO_CEE_CONV_U2:
-	case MONO_CEE_CONV_U1:
-		ins->type = STACK_I4;
-		ins->opcode += ovfops_op_map [src1->type];
-		break;
-	case MONO_CEE_CONV_I:
-	case MONO_CEE_CONV_OVF_I:
-	case MONO_CEE_CONV_OVF_U:
-		ins->type = STACK_PTR;
-		ins->opcode += ovfops_op_map [src1->type];
-		switch (ins->opcode) {
-		case OP_FCONV_TO_I:
-			ins->opcode = TARGET_SIZEOF_VOID_P == 4 ? OP_FCONV_TO_I4 : OP_FCONV_TO_I8;
-			break;
-		case OP_RCONV_TO_I:
-			ins->opcode = TARGET_SIZEOF_VOID_P == 4 ? OP_RCONV_TO_I4 : OP_RCONV_TO_I8;
-			break;
-		default:
-			break;
-		}
-		break;
-	case MONO_CEE_ADD_OVF:
-	case MONO_CEE_ADD_OVF_UN:
-	case MONO_CEE_MUL_OVF:
-	case MONO_CEE_MUL_OVF_UN:
-	case MONO_CEE_SUB_OVF:
-	case MONO_CEE_SUB_OVF_UN:
-		ins->type = bin_num_table [src1->type] [src2->type];
-		ins->opcode += ovfops_op_map [src1->type];
-		if (ins->type == STACK_R8)
-			ins->type = STACK_INV;
-		break;
-	case OP_LOAD_MEMBASE:
-		ins->type = STACK_PTR;
-		break;
-	case OP_LOADI1_MEMBASE:
-	case OP_LOADU1_MEMBASE:
-	case OP_LOADI2_MEMBASE:
-	case OP_LOADU2_MEMBASE:
-	case OP_LOADI4_MEMBASE:
-	case OP_LOADU4_MEMBASE:
-		ins->type = STACK_PTR;
-		break;
-	case OP_LOADI8_MEMBASE:
-		ins->type = STACK_I8;
-		break;
-	case OP_LOADR4_MEMBASE:
-		ins->type = GINT_TO_UINT8 (cfg->r4_stack_type);
-		break;
-	case OP_LOADR8_MEMBASE:
-		ins->type = STACK_R8;
-		break;
-	default:
-		g_error ("opcode 0x%04x not handled in type from op", ins->opcode);
-		break;
-	}
-	if (ins->type == STACK_MP) {
-		if (src1->type == STACK_MP)
-			ins->klass = src1->klass;
-		else
-			ins->klass = mono_defaults.object_class;
-	}
-}
-void
-mini_type_from_op (MonoCompile *cfg, MonoInst *ins, MonoInst *src1, MonoInst *src2)
-{
-	type_from_op (cfg, ins, src1, src2);
-}
-static MonoClass*
-ldind_to_type (int op)
-{
-	switch (op) {
-	case MONO_CEE_LDIND_I1: return mono_defaults.sbyte_class;
-	case MONO_CEE_LDIND_U1: return mono_defaults.byte_class;
-	case MONO_CEE_LDIND_I2: return mono_defaults.int16_class;
-	case MONO_CEE_LDIND_U2: return mono_defaults.uint16_class;
-	case MONO_CEE_LDIND_I4: return mono_defaults.int32_class;
-	case MONO_CEE_LDIND_U4: return mono_defaults.uint32_class;
-	case MONO_CEE_LDIND_I8: return mono_defaults.int64_class;
-	case MONO_CEE_LDIND_I: return mono_defaults.int_class;
-	case MONO_CEE_LDIND_R4: return mono_defaults.single_class;
-	case MONO_CEE_LDIND_R8: return mono_defaults.double_class;
-	case MONO_CEE_LDIND_REF:return mono_defaults.object_class; //FIXME we should try to return a more specific type
-	default: g_error ("Unknown ldind type %d", op);
-	}
-}
-static MonoClass*
-stind_to_type (int op)
-{
-	switch (op) {
-	case MONO_CEE_STIND_I1: return mono_defaults.sbyte_class;
-	case MONO_CEE_STIND_I2: return mono_defaults.int16_class;
-	case MONO_CEE_STIND_I4: return mono_defaults.int32_class;
-	case MONO_CEE_STIND_I8: return mono_defaults.int64_class;
-	case MONO_CEE_STIND_I: return mono_defaults.int_class;
-	case MONO_CEE_STIND_R4: return mono_defaults.single_class;
-	case MONO_CEE_STIND_R8: return mono_defaults.double_class;
-	case MONO_CEE_STIND_REF: return mono_defaults.object_class;
-	default: g_error ("Unknown stind type %d", op);
-	}
-}
-#if 0
-static const char
-param_table [STACK_MAX] [STACK_MAX] = {
-	{0},
-};
-static int
-check_values_to_signature (MonoInst *args, MonoType *this_ins, MonoMethodSignature *sig)
-{
-	int i;
-	if (sig->hasthis) {
-		switch (args->type) {
-		case STACK_I4:
-		case STACK_I8:
-		case STACK_R8:
-		case STACK_VTYPE:
-		case STACK_INV:
-			return 0;
-		}
-		args++;
-	}
-	for (i = 0; i < sig->param_count; ++i) {
-		switch (args [i].type) {
-		case STACK_INV:
-			return 0;
-		case STACK_MP:
-			if (m_type_is_byref (!sig->params [i]))
-				return 0;
-			continue;
-		case STACK_OBJ:
-			if (m_type_is_byref (sig->params [i]))
-				return 0;
-			switch (m_type_is_byref (sig->params [i])) {
-			case MONO_TYPE_CLASS:
-			case MONO_TYPE_STRING:
-			case MONO_TYPE_OBJECT:
-			case MONO_TYPE_SZARRAY:
-			case MONO_TYPE_ARRAY:
-				break;
-			default:
-				return 0;
-			}
-			continue;
-		case STACK_R8:
-			if (m_type_is_byref (sig->params [i]))
-				return 0;
-			if (sig->params [i]->type != MONO_TYPE_R4 && sig->params [i]->type != MONO_TYPE_R8)
-				return 0;
-			continue;
-		case STACK_PTR:
-		case STACK_I4:
-		case STACK_I8:
-		case STACK_VTYPE:
-			break;
-		}
-		/*if (!param_table [args [i].type] [sig->params [i]->type])
-			return 0;*/
-	}
-	return 1;
-}
-#endif
-/*
- * The got_var contains the address of the Global Offset Table when AOT
- * compiling.
- */
-MonoInst *
-mono_get_got_var (MonoCompile *cfg)
-{
-	if (!cfg->compile_aot || !cfg->backend->need_got_var || cfg->llvm_only)
-		return NULL;
-	if (!cfg->got_var) {
-		cfg->got_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
-	}
-	return cfg->got_var;
-}
-static void
-mono_create_rgctx_var (MonoCompile *cfg)
-{
-	if (!cfg->rgctx_var) {
-		cfg->rgctx_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
-		/* force the var to be stack allocated */
-		if (!cfg->llvm_only)
-			cfg->rgctx_var->flags |= MONO_INST_VOLATILE;
-	}
-}
-static MonoInst *
-mono_get_mrgctx_var (MonoCompile *cfg)
-{
-	g_assert (cfg->gshared);
-	mono_create_rgctx_var (cfg);
-	return cfg->rgctx_var;
-}
-static MonoInst *
-mono_get_vtable_var (MonoCompile *cfg)
-{
-	g_assert (cfg->gshared);
-	/* The mrgctx and the vtable are stored in the same var */
-	mono_create_rgctx_var (cfg);
-	return cfg->rgctx_var;
-}
-static MonoType*
-type_from_stack_type (MonoInst *ins) {
-	switch (ins->type) {
-	case STACK_I4: return mono_get_int32_type ();
-	case STACK_I8: return m_class_get_byval_arg (mono_defaults.int64_class);
-	case STACK_PTR: return mono_get_int_type ();
-	case STACK_R4: return m_class_get_byval_arg (mono_defaults.single_class);
-	case STACK_R8: return m_class_get_byval_arg (mono_defaults.double_class);
-	case STACK_MP:
-		return m_class_get_this_arg (ins->klass);
-	case STACK_OBJ: return mono_get_object_type ();
-	case STACK_VTYPE: return m_class_get_byval_arg (ins->klass);
-	default:
-		g_error ("stack type %d to monotype not handled\n", ins->type);
-	}
-	return NULL;
-}
-MonoStackType
-mini_type_to_stack_type (MonoCompile *cfg, MonoType *t)
-{
-	t = mini_type_get_underlying_type (t);
-	switch (t->type) {
-	case MONO_TYPE_I1:
-	case MONO_TYPE_U1:
-	case MONO_TYPE_I2:
-	case MONO_TYPE_U2:
-	case MONO_TYPE_I4:
-	case MONO_TYPE_U4:
-		return STACK_I4;
-	case MONO_TYPE_I:
-	case MONO_TYPE_U:
-	case MONO_TYPE_PTR:
-	case MONO_TYPE_FNPTR:
-		return STACK_PTR;
-	case MONO_TYPE_CLASS:
-	case MONO_TYPE_STRING:
-	case MONO_TYPE_OBJECT:
-	case MONO_TYPE_SZARRAY:
-	case MONO_TYPE_ARRAY:
-		return STACK_OBJ;
-	case MONO_TYPE_I8:
-	case MONO_TYPE_U8:
-		return STACK_I8;
-	case MONO_TYPE_R4:
-		return (MonoStackType)cfg->r4_stack_type;
-	case MONO_TYPE_R8:
-		return STACK_R8;
-	case MONO_TYPE_VALUETYPE:
-	case MONO_TYPE_TYPEDBYREF:
-		return STACK_VTYPE;
-	case MONO_TYPE_GENERICINST:
-		if (mono_type_generic_inst_is_valuetype (t))
-			return STACK_VTYPE;
-		else
-			return STACK_OBJ;
-		break;
-	default:
-		g_assert_not_reached ();
-	}
-	return (MonoStackType)-1;
-}
-static MonoClass*
-array_access_to_klass (int opcode)
-{
-	switch (opcode) {
-	case MONO_CEE_LDELEM_U1:
-		return mono_defaults.byte_class;
-	case MONO_CEE_LDELEM_U2:
-		return mono_defaults.uint16_class;
-	case MONO_CEE_LDELEM_I:
-	case MONO_CEE_STELEM_I:
-		return mono_defaults.int_class;
-	case MONO_CEE_LDELEM_I1:
-	case MONO_CEE_STELEM_I1:
-		return mono_defaults.sbyte_class;
-	case MONO_CEE_LDELEM_I2:
-	case MONO_CEE_STELEM_I2:
-		return mono_defaults.int16_class;
-	case MONO_CEE_LDELEM_I4:
-	case MONO_CEE_STELEM_I4:
-		return mono_defaults.int32_class;
-	case MONO_CEE_LDELEM_U4:
-		return mono_defaults.uint32_class;
-	case MONO_CEE_LDELEM_I8:
-	case MONO_CEE_STELEM_I8:
-		return mono_defaults.int64_class;
-	case MONO_CEE_LDELEM_R4:
-	case MONO_CEE_STELEM_R4:
-		return mono_defaults.single_class;
-	case MONO_CEE_LDELEM_R8:
-	case MONO_CEE_STELEM_R8:
-		return mono_defaults.double_class;
-	case MONO_CEE_LDELEM_REF:
-	case MONO_CEE_STELEM_REF:
-		return mono_defaults.object_class;
-	default:
-		g_assert_not_reached ();
-	}
-	return NULL;
-}
-/*
- * We try to share variables when possible
- */
-static MonoInst *
-mono_compile_get_interface_var (MonoCompile *cfg, int slot, MonoInst *ins)
-{
-	MonoInst *res;
-	int pos, vnum;
-	MonoType *type;
-	type = type_from_stack_type (ins);
-	/* inlining can result in deeper stacks */
-	if (cfg->inline_depth || slot >= cfg->header->max_stack)
-		return mono_compile_create_var (cfg, type, OP_LOCAL);
-	pos = ins->type - 1 + slot * STACK_MAX;
-	switch (ins->type) {
-	case STACK_I4:
-	case STACK_I8:
-	case STACK_R8:
-	case STACK_PTR:
-	case STACK_MP:
-	case STACK_OBJ:
-		if ((vnum = cfg->intvars [pos]))
-			return cfg->varinfo [vnum];
-		res = mono_compile_create_var (cfg, type, OP_LOCAL);
-		cfg->intvars [pos] = GTMREG_TO_UINT16 (res->inst_c0);
-		break;
-	default:
-		res = mono_compile_create_var (cfg, type, OP_LOCAL);
-	}
-	return res;
-}
-static void
-mono_save_token_info (MonoCompile *cfg, MonoImage *image, guint32 token, gpointer key)
-{
-	/*
-	 * Don't use this if a generic_context is set, since that means AOT can't
-	 * look up the method using just the image+token.
-	 * table == 0 means this is a reference made from a wrapper.
-	 */
-	if (cfg->compile_aot && !cfg->generic_context && (mono_metadata_token_table (token) > 0)) {
-		MonoJumpInfoToken *jump_info_token = (MonoJumpInfoToken *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoJumpInfoToken));
-		jump_info_token->image = image;
-		jump_info_token->token = token;
-		g_hash_table_insert (cfg->token_info_hash, key, jump_info_token);
-	}
-}
-/*
- * This function is called to handle items that are left on the evaluation stack
- * at basic block boundaries. What happens is that we save the values to local variables
- * and we reload them later when first entering the target basic block (with the
- * handle_loaded_temps () function).
- * A single joint point will use the same variables (stored in the array bb->out_stack or
- * bb->in_stack, if the basic block is before or after the joint point).
- *
- * This function needs to be called _before_ emitting the last instruction of
- * the bb (i.e. before emitting a branch).
- * If the stack merge fails at a join point, cfg->unverifiable is set.
- */
-static void
-handle_stack_args (MonoCompile *cfg, MonoInst **sp, int count)
-{
-	MonoBasicBlock *bb = cfg->cbb;
-	MonoBasicBlock *outb;
-	MonoInst *inst, **locals;
-	gboolean found;
-	if (!count)
-		return;
-	if (cfg->verbose_level > 3)
-		printf ("%d item(s) on exit from B%d\n", count, bb->block_num);
-	if (!bb->out_scount) {
-		bb->out_scount = GINT_TO_UINT16 (count);
-		found = FALSE;
-		for (gint16 i = 0; i < bb->out_count; ++i) {
-			outb = bb->out_bb [i];
-			/* exception handlers are linked, but they should not be considered for stack args */
-			if (outb->flags & BB_EXCEPTION_HANDLER)
-				continue;
-			if (outb->in_stack) {
-				found = TRUE;
-				bb->out_stack = outb->in_stack;
-				break;
-			}
-		}
-		if (!found) {
-			bb->out_stack = (MonoInst **)mono_mempool_alloc (cfg->mempool, sizeof (MonoInst*) * count);
-			for (int i = 0; i < count; ++i) {
-				/*
-				 * try to reuse temps already allocated for this purpouse, if they occupy the same
-				 * stack slot and if they are of the same type.
-				 * This won't cause conflicts since if 'local' is used to
-				 * store one of the values in the in_stack of a bblock, then
-				 * the same variable will be used for the same outgoing stack
-				 * slot as well.
-				 * This doesn't work when inlining methods, since the bblocks
-				 * in the inlined methods do not inherit their in_stack from
-				 * the bblock they are inlined to. See bug #58863 for an
-				 * example.
-				 */
-				bb->out_stack [i] = mono_compile_get_interface_var (cfg, i, sp [i]);
-			}
-		}
-	}
-	for (gint16 i = 0; i < bb->out_count; ++i) {
-		outb = bb->out_bb [i];
-		/* exception handlers are linked, but they should not be considered for stack args */
-		if (outb->flags & BB_EXCEPTION_HANDLER)
-			continue;
-		if (outb->in_scount) {
-			if (outb->in_scount != bb->out_scount) {
-				cfg->unverifiable = TRUE;
-				return;
-			}
-			continue; /* check they are the same locals */
-		}
-		outb->in_scount = GINT_TO_UINT16 (count);
-		outb->in_stack = bb->out_stack;
-	}
-	locals = bb->out_stack;
-	cfg->cbb = bb;
-	for (int i = 0; i < count; ++i) {
-		sp [i] = convert_value (cfg, locals [i]->inst_vtype, sp [i]);
-		EMIT_NEW_TEMPSTORE (cfg, inst, locals [i]->inst_c0, sp [i]);
-		inst->cil_code = sp [i]->cil_code;
-		sp [i] = locals [i];
-		if (cfg->verbose_level > 3)
-			printf ("storing %d to temp %d\n", i, (int)locals [i]->inst_c0);
-	}
-	/*
-	 * It is possible that the out bblocks already have in_stack assigned, and
-	 * the in_stacks differ. In this case, we will store to all the different
-	 * in_stacks.
-	 */
-	found = TRUE;
-	gint16 bindex = 0;
-	while (found) {
-		/* Find a bblock which has a different in_stack */
-		found = FALSE;
-		while (bindex < bb->out_count) {
-			outb = bb->out_bb [bindex];
-			/* exception handlers are linked, but they should not be considered for stack args */
-			if (outb->flags & BB_EXCEPTION_HANDLER) {
-				bindex++;
-				continue;
-			}
-			if (outb->in_stack != locals) {
-				for (int i = 0; i < count; ++i) {
-					sp [i] = convert_value (cfg, outb->in_stack [i]->inst_vtype, sp [i]);
-					EMIT_NEW_TEMPSTORE (cfg, inst, outb->in_stack [i]->inst_c0, sp [i]);
-					inst->cil_code = sp [i]->cil_code;
-					sp [i] = locals [i];
-					if (cfg->verbose_level > 3)
-						printf ("storing %d to temp %d\n", i, (int)outb->in_stack [i]->inst_c0);
-				}
-				locals = outb->in_stack;
-				found = TRUE;
-				break;
-			}
-			bindex ++;
-		}
-	}
-}
-MonoInst*
-mini_emit_runtime_constant (MonoCompile *cfg, MonoJumpInfoType patch_type, gpointer data)
-{
-	MonoInst *ins;
-	if (cfg->compile_aot) {
-MONO_DISABLE_WARNING (4306) // 'type cast': conversion from 'MonoJumpInfoType' to 'MonoInst *' of greater size
-		EMIT_NEW_AOTCONST (cfg, ins, patch_type, data);
-MONO_RESTORE_WARNING
-	} else {
-		MonoJumpInfo ji;
-		gpointer target;
-		ERROR_DECL (error);
-		ji.type = patch_type;
-		ji.data.target = data;
-		target = mono_resolve_patch_target_ext (cfg->mem_manager, NULL, NULL, &ji, FALSE, error);
-		mono_error_assert_ok (error);
-		EMIT_NEW_PCONST (cfg, ins, target);
-	}
-	return ins;
-}
-static MonoInst*
-mono_create_fast_tls_getter (MonoCompile *cfg, MonoTlsKey key)
-{
-	if (cfg->compile_aot)
-		return NULL;
-	int tls_offset = mono_tls_get_tls_offset (key);
-	if (tls_offset != -1 && mono_arch_have_fast_tls ()) {
-		MonoInst *ins;
-		MONO_INST_NEW (cfg, ins, OP_TLS_GET);
-		ins->dreg = mono_alloc_preg (cfg);
-		ins->inst_offset = tls_offset;
-		return ins;
-	}
-	return NULL;
-}
-static MonoInst*
-mono_create_tls_get (MonoCompile *cfg, MonoTlsKey key)
-{
-	MonoInst *fast_tls = NULL;
-	if (!mini_debug_options.use_fallback_tls)
-		fast_tls = mono_create_fast_tls_getter (cfg, key);
-	if (fast_tls) {
-		MONO_ADD_INS (cfg->cbb, fast_tls);
-		return fast_tls;
-	}
-	const MonoJitICallId jit_icall_id = mono_get_tls_key_to_jit_icall_id (key);
-	if (cfg->compile_aot && !cfg->llvm_only) {
-		MonoInst *addr;
-		/*
-		 * tls getters are critical pieces of code and we don't want to resolve them
-		 * through the standard plt/tramp mechanism since we might expose ourselves
-		 * to crashes and infinite recursions.
-		 * Therefore the NOCALL part of MONO_PATCH_INFO_JIT_ICALL_ADDR_NOCALL, FALSE in is_plt_patch.
-		 */
-		EMIT_NEW_AOTCONST (cfg, addr, MONO_PATCH_INFO_JIT_ICALL_ADDR_NOCALL, GUINT_TO_POINTER (jit_icall_id));
-		return mini_emit_calli (cfg, mono_icall_sig_ptr, NULL, addr, NULL, NULL);
-	} else {
-		return mono_emit_jit_icall_id (cfg, jit_icall_id, NULL);
-	}
-}
-/*
- * emit_push_lmf:
- *
- *   Emit IR to push the current LMF onto the LMF stack.
- */
-static void
-emit_push_lmf (MonoCompile *cfg)
-{
-	/*
-	 * Emit IR to push the LMF:
-	 * lmf_addr = <lmf_addr from tls>
-	 * lmf->lmf_addr = lmf_addr
-	 * lmf->prev_lmf = *lmf_addr
-	 * *lmf_addr = lmf
-	 */
-	MonoInst *ins, *lmf_ins;
-	if (!cfg->lmf_ir)
-		return;
-	int lmf_reg, prev_lmf_reg;
-	/*
-	 * Store lmf_addr in a variable, so it can be allocated to a global register.
-	 */
-	if (!cfg->lmf_addr_var)
-		cfg->lmf_addr_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
-	if (!cfg->lmf_var) {
-		MonoInst *lmf_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
-		lmf_var->flags |= MONO_INST_VOLATILE;
-		lmf_var->flags |= MONO_INST_LMF;
-		cfg->lmf_var = lmf_var;
-	}
-	lmf_ins = mono_create_tls_get (cfg, TLS_KEY_LMF_ADDR);
-	g_assert (lmf_ins);
-	lmf_ins->dreg = cfg->lmf_addr_var->dreg;
-	EMIT_NEW_VARLOADA (cfg, ins, cfg->lmf_var, NULL);
-	lmf_reg = ins->dreg;
-	prev_lmf_reg = alloc_preg (cfg);
-	/* Save previous_lmf */
-	EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, prev_lmf_reg, cfg->lmf_addr_var->dreg, 0);
-	if (cfg->deopt)
-		/* Mark this as an LMFExt */
-		EMIT_NEW_BIALU_IMM (cfg, ins, OP_POR_IMM, prev_lmf_reg, prev_lmf_reg, 2);
-	EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, lmf_reg, MONO_STRUCT_OFFSET (MonoLMF, previous_lmf), prev_lmf_reg);
-	/* Set new lmf */
-	EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, cfg->lmf_addr_var->dreg, 0, lmf_reg);
-}
-/*
- * emit_pop_lmf:
- *
- *   Emit IR to pop the current LMF from the LMF stack.
- */
-static void
-emit_pop_lmf (MonoCompile *cfg)
-{
-	int lmf_reg, lmf_addr_reg;
-	MonoInst *ins;
-	if (!cfg->lmf_ir)
-		return;
- 	EMIT_NEW_VARLOADA (cfg, ins, cfg->lmf_var, NULL);
- 	lmf_reg = ins->dreg;
-	int prev_lmf_reg;
-	/*
-	 * Emit IR to pop the LMF:
-	 * *(lmf->lmf_addr) = lmf->prev_lmf
-	 */
-	/* This could be called before emit_push_lmf () */
-	if (!cfg->lmf_addr_var)
-		cfg->lmf_addr_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
-	lmf_addr_reg = cfg->lmf_addr_var->dreg;
-	prev_lmf_reg = alloc_preg (cfg);
-	EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, prev_lmf_reg, lmf_reg, MONO_STRUCT_OFFSET (MonoLMF, previous_lmf));
-	if (cfg->deopt)
-		/* Clear out the bit set by push_lmf () to mark this as LMFExt */
-		EMIT_NEW_BIALU_IMM (cfg, ins, OP_PXOR_IMM, prev_lmf_reg, prev_lmf_reg, 2);
-	EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, lmf_addr_reg, 0, prev_lmf_reg);
-}
-/*
- * target_type_is_incompatible:
- * @cfg: MonoCompile context
- *
- * Check that the item @arg on the evaluation stack can be stored
- * in the target type (can be a local, or field, etc).
- * The cfg arg can be used to check if we need verification or just
- * validity checks.
- *
- * Returns: non-0 value if arg can't be stored on a target.
- */
-static int
-target_type_is_incompatible (MonoCompile *cfg, MonoType *target, MonoInst *arg)
-{
-	MonoType *simple_type;
-	MonoClass *klass;
-	if (m_type_is_byref (target)) {
-		/* FIXME: check that the pointed to types match */
-		if (arg->type == STACK_MP) {
-			/* This is needed to handle gshared types + ldaddr. We lower the types so we can handle enums and other typedef-like types. */
-			MonoClass *target_class_lowered = mono_class_from_mono_type_internal (mini_get_underlying_type (m_class_get_byval_arg (mono_class_from_mono_type_internal (target))));
-			MonoClass *source_class_lowered = mono_class_from_mono_type_internal (mini_get_underlying_type (m_class_get_byval_arg (arg->klass)));
-			/* if the target is native int& or X* or same type */
-			if (target->type == MONO_TYPE_I || target->type == MONO_TYPE_PTR || target_class_lowered == source_class_lowered)
-				return 0;
-			/* Both are primitive type byrefs and the source points to a larger type that the destination */
-			if (MONO_TYPE_IS_PRIMITIVE_SCALAR (m_class_get_byval_arg (target_class_lowered)) && MONO_TYPE_IS_PRIMITIVE_SCALAR (m_class_get_byval_arg (source_class_lowered)) &&
-				mono_class_instance_size (target_class_lowered) <= mono_class_instance_size (source_class_lowered))
-				return 0;
-			return 1;
-		}
-		if (arg->type == STACK_PTR)
-			return 0;
-		return 1;
-	}
-	simple_type = mini_get_underlying_type (target);
-	switch (simple_type->type) {
-	case MONO_TYPE_VOID:
-		return 1;
-	case MONO_TYPE_I1:
-	case MONO_TYPE_U1:
-	case MONO_TYPE_I2:
-	case MONO_TYPE_U2:
-	case MONO_TYPE_I4:
-	case MONO_TYPE_U4:
-		if (arg->type != STACK_I4 && arg->type != STACK_PTR)
-			return 1;
-		return 0;
-	case MONO_TYPE_PTR:
-		/* STACK_MP is needed when setting pinned locals */
-		if (arg->type != STACK_I4 && arg->type != STACK_PTR && arg->type != STACK_MP)
-#if SIZEOF_VOID_P == 8
-			if (arg->type != STACK_I8)
-#endif
-				return 1;
-		return 0;
-	case MONO_TYPE_I:
-	case MONO_TYPE_U:
-	case MONO_TYPE_FNPTR:
-		/*
-		 * Some opcodes like ldloca returns 'transient pointers' which can be stored in
-		 * in native int. (#688008).
-		 */
-		if (arg->type != STACK_I4 && arg->type != STACK_PTR && arg->type != STACK_MP)
-			return 1;
-		return 0;
-	case MONO_TYPE_CLASS:
-	case MONO_TYPE_STRING:
-	case MONO_TYPE_OBJECT:
-	case MONO_TYPE_SZARRAY:
-	case MONO_TYPE_ARRAY:
-		if (arg->type != STACK_OBJ)
-			return 1;
-		/* FIXME: check type compatibility */
-		return 0;
-	case MONO_TYPE_I8:
-	case MONO_TYPE_U8:
-		if (arg->type != STACK_I8)
-#if SIZEOF_VOID_P == 8
-			if (arg->type != STACK_PTR)
-#endif
-				return 1;
-		return 0;
-	case MONO_TYPE_R4:
-		if (arg->type != cfg->r4_stack_type)
-			return 1;
-		return 0;
-	case MONO_TYPE_R8:
-		if (arg->type != STACK_R8)
-			return 1;
-		return 0;
-	case MONO_TYPE_VALUETYPE:
-		if (arg->type != STACK_VTYPE)
-			return 1;
-		klass = mono_class_from_mono_type_internal (simple_type);
-		if (klass != arg->klass)
-			return 1;
-		return 0;
-	case MONO_TYPE_TYPEDBYREF:
-		if (arg->type != STACK_VTYPE)
-			return 1;
-		klass = mono_class_from_mono_type_internal (simple_type);
-		if (klass != arg->klass)
-			return 1;
-		return 0;
-	case MONO_TYPE_GENERICINST:
-		if (mono_type_generic_inst_is_valuetype (simple_type)) {
-			MonoClass *target_class;
-			if (arg->type != STACK_VTYPE)
-				return 1;
-			klass = mono_class_from_mono_type_internal (simple_type);
-			target_class = mono_class_from_mono_type_internal (target);
-			/* The second cases is needed when doing partial sharing */
-			if (klass != arg->klass && target_class != arg->klass && target_class != mono_class_from_mono_type_internal (mini_get_underlying_type (m_class_get_byval_arg (arg->klass))))
-				return 1;
-			return 0;
-		} else {
-			if (arg->type != STACK_OBJ)
-				return 1;
-			/* FIXME: check type compatibility */
-			return 0;
-		}
-	case MONO_TYPE_VAR:
-	case MONO_TYPE_MVAR:
-		g_assert (cfg->gshared);
-		if (mini_type_var_is_vt (simple_type)) {
-			if (arg->type != STACK_VTYPE)
-				return 1;
-		} else {
-			if (arg->type != STACK_OBJ)
-				return 1;
-		}
-		return 0;
-	default:
-		g_error ("unknown type 0x%02x in target_type_is_incompatible", simple_type->type);
-	}
-	return 1;
-}
-/*
- * convert_value:
- *
- *   Emit some implicit conversions which are not part of the .net spec, but are allowed by MS.NET.
- */
-static MonoInst*
-convert_value (MonoCompile *cfg, MonoType *type, MonoInst *ins)
-{
-	if (!cfg->r4fp)
-		return ins;
-	type = mini_get_underlying_type (type);
-	switch (type->type) {
-	case MONO_TYPE_R4:
-		if (ins->type == STACK_R8) {
-			int dreg = alloc_freg (cfg);
-			MonoInst *conv;
-			EMIT_NEW_UNALU (cfg, conv, OP_FCONV_TO_R4, dreg, ins->dreg);
-			conv->type = STACK_R4;
-			return conv;
-		}
-		break;
-	case MONO_TYPE_R8:
-		if (ins->type == STACK_R4) {
-			int dreg = alloc_freg (cfg);
-			MonoInst *conv;
-			EMIT_NEW_UNALU (cfg, conv, OP_RCONV_TO_R8, dreg, ins->dreg);
-			conv->type = STACK_R8;
-			return conv;
-		}
-		break;
-	default:
-		break;
-	}
-	return ins;
-}
-/*
- * Prepare arguments for passing to a function call.
- * Return a non-zero value if the arguments can't be passed to the given
- * signature.
- * The type checks are not yet complete and some conversions may need
- * casts on 32 or 64 bit architectures.
- *
- * FIXME: implement this using target_type_is_incompatible ()
- */
-static gboolean
-check_call_signature (MonoCompile *cfg, MonoMethodSignature *sig, MonoInst **args)
-{
-	MonoType *simple_type;
-	int i;
-	if (sig->hasthis) {
-		if (args [0]->type != STACK_OBJ && args [0]->type != STACK_MP && args [0]->type != STACK_PTR)
-			return TRUE;
-		args++;
-	}
-	for (i = 0; i < sig->param_count; ++i) {
-		if (m_type_is_byref (sig->params [i])) {
-			if (args [i]->type != STACK_MP && args [i]->type != STACK_PTR)
-				return TRUE;
-			continue;
-		}
-		simple_type = mini_get_underlying_type (sig->params [i]);
-handle_enum:
-		switch (simple_type->type) {
-		case MONO_TYPE_VOID:
-			return TRUE;
-		case MONO_TYPE_I1:
-		case MONO_TYPE_U1:
-		case MONO_TYPE_I2:
-		case MONO_TYPE_U2:
-		case MONO_TYPE_I4:
-		case MONO_TYPE_U4:
-			if (args [i]->type != STACK_I4 && args [i]->type != STACK_PTR)
-				return TRUE;
-			continue;
-		case MONO_TYPE_I:
-		case MONO_TYPE_U:
-			if (args [i]->type != STACK_I4 && args [i]->type != STACK_PTR && args [i]->type != STACK_MP && args [i]->type != STACK_OBJ)
-				return TRUE;
-			continue;
-		case MONO_TYPE_PTR:
-		case MONO_TYPE_FNPTR:
-			if (args [i]->type != STACK_I4 && !(SIZEOF_VOID_P == 8 && args [i]->type == STACK_I8) &&
-				args [i]->type != STACK_PTR && args [i]->type != STACK_MP && args [i]->type != STACK_OBJ)
-				return TRUE;
-			continue;
-		case MONO_TYPE_CLASS:
-		case MONO_TYPE_STRING:
-		case MONO_TYPE_OBJECT:
-		case MONO_TYPE_SZARRAY:
-		case MONO_TYPE_ARRAY:
-			if (args [i]->type != STACK_OBJ)
-				return TRUE;
-			continue;
-		case MONO_TYPE_I8:
-		case MONO_TYPE_U8:
-			if (args [i]->type != STACK_I8 &&
-				!(SIZEOF_VOID_P == 8 && (args [i]->type == STACK_I4 || args [i]->type == STACK_PTR)))
-				return TRUE;
-			continue;
-		case MONO_TYPE_R4:
-			if (args [i]->type != cfg->r4_stack_type)
-				return TRUE;
-			continue;
-		case MONO_TYPE_R8:
-			if (args [i]->type != STACK_R8)
-				return TRUE;
-			continue;
-		case MONO_TYPE_VALUETYPE:
-			if (m_class_is_enumtype (simple_type->data.klass)) {
-				simple_type = mono_class_enum_basetype_internal (simple_type->data.klass);
-				goto handle_enum;
-			}
-			if (args [i]->type != STACK_VTYPE)
-				return TRUE;
-			continue;
-		case MONO_TYPE_TYPEDBYREF:
-			if (args [i]->type != STACK_VTYPE)
-				return TRUE;
-			continue;
-		case MONO_TYPE_GENERICINST:
-			simple_type = m_class_get_byval_arg (simple_type->data.generic_class->container_class);
-			goto handle_enum;
-		case MONO_TYPE_VAR:
-		case MONO_TYPE_MVAR:
-			/* gsharedvt */
-			if (args [i]->type != STACK_VTYPE)
-				return TRUE;
-			continue;
-		default:
-			g_error ("unknown type 0x%02x in check_call_signature",
-				 simple_type->type);
-		}
-	}
-	return FALSE;
-}
-MonoJumpInfo *
-mono_patch_info_new (MonoMemPool *mp, int ip, MonoJumpInfoType type, gconstpointer target)
-{
-	MonoJumpInfo *ji = (MonoJumpInfo *)mono_mempool_alloc (mp, sizeof (MonoJumpInfo));
-	ji->ip.i = ip;
-	ji->type = type;
-	ji->data.target = target;
-	return ji;
-}
-int
-mini_class_check_context_used (MonoCompile *cfg, MonoClass *klass)
-{
-	if (cfg->gshared)
-		return mono_class_check_context_used (klass);
-	else
-		return 0;
-}
-int
-mini_method_check_context_used (MonoCompile *cfg, MonoMethod *method)
-{
-	if (cfg->gshared)
-		return mono_method_check_context_used (method);
-	else
-		return 0;
-}
-/*
- * need_mrgctx_arg:
- *
- *   Check whenever the mrgctx needs to be passed when calling CMETHOD.
- */
-static gboolean
-need_mrgctx_arg (MonoCompile *cfg, MonoMethod *cmethod)
-{
-	gboolean pass_mrgctx = FALSE;
-	if (((cmethod->flags & METHOD_ATTRIBUTE_STATIC) || m_class_is_valuetype (cmethod->klass)) &&
-		(mono_class_is_ginst (cmethod->klass) || mono_class_is_gtd (cmethod->klass))) {
-		gboolean sharable = FALSE;
-		if (mono_method_is_generic_sharable_full (cmethod, TRUE, TRUE, TRUE))
-			sharable = TRUE;
-		/*
-		 * Pass mrgctx iff target method might
-		 * be shared, which means that sharing
-		 * is enabled for its class and its
-		 * context is sharable (and it's not a
-		 * generic method).
-		 */
-		if (sharable && !(mini_method_get_context (cmethod) && mini_method_get_context (cmethod)->method_inst))
-			pass_mrgctx = TRUE;
-	}
-	if (mini_method_needs_mrgctx (cmethod)) {
-		if (mono_method_is_generic_sharable_full (cmethod, TRUE, TRUE, TRUE)) {
-			pass_mrgctx = TRUE;
-		} else {
-			if (cfg->gsharedvt && mini_is_gsharedvt_signature (mono_method_signature_internal (cmethod)))
-				pass_mrgctx = TRUE;
-		}
-	}
-	return pass_mrgctx;
-}
-static gboolean
-direct_icalls_enabled (MonoCompile *cfg, MonoMethod *method)
-{
-	if (cfg->gen_sdb_seq_points || cfg->disable_direct_icalls)
-		return FALSE;
-	if (method && cfg->compile_aot && mono_aot_direct_icalls_enabled_for_method (cfg, method))
-		return TRUE;
-	/* LLVM on amd64 can't handle calls to non-32 bit addresses */
-#ifdef TARGET_AMD64
-	if (cfg->compile_llvm && !cfg->llvm_only)
-		return FALSE;
-#endif
-	return FALSE;
-}
-MonoInst*
-mono_emit_jit_icall_by_info (MonoCompile *cfg, int il_offset, MonoJitICallInfo *info, MonoInst **args)
-{
-	/*
-	 * Call the jit icall without a wrapper if possible.
-	 * The wrapper is needed to be able to do stack walks for asynchronously suspended
-	 * threads when debugging.
-	 */
-	if (direct_icalls_enabled (cfg, NULL)) {
-		int costs;
-		if (!info->wrapper_method) {
-			info->wrapper_method = mono_marshal_get_icall_wrapper (info, TRUE);
-			mono_memory_barrier ();
-		}
-		/*
-		 * Inline the wrapper method, which is basically a call to the C icall, and
-		 * an exception check.
-		 */
-		costs = inline_method (cfg, info->wrapper_method, NULL,
-							   args, NULL, il_offset, TRUE, NULL);
-		g_assert (costs > 0);
-		g_assert (!MONO_TYPE_IS_VOID (info->sig->ret));
-		return args [0];
-	}
-	return mono_emit_jit_icall_id (cfg, mono_jit_icall_info_id (info), args);
-}
-static MonoInst*
-mono_emit_widen_call_res (MonoCompile *cfg, MonoInst *ins, MonoMethodSignature *fsig)
-{
-	if (!MONO_TYPE_IS_VOID (fsig->ret)) {
-#ifdef MONO_ARCH_LLVM_SUPPORTED
-		gboolean might_use_llvm = TRUE;
-#else
-		gboolean might_use_llvm = FALSE;
-#endif
-		if ((fsig->pinvoke || might_use_llvm) && !m_type_is_byref (fsig->ret)) {
-			int widen_op = -1;
-			/*
-			 * Native code might return non register sized integers
-			 * without initializing the upper bits.
-			 */
-			switch (mono_type_to_load_membase (cfg, fsig->ret)) {
-			case OP_LOADI1_MEMBASE:
-				widen_op = OP_ICONV_TO_I1;
-				break;
-			case OP_LOADU1_MEMBASE:
-				widen_op = OP_ICONV_TO_U1;
-				break;
-			case OP_LOADI2_MEMBASE:
-				widen_op = OP_ICONV_TO_I2;
-				break;
-			case OP_LOADU2_MEMBASE:
-				widen_op = OP_ICONV_TO_U2;
-				break;
-			default:
-				break;
-			}
-			if (widen_op != -1) {
-				int dreg = alloc_preg (cfg);
-				MonoInst *widen;
-				EMIT_NEW_UNALU (cfg, widen, widen_op, dreg, ins->dreg);
-				widen->type = ins->type;
-				ins = widen;
-			}
-		}
-	}
-	return ins;
-}
-static MonoInst*
-emit_get_rgctx_method (MonoCompile *cfg, int context_used,
-					   MonoMethod *cmethod, MonoRgctxInfoType rgctx_type);
-static void
-emit_method_access_failure (MonoCompile *cfg, MonoMethod *caller, MonoMethod *callee)
-{
-	MonoInst *args [2];
-	args [0] = emit_get_rgctx_method (cfg, mono_method_check_context_used (caller), caller, MONO_RGCTX_INFO_METHOD);
-	args [1] = emit_get_rgctx_method (cfg, mono_method_check_context_used (callee), callee, MONO_RGCTX_INFO_METHOD);
-	mono_emit_jit_icall (cfg, mono_throw_method_access, args);
-}
-static void
-emit_bad_image_failure (MonoCompile *cfg, MonoMethod *caller, MonoMethod *callee)
-{
-	mono_emit_jit_icall (cfg, mono_throw_bad_image, NULL);
-}
-static void
-emit_not_supported_failure (MonoCompile *cfg)
-{
-	mono_emit_jit_icall (cfg, mono_throw_not_supported, NULL);
-}
-static void
-emit_invalid_program_with_msg (MonoCompile *cfg, MonoError *error_msg, MonoMethod *caller, MonoMethod *callee)
-{
-	g_assert (!is_ok (error_msg));
-	char *str = mono_mem_manager_strdup (cfg->mem_manager, mono_error_get_message (error_msg));
-	MonoInst *iargs[1];
-	if (cfg->compile_aot)
-		EMIT_NEW_LDSTRLITCONST (cfg, iargs [0], str);
-	else
-		EMIT_NEW_PCONST (cfg, iargs [0], str);
-	mono_emit_jit_icall (cfg, mono_throw_invalid_program, iargs);
-}
-static MonoMethod*
-get_method_nofail (MonoClass *klass, const char *method_name, int num_params, int flags)
-{
-	MonoMethod *method;
-	ERROR_DECL (error);
-	method = mono_class_get_method_from_name_checked (klass, method_name, num_params, flags, error);
-	mono_error_assert_ok (error);
-	g_assertf (method, "Could not lookup method %s in %s", method_name, m_class_get_name (klass));
-	return method;
-}
-MonoMethod*
-mini_get_memcpy_method (void)
-{
-	static MonoMethod *memcpy_method = NULL;
-	if (!memcpy_method) {
-		memcpy_method = get_method_nofail (mono_defaults.string_class, "memcpy", 3, 0);
-		if (!memcpy_method)
-			g_error ("Old corlib found. Install a new one");
-	}
-	return memcpy_method;
-}
-MonoInst*
-mini_emit_storing_write_barrier (MonoCompile *cfg, MonoInst *ptr, MonoInst *value)
-{
-	MonoInst *store;
-	/*
-	 * Add a release memory barrier so the object contents are flushed
-	 * to memory before storing the reference into another object.
-	 */
-	if (!mini_debug_options.weak_memory_model)
-		mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
-	EMIT_NEW_STORE_MEMBASE (cfg, store, OP_STORE_MEMBASE_REG, ptr->dreg, 0, value->dreg);
-	mini_emit_write_barrier (cfg, ptr, value);
-	return store;
-}
-void
-mini_emit_write_barrier (MonoCompile *cfg, MonoInst *ptr, MonoInst *value)
-{
-	int card_table_shift_bits;
-	target_mgreg_t card_table_mask;
-	guint8 *card_table;
-	MonoInst *dummy_use;
-	int nursery_shift_bits;
-	size_t nursery_size;
-	if (!cfg->gen_write_barriers)
-		return;
-	card_table = mono_gc_get_target_card_table (&card_table_shift_bits, &card_table_mask);
-	mono_gc_get_nursery (&nursery_shift_bits, &nursery_size);
-	if (cfg->backend->have_card_table_wb && !cfg->compile_aot && card_table && nursery_shift_bits > 0 && !COMPILE_LLVM (cfg)) {
-		MonoInst *wbarrier;
-		MONO_INST_NEW (cfg, wbarrier, OP_CARD_TABLE_WBARRIER);
-		wbarrier->sreg1 = ptr->dreg;
-		wbarrier->sreg2 = value->dreg;
-		MONO_ADD_INS (cfg->cbb, wbarrier);
-	} else if (card_table) {
-		int offset_reg = alloc_preg (cfg);
-		int card_reg;
-		MonoInst *ins;
-		/*
-		 * We emit a fast light weight write barrier. This always marks cards as in the concurrent
-		 * collector case, so, for the serial collector, it might slightly slow down nursery
-		 * collections. We also expect that the host system and the target system have the same card
-		 * table configuration, which is the case if they have the same pointer size.
-		 */
-		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_SHR_UN_IMM, offset_reg, ptr->dreg, card_table_shift_bits);
-		if (card_table_mask)
-			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_PAND_IMM, offset_reg, offset_reg, card_table_mask);
-		/*We can't use PADD_IMM since the cardtable might end up in high addresses and amd64 doesn't support
-		 * IMM's larger than 32bits.
-		 */
-		ins = mini_emit_runtime_constant (cfg, MONO_PATCH_INFO_GC_CARD_TABLE_ADDR, NULL);
-		card_reg = ins->dreg;
-		MONO_EMIT_NEW_BIALU (cfg, OP_PADD, offset_reg, offset_reg, card_reg);
-		MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STOREI1_MEMBASE_IMM, offset_reg, 0, 1);
-	} else {
-		MonoMethod *write_barrier = mono_gc_get_write_barrier ();
-		mono_emit_method_call (cfg, write_barrier, &ptr, NULL);
-	}
-	EMIT_NEW_DUMMY_USE (cfg, dummy_use, value);
-}
-MonoMethod*
-mini_get_memset_method (void)
-{
-	static MonoMethod *memset_method = NULL;
-	if (!memset_method) {
-		memset_method = get_method_nofail (mono_defaults.string_class, "memset", 3, 0);
-		if (!memset_method)
-			g_error ("Old corlib found. Install a new one");
-	}
-	return memset_method;
-}
-void
-mini_emit_initobj (MonoCompile *cfg, MonoInst *dest, const guchar *ip, MonoClass *klass)
-{
-	MonoInst *iargs [3];
-	int n;
-	guint32 align;
-	MonoMethod *memset_method;
-	MonoInst *size_ins = NULL;
-	MonoInst *bzero_ins = NULL;
-	static MonoMethod *bzero_method;
-	/* FIXME: Optimize this for the case when dest is an LDADDR */
-	mono_class_init_internal (klass);
-	if (mini_is_gsharedvt_klass (klass)) {
-		size_ins = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_VALUE_SIZE);
-		bzero_ins = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_BZERO);
-		if (!bzero_method)
-			bzero_method = get_method_nofail (mono_defaults.string_class, "bzero_aligned_1", 2, 0);
-		g_assert (bzero_method);
-		iargs [0] = dest;
-		iargs [1] = size_ins;
-		mini_emit_calli (cfg, mono_method_signature_internal (bzero_method), iargs, bzero_ins, NULL, NULL);
-		return;
-	}
-	klass = mono_class_from_mono_type_internal (mini_get_underlying_type (m_class_get_byval_arg (klass)));
-	n = mono_class_value_size (klass, &align);
-	if (n <= TARGET_SIZEOF_VOID_P * 8) {
-		mini_emit_memset (cfg, dest->dreg, 0, n, 0, align);
-	}
-	else {
-		memset_method = mini_get_memset_method ();
-		iargs [0] = dest;
-		EMIT_NEW_ICONST (cfg, iargs [1], 0);
-		EMIT_NEW_ICONST (cfg, iargs [2], n);
-		mono_emit_method_call (cfg, memset_method, iargs, NULL);
-	}
-}
-static gboolean
-context_used_is_mrgctx (MonoCompile *cfg, int context_used)
-{
-	if (mono_opt_experimental_gshared_mrgctx)
-		return context_used != 0;
-	/* gshared dim methods use an mrgctx */
-	if (mini_method_is_default_method (cfg->method))
-		return context_used != 0;
-	return context_used & MONO_GENERIC_CONTEXT_USED_METHOD;
-}
-/*
- * emit_get_rgctx:
- *
- *   Emit IR to return either the vtable or the mrgctx.
- */
-static MonoInst*
-emit_get_rgctx (MonoCompile *cfg, int context_used)
-{
-	g_assert (cfg->gshared);
-	/* Data whose context contains method type vars is stored in the mrgctx */
-	if (context_used_is_mrgctx (cfg, context_used) || cfg->gshared_info) {
-		MonoInst *mrgctx_loc, *mrgctx_var;
-		g_assert (cfg->rgctx_access == MONO_RGCTX_ACCESS_MRGCTX);
-		/*
-		if (!mini_method_is_default_method (method))
-			g_assert (method->is_inflated && mono_method_get_context (method)->method_inst);
-		*/
-		if (cfg->llvm_only) {
-			mrgctx_var = mono_get_mrgctx_var (cfg);
-		} else {
-			/* Volatile */
-			mrgctx_loc = mono_get_mrgctx_var (cfg);
-			g_assert (mrgctx_loc->flags & MONO_INST_VOLATILE);
-			EMIT_NEW_TEMPLOAD (cfg, mrgctx_var, mrgctx_loc->inst_c0);
-		}
-		return mrgctx_var;
-	}
-	/*
-	 * The rest of the entries are stored in vtable->runtime_generic_context so
-	 * have to return a vtable.
-	 */
-	if (cfg->rgctx_access == MONO_RGCTX_ACCESS_MRGCTX) {
-		MonoInst *mrgctx_loc, *mrgctx_var, *vtable_var;
-		int vtable_reg;
-		/* We are passed an mrgctx, return mrgctx->class_vtable */
-		if (cfg->llvm_only) {
-			mrgctx_var = mono_get_mrgctx_var (cfg);
-		} else {
-			mrgctx_loc = mono_get_mrgctx_var (cfg);
-			g_assert (mrgctx_loc->flags & MONO_INST_VOLATILE);
-			EMIT_NEW_TEMPLOAD (cfg, mrgctx_var, mrgctx_loc->inst_c0);
-		}
-		vtable_reg = alloc_preg (cfg);
-		EMIT_NEW_LOAD_MEMBASE (cfg, vtable_var, OP_LOAD_MEMBASE, vtable_reg, mrgctx_var->dreg, MONO_STRUCT_OFFSET (MonoMethodRuntimeGenericContext, class_vtable));
-		vtable_var->type = STACK_PTR;
-		return vtable_var;
-	} else {
-		MonoInst *ins, *this_ins;
-		int vtable_reg;
-		/* We are passed a this pointer, return this->vtable */
-		EMIT_NEW_VARLOAD (cfg, this_ins, cfg->this_arg, mono_get_object_type ());
-		vtable_reg = alloc_preg (cfg);
-		EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, vtable_reg, this_ins->dreg, MONO_STRUCT_OFFSET (MonoObject, vtable));
-		return ins;
-	}
-}
-static MonoJumpInfoRgctxEntry *
-mono_patch_info_rgctx_entry_new (MonoMemPool *mp, MonoMethod *method, gboolean in_mrgctx, MonoJumpInfoType patch_type, gconstpointer patch_data, MonoRgctxInfoType info_type)
-{
-	MonoJumpInfoRgctxEntry *res = (MonoJumpInfoRgctxEntry *)mono_mempool_alloc0 (mp, sizeof (MonoJumpInfoRgctxEntry));
-	if (in_mrgctx)
-		res->d.method = method;
-	else
-		res->d.klass = method->klass;
-	res->in_mrgctx = in_mrgctx;
-	res->data = (MonoJumpInfo *)mono_mempool_alloc0 (mp, sizeof (MonoJumpInfo));
-	res->data->type = patch_type;
-	res->data->data.target = patch_data;
-	res->info_type = info_type;
-	return res;
-}
-static MonoInst*
-emit_get_gsharedvt_info (MonoCompile *cfg, gpointer data, MonoRgctxInfoType rgctx_type);
-/*
- * get_gshared_info_slot:
- *
- *   Return a slot index in the mrgctx. PATCH_INFO describes a runtime structure, while
- * RGCTX_TYPE is a property of that structure.
- */
-static int
-get_gshared_info_slot (MonoCompile *cfg, MonoJumpInfo *patch_info, MonoRgctxInfoType rgctx_type)
-{
-	MonoGSharedMethodInfo *info = cfg->gshared_info;
-	int idx;
-	gpointer data;
-	g_assert (cfg->init_method_rgctx_ins);
-	g_assert (info);
-	/* The MonoRuntimeGenericContextInfoTemplate structure contains the 'resolved' patch_info, i.e. a MonoClass pointer etc. */
-	switch (patch_info->type) {
-	case MONO_PATCH_INFO_CLASS:
-		data = m_class_get_byval_arg (patch_info->data.klass);
-		break;
-	case MONO_PATCH_INFO_METHODCONST:
-	case MONO_PATCH_INFO_FIELD:
-	case MONO_PATCH_INFO_VIRT_METHOD:
-	case MONO_PATCH_INFO_DELEGATE_INFO:
-	case MONO_PATCH_INFO_GSHAREDVT_METHOD:
-	case MONO_PATCH_INFO_GSHAREDVT_CALL:
-	case MONO_PATCH_INFO_SIGNATURE:
-		data = (gpointer)patch_info->data.target;
-		break;
-	default:
-		g_assert_not_reached ();
-		break;
-	}
-	g_assert (data);
-	for (int i = 0; i < info->num_entries; ++i) {
-		if (info->entries [i].info_type == rgctx_type && info->entries [i].data == data && rgctx_type != MONO_RGCTX_INFO_LOCAL_OFFSET)
-			return i;
-	}
-	if (info->num_entries == info->count_entries) {
-		MonoRuntimeGenericContextInfoTemplate *new_entries;
-		int new_count_entries = info->count_entries ? info->count_entries * 2 : 16;
-		new_entries = (MonoRuntimeGenericContextInfoTemplate *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoRuntimeGenericContextInfoTemplate) * new_count_entries);
-		memcpy (new_entries, info->entries, sizeof (MonoRuntimeGenericContextInfoTemplate) * info->count_entries);
-		info->entries = new_entries;
-		info->count_entries = new_count_entries;
-	}
-	idx = info->num_entries;
-	info->entries [idx].info_type = rgctx_type;
-	info->entries [idx].data = data;
-	info->num_entries++;
-	return idx;
-}
-static MonoInst*
-emit_rgctx_fetch_inline (MonoCompile *cfg, MonoInst *rgctx, MonoJumpInfoRgctxEntry *entry)
-{
-	MonoInst *call;
-	MonoInst *slot_ins;
-	EMIT_NEW_AOTCONST (cfg, slot_ins, MONO_PATCH_INFO_RGCTX_SLOT_INDEX, entry);
-	if (cfg->disable_inline_rgctx_fetch || cfg->interp_entry_only) {
-		MonoInst *args [2] = { rgctx, slot_ins };
-		if (entry->in_mrgctx)
-			call = mono_emit_jit_icall (cfg, mono_fill_method_rgctx, args);
-		else
-			call = mono_emit_jit_icall (cfg, mono_fill_class_rgctx, args);
-		return call;
-	}
-	MonoBasicBlock *slowpath_bb, *end_bb;
-	MonoInst *ins, *res;
-	int rgctx_reg, res_reg;
-	/*
-	 * rgctx = vtable->runtime_generic_context;
-	 * if (rgctx) {
-	 *    val = rgctx [slot + 1];
-	 *    if (val)
-	 *       return val;
-	 * }
-	 * <slowpath>
-	 */
-	NEW_BBLOCK (cfg, end_bb);
-	NEW_BBLOCK (cfg, slowpath_bb);
-	if (entry->in_mrgctx) {
-		rgctx_reg = rgctx->dreg;
-	} else {
-		rgctx_reg = alloc_preg (cfg);
-		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, rgctx_reg, rgctx->dreg, MONO_STRUCT_OFFSET (MonoVTable, runtime_generic_context));
-		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, rgctx_reg, 0);
-		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, slowpath_bb);
-	}
-	int table_size = mono_class_rgctx_get_array_size (0, entry->in_mrgctx);
-	if (entry->in_mrgctx)
-		table_size -= MONO_SIZEOF_METHOD_RUNTIME_GENERIC_CONTEXT / TARGET_SIZEOF_VOID_P;
-	MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, slot_ins->dreg, table_size - 1);
-	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBGE, slowpath_bb);
-	int shifted_slot_reg = alloc_ireg (cfg);
-	EMIT_NEW_BIALU_IMM (cfg, ins, OP_ISHL_IMM, shifted_slot_reg, slot_ins->dreg, TARGET_SIZEOF_VOID_P == 8 ? 3 : 2);
-	int addr_reg = alloc_preg (cfg);
-	EMIT_NEW_UNALU (cfg, ins, OP_MOVE, addr_reg, rgctx_reg);
-	EMIT_NEW_BIALU (cfg, ins, OP_PADD, addr_reg, addr_reg, shifted_slot_reg);
-	int val_reg = alloc_preg (cfg);
-	MONO_EMIT_NEW_LOAD_MEMBASE (cfg, val_reg, addr_reg, TARGET_SIZEOF_VOID_P + (entry->in_mrgctx ? MONO_SIZEOF_METHOD_RUNTIME_GENERIC_CONTEXT : 0));
-	MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, val_reg, 0);
-	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, slowpath_bb);
-	res_reg = alloc_preg (cfg);
-	EMIT_NEW_UNALU (cfg, ins, OP_MOVE, res_reg, val_reg);
-	res = ins;
-	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-	MONO_START_BB (cfg, slowpath_bb);
-	slowpath_bb->out_of_line = TRUE;
-	MonoInst *args[2] = { rgctx, slot_ins };
-	if (entry->in_mrgctx)
-		call = mono_emit_jit_icall (cfg, mono_fill_method_rgctx, args);
-	else
-		call = mono_emit_jit_icall (cfg, mono_fill_class_rgctx, args);
-	EMIT_NEW_UNALU (cfg, ins, OP_MOVE, res_reg, call->dreg);
-	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-	MONO_START_BB (cfg, end_bb);
-	return res;
-}
-/*
- * emit_rgctx_fetch:
- *
- *   Emit IR to load the value of the rgctx entry ENTRY from the rgctx.
- */
-static MonoInst*
-emit_rgctx_fetch (MonoCompile *cfg, int context_used, MonoJumpInfoRgctxEntry *entry)
-{
-	MonoInst *rgctx = emit_get_rgctx (cfg, context_used);
-	if (cfg->gshared_info) {
-		MonoInst *ins;
-		int dreg, entries_reg, idx;
-		int ninlines = mono_class_rgctx_get_array_size (0, TRUE);
-		idx = get_gshared_info_slot (cfg, entry->data, entry->info_type);
-		/* The first few entries are stored inline, the rest are stored in mrgctx->entries */
-		if (idx < ninlines) {
-			/* Load mrgctx->infos [idx] */
-			dreg = alloc_preg (cfg);
-			EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, dreg, rgctx->dreg, MONO_STRUCT_OFFSET (MonoMethodRuntimeGenericContext, infos) + idx * TARGET_SIZEOF_VOID_P);
-		} else {
-			/* Load mrgctx->entries [idx - ninlines] */
-			entries_reg = alloc_preg (cfg);
-			EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, entries_reg, rgctx->dreg, MONO_STRUCT_OFFSET (MonoMethodRuntimeGenericContext, entries));
-			dreg = alloc_preg (cfg);
-			EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, dreg, entries_reg, (idx - ninlines) * TARGET_SIZEOF_VOID_P);
-		}
-		return ins;
-	}
-	if (cfg->llvm_only)
-		return emit_rgctx_fetch_inline (cfg, rgctx, entry);
-	else
-		return mini_emit_abs_call (cfg, MONO_PATCH_INFO_RGCTX_FETCH, entry, mono_icall_sig_ptr_ptr, &rgctx);
-}
-/*
- * mini_emit_get_rgctx_klass:
- *
- *   Emit IR to load the property RGCTX_TYPE of KLASS. If context_used is 0, emit
- * normal constants, else emit a load from the rgctx.
- */
-MonoInst*
-mini_emit_get_rgctx_klass (MonoCompile *cfg, int context_used,
-						   MonoClass *klass, MonoRgctxInfoType rgctx_type)
-{
-	if (!context_used) {
-		MonoInst *ins;
-		switch (rgctx_type) {
-		case MONO_RGCTX_INFO_KLASS:
-			EMIT_NEW_CLASSCONST (cfg, ins, klass);
-			return ins;
-		case MONO_RGCTX_INFO_VTABLE: {
-			MonoVTable *vtable = mono_class_vtable_checked (klass, cfg->error);
-			CHECK_CFG_ERROR;
-			EMIT_NEW_VTABLECONST (cfg, ins, vtable);
-			return ins;
-		}
-		default:
-			g_assert_not_reached ();
-		}
-	}
-	if (cfg->llvm_only && cfg->gsharedvt && !cfg->gshared_info)
-		return mini_emit_get_gsharedvt_info_klass (cfg, klass, rgctx_type);
-	MonoJumpInfoRgctxEntry *entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_CLASS, klass, rgctx_type);
-	return emit_rgctx_fetch (cfg, context_used, entry);
-mono_error_exit:
-	return NULL;
-}
-static MonoInst*
-emit_get_rgctx_sig (MonoCompile *cfg, int context_used,
-					MonoMethodSignature *sig, MonoRgctxInfoType rgctx_type)
-{
-	MonoJumpInfoRgctxEntry *entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_SIGNATURE, sig, rgctx_type);
-	return emit_rgctx_fetch (cfg, context_used, entry);
-}
-static MonoInst*
-emit_get_rgctx_gsharedvt_call (MonoCompile *cfg, int context_used,
-							   MonoMethodSignature *sig, MonoMethod *cmethod, MonoRgctxInfoType rgctx_type)
-{
-	MonoJumpInfoGSharedVtCall *call_info;
-	MonoJumpInfoRgctxEntry *entry;
-	call_info = (MonoJumpInfoGSharedVtCall *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoJumpInfoGSharedVtCall));
-	call_info->sig = sig;
-	call_info->method = cmethod;
-	entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_GSHAREDVT_CALL, call_info, rgctx_type);
-	return emit_rgctx_fetch (cfg, context_used, entry);
-}
-/*
- * emit_get_rgctx_virt_method:
- *
- *   Return data for method VIRT_METHOD for a receiver of type KLASS.
- */
-static MonoInst*
-emit_get_rgctx_virt_method (MonoCompile *cfg, int context_used,
-							MonoClass *klass, MonoMethod *virt_method, MonoRgctxInfoType rgctx_type)
-{
-	MonoJumpInfoVirtMethod *info;
-	MonoJumpInfoRgctxEntry *entry;
-	if (context_used == -1)
-		context_used = mono_class_check_context_used (klass) | mono_method_check_context_used (virt_method);
-	info = (MonoJumpInfoVirtMethod *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoJumpInfoVirtMethod));
-	info->klass = klass;
-	info->method = virt_method;
-	entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_VIRT_METHOD, info, rgctx_type);
-	return emit_rgctx_fetch (cfg, context_used, entry);
-}
-static MonoInst*
-emit_get_rgctx_gsharedvt_method (MonoCompile *cfg, int context_used,
-								 MonoMethod *cmethod, MonoGSharedVtMethodInfo *info)
-{
-	MonoJumpInfoRgctxEntry *entry;
-	entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_GSHAREDVT_METHOD, info, MONO_RGCTX_INFO_METHOD_GSHAREDVT_INFO);
-	return emit_rgctx_fetch (cfg, context_used, entry);
-}
-/*
- * emit_get_rgctx_method:
- *
- *   Emit IR to load the property RGCTX_TYPE of CMETHOD. If context_used is 0, emit
- * normal constants, else emit a load from the rgctx.
- */
-static MonoInst*
-emit_get_rgctx_method (MonoCompile *cfg, int context_used,
-					   MonoMethod *cmethod, MonoRgctxInfoType rgctx_type)
-{
-	if (context_used == -1)
-		context_used = mono_method_check_context_used (cmethod);
-	if (!context_used) {
-		MonoInst *ins;
-		switch (rgctx_type) {
-		case MONO_RGCTX_INFO_METHOD:
-			EMIT_NEW_METHODCONST (cfg, ins, cmethod);
-			return ins;
-		case MONO_RGCTX_INFO_METHOD_RGCTX:
-			EMIT_NEW_METHOD_RGCTX_CONST (cfg, ins, cmethod);
-			return ins;
-		case MONO_RGCTX_INFO_METHOD_FTNDESC:
-			EMIT_NEW_AOTCONST (cfg, ins, MONO_PATCH_INFO_METHOD_FTNDESC, cmethod);
-			return ins;
-		case MONO_RGCTX_INFO_LLVMONLY_INTERP_ENTRY:
-			EMIT_NEW_AOTCONST (cfg, ins, MONO_PATCH_INFO_LLVMONLY_INTERP_ENTRY, cmethod);
-			return ins;
-		default:
-			g_assert_not_reached ();
-		}
-	} else {
-		if (cfg->llvm_only && cfg->gsharedvt && !cfg->gshared_info)
-			return emit_get_gsharedvt_info (cfg, cmethod, rgctx_type);
-		MonoJumpInfoRgctxEntry *entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_METHODCONST, cmethod, rgctx_type);
-		return emit_rgctx_fetch (cfg, context_used, entry);
-	}
-}
-static MonoInst*
-emit_get_rgctx_field (MonoCompile *cfg, int context_used,
-					  MonoClassField *field, MonoRgctxInfoType rgctx_type)
-{
-	if (cfg->llvm_only && cfg->gsharedvt && !cfg->gshared_info)
-		return emit_get_gsharedvt_info (cfg, field, rgctx_type);
-	MonoJumpInfoRgctxEntry *entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_FIELD, field, rgctx_type);
-	return emit_rgctx_fetch (cfg, context_used, entry);
-}
-MonoInst*
-mini_emit_get_rgctx_method (MonoCompile *cfg, int context_used,
-							MonoMethod *cmethod, MonoRgctxInfoType rgctx_type)
-{
-	return emit_get_rgctx_method (cfg, context_used, cmethod, rgctx_type);
-}
-static int
-get_gsharedvt_info_slot (MonoCompile *cfg, gpointer data, MonoRgctxInfoType rgctx_type)
-{
-	MonoGSharedVtMethodInfo *info = cfg->gsharedvt_info;
-	MonoRuntimeGenericContextInfoTemplate *template_;
-	int i, idx;
-	g_assert (info);
-	for (i = 0; i < info->num_entries; ++i) {
-		MonoRuntimeGenericContextInfoTemplate *otemplate = &info->entries [i];
-		if (otemplate->info_type == rgctx_type && otemplate->data == data && rgctx_type != MONO_RGCTX_INFO_LOCAL_OFFSET)
-			return i;
-	}
-	if (info->num_entries == info->count_entries) {
-		MonoRuntimeGenericContextInfoTemplate *new_entries;
-		int new_count_entries = info->count_entries ? info->count_entries * 2 : 16;
-		new_entries = (MonoRuntimeGenericContextInfoTemplate *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoRuntimeGenericContextInfoTemplate) * new_count_entries);
-		memcpy (new_entries, info->entries, sizeof (MonoRuntimeGenericContextInfoTemplate) * info->count_entries);
-		info->entries = new_entries;
-		info->count_entries = new_count_entries;
-	}
-	idx = info->num_entries;
-	template_ = &info->entries [idx];
-	template_->info_type = rgctx_type;
-	template_->data = data;
-	info->num_entries ++;
-	return idx;
-}
-/*
- * emit_get_gsharedvt_info:
- *
- *   This is similar to emit_get_rgctx_.., but loads the data from the gsharedvt info var instead of calling an rgctx fetch trampoline.
- */
-static MonoInst*
-emit_get_gsharedvt_info (MonoCompile *cfg, gpointer data, MonoRgctxInfoType rgctx_type)
-{
-	MonoInst *ins;
-	int idx, dreg;
-	idx = get_gsharedvt_info_slot (cfg, data, rgctx_type);
-	/* Load info->entries [idx] */
-	dreg = alloc_preg (cfg);
-	EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, dreg, cfg->gsharedvt_info_var->dreg, MONO_STRUCT_OFFSET (MonoGSharedVtMethodRuntimeInfo, entries) + (idx * TARGET_SIZEOF_VOID_P));
-	return ins;
-}
-MonoInst*
-mini_emit_get_gsharedvt_info_klass (MonoCompile *cfg, MonoClass *klass, MonoRgctxInfoType rgctx_type)
-{
-	return emit_get_gsharedvt_info (cfg, m_class_get_byval_arg (klass), rgctx_type);
-}
-/*
- * On return the caller must check @klass for load errors.
- */
-static void
-emit_class_init (MonoCompile *cfg, MonoClass *klass, gboolean for_field_access)
-{
-	MonoInst *vtable_arg;
-	int context_used;
-	context_used = mini_class_check_context_used (cfg, klass);
-	if (cfg->compile_aot && !for_field_access && mono_class_is_before_field_init (klass))
-		/* Only field accesses trigger initialization */
-		return;
-	if (context_used) {
-		vtable_arg = mini_emit_get_rgctx_klass (cfg, context_used,
-										   klass, MONO_RGCTX_INFO_VTABLE);
-	} else {
-		MonoVTable *vtable = mono_class_vtable_checked (klass, cfg->error);
-		if (!is_ok (cfg->error)) {
-			mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
-			return;
-		}
-		EMIT_NEW_VTABLECONST (cfg, vtable_arg, vtable);
-	}
-	if (!COMPILE_LLVM (cfg) && cfg->backend->have_op_generic_class_init) {
-		MonoInst *ins;
-		/*
-		 * Using an opcode instead of emitting IR here allows the hiding of the call inside the opcode,
-		 * so this doesn't have to clobber any regs and it doesn't break basic blocks.
-		 */
-		MONO_INST_NEW (cfg, ins, OP_GENERIC_CLASS_INIT);
-		ins->sreg1 = vtable_arg->dreg;
-		MONO_ADD_INS (cfg->cbb, ins);
-	} else {
-		int inited_reg;
-		MonoBasicBlock *inited_bb;
-		inited_reg = alloc_ireg (cfg);
-		MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADU1_MEMBASE, inited_reg, vtable_arg->dreg, MONO_STRUCT_OFFSET (MonoVTable, initialized));
-		NEW_BBLOCK (cfg, inited_bb);
-		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, inited_reg, 0);
-		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBNE_UN, inited_bb);
-		cfg->cbb->out_of_line = TRUE;
-		mono_emit_jit_icall (cfg, mono_generic_class_init, &vtable_arg);
-		MONO_START_BB (cfg, inited_bb);
-	}
-}
-static void
-emit_seq_point (MonoCompile *cfg, MonoMethod *method, guint8* ip, gboolean intr_loc, gboolean nonempty_stack)
-{
-	MonoInst *ins;
-	if (cfg->gen_seq_points && cfg->method == method) {
-		NEW_SEQ_POINT (cfg, ins, ip - cfg->header->code, intr_loc);
-		if (nonempty_stack)
-			ins->flags |= MONO_INST_NONEMPTY_STACK;
-		MONO_ADD_INS (cfg->cbb, ins);
-		cfg->last_seq_point = ins;
-	}
-}
-void
-mini_save_cast_details (MonoCompile *cfg, MonoClass *klass, int obj_reg, gboolean null_check)
-{
-	if (mini_debug_options.better_cast_details) {
-		int vtable_reg = alloc_preg (cfg);
-		int klass_reg = alloc_preg (cfg);
-		MonoBasicBlock *is_null_bb = NULL;
-		MonoInst *tls_get;
-		if (null_check) {
-			NEW_BBLOCK (cfg, is_null_bb);
-			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, obj_reg, 0);
-			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, is_null_bb);
-		}
-		tls_get = mono_create_tls_get (cfg, TLS_KEY_JIT_TLS);
-		if (!tls_get) {
-			fprintf (stderr, "error: --debug=casts not supported on this platform.\n.");
-			exit (1);
-		}
-		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, vtable_reg, obj_reg, MONO_STRUCT_OFFSET (MonoObject, vtable));
-		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, klass_reg, vtable_reg, MONO_STRUCT_OFFSET (MonoVTable, klass));
-		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, tls_get->dreg, MONO_STRUCT_OFFSET (MonoJitTlsData, class_cast_from), klass_reg);
-		MonoInst *class_ins = mini_emit_get_rgctx_klass (cfg, mini_class_check_context_used (cfg, klass), klass, MONO_RGCTX_INFO_KLASS);
-		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, tls_get->dreg, MONO_STRUCT_OFFSET (MonoJitTlsData, class_cast_to), class_ins->dreg);
-		if (null_check)
-			MONO_START_BB (cfg, is_null_bb);
-	}
-}
-void
-mini_reset_cast_details (MonoCompile *cfg)
-{
-	/* Reset the variables holding the cast details */
-	if (mini_debug_options.better_cast_details) {
-		MonoInst *tls_get = mono_create_tls_get (cfg, TLS_KEY_JIT_TLS);
-		/* It is enough to reset the from field */
-		MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STORE_MEMBASE_IMM, tls_get->dreg, MONO_STRUCT_OFFSET (MonoJitTlsData, class_cast_from), 0);
-	}
-}
-/*
- * On return the caller must check @array_class for load errors
- */
-static void
-mini_emit_check_array_type (MonoCompile *cfg, MonoInst *obj, MonoClass *array_class)
-{
-	int vtable_reg = alloc_preg (cfg);
-	int context_used;
-	context_used = mini_class_check_context_used (cfg, array_class);
-	mini_save_cast_details (cfg, array_class, obj->dreg, FALSE);
-	MONO_EMIT_NEW_LOAD_MEMBASE_FAULT (cfg, vtable_reg, obj->dreg, MONO_STRUCT_OFFSET (MonoObject, vtable));
-	if (context_used) {
-		MonoInst *vtable_ins;
-		vtable_ins = mini_emit_get_rgctx_klass (cfg, context_used, array_class, MONO_RGCTX_INFO_VTABLE);
-		MONO_EMIT_NEW_BIALU (cfg, OP_COMPARE, -1, vtable_reg, vtable_ins->dreg);
-	} else {
-		if (cfg->compile_aot) {
-			int vt_reg;
-			MonoVTable *vtable;
-			if (!(vtable = mono_class_vtable_checked (array_class, cfg->error))) {
-				mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
-				return;
-			}
-			vt_reg = alloc_preg (cfg);
-			MONO_EMIT_NEW_VTABLECONST (cfg, vt_reg, vtable);
-			MONO_EMIT_NEW_BIALU (cfg, OP_COMPARE, -1, vtable_reg, vt_reg);
-		} else {
-			MonoVTable *vtable;
-			if (!(vtable = mono_class_vtable_checked (array_class, cfg->error))) {
-				mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
-				return;
-			}
-			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, vtable_reg, (gssize)vtable);
-		}
-	}
-	MONO_EMIT_NEW_COND_EXC (cfg, NE_UN, "ArrayTypeMismatchException");
-	mini_reset_cast_details (cfg);
-}
-/**
- * Handles unbox of a Nullable<T>. If context_used is non zero, then shared
- * generic code is generated.
- */
-static MonoInst*
-handle_unbox_nullable (MonoCompile* cfg, MonoInst* val, MonoClass* klass, int context_used)
-{
-	MonoMethod* method;
-	if (m_class_is_enumtype (mono_class_get_nullable_param_internal (klass)))
-		method = get_method_nofail (klass, "UnboxExact", 1, 0);
-	else
-		method = get_method_nofail (klass, "Unbox", 1, 0);
-	g_assert (method);
-	if (context_used) {
-		MonoInst *rgctx, *addr;
-		/* FIXME: What if the class is shared?  We might not
-		   have to get the address of the method from the
-		   RGCTX. */
-		if (cfg->llvm_only) {
-			addr = emit_get_rgctx_method (cfg, context_used, method,
-										  MONO_RGCTX_INFO_METHOD_FTNDESC);
-			cfg->signatures = g_slist_prepend_mempool (cfg->mempool, cfg->signatures, mono_method_signature_internal (method));
-			return mini_emit_llvmonly_calli (cfg, mono_method_signature_internal (method), &val, addr);
-		} else {
-			addr = emit_get_rgctx_method (cfg, context_used, method,
-										  MONO_RGCTX_INFO_GENERIC_METHOD_CODE);
-			rgctx = emit_get_rgctx (cfg, context_used);
-			return mini_emit_calli (cfg, mono_method_signature_internal (method), &val, addr, NULL, rgctx);
-		}
-	} else {
-		MonoInst *rgctx_arg = NULL;
-		if (need_mrgctx_arg (cfg, method))
-			rgctx_arg = emit_get_rgctx_method (cfg, context_used, method,
-											   MONO_RGCTX_INFO_METHOD_RGCTX);
-		return mini_emit_method_call_full (cfg, method, NULL, FALSE, &val, NULL, NULL, rgctx_arg);
-	}
-}
-MonoInst*
-mini_handle_unbox (MonoCompile *cfg, MonoClass *klass, MonoInst *val, int context_used)
-{
-	MonoInst *add;
-	int obj_reg;
-	int vtable_reg = alloc_dreg (cfg ,STACK_PTR);
-	int klass_reg = alloc_dreg (cfg ,STACK_PTR);
-	int eclass_reg = alloc_dreg (cfg ,STACK_PTR);
-	int rank_reg = alloc_dreg (cfg ,STACK_I4);
-	obj_reg = val->dreg;
-	MONO_EMIT_NEW_LOAD_MEMBASE_FAULT (cfg, vtable_reg, obj_reg, MONO_STRUCT_OFFSET (MonoObject, vtable));
-	MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADU1_MEMBASE, rank_reg, vtable_reg, MONO_STRUCT_OFFSET (MonoVTable, rank));
-	/* FIXME: generics */
-	g_assert (m_class_get_rank (klass) == 0);
-	MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, rank_reg, 0);
-	MONO_EMIT_NEW_COND_EXC (cfg, NE_UN, "InvalidCastException");
-	MONO_EMIT_NEW_LOAD_MEMBASE (cfg, klass_reg, vtable_reg, MONO_STRUCT_OFFSET (MonoVTable, klass));
-	MONO_EMIT_NEW_LOAD_MEMBASE (cfg, eclass_reg, klass_reg, m_class_offsetof_element_class ());
-	if (context_used) {
-		MonoInst *element_class;
-		/* This assertion is from the unboxcast insn */
-		g_assert (m_class_get_rank (klass) == 0);
-		element_class = mini_emit_get_rgctx_klass (cfg, context_used,
-				klass, MONO_RGCTX_INFO_ELEMENT_KLASS);
-		MONO_EMIT_NEW_BIALU (cfg, OP_COMPARE, -1, eclass_reg, element_class->dreg);
-		MONO_EMIT_NEW_COND_EXC (cfg, NE_UN, "InvalidCastException");
-	} else {
-		mini_save_cast_details (cfg, m_class_get_element_class (klass), obj_reg, FALSE);
-		mini_emit_class_check (cfg, eclass_reg, m_class_get_element_class (klass));
-		mini_reset_cast_details (cfg);
-	}
-	NEW_BIALU_IMM (cfg, add, OP_ADD_IMM, alloc_dreg (cfg, STACK_MP), obj_reg, MONO_ABI_SIZEOF (MonoObject));
-	MONO_ADD_INS (cfg->cbb, add);
-	add->type = STACK_MP;
-	add->klass = klass;
-	return add;
-}
-static MonoInst*
-handle_unbox_gsharedvt (MonoCompile *cfg, MonoClass *klass, MonoInst *obj)
-{
-	MonoInst *addr, *klass_inst, *is_ref, *args[16];
-	MonoBasicBlock *is_ref_bb, *is_nullable_bb, *end_bb;
-	MonoInst *ins;
-	int dreg, addr_reg;
-	klass_inst = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_KLASS);
-	/* obj */
-	args [0] = obj;
-	/* klass */
-	args [1] = klass_inst;
-	/* CASTCLASS */
-	obj = mono_emit_jit_icall (cfg, mono_object_castclass_unbox, args);
-	NEW_BBLOCK (cfg, is_ref_bb);
-	NEW_BBLOCK (cfg, is_nullable_bb);
-	NEW_BBLOCK (cfg, end_bb);
-	is_ref = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_CLASS_BOX_TYPE);
-	MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, is_ref->dreg, MONO_GSHAREDVT_BOX_TYPE_REF);
-	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBEQ, is_ref_bb);
-	MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, is_ref->dreg, MONO_GSHAREDVT_BOX_TYPE_NULLABLE);
-	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBEQ, is_nullable_bb);
-	/* This will contain either the address of the unboxed vtype, or an address of the temporary where the ref is stored */
-	addr_reg = alloc_dreg (cfg, STACK_MP);
-	/* Non-ref case */
-	/* UNBOX */
-	NEW_BIALU_IMM (cfg, addr, OP_ADD_IMM, addr_reg, obj->dreg, MONO_ABI_SIZEOF (MonoObject));
-	MONO_ADD_INS (cfg->cbb, addr);
-	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-	/* Ref case */
-	MONO_START_BB (cfg, is_ref_bb);
-	/* Save the ref to a temporary */
-	dreg = alloc_ireg (cfg);
-	EMIT_NEW_VARLOADA_VREG (cfg, addr, dreg, m_class_get_byval_arg (klass));
-	addr->dreg = addr_reg;
-	MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, addr->dreg, 0, obj->dreg);
-	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-	/* Nullable case */
-	MONO_START_BB (cfg, is_nullable_bb);
-	{
-		MonoInst *unbox_addr = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_NULLABLE_CLASS_UNBOX);
-		MonoInst *unbox_call;
-		MonoMethodSignature *unbox_sig;
-		unbox_sig = (MonoMethodSignature *)mono_mempool_alloc0 (cfg->mempool, MONO_SIZEOF_METHOD_SIGNATURE + (1 * sizeof (MonoType *)));
-		unbox_sig->ret = m_class_get_byval_arg (klass);
-		unbox_sig->param_count = 1;
-		unbox_sig->params [0] = mono_get_object_type ();
-		if (cfg->llvm_only)
-			unbox_call = mini_emit_llvmonly_calli (cfg, unbox_sig, &obj, unbox_addr );
-		else
-			unbox_call = mini_emit_calli (cfg, unbox_sig, &obj, unbox_addr , NULL, NULL);
-		EMIT_NEW_VARLOADA_VREG (cfg, unbox_addr , unbox_call->dreg, m_class_get_byval_arg (klass));
-		unbox_addr ->dreg = addr_reg;
-	}
-	MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-	/* End */
-	MONO_START_BB (cfg, end_bb);
-	/* LDOBJ */
-	EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), addr_reg, 0);
-	return ins;
-}
-/*
- * Returns NULL and set the cfg exception on error.
- */
-static MonoInst*
-handle_alloc (MonoCompile *cfg, MonoClass *klass, gboolean for_box, int context_used)
-{
-	MonoInst *iargs [2];
-	MonoJitICallId alloc_ftn;
-	if (mono_class_get_flags (klass) & TYPE_ATTRIBUTE_ABSTRACT) {
-		char* full_name = mono_type_get_full_name (klass);
-		mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
-		mono_error_set_member_access (cfg->error, "Cannot create an abstract class: %s", full_name);
-		g_free (full_name);
-		return NULL;
-	}
-	if (context_used) {
-		gboolean known_instance_size = !mini_is_gsharedvt_klass (klass);
-		MonoMethod *managed_alloc = mono_gc_get_managed_allocator (klass, for_box, known_instance_size);
-		iargs [0] = mini_emit_get_rgctx_klass (cfg, context_used, klass, MONO_RGCTX_INFO_VTABLE);
-		alloc_ftn = MONO_JIT_ICALL_ves_icall_object_new_specific;
-		if (managed_alloc) {
-			if (known_instance_size) {
-				int size = mono_class_instance_size (klass);
-				if (size < MONO_ABI_SIZEOF (MonoObject))
-					g_error ("Invalid size %d for class %s", size, mono_type_get_full_name (klass));
-				EMIT_NEW_ICONST (cfg, iargs [1], size);
-			}
-			return mono_emit_method_call (cfg, managed_alloc, iargs, NULL);
-		}
-		return mono_emit_jit_icall_id (cfg, alloc_ftn, iargs);
-	}
-	if (cfg->compile_aot && cfg->cbb->out_of_line && m_class_get_type_token (klass) && m_class_get_image (klass) == mono_defaults.corlib && !mono_class_is_ginst (klass)) {
-		/* This happens often in argument checking code, eg. throw new FooException... */
-		/* Avoid relocations and save some space by calling a helper function specialized to mscorlib */
-		EMIT_NEW_ICONST (cfg, iargs [0], mono_metadata_token_index (m_class_get_type_token (klass)));
-		alloc_ftn = MONO_JIT_ICALL_mono_helper_newobj_mscorlib;
-	} else {
-		MonoVTable *vtable = mono_class_vtable_checked (klass, cfg->error);
-		if (!is_ok (cfg->error)) {
-			mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
-			return NULL;
-		}
-		MonoMethod *managed_alloc = mono_gc_get_managed_allocator (klass, for_box, TRUE);
-		if (managed_alloc) {
-			int size = mono_class_instance_size (klass);
-			if (size < MONO_ABI_SIZEOF (MonoObject))
-				g_error ("Invalid size %d for class %s", size, mono_type_get_full_name (klass));
-			EMIT_NEW_VTABLECONST (cfg, iargs [0], vtable);
-			EMIT_NEW_ICONST (cfg, iargs [1], size);
-			return mono_emit_method_call (cfg, managed_alloc, iargs, NULL);
-		}
-		alloc_ftn = MONO_JIT_ICALL_ves_icall_object_new_specific;
-		EMIT_NEW_VTABLECONST (cfg, iargs [0], vtable);
-	}
-	return mono_emit_jit_icall_id (cfg, alloc_ftn, iargs);
-}
-/*
- * Returns NULL and set the cfg exception on error.
- */
-MonoInst*
-mini_emit_box (MonoCompile *cfg, MonoInst *val, MonoClass *klass, int context_used)
-{
-	MonoInst *alloc, *ins;
-	if (G_UNLIKELY (m_class_is_byreflike (klass))) {
-		mono_error_set_bad_image (cfg->error, m_class_get_image (cfg->method->klass), "Cannot box IsByRefLike type '%s.%s'", m_class_get_name_space (klass), m_class_get_name (klass));
-		mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
-		return NULL;
-	}
-	if (mono_class_is_nullable (klass)) {
-		MonoMethod* method = get_method_nofail (klass, "Box", 1, 0);
-		if (context_used) {
-			if (cfg->llvm_only) {
-				MonoInst *addr;
-				MonoMethodSignature *sig = mono_method_signature_internal (method);
-				if (mini_is_gsharedvt_klass (klass))
-					addr = mini_emit_get_gsharedvt_info_klass (cfg, klass,
-															   MONO_RGCTX_INFO_NULLABLE_CLASS_BOX);
-				else
-					addr = emit_get_rgctx_method (cfg, context_used, method,
-												  MONO_RGCTX_INFO_METHOD_FTNDESC);
-				cfg->interp_in_signatures = g_slist_prepend_mempool (cfg->mempool, cfg->interp_in_signatures, sig);
-				return mini_emit_llvmonly_calli (cfg, sig, &val, addr);
-			} else {
-				/* FIXME: What if the class is shared?  We might not
-				   have to get the method address from the RGCTX. */
-				MonoInst *addr = emit_get_rgctx_method (cfg, context_used, method,
-														MONO_RGCTX_INFO_GENERIC_METHOD_CODE);
-				MonoInst *rgctx = emit_get_rgctx (cfg, context_used);
-				return mini_emit_calli (cfg, mono_method_signature_internal (method), &val, addr, NULL, rgctx);
-			}
-		} else {
-			MonoInst *rgctx_arg = NULL;
-			if (need_mrgctx_arg (cfg, method))
-				rgctx_arg = emit_get_rgctx_method (cfg, context_used, method,
-												   MONO_RGCTX_INFO_METHOD_RGCTX);
-			return mini_emit_method_call_full (cfg, method, NULL, FALSE, &val, NULL, NULL, rgctx_arg);
-		}
-	}
-	if (mini_is_gsharedvt_klass (klass)) {
-		MonoBasicBlock *is_ref_bb, *is_nullable_bb, *end_bb;
-		MonoInst *res, *is_ref, *src_var, *addr;
-		int dreg;
-		dreg = alloc_ireg (cfg);
-		NEW_BBLOCK (cfg, is_ref_bb);
-		NEW_BBLOCK (cfg, is_nullable_bb);
-		NEW_BBLOCK (cfg, end_bb);
-		is_ref = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_CLASS_BOX_TYPE);
-		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, is_ref->dreg, MONO_GSHAREDVT_BOX_TYPE_REF);
-		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBEQ, is_ref_bb);
-		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, is_ref->dreg, MONO_GSHAREDVT_BOX_TYPE_NULLABLE);
-		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBEQ, is_nullable_bb);
-		/* Non-ref case */
-		alloc = handle_alloc (cfg, klass, TRUE, context_used);
-		if (!alloc)
-			return NULL;
-		EMIT_NEW_STORE_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), alloc->dreg, MONO_ABI_SIZEOF (MonoObject), val->dreg);
-		ins->opcode = OP_STOREV_MEMBASE;
-		EMIT_NEW_UNALU (cfg, res, OP_MOVE, dreg, alloc->dreg);
-		res->type = STACK_OBJ;
-		res->klass = klass;
-		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-		/* Ref case */
-		MONO_START_BB (cfg, is_ref_bb);
-		/* val is a vtype, so has to load the value manually */
-		src_var = get_vreg_to_inst (cfg, val->dreg);
-		if (!src_var)
-			src_var = mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (klass), OP_LOCAL, val->dreg);
-		EMIT_NEW_VARLOADA (cfg, addr, src_var, src_var->inst_vtype);
-		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, dreg, addr->dreg, 0);
-		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-		/* Nullable case */
-		MONO_START_BB (cfg, is_nullable_bb);
-		{
-			MonoInst *box_addr = mini_emit_get_gsharedvt_info_klass (cfg, klass,
-													MONO_RGCTX_INFO_NULLABLE_CLASS_BOX);
-			MonoInst *box_call;
-			MonoMethodSignature *box_sig;
-			/*
-			 * klass is Nullable<T>, need to call Nullable<T>.Box () using a gsharedvt signature, but we cannot
-			 * construct that method at JIT time, so have to do things by hand.
-			 */
-			box_sig = (MonoMethodSignature *)mono_mempool_alloc0 (cfg->mempool, MONO_SIZEOF_METHOD_SIGNATURE + (1 * sizeof (MonoType *)));
-			box_sig->ret = mono_get_object_type ();
-			box_sig->param_count = 1;
-			box_sig->params [0] = m_class_get_byval_arg (klass);
-			if (cfg->llvm_only)
-				box_call = mini_emit_llvmonly_calli (cfg, box_sig, &val, box_addr);
-			else
-				box_call = mini_emit_calli (cfg, box_sig, &val, box_addr, NULL, NULL);
-			EMIT_NEW_UNALU (cfg, res, OP_MOVE, dreg, box_call->dreg);
-			res->type = STACK_OBJ;
-			res->klass = klass;
-		}
-		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-		MONO_START_BB (cfg, end_bb);
-		return res;
-	}
-	alloc = handle_alloc (cfg, klass, TRUE, context_used);
-	if (!alloc)
-		return NULL;
-	EMIT_NEW_STORE_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), alloc->dreg, MONO_ABI_SIZEOF (MonoObject), val->dreg);
-	return alloc;
-}
-static gboolean
-method_needs_stack_walk (MonoCompile *cfg, MonoMethod *cmethod)
-{
-	if (cmethod->klass == mono_defaults.systemtype_class) {
-		if (!strcmp (cmethod->name, "GetType"))
-			return TRUE;
-	}
-	/*
-	 * In corelib code, methods which need to do a stack walk declare a StackCrawlMark local and pass it as an
-	 * arguments until it reaches an icall. Its hard to detect which methods do that especially with
-	 * StackCrawlMark.LookForMyCallersCaller, so for now, just hardcode the classes which contain the public
-	 * methods whose caller is needed.
-	 */
-	if (mono_is_corlib_image (m_class_get_image (cmethod->klass))) {
-		const char *cname = m_class_get_name (cmethod->klass);
-		if (!strcmp (cname, "Assembly") ||
-			!strcmp (cname, "AssemblyLoadContext") ||
-			(!strcmp (cname, "Activator"))) {
-			if (!strcmp (cmethod->name, "op_Equality"))
-				return FALSE;
-			return TRUE;
-		}
-	}
-	return FALSE;
-}
-G_GNUC_UNUSED MonoInst*
-mini_handle_enum_has_flag (MonoCompile *cfg, MonoClass *klass, MonoInst *enum_this, int enum_val_reg, MonoInst *enum_flag)
-{
-	MonoType *enum_type = mono_type_get_underlying_type (m_class_get_byval_arg (klass));
-	guint32 load_opc = mono_type_to_load_membase (cfg, enum_type);
-	gboolean is_i4;
-	switch (enum_type->type) {
-	case MONO_TYPE_I8:
-	case MONO_TYPE_U8:
-#if SIZEOF_REGISTER == 8
-	case MONO_TYPE_I:
-	case MONO_TYPE_U:
-#endif
-		is_i4 = FALSE;
-		break;
-	default:
-		is_i4 = TRUE;
-		break;
-	}
-	{
-		MonoInst *load = NULL, *and_, *cmp, *ceq;
-		int enum_reg = is_i4 ? alloc_ireg (cfg) : alloc_lreg (cfg);
-		int and_reg = is_i4 ? alloc_ireg (cfg) : alloc_lreg (cfg);
-		int dest_reg = alloc_ireg (cfg);
-		if (enum_this) {
-			EMIT_NEW_LOAD_MEMBASE (cfg, load, load_opc, enum_reg, enum_this->dreg, 0);
-		} else {
-			g_assert (enum_val_reg != -1);
-			enum_reg = enum_val_reg;
-		}
-		EMIT_NEW_BIALU (cfg, and_, is_i4 ? OP_IAND : OP_LAND, and_reg, enum_reg, enum_flag->dreg);
-		EMIT_NEW_BIALU (cfg, cmp, is_i4 ? OP_ICOMPARE : OP_LCOMPARE, -1, and_reg, enum_flag->dreg);
-		EMIT_NEW_UNALU (cfg, ceq, is_i4 ? OP_ICEQ : OP_LCEQ, dest_reg, -1);
-		ceq->type = STACK_I4;
-		if (!is_i4) {
-			load = load ? mono_decompose_opcode (cfg, load) : NULL;
-			and_ = mono_decompose_opcode (cfg, and_);
-			cmp = mono_decompose_opcode (cfg, cmp);
-			ceq = mono_decompose_opcode (cfg, ceq);
-		}
-		return ceq;
-	}
-}
-static void
-emit_set_deopt_il_offset (MonoCompile *cfg, int offset)
-{
-	MonoInst *ins;
-	if (!(cfg->deopt && cfg->method == cfg->current_method))
-		return;
-	EMIT_NEW_VARLOADA (cfg, ins, cfg->il_state_var, NULL);
-	MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STOREI4_MEMBASE_IMM, ins->dreg, MONO_STRUCT_OFFSET (MonoMethodILState, il_offset), offset);
-}
-static MonoInst*
-emit_get_rgctx_dele_tramp_info (MonoCompile *cfg, int context_used,
-								MonoClass *klass, MonoMethod *method, gboolean is_virtual, MonoRgctxInfoType rgctx_type)
-{
-	MonoDelegateClassMethodPair *info;
-	MonoJumpInfoRgctxEntry *entry;
-	info = (MonoDelegateClassMethodPair *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoDelegateClassMethodPair));
-	info->klass = klass;
-	info->method = method;
-	info->is_virtual = is_virtual;
-	if (!context_used) {
-		MonoInst *ins;
-		g_assert (rgctx_type == MONO_RGCTX_INFO_DELEGATE_TRAMP_INFO);
-		if (cfg->compile_aot) {
-			EMIT_NEW_AOTCONST (cfg, ins, MONO_PATCH_INFO_DELEGATE_INFO, info);
-		} else {
-			MonoDelegateTrampInfo *tramp_info = mono_create_delegate_trampoline_info (klass, method, is_virtual);
-			EMIT_NEW_PCONST (cfg, ins, tramp_info);
-		}
-		return ins;
-	}
-	entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_DELEGATE_INFO, info, rgctx_type);
-	return emit_rgctx_fetch (cfg, context_used, entry);
-}
-/*
- * Returns NULL and set the cfg exception on error.
- */
-static G_GNUC_UNUSED MonoInst*
-handle_delegate_ctor (MonoCompile *cfg, MonoClass *klass, MonoInst *target, MonoMethod *method, int target_method_context_used, int invoke_context_used, gboolean is_virtual)
-{
-	MonoInst *ptr;
-	int dreg;
-	MonoInst *obj, *info_ins;
-	if (is_virtual && !cfg->llvm_only) {
-		MonoMethod *invoke = mono_get_delegate_invoke_internal (klass);
-		g_assert (invoke);
-		if (invoke_context_used || !mono_get_delegate_virtual_invoke_impl (mono_method_signature_internal (invoke), target_method_context_used ? NULL : method))
-			return NULL;
-	}
-	obj = handle_alloc (cfg, klass, FALSE, invoke_context_used);
-	if (!obj)
-		return NULL;
-	/* Inline the contents of mini_init_delegate */
-	/* Set target field */
-	/* Optimize away setting of NULL target */
-	if (!MONO_INS_IS_PCONST_NULL (target)) {
-		if (!(method->flags & METHOD_ATTRIBUTE_STATIC)) {
-			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, target->dreg, 0);
-			MONO_EMIT_NEW_COND_EXC (cfg, EQ, "NullReferenceException");
-		}
-		if (!mini_debug_options.weak_memory_model)
-			mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
-		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, target), target->dreg);
-		if (cfg->gen_write_barriers) {
-			dreg = alloc_preg (cfg);
-			EMIT_NEW_BIALU_IMM (cfg, ptr, OP_PADD_IMM, dreg, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, target));
-			mini_emit_write_barrier (cfg, ptr, target);
-		}
-	}
-	info_ins = emit_get_rgctx_dele_tramp_info (cfg, target_method_context_used | invoke_context_used, klass, method, is_virtual, MONO_RGCTX_INFO_DELEGATE_TRAMP_INFO);
-	if (cfg->llvm_only) {
-		MonoInst *args [] = {
-			obj,
-			info_ins
-		};
-		mono_emit_jit_icall (cfg, mini_llvmonly_init_delegate, args);
-		return obj;
-	}
-	/* Set invoke_info field */
-	MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, invoke_info), info_ins->dreg);
-	/* Set method field */
-	if (target_method_context_used || invoke_context_used) {
-		dreg = alloc_preg (cfg);
-		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, dreg, info_ins->dreg, MONO_STRUCT_OFFSET (MonoDelegateTrampInfo, method));
-		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, method), dreg);
-	} else {
-		MonoInst *method_ins = emit_get_rgctx_method (cfg, target_method_context_used, method, MONO_RGCTX_INFO_METHOD);
-		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, method), method_ins->dreg);
-	}
-	/* Set invoke_impl field */
-	dreg = alloc_preg (cfg);
-	MONO_EMIT_NEW_LOAD_MEMBASE (cfg, dreg, info_ins->dreg, MONO_STRUCT_OFFSET (MonoDelegateTrampInfo, invoke_impl));
-	MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, invoke_impl), dreg);
-	if (!is_virtual) {
-		dreg = alloc_preg (cfg);
-		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, dreg, info_ins->dreg, MONO_STRUCT_OFFSET (MonoDelegateTrampInfo, method_ptr));
-		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, method_ptr), dreg);
-	}
-	if (is_virtual) {
-		dreg = alloc_preg (cfg);
-		MONO_EMIT_NEW_ICONST (cfg, dreg, 1);
-		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STOREI1_MEMBASE_REG, obj->dreg, MONO_STRUCT_OFFSET (MonoDelegate, method_is_virtual), dreg);
-	}
-	/* All the checks which are in mono_delegate_ctor () are done by the delegate trampoline */
-	return obj;
-}
-/*
- * handle_constrained_gsharedvt_call:
- *
- *   Handle constrained calls where the receiver is a gsharedvt type.
- * Return the instruction representing the call. Set the cfg exception on failure.
- */
-static MonoInst*
-handle_constrained_gsharedvt_call (MonoCompile *cfg, MonoMethod *cmethod, MonoMethodSignature *fsig, MonoInst **sp, MonoClass *constrained_class,
-								   gboolean *ref_emit_widen)
-{
-	MonoInst *ins = NULL;
-	gboolean emit_widen = *ref_emit_widen;
-	gboolean supported;
-	MonoJumpInfoVirtMethod *info;
-	MonoJumpInfoRgctxEntry *entry;
-	MonoInst *call_info_ins;
-	int context_used;
-	MonoBasicBlock *end_bb = NULL, *slowpath_bb = NULL;
-	MonoInst *calls [2];
-	MonoInst *args [7];
-	MonoInst *orig_receiver = sp [0];
-	/*
-	 * The calls are of the form:
-	 * .constrained T_GSHAREDVT
-	 * callvirt <method>
-	 *
-	 * There are 3 basic cases:
-	 * - T is a vtype and the called method is a vtype method (ie. on T).
-	 *   In this case a normal call is made.
-	 * - T is a vtype, and the called method is a method on a reference type
-	 *   (i.e. a method on Object/Valuetype/Enum)
-	 *   In this case the receiver needs to be boxed.
-	 * - T is a reference type.
-	 *   In this case, it needs to be dereferenced (since its type is T&), and
-	 *   a virtual call is made based on its runtime type.
-	 *
-	 * This is implemented by precomputing some data into an rgctx slot, then
-	 * passing that data to jit icalls.
-	 */
-	supported = ((cmethod->klass == mono_defaults.object_class) || mono_class_is_interface (cmethod->klass) || (!m_class_is_valuetype (cmethod->klass) && m_class_get_image (cmethod->klass) != mono_defaults.corlib));
-	if (supported)
-		supported = (MONO_TYPE_IS_VOID (fsig->ret) || MONO_TYPE_IS_PRIMITIVE (fsig->ret) || MONO_TYPE_IS_REFERENCE (fsig->ret) || MONO_TYPE_ISSTRUCT (fsig->ret) || m_class_is_enumtype (mono_class_from_mono_type_internal (fsig->ret)) || mini_is_gsharedvt_type (fsig->ret));
-	if (supported) {
-		if (fsig->param_count == 0 || (!fsig->hasthis && fsig->param_count == 1)) {
-			supported = TRUE;
-		} else {
-			supported = TRUE;
-			for (int i = 0; i < fsig->param_count; ++i) {
-				if (!(m_type_is_byref (fsig->params [i]) || MONO_TYPE_IS_PRIMITIVE (fsig->params [i]) || MONO_TYPE_IS_REFERENCE (fsig->params [i]) || MONO_TYPE_ISSTRUCT (fsig->params [i]) || mini_is_gsharedvt_type (fsig->params [i])))
-					supported = FALSE;
-			}
-		}
-	}
-	if (!supported)
-		GSHAREDVT_FAILURE (CEE_CALLVIRT);
-	/* rgctx entry containing precomputed data */
-	context_used = mono_method_check_context_used (cmethod) | mono_class_check_context_used (constrained_class);
-	info = (MonoJumpInfoVirtMethod *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoJumpInfoVirtMethod));
-	info->klass = constrained_class;
-	info->method = cmethod;
-	entry = mono_patch_info_rgctx_entry_new (cfg->mempool, cfg->method, context_used_is_mrgctx (cfg, context_used), MONO_PATCH_INFO_VIRT_METHOD, info, MONO_RGCTX_INFO_GSHAREDVT_CONSTRAINED_CALL_INFO);
-	call_info_ins = emit_rgctx_fetch (cfg, context_used, entry);
-	/*
-	 * Fastpath: call mono_gsharedvt_constrained_call_fast, which returns
-	 * both the boxed/unboxed etc. receiver and the address to call, then
-	 * do an indirect call.
-	 */
-	calls [0] = NULL;
-	if (fsig->hasthis && (fsig->ret->type == MONO_TYPE_VOID || MONO_TYPE_IS_PRIMITIVE (fsig->ret) || MONO_TYPE_IS_REFERENCE (fsig->ret)) && !mini_is_gsharedvt_signature (fsig)) {
-		/* Call mono_gsharedvt_constrained_call_fast (receiver, info, &new_receiver) */
-		args [0] = sp [0];
-		args [1] = call_info_ins;
-		int receiver_vreg = alloc_preg (cfg);
-		MONO_EMIT_NEW_PCONST (cfg, receiver_vreg, NULL);
-		EMIT_NEW_VARLOADA_VREG (cfg, args [2], receiver_vreg, mono_get_int_type ());
-		/* This returns the address/ftndesc to call */
-		MonoInst *code_ins = mono_emit_jit_icall (cfg, mono_gsharedvt_constrained_call_fast, args);
-		NEW_BBLOCK (cfg, end_bb);
-		NEW_BBLOCK (cfg, slowpath_bb);
-		/* If NULL, go to slowpath */
-		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, code_ins->dreg, 0);
-		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, slowpath_bb);
-		/* Change the receiver to the new receiver returned by mono_gsharedvt_constrained_call_fast () */
-		int tmp_reg = alloc_preg (cfg);
-		EMIT_NEW_UNALU (cfg, ins, OP_MOVE, tmp_reg, receiver_vreg);
-		sp [0] = ins;
-		if (cfg->llvm_only)
-			calls [0] = mini_emit_llvmonly_calli (cfg, fsig, sp, code_ins);
-		else
-			calls [0] = mini_emit_calli (cfg, fsig, sp, code_ins, NULL, NULL);
-		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-		MONO_START_BB (cfg, slowpath_bb);
-	}
-	/*
-	 * Slowpath: store the arguments to an array on the stack, then call
-	 * mono_gsharedvt_constrained_call () which computes the target method and calls it using
-	 * runtime invoke.
-	 */
-	if (fsig->hasthis)
-		args [0] = orig_receiver;
-	else
-		EMIT_NEW_PCONST (cfg, args [0], NULL);
-	args [1] = emit_get_rgctx_method (cfg, mono_method_check_context_used (cmethod), cmethod, MONO_RGCTX_INFO_METHOD);
-	args [2] = mini_emit_get_rgctx_klass (cfg, mono_class_check_context_used (constrained_class), constrained_class, MONO_RGCTX_INFO_KLASS);
-	args [3] = call_info_ins;
-	MonoInst *is_gsharedvt_ins = NULL, *args_ins = NULL;
-	/* !fsig->hasthis is for the wrapper for the Object.GetType () icall or static virtual methods */
-	if ((fsig->hasthis || m_method_is_static (cmethod)) && fsig->param_count) {
-		/* Call mono_gsharedvt_constrained_call () */
-		gboolean has_gsharedvt = FALSE;
-		for (int i = 0; i < fsig->param_count; ++i) {
-			if (mini_is_gsharedvt_type (fsig->params [i]))
-				has_gsharedvt = TRUE;
-		}
-		/* Pass an array of bools which signal whenever the corresponding argument is a gsharedvt ref type */
-		if (has_gsharedvt) {
-			MONO_INST_NEW (cfg, ins, OP_LOCALLOC_IMM);
-			ins->dreg = alloc_preg (cfg);
-			ins->inst_imm = fsig->param_count;
-			MONO_ADD_INS (cfg->cbb, ins);
-			is_gsharedvt_ins = ins;
-		} else {
-			EMIT_NEW_PCONST (cfg, is_gsharedvt_ins, 0);
-		}
-		/* Pass the arguments using a localloc-ed array using the format expected by runtime_invoke () */
-		MONO_INST_NEW (cfg, ins, OP_LOCALLOC_IMM);
-		ins->dreg = alloc_preg (cfg);
-		ins->inst_imm = fsig->param_count * sizeof (target_mgreg_t);
-		MONO_ADD_INS (cfg->cbb, ins);
-		args_ins = ins;
-		for (int i = 0; i < fsig->param_count; ++i) {
-			int addr_reg;
-			if (mini_is_gsharedvt_type (fsig->params [i])) {
-				ins = mini_emit_get_gsharedvt_info_klass (cfg, mono_class_from_mono_type_internal (fsig->params [i]), MONO_RGCTX_INFO_CLASS_BOX_TYPE);
-				MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STOREI1_MEMBASE_REG, is_gsharedvt_ins->dreg, i, ins->dreg);
-			} else if (has_gsharedvt) {
-				MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STOREI1_MEMBASE_IMM, is_gsharedvt_ins->dreg, i, 0);
-			}
-			MonoInst *arg = sp [i + fsig->hasthis];
-			if (mini_is_gsharedvt_type (fsig->params [i]) || MONO_TYPE_IS_PRIMITIVE (fsig->params [i]) || MONO_TYPE_ISSTRUCT (fsig->params [i])) {
-				EMIT_NEW_VARLOADA_VREG (cfg, ins, arg->dreg, fsig->params [i]);
-				addr_reg = ins->dreg;
-				EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, args_ins->dreg, i * sizeof (target_mgreg_t), addr_reg);
-			} else {
-				EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, args_ins->dreg, i * sizeof (target_mgreg_t), arg->dreg);
-			}
-		}
-	} else {
-		EMIT_NEW_ICONST (cfg, is_gsharedvt_ins, 0);
-		EMIT_NEW_ICONST (cfg, args_ins, 0);
-	}
-	args [4] = is_gsharedvt_ins;
-	args [5] = args_ins;
-	ins = mono_emit_jit_icall (cfg, mono_gsharedvt_constrained_call, args);
-	emit_widen = FALSE;
-	/* Unbox the return value */
-	if (mini_is_gsharedvt_type (fsig->ret)) {
-		ins = handle_unbox_gsharedvt (cfg, mono_class_from_mono_type_internal (fsig->ret), ins);
-	} else if (MONO_TYPE_IS_PRIMITIVE (fsig->ret) || MONO_TYPE_ISSTRUCT (fsig->ret) || m_class_is_enumtype (mono_class_from_mono_type_internal (fsig->ret))) {
-		MonoInst *add;
-		/* Unbox */
-		NEW_BIALU_IMM (cfg, add, OP_ADD_IMM, alloc_dreg (cfg, STACK_MP), ins->dreg, MONO_ABI_SIZEOF (MonoObject));
-		MONO_ADD_INS (cfg->cbb, add);
-		/* Load value */
-		NEW_LOAD_MEMBASE_TYPE (cfg, ins, fsig->ret, add->dreg, 0);
-		MONO_ADD_INS (cfg->cbb, ins);
-	}
-	calls [1] = ins;
-	/* Merge fastpath/slowpath */
-	if (slowpath_bb) {
-		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-		MONO_START_BB (cfg, end_bb);
-	}
-	if (calls [0] && fsig->ret->type != MONO_TYPE_VOID)
-		calls [0]->dreg = calls [1]->dreg;
-	*ref_emit_widen = emit_widen;
-	return calls [1];
- exception_exit:
-	return NULL;
-}
-static void
-mono_emit_load_got_addr (MonoCompile *cfg)
-{
-	MonoInst *getaddr, *dummy_use;
-	if (!cfg->got_var || cfg->got_var_allocated)
-		return;
-	MONO_INST_NEW (cfg, getaddr, OP_LOAD_GOTADDR);
-	getaddr->cil_code = cfg->header->code;
-	getaddr->dreg = cfg->got_var->dreg;
-	/* Add it to the start of the first bblock */
-	if (cfg->bb_entry->code) {
-		getaddr->next = cfg->bb_entry->code;
-		cfg->bb_entry->code = getaddr;
-	}
-	else
-		MONO_ADD_INS (cfg->bb_entry, getaddr);
-	cfg->got_var_allocated = TRUE;
-	/*
-	 * Add a dummy use to keep the got_var alive, since real uses might
-	 * only be generated by the back ends.
-	 * Add it to end_bblock, so the variable's lifetime covers the whole
-	 * method.
-	 * It would be better to make the usage of the got var explicit in all
-	 * cases when the backend needs it (i.e. calls, throw etc.), so this
-	 * wouldn't be needed.
-	 */
-	NEW_DUMMY_USE (cfg, dummy_use, cfg->got_var);
-	MONO_ADD_INS (cfg->bb_exit, dummy_use);
-}
-static MonoMethod*
-get_constrained_method (MonoCompile *cfg, MonoImage *image, guint32 token,
-						MonoMethod *cil_method, MonoClass *constrained_class,
-						MonoGenericContext *generic_context)
-{
-	MonoMethod *cmethod = cil_method;
-	gboolean constrained_is_generic_param =
-		m_class_get_byval_arg (constrained_class)->type == MONO_TYPE_VAR ||
-		m_class_get_byval_arg (constrained_class)->type == MONO_TYPE_MVAR;
-	if (cfg->current_method->wrapper_type != MONO_WRAPPER_NONE) {
-		if (cfg->verbose_level > 2)
-			printf ("DM Constrained call to %s\n", mono_type_get_full_name (constrained_class));
-		if (!(constrained_is_generic_param &&
-			  cfg->gshared)) {
-			cmethod = mono_get_method_constrained_with_method (image, cil_method, constrained_class, generic_context, cfg->error);
-			CHECK_CFG_ERROR;
-		}
-	} else {
-		if (cfg->verbose_level > 2)
-			printf ("Constrained call to %s\n", mono_type_get_full_name (constrained_class));
-		if (constrained_is_generic_param && cfg->gshared) {
-			/*
-			 * This is needed since get_method_constrained can't find
-			 * the method in klass representing a type var.
-			 * The type var is guaranteed to be a reference type in this
-			 * case.
-			 */
-			if (!mini_is_gsharedvt_klass (constrained_class))
-				g_assert (!m_class_is_valuetype (cmethod->klass));
-		} else {
-			cmethod = mono_get_method_constrained_checked (image, token, constrained_class, generic_context, &cil_method, cfg->error);
-			CHECK_CFG_ERROR;
-		}
-	}
-	return cmethod;
- mono_error_exit:
-	return NULL;
-}
-static gboolean
-method_does_not_return (MonoMethod *method)
-{
-	return m_class_get_image (method->klass) == mono_defaults.corlib &&
-		!strcmp (m_class_get_name (method->klass), "ThrowHelper") &&
-		strstr (method->name, "Throw") == method->name &&
-		!method->is_inflated;
-}
-static int inline_limit, llvm_jit_inline_limit, llvm_aot_inline_limit;
-static gboolean inline_limit_inited;
-static gboolean
-mono_method_check_inlining (MonoCompile *cfg, MonoMethod *method)
-{
-	MonoMethodHeaderSummary header;
-	MonoVTable *vtable;
-	int limit;
-#ifdef MONO_ARCH_SOFT_FLOAT_FALLBACK
-	MonoMethodSignature *sig = mono_method_signature_internal (method);
-	int i;
-#endif
-	if (cfg->disable_inline)
-		return FALSE;
-	if (cfg->gsharedvt)
-		return FALSE;
-	if (cfg->inline_depth > 10)
-		return FALSE;
-	if (!mono_method_get_header_summary (method, &header))
-		return FALSE;
-	/*runtime, icall and pinvoke are checked by summary call*/
-	if ((method->iflags & METHOD_IMPL_ATTRIBUTE_NOINLINING) ||
-	    (method->iflags & METHOD_IMPL_ATTRIBUTE_SYNCHRONIZED) ||
-	    header.has_clauses)
-		return FALSE;
-	if (method->flags & METHOD_ATTRIBUTE_REQSECOBJ)
-		/* Used to mark methods containing StackCrawlMark locals */
-		return FALSE;
-	/* also consider num_locals? */
-	/* Do the size check early to avoid creating vtables */
-	if (!inline_limit_inited) {
-		char *inlinelimit;
-		if ((inlinelimit = g_getenv ("MONO_INLINELIMIT"))) {
-			inline_limit = atoi (inlinelimit);
-			llvm_jit_inline_limit = inline_limit;
-			llvm_aot_inline_limit = inline_limit;
-			g_free (inlinelimit);
-		} else {
-			inline_limit = INLINE_LENGTH_LIMIT;
-			llvm_jit_inline_limit = LLVM_JIT_INLINE_LENGTH_LIMIT;
-			llvm_aot_inline_limit = LLVM_AOT_INLINE_LENGTH_LIMIT;
-		}
-		inline_limit_inited = TRUE;
-	}
-	if (COMPILE_LLVM (cfg)) {
-		if (cfg->compile_aot)
-			limit = llvm_aot_inline_limit;
-		else
-			limit = llvm_jit_inline_limit;
-	} else {
-		limit = inline_limit;
-	}
-	if (header.code_size >= GINT_TO_UINT32(limit) && !(method->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING))
-		return FALSE;
-	/*
-	 * if we can initialize the class of the method right away, we do,
-	 * otherwise we don't allow inlining if the class needs initialization,
-	 * since it would mean inserting a call to mono_runtime_class_init()
-	 * inside the inlined code
-	 */
-	if (cfg->gshared && m_class_has_cctor (method->klass) && mini_class_check_context_used (cfg, method->klass))
-		return FALSE;
-	{
-		/* The AggressiveInlining hint is a good excuse to force that cctor to run. */
-		if ((cfg->opt & MONO_OPT_AGGRESSIVE_INLINING) || method->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING) {
-			if (m_class_has_cctor (method->klass)) {
-				ERROR_DECL (error);
-				vtable = mono_class_vtable_checked (method->klass, error);
-				if (!is_ok (error)) {
-					mono_error_cleanup (error);
-					return FALSE;
-				}
-				if (!cfg->compile_aot) {
-					if (!mono_runtime_class_init_full (vtable, error)) {
-						mono_error_cleanup (error);
-						return FALSE;
-					}
-				}
-			}
-		} else if (mono_class_is_before_field_init (method->klass)) {
-			if (cfg->run_cctors && m_class_has_cctor (method->klass)) {
-				ERROR_DECL (error);
-				/*FIXME it would easier and lazier to just use mono_class_try_get_vtable */
-				if (!m_class_get_runtime_vtable (method->klass))
-					/* No vtable created yet */
-					return FALSE;
-				vtable = mono_class_vtable_checked (method->klass, error);
-				if (!is_ok (error)) {
-					mono_error_cleanup (error);
-					return FALSE;
-				}
-				/* This makes so that inline cannot trigger */
-				/* .cctors: too many apps depend on them */
-				/* running with a specific order... */
-				if (! vtable->initialized)
-					return FALSE;
-				if (!mono_runtime_class_init_full (vtable, error)) {
-					mono_error_cleanup (error);
-					return FALSE;
-				}
-			}
-		} else if (mono_class_needs_cctor_run (method->klass, NULL)) {
-			ERROR_DECL (error);
-			if (!m_class_get_runtime_vtable (method->klass))
-				/* No vtable created yet */
-				return FALSE;
-			vtable = mono_class_vtable_checked (method->klass, error);
-			if (!is_ok (error)) {
-				mono_error_cleanup (error);
-				return FALSE;
-			}
-			if (!vtable->initialized)
-				return FALSE;
-		}
-	}
-#ifdef MONO_ARCH_SOFT_FLOAT_FALLBACK
-	if (mono_arch_is_soft_float ()) {
-		/* FIXME: */
-		if (sig->ret && sig->ret->type == MONO_TYPE_R4)
-			return FALSE;
-		for (i = 0; i < sig->param_count; ++i)
-			if (!m_type_is_byref (sig->params [i]) && sig->params [i]->type == MONO_TYPE_R4)
-				return FALSE;
-	}
-#endif
-	if (g_list_find (cfg->dont_inline, method))
-		return FALSE;
-	if (mono_profiler_get_call_instrumentation_flags (method))
-		return FALSE;
-	if (mono_profiler_coverage_instrumentation_enabled (method))
-		return FALSE;
-	if (method_does_not_return (method))
-		return FALSE;
-	return TRUE;
-}
-static gboolean
-mini_field_access_needs_cctor_run (MonoCompile *cfg, MonoMethod *method, MonoClass *klass, MonoVTable *vtable)
-{
-	if (!cfg->compile_aot) {
-		g_assert (vtable);
-		if (vtable->initialized)
-			return FALSE;
-	}
-	if (mono_class_is_before_field_init (klass)) {
-		if (cfg->method == method)
-			return FALSE;
-	}
-	if (!mono_class_needs_cctor_run (klass, method))
-		return FALSE;
-	if (! (method->flags & METHOD_ATTRIBUTE_STATIC) && (klass == method->klass))
-		/* The initialization is already done before the method is called */
-		return FALSE;
-	return TRUE;
-}
-int
-mini_emit_sext_index_reg (MonoCompile *cfg, MonoInst *index)
-{
-	int index_reg = index->dreg;
-	int index2_reg;
-#if SIZEOF_REGISTER == 8
-	/* The array reg is 64 bits but the index reg is only 32 */
-	if (COMPILE_LLVM (cfg)) {
-		/*
-		 * abcrem can't handle the OP_SEXT_I4, so add this after abcrem,
-		 * during OP_BOUNDS_CHECK decomposition, and in the implementation
-		 * of OP_X86_LEA for llvm.
-		 */
-		index2_reg = index_reg;
-	} else {
-		index2_reg = alloc_preg (cfg);
-		MONO_EMIT_NEW_UNALU (cfg, OP_SEXT_I4, index2_reg, index_reg);
-	}
-#else
-	if (index->type == STACK_I8) {
-		index2_reg = alloc_preg (cfg);
-		MONO_EMIT_NEW_UNALU (cfg, OP_LCONV_TO_I4, index2_reg, index_reg);
-	} else {
-		index2_reg = index_reg;
-	}
-#endif
-	return index2_reg;
-}
-MonoInst*
-mini_emit_ldelema_1_ins (MonoCompile *cfg, MonoClass *klass, MonoInst *arr, MonoInst *index, gboolean bcheck, gboolean bounded)
-{
-	MonoInst *ins;
-	guint32 size;
-	int mult_reg, add_reg, array_reg, index2_reg, bounds_reg, lower_bound_reg, realidx2_reg;
-	int context_used;
-	if (mini_is_gsharedvt_variable_klass (klass)) {
-		size = -1;
-	} else {
-		mono_class_init_internal (klass);
-		size = mono_class_array_element_size (klass);
-	}
-	mult_reg = alloc_preg (cfg);
-	array_reg = arr->dreg;
-	realidx2_reg = index2_reg = mini_emit_sext_index_reg (cfg, index);
-	if (bounded) {
-		bounds_reg = alloc_preg (cfg);
-		lower_bound_reg = alloc_preg (cfg);
-		realidx2_reg = alloc_preg (cfg);
-		MonoBasicBlock *is_null_bb = NULL;
-		NEW_BBLOCK (cfg, is_null_bb);
-		MONO_EMIT_NEW_PCONST (cfg, lower_bound_reg, NULL);
-		MONO_EMIT_NEW_LOAD_MEMBASE (cfg, bounds_reg, arr->dreg, MONO_STRUCT_OFFSET (MonoArray, bounds));
-		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, bounds_reg, 0);
-		MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, is_null_bb);
-		MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADI4_MEMBASE, lower_bound_reg, bounds_reg, MONO_STRUCT_OFFSET (MonoArrayBounds, lower_bound));
-		MONO_START_BB (cfg, is_null_bb);
-		MONO_EMIT_NEW_BIALU (cfg, OP_PSUB, realidx2_reg, index2_reg, lower_bound_reg);
-	}
-	if (bcheck)
-		MONO_EMIT_BOUNDS_CHECK (cfg, array_reg, MonoArray, max_length, realidx2_reg);
-#if defined(TARGET_X86) || defined(TARGET_AMD64)
-	if (size == 1 || size == 2 || size == 4 || size == 8) {
-		static const int fast_log2 [] = { 1, 0, 1, -1, 2, -1, -1, -1, 3 };
-		EMIT_NEW_X86_LEA (cfg, ins, array_reg, realidx2_reg, fast_log2 [size], MONO_STRUCT_OFFSET (MonoArray, vector));
-		ins->klass = klass;
-		ins->type = STACK_MP;
-		return ins;
-	}
-#endif
-	add_reg = alloc_ireg_mp (cfg);
-	if (size == -1) {
-		MonoInst *rgctx_ins;
-		/* gsharedvt */
-		g_assert (cfg->gshared);
-		context_used = mini_class_check_context_used (cfg, klass);
-		g_assert (context_used);
-		rgctx_ins = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_ARRAY_ELEMENT_SIZE);
-		MONO_EMIT_NEW_BIALU (cfg, OP_IMUL, mult_reg, realidx2_reg, rgctx_ins->dreg);
-	} else {
-		MONO_EMIT_NEW_BIALU_IMM (cfg, OP_MUL_IMM, mult_reg, realidx2_reg, size);
-	}
-	MONO_EMIT_NEW_BIALU (cfg, OP_PADD, add_reg, array_reg, mult_reg);
-	NEW_BIALU_IMM (cfg, ins, OP_PADD_IMM, add_reg, add_reg, MONO_STRUCT_OFFSET (MonoArray, vector));
-	ins->klass = klass;
-	ins->type = STACK_MP;
-	MONO_ADD_INS (cfg->cbb, ins);
-	return ins;
-}
-static MonoInst*
-mini_emit_ldelema_2_ins (MonoCompile *cfg, MonoClass *klass, MonoInst *arr, MonoInst *index_ins1, MonoInst *index_ins2)
-{
-	int bounds_reg = alloc_preg (cfg);
-	int add_reg = alloc_ireg_mp (cfg);
-	int mult_reg = alloc_preg (cfg);
-	int mult2_reg = alloc_preg (cfg);
-	int low1_reg = alloc_preg (cfg);
-	int low2_reg = alloc_preg (cfg);
-	int high1_reg = alloc_preg (cfg);
-	int high2_reg = alloc_preg (cfg);
-	int realidx1_reg = alloc_preg (cfg);
-	int realidx2_reg = alloc_preg (cfg);
-	int sum_reg = alloc_preg (cfg);
-	int index1, index2;
-	MonoInst *ins;
-	guint32 size;
-	mono_class_init_internal (klass);
-	size = mono_class_array_element_size (klass);
-	index1 = index_ins1->dreg;
-	index2 = index_ins2->dreg;
-#if SIZEOF_REGISTER == 8
-	/* The array reg is 64 bits but the index reg is only 32 */
-	if (COMPILE_LLVM (cfg)) {
-		/* Not needed */
-	} else {
-		int tmpreg = alloc_preg (cfg);
-		MONO_EMIT_NEW_UNALU (cfg, OP_SEXT_I4, tmpreg, index1);
-		index1 = tmpreg;
-		tmpreg = alloc_preg (cfg);
-		MONO_EMIT_NEW_UNALU (cfg, OP_SEXT_I4, tmpreg, index2);
-		index2 = tmpreg;
-	}
-#else
-#endif
-	/* range checking */
-	MONO_EMIT_NEW_LOAD_MEMBASE_FAULT (cfg, bounds_reg,
-				       arr->dreg, MONO_STRUCT_OFFSET (MonoArray, bounds));
-	MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADI4_MEMBASE, low1_reg,
-				       bounds_reg, MONO_STRUCT_OFFSET (MonoArrayBounds, lower_bound));
-	MONO_EMIT_NEW_BIALU (cfg, OP_PSUB, realidx1_reg, index1, low1_reg);
-	MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADI4_MEMBASE, high1_reg,
-				       bounds_reg, MONO_STRUCT_OFFSET (MonoArrayBounds, length));
-	MONO_EMIT_NEW_BIALU (cfg, OP_COMPARE, -1, high1_reg, realidx1_reg);
-	MONO_EMIT_NEW_COND_EXC (cfg, LE_UN, "IndexOutOfRangeException");
-	MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADI4_MEMBASE, low2_reg,
-				       bounds_reg, sizeof (MonoArrayBounds) + MONO_STRUCT_OFFSET (MonoArrayBounds, lower_bound));
-	MONO_EMIT_NEW_BIALU (cfg, OP_PSUB, realidx2_reg, index2, low2_reg);
-	MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADI4_MEMBASE, high2_reg,
-				       bounds_reg, sizeof (MonoArrayBounds) + MONO_STRUCT_OFFSET (MonoArrayBounds, length));
-	MONO_EMIT_NEW_BIALU (cfg, OP_COMPARE, -1, high2_reg, realidx2_reg);
-	MONO_EMIT_NEW_COND_EXC (cfg, LE_UN, "IndexOutOfRangeException");
-	MONO_EMIT_NEW_BIALU (cfg, OP_PMUL, mult_reg, high2_reg, realidx1_reg);
-	MONO_EMIT_NEW_BIALU (cfg, OP_PADD, sum_reg, mult_reg, realidx2_reg);
-	MONO_EMIT_NEW_BIALU_IMM (cfg, OP_PMUL_IMM, mult2_reg, sum_reg, size);
-	MONO_EMIT_NEW_BIALU (cfg, OP_PADD, add_reg, mult2_reg, arr->dreg);
-	NEW_BIALU_IMM (cfg, ins, OP_PADD_IMM, add_reg, add_reg, MONO_STRUCT_OFFSET (MonoArray, vector));
-	ins->type = STACK_MP;
-	ins->klass = klass;
-	MONO_ADD_INS (cfg->cbb, ins);
-	return ins;
-}
-static MonoInst*
-mini_emit_ldelema_ins (MonoCompile *cfg, MonoMethod *cmethod, MonoInst **sp, guchar *ip, gboolean is_set)
-{
-	int rank;
-	MonoInst *addr;
-	MonoMethod *addr_method;
-	int element_size;
-	MonoClass *eclass = m_class_get_element_class (cmethod->klass);
-	gboolean bounded = m_class_get_byval_arg (cmethod->klass) ? m_class_get_byval_arg (cmethod->klass)->type == MONO_TYPE_ARRAY : FALSE;
-	rank = mono_method_signature_internal (cmethod)->param_count - (is_set? 1: 0);
-	if (rank == 1)
-		return mini_emit_ldelema_1_ins (cfg, eclass, sp [0], sp [1], TRUE, bounded);
-	/* emit_ldelema_2 depends on OP_LMUL */
-	if (!cfg->backend->emulate_mul_div && rank == 2 && (cfg->opt & MONO_OPT_INTRINS) && !mini_is_gsharedvt_variable_klass (eclass)) {
-		return mini_emit_ldelema_2_ins (cfg, eclass, sp [0], sp [1], sp [2]);
-	}
-	if (mini_is_gsharedvt_variable_klass (eclass))
-		element_size = 0;
-	else
-		element_size = mono_class_array_element_size (eclass);
-	addr_method = mono_marshal_get_array_address (rank, element_size);
-	addr = mono_emit_method_call (cfg, addr_method, sp, NULL);
-	return addr;
-}
-static gboolean
-mini_class_is_reference (MonoClass *klass)
-{
-	return mini_type_is_reference (m_class_get_byval_arg (klass));
-}
-MonoInst*
-mini_emit_array_store (MonoCompile *cfg, MonoClass *klass, MonoInst **sp, gboolean safety_checks)
-{
-	if (safety_checks && mini_class_is_reference (klass) &&
-		!(MONO_INS_IS_PCONST_NULL (sp [2]))) {
-		MonoClass *obj_array = mono_array_class_get_cached (mono_defaults.object_class);
-		MonoMethod *helper;
-		MonoInst *iargs [3];
-		if (sp [0]->type != STACK_OBJ)
-			return NULL;
-		if (sp [2]->type != STACK_OBJ)
-			return NULL;
-		iargs [2] = sp [2];
-		iargs [1] = sp [1];
-		iargs [0] = sp [0];
-		MonoClass *array_class = sp [0]->klass;
-		if (array_class && m_class_get_rank (array_class) == 1) {
-			MonoClass *eclass = m_class_get_element_class (array_class);
-			if (m_class_is_sealed (eclass)) {
-				helper = mono_marshal_get_virtual_stelemref (array_class);
-				/* Make a non-virtual call if possible */
-				return mono_emit_method_call (cfg, helper, iargs, NULL);
-			}
-		}
-		helper = mono_marshal_get_virtual_stelemref (obj_array);
-		if (!helper->slot)
-			mono_class_setup_vtable (obj_array);
-		g_assert (helper->slot);
-		return mono_emit_method_call (cfg, helper, iargs, sp [0]);
-	} else {
-		MonoInst *ins;
-		if (mini_is_gsharedvt_variable_klass (klass)) {
-			MonoInst *addr;
-			addr = mini_emit_ldelema_1_ins (cfg, klass, sp [0], sp [1], TRUE, FALSE);
-			EMIT_NEW_STORE_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), addr->dreg, 0, sp [2]->dreg);
-			ins->opcode = OP_STOREV_MEMBASE;
-		} else if (sp [1]->opcode == OP_ICONST) {
-			int array_reg = sp [0]->dreg;
-			int index_reg = sp [1]->dreg;
-			size_t offset = (mono_class_array_element_size (klass) * sp [1]->inst_c0) + MONO_STRUCT_OFFSET (MonoArray, vector);
-			if (SIZEOF_REGISTER == 8 && COMPILE_LLVM (cfg) && sp [1]->inst_c0 < 0)
-				MONO_EMIT_NEW_UNALU (cfg, OP_ZEXT_I4, index_reg, index_reg);
-			if (safety_checks)
-				MONO_EMIT_BOUNDS_CHECK (cfg, array_reg, MonoArray, max_length, index_reg);
-			EMIT_NEW_STORE_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), array_reg, (target_mgreg_t)offset, sp [2]->dreg);
-		} else {
-			MonoInst *addr = mini_emit_ldelema_1_ins (cfg, klass, sp [0], sp [1], safety_checks, FALSE);
-			if (!mini_debug_options.weak_memory_model && mini_class_is_reference (klass))
-				mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
-			EMIT_NEW_STORE_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), addr->dreg, 0, sp [2]->dreg);
-			if (mini_class_is_reference (klass))
-				mini_emit_write_barrier (cfg, addr, sp [2]);
-		}
-		return ins;
-	}
-}
-MonoInst*
-mini_emit_memory_barrier (MonoCompile *cfg, int kind)
-{
-	MonoInst *ins = NULL;
-	MONO_INST_NEW (cfg, ins, OP_MEMORY_BARRIER);
-	MONO_ADD_INS (cfg->cbb, ins);
-	ins->backend.memory_barrier_kind = kind;
-	return ins;
-}
-/*
- * This entry point could be used later for arbitrary method
- * redirection.
- */
-inline static MonoInst*
-mini_redirect_call (MonoCompile *cfg, MonoMethod *method,
-					MonoMethodSignature *signature, MonoInst **args, MonoInst *this_ins)
-{
-	if (method->klass == mono_defaults.string_class) {
-		/* managed string allocation support */
-		if (strcmp (method->name, "FastAllocateString") == 0) {
-			MonoInst *iargs [2];
-			MonoVTable *vtable = mono_class_vtable_checked (method->klass, cfg->error);
-			MonoMethod *managed_alloc = NULL;
-			mono_error_assert_ok (cfg->error); /*Should not fail since it System.String*/
-#ifndef MONO_CROSS_COMPILE
-			managed_alloc = mono_gc_get_managed_allocator (method->klass, FALSE, FALSE);
-#endif
-			if (!managed_alloc)
-				return NULL;
-			EMIT_NEW_VTABLECONST (cfg, iargs [0], vtable);
-			iargs [1] = args [0];
-			return mono_emit_method_call (cfg, managed_alloc, iargs, this_ins);
-		}
-	}
-	return NULL;
-}
-static void
-mono_save_args (MonoCompile *cfg, MonoMethodSignature *sig, MonoInst **sp)
-{
-	MonoInst *store, *temp;
-	for (guint i = 0; i < sig->param_count + sig->hasthis; ++i) {
-		MonoType *argtype = (sig->hasthis && (i == 0)) ? type_from_stack_type (*sp) : sig->params [i - sig->hasthis];
-		/*
-		 * FIXME: We should use *args++ = sp [0], but that would mean the arg
-		 * would be different than the MonoInst's used to represent arguments, and
-		 * the ldelema implementation can't deal with that.
-		 * Solution: When ldelema is used on an inline argument, create a var for
-		 * it, emit ldelema on that var, and emit the saving code below in
-		 * inline_method () if needed.
-		 */
-		temp = mono_compile_create_var (cfg, argtype, OP_LOCAL);
-		cfg->args [i] = temp;
-		/* This uses cfg->args [i] which is set by the preceding line */
-		EMIT_NEW_ARGSTORE (cfg, store, i, *sp);
-		store->cil_code = sp [0]->cil_code;
-		sp++;
-	}
-}
-#define MONO_INLINE_CALLED_LIMITED_METHODS 1
-#define MONO_INLINE_CALLER_LIMITED_METHODS 1
-#if (MONO_INLINE_CALLED_LIMITED_METHODS)
-static gboolean
-check_inline_called_method_name_limit (MonoMethod *called_method)
-{
-	int strncmp_result;
-	static const char *limit = NULL;
-	if (limit == NULL) {
-		const char *limit_string = g_getenv ("MONO_INLINE_CALLED_METHOD_NAME_LIMIT");
-		if (limit_string != NULL)
-			limit = limit_string;
-		else
-			limit = "";
-	}
-	if (limit [0] != '\0') {
-		char *called_method_name = mono_method_full_name (called_method, TRUE);
-		strncmp_result = strncmp (called_method_name, limit, strlen (limit));
-		g_free (called_method_name);
-		return (strncmp_result == 0);
-	} else {
-		return TRUE;
-	}
-}
-#endif
-#if (MONO_INLINE_CALLER_LIMITED_METHODS)
-static gboolean
-check_inline_caller_method_name_limit (MonoMethod *caller_method)
-{
-	int strncmp_result;
-	static const char *limit = NULL;
-	if (limit == NULL) {
-		const char *limit_string = g_getenv ("MONO_INLINE_CALLER_METHOD_NAME_LIMIT");
-		if (limit_string != NULL) {
-			limit = limit_string;
-		} else {
-			limit = "";
-		}
-	}
-	if (limit [0] != '\0') {
-		char *caller_method_name = mono_method_full_name (caller_method, TRUE);
-		strncmp_result = strncmp (caller_method_name, limit, strlen (limit));
-		g_free (caller_method_name);
-		return (strncmp_result == 0);
-	} else {
-		return TRUE;
-	}
-}
-#endif
-void
-mini_emit_init_rvar (MonoCompile *cfg, int dreg, MonoType *rtype)
-{
-	static double r8_0 = 0.0;
-	static float r4_0 = 0.0;
-	MonoInst *ins;
-	int t;
-	rtype = mini_get_underlying_type (rtype);
-	t = rtype->type;
-	if (m_type_is_byref (rtype)) {
-		MONO_EMIT_NEW_PCONST (cfg, dreg, NULL);
-	} else if (t >= MONO_TYPE_BOOLEAN && t <= MONO_TYPE_U4) {
-		MONO_EMIT_NEW_ICONST (cfg, dreg, 0);
-	} else if (t == MONO_TYPE_I8 || t == MONO_TYPE_U8) {
-		MONO_EMIT_NEW_I8CONST (cfg, dreg, 0);
-	} else if (cfg->r4fp && t == MONO_TYPE_R4) {
-		MONO_INST_NEW (cfg, ins, OP_R4CONST);
-		ins->type = STACK_R4;
-		ins->inst_p0 = (void*)&r4_0;
-		ins->dreg = dreg;
-		MONO_ADD_INS (cfg->cbb, ins);
-	} else if (t == MONO_TYPE_R4 || t == MONO_TYPE_R8) {
-		MONO_INST_NEW (cfg, ins, OP_R8CONST);
-		ins->type = STACK_R8;
-		ins->inst_p0 = (void*)&r8_0;
-		ins->dreg = dreg;
-		MONO_ADD_INS (cfg->cbb, ins);
-	} else if ((t == MONO_TYPE_VALUETYPE) || (t == MONO_TYPE_TYPEDBYREF) ||
-		   ((t == MONO_TYPE_GENERICINST) && mono_type_generic_inst_is_valuetype (rtype))) {
-		MONO_EMIT_NEW_VZERO (cfg, dreg, mono_class_from_mono_type_internal (rtype));
-	} else if (((t == MONO_TYPE_VAR) || (t == MONO_TYPE_MVAR)) && mini_type_var_is_vt (rtype)) {
-		MONO_EMIT_NEW_VZERO (cfg, dreg, mono_class_from_mono_type_internal (rtype));
-	} else {
-		MONO_EMIT_NEW_PCONST (cfg, dreg, NULL);
-	}
-}
-static void
-emit_dummy_init_rvar (MonoCompile *cfg, int dreg, MonoType *rtype)
-{
-	int t;
-	rtype = mini_get_underlying_type (rtype);
-	t = rtype->type;
-	if (m_type_is_byref (rtype)) {
-		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_PCONST);
-	} else if (t >= MONO_TYPE_BOOLEAN && t <= MONO_TYPE_U4) {
-		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_ICONST);
-	} else if (t == MONO_TYPE_I8 || t == MONO_TYPE_U8) {
-		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_I8CONST);
-	} else if (cfg->r4fp && t == MONO_TYPE_R4) {
-		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_R4CONST);
-	} else if (t == MONO_TYPE_R4 || t == MONO_TYPE_R8) {
-		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_R8CONST);
-	} else if ((t == MONO_TYPE_VALUETYPE) || (t == MONO_TYPE_TYPEDBYREF) ||
-		   ((t == MONO_TYPE_GENERICINST) && mono_type_generic_inst_is_valuetype (rtype))) {
-		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_VZERO);
-	} else if (((t == MONO_TYPE_VAR) || (t == MONO_TYPE_MVAR)) && mini_type_var_is_vt (rtype)) {
-		MONO_EMIT_NEW_DUMMY_INIT (cfg, dreg, OP_DUMMY_VZERO);
-	} else {
-		mini_emit_init_rvar (cfg, dreg, rtype);
-	}
-}
-/* If INIT is FALSE, emit dummy initialization statements to keep the IR valid */
-static void
-emit_init_local (MonoCompile *cfg, int local, MonoType *type, gboolean init)
-{
-	MonoInst *var = cfg->locals [local];
-	if (COMPILE_SOFT_FLOAT (cfg)) {
-		MonoInst *store;
-		int reg = alloc_dreg (cfg, (MonoStackType)var->type);
-		mini_emit_init_rvar (cfg, reg, type);
-		EMIT_NEW_LOCSTORE (cfg, store, local, cfg->cbb->last_ins);
-	} else {
-		if (init)
-			mini_emit_init_rvar (cfg, var->dreg, type);
-		else
-			emit_dummy_init_rvar (cfg, var->dreg, type);
-	}
-}
-int
-mini_inline_method (MonoCompile *cfg, MonoMethod *cmethod, MonoMethodSignature *fsig, MonoInst **sp, guchar *ip, guint real_offset, gboolean inline_always)
-{
-	return inline_method (cfg, cmethod, fsig, sp, ip, real_offset, inline_always, NULL);
-}
-/*
- * inline_method:
- *
- * Return the cost of inlining CMETHOD, or zero if it should not be inlined.
- */
-static int
-inline_method (MonoCompile *cfg, MonoMethod *cmethod, MonoMethodSignature *fsig, MonoInst **sp,
-			   guchar *ip, guint real_offset, gboolean inline_always, gboolean *is_empty)
-{
-	ERROR_DECL (error);
-	MonoInst *ins, *rvar = NULL;
-	MonoMethodHeader *cheader;
-	MonoBasicBlock *ebblock, *sbblock;
-	int i, costs;
-	MonoInst **prev_locals, **prev_args;
-	MonoType **prev_arg_types;
-	guint prev_real_offset;
-	GHashTable *prev_cbb_hash;
-	MonoBasicBlock **prev_cil_offset_to_bb;
-	MonoBasicBlock *prev_cbb;
-	const guchar *prev_ip;
-	guchar *prev_cil_start;
-	guint32 prev_cil_offset_to_bb_len;
-	MonoMethod *prev_current_method;
-	MonoGenericContext *prev_generic_context;
-	gboolean ret_var_set, prev_ret_var_set, prev_disable_inline, virtual_ = FALSE;
-	g_assert (cfg->exception_type == MONO_EXCEPTION_NONE);
-#if (MONO_INLINE_CALLED_LIMITED_METHODS)
-	if ((! inline_always) && ! check_inline_called_method_name_limit (cmethod))
-		return 0;
-#endif
-#if (MONO_INLINE_CALLER_LIMITED_METHODS)
-	if ((! inline_always) && ! check_inline_caller_method_name_limit (cfg->method))
-		return 0;
-#endif
-	if (!fsig)
-		fsig = mono_method_signature_internal (cmethod);
-	if (cfg->verbose_level > 2)
-		printf ("INLINE START %p %s -> %s\n", cmethod,  mono_method_full_name (cfg->method, TRUE), mono_method_full_name (cmethod, TRUE));
-	if (!cmethod->inline_info) {
-		cfg->stat_inlineable_methods++;
-		cmethod->inline_info = 1;
-	}
-	if (is_empty)
-		*is_empty = FALSE;
-	/* allocate local variables */
-	cheader = mono_method_get_header_checked (cmethod, error);
-	if (!cheader) {
-		if (inline_always) {
-			mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
-			mono_error_move (cfg->error, error);
-		} else {
-			mono_error_cleanup (error);
-		}
-		return 0;
-	}
-	if (is_empty && cheader->code_size == 1 && cheader->code [0] == CEE_RET)
-		*is_empty = TRUE;
-	/* allocate space to store the return value */
-	if (!MONO_TYPE_IS_VOID (fsig->ret)) {
-		rvar = mono_compile_create_var (cfg, fsig->ret, OP_LOCAL);
-	}
-	prev_locals = cfg->locals;
-	cfg->locals = (MonoInst **)mono_mempool_alloc0 (cfg->mempool, cheader->num_locals * sizeof (MonoInst*));
-	for (i = 0; i < cheader->num_locals; ++i)
-		cfg->locals [i] = mono_compile_create_var (cfg, cheader->locals [i], OP_LOCAL);
-	/* allocate start and end blocks */
-	/* This is needed so if the inline is aborted, we can clean up */
-	NEW_BBLOCK (cfg, sbblock);
-	sbblock->real_offset = real_offset;
-	NEW_BBLOCK (cfg, ebblock);
-	ebblock->block_num = cfg->num_bblocks++;
-	ebblock->real_offset = real_offset;
-	prev_args = cfg->args;
-	prev_arg_types = cfg->arg_types;
-	prev_ret_var_set = cfg->ret_var_set;
-	prev_real_offset = cfg->real_offset;
-	prev_cbb_hash = cfg->cbb_hash;
-	prev_cil_offset_to_bb = cfg->cil_offset_to_bb;
-	prev_cil_offset_to_bb_len = cfg->cil_offset_to_bb_len;
-	prev_cil_start = cfg->cil_start;
-	prev_ip = cfg->ip;
-	prev_cbb = cfg->cbb;
-	prev_current_method = cfg->current_method;
-	prev_generic_context = cfg->generic_context;
-	prev_disable_inline = cfg->disable_inline;
-	cfg->ret_var_set = FALSE;
-	cfg->inline_depth ++;
-	if (ip && *ip == CEE_CALLVIRT && !(cmethod->flags & METHOD_ATTRIBUTE_STATIC))
-		virtual_ = TRUE;
-	costs = mono_method_to_ir (cfg, cmethod, sbblock, ebblock, rvar, sp, real_offset, virtual_);
-	ret_var_set = cfg->ret_var_set;
-	cfg->real_offset = prev_real_offset;
-	cfg->cbb_hash = prev_cbb_hash;
-	cfg->cil_offset_to_bb = prev_cil_offset_to_bb;
-	cfg->cil_offset_to_bb_len = prev_cil_offset_to_bb_len;
-	cfg->cil_start = prev_cil_start;
-	cfg->ip = prev_ip;
-	cfg->locals = prev_locals;
-	cfg->args = prev_args;
-	cfg->arg_types = prev_arg_types;
-	cfg->current_method = prev_current_method;
-	cfg->generic_context = prev_generic_context;
-	cfg->ret_var_set = prev_ret_var_set;
-	cfg->disable_inline = prev_disable_inline;
-	cfg->inline_depth --;
-	if ((costs >= 0 && costs < 60) || inline_always || (costs >= 0 && (cmethod->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING))) {
-		if (cfg->verbose_level > 2)
-			printf ("INLINE END %s -> %s\n", mono_method_full_name (cfg->method, TRUE), mono_method_full_name (cmethod, TRUE));
-		mono_error_assert_ok (cfg->error);
-		cfg->stat_inlined_methods++;
-		/* always add some code to avoid block split failures */
-		MONO_INST_NEW (cfg, ins, OP_NOP);
-		MONO_ADD_INS (prev_cbb, ins);
-		prev_cbb->next_bb = sbblock;
-		link_bblock (cfg, prev_cbb, sbblock);
-		/*
-		 * Get rid of the begin and end bblocks if possible to aid local
-		 * optimizations.
-		 */
-		if (prev_cbb->out_count == 1)
-			mono_merge_basic_blocks (cfg, prev_cbb, sbblock);
-		if ((prev_cbb->out_count == 1) && (prev_cbb->out_bb [0]->in_count == 1) && (prev_cbb->out_bb [0] != ebblock))
-			mono_merge_basic_blocks (cfg, prev_cbb, prev_cbb->out_bb [0]);
-		if ((ebblock->in_count == 1) && ebblock->in_bb [0]->out_count == 1) {
-			MonoBasicBlock *prev = ebblock->in_bb [0];
-			if (prev->next_bb == ebblock) {
-				mono_merge_basic_blocks (cfg, prev, ebblock);
-				cfg->cbb = prev;
-				if ((prev_cbb->out_count == 1) && (prev_cbb->out_bb [0]->in_count == 1) && (prev_cbb->out_bb [0] == prev)) {
-					mono_merge_basic_blocks (cfg, prev_cbb, prev);
-					cfg->cbb = prev_cbb;
-				}
-			} else {
-				/* There could be a bblock after 'prev', and making 'prev' the current bb could cause problems */
-				cfg->cbb = ebblock;
-			}
-		} else {
-			/*
-			 * Its possible that the rvar is set in some prev bblock, but not in others.
-			 * (#1835).
-			 */
-			if (rvar) {
-				MonoBasicBlock *bb;
-				for (i = 0; i < ebblock->in_count; ++i) {
-					bb = ebblock->in_bb [i];
-					if (bb->last_ins && bb->last_ins->opcode == OP_NOT_REACHED) {
-						cfg->cbb = bb;
-						mini_emit_init_rvar (cfg, rvar->dreg, fsig->ret);
-					}
-				}
-			}
-			cfg->cbb = ebblock;
-		}
-		if (rvar) {
-			/*
-			 * If the inlined method contains only a throw, then the ret var is not
-			 * set, so set it to a dummy value.
-			 */
-			if (!ret_var_set)
-				mini_emit_init_rvar (cfg, rvar->dreg, fsig->ret);
-			EMIT_NEW_TEMPLOAD (cfg, ins, rvar->inst_c0);
-			*sp++ = ins;
-		}
-		cfg->headers_to_free = g_slist_prepend_mempool (cfg->mempool, cfg->headers_to_free, cheader);
-		return costs + 1;
-	} else {
-		if (cfg->verbose_level > 2) {
-			const char *msg = mono_error_get_message (cfg->error);
-			printf ("INLINE ABORTED %s (cost %d) %s\n", mono_method_full_name (cmethod, TRUE), costs, msg ? msg : "");
-		}
-		cfg->exception_type = MONO_EXCEPTION_NONE;
-		clear_cfg_error (cfg);
-		/* This gets rid of the newly added bblocks */
-		cfg->cbb = prev_cbb;
-	}
-	cfg->headers_to_free = g_slist_prepend_mempool (cfg->mempool, cfg->headers_to_free, cheader);
-	return 0;
-}
-/*
- * Some of these comments may well be out-of-date.
- * Design decisions: we do a single pass over the IL code (and we do bblock
- * splitting/merging in the few cases when it's required: a back jump to an IL
- * address that was not already seen as bblock starting point).
- * Code is validated as we go (full verification is still better left to metadata/verify.c).
- * Complex operations are decomposed in simpler ones right away. We need to let the
- * arch-specific code peek and poke inside this process somehow (except when the
- * optimizations can take advantage of the full semantic info of coarse opcodes).
- * All the opcodes of the form opcode.s are 'normalized' to opcode.
- * MonoInst->opcode initially is the IL opcode or some simplification of that
- * (OP_LOAD, OP_STORE). The arch-specific code may rearrange it to an arch-specific
- * opcode with value bigger than OP_LAST.
- * At this point the IR can be handed over to an interpreter, a dumb code generator
- * or to the optimizing code generator that will translate it to SSA form.
- *
- * Profiling directed optimizations.
- * We may compile by default with few or no optimizations and instrument the code
- * or the user may indicate what methods to optimize the most either in a config file
- * or through repeated runs where the compiler applies offline the optimizations to
- * each method and then decides if it was worth it.
- */
-#define CHECK_TYPE(ins) if (!(ins)->type) UNVERIFIED
-#define CHECK_STACK(num) if ((sp - stack_start) < (num)) UNVERIFIED
-#define CHECK_STACK_OVF() if (((sp - stack_start) + 1) > header->max_stack) UNVERIFIED
-#define CHECK_ARG(num) if ((unsigned)(num) >= (unsigned)num_args) UNVERIFIED
-#define CHECK_LOCAL(num) if ((unsigned)(num) >= (unsigned)header->num_locals) UNVERIFIED
-#define CHECK_OPSIZE(size) if ((size) < 1 || ip + (size) > end) UNVERIFIED
-#define CHECK_UNVERIFIABLE(cfg) if (cfg->unverifiable) UNVERIFIED
-#define CHECK_TYPELOAD(klass) if (!(klass) || mono_class_has_failure (klass)) TYPE_LOAD_ERROR ((klass))
-/* offset from br.s -> br like opcodes */
-#define BIG_BRANCH_OFFSET 13
-static gboolean
-ip_in_bb (MonoCompile *cfg, MonoBasicBlock *bb, const guint8* ip)
-{
-	MonoBasicBlock *b = cfg->cil_offset_to_bb [ip - cfg->cil_start];
-	return b == NULL || b == bb;
-}
-static int
-get_basic_blocks (MonoCompile *cfg, MonoMethodHeader* header, guint real_offset, guchar *start, guchar *end, guchar **pos)
-{
-	guchar *ip = start;
-	guchar *target;
-	int i;
-	guint cli_addr;
-	MonoBasicBlock *bblock;
-	const MonoOpcode *opcode;
-	while (ip < end) {
-		cli_addr = GPTRDIFF_TO_UINT (ip - start);
-		i = mono_opcode_value ((const guint8 **)&ip, end);
-		if (i < 0)
-			UNVERIFIED;
-		opcode = &mono_opcodes [i];
-		switch (opcode->argument) {
-		case MonoInlineNone:
-			ip++;
-			break;
-		case MonoInlineString:
-		case MonoInlineType:
-		case MonoInlineField:
-		case MonoInlineMethod:
-		case MonoInlineTok:
-		case MonoInlineSig:
-		case MonoShortInlineR:
-		case MonoInlineI:
-			ip += 5;
-			break;
-		case MonoInlineVar:
-			ip += 3;
-			break;
-		case MonoShortInlineVar:
-		case MonoShortInlineI:
-			ip += 2;
-			break;
-		case MonoShortInlineBrTarget:
-			target = start + cli_addr + 2 + (signed char)ip [1];
-			GET_BBLOCK (cfg, bblock, target);
-			ip += 2;
-			if (ip < end)
-				GET_BBLOCK (cfg, bblock, ip);
-			break;
-		case MonoInlineBrTarget:
-			target = start + cli_addr + 5 + (gint32)read32 (ip + 1);
-			GET_BBLOCK (cfg, bblock, target);
-			ip += 5;
-			if (ip < end)
-				GET_BBLOCK (cfg, bblock, ip);
-			break;
-		case MonoInlineSwitch: {
-			guint32 n = read32 (ip + 1);
-			guint32 j;
-			ip += 5;
-			cli_addr += 5 + 4 * n;
-			target = start + cli_addr;
-			GET_BBLOCK (cfg, bblock, target);
-			for (j = 0; j < n; ++j) {
-				target = start + cli_addr + (gint32)read32 (ip);
-				GET_BBLOCK (cfg, bblock, target);
-				ip += 4;
-			}
-			break;
-		}
-		case MonoInlineR:
-		case MonoInlineI8:
-			ip += 9;
-			break;
-		default:
-			g_assert_not_reached ();
-		}
-		if (i == CEE_THROW) {
-			guchar *bb_start = ip - 1;
-			/* Find the start of the bblock containing the throw */
-			bblock = NULL;
-			while ((bb_start >= start) && !bblock) {
-				bblock = cfg->cil_offset_to_bb [(bb_start) - start];
-				bb_start --;
-			}
-			if (bblock)
-				bblock->out_of_line = 1;
-		}
-	}
-	return 0;
-unverified:
-exception_exit:
-	*pos = ip;
-	return 1;
-}
-static MonoMethod *
-mini_get_method_allow_open (MonoMethod *m, guint32 token, MonoClass *klass, MonoGenericContext *context, MonoError *error)
-{
-	MonoMethod *method;
-	error_init (error);
-	if (m->wrapper_type != MONO_WRAPPER_NONE) {
-		method = (MonoMethod *)mono_method_get_wrapper_data (m, token);
-		if (context) {
-			method = mono_class_inflate_generic_method_checked (method, context, error);
-		}
-	} else {
-		method = mono_get_method_checked (m_class_get_image (m->klass), token, klass, context, error);
-	}
-	return method;
-}
-static MonoMethod *
-mini_get_method (MonoCompile *cfg, MonoMethod *m, guint32 token, MonoClass *klass, MonoGenericContext *context)
-{
-	ERROR_DECL (error);
-	MonoMethod *method = mini_get_method_allow_open (m, token, klass, context, cfg ? cfg->error : error);
-	if (method && cfg && !cfg->gshared && mono_class_is_open_constructed_type (m_class_get_byval_arg (method->klass))) {
-		mono_error_set_bad_image (cfg->error, m_class_get_image (cfg->method->klass), "Method with open type while not compiling gshared");
-		method = NULL;
-	}
-	if (!method && !cfg)
-		mono_error_cleanup (error); /* FIXME don't swallow the error */
-	return method;
-}
-static MonoMethodSignature*
-mini_get_signature (MonoMethod *method, guint32 token, MonoGenericContext *context, MonoError *error)
-{
-	MonoMethodSignature *fsig;
-	error_init (error);
-	if (method->wrapper_type != MONO_WRAPPER_NONE) {
-		fsig = (MonoMethodSignature *)mono_method_get_wrapper_data (method, token);
-	} else {
-		fsig = mono_metadata_parse_signature_checked (m_class_get_image (method->klass), token, error);
-		return_val_if_nok (error, NULL);
-	}
-	if (context) {
-		fsig = mono_inflate_generic_signature(fsig, context, error);
-	}
-	return fsig;
-}
-/*
- * Return the original method is a wrapper is specified. We can only access
- * the custom attributes from the original method.
- */
-static MonoMethod*
-get_original_method (MonoMethod *method)
-{
-	if (method->wrapper_type == MONO_WRAPPER_NONE)
-		return method;
-	/* native code (which is like Critical) can call any managed method XXX FIXME XXX to validate all usages */
-	if (method->wrapper_type == MONO_WRAPPER_NATIVE_TO_MANAGED)
-		return NULL;
-	/* in other cases we need to find the original method */
-	return mono_marshal_method_from_wrapper (method);
-}
-static guchar*
-il_read_op (guchar *ip, guchar *end, guchar first_byte, MonoOpcodeEnum desired_il_op)
-{
-	if (G_LIKELY (ip < end) && G_UNLIKELY (*ip == first_byte)) {
-		MonoOpcodeEnum il_op = MonoOpcodeEnum_Invalid;
-		const guchar *temp_ip = ip;
-		const int size = mono_opcode_value_and_size (&temp_ip, end, &il_op);
-		return (G_LIKELY (size > 0) && G_UNLIKELY (il_op == desired_il_op)) ? (ip + size) : NULL;
-	}
-	return NULL;
-}
-static guchar*
-il_read_op_and_token (guchar *ip, guchar *end, guchar first_byte, MonoOpcodeEnum desired_il_op, guint32 *token)
-{
-	ip = il_read_op (ip, end, first_byte, desired_il_op);
-	if (ip)
-		*token = read32 (ip - 4); // could be +1 or +2 from start
-	return ip;
-}
-static guchar*
-il_read_branch_and_target (guchar *ip, guchar *end, guchar first_byte, MonoOpcodeEnum desired_il_op, int size, guchar **target)
-{
-	ip = il_read_op (ip, end, first_byte, desired_il_op);
-	if (ip) {
-		gint32 delta = 0;
-		switch (size) {
-		case  1:
-			delta = (signed char)ip [-1];
-			break;
-		case  4:
-			delta = (gint32)read32 (ip - 4);
-			break;
-		}
-		*target = ip + delta;
-		return ip;
-	}
-	return NULL;
-}
-#define il_read_brtrue(ip, end, target) 	(il_read_branch_and_target (ip, end, CEE_BRTRUE,    MONO_CEE_BRTRUE,    4, target))
-#define il_read_brtrue_s(ip, end, target) 	(il_read_branch_and_target (ip, end, CEE_BRTRUE_S,  MONO_CEE_BRTRUE_S,  1, target))
-#define il_read_brfalse(ip, end, target) 	(il_read_branch_and_target (ip, end, CEE_BRFALSE,   MONO_CEE_BRFALSE,   4, target))
-#define il_read_brfalse_s(ip, end, target) 	(il_read_branch_and_target (ip, end, CEE_BRFALSE_S, MONO_CEE_BRFALSE_S, 1, target))
-#define il_read_dup(ip, end) 			(il_read_op 		   (ip, end, CEE_DUP, MONO_CEE_DUP))
-#define il_read_newobj(ip, end, token) 		(il_read_op_and_token 	   (ip, end, CEE_NEW_OBJ, MONO_CEE_NEWOBJ, token))
-#define il_read_ldtoken(ip, end, token) 	(il_read_op_and_token 	   (ip, end, CEE_LDTOKEN, MONO_CEE_LDTOKEN, token))
-#define il_read_call(ip, end, token) 		(il_read_op_and_token      (ip, end, CEE_CALL, MONO_CEE_CALL, token))
-#define il_read_callvirt(ip, end, token)	(il_read_op_and_token 	   (ip, end, CEE_CALLVIRT, MONO_CEE_CALLVIRT, token))
-#define il_read_initobj(ip, end, token)         (il_read_op_and_token 	   (ip, end, CEE_PREFIX1, MONO_CEE_INITOBJ, token))
-#define il_read_constrained(ip, end, token)     (il_read_op_and_token      (ip, end, CEE_PREFIX1, MONO_CEE_CONSTRAINED_, token))
-#define il_read_unbox_any(ip, end, token)     (il_read_op_and_token      (ip, end, CEE_UNBOX_ANY, MONO_CEE_UNBOX_ANY, token))
-/*
- * Check that the IL instructions at ip are the array initialization
- * sequence and return the pointer to the data and the size.
- */
-static const char*
-initialize_array_data (MonoCompile *cfg, MonoMethod *method, gboolean aot, guchar *ip,
-		guchar *end, MonoClass *klass, guint32 len, int *out_size,
-		guint32 *out_field_token, MonoOpcodeEnum *il_op, guchar **next_ip)
-{
-	/*
-	 * newarr[System.Int32]
-	 * dup
-	 * ldtoken field valuetype ...
-	 * call void class [mscorlib]System.Runtime.CompilerServices.RuntimeHelpers::InitializeArray(class [mscorlib]System.Array, valuetype [mscorlib]System.RuntimeFieldHandle)
-	 */
-	guint32 token;
-	guint32 field_token;
-	if  ((ip = il_read_dup (ip, end))
-			&& ip_in_bb (cfg, cfg->cbb, ip)
-			&& (ip = il_read_ldtoken (ip, end, &field_token))
-			&& IS_FIELD_DEF (field_token)
-			&& ip_in_bb (cfg, cfg->cbb, ip)
-			&& (ip = il_read_call (ip, end, &token))) {
-		ERROR_DECL (error);
-		guint32 rva;
-		const char *data_ptr;
-		int size = 0;
-		MonoMethod *cmethod;
-		MonoClass *dummy_class;
-		MonoClassField *field = mono_field_from_token_checked (m_class_get_image (method->klass), field_token, &dummy_class, NULL, error);
-		int dummy_align;
-		if (!field) {
-			mono_error_cleanup (error); /* FIXME don't swallow the error */
-			return NULL;
-		}
-		*out_field_token = field_token;
-		cmethod = mini_get_method (NULL, method, token, NULL, NULL);
-		if (!cmethod)
-			return NULL;
-		if (strcmp (cmethod->name, "InitializeArray") || strcmp (m_class_get_name (cmethod->klass), "RuntimeHelpers") || m_class_get_image (cmethod->klass) != mono_defaults.corlib)
-			return NULL;
-		switch (mini_get_underlying_type (m_class_get_byval_arg (klass))->type) {
-		case MONO_TYPE_I1:
-		case MONO_TYPE_U1:
-			size = 1; break;
-		/* we need to swap on big endian, so punt. Should we handle R4 and R8 as well? */
-#if TARGET_BYTE_ORDER == G_LITTLE_ENDIAN
-		case MONO_TYPE_I2:
-		case MONO_TYPE_U2:
-			size = 2; break;
-		case MONO_TYPE_I4:
-		case MONO_TYPE_U4:
-		case MONO_TYPE_R4:
-			size = 4; break;
-		case MONO_TYPE_R8:
-		case MONO_TYPE_I8:
-		case MONO_TYPE_U8:
-			size = 8; break;
-#endif
-		default:
-			return NULL;
-		}
-		size *= len;
-		if (size > mono_type_size (field->type, &dummy_align))
-		    return NULL;
-		*out_size = size;
-		/*g_print ("optimized in %s: size: %d, numelems: %d\n", method->name, size, newarr->inst_newa_len->inst_c0);*/
-		MonoImage *method_klass_image = m_class_get_image (method->klass);
-		if (!image_is_dynamic (method_klass_image)) {
-			guint32 field_index = mono_metadata_token_index (field_token);
-			mono_metadata_field_info (method_klass_image, field_index - 1, NULL, &rva, NULL);
-			data_ptr = mono_image_rva_map (method_klass_image, rva);
-			/*g_print ("field: 0x%08x, rva: %d, rva_ptr: %p\n", read32 (ip + 2), rva, data_ptr);*/
-			/* for aot code we do the lookup on load */
-			if (aot && data_ptr)
-				data_ptr = (const char *)GUINT_TO_POINTER (rva);
-		} else {
-			/*FIXME is it possible to AOT a SRE assembly not meant to be saved? */
-			g_assert (!aot);
-			data_ptr = mono_field_get_data (field);
-		}
-		if (!data_ptr)
-			return NULL;
-		*il_op = MONO_CEE_CALL;
-		*next_ip = ip;
-		return data_ptr;
-	}
-	return NULL;
-}
-static void
-set_exception_type_from_invalid_il (MonoCompile *cfg, MonoMethod *method, guchar *ip)
-{
-	ERROR_DECL (error);
-	char *method_fname = mono_method_full_name (method, TRUE);
-	char *method_code;
-	MonoMethodHeader *header = mono_method_get_header_checked (method, error);
-	if (!header) {
-		method_code = g_strdup_printf ("could not parse method body due to %s", mono_error_get_message (error));
-		mono_error_cleanup (error);
-	} else if (header->code_size == 0)
-		method_code = g_strdup ("method body is empty.");
-	else
-		method_code = mono_disasm_code_one (NULL, method, ip, NULL);
-	mono_cfg_set_exception_invalid_program (cfg, g_strdup_printf ("Invalid IL code in %s: %s\n", method_fname, method_code));
- 	g_free (method_fname);
- 	g_free (method_code);
-	cfg->headers_to_free = g_slist_prepend_mempool (cfg->mempool, cfg->headers_to_free, header);
-}
-guint32
-mono_type_to_stloc_coerce (MonoType *type)
-{
-	if (m_type_is_byref (type))
-		return 0;
-	type = mini_get_underlying_type (type);
-handle_enum:
-	switch (type->type) {
-	case MONO_TYPE_I1:
-		return OP_ICONV_TO_I1;
-	case MONO_TYPE_U1:
-		return OP_ICONV_TO_U1;
-	case MONO_TYPE_I2:
-		return OP_ICONV_TO_I2;
-	case MONO_TYPE_U2:
-		return OP_ICONV_TO_U2;
-	case MONO_TYPE_I4:
-	case MONO_TYPE_U4:
-	case MONO_TYPE_I:
-	case MONO_TYPE_U:
-	case MONO_TYPE_PTR:
-	case MONO_TYPE_FNPTR:
-	case MONO_TYPE_CLASS:
-	case MONO_TYPE_STRING:
-	case MONO_TYPE_OBJECT:
-	case MONO_TYPE_SZARRAY:
-	case MONO_TYPE_ARRAY:
-	case MONO_TYPE_I8:
-	case MONO_TYPE_U8:
-	case MONO_TYPE_R4:
-	case MONO_TYPE_R8:
-	case MONO_TYPE_TYPEDBYREF:
-	case MONO_TYPE_GENERICINST:
-		return 0;
-	case MONO_TYPE_VALUETYPE:
-		if (m_class_is_enumtype (type->data.klass)) {
-			type = mono_class_enum_basetype_internal (type->data.klass);
-			goto handle_enum;
-		}
-		return 0;
-	case MONO_TYPE_VAR:
-	case MONO_TYPE_MVAR: //TODO I believe we don't need to handle gsharedvt as there won't be match and, for example, u1 is not covariant to u32
-		return 0;
-	default:
-		g_error ("unknown type 0x%02x in mono_type_to_stloc_coerce", type->type);
-	}
-	return -1;
-}
-static void
-emit_stloc_ir (MonoCompile *cfg, MonoInst **sp, MonoMethodHeader *header, int n)
-{
-	MonoInst *ins;
-	guint32 coerce_op = mono_type_to_stloc_coerce (header->locals [n]);
-	if (coerce_op) {
-		if (cfg->cbb->last_ins == sp [0] && sp [0]->opcode == coerce_op) {
-			if (cfg->verbose_level > 2)
-				printf ("Found existing coercing is enough for stloc\n");
-		} else {
-			MONO_INST_NEW (cfg, ins, coerce_op);
-			ins->dreg = alloc_ireg (cfg);
-			ins->sreg1 = sp [0]->dreg;
-			ins->type = STACK_I4;
-			ins->klass = mono_class_from_mono_type_internal (header->locals [n]);
-			MONO_ADD_INS (cfg->cbb, ins);
-			*sp = mono_decompose_opcode (cfg, ins);
-		}
-	}
-	guint32 opcode = mono_type_to_regmove (cfg, header->locals [n]);
-	if (!cfg->deopt && (opcode == OP_MOVE) && cfg->cbb->last_ins == sp [0]  &&
-			((sp [0]->opcode == OP_ICONST) || (sp [0]->opcode == OP_I8CONST))) {
-		/* Optimize reg-reg moves away */
-		/*
-		 * Can't optimize other opcodes, since sp[0] might point to
-		 * the last ins of a decomposed opcode.
-		 */
-		sp [0]->dreg = (cfg)->locals [n]->dreg;
-	} else {
-		EMIT_NEW_LOCSTORE (cfg, ins, n, *sp);
-	}
-}
-static void
-emit_starg_ir (MonoCompile *cfg, MonoInst **sp, int n)
-{
-	MonoInst *ins;
-	guint32 coerce_op = mono_type_to_stloc_coerce (cfg->arg_types [n]);
-	if (coerce_op) {
-		if (cfg->cbb->last_ins == sp [0] && sp [0]->opcode == coerce_op) {
-			if (cfg->verbose_level > 2)
-				printf ("Found existing coercing is enough for starg\n");
-		} else {
-			MONO_INST_NEW (cfg, ins, coerce_op);
-			ins->dreg = alloc_ireg (cfg);
-			ins->sreg1 = sp [0]->dreg;
-			ins->type = STACK_I4;
-			ins->klass = mono_class_from_mono_type_internal (cfg->arg_types [n]);
-			MONO_ADD_INS (cfg->cbb, ins);
-			*sp = mono_decompose_opcode (cfg, ins);
-		}
-	}
-	EMIT_NEW_ARGSTORE (cfg, ins, n, *sp);
-}
-/*
- * ldloca inhibits many optimizations so try to get rid of it in common
- * cases.
- */
-static guchar *
-emit_optimized_ldloca_ir (MonoCompile *cfg, guchar *ip, guchar *end, int local)
-{
-	guint32 token;
-	MonoClass *klass;
-	MonoType *type;
-	guchar *start = ip;
-	if  ((ip = il_read_initobj (ip, end, &token)) && ip_in_bb (cfg, cfg->cbb, start + 1)) {
-		/* From the INITOBJ case */
-		klass = mini_get_class (cfg->current_method, token, cfg->generic_context);
-		CHECK_TYPELOAD (klass);
-		type = mini_get_underlying_type (m_class_get_byval_arg (klass));
-		emit_init_local (cfg, local, type, TRUE);
-		return ip;
-	}
- exception_exit:
-	return NULL;
-}
-static MonoInst*
-handle_call_res_devirt (MonoCompile *cfg, MonoMethod *cmethod, MonoInst *call_res)
-{
-	MonoClass *ret_klass = mini_handle_call_res_devirt (cmethod);
-	if (ret_klass) {
-		MonoInst *typed_objref;
-		MONO_INST_NEW (cfg, typed_objref, OP_TYPED_OBJREF);
-		typed_objref->type = STACK_OBJ;
-		typed_objref->dreg = alloc_ireg_ref (cfg);
-		typed_objref->sreg1 = call_res->dreg;
-		typed_objref->klass = ret_klass;
-		MONO_ADD_INS (cfg->cbb, typed_objref);
-		call_res = typed_objref;
-		/* Force decompose */
-		cfg->flags |= MONO_CFG_NEEDS_DECOMPOSE;
-		cfg->cbb->needs_decompose = TRUE;
-	}
-	return call_res;
-}
-static gboolean
-is_exception_class (MonoClass *klass)
-{
-	if (G_LIKELY (m_class_get_supertypes (klass)))
-		return mono_class_has_parent_fast (klass, mono_defaults.exception_class);
-	while (klass) {
-		if (klass == mono_defaults.exception_class)
-			return TRUE;
-		klass = m_class_get_parent (klass);
-	}
-	return FALSE;
-}
-/*
- * is_jit_optimizer_disabled:
- *
- *   Determine whenever M's assembly has a DebuggableAttribute with the
- * IsJITOptimizerDisabled flag set.
- */
-static gboolean
-is_jit_optimizer_disabled (MonoMethod *m)
-{
-	MonoAssembly *ass = m_class_get_image (m->klass)->assembly;
-	g_assert (ass);
-	if (ass->jit_optimizer_disabled_inited)
-		return ass->jit_optimizer_disabled;
-	return mono_assembly_is_jit_optimizer_disabled (ass);
-}
-gboolean
-mono_is_supported_tailcall_helper (gboolean value, const char *svalue)
-{
-	if (!value)
-		mono_tailcall_print ("%s %s\n", __func__, svalue);
-	return value;
-}
-static gboolean
-mono_is_not_supported_tailcall_helper (gboolean value, const char *svalue, MonoMethod *method, MonoMethod *cmethod)
-{
-	if (value && mono_tailcall_print_enabled ()) {
-		const char *lparen = strchr (svalue, ' ') ? "(" : "";
-		const char *rparen = *lparen ? ")" : "";
-		mono_tailcall_print ("%s %s -> %s %s%s%s:%d\n", __func__, method->name, cmethod->name, lparen, svalue, rparen, value);
-	}
-	return value;
-}
-#define IS_NOT_SUPPORTED_TAILCALL(x) (mono_is_not_supported_tailcall_helper((x), #x, method, cmethod))
-static gboolean
-is_supported_tailcall (MonoCompile *cfg, const guint8 *ip, MonoMethod *method, MonoMethod *cmethod, MonoMethodSignature *fsig,
-	gboolean virtual_, gboolean extra_arg, gboolean *ptailcall_calli)
-{
-	gboolean tailcall = TRUE;
-	gboolean tailcall_calli = TRUE;
-	if (IS_NOT_SUPPORTED_TAILCALL (virtual_ && !cfg->backend->have_op_tailcall_membase))
-		tailcall = FALSE;
-	if (IS_NOT_SUPPORTED_TAILCALL (!cfg->backend->have_op_tailcall_reg))
-		tailcall_calli = FALSE;
-	if (!tailcall && !tailcall_calli)
-		goto exit;
-	if (       IS_NOT_SUPPORTED_TAILCALL (cmethod && fsig->hasthis && m_class_is_valuetype (cmethod->klass)) // This might point to the current method's stack. Emit range check?
-		|| IS_NOT_SUPPORTED_TAILCALL (cmethod && (cmethod->flags & METHOD_ATTRIBUTE_PINVOKE_IMPL))
-		|| IS_NOT_SUPPORTED_TAILCALL (fsig->pinvoke) // i.e. if !cmethod (calli)
-		|| IS_NOT_SUPPORTED_TAILCALL (cfg->method->save_lmf)
-		|| IS_NOT_SUPPORTED_TAILCALL (!cmethod && fsig->hasthis) // FIXME could be valuetype to current frame; range check
-		|| IS_NOT_SUPPORTED_TAILCALL (cmethod && cmethod->wrapper_type && cmethod->wrapper_type != MONO_WRAPPER_DYNAMIC_METHOD)
-		|| IS_NOT_SUPPORTED_TAILCALL (extra_arg && !cfg->backend->have_volatile_non_param_register)
-		|| IS_NOT_SUPPORTED_TAILCALL (cfg->gsharedvt)
-		) {
-		tailcall_calli = FALSE;
-		tailcall = FALSE;
-		goto exit;
-	}
-	for (int i = 0; i < fsig->param_count; ++i) {
-		if (IS_NOT_SUPPORTED_TAILCALL (m_type_is_byref (fsig->params [i]) || fsig->params [i]->type == MONO_TYPE_PTR || fsig->params [i]->type == MONO_TYPE_FNPTR)) {
-			tailcall_calli = FALSE;
-			tailcall = FALSE; // These can point to the current method's stack. Emit range check?
-			goto exit;
-		}
-	}
-	MonoMethodSignature *caller_signature;
-	MonoMethodSignature *callee_signature;
-	caller_signature = mono_method_signature_internal (method);
-	callee_signature = cmethod ? mono_method_signature_internal (cmethod) : fsig;
-	g_assert (caller_signature);
-	g_assert (callee_signature);
-	if (IS_NOT_SUPPORTED_TAILCALL (mini_get_underlying_type (caller_signature->ret)->type != mini_get_underlying_type (callee_signature->ret)->type)
-		|| IS_NOT_SUPPORTED_TAILCALL (!mono_arch_tailcall_supported (cfg, caller_signature, callee_signature, virtual_))) {
-		tailcall_calli = FALSE;
-		tailcall = FALSE;
-		goto exit;
-	}
-	/* Debugging support */
-#if 0
-	if (!mono_debug_count ()) {
-		tailcall_calli = FALSE;
-		tailcall = FALSE;
-		goto exit;
-	}
-#endif
-	if (tailcall_calli && IS_NOT_SUPPORTED_TAILCALL (mini_should_check_stack_pointer (cfg)))
-		tailcall_calli = FALSE;
-exit:
-	mono_tailcall_print ("tail.%s %s -> %s tailcall:%d tailcall_calli:%d gshared:%d extra_arg:%d virtual_:%d\n",
-			mono_opcode_name (*ip), method->name, cmethod ? cmethod->name : "calli", tailcall, tailcall_calli,
-			cfg->gshared, extra_arg, virtual_);
-	*ptailcall_calli = tailcall_calli;
-	return tailcall;
-}
-/*
- * is_addressable_valuetype_load
- *
- *    Returns true if a previous load can be done without doing an extra copy, given the new instruction ip and the type of the object being loaded ldtype
- */
-static gboolean
-is_addressable_valuetype_load (MonoCompile* cfg, guint8* ip, MonoType* ldtype)
-{
-	/* Avoid loading a struct just to load one of its fields */
-	gboolean is_load_instruction = (*ip == CEE_LDFLD);
-	gboolean is_in_previous_bb = ip_in_bb(cfg, cfg->cbb, ip);
-	gboolean is_struct = MONO_TYPE_ISSTRUCT(ldtype);
-	return is_load_instruction && is_in_previous_bb && is_struct;
-}
-/*
- * check_get_virtual_method_assumptions:
- * 
- * This shadows mono_class_get_virtual_method, but instead of actually resolving
- * the virtual method, this only checks if mono_class_get_virtual_method would
- * succeed. This is in place because that function fails catastrophically in some
- * cases, bringing down the entire runtime. Returns TRUE if the function is safe 
- * to call, FALSE otherwise.
- */
-static gboolean
-check_get_virtual_method_assumptions (MonoClass* klass, MonoMethod* method)
-{
-	if (m_class_is_abstract(klass))
-		return FALSE;
-	if (((method->flags & METHOD_ATTRIBUTE_FINAL) || !(method->flags & METHOD_ATTRIBUTE_VIRTUAL)))
-		return TRUE;
-	mono_class_setup_vtable (klass);
-	if (m_class_get_vtable (klass) == NULL)
-		return FALSE;
-	if (method->slot == -1) {
-		if (method->is_inflated) {
-			if (((MonoMethodInflated*)method)->declaring->slot == -1)
-				return FALSE;
-		} else {
-			return FALSE;
-		}
-	}
-	if (method->slot != -1 && mono_class_is_interface (method->klass)) {
-		gboolean variance_used = FALSE;
-		int iface_offset = mono_class_interface_offset_with_variance (klass, method->klass, &variance_used);
-		if (iface_offset <= 0)
-			return FALSE;
-    }
-	if (method->is_inflated)
-		return FALSE;
-	return TRUE;
-}
-/*
- * try_prepare_objaddr_callvirt_optimization:
- * 
- * Determine in a load+callvirt optimization can be performed and if so,
- * resolve the callvirt target method, so that it can behave as call.
- * Returns null, if the optimization cannot be performed.
- */
-static MonoMethod*
-try_prepare_objaddr_callvirt_optimization (MonoCompile *cfg, guchar *next_ip, guchar* end, MonoMethod *method, MonoGenericContext* generic_context, MonoType *param_type)
-{
-	g_assert(param_type);
-	MonoClass *klass = mono_class_from_mono_type_internal (param_type);
-	if (cfg->compile_aot || cfg->compile_llvm || !klass || !mono_class_is_def (klass))
-		return NULL;
-	guchar* callvirt_ip;
-	guint32 callvirt_proc_token;
-	if (!(callvirt_ip = il_read_callvirt (next_ip, end, &callvirt_proc_token)) ||
-		!ip_in_bb (cfg, cfg->cbb, callvirt_ip))
-		return NULL;
-	MonoMethod* iface_method = mini_get_method (cfg, method, callvirt_proc_token, NULL, generic_context);
-	if (!iface_method ||
-		iface_method->is_generic ||
-		iface_method->dynamic || 					// Reflection.Emit-generated methods should have this flag
-		!strcmp (iface_method->name, "GetHashCode") || // the callvirt handler itself optimizes those
-		(iface_method->iflags & METHOD_IMPL_ATTRIBUTE_RUNTIME))
-		return NULL;
-	MonoMethodSignature* iface_method_sig;
-	if (!((iface_method_sig = mono_method_signature_internal (iface_method)) &&
-		iface_method_sig->hasthis && 
-		iface_method_sig->param_count == 0 && 
-		!iface_method_sig->has_type_parameters &&
-		iface_method_sig->generic_param_count == 0))
-		return NULL;
-	if (!check_get_virtual_method_assumptions (klass, iface_method))
-		return NULL;
-	ERROR_DECL (struct_method_error);
-	MonoMethod* struct_method = mono_class_get_virtual_method (klass, iface_method, struct_method_error);
-	if (is_ok (struct_method_error)) {
-		if (!struct_method || !MONO_METHOD_IS_FINAL (struct_method))
-			return NULL;
-		MonoMethodSignature* struct_method_sig = mono_method_signature_internal (struct_method);
-		if (!struct_method_sig ||
-			struct_method_sig->has_type_parameters ||
-			!mono_method_can_access_method (method, struct_method)) {
-			return NULL;
-			}
-	} else {
-		mono_error_cleanup (struct_method_error);
-		return NULL;
-	}
-	return struct_method;
-}
-/*
- * handle_ctor_call:
- *
- *   Handle calls made to ctors from NEWOBJ opcodes.
- */
-static void
-handle_ctor_call (MonoCompile *cfg, MonoMethod *cmethod, MonoMethodSignature *fsig, int context_used,
-				  MonoInst **sp, guint8 *ip, int *inline_costs)
-{
-	MonoInst *rgctx_arg = NULL, *callvirt_this_arg = NULL, *ins;
-	if (cmethod && (ins = mini_emit_inst_for_ctor (cfg, cmethod, fsig, sp))) {
-		g_assert (MONO_TYPE_IS_VOID (fsig->ret));
-		CHECK_CFG_EXCEPTION;
-		return;
-	}
-	if ((cfg->opt & MONO_OPT_INLINE) && mono_method_check_inlining (cfg, cmethod) &&
-			   !mono_class_is_subclass_of_internal (cmethod->klass, mono_defaults.exception_class, FALSE)) {
-		int costs;
-		costs = inline_method (cfg, cmethod, fsig, sp, ip, cfg->real_offset, FALSE, NULL);
-		if (costs) {
-			cfg->real_offset += 5;
-			*inline_costs += costs - 5;
-			return;
-		}
-	}
-	if (mono_class_generic_sharing_enabled (cmethod->klass) && mono_method_is_generic_sharable (cmethod, TRUE)) {
-		MonoRgctxAccess access = mini_get_rgctx_access_for_method (cmethod);
-		if (access == MONO_RGCTX_ACCESS_MRGCTX) {
-			rgctx_arg = emit_get_rgctx_method (cfg, context_used,
-												cmethod, MONO_RGCTX_INFO_METHOD_RGCTX);
-		} else {
-			g_assert (access == MONO_RGCTX_ACCESS_THIS);
-		}
-	}
-	/* Avoid virtual calls to ctors if possible */
-	if (!context_used && !rgctx_arg) {
-		if (!m_method_is_aggressive_inlining (cfg->current_method) && !m_method_is_aggressive_inlining (cmethod))
-			INLINE_FAILURE ("ctor call");
-		if (cfg->gsharedvt && mini_is_gsharedvt_signature (fsig))
-			GSHAREDVT_FAILURE(*ip);
-		mini_emit_method_call_full (cfg, cmethod, fsig, FALSE, sp, callvirt_this_arg, NULL, NULL);
-	} else if (cfg->gsharedvt && mini_is_gsharedvt_signature (fsig)) {
-		MonoInst *addr;
-		addr = emit_get_rgctx_gsharedvt_call (cfg, context_used, fsig, cmethod, MONO_RGCTX_INFO_METHOD_GSHAREDVT_OUT_TRAMPOLINE);
-		if (cfg->llvm_only) {
-			mini_emit_llvmonly_calli (cfg, fsig, sp, addr);
-		} else {
-			mini_emit_calli (cfg, fsig, sp, addr, NULL, rgctx_arg);
-		}
-	} else if (context_used &&
-			   ((!mono_method_is_generic_sharable_full (cmethod, TRUE, FALSE, FALSE) ||
-				 !mono_class_generic_sharing_enabled (cmethod->klass)) || cfg->gsharedvt)) {
-		MonoInst *cmethod_addr;
-		/* Generic calls made out of gsharedvt methods cannot be patched, so use an indirect call */
-		if (cfg->llvm_only) {
-			MonoInst *addr = emit_get_rgctx_method (cfg, context_used, cmethod,
-													MONO_RGCTX_INFO_METHOD_FTNDESC);
-			/* Need wrappers for this signature to be able to enter interpreter */
-			cfg->interp_in_signatures = g_slist_prepend_mempool (cfg->mempool, cfg->interp_in_signatures, fsig);
-			mini_emit_llvmonly_calli (cfg, fsig, sp, addr);
-		} else {
-			cmethod_addr = emit_get_rgctx_method (cfg, context_used,
-												  cmethod, MONO_RGCTX_INFO_GENERIC_METHOD_CODE);
-			mini_emit_calli (cfg, fsig, sp, cmethod_addr, NULL, rgctx_arg);
-		}
-	} else {
-		INLINE_FAILURE ("ctor call");
-		ins = mini_emit_method_call_full (cfg, cmethod, fsig, FALSE, sp,
-						  callvirt_this_arg, NULL, rgctx_arg);
-	}
- exception_exit:
-	return;
-}
-typedef struct {
-	MonoMethod *method;
-	gboolean inst_tailcall;
-} HandleCallData;
-/*
- * handle_constrained_call:
- *
- *   Handle constrained calls. Return a MonoInst* representing the call or NULL.
- * May overwrite sp [0] and modify the ref_... parameters.
- */
-static MonoInst*
-handle_constrained_call (MonoCompile *cfg, MonoMethod *cmethod, MonoMethodSignature *fsig, MonoClass *constrained_class, MonoInst **sp,
-						 HandleCallData *cdata, MonoMethod **ref_cmethod, gboolean *ref_virtual, gboolean *ref_emit_widen)
-{
-	MonoInst *ins, *addr;
-	MonoMethod *method = cdata->method;
-	gboolean constrained_partial_call = FALSE;
-	gboolean constrained_is_generic_param =
-		m_class_get_byval_arg (constrained_class)->type == MONO_TYPE_VAR ||
-		m_class_get_byval_arg (constrained_class)->type == MONO_TYPE_MVAR;
-	MonoType *gshared_constraint = NULL;
-	if (constrained_is_generic_param && cfg->gshared) {
-		if (!mini_is_gsharedvt_klass (constrained_class)) {
-			g_assert (!m_class_is_valuetype (cmethod->klass));
-			if (!mini_type_is_reference (m_class_get_byval_arg (constrained_class)))
-				constrained_partial_call = TRUE;
-			MonoType *t = m_class_get_byval_arg (constrained_class);
-			MonoGenericParam *gparam = t->data.generic_param;
-			gshared_constraint = gparam->gshared_constraint;
-		}
-	}
-	if (mini_is_gsharedvt_klass (constrained_class)) {
-		if ((cmethod->klass != mono_defaults.object_class) && m_class_is_valuetype (constrained_class) && m_class_is_valuetype (cmethod->klass)) {
-			/* The 'Own method' case below */
-		} else if (m_class_get_image (cmethod->klass) != mono_defaults.corlib && !mono_class_is_interface (cmethod->klass) && !m_class_is_valuetype (cmethod->klass)) {
-			/* 'The type parameter is instantiated as a reference type' case below. */
-		} else {
-			ins = handle_constrained_gsharedvt_call (cfg, cmethod, fsig, sp, constrained_class, ref_emit_widen);
-			CHECK_CFG_EXCEPTION;
-			g_assert (ins);
-			if (cdata->inst_tailcall) // FIXME
-				mono_tailcall_print ("missed tailcall constrained_class %s -> %s\n", method->name, cmethod->name);
-			return ins;
-		}
-	}
-	if (m_method_is_static (cmethod)) {
-		/* Call to an abstract static method, handled normally */
-		return NULL;
-	} else if (constrained_partial_call) {
-		gboolean need_box = TRUE;
-		/*
-		 * The receiver is a valuetype, but the exact type is not known at compile time. This means the
-		 * called method is not known at compile time either. The called method could end up being
-		 * one of the methods on the parent classes (object/valuetype/enum), in which case we need
-		 * to box the receiver.
-		 * A simple solution would be to box always and make a normal virtual call, but that would
-		 * be bad performance wise.
-		 */
-		if (mono_class_is_interface (cmethod->klass) && mono_class_is_ginst (cmethod->klass) &&
-		    (cmethod->flags & METHOD_ATTRIBUTE_ABSTRACT)) {
-			/*
-			 * The parent classes implement no generic interfaces, so the called method will be a vtype method, so no boxing necessary.
-			 */
-			/* If the method is not abstract, it's a default interface method, and we need to box */
-			need_box = FALSE;
-		}
-		if (gshared_constraint && MONO_TYPE_IS_PRIMITIVE (gshared_constraint) && cmethod->klass == mono_defaults.object_class &&
-			!strcmp (cmethod->name, "GetHashCode")) {
-			/*
-			 * The receiver is constrained to a primitive type or an enum with the same basetype.
-			 * Enum.GetHashCode () returns the hash code of the underlying type (see comments in Enum.cs),
-			 * so the constrained call can be replaced with a normal call to the basetype GetHashCode ()
-			 * method.
-			 */
-			MonoClass *gshared_constraint_class = mono_class_from_mono_type_internal (gshared_constraint);
-			cmethod = get_method_nofail (gshared_constraint_class, cmethod->name, 0, 0);
-			g_assert (cmethod);
-			*ref_cmethod = cmethod;
-			*ref_virtual = FALSE;
-			if (cfg->verbose_level)
-				printf (" -> %s\n", mono_method_get_full_name (cmethod));
-			return NULL;
-		}
-		if (!(cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL) && (cmethod->klass == mono_defaults.object_class || cmethod->klass == m_class_get_parent (mono_defaults.enum_class) || cmethod->klass == mono_defaults.enum_class)) {
-			/* The called method is not virtual, i.e. Object:GetType (), the receiver is a vtype, has to box */
-			EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (constrained_class), sp [0]->dreg, 0);
-			ins->klass = constrained_class;
-			sp [0] = mini_emit_box (cfg, ins, constrained_class, mono_class_check_context_used (constrained_class));
-			CHECK_CFG_EXCEPTION;
-		} else if (need_box) {
-			MonoInst *box_type;
-			MonoBasicBlock *is_ref_bb, *end_bb;
-			MonoInst *nonbox_call, *nonbox_addr;
-			/*
-			 * Determine at runtime whenever the called method is defined on object/valuetype/enum, and emit a boxing call
-			 * if needed.
-			 * FIXME: It is possible to inline the called method in a lot of cases, i.e. for T_INT,
-			 * the no-box case goes to a method in Int32, while the box case goes to a method in Enum.
-			 */
-			nonbox_addr = emit_get_rgctx_virt_method (cfg, mono_class_check_context_used (constrained_class), constrained_class, cmethod, MONO_RGCTX_INFO_VIRT_METHOD_CODE);
-			NEW_BBLOCK (cfg, is_ref_bb);
-			NEW_BBLOCK (cfg, end_bb);
-			box_type = emit_get_rgctx_virt_method (cfg, mono_class_check_context_used (constrained_class), constrained_class, cmethod, MONO_RGCTX_INFO_VIRT_METHOD_BOX_TYPE);
-			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, box_type->dreg, MONO_GSHAREDVT_BOX_TYPE_REF);
-			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBEQ, is_ref_bb);
-			/* Non-ref case */
-			if (cfg->llvm_only)
-				/* nonbox_addr is an ftndesc in this case */
-				nonbox_call = mini_emit_llvmonly_calli (cfg, fsig, sp, nonbox_addr);
-			else
-				nonbox_call = (MonoInst*)mini_emit_calli (cfg, fsig, sp, nonbox_addr, NULL, NULL);
-			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-			/* Ref case */
-			MONO_START_BB (cfg, is_ref_bb);
-			EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (constrained_class), sp [0]->dreg, 0);
-			ins->klass = constrained_class;
-			sp [0] = mini_emit_box (cfg, ins, constrained_class, mono_class_check_context_used (constrained_class));
-			CHECK_CFG_EXCEPTION;
-			if (cfg->llvm_only)
-				ins = mini_emit_llvmonly_calli (cfg, fsig, sp, nonbox_addr);
-			else
-				ins = (MonoInst*)mini_emit_calli (cfg, fsig, sp, nonbox_addr, NULL, NULL);
-			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-			MONO_START_BB (cfg, end_bb);
-			cfg->cbb = end_bb;
-			nonbox_call->dreg = ins->dreg;
-			if (cdata->inst_tailcall) // FIXME
-				mono_tailcall_print ("missed tailcall constrained_partial_need_box %s -> %s\n", method->name, cmethod->name);
-			return ins;
-		} else {
-			g_assert (mono_class_is_interface (cmethod->klass));
-			addr = emit_get_rgctx_virt_method (cfg, mono_class_check_context_used (constrained_class), constrained_class, cmethod, MONO_RGCTX_INFO_VIRT_METHOD_CODE);
-			if (cfg->llvm_only)
-				ins = mini_emit_llvmonly_calli (cfg, fsig, sp, addr);
-			else
-				ins = (MonoInst*)mini_emit_calli (cfg, fsig, sp, addr, NULL, NULL);
-			if (cdata->inst_tailcall) // FIXME
-				mono_tailcall_print ("missed tailcall constrained_partial %s -> %s\n", method->name, cmethod->name);
-			return ins;
-		}
-	} else if (!m_class_is_valuetype (constrained_class)) {
-		int dreg = alloc_ireg_ref (cfg);
-		/*
-		 * The type parameter is instantiated as a reference
-		 * type.  We have a managed pointer on the stack, so
-		 * we need to dereference it here.
-		 */
-		EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, dreg, sp [0]->dreg, 0);
-		ins->type = STACK_OBJ;
-		sp [0] = ins;
-	} else if (cmethod->klass == mono_defaults.object_class || cmethod->klass == m_class_get_parent (mono_defaults.enum_class) || cmethod->klass == mono_defaults.enum_class) {
-		/*
-		 * The type parameter is instantiated as a valuetype,
-		 * but that type doesn't override the method we're
-		 * calling, so we need to box `this'.
-		 */
-		EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (constrained_class), sp [0]->dreg, 0);
-		ins->klass = constrained_class;
-		sp [0] = mini_emit_box (cfg, ins, constrained_class, mono_class_check_context_used (constrained_class));
-		CHECK_CFG_EXCEPTION;
-	} else {
-		if (cmethod->klass != constrained_class) {
-			/* Enums/default interface methods */
-			EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (constrained_class), sp [0]->dreg, 0);
-			ins->klass = constrained_class;
-			sp [0] = mini_emit_box (cfg, ins, constrained_class, mono_class_check_context_used (constrained_class));
-			CHECK_CFG_EXCEPTION;
-		}
-		*ref_virtual = FALSE;
-	}
- exception_exit:
-	return NULL;
-}
-static void
-emit_setret (MonoCompile *cfg, MonoInst *val)
-{
-	MonoType *ret_type = mini_get_underlying_type (mono_method_signature_internal (cfg->method)->ret);
-	MonoInst *ins;
-	if (mini_type_to_stind (cfg, ret_type) == CEE_STOBJ) {
-		MonoInst *ret_addr;
-		if (!cfg->vret_addr) {
-			EMIT_NEW_VARSTORE (cfg, ins, cfg->ret, ret_type, val);
-		} else {
-			EMIT_NEW_RETLOADA (cfg, ret_addr);
-			MonoClass *ret_class = mono_class_from_mono_type_internal (ret_type);
-			if (mini_class_is_simd (cfg, ret_class))
-				EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STOREX_MEMBASE, ret_addr->dreg, 0, val->dreg);
-			else
-				EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STOREV_MEMBASE, ret_addr->dreg, 0, val->dreg);
-			ins->klass = ret_class;
-		}
-	} else {
-#ifdef MONO_ARCH_SOFT_FLOAT_FALLBACK
-		if (COMPILE_SOFT_FLOAT (cfg) && !m_type_is_byref (ret_type) && ret_type->type == MONO_TYPE_R4) {
-			MonoInst *conv;
-			MonoInst *iargs [ ] = { val };
-			conv = mono_emit_jit_icall (cfg, mono_fload_r4_arg, iargs);
-			mono_arch_emit_setret (cfg, cfg->method, conv);
-		} else {
-			mono_arch_emit_setret (cfg, cfg->method, val);
-		}
-#else
-		mono_arch_emit_setret (cfg, cfg->method, val);
-#endif
-	}
-}
-/*
- * Emit a call to enter the interpreter for methods with filter clauses.
- */
-static void
-emit_llvmonly_interp_entry (MonoCompile *cfg, MonoMethodHeader *header)
-{
-	MonoInst *ins;
-	MonoInst **iargs;
-	MonoMethodSignature *sig = mono_method_signature_internal (cfg->method);
-	MonoInst *ftndesc;
-	cfg->interp_in_signatures = g_slist_prepend_mempool (cfg->mempool, cfg->interp_in_signatures, sig);
-	/*
-	 * Emit a call to the interp entry function. We emit it here instead of the llvm backend since
-	 * calling conventions etc. are easier to handle here. The LLVM backend will only emit the
-	 * entry/exit bblocks.
-	 */
-	g_assert (cfg->cbb == cfg->bb_init);
-	if (cfg->gsharedvt && mini_is_gsharedvt_variable_signature (sig)) {
-		/*
-		 * Would have to generate a gsharedvt out wrapper which calls the interp entry wrapper, but
-		 * the gsharedvt out wrapper might not exist if the caller is also a gsharedvt method since
-		 * the concrete signature of the call might not exist in the program.
-		 * So transition directly to the interpreter without the wrappers.
-		 */
-		MonoInst *args_ins;
-		MONO_INST_NEW (cfg, ins, OP_LOCALLOC_IMM);
-		ins->dreg = alloc_preg (cfg);
-		ins->inst_imm = sig->param_count * sizeof (target_mgreg_t);
-		MONO_ADD_INS (cfg->cbb, ins);
-		args_ins = ins;
-		for (unsigned int i = 0; i < sig->hasthis + sig->param_count; ++i) {
-			MonoInst *arg_addr_ins;
-			EMIT_NEW_VARLOADA ((cfg), arg_addr_ins, cfg->args [i], cfg->arg_types [i]);
-			EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, args_ins->dreg, i * sizeof (target_mgreg_t), arg_addr_ins->dreg);
-		}
-		MonoInst *ret_var = NULL;
-		MonoInst *ret_arg_ins;
-		if (!MONO_TYPE_IS_VOID (sig->ret)) {
-			ret_var = mono_compile_create_var (cfg, sig->ret, OP_LOCAL);
-			EMIT_NEW_VARLOADA (cfg, ret_arg_ins, ret_var, sig->ret);
-		} else {
-			EMIT_NEW_PCONST (cfg, ret_arg_ins, NULL);
-		}
-		iargs = g_newa (MonoInst*, 3);
-		iargs [0] = emit_get_rgctx_method (cfg, -1, cfg->method, MONO_RGCTX_INFO_INTERP_METHOD);
-		iargs [1] = ret_arg_ins;
-		iargs [2] = args_ins;
-		mono_emit_jit_icall_id (cfg, MONO_JIT_ICALL_mini_llvmonly_interp_entry_gsharedvt, iargs);
-		if (!MONO_TYPE_IS_VOID (sig->ret))
-			EMIT_NEW_VARLOAD (cfg, ins, ret_var, sig->ret);
-		else
-			ins = NULL;
-	} else {
-		/* Obtain the interp entry function */
-		ftndesc = emit_get_rgctx_method (cfg, -1, cfg->method, MONO_RGCTX_INFO_LLVMONLY_INTERP_ENTRY);
-		/* Call it */
-		iargs = g_newa (MonoInst*, sig->param_count + 1);
-		for (unsigned int i = 0; i < sig->param_count + sig->hasthis; ++i)
-			EMIT_NEW_ARGLOAD (cfg, iargs [i], i);
-		ins = mini_emit_llvmonly_calli (cfg, sig, iargs, ftndesc);
-	}
-	/* Do a normal return */
-	if (cfg->ret) {
-		emit_setret (cfg, ins);
-		/*
-		 * Since only bb_entry/bb_exit is emitted if interp_entry_only is set,
-		 * its possible that the return value becomes an OP_PHI node whose inputs
-		 * are not emitted. Make it volatile to prevent that.
-		 */
-		cfg->ret->flags |= MONO_INST_VOLATILE;
-	}
-	MONO_INST_NEW (cfg, ins, OP_BR);
-	ins->inst_target_bb = cfg->bb_exit;
-	MONO_ADD_INS (cfg->cbb, ins);
-	link_bblock (cfg, cfg->cbb, cfg->bb_exit);
-}
-typedef union _MonoOpcodeParameter {
-	gint32 i32;
-	gint64 i64;
-	float f;
-	double d;
-	guchar *branch_target;
-} MonoOpcodeParameter;
-typedef struct _MonoOpcodeInfo {
-	guint constant : 4; // private
-	gint  pops     : 3; // public -1 means variable
-	gint  pushes   : 3; // public -1 means variable
-} MonoOpcodeInfo;
-static const MonoOpcodeInfo*
-mono_opcode_decode (guchar *ip, guint op_size, MonoOpcodeEnum il_op, MonoOpcodeParameter *parameter)
-{
-#define Push0 (0)
-#define Pop0 (0)
-#define Push1 (1)
-#define Pop1 (1)
-#define PushI (1)
-#define PopI (1)
-#define PushI8 (1)
-#define PopI8 (1)
-#define PushRef (1)
-#define PopRef (1)
-#define PushR4 (1)
-#define PopR4 (1)
-#define PushR8 (1)
-#define PopR8 (1)
-#define VarPush (-1)
-#define VarPop (-1)
-	static const MonoOpcodeInfo mono_opcode_info [ ] = {
-#define OPDEF(name, str, pops, pushes, param, param_constant, a, b, c, flow) {param_constant + 1, pops, pushes },
-#include "mono/cil/opcode.def"
-#undef OPDEF
-	};
-#undef Push0
-#undef Pop0
-#undef Push1
-#undef Pop1
-#undef PushI
-#undef PopI
-#undef PushI8
-#undef PopI8
-#undef PushRef
-#undef PopRef
-#undef PushR4
-#undef PopR4
-#undef PushR8
-#undef PopR8
-#undef VarPush
-#undef VarPop
-	gint32 delta;
-	guchar *next_ip = ip + op_size;
-	const MonoOpcodeInfo *info = &mono_opcode_info [il_op];
-	switch (mono_opcodes [il_op].argument) {
-	case MonoInlineNone:
-		parameter->i32 = (int)info->constant - 1;
-		break;
-	case MonoInlineString:
-	case MonoInlineType:
-	case MonoInlineField:
-	case MonoInlineMethod:
-	case MonoInlineTok:
-	case MonoInlineSig:
-	case MonoShortInlineR:
-	case MonoInlineI:
-		parameter->i32 = read32 (next_ip - 4);
-		break;
-	case MonoShortInlineI:
-		parameter->i32 = (signed char)next_ip [-1];
-		break;
-	case MonoInlineVar:
-		parameter->i32 = read16 (next_ip - 2);
-		break;
-	case MonoShortInlineVar:
-		parameter->i32 = next_ip [-1];
-		break;
-	case MonoInlineR:
-	case MonoInlineI8:
-		parameter->i64 = read64 (next_ip - 8);
-		break;
-	case MonoShortInlineBrTarget:
-		delta = (signed char)next_ip [-1];
-		goto branch_target;
-	case MonoInlineBrTarget:
-		delta = (gint32)read32 (next_ip - 4);
-branch_target:
-		parameter->branch_target = delta + next_ip;
-		break;
-	case MonoInlineSwitch: // complicated
-		break;
-	default:
-		g_error ("%s %d %d\n", __func__, il_op, mono_opcodes [il_op].argument);
-	}
-	return info;
-}
-/*
- * mono_method_to_ir:
- *
- * Translate the .net IL into linear IR.
- *
- * @start_bblock: if not NULL, the starting basic block, used during inlining.
- * @end_bblock: if not NULL, the ending basic block, used during inlining.
- * @return_var: if not NULL, the place where the return value is stored, used during inlining.
- * @inline_args: if not NULL, contains the arguments to the inline call
- * @inline_offset: if not zero, the real offset from the inline call, or zero otherwise.
- * @is_virtual_call: whether this method is being called as a result of a call to callvirt
- *
- * This method is used to turn ECMA IL into Mono's internal Linear IR
- * reprensetation.  It is used both for entire methods, as well as
- * inlining existing methods.  In the former case, the @start_bblock,
- * @end_bblock, @return_var, @inline_args are all set to NULL, and the
- * inline_offset is set to zero.
- *
- * Returns: the inline cost, or -1 if there was an error processing this method.
- */
-int
-mono_method_to_ir (MonoCompile *cfg, MonoMethod *method, MonoBasicBlock *start_bblock, MonoBasicBlock *end_bblock,
-		   MonoInst *return_var, MonoInst **inline_args,
-		   guint inline_offset, gboolean is_virtual_call)
-{
-	ERROR_DECL (error);
-	MonoInst *array_new_localalloc_ins = NULL;
-	MonoInst *ins, **sp, **stack_start;
-	MonoBasicBlock *tblock = NULL;
-	MonoBasicBlock *init_localsbb = NULL, *init_localsbb2 = NULL;
-	MonoSimpleBasicBlock *bb = NULL, *original_bb = NULL;
-	MonoMethod *method_definition;
-	MonoInst **arg_array;
-	MonoMethodHeader *header;
-	MonoImage *image;
-	guint32 token, ins_flag;
-	MonoClass *klass;
-	MonoClass *constrained_class = NULL;
-	gboolean save_last_error = FALSE;
-	guchar *ip, *end, *target, *err_pos;
-	MonoMethodSignature *sig;
-	MonoGenericContext *generic_context = NULL;
-	MonoGenericContainer *generic_container = NULL;
-	MonoType **param_types;
-	int n, start_new_bblock;
-	int num_calls = 0, inline_costs = 0;
-	guint num_args;
-	GSList *class_inits = NULL;
-	gboolean dont_verify, dont_verify_stloc, readonly = FALSE;
-	int context_used;
-	gboolean init_locals, seq_points, skip_dead_blocks;
-	gboolean sym_seq_points = FALSE;
-	MonoDebugMethodInfo *minfo;
-	MonoBitSet *seq_point_locs = NULL;
-	MonoBitSet *seq_point_set_locs = NULL;
-	const char *ovf_exc = NULL;
-	gboolean emitted_funccall_seq_point = FALSE;
-	gboolean detached_before_ret = FALSE;
-	gboolean ins_has_side_effect;
-	MonoMethod* cmethod_override = NULL; // this is ised in call/callvirt handler to override the method to be called (e.g. from box handler)
-	if (!cfg->disable_inline)
-		cfg->disable_inline = (method->iflags & METHOD_IMPL_ATTRIBUTE_NOOPTIMIZATION) || is_jit_optimizer_disabled (method);
-	cfg->current_method = method;
-	image = m_class_get_image (method->klass);
-	/* serialization and xdomain stuff may need access to private fields and methods */
-	dont_verify = FALSE;
- 	dont_verify |= method->wrapper_type == MONO_WRAPPER_MANAGED_TO_NATIVE; /* bug #77896 */
-	dont_verify |= method->wrapper_type == MONO_WRAPPER_COMINTEROP;
-	dont_verify |= method->wrapper_type == MONO_WRAPPER_COMINTEROP_INVOKE;
-	/* still some type unsafety issues in marshal wrappers... (unknown is PtrToStructure) */
-	dont_verify_stloc = method->wrapper_type == MONO_WRAPPER_MANAGED_TO_NATIVE;
-	dont_verify_stloc |= method->wrapper_type == MONO_WRAPPER_OTHER;
-	dont_verify_stloc |= method->wrapper_type == MONO_WRAPPER_NATIVE_TO_MANAGED;
-	dont_verify_stloc |= method->wrapper_type == MONO_WRAPPER_STELEMREF;
-	header = mono_method_get_header_checked (method, cfg->error);
-	if (!header) {
-		mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
-		goto exception_exit;
-	} else {
-		cfg->headers_to_free = g_slist_prepend_mempool (cfg->mempool, cfg->headers_to_free, header);
-	}
-	generic_container = mono_method_get_generic_container (method);
-	sig = mono_method_signature_internal (method);
-	num_args = sig->hasthis + sig->param_count;
-	ip = (guchar*)header->code;
-	cfg->cil_start = ip;
-	end = ip + header->code_size;
-	cfg->stat_cil_code_size += header->code_size;
-	seq_points = cfg->gen_seq_points && cfg->method == method;
-	if (method->wrapper_type == MONO_WRAPPER_NATIVE_TO_MANAGED) {
-		/* We could hit a seq point before attaching to the JIT (#8338) */
-		seq_points = FALSE;
-	}
-	if (method->wrapper_type == MONO_WRAPPER_OTHER)	{
-		WrapperInfo *info = mono_marshal_get_wrapper_info (method);
-		if (info->subtype == WRAPPER_SUBTYPE_INTERP_IN) {
-			/* We could hit a seq point before attaching to the JIT (#8338) */
-			seq_points = FALSE;
-		}
-	}
-	if (cfg->prof_coverage) {
-		if (cfg->compile_aot)
-			g_error ("Coverage profiling is not supported with AOT.");
-		INLINE_FAILURE ("coverage profiling");
-		cfg->coverage_info = mono_profiler_coverage_alloc (cfg->method, header->code_size);
-	}
-	if ((cfg->gen_sdb_seq_points && cfg->method == method) || cfg->prof_coverage) {
-		minfo = mono_debug_lookup_method (method);
-		if (minfo) {
-			MonoSymSeqPoint *sps;
-			int n_il_offsets;
-			mono_debug_get_seq_points (minfo, NULL, NULL, NULL, &sps, &n_il_offsets);
-			seq_point_locs = mono_bitset_mem_new (mono_mempool_alloc0 (cfg->mempool, mono_bitset_alloc_size (header->code_size, 0)), header->code_size, 0);
-			seq_point_set_locs = mono_bitset_mem_new (mono_mempool_alloc0 (cfg->mempool, mono_bitset_alloc_size (header->code_size, 0)), header->code_size, 0);
-			sym_seq_points = TRUE;
-			for (int i = 0; i < n_il_offsets; ++i) {
-				if (GINT_TO_UINT32(sps [i].il_offset) < header->code_size)
-					mono_bitset_set_fast (seq_point_locs, sps [i].il_offset);
-			}
-			g_free (sps);
-			MonoDebugMethodAsyncInfo* asyncMethod = mono_debug_lookup_method_async_debug_info (method);
-			if (asyncMethod) {
-				for (int i = 0; asyncMethod != NULL && i < asyncMethod->num_awaits; i++)
-				{
-					mono_bitset_set_fast (seq_point_locs, asyncMethod->resume_offsets[i]);
-					mono_bitset_set_fast (seq_point_locs, asyncMethod->yield_offsets[i]);
-				}
-				mono_debug_free_method_async_debug_info (asyncMethod);
-			}
-		} else if (!method->wrapper_type && !method->dynamic && mono_debug_image_has_debug_info (m_class_get_image (method->klass))) {
-			/* Methods without line number info like auto-generated property accessors */
-			seq_point_locs = mono_bitset_mem_new (mono_mempool_alloc0 (cfg->mempool, mono_bitset_alloc_size (header->code_size, 0)), header->code_size, 0);
-			seq_point_set_locs = mono_bitset_mem_new (mono_mempool_alloc0 (cfg->mempool, mono_bitset_alloc_size (header->code_size, 0)), header->code_size, 0);
-			sym_seq_points = TRUE;
-		}
-	}
-	/*
-	 * Methods without init_locals set could cause asserts in various passes
-	 * (#497220). To work around this, we emit dummy initialization opcodes
-	 * (OP_DUMMY_ICONST etc.) which generate no code. These are only supported
-	 * on some platforms.
-	 */
-	if (cfg->opt & MONO_OPT_UNSAFE)
-		init_locals = header->init_locals;
-	else
-		init_locals = TRUE;
-	method_definition = method;
-	while (method_definition->is_inflated) {
-		MonoMethodInflated *imethod = (MonoMethodInflated *) method_definition;
-		method_definition = imethod->declaring;
-	}
-	if (sig->is_inflated)
-		generic_context = mono_method_get_context (method);
-	else if (generic_container)
-		generic_context = &generic_container->context;
-	cfg->generic_context = generic_context;
-	if (!cfg->gshared) {
-		gboolean check_type_parameter = TRUE;
-		if (method->wrapper_type == MONO_WRAPPER_OTHER) {
-			WrapperInfo *info = mono_marshal_get_wrapper_info (method);
-			g_assert (info);
-			if (info->subtype == WRAPPER_SUBTYPE_UNSAFE_ACCESSOR)
-				check_type_parameter = FALSE;
-		}
-		if (check_type_parameter)
-			g_assert (!sig->has_type_parameters);
-	}
-	if (sig->generic_param_count && method->wrapper_type == MONO_WRAPPER_NONE) {
-		g_assert (method->is_inflated);
-		g_assert (mono_method_get_context (method)->method_inst);
-	}
-	if (method->is_inflated && mono_method_get_context (method)->method_inst)
-		g_assert (sig->generic_param_count);
-	if (cfg->method == method) {
-		cfg->real_offset = 0;
-	} else {
-		cfg->real_offset = inline_offset;
-	}
-	cfg->cil_offset_to_bb = (MonoBasicBlock **)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoBasicBlock*) * header->code_size);
-	cfg->cil_offset_to_bb_len = header->code_size;
-	if (cfg->verbose_level > 2)
-		printf ("method to IR %s\n", mono_method_full_name (method, TRUE));
-	param_types = (MonoType **)mono_mempool_alloc (cfg->mempool, sizeof (MonoType*) * num_args);
-	if (sig->hasthis)
-		param_types [0] = m_class_is_valuetype (method->klass) ? m_class_get_this_arg (method->klass) : m_class_get_byval_arg (method->klass);
-	for (n = 0; n < sig->param_count; ++n)
-		param_types [n + sig->hasthis] = sig->params [n];
-	cfg->arg_types = param_types;
-	cfg->dont_inline = g_list_prepend (cfg->dont_inline, method);
-	if (cfg->method == method) {
-		/* ENTRY BLOCK */
-		NEW_BBLOCK (cfg, start_bblock);
-		cfg->bb_entry = start_bblock;
-		start_bblock->cil_code = NULL;
-		start_bblock->cil_length = 0;
-		/* EXIT BLOCK */
-		NEW_BBLOCK (cfg, end_bblock);
-		cfg->bb_exit = end_bblock;
-		end_bblock->cil_code = NULL;
-		end_bblock->cil_length = 0;
-		end_bblock->flags |= BB_INDIRECT_JUMP_TARGET;
-		g_assert (cfg->num_bblocks == 2);
-		arg_array = cfg->args;
-		if (header->num_clauses) {
-			cfg->spvars = g_hash_table_new (NULL, NULL);
-			cfg->exvars = g_hash_table_new (NULL, NULL);
-		}
-		cfg->clause_is_dead = mono_mempool_alloc0 (cfg->mempool, sizeof (gboolean) * header->num_clauses);
-		/* handle exception clauses */
-		for (unsigned int i = 0; i < header->num_clauses; ++i) {
-			MonoBasicBlock *try_bb;
-			MonoExceptionClause *clause = &header->clauses [i];
-			GET_BBLOCK (cfg, try_bb, ip + clause->try_offset);
-			try_bb->real_offset = clause->try_offset;
-			try_bb->try_start = TRUE;
-			GET_BBLOCK (cfg, tblock, ip + clause->handler_offset);
-			tblock->real_offset = clause->handler_offset;
-			tblock->flags |= BB_EXCEPTION_HANDLER;
-			if (clause->flags == MONO_EXCEPTION_CLAUSE_FINALLY)
-				mono_create_exvar_for_offset (cfg, clause->handler_offset);
-			/*
-			 * Linking the try block with the EH block hinders inlining as we won't be able to
-			 * merge the bblocks from inlining and produce an artificial hole for no good reason.
-			 */
-			if (COMPILE_LLVM (cfg))
-				link_bblock (cfg, try_bb, tblock);
-			if (*(ip + clause->handler_offset) == CEE_POP)
-				tblock->flags |= BB_EXCEPTION_DEAD_OBJ;
-			if (clause->flags == MONO_EXCEPTION_CLAUSE_FINALLY ||
-			    clause->flags == MONO_EXCEPTION_CLAUSE_FILTER ||
-			    clause->flags == MONO_EXCEPTION_CLAUSE_FAULT) {
-				MONO_INST_NEW (cfg, ins, OP_START_HANDLER);
-				MONO_ADD_INS (tblock, ins);
-				if (seq_points && clause->flags != MONO_EXCEPTION_CLAUSE_FINALLY && clause->flags != MONO_EXCEPTION_CLAUSE_FILTER) {
-					/* finally clauses already have a seq point */
-					/* seq points for filter clauses are emitted below */
-					NEW_SEQ_POINT (cfg, ins, clause->handler_offset, TRUE);
-					MONO_ADD_INS (tblock, ins);
-				}
-				/* todo: is a fault block unsafe to optimize? */
-				if (clause->flags == MONO_EXCEPTION_CLAUSE_FAULT)
-					tblock->flags |= BB_EXCEPTION_UNSAFE;
-			}
-			/*printf ("clause try IL_%04x to IL_%04x handler %d at IL_%04x to IL_%04x\n", clause->try_offset, clause->try_offset + clause->try_len, clause->flags, clause->handler_offset, clause->handler_offset + clause->handler_len);
-			  while (p < end) {
-			  printf ("%s", mono_disasm_code_one (NULL, method, p, &p));
-			  }*/
-			/* catch and filter blocks get the exception object on the stack */
-			if (clause->flags == MONO_EXCEPTION_CLAUSE_NONE ||
-			    clause->flags == MONO_EXCEPTION_CLAUSE_FILTER) {
-				/* mostly like handle_stack_args (), but just sets the input args */
-				/* printf ("handling clause at IL_%04x\n", clause->handler_offset); */
-				tblock->in_scount = 1;
-				tblock->in_stack = (MonoInst **)mono_mempool_alloc (cfg->mempool, sizeof (MonoInst*));
-				tblock->in_stack [0] = mono_create_exvar_for_offset (cfg, clause->handler_offset);
-				cfg->cbb = tblock;
-#ifdef MONO_CONTEXT_SET_LLVM_EXC_REG
-				/* The EH code passes in the exception in a register to both JITted and LLVM compiled code */
-				if (!cfg->compile_llvm) {
-					MONO_INST_NEW (cfg, ins, OP_GET_EX_OBJ);
-					ins->dreg = tblock->in_stack [0]->dreg;
-					MONO_ADD_INS (tblock, ins);
-				}
-#else
-				MonoInst *dummy_use;
-				/*
-				 * Add a dummy use for the exvar so its liveness info will be
-				 * correct.
-				 */
-				EMIT_NEW_DUMMY_USE (cfg, dummy_use, tblock->in_stack [0]);
-#endif
-				if (seq_points && clause->flags == MONO_EXCEPTION_CLAUSE_FILTER) {
-					NEW_SEQ_POINT (cfg, ins, clause->handler_offset, TRUE);
-					MONO_ADD_INS (tblock, ins);
-				}
-				if (clause->flags == MONO_EXCEPTION_CLAUSE_FILTER) {
-					GET_BBLOCK (cfg, tblock, ip + clause->data.filter_offset);
-					tblock->flags |= BB_EXCEPTION_HANDLER;
-					tblock->real_offset = clause->data.filter_offset;
-					tblock->in_scount = 1;
-					tblock->in_stack = (MonoInst **)mono_mempool_alloc (cfg->mempool, sizeof (MonoInst*));
-					/* The filter block shares the exvar with the handler block */
-					tblock->in_stack [0] = mono_create_exvar_for_offset (cfg, clause->handler_offset);
-					MONO_INST_NEW (cfg, ins, OP_START_HANDLER);
-					MONO_ADD_INS (tblock, ins);
-				}
-			}
-			if (clause->flags != MONO_EXCEPTION_CLAUSE_FILTER &&
-					clause->data.catch_class &&
-					cfg->gshared &&
-					mono_class_check_context_used (clause->data.catch_class)) {
-				/*
-				 * In shared generic code with catch
-				 * clauses containing type variables
-				 * the exception handling code has to
-				 * be able to get to the rgctx.
-				 * Therefore we have to make sure that
-				 * the vtable/mrgctx argument (for
-				 * static or generic methods) or the
-				 * "this" argument (for non-static
-				 * methods) are live.
-				 */
-				if ((method->flags & METHOD_ATTRIBUTE_STATIC) ||
-						mini_method_get_context (method)->method_inst ||
-						m_class_is_valuetype (method->klass)) {
-					mono_get_vtable_var (cfg);
-				} else {
-					MonoInst *dummy_use;
-					EMIT_NEW_DUMMY_USE (cfg, dummy_use, arg_array [0]);
-				}
-			}
-		}
-	} else {
-		arg_array = g_newa (MonoInst*, num_args);
-		cfg->cbb = start_bblock;
-		cfg->args = arg_array;
-		mono_save_args (cfg, sig, inline_args);
-	}
-	if (cfg->method == method && cfg->self_init && cfg->compile_aot && !COMPILE_LLVM (cfg)) {
-		MonoMethod *wrapper;
-		MonoInst *args [2];
-		int idx;
-		/*
-		 * Emit code to initialize this method by calling the init wrapper emitted by LLVM.
-		 * This is not efficient right now, but its only used for the methods which fail
-		 * LLVM compilation.
-		 * FIXME: Optimize this
-		 */
-		g_assert (!cfg->gshared);
-		wrapper = mono_marshal_get_aot_init_wrapper (AOT_INIT_METHOD);
-		/* Emit this into the entry bb so it comes before the GC safe point which depends on an inited GOT */
-		cfg->cbb = cfg->bb_entry;
-		idx = mono_aot_get_method_index (cfg->method);
-		EMIT_NEW_ICONST (cfg, args [0], idx);
-		/* Dummy */
-		EMIT_NEW_ICONST (cfg, args [1], 0);
-		mono_emit_method_call (cfg, wrapper, args, NULL);
-	}
-	if (cfg->llvm_only)
-		g_assert (cfg->interp);
-	if (cfg->llvm_only && cfg->interp && cfg->method == method && !cfg->deopt && !cfg->interp_entry_only) {
-		if (header->num_clauses) {
-			for (guint i = 0; i < header->num_clauses; ++i) {
-				MonoExceptionClause *clause = &header->clauses [i];
-				/* Finally clauses are checked after the remove_finally pass */
-				if (clause->flags != MONO_EXCEPTION_CLAUSE_FINALLY)
-					cfg->interp_entry_only = TRUE;
-			}
-		}
-	}
-	/* we use a separate basic block for the initialization code */
-	NEW_BBLOCK (cfg, init_localsbb);
-	if (cfg->method == method)
-		cfg->bb_init = init_localsbb;
-	init_localsbb->real_offset = cfg->real_offset;
-	start_bblock->next_bb = init_localsbb;
-	link_bblock (cfg, start_bblock, init_localsbb);
-	init_localsbb2 = init_localsbb;
-	cfg->cbb = init_localsbb;
-	/*
-	 * If the method receives an mrgctx, store all rgctx entries in mrgctx->entries instead of in the
-	 * class rgctx.
-	 * Disable for gsharedvt for now since the handling of gsharedvt related rgctx entries for
-	 * MONO_PATCH_INFO_GSHARED_METHOD_INFO is not implemented yet.
-	 */
-	if (cfg->gshared && cfg->method == method && cfg->rgctx_access == MONO_RGCTX_ACCESS_MRGCTX) {
-		MonoGSharedMethodInfo *info;
-		MonoInst *args [2];
-		/* Allocate into permanent memory since its the key in MonoJumpInfo */
-		info = (MonoGSharedMethodInfo *)mono_mem_manager_alloc0 (cfg->mem_manager, sizeof (MonoGSharedMethodInfo));
-		/* Will be copied into permanent memory in mini_method_compile () */
-		info->method = cfg->method;
-		info->count_entries = 16;
-		info->entries = (MonoRuntimeGenericContextInfoTemplate *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoRuntimeGenericContextInfoTemplate) * info->count_entries);
-		cfg->gshared_info = info;
-		args [0] = mono_get_mrgctx_var (cfg);
-		if (cfg->compile_aot)
-			args [1] = mini_emit_runtime_constant (cfg, MONO_PATCH_INFO_GSHARED_METHOD_INFO, info);
-		else
-			EMIT_NEW_PCONST (cfg, args [1], info);
-		cfg->init_method_rgctx_ins_arg = args [1];
-		if (COMPILE_LLVM (cfg) || cfg->backend->have_init_mrgctx) {
-			MONO_INST_NEW (cfg, ins, OP_INIT_MRGCTX);
-			ins->sreg1 = args [0]->dreg;
-			ins->sreg2 = args [1]->dreg;
-			MONO_ADD_INS (cfg->cbb, ins);
-			cfg->init_method_rgctx_ins = ins;
-		} else {
-			cfg->init_method_rgctx_ins = mono_emit_jit_icall (cfg, mini_init_method_rgctx, args);
-		}
-	}
-	if (cfg->gsharedvt && cfg->method == method) {
-		MonoGSharedVtMethodInfo *info;
-		MonoInst *var, *locals_var;
-		int dreg;
-		info = (MonoGSharedVtMethodInfo *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoGSharedVtMethodInfo));
-		info->method = cfg->method;
-		info->count_entries = 16;
-		info->entries = (MonoRuntimeGenericContextInfoTemplate *)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoRuntimeGenericContextInfoTemplate) * info->count_entries);
-		cfg->gsharedvt_info = info;
-		var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
-		/*
-		 * Decomposing ldaddr creates uses for this and gsharedvt_locals_var, so
-		 * when we emit an ldaddr, we emit dummy uses for these in handle_gsharedvt_ldaddr ().
-		 */
-		cfg->gsharedvt_info_var = var;
-		ins = emit_get_rgctx_gsharedvt_method (cfg, mini_method_check_context_used (cfg, method), method, info);
-		MONO_EMIT_NEW_UNALU (cfg, OP_MOVE, var->dreg, ins->dreg);
-		/* Allocate locals */
-		locals_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
-		/* prevent it from being register allocated */
-		cfg->gsharedvt_locals_var = locals_var;
-		dreg = alloc_ireg (cfg);
-		MONO_EMIT_NEW_LOAD_MEMBASE_OP (cfg, OP_LOADI4_MEMBASE, dreg, var->dreg, MONO_STRUCT_OFFSET (MonoGSharedVtMethodRuntimeInfo, locals_size));
-		MONO_INST_NEW (cfg, ins, OP_LOCALLOC);
-		ins->dreg = locals_var->dreg;
-		ins->sreg1 = dreg;
-		MONO_ADD_INS (cfg->cbb, ins);
-		cfg->gsharedvt_locals_var_ins = ins;
-		cfg->flags |= MONO_CFG_HAS_ALLOCA;
-		/*
-		if (init_locals)
-			ins->flags |= MONO_INST_INIT;
-		*/
-		if (cfg->llvm_only) {
-			init_localsbb = cfg->cbb;
-			init_localsbb2 = cfg->cbb;
-		}
-	}
-	if (cfg->deopt) {
-		/*
-		 * Push an LMFExt frame which points to a MonoMethodILState structure.
-		 */
-		emit_push_lmf (cfg);
-		/* The type doesn't matter, the llvm backend will use the correct type */
-		MonoInst *il_state_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
-		il_state_var->flags |= MONO_INST_VOLATILE;
-		cfg->il_state_var = il_state_var;
-		EMIT_NEW_VARLOADA (cfg, ins, cfg->il_state_var, NULL);
-		int il_state_addr_reg = ins->dreg;
-		/* il_state->method = method */
-		MonoInst *method_ins = emit_get_rgctx_method (cfg, -1, cfg->method, MONO_RGCTX_INFO_METHOD);
-		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, il_state_addr_reg, MONO_STRUCT_OFFSET (MonoMethodILState, method), method_ins->dreg);
-		EMIT_NEW_VARLOADA (cfg, ins, cfg->lmf_var, NULL);
-		int lmf_reg = ins->dreg;
-		/* lmf->kind = MONO_LMFEXT_IL_STATE */
-		MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STOREI4_MEMBASE_IMM, lmf_reg, MONO_STRUCT_OFFSET (MonoLMFExt, kind), MONO_LMFEXT_IL_STATE);
-		/* lmf->il_state = il_state */
-		MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, lmf_reg, MONO_STRUCT_OFFSET (MonoLMFExt, il_state), il_state_addr_reg);
-		/* emit_get_rgctx_method () might create new bblocks */
-		if (cfg->llvm_only) {
-			init_localsbb = cfg->cbb;
-			init_localsbb2 = cfg->cbb;
-		}
-	}
-	if (cfg->llvm_only && cfg->interp && cfg->method == method) {
-		if (cfg->interp_entry_only)
-			emit_llvmonly_interp_entry (cfg, header);
-	}
-	/* FIRST CODE BLOCK */
-	NEW_BBLOCK (cfg, tblock);
-	tblock->cil_code = ip;
-	cfg->cbb = tblock;
-	cfg->ip = ip;
-	init_localsbb->next_bb = cfg->cbb;
-	link_bblock (cfg, init_localsbb, cfg->cbb);
-	ADD_BBLOCK (cfg, tblock);
-	CHECK_CFG_EXCEPTION;
-	if (header->code_size == 0)
-		UNVERIFIED;
-	if (get_basic_blocks (cfg, header, cfg->real_offset, ip, end, &err_pos)) {
-		ip = err_pos;
-		UNVERIFIED;
-	}
-	if (cfg->method == method) {
-		int breakpoint_id = mono_debugger_method_has_breakpoint (method);
-		if (breakpoint_id) {
-			if (COMPILE_LLVM (cfg)) {
-				mono_emit_jit_icall (cfg, mono_break, NULL);
-			} else {
-				MONO_INST_NEW (cfg, ins, OP_BREAK);
-				MONO_ADD_INS (cfg->cbb, ins);
-			}
-		}
-		mono_debug_init_method (cfg, cfg->cbb, breakpoint_id);
-	}
-	for (n = 0; n < header->num_locals; ++n) {
-		if (header->locals [n]->type == MONO_TYPE_VOID && !m_type_is_byref (header->locals [n]))
-			UNVERIFIED;
-	}
-	class_inits = NULL;
-	/* We force the vtable variable here for all shared methods
-	   for the possibility that they might show up in a stack
-	   trace where their exact instantiation is needed. */
-	if (cfg->gshared && method == cfg->method) {
-		if ((method->flags & METHOD_ATTRIBUTE_STATIC) ||
-				mini_method_get_context (method)->method_inst ||
-				m_class_is_valuetype (method->klass)) {
-			mono_get_vtable_var (cfg);
-		} else {
-			/* FIXME: Is there a better way to do this?
-			   We need the variable live for the duration
-			   of the whole method. */
-			if (!COMPILE_LLVM (cfg))
-				cfg->args [0]->flags |= MONO_INST_VOLATILE;
-		}
-	}
-	/* add a check for this != NULL to inlined methods */
-	if (is_virtual_call) {
-		MonoInst *arg_ins;
-		if (!(cfg->llvm_only && m_class_is_valuetype (method->klass) && header->code_size == 1 && header->code [0] == CEE_RET)) {
-			NEW_ARGLOAD (cfg, arg_ins, 0);
-			MONO_ADD_INS (cfg->cbb, arg_ins);
-			MONO_EMIT_NEW_CHECK_THIS (cfg, arg_ins->dreg);
-		}
-	}
-	skip_dead_blocks = !dont_verify;
-	if (skip_dead_blocks) {
-		original_bb = bb = mono_basic_block_split (method, cfg->error, header);
-		CHECK_CFG_ERROR;
-		g_assert (bb);
-	}
-	if (cfg->gsharedvt_min) {
-		if (mini_is_gsharedvt_variable_signature (sig))
-			GSHAREDVT_FAILURE (*cfg->cil_start);
-		for (int i = 0; i < header->num_locals; ++i) {
-			if (mini_is_gsharedvt_variable_type (header->locals [i]))
-				GSHAREDVT_FAILURE (*cfg->cil_start);
-		}
-	}
-	/* we use a spare stack slot in SWITCH and NEWOBJ and others */
-	stack_start = sp = (MonoInst **)mono_mempool_alloc0 (cfg->mempool, sizeof (MonoInst*) * (header->max_stack + 1));
-	ins_flag = 0;
-	start_new_bblock = 0;
-	MonoOpcodeEnum il_op; il_op = MonoOpcodeEnum_Invalid;
-	emit_set_deopt_il_offset (cfg, GPTRDIFF_TO_INT (ip - cfg->cil_start));
-	for (guchar *next_ip = ip; ip < end; ip = next_ip) {
-		MonoOpcodeEnum previous_il_op = il_op;
-		const guchar *tmp_ip = ip;
-		const int op_size = mono_opcode_value_and_size (&tmp_ip, end, &il_op);
-		CHECK_OPSIZE (op_size);
-		next_ip += op_size;
-		if (cfg->method == method)
-			cfg->real_offset = GPTRDIFF_TO_UINT (ip - header->code);
-		else
-			cfg->real_offset = inline_offset;
-		cfg->ip = ip;
-		context_used = 0;
-		if (start_new_bblock) {
-			cfg->cbb->cil_length = GPTRDIFF_TO_INT32 (ip - cfg->cbb->cil_code);
-			if (start_new_bblock == 2) {
-				g_assert (ip == tblock->cil_code);
-			} else {
-				GET_BBLOCK (cfg, tblock, ip);
-			}
-			cfg->cbb->next_bb = tblock;
-			cfg->cbb = tblock;
-			start_new_bblock = 0;
-			for (int i = 0; i < cfg->cbb->in_scount; ++i) {
-				if (cfg->verbose_level > 3)
-					printf ("loading %d from temp %d\n", i, (int)cfg->cbb->in_stack [i]->inst_c0);
-				EMIT_NEW_TEMPLOAD (cfg, ins, cfg->cbb->in_stack [i]->inst_c0);
-				*sp++ = ins;
-			}
-			if (class_inits)
-				g_slist_free (class_inits);
-			class_inits = NULL;
-			emit_set_deopt_il_offset (cfg, GPTRDIFF_TO_INT (ip - cfg->cil_start));
-		} else {
-			if ((tblock = cfg->cil_offset_to_bb [ip - cfg->cil_start]) && (tblock != cfg->cbb)) {
-				link_bblock (cfg, cfg->cbb, tblock);
-				if (sp != stack_start) {
-					handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start));
-					sp = stack_start;
-					CHECK_UNVERIFIABLE (cfg);
-				}
-				cfg->cbb->next_bb = tblock;
-				cfg->cbb = tblock;
-				for (int i = 0; i < cfg->cbb->in_scount; ++i) {
-					if (cfg->verbose_level > 3)
-						printf ("loading %d from temp %d\n", i, (int)cfg->cbb->in_stack [i]->inst_c0);
-					EMIT_NEW_TEMPLOAD (cfg, ins, cfg->cbb->in_stack [i]->inst_c0);
-					*sp++ = ins;
-				}
-				g_slist_free (class_inits);
-				class_inits = NULL;
-				emit_set_deopt_il_offset (cfg, GPTRDIFF_TO_INT (ip - cfg->cil_start));
-			}
-		}
-		/*
-		 * Methods with AggressiveInline flag could be inlined even if the class has a cctor.
-		 * This might create a branch so emit it in the first code bblock instead of into initlocals_bb.
-		 */
-		if (ip - header->code == 0 && cfg->method != method && cfg->compile_aot && (method->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING) && mono_class_needs_cctor_run (method->klass, method))
-			emit_class_init (cfg, method->klass, FALSE);
-		if (skip_dead_blocks) {
-			int ip_offset = GPTRDIFF_TO_INT (ip - header->code);
-			if (ip_offset == bb->end)
-				bb = bb->next;
-			if (bb->dead) {
-				g_assert (op_size > 0); /*The BB formation pass must catch all bad ops*/
-				if (cfg->verbose_level > 3) printf ("SKIPPING DEAD OP at %x\n", ip_offset);
-				if (ip_offset + op_size == bb->end) {
-					MONO_INST_NEW (cfg, ins, OP_NOP);
-					MONO_ADD_INS (cfg->cbb, ins);
-					start_new_bblock = 1;
-				}
-				continue;
-			}
-		}
-		/*
-		 * Sequence points are points where the debugger can place a breakpoint.
-		 * Currently, we generate these automatically at points where the IL
-		 * stack is empty.
-		 */
-		if (seq_points && ((!sym_seq_points && (sp == stack_start)) || (sym_seq_points && mono_bitset_test_fast (seq_point_locs, ip - header->code)))) {
-			/*
-			 * Make methods interruptible at the beginning, and at the targets of
-			 * backward branches.
-			 * Also, do this at the start of every bblock in methods with clauses too,
-			 * to be able to handle instructions with inprecise control flow like
-			 * throw/endfinally.
-			 * Backward branches are handled at the end of method-to-ir ().
-			 */
-			gboolean intr_loc = ip == header->code || (!cfg->cbb->last_ins && cfg->header->num_clauses);
-			gboolean sym_seq_point = sym_seq_points && mono_bitset_test_fast (seq_point_locs, ip - header->code);
-			/* Avoid sequence points on empty IL like .volatile */
-			NEW_SEQ_POINT (cfg, ins, ip - header->code, intr_loc);
-			if ((sp != stack_start) && !sym_seq_point)
-				ins->flags |= MONO_INST_NONEMPTY_STACK;
-			MONO_ADD_INS (cfg->cbb, ins);
-			if (sym_seq_points)
-				mono_bitset_set_fast (seq_point_set_locs, ip - header->code);
-			if (cfg->prof_coverage) {
-				ptrdiff_t cil_offset = ip - header->code;
-				gpointer counter = &cfg->coverage_info->data [cil_offset].count;
-				cfg->coverage_info->data [cil_offset].cil_code = ip;
-				if (mono_arch_opcode_supported (OP_ATOMIC_ADD_I4)) {
-					MonoInst *one_ins, *load_ins;
-					EMIT_NEW_PCONST (cfg, load_ins, counter);
-					EMIT_NEW_ICONST (cfg, one_ins, 1);
-					MONO_INST_NEW (cfg, ins, OP_ATOMIC_ADD_I4);
-					ins->dreg = mono_alloc_ireg (cfg);
-					ins->inst_basereg = load_ins->dreg;
-					ins->inst_offset = 0;
-					ins->sreg2 = one_ins->dreg;
-					ins->type = STACK_I4;
-					MONO_ADD_INS (cfg->cbb, ins);
-				} else {
-					EMIT_NEW_PCONST (cfg, ins, counter);
-					MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STORE_MEMBASE_IMM, ins->dreg, 0, 1);
-				}
-			}
-		}
-		cfg->cbb->real_offset = cfg->real_offset;
-		if (cfg->verbose_level > 3)
-			printf ("converting (in B%d: stack: %d) %s", cfg->cbb->block_num, GPTRDIFF_TO_INT (sp - stack_start), mono_disasm_code_one (NULL, method, ip, NULL));
-		/*
-		 * This is used to compute BB_HAS_SIDE_EFFECTS, which is used for the elimination of
-		 * foreach finally clauses, so only IL opcodes which occur in such clauses
-		 * need to set this.
-		 */
-		ins_has_side_effect = TRUE;
-		gboolean emit_widen = TRUE;
-		gboolean tailcall = FALSE;
-		gboolean common_call = FALSE;
-		MonoInst *keep_this_alive = NULL;
-		MonoMethod *cmethod = NULL;
-		MonoMethodSignature *fsig = NULL;
-		gboolean need_seq_point = FALSE;
-		gboolean push_res = TRUE;
-		gboolean skip_ret = FALSE;
-		gboolean tailcall_remove_ret = FALSE;
-		MonoOpcodeParameter parameter;
-		const MonoOpcodeInfo* info = mono_opcode_decode (ip, op_size, il_op, &parameter);
-		g_assert (info);
-		n = parameter.i32;
-		token = parameter.i32;
-		target = parameter.branch_target;
-		const int pushes = info->pushes;
-		const int pops = info->pops;
-		if (pushes >= 0 && pops >= 0) {
-			g_assert (pushes - pops <= 1);
-			if (pushes - pops == 1)
-				CHECK_STACK_OVF ();
-		}
-		if (pops >= 0)
-			CHECK_STACK (pops);
-		switch (il_op) {
-		case MONO_CEE_NOP:
-			if (seq_points && !sym_seq_points && sp != stack_start) {
-				/*
-				 * The C# compiler uses these nops to notify the JIT that it should
-				 * insert seq points.
-				 */
-				NEW_SEQ_POINT (cfg, ins, ip - header->code, FALSE);
-				MONO_ADD_INS (cfg->cbb, ins);
-			}
-			if (cfg->keep_cil_nops)
-				MONO_INST_NEW (cfg, ins, OP_HARD_NOP);
-			else
-				MONO_INST_NEW (cfg, ins, OP_NOP);
-			MONO_ADD_INS (cfg->cbb, ins);
-			emitted_funccall_seq_point = FALSE;
-			ins_has_side_effect = FALSE;
-			break;
-		case MONO_CEE_BREAK:
-			if (mini_should_insert_breakpoint (cfg->method)) {
-				ins = mono_emit_jit_icall (cfg, mono_debugger_agent_user_break, NULL);
-			} else {
-				MONO_INST_NEW (cfg, ins, OP_NOP);
-				MONO_ADD_INS (cfg->cbb, ins);
-			}
-			break;
-		case MONO_CEE_LDARG_0:
-		case MONO_CEE_LDARG_1:
-		case MONO_CEE_LDARG_2:
-		case MONO_CEE_LDARG_3:
-		case MONO_CEE_LDARG_S:
-		case MONO_CEE_LDARG:
-			CHECK_ARG (n);
-			if (next_ip < end && is_addressable_valuetype_load (cfg, next_ip, cfg->arg_types [n])) {
-				EMIT_NEW_ARGLOADA (cfg, ins, n);
-			} else {
-				EMIT_NEW_ARGLOAD (cfg, ins, n);
-			}
-			*sp++ = ins;
-			/*if (!m_method_is_icall (method)) */{
-				MonoMethod* callvirt_target = try_prepare_objaddr_callvirt_optimization (cfg, next_ip, end, method, generic_context, param_types [n]);
-				if (callvirt_target)
-					cmethod_override = callvirt_target;
-			}
-			break;
-		case MONO_CEE_LDLOC_0:
-		case MONO_CEE_LDLOC_1:
-		case MONO_CEE_LDLOC_2:
-		case MONO_CEE_LDLOC_3:
-		case MONO_CEE_LDLOC_S:
-		case MONO_CEE_LDLOC:
-			CHECK_LOCAL (n);
-			if (next_ip < end && is_addressable_valuetype_load (cfg, next_ip, header->locals [n])) {
-				EMIT_NEW_LOCLOADA (cfg, ins, n);
-			} else {
-				EMIT_NEW_LOCLOAD (cfg, ins, n);
-			}
-			*sp++ = ins;
-			break;
-		case MONO_CEE_STLOC_0:
-		case MONO_CEE_STLOC_1:
-		case MONO_CEE_STLOC_2:
-		case MONO_CEE_STLOC_3:
-		case MONO_CEE_STLOC_S:
-		case MONO_CEE_STLOC:
-			CHECK_LOCAL (n);
-			--sp;
-			*sp = convert_value (cfg, header->locals [n], *sp);
-			if (!dont_verify_stloc && target_type_is_incompatible (cfg, header->locals [n], *sp))
-				UNVERIFIED;
-			emit_stloc_ir (cfg, sp, header, n);
-			inline_costs += 1;
-			break;
-		case MONO_CEE_LDARGA_S:
-		case MONO_CEE_LDARGA:
-			CHECK_ARG (n);
-			NEW_ARGLOADA (cfg, ins, n);
-			MONO_ADD_INS (cfg->cbb, ins);
-			*sp++ = ins;
-			break;
-		case MONO_CEE_STARG_S:
-		case MONO_CEE_STARG:
-			--sp;
-			CHECK_ARG (n);
-			*sp = convert_value (cfg, param_types [n], *sp);
-			if (!dont_verify_stloc && target_type_is_incompatible (cfg, param_types [n], *sp))
-				UNVERIFIED;
-			emit_starg_ir (cfg, sp, n);
-			break;
-		case MONO_CEE_LDLOCA:
-		case MONO_CEE_LDLOCA_S: {
-			guchar *ldloca_ip;
-			CHECK_LOCAL (n);
-			if ((ldloca_ip = emit_optimized_ldloca_ir (cfg, next_ip, end, n))) {
-				next_ip = ldloca_ip;
-				il_op = MONO_CEE_INITOBJ;
-				inline_costs += 1;
-				break;
-			}
-			ins_has_side_effect = FALSE;
-			EMIT_NEW_LOCLOADA (cfg, ins, n);
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_LDNULL:
-			EMIT_NEW_PCONST (cfg, ins, NULL);
-			ins->type = STACK_OBJ;
-			*sp++ = ins;
-			break;
-		case MONO_CEE_LDC_I4_M1:
-		case MONO_CEE_LDC_I4_0:
-		case MONO_CEE_LDC_I4_1:
-		case MONO_CEE_LDC_I4_2:
-		case MONO_CEE_LDC_I4_3:
-		case MONO_CEE_LDC_I4_4:
-		case MONO_CEE_LDC_I4_5:
-		case MONO_CEE_LDC_I4_6:
-		case MONO_CEE_LDC_I4_7:
-		case MONO_CEE_LDC_I4_8:
-		case MONO_CEE_LDC_I4_S:
-		case MONO_CEE_LDC_I4:
-			EMIT_NEW_ICONST (cfg, ins, n);
-			*sp++ = ins;
-			break;
-		case MONO_CEE_LDC_I8:
-			MONO_INST_NEW (cfg, ins, OP_I8CONST);
-			ins->type = STACK_I8;
-			ins->dreg = alloc_dreg (cfg, STACK_I8);
-			ins->inst_l = parameter.i64;
-			MONO_ADD_INS (cfg->cbb, ins);
-			*sp++ = ins;
-			break;
-		case MONO_CEE_LDC_R4: {
-			float *f;
-			gboolean use_aotconst = FALSE;
-#ifdef TARGET_POWERPC
-			/* FIXME: Clean this up */
-			if (cfg->compile_aot)
-				use_aotconst = TRUE;
-#endif
-			/* FIXME: we should really allocate this only late in the compilation process */
-			f = (float *)mono_mem_manager_alloc (cfg->mem_manager, sizeof (float));
-			if (use_aotconst) {
-				MonoInst *cons;
-				int dreg;
-				EMIT_NEW_AOTCONST (cfg, cons, MONO_PATCH_INFO_R4, f);
-				dreg = alloc_freg (cfg);
-				EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOADR4_MEMBASE, dreg, cons->dreg, 0);
-				ins->type = GINT_TO_UINT8 (cfg->r4_stack_type);
-			} else {
-				MONO_INST_NEW (cfg, ins, OP_R4CONST);
-				ins->type = GINT_TO_UINT8 (cfg->r4_stack_type);
-				ins->dreg = alloc_dreg (cfg, STACK_R8);
-				ins->inst_p0 = f;
-				MONO_ADD_INS (cfg->cbb, ins);
-			}
-			*f = parameter.f;
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_LDC_R8: {
-			double *d;
-			gboolean use_aotconst = FALSE;
-#ifdef TARGET_POWERPC
-			/* FIXME: Clean this up */
-			if (cfg->compile_aot)
-				use_aotconst = TRUE;
-#endif
-			/* FIXME: we should really allocate this only late in the compilation process */
-			d = (double *)mono_mem_manager_alloc (cfg->mem_manager, sizeof (double));
-			if (use_aotconst) {
-				MonoInst *cons;
-				int dreg;
-				EMIT_NEW_AOTCONST (cfg, cons, MONO_PATCH_INFO_R8, d);
-				dreg = alloc_freg (cfg);
-				EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOADR8_MEMBASE, dreg, cons->dreg, 0);
-				ins->type = STACK_R8;
-			} else {
-				MONO_INST_NEW (cfg, ins, OP_R8CONST);
-				ins->type = STACK_R8;
-				ins->dreg = alloc_dreg (cfg, STACK_R8);
-				ins->inst_p0 = d;
-				MONO_ADD_INS (cfg->cbb, ins);
-			}
-			*d = parameter.d;
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_DUP: {
-			MonoInst *temp, *store;
-			sp--;
-			ins = *sp;
-			klass = ins->klass;
-			temp = mono_compile_create_var (cfg, type_from_stack_type (ins), OP_LOCAL);
-			EMIT_NEW_TEMPSTORE (cfg, store, temp->inst_c0, ins);
-			EMIT_NEW_TEMPLOAD (cfg, ins, temp->inst_c0);
-			ins->klass = klass;
-			*sp++ = ins;
-			EMIT_NEW_TEMPLOAD (cfg, ins, temp->inst_c0);
-			ins->klass = klass;
-			*sp++ = ins;
-			inline_costs += 2;
-			break;
-		}
-		case MONO_CEE_POP:
-			--sp;
-			break;
-		case MONO_CEE_JMP: {
-			MonoCallInst *call;
-			INLINE_FAILURE ("jmp");
-			GSHAREDVT_FAILURE (il_op);
-			if (stack_start != sp)
-				UNVERIFIED;
-			/* FIXME: check the signature matches */
-			cmethod = mini_get_method (cfg, method, token, NULL, generic_context);
-			CHECK_CFG_ERROR;
-			if (cfg->gshared && mono_method_check_context_used (cmethod))
-				GENERIC_SHARING_FAILURE (CEE_JMP);
-			mini_profiler_emit_tail_call (cfg, cmethod);
-			fsig = mono_method_signature_internal (cmethod);
-			int nargs = fsig->param_count + fsig->hasthis;
-			if (cfg->llvm_only) {
-				MonoInst **args;
-				args = (MonoInst **)mono_mempool_alloc (cfg->mempool, sizeof (MonoInst*) * nargs);
-				for (int i = 0; i < nargs; ++i)
-					EMIT_NEW_ARGLOAD (cfg, args [i], i);
-				ins = mini_emit_method_call_full (cfg, cmethod, fsig, TRUE, args, NULL, NULL, NULL);
-				/*
-				 * The code in mono-basic-block.c treats the rest of the code as dead, but we
-				 * have to emit a normal return since llvm expects it.
-				 */
-				if (cfg->ret)
-					emit_setret (cfg, ins);
-				MONO_INST_NEW (cfg, ins, OP_BR);
-				ins->inst_target_bb = end_bblock;
-				MONO_ADD_INS (cfg->cbb, ins);
-				link_bblock (cfg, cfg->cbb, end_bblock);
-				break;
-			} else {
-				/* Handle tailcalls similarly to calls */
-				DISABLE_AOT (cfg);
-				mini_emit_tailcall_parameters (cfg, fsig);
-				MONO_INST_NEW_CALL (cfg, call, OP_TAILCALL);
-				call->method = cmethod;
-				call->tailcall = TRUE;
-				call->signature = fsig;
-				call->args = (MonoInst **)mono_mempool_alloc (cfg->mempool, sizeof (MonoInst*) * nargs);
-				call->inst.inst_p0 = cmethod;
-				for (int i = 0; i < nargs; ++i)
-					EMIT_NEW_ARGLOAD (cfg, call->args [i], i);
-				if (mini_type_is_vtype (mini_get_underlying_type (call->signature->ret)))
-					call->vret_var = cfg->vret_addr;
-				mono_arch_emit_call (cfg, call);
-				cfg->param_area = MAX(cfg->param_area, call->stack_usage);
-				MONO_ADD_INS (cfg->cbb, (MonoInst*)call);
-			}
-			start_new_bblock = 1;
-			break;
-		}
-		case MONO_CEE_CALLI: {
-			MonoInst *addr;
-			MonoInst *callee = NULL;
-			common_call = TRUE; // i.e. skip_ret/push_res/seq_point logic
-			cmethod = NULL;
-			gboolean const inst_tailcall = G_UNLIKELY (debug_tailcall_try_all
-							? (next_ip < end && next_ip [0] == CEE_RET)
-							: ((ins_flag & MONO_INST_TAILCALL) != 0));
-			ins = NULL;
-			CHECK_STACK (1);
-			--sp;
-			addr = *sp;
-			g_assert (addr);
-			fsig = mini_get_signature (method, token, generic_context, cfg->error);
-			CHECK_CFG_ERROR;
-			if (cfg->gsharedvt_min && mini_is_gsharedvt_variable_signature (fsig))
-				GSHAREDVT_FAILURE (il_op);
-			if (method->dynamic && fsig->pinvoke) {
-				MonoInst *args [3];
-				/*
-				 * This is a call through a function pointer using a pinvoke
-				 * signature. Have to create a wrapper and call that instead.
-				 * FIXME: This is very slow, need to create a wrapper at JIT time
-				 * instead based on the signature.
-				 */
-				EMIT_NEW_IMAGECONST (cfg, args [0], ((MonoDynamicMethod*)method)->assembly->image);
-				EMIT_NEW_PCONST (cfg, args [1], fsig);
-				args [2] = addr;
-				addr = mono_emit_jit_icall (cfg, mono_get_native_calli_wrapper, args);
-			}
-			if (!method->dynamic && fsig->pinvoke &&
-			    !method->wrapper_type) {
-				/* MONO_WRAPPER_DYNAMIC_METHOD dynamic method handled above in the
-				method->dynamic case; for other wrapper types assume the code knows
-				what its doing and added its own GC transitions */
-				gboolean skip_gc_trans = fsig->suppress_gc_transition;
-				if (!skip_gc_trans) {
-#if 0
-					fprintf (stderr, "generating wrapper for calli in method %s with wrapper type %s\n", method->name, mono_wrapper_type_to_str (method->wrapper_type));
-#endif
-					if (cfg->compile_aot)
-						cfg->pinvoke_calli_signatures = g_slist_prepend_mempool (cfg->mempool, cfg->pinvoke_calli_signatures, fsig);
-					/* Call the wrapper that will do the GC transition instead */
-					MonoMethod *wrapper = mono_marshal_get_native_func_wrapper_indirect (method->klass, fsig, cfg->compile_aot);
-					fsig = mono_method_signature_internal (wrapper);
-					n = fsig->param_count - 1; /* wrapper has extra fnptr param */
-					CHECK_STACK (n);
-					/* move the args to allow room for 'this' in the first position */
-					while (n--) {
-						--sp;
-						sp [1] = sp [0];
-					}
-					sp[0] = addr; /* n+1 args, first arg is the address of the indirect method to call */
-					g_assert (!fsig->hasthis && !fsig->pinvoke);
-					ins = mono_emit_method_call (cfg, wrapper, /*args*/sp, NULL);
-					goto calli_end;
-				}
-			}
-			n = fsig->param_count + fsig->hasthis;
-			CHECK_STACK (n);
-			if (n == 0 && fsig->call_convention == MONO_CALL_THISCALL)
-				mono_cfg_set_exception_invalid_program(cfg, "thiscall with 0 arguments");
-			sp -= n;
-			if (!(cfg->method->wrapper_type && cfg->method->wrapper_type != MONO_WRAPPER_DYNAMIC_METHOD) && check_call_signature (cfg, fsig, sp)) {
-				if (break_on_unverified ())
-					check_call_signature (cfg, fsig, sp); // Again, step through it.
-				UNVERIFIED;
-			}
-			inline_costs += CALL_COST * MIN(10, num_calls++);
-			/*
-			 * Making generic calls out of gsharedvt methods.
-			 * This needs to be used for all generic calls, not just ones with a gsharedvt signature, to avoid
-			 * patching gshared method addresses into a gsharedvt method.
-			 */
-			if (cfg->gsharedvt && mini_is_gsharedvt_signature (fsig)) {
-				/*
-				 * We pass the address to the gsharedvt trampoline in the rgctx reg
-				 */
-				callee = addr;
-				g_assert (addr); // Doubles as boolean after tailcall check.
-			}
-			inst_tailcall && is_supported_tailcall (cfg, ip, method, NULL, fsig,
-						FALSE/*virtual irrelevant*/, addr != NULL, &tailcall);
-			if (save_last_error)
-				mono_emit_jit_icall (cfg, mono_marshal_clear_last_error, NULL);
-			if (callee) {
-				if (method->wrapper_type != MONO_WRAPPER_DELEGATE_INVOKE)
-					/* Not tested */
-					GSHAREDVT_FAILURE (il_op);
-				if (cfg->llvm_only)
-					GSHAREDVT_FAILURE (il_op);
-				addr = emit_get_rgctx_sig (cfg, context_used, fsig, MONO_RGCTX_INFO_SIG_GSHAREDVT_OUT_TRAMPOLINE_CALLI);
-				ins = (MonoInst*)mini_emit_calli_full (cfg, fsig, sp, addr, NULL, callee, tailcall);
-				goto calli_end;
-			}
-			/* Prevent inlining of methods with indirect calls */
-			INLINE_FAILURE ("indirect call");
-			if (addr->opcode == OP_PCONST || addr->opcode == OP_AOTCONST || addr->opcode == OP_GOT_ENTRY) {
-				MonoJumpInfoType info_type;
-				gpointer info_data;
-				/*
-				 * Instead of emitting an indirect call, emit a direct call
-				 * with the contents of the aotconst as the patch info.
-				 */
-				if (addr->opcode == OP_PCONST || addr->opcode == OP_AOTCONST) {
-					info_type = (MonoJumpInfoType)addr->inst_c1;
-					info_data = addr->inst_p0;
-				} else {
-					info_type = (MonoJumpInfoType)addr->inst_right->inst_c1;
-					info_data = addr->inst_right->inst_left;
-				}
-				if (info_type == MONO_PATCH_INFO_ICALL_ADDR) {
-					tailcall = FALSE;
-					ins = (MonoInst*)mini_emit_abs_call (cfg, MONO_PATCH_INFO_ICALL_ADDR_CALL, info_data, fsig, sp);
-					NULLIFY_INS (addr);
-					goto calli_end;
-				} else if (info_type == MONO_PATCH_INFO_JIT_ICALL_ADDR
-						|| info_type == MONO_PATCH_INFO_SPECIFIC_TRAMPOLINE_LAZY_FETCH_ADDR) {
-					tailcall = FALSE;
-					ins = (MonoInst*)mini_emit_abs_call (cfg, info_type, info_data, fsig, sp);
-					NULLIFY_INS (addr);
-					goto calli_end;
-				}
-			}
-			/* Some wrappers use calli with ftndesc-es */
-			if (cfg->llvm_only && !(cfg->method->wrapper_type &&
-									cfg->method->wrapper_type != MONO_WRAPPER_DYNAMIC_METHOD &&
-									cfg->method->wrapper_type != MONO_WRAPPER_DELEGATE_INVOKE))
-				ins = mini_emit_llvmonly_calli (cfg, fsig, sp, addr);
-			else
-				ins = (MonoInst*)mini_emit_calli_full (cfg, fsig, sp, addr, NULL, NULL, tailcall);
-			goto calli_end;
-		}
-		case MONO_CEE_CALL:
-		case MONO_CEE_CALLVIRT: {
-			MonoInst *addr; addr = NULL;
-			int array_rank; array_rank = 0;
-			gboolean virtual_; virtual_ = il_op == MONO_CEE_CALLVIRT;
-			gboolean pass_imt_from_rgctx; pass_imt_from_rgctx = FALSE;
-			MonoInst *imt_arg; imt_arg = NULL;
-			gboolean pass_mrgctx; pass_mrgctx = FALSE;
-			MonoInst *vtable_arg; vtable_arg = NULL;
-			gboolean check_this; check_this = FALSE;
-			gboolean delegate_invoke; delegate_invoke = FALSE;
-			gboolean direct_icall; direct_icall = FALSE;
-			gboolean tailcall_calli; tailcall_calli = FALSE;
-			gboolean noreturn; noreturn = FALSE;
-			gboolean gshared_static_virtual; gshared_static_virtual = FALSE;
-#ifdef TARGET_WASM
-			gboolean needs_stack_walk; needs_stack_walk = FALSE;
-#endif
-			common_call = FALSE;
-			gboolean called_is_supported_tailcall; called_is_supported_tailcall = FALSE;
-			MonoMethod *tailcall_method; tailcall_method = NULL;
-			MonoMethod *tailcall_cmethod; tailcall_cmethod = NULL;
-			MonoMethodSignature *tailcall_fsig; tailcall_fsig = NULL;
-			gboolean tailcall_virtual; tailcall_virtual = FALSE;
-			gboolean tailcall_extra_arg; tailcall_extra_arg = FALSE;
-			gboolean inst_tailcall; inst_tailcall = G_UNLIKELY (debug_tailcall_try_all
-							? (next_ip < end && next_ip [0] == CEE_RET)
-							: ((ins_flag & MONO_INST_TAILCALL) != 0));
-			gboolean make_generic_call_out_of_gsharedvt_method = FALSE;
-			gboolean will_have_imt_arg = FALSE;
-			ins = NULL;
-			/* Used to pass arguments to called functions */
-			HandleCallData cdata;
-			memset (&cdata, 0, sizeof (HandleCallData));
-			if (cmethod_override) {
-				cmethod = cmethod_override;
-				cmethod_override = NULL;
-				virtual_ = FALSE;
-			} else {
-				cmethod = mini_get_method (cfg, method, token, NULL, generic_context);
-			}
-			CHECK_CFG_ERROR;
-			if (cfg->verbose_level > 3)
-				printf ("cmethod = %s\n", mono_method_get_full_name (cmethod));
-			MonoMethod *cil_method; cil_method = cmethod;
-			if (constrained_class) {
-				if (m_method_is_static (cil_method) && mini_class_check_context_used (cfg, constrained_class)) {
-					/* get_constrained_method () doesn't work on the gparams used by generic sharing */
-					gshared_static_virtual = TRUE;
-					if (!cfg->gsharedvt)
-						/*
-						 * We can't resolve these calls at compile time, and they are used in
-						 * perf-sensitive code in the BCL, so ask the AOT compiler to try to use specific instances
-						 * instead of this gshared method.
-						 */
-						cfg->prefer_instances = TRUE;
-				} else {
-					cmethod = get_constrained_method (cfg, image, token, cil_method, constrained_class, generic_context);
-					CHECK_CFG_ERROR;
-					if (mono_class_has_dim_conflicts (constrained_class) &&
-							mono_class_is_method_ambiguous (constrained_class, cil_method))
-						mono_emit_jit_icall (cfg, mono_throw_ambiguous_implementation, NULL);
-					if (m_class_is_enumtype (constrained_class) && !strcmp (cmethod->name, "GetHashCode")) {
-						/* Use the corresponding method from the base type to avoid boxing */
-						MonoType *base_type = mono_class_enum_basetype_internal (constrained_class);
-						g_assert (base_type);
-						constrained_class = mono_class_from_mono_type_internal (base_type);
-						cmethod = get_method_nofail (constrained_class, cmethod->name, 0, 0);
-						g_assert (cmethod);
-					}
-				}
-			}
-			if (!dont_verify && !cfg->skip_visibility) {
-				MonoMethod *target_method = cil_method;
-				if (method->is_inflated) {
-					MonoGenericContainer *container = mono_method_get_generic_container(method_definition);
-					MonoGenericContext *context = (container != NULL ? &container->context : NULL);
-					target_method = mini_get_method_allow_open (method, token, NULL, context, cfg->error);
-					CHECK_CFG_ERROR;
-				}
-				if (!mono_method_can_access_method (method_definition, target_method) &&
-					!mono_method_can_access_method (method, cil_method))
-					emit_method_access_failure (cfg, method, cil_method);
-			}
-			if (cfg->llvm_only && cmethod && method_needs_stack_walk (cfg, cmethod)) {
-				if (cfg->interp && !cfg->interp_entry_only) {
-					/* Use the interpreter instead */
-					cfg->exception_message = g_strdup ("stack walk");
-					cfg->disable_llvm = TRUE;
-				}
-#ifdef TARGET_WASM
-				else {
-					needs_stack_walk = TRUE;
-				}
-#endif
-			}
-			if (!virtual_ && (cmethod->flags & METHOD_ATTRIBUTE_ABSTRACT) && !gshared_static_virtual) {
-				if (!mono_class_is_interface (method->klass))
-					emit_bad_image_failure (cfg, method, cil_method);
-				else
-					virtual_ = TRUE;
-			}
-			if (!m_class_is_inited (cmethod->klass))
-				if (!mono_class_init_internal (cmethod->klass))
-					TYPE_LOAD_ERROR (cmethod->klass);
-			fsig = mono_method_signature_internal (cmethod);
-			if (!fsig)
-				LOAD_ERROR;
-			if (cmethod->iflags & METHOD_IMPL_ATTRIBUTE_INTERNAL_CALL &&
-				mini_class_is_system_array (cmethod->klass)) {
-				array_rank = m_class_get_rank (cmethod->klass);
-			} else if ((cmethod->iflags & METHOD_IMPL_ATTRIBUTE_INTERNAL_CALL) && direct_icalls_enabled (cfg, cmethod)) {
-				direct_icall = TRUE;
-			} else if (fsig->pinvoke) {
-				if (cmethod->flags & METHOD_ATTRIBUTE_PINVOKE_IMPL) {
-					/*
-					 * Avoid calling mono_marshal_get_native_wrapper () too early, it might call managed
-					 * callbacks on netcore.
-					 */
-					fsig = mono_metadata_signature_dup_mempool (cfg->mempool, fsig);
-					fsig->pinvoke = FALSE;
-				} else {
-					MonoMethod *wrapper = mono_marshal_get_native_wrapper (cmethod, TRUE, cfg->compile_aot);
-					fsig = mono_method_signature_internal (wrapper);
-				}
-			} else if (constrained_class) {
-			} else {
-				fsig = mono_method_get_signature_checked (cmethod, image, token, generic_context, cfg->error);
-				CHECK_CFG_ERROR;
-			}
-			if (cfg->llvm_only && !cfg->method->wrapper_type && (!cmethod || cmethod->is_inflated))
-				cfg->signatures = g_slist_prepend_mempool (cfg->mempool, cfg->signatures, fsig);
-			if (cfg->gsharedvt_min && mini_is_gsharedvt_variable_signature (fsig))
-				GSHAREDVT_FAILURE (il_op);
-			/* See code below */
-			if (cmethod->klass == mono_defaults.monitor_class && !strcmp (cmethod->name, "Enter") && mono_method_signature_internal (cmethod)->param_count == 1) {
-				MonoBasicBlock *tbb;
-				GET_BBLOCK (cfg, tbb, next_ip);
-				if (tbb->try_start && MONO_REGION_FLAGS(tbb->region) == MONO_EXCEPTION_CLAUSE_FINALLY) {
-					/*
-					 * We want to extend the try block to cover the call, but we can't do it if the
-					 * call is made directly since its followed by an exception check.
-					 */
-					direct_icall = FALSE;
-				}
-			}
-			mono_save_token_info (cfg, image, token, cil_method);
-			if (!(seq_point_locs && mono_bitset_test_fast (seq_point_locs, next_ip - header->code)))
-				need_seq_point = TRUE;
-			/* Don't support calls made using type arguments for now */
-			/*
-			  if (cfg->gsharedvt) {
-			  if (mini_is_gsharedvt_signature (fsig))
-			  GSHAREDVT_FAILURE (il_op);
-			  }
-			*/
-			if (cmethod->string_ctor && method->wrapper_type != MONO_WRAPPER_RUNTIME_INVOKE)
-				g_assert_not_reached ();
-			n = fsig->param_count + fsig->hasthis;
-			if (!cfg->gshared && mono_class_is_gtd (cmethod->klass))
-				UNVERIFIED;
-			if (!cfg->gshared)
-				g_assert (!mono_method_check_context_used (cmethod));
-			CHECK_STACK (n);
-			sp -= n;
-			if (virtual_ && cmethod && sp [0] && sp [0]->opcode == OP_TYPED_OBJREF) {
-				error_init_reuse (error);
-				MonoMethod *new_cmethod = mono_class_get_virtual_method (sp [0]->klass, cmethod, error);
-				if (is_ok (error)) {
-					cmethod = new_cmethod;
-					virtual_ = FALSE;
-				} else {
-					mono_error_cleanup (error);
-				}
-			}
-			if (cmethod && method_does_not_return (cmethod)) {
-				cfg->cbb->out_of_line = TRUE;
-				noreturn = TRUE;
-			}
-			cdata.method = method;
-			cdata.inst_tailcall = inst_tailcall;
-			/*
-			 * We have the `constrained.' prefix opcode.
-			 */
-			if (constrained_class) {
-				ins = handle_constrained_call (cfg, cmethod, fsig, constrained_class, sp, &cdata, &cmethod, &virtual_, &emit_widen);
-				CHECK_CFG_EXCEPTION;
-				if (!gshared_static_virtual)
-					constrained_class = NULL;
-				if (ins)
-					goto call_end;
-			}
-			for (int i = 0; i < fsig->param_count; ++i)
-				sp [i + fsig->hasthis] = convert_value (cfg, fsig->params [i], sp [i + fsig->hasthis]);
-			if (check_call_signature (cfg, fsig, sp)) {
-				if (break_on_unverified ())
-					check_call_signature (cfg, fsig, sp); // Again, step through it.
-				UNVERIFIED;
-			}
-			if ((m_class_get_parent (cmethod->klass) == mono_defaults.multicastdelegate_class) && !strcmp (cmethod->name, "Invoke"))
-				delegate_invoke = TRUE;
-			/*
-			 * Implement a workaround for the inherent races involved in locking:
-			 * Monitor.Enter ()
-			 * try {
-			 * } finally {
-			 *    Monitor.Exit ()
-			 * }
-			 * If a thread abort happens between the call to Monitor.Enter () and the start of the
-			 * try block, the Exit () won't be executed, see:
-			 * http://www.bluebytesoftware.com/blog/2007/01/30/MonitorEnterThreadAbortsAndOrphanedLocks.aspx
-			 * To work around this, we extend such try blocks to include the last x bytes
-			 * of the Monitor.Enter () call.
-			 */
-			if (cmethod->klass == mono_defaults.monitor_class && !strcmp (cmethod->name, "Enter") && mono_method_signature_internal (cmethod)->param_count == 1) {
-				MonoBasicBlock *tbb;
-				GET_BBLOCK (cfg, tbb, next_ip);
-				/*
-				 * Only extend try blocks with a finally, to avoid catching exceptions thrown
-				 * from Monitor.Enter like ArgumentNullException.
-				 */
-				if (tbb->try_start && MONO_REGION_FLAGS(tbb->region) == MONO_EXCEPTION_CLAUSE_FINALLY) {
-					/* Mark this bblock as needing to be extended */
-					tbb->extend_try_block = TRUE;
-				}
-			}
-			/* Conversion to a JIT intrinsic */
-			gboolean ins_type_initialized;
-			if ((ins = mini_emit_inst_for_method (cfg, cmethod, fsig, sp, &ins_type_initialized))) {
-				if (!MONO_TYPE_IS_VOID (fsig->ret)) {
-					if (!ins_type_initialized)
-						mini_type_to_eval_stack_type ((cfg), fsig->ret, ins);
-					emit_widen = FALSE;
-				}
-				if (inst_tailcall) // FIXME
-					mono_tailcall_print ("missed tailcall intrins %s -> %s\n", method->name, cmethod->name);
-				goto call_end;
-			}
-			CHECK_CFG_ERROR;
-			/*
-			 * If the callee is a shared method, then its static cctor
-			 * might not get called after the call was patched.
-			 */
-			if (cfg->gshared && cmethod->klass != method->klass && mono_class_is_ginst (cmethod->klass) && mono_method_is_generic_sharable (cmethod, TRUE) && mono_class_needs_cctor_run (cmethod->klass, method)) {
-				emit_class_init (cfg, cmethod->klass, FALSE);
-				CHECK_TYPELOAD (cmethod->klass);
-			}
-			/* Inlining */
-			if ((cfg->opt & MONO_OPT_INLINE) && !inst_tailcall && !gshared_static_virtual &&
-				(!virtual_ || !(cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL) || MONO_METHOD_IS_FINAL (cmethod)) &&
-			    mono_method_check_inlining (cfg, cmethod)) {
-				int costs;
-				gboolean always = FALSE;
-				gboolean is_empty = FALSE;
-				if (cmethod->iflags & METHOD_IMPL_ATTRIBUTE_INTERNAL_CALL) {
-					/* Prevent inlining of methods that call wrappers */
-					INLINE_FAILURE ("wrapper call");
-					cmethod = mono_marshal_get_native_wrapper (cmethod, TRUE, FALSE);
-					always = TRUE;
-				}
-				costs = inline_method (cfg, cmethod, fsig, sp, ip, cfg->real_offset, always, &is_empty);
-				if (costs) {
-					cfg->real_offset += 5;
-					if (!MONO_TYPE_IS_VOID (fsig->ret))
-						/* *sp is already set by inline_method */
-						ins = *sp;
-					inline_costs += costs;
-					if (inst_tailcall) // FIXME
-						mono_tailcall_print ("missed tailcall inline %s -> %s\n", method->name, cmethod->name);
-					if (is_empty)
-						ins_has_side_effect = FALSE;
-					goto call_end;
-				}
-			}
-			pass_mrgctx = need_mrgctx_arg (cfg, cmethod);
-			if (cfg->gshared) {
-				MonoGenericContext *cmethod_context = mono_method_get_context (cmethod);
-				context_used = mini_method_check_context_used (cfg, cmethod);
-				if (!context_used && gshared_static_virtual)
-					context_used = mini_class_check_context_used (cfg, constrained_class);
-				if (context_used && mono_class_is_interface (cmethod->klass) && !m_method_is_static (cmethod)) {
-					/* Generic method interface
-					   calls are resolved via a
-					   helper function and don't
-					   need an imt. */
-					if (!cmethod_context || !cmethod_context->method_inst)
-						pass_imt_from_rgctx = TRUE;
-				}
-				/*
-				 * If a shared method calls another
-				 * shared method then the caller must
-				 * have a generic sharing context
-				 * because the magic trampoline
-				 * requires it.  FIXME: We shouldn't
-				 * have to force the vtable/mrgctx
-				 * variable here.  Instead there
-				 * should be a flag in the cfg to
-				 * request a generic sharing context.
-				 */
-				if (context_used &&
-				    ((cfg->method->flags & METHOD_ATTRIBUTE_STATIC) || m_class_is_valuetype (cfg->method->klass)))
-					mono_get_vtable_var (cfg);
-			}
-			if (pass_mrgctx) {
-				g_assert (!vtable_arg);
-				if (!cfg->compile_aot) {
-					/*
-					 * emit_get_rgctx_method () calls mono_class_vtable () so check
-					 * for type load errors before.
-					 */
-					mono_class_setup_vtable (cmethod->klass);
-					CHECK_TYPELOAD (cmethod->klass);
-				}
-				vtable_arg = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_METHOD_RGCTX);
-				if ((!(cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL) || MONO_METHOD_IS_FINAL (cmethod)) && !delegate_invoke) {
-					if (virtual_)
-						check_this = TRUE;
-					virtual_ = FALSE;
-				}
-			}
-			if (pass_imt_from_rgctx) {
-				imt_arg = emit_get_rgctx_method (cfg, context_used,
-					cmethod, MONO_RGCTX_INFO_METHOD);
-				g_assert (imt_arg);
-			}
-			if (check_this)
-				MONO_EMIT_NEW_CHECK_THIS (cfg, sp [0]->dreg);
-			/* Calling virtual generic methods */
-			gboolean virtual_generic; virtual_generic = FALSE;
-			gboolean virtual_generic_imt; virtual_generic_imt = FALSE;
-			if (virtual_ && (cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL) &&
-			    !MONO_METHOD_IS_FINAL (cmethod) &&
-			    fsig->generic_param_count &&
-				!(cfg->gsharedvt && mini_is_gsharedvt_signature (fsig)) &&
-				!cfg->llvm_only) {
-				g_assert (fsig->is_inflated);
-				virtual_generic = TRUE;
-				/* Prevent inlining of methods that contain indirect calls */
-				INLINE_FAILURE ("virtual generic call");
-				if (cfg->gsharedvt && mini_is_gsharedvt_signature (fsig))
-					GSHAREDVT_FAILURE (il_op);
-				if (cfg->backend->have_generalized_imt_trampoline && cfg->backend->gshared_supported && cmethod->wrapper_type == MONO_WRAPPER_NONE) {
-					virtual_generic_imt = TRUE;
-					g_assert (!imt_arg);
-					if (!context_used)
-						g_assert (cmethod->is_inflated);
-					imt_arg = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_METHOD);
-					g_assert (imt_arg);
-					virtual_ = TRUE;
-					vtable_arg = NULL;
-				}
-			}
-			/*
-			 * Making generic calls out of gsharedvt methods.
-			 * This needs to be used for all generic calls, not just ones with a gsharedvt signature, to avoid
-			 * patching gshared method addresses into a gsharedvt method.
-			 */
-			if (cfg->gsharedvt && (mini_is_gsharedvt_signature (fsig) || cmethod->is_inflated || mono_class_is_ginst (cmethod->klass)) &&
-				!(m_class_get_rank (cmethod->klass) && m_class_get_byval_arg (cmethod->klass)->type != MONO_TYPE_SZARRAY) &&
-				(!(cfg->llvm_only && virtual_ && (cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL)))) {
-				make_generic_call_out_of_gsharedvt_method = TRUE;
-				if (virtual_) {
-					if (fsig->generic_param_count) {
-						will_have_imt_arg = TRUE;
-					} else if (mono_class_is_interface (cmethod->klass) && !imt_arg) {
-						will_have_imt_arg = TRUE;
-					}
-				}
-			}
-			/* Tail prefix / tailcall optimization */
-			/* FIXME: Enabling TAILC breaks some inlining/stack trace/etc tests.
-				  Inlining and stack traces are not guaranteed however. */
-			/* FIXME: runtime generic context pointer for jumps? */
-			/* FIXME: handle this for generic sharing eventually */
-			tailcall_extra_arg = vtable_arg || imt_arg || will_have_imt_arg || mono_class_is_interface (cmethod->klass);
-			tailcall = inst_tailcall && is_supported_tailcall (cfg, ip, method, cmethod, fsig,
-						virtual_, tailcall_extra_arg, &tailcall_calli);
-			called_is_supported_tailcall = TRUE;
-			tailcall_method = method;
-			tailcall_cmethod = cmethod;
-			tailcall_fsig = fsig;
-			tailcall_virtual = virtual_;
-			if (virtual_generic) {
-				if (virtual_generic_imt) {
-					if (tailcall) {
-						/* Prevent inlining of methods with tailcalls (the call stack would be altered) */
-						INLINE_FAILURE ("tailcall");
-					}
-					common_call = TRUE;
-					goto call_end;
-				}
-				MonoInst *this_temp, *this_arg_temp, *store;
-				MonoInst *iargs [4];
-				this_temp = mono_compile_create_var (cfg, type_from_stack_type (sp [0]), OP_LOCAL);
-				NEW_TEMPSTORE (cfg, store, this_temp->inst_c0, sp [0]);
-				MONO_ADD_INS (cfg->cbb, store);
-				/* FIXME: This should be a managed pointer */
-				this_arg_temp = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
-				EMIT_NEW_TEMPLOAD (cfg, iargs [0], this_temp->inst_c0);
-				iargs [1] = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_METHOD);
-				EMIT_NEW_TEMPLOADA (cfg, iargs [2], this_arg_temp->inst_c0);
-				addr = mono_emit_jit_icall (cfg, mono_helper_compile_generic_method, iargs);
-				EMIT_NEW_TEMPLOAD (cfg, sp [0], this_arg_temp->inst_c0);
-				ins = (MonoInst*)mini_emit_calli (cfg, fsig, sp, addr, NULL, NULL);
-				if (inst_tailcall) // FIXME
-					mono_tailcall_print ("missed tailcall virtual generic %s -> %s\n", method->name, cmethod->name);
-				goto call_end;
-			}
-			CHECK_CFG_ERROR;
-			/* Tail recursion elimination */
-			if (((cfg->opt & MONO_OPT_TAILCALL) || inst_tailcall) && il_op == MONO_CEE_CALL && cmethod == method && next_ip < end && next_ip [0] == CEE_RET && !vtable_arg) {
-				gboolean has_vtargs = FALSE;
-				int i;
-				/* Prevent inlining of methods with tailcalls (the call stack would be altered) */
-				INLINE_FAILURE ("tailcall");
-				/* keep it simple */
-				for (i = fsig->param_count - 1; !has_vtargs && i >= 0; i--)
-					has_vtargs = MONO_TYPE_ISSTRUCT (mono_method_signature_internal (cmethod)->params [i]);
-				if (!has_vtargs) {
-					if (need_seq_point) {
-						emit_seq_point (cfg, method, ip, FALSE, TRUE);
-						need_seq_point = FALSE;
-					}
-					for (i = 0; i < n; ++i)
-						EMIT_NEW_ARGSTORE (cfg, ins, i, sp [i]);
-					mini_profiler_emit_tail_call (cfg, cmethod);
-					MONO_INST_NEW (cfg, ins, OP_BR);
-					MONO_ADD_INS (cfg->cbb, ins);
-					tblock = start_bblock->out_bb [0];
-					link_bblock (cfg, cfg->cbb, tblock);
-					ins->inst_target_bb = tblock;
-					start_new_bblock = 1;
-					/* skip the CEE_RET, too */
-					if (ip_in_bb (cfg, cfg->cbb, next_ip))
-						skip_ret = TRUE;
-					push_res = FALSE;
-					need_seq_point = FALSE;
-					goto call_end;
-				}
-			}
-			inline_costs += CALL_COST * MIN(10, num_calls++);
-			/*
-			 * Synchronized wrappers.
-			 * Its hard to determine where to replace a method with its synchronized
-			 * wrapper without causing an infinite recursion. The current solution is
-			 * to add the synchronized wrapper in the trampolines, and to
-			 * change the called method to a dummy wrapper, and resolve that wrapper
-			 * to the real method in mono_jit_compile_method ().
-			 */
-			if (cfg->method->wrapper_type == MONO_WRAPPER_SYNCHRONIZED) {
-				MonoMethod *orig = mono_marshal_method_from_wrapper (cfg->method);
-				if (cmethod == orig || (cmethod->is_inflated && mono_method_get_declaring_generic_method (cmethod) == orig)) {
-					cmethod = mono_marshal_get_synchronized_inner_wrapper (cmethod);
-				}
-			}
-			/*
-			 * Making generic calls out of gsharedvt methods.
-			 * This needs to be used for all generic calls, not just ones with a gsharedvt signature, to avoid
-			 * patching gshared method addresses into a gsharedvt method.
-			 */
-			if (make_generic_call_out_of_gsharedvt_method) {
-				if (virtual_) {
-					if (fsig->hasthis && method->klass == mono_defaults.object_class)
-						GSHAREDVT_FAILURE (il_op);
-					if (fsig->generic_param_count) {
-						/* virtual generic call */
-						g_assert (!imt_arg);
-						g_assert (will_have_imt_arg);
-						/* Same as the virtual generic case above */
-						imt_arg = emit_get_rgctx_method (cfg, context_used,
-														 cmethod, MONO_RGCTX_INFO_METHOD);
-						g_assert (imt_arg);
-					} else if (mono_class_is_interface (cmethod->klass) && !imt_arg) {
-						/* This can happen when we call a fully instantiated iface method */
-						g_assert (will_have_imt_arg);
-						imt_arg = emit_get_rgctx_method (cfg, context_used,
-														 cmethod, MONO_RGCTX_INFO_METHOD);
-						g_assert (imt_arg);
-					}
-					/* This is not needed, as the trampoline code will pass one, and it might be passed in the same reg as the imt arg */
-					vtable_arg = NULL;
-				}
-				if ((m_class_get_parent (cmethod->klass) == mono_defaults.multicastdelegate_class) && (!strcmp (cmethod->name, "Invoke")))
-					keep_this_alive = sp [0];
-				MonoRgctxInfoType info_type;
-				if (virtual_ && (cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL))
-					info_type = MONO_RGCTX_INFO_METHOD_GSHAREDVT_OUT_TRAMPOLINE_VIRT;
-				else
-					info_type = MONO_RGCTX_INFO_METHOD_GSHAREDVT_OUT_TRAMPOLINE;
-				addr = emit_get_rgctx_gsharedvt_call (cfg, context_used, fsig, cmethod, info_type);
-				if (cfg->llvm_only) {
-					ins = mini_emit_llvmonly_calli (cfg, fsig, sp, addr);
-					if (inst_tailcall) // FIXME
-						mono_tailcall_print ("missed tailcall llvmonly gsharedvt %s -> %s\n", method->name, cmethod->name);
-				} else {
-					tailcall = tailcall_calli;
-					ins = (MonoInst*)mini_emit_calli_full (cfg, fsig, sp, addr, imt_arg, vtable_arg, tailcall);
-					tailcall_remove_ret |= tailcall;
-				}
-				goto call_end;
-			}
-			/* Generic sharing */
-			/*
-			 * Calls to generic methods from shared code cannot go through the trampoline infrastructure
-			 * in some cases, because the called method might end up being different on every call.
-			 * Load the called method address from the rgctx and do an indirect call in these cases.
-			 * Use this if the callee is gsharedvt sharable too, since
-			 * at runtime we might find an instantiation so the call cannot
-			 * be patched (the 'no_patch' code path in mini-trampolines.c).
-			 */
-			gboolean gshared_indirect;
-			gshared_indirect = context_used && !imt_arg && !array_rank && !delegate_invoke;
-			if (gshared_indirect)
-				gshared_indirect = (!mono_method_is_generic_sharable_full (cmethod, TRUE, FALSE, FALSE) ||
-									!mono_class_generic_sharing_enabled (cmethod->klass) ||
-									gshared_static_virtual);
-			if (gshared_indirect)
-				gshared_indirect = (!virtual_ || MONO_METHOD_IS_FINAL (cmethod) ||
-									!(cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL));
-			if (gshared_indirect) {
-				INLINE_FAILURE ("gshared");
-				g_assert (cfg->gshared && cmethod);
-				g_assert (!addr);
-				if (fsig->hasthis)
-					MONO_EMIT_NEW_CHECK_THIS (cfg, sp [0]->dreg);
-				if (cfg->llvm_only) {
-					if (cfg->gsharedvt && mini_is_gsharedvt_variable_signature (fsig)) {
-						/* Handled in handle_constrained_gsharedvt_call () */
-						g_assert (!gshared_static_virtual);
-						addr = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_GSHAREDVT_OUT_WRAPPER);
-					} else {
-						if (gshared_static_virtual)
-							addr = emit_get_rgctx_virt_method (cfg, -1, constrained_class, cmethod, MONO_RGCTX_INFO_VIRT_METHOD_CODE);
-						else
-							addr = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_METHOD_FTNDESC);
-					}
-					ins = mini_emit_llvmonly_calli (cfg, fsig, sp, addr);
-					if (inst_tailcall) // FIXME
-						mono_tailcall_print ("missed tailcall context_used_llvmonly %s -> %s\n", method->name, cmethod->name);
-				} else {
-					if (gshared_static_virtual) {
-						/*
-						 * cmethod is a static interface method, the actual called method at runtime
-						 * needs to be computed using constrained_class and cmethod.
-						 */
-						addr = emit_get_rgctx_virt_method (cfg, -1, constrained_class, cmethod, MONO_RGCTX_INFO_VIRT_METHOD_CODE);
-					} else {
-						addr = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_GENERIC_METHOD_CODE);
-					}
-					if (inst_tailcall)
-						mono_tailcall_print ("%s tailcall_calli#2 %s -> %s\n", tailcall_calli ? "making" : "missed", method->name, cmethod->name);
-					tailcall = tailcall_calli;
-					ins = (MonoInst*)mini_emit_calli_full (cfg, fsig, sp, addr, imt_arg, vtable_arg, tailcall);
-					tailcall_remove_ret |= tailcall;
-				}
-				goto call_end;
-			}
-			/* Direct calls to icalls */
-			if (direct_icall) {
-				MonoMethod *wrapper;
-				int costs;
-				/* Inline the wrapper */
-				wrapper = mono_marshal_get_native_wrapper (cmethod, TRUE, cfg->compile_aot);
-				costs = inline_method (cfg, wrapper, fsig, sp, ip, cfg->real_offset, TRUE, NULL);
-				g_assert (costs > 0);
-				cfg->real_offset += 5;
-				if (!MONO_TYPE_IS_VOID (fsig->ret))
-					/* *sp is already set by inline_method */
-					ins = *sp;
-				inline_costs += costs;
-				if (inst_tailcall) // FIXME
-					mono_tailcall_print ("missed tailcall direct_icall %s -> %s\n", method->name, cmethod->name);
-				goto call_end;
-			}
-			/* Array methods */
-			if (array_rank) {
-				MonoInst *ldelema_addr;
-				if (strcmp (cmethod->name, "Set") == 0) { /* array Set */
-					MonoInst *val = sp [fsig->param_count];
-					if (val->type == STACK_OBJ) {
-						MonoInst *iargs [ ] = { sp [0], val };
-						mono_emit_jit_icall (cfg, mono_helper_stelem_ref_check, iargs);
-					}
-					ldelema_addr = mini_emit_ldelema_ins (cfg, cmethod, sp, ip, TRUE);
-					if (!mini_debug_options.weak_memory_model && val->type == STACK_OBJ)
-						mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
-					EMIT_NEW_STORE_MEMBASE_TYPE (cfg, ins, fsig->params [fsig->param_count - 1], ldelema_addr->dreg, 0, val->dreg);
-					if (cfg->gen_write_barriers && val->type == STACK_OBJ && !MONO_INS_IS_PCONST_NULL (val))
-						mini_emit_write_barrier (cfg, ldelema_addr, val);
-					if (cfg->gen_write_barriers && mini_is_gsharedvt_klass (cmethod->klass))
-						GSHAREDVT_FAILURE (il_op);
-				} else if (strcmp (cmethod->name, "Get") == 0) { /* array Get */
-					ldelema_addr = mini_emit_ldelema_ins (cfg, cmethod, sp, ip, FALSE);
-					EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, fsig->ret, ldelema_addr->dreg, 0);
-				} else if (strcmp (cmethod->name, "Address") == 0) { /* array Address */
-					if (!m_class_is_valuetype (m_class_get_element_class (cmethod->klass)) && !readonly)
-						mini_emit_check_array_type (cfg, sp [0], cmethod->klass);
-					CHECK_TYPELOAD (cmethod->klass);
-					readonly = FALSE;
-					ldelema_addr = mini_emit_ldelema_ins (cfg, cmethod, sp, ip, FALSE);
-					ins = ldelema_addr;
-				} else {
-					g_assert_not_reached ();
-				}
-				emit_widen = FALSE;
-				if (inst_tailcall) // FIXME
-					mono_tailcall_print ("missed tailcall array_rank %s -> %s\n", method->name, cmethod->name);
-				goto call_end;
-			}
-			ins = mini_redirect_call (cfg, cmethod, fsig, sp, virtual_ ? sp [0] : NULL);
-			if (ins) {
-				if (inst_tailcall) // FIXME
-					mono_tailcall_print ("missed tailcall redirect %s -> %s\n", method->name, cmethod->name);
-				goto call_end;
-			}
-			/* Tail prefix / tailcall optimization */
-			if (tailcall) {
-				/* Prevent inlining of methods with tailcalls (the call stack would be altered) */
-				INLINE_FAILURE ("tailcall");
-			}
-			/*
-			 * Virtual calls in llvm-only mode.
-			 */
-			if (cfg->llvm_only && virtual_ && cmethod && (cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL)) {
-				ins = mini_emit_llvmonly_virtual_call (cfg, cmethod, fsig, context_used, sp);
-				goto call_end;
-			}
-			/* Common call */
-			if (!(cfg->opt & MONO_OPT_AGGRESSIVE_INLINING) && !(method->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING) && !(cmethod->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING) && !method_does_not_return (cmethod))
-				INLINE_FAILURE ("call");
-			common_call = TRUE;
-#ifdef TARGET_WASM
-			/* Push an LMF so these frames can be enumerated during stack walks by mono_arch_unwind_frame () */
-			if (needs_stack_walk && !cfg->deopt) {
-				MonoInst *method_ins;
-				int lmf_reg;
-				emit_push_lmf (cfg);
-				EMIT_NEW_VARLOADA (cfg, ins, cfg->lmf_var, NULL);
-				lmf_reg = ins->dreg;
-				/* The lmf->method field will be used to look up the MonoJitInfo for this method */
-				method_ins = emit_get_rgctx_method (cfg, mono_method_check_context_used (cfg->method), cfg->method, MONO_RGCTX_INFO_METHOD);
-				EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, lmf_reg, MONO_STRUCT_OFFSET (MonoLMF, method), method_ins->dreg);
-			}
-#endif
-call_end:
-			g_assert (!called_is_supported_tailcall || tailcall_method == method);
-			g_assert (!called_is_supported_tailcall || !tailcall || tailcall_cmethod == cmethod);
-			g_assert (!called_is_supported_tailcall || tailcall_fsig == fsig);
-			g_assert (!called_is_supported_tailcall || tailcall_virtual == virtual_);
-			if (common_call) // FIXME goto call_end && !common_call often skips tailcall processing.
-				ins = mini_emit_method_call_full (cfg, cmethod, fsig, tailcall, sp, virtual_ ? sp [0] : NULL,
-												  imt_arg, vtable_arg);
-			/*
-			 * Handle devirt of some A.B.C calls by replacing the result of A.B with a OP_TYPED_OBJREF instruction, so the .C
-			 * call can be devirtualized above.
-			 */
-			if (cmethod)
-				ins = handle_call_res_devirt (cfg, cmethod, ins);
-#ifdef TARGET_WASM
-			if (common_call && needs_stack_walk && !cfg->deopt)
-				emit_pop_lmf (cfg);
-#endif
-			if (noreturn) {
-				MONO_INST_NEW (cfg, ins, OP_NOT_REACHED);
-				MONO_ADD_INS (cfg->cbb, ins);
-			}
-calli_end:
-			if ((tailcall_remove_ret || (common_call && tailcall)) && !cfg->llvm_only) {
-				link_bblock (cfg, cfg->cbb, end_bblock);
-				start_new_bblock = 1;
-				/*
-				 * OP_TAILCALL has no return value, so skip the CEE_RET if it is
-				 * only reachable from this call.
-				 */
-				GET_BBLOCK (cfg, tblock, next_ip);
-				if (tblock == cfg->cbb || tblock->in_count == 0)
-					skip_ret = TRUE;
-				push_res = FALSE;
-				need_seq_point = FALSE;
-			}
-			if (ins_flag & MONO_INST_TAILCALL)
-				mini_test_tailcall (cfg, tailcall);
-			/* End of call, INS should contain the result of the call, if any */
-			if (push_res && !MONO_TYPE_IS_VOID (fsig->ret)) {
-				g_assert (ins);
-				if (emit_widen)
-					*sp++ = mono_emit_widen_call_res (cfg, ins, fsig);
-				else
-					*sp++ = ins;
-			}
-			if (save_last_error) {
-				save_last_error = FALSE;
-#ifdef TARGET_WIN32
-				MONO_INST_NEW (cfg, ins, OP_GET_LAST_ERROR);
-				ins->dreg = alloc_dreg (cfg, STACK_I4);
-				ins->type = STACK_I4;
-				MONO_ADD_INS (cfg->cbb, ins);
-				mono_emit_jit_icall (cfg, mono_marshal_set_last_error_windows, &ins);
-#else
-				mono_emit_jit_icall (cfg, mono_marshal_set_last_error, NULL);
-#endif
-			}
-			if (keep_this_alive) {
-				MonoInst *dummy_use;
-				/* See mini_emit_method_call_full () */
-				EMIT_NEW_DUMMY_USE (cfg, dummy_use, keep_this_alive);
-			}
-			if (cfg->llvm_only && cmethod && method_needs_stack_walk (cfg, cmethod)) {
-				/*
-				 * Clang can convert these calls to tailcalls which screw up the stack
-				 * walk. This happens even when the -fno-optimize-sibling-calls
-				 * option is passed to clang.
-				 * Work around this by emitting a dummy call.
-				 */
-				mono_emit_jit_icall (cfg, mono_dummy_jit_icall, NULL);
-			}
-			CHECK_CFG_EXCEPTION;
-			if (skip_ret) {
-				g_assert (next_ip [0] == CEE_RET);
-				next_ip += 1;
-				il_op = MonoOpcodeEnum_Invalid; // Call or ret? Unclear.
-			}
-			ins_flag = 0;
-			constrained_class = NULL;
-			if (need_seq_point) {
-				if (!(method->flags & METHOD_IMPL_ATTRIBUTE_NATIVE)) {
-					if (emitted_funccall_seq_point) {
-						if (cfg->last_seq_point)
-							cfg->last_seq_point->flags |= MONO_INST_NESTED_CALL;
-					}
-					else
-						emitted_funccall_seq_point = TRUE;
-				}
-				emit_seq_point (cfg, method, next_ip, FALSE, TRUE);
-			}
-			break;
-		}
-		case MONO_CEE_RET:
-			if (!detached_before_ret)
-				mini_profiler_emit_leave (cfg, sig->ret->type != MONO_TYPE_VOID ? sp [-1] : NULL);
-			g_assert (!method_does_not_return (method));
-			if (cfg->method != method) {
-				/* return from inlined method */
-				/*
-				 * If in_count == 0, that means the ret is unreachable due to
-				 * being preceded by a throw. In that case, inline_method () will
-				 * handle setting the return value
-				 * (test case: test_0_inline_throw ()).
-				 */
-				if (return_var && cfg->cbb->in_count) {
-					MonoType *ret_type = mono_method_signature_internal (method)->ret;
-					MonoInst *store;
-					CHECK_STACK (1);
-					--sp;
-					*sp = convert_value (cfg, ret_type, *sp);
-					if ((method->wrapper_type == MONO_WRAPPER_DYNAMIC_METHOD || method->wrapper_type == MONO_WRAPPER_NONE) && target_type_is_incompatible (cfg, ret_type, *sp))
-						UNVERIFIED;
-					EMIT_NEW_TEMPSTORE (cfg, store, return_var->inst_c0, *sp);
-					cfg->ret_var_set = TRUE;
-				}
-			} else {
-				if (cfg->lmf_var && cfg->cbb->in_count && (!cfg->llvm_only || cfg->deopt))
-					emit_pop_lmf (cfg);
-				if (cfg->ret) {
-					MonoType *ret_type = mini_get_underlying_type (mono_method_signature_internal (method)->ret);
-					if (seq_points && !sym_seq_points) {
-						/*
-						 * Place a seq point here too even through the IL stack is not
-						 * empty, so a step over on
-						 * call <FOO>
-						 * ret
-						 * will work correctly.
-						 */
-						NEW_SEQ_POINT (cfg, ins, ip - header->code, TRUE);
-						MONO_ADD_INS (cfg->cbb, ins);
-					}
-					g_assert (!return_var);
-					CHECK_STACK (1);
-					--sp;
-					*sp = convert_value (cfg, ret_type, *sp);
-					if ((method->wrapper_type == MONO_WRAPPER_DYNAMIC_METHOD || method->wrapper_type == MONO_WRAPPER_NONE) && target_type_is_incompatible (cfg, ret_type, *sp))
-						UNVERIFIED;
-					emit_setret (cfg, *sp);
-				}
-			}
-			if (sp != stack_start)
-				UNVERIFIED;
-			MONO_INST_NEW (cfg, ins, OP_BR);
-			ins->inst_target_bb = end_bblock;
-			MONO_ADD_INS (cfg->cbb, ins);
-			link_bblock (cfg, cfg->cbb, end_bblock);
-			start_new_bblock = 1;
-			break;
-		case MONO_CEE_BR_S:
-			MONO_INST_NEW (cfg, ins, OP_BR);
-			GET_BBLOCK (cfg, tblock, target);
-			link_bblock (cfg, cfg->cbb, tblock);
-			ins->inst_target_bb = tblock;
-			if (sp != stack_start) {
-				handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start));
-				sp = stack_start;
-				CHECK_UNVERIFIABLE (cfg);
-			}
-			MONO_ADD_INS (cfg->cbb, ins);
-			start_new_bblock = 1;
-			inline_costs += BRANCH_COST;
-			break;
-		case MONO_CEE_BEQ_S:
-		case MONO_CEE_BGE_S:
-		case MONO_CEE_BGT_S:
-		case MONO_CEE_BLE_S:
-		case MONO_CEE_BLT_S:
-		case MONO_CEE_BNE_UN_S:
-		case MONO_CEE_BGE_UN_S:
-		case MONO_CEE_BGT_UN_S:
-		case MONO_CEE_BLE_UN_S:
-		case MONO_CEE_BLT_UN_S:
-			MONO_INST_NEW (cfg, ins, il_op + BIG_BRANCH_OFFSET);
-			ADD_BINCOND (NULL);
-			sp = stack_start;
-			inline_costs += BRANCH_COST;
-			break;
-		case MONO_CEE_BR:
-			MONO_INST_NEW (cfg, ins, OP_BR);
-			GET_BBLOCK (cfg, tblock, target);
-			link_bblock (cfg, cfg->cbb, tblock);
-			ins->inst_target_bb = tblock;
-			if (sp != stack_start) {
-				handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start));
-				sp = stack_start;
-				CHECK_UNVERIFIABLE (cfg);
-			}
-			MONO_ADD_INS (cfg->cbb, ins);
-			start_new_bblock = 1;
-			inline_costs += BRANCH_COST;
-			break;
-		case MONO_CEE_BRFALSE_S:
-		case MONO_CEE_BRTRUE_S:
-		case MONO_CEE_BRFALSE:
-		case MONO_CEE_BRTRUE: {
-			MonoInst *cmp;
-			gboolean is_true = il_op == MONO_CEE_BRTRUE_S || il_op == MONO_CEE_BRTRUE;
-			if (sp [-1]->type == STACK_VTYPE || sp [-1]->type == STACK_R8)
-				UNVERIFIED;
-			sp--;
-			GET_BBLOCK (cfg, tblock, target);
-			link_bblock (cfg, cfg->cbb, tblock);
-			GET_BBLOCK (cfg, tblock, next_ip);
-			link_bblock (cfg, cfg->cbb, tblock);
-			if (sp != stack_start) {
-				handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start));
-				CHECK_UNVERIFIABLE (cfg);
-			}
-			MONO_INST_NEW(cfg, cmp, OP_ICOMPARE_IMM);
-			cmp->sreg1 = sp [0]->dreg;
-			type_from_op (cfg, cmp, sp [0], NULL);
-			CHECK_TYPE (cmp);
-#if SIZEOF_REGISTER == 4
-			if (cmp->opcode == OP_LCOMPARE_IMM) {
-				/* Convert it to OP_LCOMPARE */
-				MONO_INST_NEW (cfg, ins, OP_I8CONST);
-				ins->type = STACK_I8;
-				ins->dreg = alloc_dreg (cfg, STACK_I8);
-				ins->inst_l = 0;
-				MONO_ADD_INS (cfg->cbb, ins);
-				cmp->opcode = OP_LCOMPARE;
-				cmp->sreg2 = ins->dreg;
-			}
-#endif
-			MONO_ADD_INS (cfg->cbb, cmp);
-			MONO_INST_NEW (cfg, ins, is_true ? CEE_BNE_UN : CEE_BEQ);
-			type_from_op (cfg, ins, sp [0], NULL);
-			MONO_ADD_INS (cfg->cbb, ins);
-			ins->inst_many_bb = (MonoBasicBlock **)mono_mempool_alloc (cfg->mempool, sizeof (gpointer) * 2);
-			GET_BBLOCK (cfg, tblock, target);
-			ins->inst_true_bb = tblock;
-			GET_BBLOCK (cfg, tblock, next_ip);
-			ins->inst_false_bb = tblock;
-			start_new_bblock = 2;
-			sp = stack_start;
-			inline_costs += BRANCH_COST;
-			break;
-		}
-		case MONO_CEE_BEQ:
-		case MONO_CEE_BGE:
-		case MONO_CEE_BGT:
-		case MONO_CEE_BLE:
-		case MONO_CEE_BLT:
-		case MONO_CEE_BNE_UN:
-		case MONO_CEE_BGE_UN:
-		case MONO_CEE_BGT_UN:
-		case MONO_CEE_BLE_UN:
-		case MONO_CEE_BLT_UN:
-			MONO_INST_NEW (cfg, ins, il_op);
-			ADD_BINCOND (NULL);
-			sp = stack_start;
-			inline_costs += BRANCH_COST;
-			break;
-		case MONO_CEE_SWITCH: {
-			MonoInst *src1;
-			MonoBasicBlock **targets;
-			MonoBasicBlock *default_bblock;
-			MonoJumpInfoBBTable *table;
-			int offset_reg = alloc_preg (cfg);
-			int target_reg = alloc_preg (cfg);
-			int table_reg = alloc_preg (cfg);
-			int sum_reg = alloc_preg (cfg);
-			gboolean use_op_switch;
-			n = read32 (ip + 1);
-			--sp;
-			src1 = sp [0];
-			if ((src1->type != STACK_I4) && (src1->type != STACK_PTR))
-				UNVERIFIED;
-			ip += 5;
-			GET_BBLOCK (cfg, default_bblock, next_ip);
-			default_bblock->flags |= BB_INDIRECT_JUMP_TARGET;
-			targets = (MonoBasicBlock **)mono_mempool_alloc (cfg->mempool, sizeof (MonoBasicBlock*) * n);
-			for (int i = 0; i < n; ++i) {
-				GET_BBLOCK (cfg, tblock, next_ip + (gint32)read32 (ip));
-				targets [i] = tblock;
-				targets [i]->flags |= BB_INDIRECT_JUMP_TARGET;
-				ip += 4;
-			}
-			if (sp != stack_start) {
-				/*
-				 * Link the current bb with the targets as well, so handle_stack_args
-				 * will set their in_stack correctly.
-				 */
-				link_bblock (cfg, cfg->cbb, default_bblock);
-				for (int i = 0; i < n; ++i)
-					link_bblock (cfg, cfg->cbb, targets [i]);
-				handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start));
-				sp = stack_start;
-				CHECK_UNVERIFIABLE (cfg);
-				/* Undo the links */
-				mono_unlink_bblock (cfg, cfg->cbb, default_bblock);
-				for (int i = 0; i < n; ++i)
-					mono_unlink_bblock (cfg, cfg->cbb, targets [i]);
-			}
-			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_ICOMPARE_IMM, -1, src1->dreg, n);
-			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_IBGE_UN, default_bblock);
-			for (int i = 0; i < n; ++i)
-				link_bblock (cfg, cfg->cbb, targets [i]);
-			table = (MonoJumpInfoBBTable *)mono_mempool_alloc (cfg->mempool, sizeof (MonoJumpInfoBBTable));
-			table->table = targets;
-			table->table_size = n;
-			use_op_switch = FALSE;
-#ifdef TARGET_ARM
-			/* ARM implements SWITCH statements differently */
-			/* FIXME: Make it use the generic implementation */
-			if (!cfg->compile_aot)
-				use_op_switch = TRUE;
-#endif
-			if (COMPILE_LLVM (cfg))
-				use_op_switch = TRUE;
-			cfg->cbb->has_jump_table = 1;
-			if (use_op_switch) {
-				MONO_INST_NEW (cfg, ins, OP_SWITCH);
-				ins->sreg1 = src1->dreg;
-				ins->inst_p0 = table;
-				ins->inst_many_bb = targets;
-				ins->klass = (MonoClass *)GUINT_TO_POINTER (n);
-				MONO_ADD_INS (cfg->cbb, ins);
-			} else {
-				if (TARGET_SIZEOF_VOID_P == 8)
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_SHL_IMM, offset_reg, src1->dreg, 3);
-				else
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_SHL_IMM, offset_reg, src1->dreg, 2);
-#if SIZEOF_REGISTER == 8
-				/* The upper word might not be zero, and we add it to a 64 bit address later */
-				MONO_EMIT_NEW_UNALU (cfg, OP_ZEXT_I4, offset_reg, offset_reg);
-#endif
-				if (cfg->compile_aot) {
-					MONO_EMIT_NEW_AOTCONST (cfg, table_reg, table, MONO_PATCH_INFO_SWITCH);
-				} else {
-					MONO_INST_NEW (cfg, ins, OP_JUMP_TABLE);
-					ins->inst_c1 = MONO_PATCH_INFO_SWITCH;
-					ins->inst_p0 = table;
-					ins->dreg = table_reg;
-					MONO_ADD_INS (cfg->cbb, ins);
-				}
-				/* FIXME: Use load_memindex */
-				MONO_EMIT_NEW_BIALU (cfg, OP_PADD, sum_reg, table_reg, offset_reg);
-				MONO_EMIT_NEW_LOAD_MEMBASE (cfg, target_reg, sum_reg, 0);
-				MONO_EMIT_NEW_UNALU (cfg, OP_BR_REG, -1, target_reg);
-			}
-			start_new_bblock = 1;
-			inline_costs += BRANCH_COST * 2;
-			break;
-		}
-		case MONO_CEE_LDIND_I1:
-		case MONO_CEE_LDIND_U1:
-		case MONO_CEE_LDIND_I2:
-		case MONO_CEE_LDIND_U2:
-		case MONO_CEE_LDIND_I4:
-		case MONO_CEE_LDIND_U4:
-		case MONO_CEE_LDIND_I8:
-		case MONO_CEE_LDIND_I:
-		case MONO_CEE_LDIND_R4:
-		case MONO_CEE_LDIND_R8:
-		case MONO_CEE_LDIND_REF:
-			--sp;
-			if (!(ins_flag & MONO_INST_NONULLCHECK))
-				MONO_EMIT_NULL_CHECK (cfg, sp [0]->dreg, FALSE);
-			ins = mini_emit_memory_load (cfg, m_class_get_byval_arg (ldind_to_type (il_op)), sp [0], 0, ins_flag);
-			*sp++ = ins;
-			ins_flag = 0;
-			break;
-		case MONO_CEE_STIND_REF:
-		case MONO_CEE_STIND_I1:
-		case MONO_CEE_STIND_I2:
-		case MONO_CEE_STIND_I4:
-		case MONO_CEE_STIND_I8:
-		case MONO_CEE_STIND_R4:
-		case MONO_CEE_STIND_R8:
-		case MONO_CEE_STIND_I: {
-			sp -= 2;
-			if (il_op == MONO_CEE_STIND_REF && sp [1]->type != STACK_OBJ) {
-				/* stind.ref must only be used with object references. */
-				UNVERIFIED;
-			}
-			if (il_op == MONO_CEE_STIND_R4 && sp [1]->type == STACK_R8)
-				sp [1] = convert_value (cfg, m_class_get_byval_arg (mono_defaults.single_class), sp [1]);
-			mini_emit_memory_store (cfg, m_class_get_byval_arg (stind_to_type (il_op)), sp [0], sp [1], ins_flag);
-			ins_flag = 0;
-			inline_costs += 1;
-			break;
-		}
-		case MONO_CEE_MUL: {
-			MONO_INST_NEW (cfg, ins, il_op);
-			sp -= 2;
-			ins->sreg1 = sp [0]->dreg;
-			ins->sreg2 = sp [1]->dreg;
-			type_from_op (cfg, ins, sp [0], sp [1]);
-			CHECK_TYPE (ins);
-			ins->dreg = alloc_dreg ((cfg), (MonoStackType)(ins)->type);
-			/* Use the immediate opcodes if possible */
-			int imm_opcode; imm_opcode = mono_op_to_op_imm_noemul (ins->opcode);
-			if ((sp [1]->opcode == OP_ICONST) && mono_arch_is_inst_imm (ins->opcode, imm_opcode, sp [1]->inst_c0)) {
-				if (imm_opcode != -1) {
-					ins->opcode = GINT_TO_OPCODE (imm_opcode);
-					ins->inst_p1 = (gpointer)(gssize)(sp [1]->inst_c0);
-					ins->sreg2 = -1;
-					NULLIFY_INS (sp [1]);
-				}
-			}
-			MONO_ADD_INS ((cfg)->cbb, (ins));
-			*sp++ = mono_decompose_opcode (cfg, ins);
-			break;
-		}
-		case MONO_CEE_ADD:
-		case MONO_CEE_SUB:
-		case MONO_CEE_DIV:
-		case MONO_CEE_DIV_UN:
-		case MONO_CEE_REM:
-		case MONO_CEE_REM_UN:
-		case MONO_CEE_AND:
-		case MONO_CEE_OR:
-		case MONO_CEE_XOR:
-		case MONO_CEE_SHL:
-		case MONO_CEE_SHR:
-		case MONO_CEE_SHR_UN: {
-			MONO_INST_NEW (cfg, ins, il_op);
-			sp -= 2;
-			ins->sreg1 = sp [0]->dreg;
-			ins->sreg2 = sp [1]->dreg;
-			type_from_op (cfg, ins, sp [0], sp [1]);
-			CHECK_TYPE (ins);
-			add_widen_op (cfg, ins, &sp [0], &sp [1]);
-			ins->dreg = alloc_dreg ((cfg), (MonoStackType)(ins)->type);
-			/* Use the immediate opcodes if possible */
-			int imm_opcode; imm_opcode = mono_op_to_op_imm_noemul (ins->opcode);
-			if (((sp [1]->opcode == OP_ICONST) || (sp [1]->opcode == OP_I8CONST)) &&
-			    mono_arch_is_inst_imm (ins->opcode, imm_opcode, sp [1]->opcode == OP_ICONST ? sp [1]->inst_c0 : sp [1]->inst_l)) {
-				if (imm_opcode != -1) {
-					ins->opcode = GINT_TO_OPCODE (imm_opcode);
-					if (sp [1]->opcode == OP_I8CONST) {
-#if SIZEOF_REGISTER == 8
-						ins->inst_imm = sp [1]->inst_l;
-#else
-						ins->inst_l = sp [1]->inst_l;
-#endif
-					} else {
-						ins->inst_imm = (gssize)(sp [1]->inst_c0);
-					}
-					ins->sreg2 = -1;
-					/* Might be followed by an instruction added by add_widen_op */
-					if (sp [1]->next == NULL)
-						NULLIFY_INS (sp [1]);
-				}
-			}
-			MONO_ADD_INS ((cfg)->cbb, (ins));
-			*sp++ = mono_decompose_opcode (cfg, ins);
-			break;
-		}
-		case MONO_CEE_NEG:
-		case MONO_CEE_NOT:
-		case MONO_CEE_CONV_I1:
-		case MONO_CEE_CONV_I2:
-		case MONO_CEE_CONV_I4:
-		case MONO_CEE_CONV_R4:
-		case MONO_CEE_CONV_R8:
-		case MONO_CEE_CONV_U4:
-		case MONO_CEE_CONV_I8:
-		case MONO_CEE_CONV_U8:
-		case MONO_CEE_CONV_OVF_I8:
-		case MONO_CEE_CONV_OVF_U8:
-		case MONO_CEE_CONV_R_UN:
-			/* Special case this earlier so we have long constants in the IR */
-			if ((il_op == MONO_CEE_CONV_I8 || il_op == MONO_CEE_CONV_U8) && (sp [-1]->opcode == OP_ICONST)) {
-				int data = GTMREG_TO_INT (sp [-1]->inst_c0);
-				sp [-1]->opcode = OP_I8CONST;
-				sp [-1]->type = STACK_I8;
-#if SIZEOF_REGISTER == 8
-				if (il_op == MONO_CEE_CONV_U8)
-					sp [-1]->inst_c0 = (guint32)data;
-				else
-					sp [-1]->inst_c0 = data;
-#else
-				if (il_op == MONO_CEE_CONV_U8)
-					sp [-1]->inst_l = (guint32)data;
-				else
-					sp [-1]->inst_l = data;
-#endif
-				sp [-1]->dreg = alloc_dreg (cfg, STACK_I8);
-			}
-			else {
-				ADD_UNOP (il_op);
-			}
-			break;
-		case MONO_CEE_CONV_OVF_I4:
-		case MONO_CEE_CONV_OVF_I1:
-		case MONO_CEE_CONV_OVF_I2:
-		case MONO_CEE_CONV_OVF_I:
-		case MONO_CEE_CONV_OVF_I1_UN:
-		case MONO_CEE_CONV_OVF_I2_UN:
-		case MONO_CEE_CONV_OVF_I4_UN:
-		case MONO_CEE_CONV_OVF_I8_UN:
-		case MONO_CEE_CONV_OVF_I_UN:
-			if (sp [-1]->type == STACK_R8 || sp [-1]->type == STACK_R4) {
-				/* floats are always signed, _UN has no effect */
-				ADD_UNOP (CEE_CONV_OVF_I8);
-				if (il_op == MONO_CEE_CONV_OVF_I1_UN)
-					ADD_UNOP (MONO_CEE_CONV_OVF_I1);
-				else if (il_op == MONO_CEE_CONV_OVF_I2_UN)
-					ADD_UNOP (MONO_CEE_CONV_OVF_I2);
-				else if (il_op == MONO_CEE_CONV_OVF_I4_UN)
-					ADD_UNOP (MONO_CEE_CONV_OVF_I4);
-				else if (il_op == MONO_CEE_CONV_OVF_I8_UN)
-					;
-				else
-					ADD_UNOP (il_op);
-			} else {
-				ADD_UNOP (il_op);
-			}
-			break;
-		case MONO_CEE_CONV_OVF_U1:
-		case MONO_CEE_CONV_OVF_U2:
-		case MONO_CEE_CONV_OVF_U4:
-		case MONO_CEE_CONV_OVF_U:
-		case MONO_CEE_CONV_OVF_U1_UN:
-		case MONO_CEE_CONV_OVF_U2_UN:
-		case MONO_CEE_CONV_OVF_U4_UN:
-		case MONO_CEE_CONV_OVF_U8_UN:
-		case MONO_CEE_CONV_OVF_U_UN:
-			if (sp [-1]->type == STACK_R8 || sp [-1]->type == STACK_R4) {
-				/* floats are always signed, _UN has no effect */
-				ADD_UNOP (CEE_CONV_OVF_U8);
-				if (TARGET_SIZEOF_VOID_P == 8 && il_op == MONO_CEE_CONV_OVF_U)
-					sp [-1]->type = STACK_PTR; // no additional conversion needed
-				else
-					ADD_UNOP (il_op);
-			} else {
-				ADD_UNOP (il_op);
-			}
-			break;
-		case MONO_CEE_CONV_U2:
-		case MONO_CEE_CONV_U1:
-		case MONO_CEE_CONV_U:
-		case MONO_CEE_CONV_I:
-			ADD_UNOP (il_op);
-			CHECK_CFG_EXCEPTION;
-			break;
-		case MONO_CEE_ADD_OVF:
-		case MONO_CEE_ADD_OVF_UN:
-		case MONO_CEE_MUL_OVF:
-		case MONO_CEE_MUL_OVF_UN:
-		case MONO_CEE_SUB_OVF:
-		case MONO_CEE_SUB_OVF_UN:
-			MONO_INST_NEW (cfg, ins, il_op);
-			sp -= 2;
-			ins->sreg1 = sp [0]->dreg;
-			ins->sreg2 = sp [1]->dreg;
-			type_from_op (cfg, ins, sp [0], sp [1]);
-			CHECK_TYPE (ins);
-			if (ovf_exc)
-				ins->inst_exc_name = ovf_exc;
-			else
-				ins->inst_exc_name = "OverflowException";
-			/* Have to insert a widening op */
-			add_widen_op (cfg, ins, &sp [0], &sp [1]);
-			ins->dreg = alloc_dreg (cfg, (MonoStackType)(ins)->type);
-			MONO_ADD_INS ((cfg)->cbb, ins);
-			/* The opcode might be emulated, so need to special case this */
-			if (ovf_exc && mono_find_jit_opcode_emulation (ins->opcode)) {
-				switch (ins->opcode) {
-				case OP_IMUL_OVF_UN:
-					/* This opcode is just a placeholder, it will be emulated also */
-					ins->opcode = OP_IMUL_OVF_UN_OOM;
-					break;
-				case OP_LMUL_OVF_UN:
-					/* This opcode is just a placeholder, it will be emulated also */
-					ins->opcode = OP_LMUL_OVF_UN_OOM;
-					break;
-				default:
-					g_assert_not_reached ();
-				}
-			}
-			ovf_exc = NULL;
-			*sp++ = mono_decompose_opcode (cfg, ins);
-			break;
-		case MONO_CEE_CPOBJ:
-			GSHAREDVT_FAILURE (il_op);
-			GSHAREDVT_FAILURE (*ip);
-			klass = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (klass);
-			sp -= 2;
-			mini_emit_memory_copy (cfg, sp [0], sp [1], klass, FALSE, ins_flag);
-			ins_flag = 0;
-			break;
-		case MONO_CEE_LDOBJ: {
-			int loc_index = -1;
-			int stloc_len = 0;
-			--sp;
-			klass = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (klass);
-			/* Optimize the common ldobj+stloc combination */
-			if (next_ip < end) {
-				switch (next_ip [0]) {
-				case MONO_CEE_STLOC_S:
-					CHECK_OPSIZE (7);
-					loc_index = next_ip [1];
-					stloc_len = 2;
-					break;
-				case MONO_CEE_STLOC_0:
-				case MONO_CEE_STLOC_1:
-				case MONO_CEE_STLOC_2:
-				case MONO_CEE_STLOC_3:
-					loc_index = next_ip [0] - CEE_STLOC_0;
-					stloc_len = 1;
-					break;
-				default:
-					break;
-				}
-			}
-			if ((loc_index != -1) && ip_in_bb (cfg, cfg->cbb, next_ip)) {
-				CHECK_LOCAL (loc_index);
-				EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), sp [0]->dreg, 0);
-				ins->dreg = cfg->locals [loc_index]->dreg;
-				ins->flags |= ins_flag;
-				il_op = (MonoOpcodeEnum)next_ip [0];
-				next_ip += stloc_len;
-				if (ins_flag & MONO_INST_VOLATILE) {
-					/* Volatile loads have acquire semantics, see 12.6.7 in Ecma 335 */
-					mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_ACQ);
-				}
-				ins_flag = 0;
-				break;
-			}
-			/* Optimize the ldobj+stobj combination */
-			if (next_ip + 4 < end && next_ip [0] == CEE_STOBJ && ip_in_bb (cfg, cfg->cbb, next_ip) && read32 (next_ip + 1) == token) {
-				CHECK_STACK (1);
-				sp --;
-				mini_emit_memory_copy (cfg, sp [0], sp [1], klass, FALSE, ins_flag);
-				il_op = (MonoOpcodeEnum)next_ip [0];
-				next_ip += 5;
-				ins_flag = 0;
-				break;
-			}
-			if (!(ins_flag & MONO_INST_NONULLCHECK))
-				MONO_EMIT_NULL_CHECK (cfg, sp [0]->dreg, FALSE);
-			ins = mini_emit_memory_load (cfg, m_class_get_byval_arg (klass), sp [0], 0, ins_flag);
-			*sp++ = ins;
-			ins_flag = 0;
-			inline_costs += 1;
-			break;
-		}
-		case MONO_CEE_LDSTR:
-			if (method->wrapper_type == MONO_WRAPPER_DYNAMIC_METHOD) {
-				EMIT_NEW_PCONST (cfg, ins, mono_method_get_wrapper_data (method, n));
-				ins->type = STACK_OBJ;
-				*sp = ins;
-			}
-			else if (method->wrapper_type != MONO_WRAPPER_NONE) {
-				MonoInst *iargs [1];
-				char *str = (char *)mono_method_get_wrapper_data (method, n);
-				if (cfg->compile_aot)
-					EMIT_NEW_LDSTRLITCONST (cfg, iargs [0], str);
-				else
-					EMIT_NEW_PCONST (cfg, iargs [0], str);
-				*sp = mono_emit_jit_icall (cfg, mono_string_new_wrapper_internal, iargs);
-			} else {
-				{
-					if (cfg->cbb->out_of_line) {
-						MonoInst *iargs [2];
-						if (image == mono_defaults.corlib) {
-							/*
-							 * Avoid relocations in AOT and save some space by using a
-							 * version of helper_ldstr specialized to mscorlib.
-							 */
-							EMIT_NEW_ICONST (cfg, iargs [0], mono_metadata_token_index (n));
-							*sp = mono_emit_jit_icall (cfg, mono_helper_ldstr_mscorlib, iargs);
-						} else {
-							/* Avoid creating the string object */
-							EMIT_NEW_IMAGECONST (cfg, iargs [0], image);
-							EMIT_NEW_ICONST (cfg, iargs [1], mono_metadata_token_index (n));
-							*sp = mono_emit_jit_icall (cfg, mono_helper_ldstr, iargs);
-						}
-					}
-					else
-					if (cfg->compile_aot) {
-						NEW_LDSTRCONST (cfg, ins, image, n);
-						*sp = ins;
-						MONO_ADD_INS (cfg->cbb, ins);
-					}
-					else {
-						NEW_PCONST (cfg, ins, NULL);
-						ins->type = STACK_OBJ;
-						ins->inst_p0 = mono_ldstr_checked (image, mono_metadata_token_index (n), cfg->error);
-						CHECK_CFG_ERROR;
-						if (!ins->inst_p0)
-							OUT_OF_MEMORY_FAILURE;
-						*sp = ins;
-						MONO_ADD_INS (cfg->cbb, ins);
-					}
-				}
-			}
-			sp++;
-			break;
-		case MONO_CEE_NEWOBJ: {
-			MonoInst *iargs [2];
-			MonoInst this_ins;
-			MonoInst *alloc;
-			MonoInst *vtable_arg = NULL;
-			cmethod = mini_get_method (cfg, method, token, NULL, generic_context);
-			CHECK_CFG_ERROR;
-			fsig = mono_method_get_signature_checked (cmethod, image, token, generic_context, cfg->error);
-			CHECK_CFG_ERROR;
-			mono_save_token_info (cfg, image, token, cmethod);
-			if (!mono_class_init_internal (cmethod->klass))
-				TYPE_LOAD_ERROR (cmethod->klass);
-			context_used = mini_method_check_context_used (cfg, cmethod);
-			if (!dont_verify && !cfg->skip_visibility) {
-				MonoMethod *cil_method = cmethod;
-				MonoMethod *target_method = cil_method;
-				if (method->is_inflated) {
-					MonoGenericContainer *container = mono_method_get_generic_container(method_definition);
-					MonoGenericContext *context = (container != NULL ? &container->context : NULL);
-					target_method = mini_get_method_allow_open (method, token, NULL, context, cfg->error);
-					CHECK_CFG_ERROR;
-				}
-				if (!mono_method_can_access_method (method_definition, target_method) &&
-					!mono_method_can_access_method (method, cil_method))
-					emit_method_access_failure (cfg, method, cil_method);
-			}
-			if (cfg->gshared && cmethod && cmethod->klass != method->klass && mono_class_is_ginst (cmethod->klass) && mono_method_is_generic_sharable (cmethod, TRUE) && mono_class_needs_cctor_run (cmethod->klass, method)) {
-				emit_class_init (cfg, cmethod->klass, FALSE);
-				CHECK_TYPELOAD (cmethod->klass);
-			}
-			/*
-			if (cfg->gsharedvt) {
-				if (mini_is_gsharedvt_variable_signature (sig))
-					GSHAREDVT_FAILURE (il_op);
-			}
-			*/
-			n = fsig->param_count;
-			CHECK_STACK (n);
-			if (cfg->gsharedvt_min && mini_is_gsharedvt_variable_signature (fsig))
-				GSHAREDVT_FAILURE (il_op);
-			/*
-			 * Generate smaller code for the common newobj <exception> instruction in
-			 * argument checking code.
-			 */
-			if (cfg->cbb->out_of_line && m_class_get_image (cmethod->klass) == mono_defaults.corlib &&
-				is_exception_class (cmethod->klass) && n <= 2 &&
-			    ((n < 1) || (!m_type_is_byref (fsig->params [0]) && fsig->params [0]->type == MONO_TYPE_STRING)) &&
-			    ((n < 2) || (!m_type_is_byref (fsig->params [1]) && fsig->params [1]->type == MONO_TYPE_STRING))) {
-				MonoInst *ex_iargs [3];
-				sp -= n;
-				EMIT_NEW_ICONST (cfg, ex_iargs [0], m_class_get_type_token (cmethod->klass));
-				switch (n) {
-				case 0:
-					*sp ++ = mono_emit_jit_icall (cfg, mono_create_corlib_exception_0, ex_iargs);
-					break;
-				case 1:
-					ex_iargs [1] = sp [0];
-					*sp ++ = mono_emit_jit_icall (cfg, mono_create_corlib_exception_1, ex_iargs);
-					break;
-				case 2:
-					ex_iargs [1] = sp [0];
-					ex_iargs [2] = sp [1];
-					*sp ++ = mono_emit_jit_icall (cfg, mono_create_corlib_exception_2, ex_iargs);
-					break;
-				default:
-					g_assert_not_reached ();
-				}
-				inline_costs += 5;
-				break;
-			}
-			/* move the args to allow room for 'this' in the first position */
-			while (n--) {
-				--sp;
-				sp [1] = sp [0];
-			}
-			for (int i = 0; i < fsig->param_count; ++i)
-				sp [i + fsig->hasthis] = convert_value (cfg, fsig->params [i], sp [i + fsig->hasthis]);
-			/* check_call_signature () requires sp[0] to be set */
-			this_ins.type = STACK_OBJ;
-			sp [0] = &this_ins;
-			if (check_call_signature (cfg, fsig, sp))
-				UNVERIFIED;
-			iargs [0] = NULL;
-			if (mini_class_is_system_array (cmethod->klass)) {
-				*sp = emit_get_rgctx_method (cfg, context_used,
-											 cmethod, MONO_RGCTX_INFO_METHOD);
-				MonoJitICallId function = MONO_JIT_ICALL_ZeroIsReserved;
-				int rank = m_class_get_rank (cmethod->klass);
-				int param_count = fsig->param_count;
-				/* Optimize the common cases, use ctor using length for each rank (no lbound). */
-				if (param_count == rank) {
-					switch (param_count) {
-					case 1: function = MONO_JIT_ICALL_mono_array_new_1;
-						break;
-					case 2: function = MONO_JIT_ICALL_mono_array_new_2;
-						break;
-					case 3: function = MONO_JIT_ICALL_mono_array_new_3;
-						break;
-					case 4: function = MONO_JIT_ICALL_mono_array_new_4;
-						break;
-					default:
-						break;
-					}
-				}
-				/* Regular case, rank > 4 or legnth, lbound specified per rank. */
-				if (function == MONO_JIT_ICALL_ZeroIsReserved) {
-					if  (!array_new_localalloc_ins) {
-						MONO_INST_NEW (cfg, array_new_localalloc_ins, OP_LOCALLOC_IMM);
-						array_new_localalloc_ins->dreg = alloc_preg (cfg);
-						cfg->flags |= MONO_CFG_HAS_ALLOCA;
-						MONO_ADD_INS (init_localsbb, array_new_localalloc_ins);
-					}
-					array_new_localalloc_ins->inst_imm = MAX (array_new_localalloc_ins->inst_imm, param_count * GUINT_TO_INT(sizeof (target_mgreg_t)));
-					int dreg = array_new_localalloc_ins->dreg;
-					if (2 * rank == param_count) {
-						/* [lbound, length, lbound, length, ...]
-						 * mono_array_new_n_icall expects a non-interleaved list of
-						 * lbounds and lengths, so deinterleave here.
-						 */
-						for (int l = 0; l < 2; ++l) {
-							int src = l;
-							int dst = l * rank;
-							for (int r = 0; r < rank; ++r, src += 2, ++dst) {
-								NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, dreg, dst * sizeof (target_mgreg_t), sp [src + 1]->dreg);
-								MONO_ADD_INS (cfg->cbb, ins);
-							}
-						}
-					} else {
-						/* [length, length, length, ...] */
-						for (int i = 0; i < param_count; ++i) {
-							NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_REG, dreg, i * sizeof (target_mgreg_t), sp [i + 1]->dreg);
-							MONO_ADD_INS (cfg->cbb, ins);
-						}
-					}
-					EMIT_NEW_ICONST (cfg, ins, param_count);
-					sp [1] = ins;
-					EMIT_NEW_UNALU (cfg, ins, OP_MOVE, alloc_preg (cfg), dreg);
-					ins->type = STACK_PTR;
-					sp [2] = ins;
-					function = MONO_JIT_ICALL_mono_array_new_n_icall;
-				}
-				alloc = mono_emit_jit_icall_id (cfg, function, sp);
-			} else if (cmethod->string_ctor) {
-				g_assert (!context_used);
-				g_assert (!vtable_arg);
-				/* we simply pass a null pointer */
-				EMIT_NEW_PCONST (cfg, *sp, NULL);
-				/* now call the string ctor */
-				alloc = mini_emit_method_call_full (cfg, cmethod, fsig, FALSE, sp, NULL, NULL, NULL);
-			} else {
-				if (m_class_is_valuetype (cmethod->klass)) {
-					iargs [0] = mono_compile_create_var (cfg, m_class_get_byval_arg (cmethod->klass), OP_LOCAL);
-					mini_emit_init_rvar (cfg, iargs [0]->dreg, m_class_get_byval_arg (cmethod->klass));
-					EMIT_NEW_TEMPLOADA (cfg, *sp, iargs [0]->inst_c0);
-					alloc = NULL;
-					/*
-					 * The code generated by mini_emit_virtual_call () expects
-					 * iargs [0] to be a boxed instance, but luckily the vcall
-					 * will be transformed into a normal call there.
-					 */
-				} else if (context_used) {
-					alloc = handle_alloc (cfg, cmethod->klass, FALSE, context_used);
-					*sp = alloc;
-				} else {
-					MonoVTable *vtable = NULL;
-					if (!cfg->compile_aot)
-						vtable = mono_class_vtable_checked (cmethod->klass, cfg->error);
-					CHECK_CFG_ERROR;
-					CHECK_TYPELOAD (cmethod->klass);
-					/*
-					 * TypeInitializationExceptions thrown from the mono_runtime_class_init
-					 * call in mono_jit_runtime_invoke () can abort the finalizer thread.
-					 * As a workaround, we call class cctors before allocating objects.
-					 */
-					if (mini_field_access_needs_cctor_run (cfg, method, cmethod->klass, vtable) && !(g_slist_find (class_inits, cmethod->klass))) {
-						emit_class_init (cfg, cmethod->klass, TRUE);
-						if (cfg->verbose_level > 2)
-							printf ("class %s.%s needs init call for ctor\n", m_class_get_name_space (cmethod->klass), m_class_get_name (cmethod->klass));
-						class_inits = g_slist_prepend (class_inits, cmethod->klass);
-					}
-					alloc = handle_alloc (cfg, cmethod->klass, FALSE, 0);
-					*sp = alloc;
-				}
-				CHECK_CFG_EXCEPTION; /*for handle_alloc*/
-				if (alloc)
-					MONO_EMIT_NEW_UNALU (cfg, OP_NOT_NULL, -1, alloc->dreg);
-				/* Now call the actual ctor */
-				int ctor_inline_costs = 0;
-				handle_ctor_call (cfg, cmethod, fsig, context_used, sp, ip, &ctor_inline_costs);
-				if (!COMPILE_LLVM(cfg) || !(cmethod->iflags & METHOD_IMPL_ATTRIBUTE_AGGRESSIVE_INLINING))
-					inline_costs += ctor_inline_costs;
-				CHECK_CFG_EXCEPTION;
-			}
-			if (alloc == NULL) {
-				/* Valuetype */
-				EMIT_NEW_TEMPLOAD (cfg, ins, iargs [0]->inst_c0);
-				mini_type_to_eval_stack_type (cfg, m_class_get_byval_arg (ins->klass), ins);
-				*sp++= ins;
-			} else {
-				*sp++ = alloc;
-			}
-			inline_costs += 5;
-			if (!(seq_point_locs && mono_bitset_test_fast (seq_point_locs, next_ip - header->code)))
-				emit_seq_point (cfg, method, next_ip, FALSE, TRUE);
-			break;
-		}
-		case MONO_CEE_CASTCLASS:
-		case MONO_CEE_ISINST: {
-			--sp;
-			klass = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (klass);
-			if (sp [0]->type != STACK_OBJ)
-				UNVERIFIED;
-			MONO_INST_NEW (cfg, ins, (il_op == MONO_CEE_ISINST) ? OP_ISINST : OP_CASTCLASS);
-			ins->dreg = alloc_preg (cfg);
-			ins->sreg1 = (*sp)->dreg;
-			ins->klass = klass;
-			ins->type = STACK_OBJ;
-			MONO_ADD_INS (cfg->cbb, ins);
-			CHECK_CFG_EXCEPTION;
-			*sp++ = ins;
-			cfg->flags |= MONO_CFG_HAS_TYPE_CHECK;
-			break;
-		}
-		case MONO_CEE_UNBOX_ANY: {
-			MonoInst *res, *addr;
-			--sp;
-			klass = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (klass);
-			mono_save_token_info (cfg, image, token, klass);
-			context_used = mini_class_check_context_used (cfg, klass);
-			if (cfg->gsharedvt_min && mini_is_gsharedvt_variable_klass (klass))
-				GSHAREDVT_FAILURE (il_op);
-			if (mini_is_gsharedvt_klass (klass)) {
-				res = handle_unbox_gsharedvt (cfg, klass, *sp);
-				inline_costs += 2;
-			} else if (mini_class_is_reference (klass)) {
-				if (MONO_INS_IS_PCONST_NULL (*sp)) {
-					EMIT_NEW_PCONST (cfg, res, NULL);
-					res->type = STACK_OBJ;
-				} else {
-					MONO_INST_NEW (cfg, res, OP_CASTCLASS);
-					res->dreg = alloc_preg (cfg);
-					res->sreg1 = (*sp)->dreg;
-					res->klass = klass;
-					res->type = STACK_OBJ;
-					MONO_ADD_INS (cfg->cbb, res);
-					cfg->flags |= MONO_CFG_HAS_TYPE_CHECK;
-				}
-			} else if (mono_class_is_nullable (klass)) {
-				res = handle_unbox_nullable (cfg, *sp, klass, context_used);
-			} else {
-				addr = mini_handle_unbox (cfg, klass, *sp, context_used);
-				/* LDOBJ */
-				EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), addr->dreg, 0);
-				res = ins;
-				inline_costs += 2;
-			}
-			*sp ++ = res;
-			break;
-		}
-		case MONO_CEE_BOX: {
-			MonoInst *val;
-			MonoClass *enum_class;
-			MonoMethod *has_flag;
-			MonoMethodSignature *has_flag_sig;
-			--sp;
-			val = *sp;
-			klass = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (klass);
-			mono_save_token_info (cfg, image, token, klass);
-			context_used = mini_class_check_context_used (cfg, klass);
-			if (mini_class_is_reference (klass)) {
-				*sp++ = val;
-				break;
-			}
-			val = convert_value (cfg, m_class_get_byval_arg (klass), val);
-			if (klass == mono_defaults.void_class)
-				UNVERIFIED;
-			if (target_type_is_incompatible (cfg, m_class_get_byval_arg (klass), val))
-				UNVERIFIED;
-			/* frequent check in generic code: box (struct), brtrue */
-			/*
-			 * Look for:
-			 *
-			 *   <push int/long ptr>
-			 *   <push int/long>
-			 *   box MyFlags
-			 *   constrained. MyFlags
-			 *   callvirt instance bool class [mscorlib] System.Enum::HasFlag (class [mscorlib] System.Enum)
-			 *
-			 * If we find this sequence and the operand types on box and constrained
-			 * are equal, we can emit a specialized instruction sequence instead of
-			 * the very slow HasFlag () call.
-			 * This code sequence is generated by older mcs/csc, the newer one is handled in
-			 * emit_inst_for_method ().
-			 */
-			guint32 constrained_token;
-			guint32 callvirt_token;
-			if ((cfg->opt & MONO_OPT_INTRINS) &&
-			    next_ip < end && ip_in_bb (cfg, cfg->cbb, next_ip) &&
-			    (ip = il_read_constrained (next_ip, end, &constrained_token)) &&
-			    ip_in_bb (cfg, cfg->cbb, ip) &&
-			    (ip = il_read_callvirt (ip, end, &callvirt_token)) &&
-			    ip_in_bb (cfg, cfg->cbb, ip) &&
-			    m_class_is_enumtype (klass) &&
-			    (enum_class = mini_get_class (method, constrained_token, generic_context)) &&
-			    (has_flag = mini_get_method (cfg, method, callvirt_token, NULL, generic_context)) &&
-			    has_flag->klass == mono_defaults.enum_class &&
-			    !strcmp (has_flag->name, "HasFlag") &&
-			    (has_flag_sig = mono_method_signature_internal (has_flag)) &&
-			    has_flag_sig->hasthis &&
-			    has_flag_sig->param_count == 1) {
-				CHECK_TYPELOAD (enum_class);
-				if (enum_class == klass) {
-					MonoInst *enum_this, *enum_flag;
-					next_ip = ip;
-					il_op = MONO_CEE_CALLVIRT;
-					--sp;
-					enum_this = sp [0];
-					enum_flag = sp [1];
-					*sp++ = mini_handle_enum_has_flag (cfg, klass, enum_this, -1, enum_flag);
-					break;
-				}
-			}
-			guint32 unbox_any_token;
-			/*
-			 * Common in generic code:
-			 * box T1, unbox.any T2.
-			 */
-			if ((cfg->opt & MONO_OPT_INTRINS) &&
-			    next_ip < end && ip_in_bb (cfg, cfg->cbb, next_ip) &&
-			    (ip = il_read_unbox_any (next_ip, end, &unbox_any_token))) {
-				MonoClass *unbox_klass = mini_get_class (method, unbox_any_token, generic_context);
-				CHECK_TYPELOAD (unbox_klass);
-				if (klass == unbox_klass) {
-					next_ip = ip;
-					*sp++ = val;
-					break;
-				}
-			}
-			guint32 gettype_token;
-			if ((ip = il_read_call(next_ip, end, &gettype_token)) && ip_in_bb (cfg, cfg->cbb, ip)) {
-				MonoMethod* gettype_method = mini_get_method (cfg, method, gettype_token, NULL, generic_context);
-				if (!strcmp (gettype_method->name, "GetType") && gettype_method->klass == mono_defaults.object_class) {
-					mono_class_init_internal(klass);
-					if (mono_class_get_checked (m_class_get_image (klass), m_class_get_type_token (klass), error) == klass) {
-						if (cfg->compile_aot) {
-							EMIT_NEW_TYPE_FROM_HANDLE_CONST (cfg, ins, m_class_get_image (klass), m_class_get_type_token (klass), generic_context);
-						} else {
-							MonoType *klass_type = m_class_get_byval_arg (klass);
-							MonoReflectionType* reflection_type = mono_type_get_object_checked (klass_type, cfg->error);
-							EMIT_NEW_PCONST (cfg, ins, reflection_type);
-						}
-						ins->type = STACK_OBJ;
-						ins->klass = mono_defaults.systemtype_class;
-						*sp++ = ins;
-						next_ip = ip;
-						break;
-					}
-				}
-			}
-			guchar* ldnull_ip;
-			if ((ldnull_ip = il_read_op (next_ip, end, CEE_LDNULL, MONO_CEE_LDNULL)) && ip_in_bb (cfg, cfg->cbb, ldnull_ip)) {
-				gboolean is_eq = FALSE, is_neq = FALSE;
-				if ((ip = il_read_op (ldnull_ip, end, CEE_PREFIX1, MONO_CEE_CEQ)))
-					is_eq = TRUE;
-				else if ((ip = il_read_op (ldnull_ip, end, CEE_PREFIX1, MONO_CEE_CGT_UN)))
-					is_neq = TRUE;
-				if ((is_eq || is_neq) && ip_in_bb (cfg, cfg->cbb, ip) &&
-					!mono_class_is_nullable (klass) && !mini_is_gsharedvt_klass (klass)) {
-					next_ip = ip;
-					il_op = (MonoOpcodeEnum) (is_eq ? CEE_LDC_I4_0 : CEE_LDC_I4_1);
-					EMIT_NEW_ICONST (cfg, ins, is_eq ? 0 : 1);
-					ins->type = STACK_I4;
-					*sp++ = ins;
-					break;
-				}
-			}
-			guint32 isinst_tk = 0;
-			if ((ip = il_read_op_and_token (next_ip, end, CEE_ISINST, MONO_CEE_ISINST, &isinst_tk)) &&
-				ip_in_bb (cfg, cfg->cbb, ip)) {
-				MonoClass *isinst_class = mini_get_class (method, isinst_tk, generic_context);
-				if (!mono_class_is_nullable (klass) && !mono_class_is_nullable (isinst_class) &&
-					!mini_is_gsharedvt_variable_klass (klass) && !mini_is_gsharedvt_variable_klass (isinst_class) &&
-					!mono_class_is_open_constructed_type (m_class_get_byval_arg (klass)) &&
-					!mono_class_is_open_constructed_type (m_class_get_byval_arg (isinst_class))) {
-					guchar* br_ip = NULL;
-					if ((br_ip = il_read_brtrue (ip, end, &target)) || (br_ip = il_read_brtrue_s (ip, end, &target)) ||
-						(br_ip = il_read_brfalse (ip, end, &target)) || (br_ip = il_read_brfalse_s (ip, end, &target))) {
-						gboolean isinst = mono_class_is_assignable_from_internal (isinst_class, klass);
-						next_ip = ip;
-						il_op = (MonoOpcodeEnum) (isinst ? CEE_LDC_I4_1 : CEE_LDC_I4_0);
-						EMIT_NEW_ICONST (cfg, ins, isinst ? 1 : 0);
-						ins->type = STACK_I4;
-						*sp++ = ins;
-						break;
-					}
-					ldnull_ip = NULL;
-					if ((ldnull_ip = il_read_op (ip, end, CEE_LDNULL, MONO_CEE_LDNULL)) && ip_in_bb (cfg, cfg->cbb, ldnull_ip)) {
-						gboolean is_eq = FALSE, is_neq = FALSE;
-						if ((ip = il_read_op (ldnull_ip, end, CEE_PREFIX1, MONO_CEE_CEQ)))
-							is_eq = TRUE;
-						else if ((ip = il_read_op (ldnull_ip, end, CEE_PREFIX1, MONO_CEE_CGT_UN)))
-							is_neq = TRUE;
-						if ((is_eq || is_neq) && ip_in_bb (cfg, cfg->cbb, ip) &&
-							!mono_class_is_nullable (klass) && !mini_is_gsharedvt_klass (klass)) {
-							gboolean isinst = mono_class_is_assignable_from_internal (isinst_class, klass);
-							next_ip = ip;
-							if (is_eq)
-								isinst = !isinst;
-							il_op = (MonoOpcodeEnum) (isinst ? CEE_LDC_I4_1 : CEE_LDC_I4_0);
-							EMIT_NEW_ICONST (cfg, ins, isinst ? 1 : 0);
-							ins->type = STACK_I4;
-							*sp++ = ins;
-							break;
-						}
-					}
-					guchar* unbox_ip = NULL;
-					guint32 unbox_token = 0;
-					if ((unbox_ip = il_read_unbox_any (ip, end, &unbox_token)) && ip_in_bb (cfg, cfg->cbb, unbox_ip)) {
-						MonoClass *unbox_klass = mini_get_class (method, unbox_token, generic_context);
-						CHECK_TYPELOAD (unbox_klass);
-						if (!mono_class_is_nullable (unbox_klass) &&
-							!mini_is_gsharedvt_klass (unbox_klass) &&
-							klass == isinst_class &&
-							klass == unbox_klass)
-						{
-							*sp++ = val;
-							next_ip = unbox_ip;
-							break;
-						}
-					}
-				}
-			}
-			guint32 callvirt_proc_token;
-			if (!((cfg->compile_aot || cfg->compile_llvm) && !mono_class_is_def(klass)) && // we cannot devirtualize in AOT when using generics
-				next_ip < end &&
-				il_read_callvirt (next_ip, end, &callvirt_proc_token) &&
-				ip_in_bb (cfg, cfg->cbb, next_ip) ) {
-				MonoMethod* iface_method;
-				MonoMethodSignature* iface_method_sig;
-				if (val &&
-					val->flags != MONO_INST_FAULT && // not null
-					!mono_class_is_nullable (klass) &&
-					!mini_is_gsharedvt_klass (klass) &&
-					(iface_method = mini_get_method (cfg, method, callvirt_proc_token, NULL, generic_context)) &&
-					(iface_method_sig = mono_method_signature_internal (iface_method)) && // callee signture is healthy
-					iface_method_sig->hasthis && 
-					iface_method_sig->param_count == 0 && // the callee has no args (other than this)
-					!iface_method_sig->has_type_parameters &&
-					iface_method_sig->generic_param_count == 0) { // and no type params, apparently virtual generic methods require special handling
-					if (!m_class_is_inited (iface_method->klass)) {
-						if (!mono_class_init_internal (iface_method->klass))
-							TYPE_LOAD_ERROR (iface_method->klass);
-					}
-					ERROR_DECL (struct_method_error);
-					MonoMethod* struct_method = mono_class_get_virtual_method (klass, iface_method, struct_method_error);
-					if (is_ok (struct_method_error)) {
-						MonoMethodSignature* struct_method_sig = mono_method_signature_internal (struct_method);
-						if (!struct_method ||
-							!MONO_METHOD_IS_FINAL (struct_method) ||
-							!struct_method_sig ||
-							struct_method_sig->has_type_parameters ||
-							!mono_method_can_access_method (method, struct_method)) {
-						} else if (val->opcode == OP_TYPED_OBJREF) {
-							*sp++ = val;
-							cmethod_override = struct_method;
-							break;
-						} else {
-							MonoInst* srcvar = get_vreg_to_inst (cfg, val->dreg);
-							if (!srcvar)
-								srcvar = mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (klass), OP_LOCAL, val->dreg);
-							EMIT_NEW_VARLOADA (cfg, ins, srcvar, m_class_get_byval_arg (klass));
-							*sp++= ins;
-							cmethod_override = struct_method;
-							break;
-						}
-					} else {
-						mono_error_cleanup (struct_method_error);
-					}
-				} 
-			}			
-			gboolean is_true;
-			if (!mono_class_is_nullable (klass) &&
-				!mini_is_gsharedvt_klass (klass) &&
-				next_ip < end && ip_in_bb (cfg, cfg->cbb, next_ip) &&
-				( (is_true = !!(ip = il_read_brtrue   (next_ip, end, &target))) ||
-				  (is_true = !!(ip = il_read_brtrue_s (next_ip, end, &target))) ||
-					       (ip = il_read_brfalse  (next_ip, end, &target))  ||
-					       (ip = il_read_brfalse_s (next_ip, end, &target)))) {
-				int dreg;
-				MonoBasicBlock *true_bb, *false_bb;
-				il_op = (MonoOpcodeEnum)next_ip [0];
-				next_ip = ip;
-				if (cfg->verbose_level > 3) {
-					printf ("converting (in B%d: stack: %d) %s", cfg->cbb->block_num, GPTRDIFF_TO_INT (sp - stack_start), mono_disasm_code_one (NULL, method, ip, NULL));
-					printf ("<box+brtrue opt>\n");
-				}
-				/*
-				 * We need to link both bblocks, since it is needed for handling stack
-				 * arguments correctly (See test_0_box_brtrue_opt_regress_81102).
-				 * Branching to only one of them would lead to inconsistencies, so
-				 * generate an ICONST+BRTRUE, the branch opts will get rid of them.
-				 */
-				GET_BBLOCK (cfg, true_bb, target);
-				GET_BBLOCK (cfg, false_bb, next_ip);
-				mono_link_bblock (cfg, cfg->cbb, true_bb);
-				mono_link_bblock (cfg, cfg->cbb, false_bb);
-				if (sp != stack_start) {
-					handle_stack_args (cfg, stack_start, GPTRDIFF_TO_INT (sp - stack_start));
-					sp = stack_start;
-					CHECK_UNVERIFIABLE (cfg);
-				}
-				if (COMPILE_LLVM (cfg)) {
-					dreg = alloc_ireg (cfg);
-					MONO_EMIT_NEW_ICONST (cfg, dreg, 0);
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, dreg, is_true ? 0 : 1);
-					MONO_EMIT_NEW_BRANCH_BLOCK2 (cfg, OP_IBEQ, true_bb, false_bb);
-				} else {
-					/* The JIT can't eliminate the iconst+compare */
-					MONO_INST_NEW (cfg, ins, OP_BR);
-					ins->inst_target_bb = is_true ? true_bb : false_bb;
-					MONO_ADD_INS (cfg->cbb, ins);
-				}
-				start_new_bblock = 1;
-				break;
-			}
-			if (m_class_is_enumtype (klass) && !mini_is_gsharedvt_klass (klass) && !(val->type == STACK_I8 && TARGET_SIZEOF_VOID_P == 4)) {
-				/* Can't do this with 64 bit enums on 32 bit since the vtype decomp pass is ran after the long decomp pass */
-				if (val->opcode == OP_ICONST) {
-					MONO_INST_NEW (cfg, ins, OP_BOX_ICONST);
-					ins->type = STACK_OBJ;
-					ins->klass = klass;
-					ins->inst_c0 = val->inst_c0;
-					ins->dreg = alloc_dreg (cfg, (MonoStackType)val->type);
-				} else {
-					MONO_INST_NEW (cfg, ins, OP_BOX);
-					ins->type = STACK_OBJ;
-					ins->klass = klass;
-					ins->sreg1 = val->dreg;
-					ins->dreg = alloc_dreg (cfg, (MonoStackType)val->type);
-				}
-				MONO_ADD_INS (cfg->cbb, ins);
-				*sp++ = ins;
-			} else {
-				*sp++ = mini_emit_box (cfg, val, klass, context_used);
-			}
-			CHECK_CFG_EXCEPTION;
-			inline_costs += 1;
-			break;
-		}
-		case MONO_CEE_UNBOX: {
-			--sp;
-			klass = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (klass);
-			mono_save_token_info (cfg, image, token, klass);
-			context_used = mini_class_check_context_used (cfg, klass);
-			if (mono_class_is_nullable (klass)) {
-				MonoInst *val;
-				val = handle_unbox_nullable (cfg, *sp, klass, context_used);
-				EMIT_NEW_VARLOADA (cfg, ins, get_vreg_to_inst (cfg, val->dreg), m_class_get_byval_arg (val->klass));
-				*sp++= ins;
-			} else {
-				ins = mini_handle_unbox (cfg, klass, *sp, context_used);
-				*sp++ = ins;
-			}
-			inline_costs += 2;
-			break;
-		}
-		case MONO_CEE_LDFLD:
-		case MONO_CEE_LDFLDA:
-		case MONO_CEE_STFLD:
-		case MONO_CEE_LDSFLD:
-		case MONO_CEE_LDSFLDA:
-		case MONO_CEE_STSFLD: {
-			MonoClassField *field;
-			guint foffset;
-			gboolean is_instance;
-			gpointer addr = NULL;
-			gboolean is_special_static;
-			MonoType *ftype;
-			MonoInst *store_val = NULL;
-			MonoInst *thread_ins;
-			is_instance = (il_op == MONO_CEE_LDFLD || il_op == MONO_CEE_LDFLDA || il_op == MONO_CEE_STFLD);
-			if (is_instance) {
-				if (il_op == MONO_CEE_STFLD) {
-					sp -= 2;
-					store_val = sp [1];
-				} else {
-					--sp;
-				}
-				if (sp [0]->type == STACK_I4 || sp [0]->type == STACK_I8 || sp [0]->type == STACK_R8)
-					UNVERIFIED;
-				if (il_op != MONO_CEE_LDFLD && sp [0]->type == STACK_VTYPE)
-					UNVERIFIED;
-			} else {
-				if (il_op == MONO_CEE_STSFLD) {
-					sp--;
-					store_val = sp [0];
-				}
-			}
-			if (method->wrapper_type != MONO_WRAPPER_NONE) {
-				field = (MonoClassField *)mono_method_get_wrapper_data (method, token);
-				klass = m_field_get_parent (field);
-			}
-			else {
-				klass = NULL;
-				field = mono_field_from_token_checked (image, token, &klass, generic_context, cfg->error);
-				if (!field)
-					CHECK_TYPELOAD (klass);
-				CHECK_CFG_ERROR;
-			}
-			if (!dont_verify && !cfg->skip_visibility && !mono_method_can_access_field (method, field))
-				FIELD_ACCESS_FAILURE (method, field);
-			mono_class_init_internal (klass);
-			mono_class_setup_fields (klass);
-			ftype = mono_field_get_type_internal (field);
-			/*
-			 * LDFLD etc. is usable on static fields as well, so convert those cases to
-			 * the static case.
-			 */
-			if (is_instance && ftype->attrs & FIELD_ATTRIBUTE_STATIC) {
-				switch (il_op) {
-				case MONO_CEE_LDFLD:
-					il_op = MONO_CEE_LDSFLD;
-					break;
-				case MONO_CEE_STFLD:
-					il_op = MONO_CEE_STSFLD;
-					break;
-				case MONO_CEE_LDFLDA:
-					il_op = MONO_CEE_LDSFLDA;
-					break;
-				default:
-					g_assert_not_reached ();
-				}
-				is_instance = FALSE;
-			}
-			context_used = mini_class_check_context_used (cfg, klass);
-			if (il_op == MONO_CEE_LDSFLD) {
-				ins = mini_emit_inst_for_field_load (cfg, field);
-				if (ins) {
-					*sp++ = ins;
-					goto field_access_end;
-				}
-			}
-			/* INSTANCE CASE */
-			if (is_instance)
-				g_assert (field->offset);
-			/* metadata-update: no hot reload in the JIT.  But if it was supported,
-			 * field->offset here could be wrong for added (m_field_is_from_update)
-			 * fields */
-			foffset = m_class_is_valuetype (klass) ? field->offset - MONO_ABI_SIZEOF (MonoObject): field->offset;
-			if (il_op == MONO_CEE_STFLD) {
-				sp [1] = convert_value (cfg, field->type, sp [1]);
-				if (target_type_is_incompatible (cfg, field->type, sp [1]))
-					UNVERIFIED;
-				{
-					MonoInst *store;
-					MONO_EMIT_NULL_CHECK (cfg, sp [0]->dreg, foffset > mono_target_pagesize ());
-					if (ins_flag & MONO_INST_VOLATILE) {
-						/* Volatile stores have release semantics, see 12.6.7 in Ecma 335 */
-						mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
-					}
-					if (mini_is_gsharedvt_klass (klass)) {
-						MonoInst *offset_ins;
-						context_used = mini_class_check_context_used (cfg, klass);
-						offset_ins = emit_get_gsharedvt_info (cfg, field, MONO_RGCTX_INFO_FIELD_OFFSET);
-						/* The value is offset by 1 */
-						EMIT_NEW_BIALU_IMM (cfg, ins, OP_PSUB_IMM, offset_ins->dreg, offset_ins->dreg, 1);
-						int dreg = alloc_ireg_mp (cfg);
-						EMIT_NEW_BIALU (cfg, ins, OP_PADD, dreg, sp [0]->dreg, offset_ins->dreg);
-						if (cfg->gen_write_barriers && mini_type_to_stind (cfg, field->type) == CEE_STIND_REF && !MONO_INS_IS_PCONST_NULL (sp [1])) {
-							store = mini_emit_storing_write_barrier (cfg, ins, sp [1]);
-						} else {
-							/* The decomposition will call mini_emit_memory_copy () which will emit a wbarrier if needed */
-							EMIT_NEW_STORE_MEMBASE_TYPE (cfg, store, field->type, dreg, 0, sp [1]->dreg);
-						}
-					} else {
-						if (cfg->gen_write_barriers && mini_type_to_stind (cfg, field->type) == CEE_STIND_REF && !MONO_INS_IS_PCONST_NULL (sp [1])) {
-							/* insert call to write barrier */
-							MonoInst *ptr;
-							int dreg;
-							dreg = alloc_ireg_mp (cfg);
-							EMIT_NEW_BIALU_IMM (cfg, ptr, OP_PADD_IMM, dreg, sp [0]->dreg, foffset);
-							store = mini_emit_storing_write_barrier (cfg, ptr, sp [1]);
-						} else {
-							if (MONO_TYPE_ISSTRUCT (field->type))
-								/* The decomposition might end up calling a copy/wbarrier function which doesn't do null checks */
-								MONO_EMIT_EXPLICIT_NULL_CHECK (cfg, sp [0]->dreg);
-							EMIT_NEW_STORE_MEMBASE_TYPE (cfg, store, field->type, sp [0]->dreg, foffset, sp [1]->dreg);
-						}
-					}
-					if (sp [0]->opcode != OP_LDADDR)
-						store->flags |= MONO_INST_FAULT;
-					store->flags |= ins_flag;
-				}
-				goto field_access_end;
-			}
-			if (is_instance) {
-				if (sp [0]->type == STACK_VTYPE) {
-					MonoInst *var;
-					/* Have to compute the address of the variable */
-					var = get_vreg_to_inst (cfg, sp [0]->dreg);
-					if (!var)
-						var = mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (klass), OP_LOCAL, sp [0]->dreg);
-					else
-						g_assert (var->klass == klass);
-					EMIT_NEW_VARLOADA (cfg, ins, var, m_class_get_byval_arg (var->klass));
-					sp [0] = ins;
-				}
-				if (il_op == MONO_CEE_LDFLDA) {
-					if (sp [0]->type == STACK_OBJ || sp [0]->type == STACK_PTR) {
-						MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, sp [0]->dreg, 0);
-						MONO_EMIT_NEW_COND_EXC (cfg, EQ, "NullReferenceException");
-					}
-					int dreg = alloc_ireg_mp (cfg);
-					if (mini_is_gsharedvt_klass (klass)) {
-						MonoInst *offset_ins;
-						offset_ins = emit_get_gsharedvt_info (cfg, field, MONO_RGCTX_INFO_FIELD_OFFSET);
-						/* The value is offset by 1 */
-						EMIT_NEW_BIALU_IMM (cfg, ins, OP_PSUB_IMM, offset_ins->dreg, offset_ins->dreg, 1);
-						EMIT_NEW_BIALU (cfg, ins, OP_PADD, dreg, sp [0]->dreg, offset_ins->dreg);
-					} else {
-						EMIT_NEW_BIALU_IMM (cfg, ins, OP_PADD_IMM, dreg, sp [0]->dreg, foffset);
-					}
-					ins->klass = mono_class_from_mono_type_internal (field->type);
-					ins->type = STACK_MP;
-					*sp++ = ins;
-				} else {
-					MonoInst *load;
-					MONO_EMIT_NULL_CHECK (cfg, sp [0]->dreg, foffset > mono_target_pagesize ());
-#ifdef MONO_ARCH_SIMD_INTRINSICS
-					if (sp [0]->opcode == OP_LDADDR && m_class_is_simd_type (klass) && cfg->opt & MONO_OPT_SIMD) {
-						ins = mono_emit_simd_field_load (cfg, field, sp [0]);
-						if (ins) {
-							*sp++ = ins;
-							goto field_access_end;
-						}
-					}
-#endif
-					MonoInst *field_add_inst = sp [0];
-					if (mini_is_gsharedvt_klass (klass)) {
-						MonoInst *offset_ins;
-						offset_ins = emit_get_gsharedvt_info (cfg, field, MONO_RGCTX_INFO_FIELD_OFFSET);
-						/* The value is offset by 1 */
-						EMIT_NEW_BIALU_IMM (cfg, ins, OP_PSUB_IMM, offset_ins->dreg, offset_ins->dreg, 1);
-						EMIT_NEW_BIALU (cfg, field_add_inst, OP_PADD, alloc_ireg_mp (cfg), sp [0]->dreg, offset_ins->dreg);
-						foffset = 0;
-					}
-					load = mini_emit_memory_load (cfg, field->type, field_add_inst, foffset, ins_flag);
-					if (sp [0]->opcode != OP_LDADDR)
-						load->flags |= MONO_INST_FAULT;
-					*sp++ = load;
-				}
-			}
-			if (is_instance)
-				goto field_access_end;
-			/* STATIC CASE */
-			context_used = mini_class_check_context_used (cfg, klass);
-			if (ftype->attrs & FIELD_ATTRIBUTE_LITERAL) {
-				mono_error_set_field_missing (cfg->error, m_field_get_parent (field), field->name, NULL, "Using static instructions with literal field");
-				CHECK_CFG_ERROR;
-			}
-			/* The special_static_fields field is init'd in mono_class_vtable, so it needs
-			 * to be called here.
-			 */
-			if (!context_used) {
-				mono_class_vtable_checked (klass, cfg->error);
-				CHECK_CFG_ERROR;
-				CHECK_TYPELOAD (klass);
-			}
-			is_special_static = mono_class_field_is_special_static (field);
-			if (is_special_static) {
-				addr = mono_special_static_field_get_offset (field, cfg->error);
-				CHECK_CFG_ERROR;
-				CHECK_TYPELOAD (klass);
-			} else {
-				addr = NULL;
-			}
-			if (is_special_static && ((gsize)addr & 0x80000000) == 0)
-				thread_ins = mono_create_tls_get (cfg, TLS_KEY_THREAD);
-			else
-				thread_ins = NULL;
-			/* Generate IR to compute the field address */
-			if (is_special_static && ((gsize)addr & 0x80000000) == 0 && thread_ins &&
-				!(context_used && cfg->gsharedvt && mini_is_gsharedvt_klass (klass))) {
-				/*
-				 * Fast access to TLS data
-				 * Inline version of get_thread_static_data () in
-				 * threads.c.
-				 */
-				guint32 offset;
-				int idx, static_data_reg, array_reg, dreg;
-				static_data_reg = alloc_ireg (cfg);
-				MONO_EMIT_NEW_LOAD_MEMBASE (cfg, static_data_reg, thread_ins->dreg, MONO_STRUCT_OFFSET (MonoInternalThread, static_data));
-				if (cfg->compile_aot || context_used) {
-					int offset_reg, offset2_reg, idx_reg;
-					/* For TLS variables, this will return the TLS offset */
-					if (context_used) {
-						MonoInst *addr_ins = emit_get_rgctx_field (cfg, context_used, field, MONO_RGCTX_INFO_FIELD_OFFSET);
-						/* The value is offset by 1 */
-						EMIT_NEW_BIALU_IMM (cfg, ins, OP_PSUB_IMM, addr_ins->dreg, addr_ins->dreg, 1);
-					} else {
-						EMIT_NEW_SFLDACONST (cfg, ins, field);
-					}
-					offset_reg = ins->dreg;
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_IAND_IMM, offset_reg, offset_reg, 0x7fffffff);
-					idx_reg = alloc_ireg (cfg);
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_IAND_IMM, idx_reg, offset_reg, 0x3f);
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_ISHL_IMM, idx_reg, idx_reg, TARGET_SIZEOF_VOID_P == 8 ? 3 : 2);
-					MONO_EMIT_NEW_BIALU (cfg, OP_PADD, static_data_reg, static_data_reg, idx_reg);
-					array_reg = alloc_ireg (cfg);
-					MONO_EMIT_NEW_LOAD_MEMBASE (cfg, array_reg, static_data_reg, 0);
-					offset2_reg = alloc_ireg (cfg);
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_ISHR_UN_IMM, offset2_reg, offset_reg, 6);
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_IAND_IMM, offset2_reg, offset2_reg, 0x1ffffff);
-					dreg = alloc_ireg (cfg);
-					EMIT_NEW_BIALU (cfg, ins, OP_PADD, dreg, array_reg, offset2_reg);
-				} else {
-					offset = (gsize)addr & 0x7fffffff;
-					idx = offset & 0x3f;
-					array_reg = alloc_ireg (cfg);
-					MONO_EMIT_NEW_LOAD_MEMBASE (cfg, array_reg, static_data_reg, idx * TARGET_SIZEOF_VOID_P);
-					dreg = alloc_ireg (cfg);
-					EMIT_NEW_BIALU_IMM (cfg, ins, OP_ADD_IMM, dreg, array_reg, ((offset >> 6) & 0x1ffffff));
-				}
-			} else if ((cfg->compile_aot && is_special_static) ||
-					(context_used && is_special_static)) {
-				MonoInst *iargs [1];
-				g_assert (m_field_get_parent (field));
-				if (context_used) {
-					iargs [0] = emit_get_rgctx_field (cfg, context_used,
-						field, MONO_RGCTX_INFO_CLASS_FIELD);
-				} else {
-					EMIT_NEW_FIELDCONST (cfg, iargs [0], field);
-				}
-				ins = mono_emit_jit_icall (cfg, mono_class_static_field_address, iargs);
-			} else if (context_used) {
-				MonoInst *static_data;
-				/*
-				g_print ("sharing static field access in %s.%s.%s - depth %d offset %d\n",
-					method->klass->name_space, method->klass->name, method->name,
-					depth, field->offset);
-				*/
-				if (mono_class_needs_cctor_run (klass, method))
-					emit_class_init (cfg, klass, TRUE);
-				/*
-				 * The pointer we're computing here is
-				 *
-				 *   super_info.static_data + field->offset
-				 */
-				static_data = mini_emit_get_rgctx_klass (cfg, context_used,
-					klass, MONO_RGCTX_INFO_STATIC_DATA);
-				if (mini_is_gsharedvt_klass (klass)) {
-					MonoInst *offset_ins;
-					offset_ins = emit_get_rgctx_field (cfg, context_used, field, MONO_RGCTX_INFO_FIELD_OFFSET);
-					/* The value is offset by 1 */
-					EMIT_NEW_BIALU_IMM (cfg, ins, OP_PSUB_IMM, offset_ins->dreg, offset_ins->dreg, 1);
-					int dreg = alloc_ireg_mp (cfg);
-					EMIT_NEW_BIALU (cfg, ins, OP_PADD, dreg, static_data->dreg, offset_ins->dreg);
-				} else if (field->offset == 0) {
-					ins = static_data;
-				} else {
-					int addr_reg = mono_alloc_preg (cfg);
-					EMIT_NEW_BIALU_IMM (cfg, ins, OP_PADD_IMM, addr_reg, static_data->dreg, field->offset);
-				}
-			} else if (cfg->compile_aot && addr) {
-				MonoInst *iargs [1];
-				g_assert (m_field_get_parent (field));
-				EMIT_NEW_FIELDCONST (cfg, iargs [0], field);
-				ins = mono_emit_jit_icall (cfg, mono_class_static_field_address, iargs);
-			} else {
-				MonoVTable *vtable = NULL;
-				if (!cfg->compile_aot)
-					vtable = mono_class_vtable_checked (klass, cfg->error);
-				CHECK_CFG_ERROR;
-				CHECK_TYPELOAD (klass);
-				if (!addr) {
-					if (mini_field_access_needs_cctor_run (cfg, method, klass, vtable)) {
-						if (!(g_slist_find (class_inits, klass))) {
-							emit_class_init (cfg, klass, TRUE);
-							if (cfg->verbose_level > 2)
-								printf ("class %s.%s needs init call for %s\n", m_class_get_name_space (klass), m_class_get_name (klass), mono_field_get_name (field));
-							class_inits = g_slist_prepend (class_inits, klass);
-						}
-					} else {
-						if (cfg->run_cctors) {
-							/* This makes so that inline cannot trigger */
-							/* .cctors: too many apps depend on them */
-							/* running with a specific order... */
-							g_assert (vtable);
-							if (!vtable->initialized && m_class_has_cctor (vtable->klass))
-								INLINE_FAILURE ("class init");
-							if (!mono_runtime_class_init_full (vtable, cfg->error)) {
-								mono_cfg_set_exception (cfg, MONO_EXCEPTION_MONO_ERROR);
-								goto exception_exit;
-							}
-						}
-					}
-					if (cfg->compile_aot)
-						EMIT_NEW_SFLDACONST (cfg, ins, field);
-					else {
-						g_assert (vtable);
-						addr = mono_static_field_get_addr (vtable, field);
-						g_assert (addr);
-						EMIT_NEW_PCONST (cfg, ins, addr);
-					}
-				} else {
-					MonoInst *iargs [1];
-					EMIT_NEW_ICONST (cfg, iargs [0], GPOINTER_TO_UINT (addr));
-					ins = mono_emit_jit_icall (cfg, mono_get_special_static_data, iargs);
-				}
-			}
-			/* Generate IR to do the actual load/store operation */
-			if ((il_op == MONO_CEE_STFLD || il_op == MONO_CEE_STSFLD)) {
-				if (ins_flag & MONO_INST_VOLATILE) {
-					/* Volatile stores have release semantics, see 12.6.7 in Ecma 335 */
-					mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
-				} else if (!mini_debug_options.weak_memory_model && mini_type_is_reference (ftype)) {
-					mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_REL);
-				}
-			}
-			if (il_op == MONO_CEE_LDSFLDA) {
-				ins->klass = mono_class_from_mono_type_internal (ftype);
-				ins->type = STACK_PTR;
-				*sp++ = ins;
-			} else if (il_op == MONO_CEE_STSFLD) {
-				MonoInst *store;
-				if (m_class_get_mem_manager (m_field_get_parent (field))->collectible && (mini_type_is_reference (ftype) || m_class_has_references (mono_class_from_mono_type_internal (ftype)))) {
-					/* These are stored on the GC heap, so they need GC barriers */
-					mini_emit_memory_store (cfg, ftype, ins, store_val, 0);
-				} else {
-					EMIT_NEW_STORE_MEMBASE_TYPE (cfg, store, ftype, ins->dreg, 0, store_val->dreg);
-					store->flags |= ins_flag;
-				}
-			} else {
-				gboolean is_const = FALSE;
-				MonoVTable *vtable = NULL;
-				addr = NULL;
-				if (!context_used) {
-					vtable = mono_class_vtable_checked (klass, cfg->error);
-					CHECK_CFG_ERROR;
-					CHECK_TYPELOAD (klass);
-				}
-				if ((ftype->attrs & FIELD_ATTRIBUTE_INIT_ONLY) && (((addr = mono_aot_readonly_field_override (field)) != NULL) ||
-						(!context_used && !cfg->compile_aot && vtable->initialized))) {
-					int ro_type = ftype->type;
-					if (!addr)
-						addr = mono_static_field_get_addr (vtable, field);
-					if (ro_type == MONO_TYPE_VALUETYPE && m_class_is_enumtype (ftype->data.klass)) {
-						ro_type = mono_class_enum_basetype_internal (ftype->data.klass)->type;
-					}
-					GSHAREDVT_FAILURE (il_op);
-					/* printf ("RO-FIELD %s.%s:%s\n", klass->name_space, klass->name, mono_field_get_name (field));*/
-					is_const = TRUE;
-					switch (ro_type) {
-					case MONO_TYPE_BOOLEAN:
-					case MONO_TYPE_U1:
-						EMIT_NEW_ICONST (cfg, *sp, *((guint8 *)addr));
-						sp++;
-						break;
-					case MONO_TYPE_I1:
-						EMIT_NEW_ICONST (cfg, *sp, *((gint8 *)addr));
-						sp++;
-						break;
-					case MONO_TYPE_CHAR:
-					case MONO_TYPE_U2:
-						EMIT_NEW_ICONST (cfg, *sp, *((guint16 *)addr));
-						sp++;
-						break;
-					case MONO_TYPE_I2:
-						EMIT_NEW_ICONST (cfg, *sp, *((gint16 *)addr));
-						sp++;
-						break;
-						break;
-					case MONO_TYPE_I4:
-						EMIT_NEW_ICONST (cfg, *sp, *((gint32 *)addr));
-						sp++;
-						break;
-					case MONO_TYPE_U4:
-						EMIT_NEW_ICONST (cfg, *sp, *((guint32 *)addr));
-						sp++;
-						break;
-					case MONO_TYPE_I:
-					case MONO_TYPE_U:
-					case MONO_TYPE_PTR:
-					case MONO_TYPE_FNPTR:
-						EMIT_NEW_PCONST (cfg, *sp, *((gpointer *)addr));
-						mini_type_to_eval_stack_type ((cfg), field->type, *sp);
-						sp++;
-						break;
-					case MONO_TYPE_STRING:
-					case MONO_TYPE_OBJECT:
-					case MONO_TYPE_CLASS:
-					case MONO_TYPE_SZARRAY:
-					case MONO_TYPE_ARRAY:
-						if (!mono_gc_is_moving ()) {
-							EMIT_NEW_PCONST (cfg, *sp, *((gpointer *)addr));
-							mini_type_to_eval_stack_type ((cfg), field->type, *sp);
-							sp++;
-						} else {
-							is_const = FALSE;
-						}
-						break;
-					case MONO_TYPE_I8:
-					case MONO_TYPE_U8:
-						EMIT_NEW_I8CONST (cfg, *sp, *((gint64 *)addr));
-						sp++;
-						break;
-					case MONO_TYPE_R4:
-					case MONO_TYPE_R8:
-					case MONO_TYPE_VALUETYPE:
-					default:
-						is_const = FALSE;
-						break;
-					}
-				}
-				if (!is_const) {
-					MonoInst *load;
-					EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, load, field->type, ins->dreg, 0);
-					load->flags |= ins_flag;
-					*sp++ = load;
-				}
-			}
-field_access_end:
-			if ((il_op == MONO_CEE_LDFLD || il_op == MONO_CEE_LDSFLD) && (ins_flag & MONO_INST_VOLATILE)) {
-				/* Volatile loads have acquire semantics, see 12.6.7 in Ecma 335 */
-				mini_emit_memory_barrier (cfg, MONO_MEMORY_BARRIER_ACQ);
-			}
-			ins_flag = 0;
-			break;
-		}
-		case MONO_CEE_STOBJ:
-			sp -= 2;
-			klass = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (klass);
-			/* FIXME: should check item at sp [1] is compatible with the type of the store. */
-			mini_emit_memory_store (cfg, m_class_get_byval_arg (klass), sp [0], sp [1], ins_flag);
-			ins_flag = 0;
-			inline_costs += 1;
-			break;
-			/*
-			 * Array opcodes
-			 */
-		case MONO_CEE_NEWARR: {
-			MonoInst *len_ins;
-			const char *data_ptr;
-			int data_size = 0;
-			guint32 field_token;
-			--sp;
-			klass = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (klass);
-			if (m_class_get_byval_arg (klass)->type == MONO_TYPE_VOID)
-				UNVERIFIED;
-			context_used = mini_class_check_context_used (cfg, klass);
-#ifndef TARGET_S390X
-			if (sp [0]->type == STACK_I8 && TARGET_SIZEOF_VOID_P == 4) {
-				MONO_INST_NEW (cfg, ins, OP_LCONV_TO_OVF_U4);
-				ins->sreg1 = sp [0]->dreg;
-				ins->type = STACK_I4;
-				ins->dreg = alloc_ireg (cfg);
-				MONO_ADD_INS (cfg->cbb, ins);
-				*sp = mono_decompose_opcode (cfg, ins);
-			}
-#else
-			/* The array allocator expects a 64-bit input, and we cannot rely
-			   on the high bits of a 32-bit result, so we have to extend.  */
-			if (sp [0]->type == STACK_I4 && TARGET_SIZEOF_VOID_P == 8) {
-				MONO_INST_NEW (cfg, ins, OP_ICONV_TO_I8);
-				ins->sreg1 = sp [0]->dreg;
-				ins->type = STACK_I8;
-				ins->dreg = alloc_ireg (cfg);
-				MONO_ADD_INS (cfg->cbb, ins);
-				*sp = mono_decompose_opcode (cfg, ins);
-			}
-#endif
-			if (context_used) {
-				MonoInst *args [3];
-				MonoClass *array_class = mono_class_create_array (klass, 1);
-				MonoMethod *managed_alloc = mono_gc_get_managed_array_allocator (array_class);
-				/* FIXME: Use OP_NEWARR and decompose later to help abcrem */
-				/* vtable */
-				args [0] = mini_emit_get_rgctx_klass (cfg, context_used,
-					array_class, MONO_RGCTX_INFO_VTABLE);
-				/* array len */
-				args [1] = sp [0];
-				if (managed_alloc)
-					ins = mono_emit_method_call (cfg, managed_alloc, args, NULL);
-				else
-					ins = mono_emit_jit_icall (cfg, ves_icall_array_new_specific, args);
-			} else {
-				/* Decompose later since it is needed by abcrem */
-				MonoClass *array_type = mono_class_create_array (klass, 1);
-				mono_class_vtable_checked (array_type, cfg->error);
-				CHECK_CFG_ERROR;
-				CHECK_TYPELOAD (array_type);
-				MONO_INST_NEW (cfg, ins, OP_NEWARR);
-				ins->dreg = alloc_ireg_ref (cfg);
-				ins->sreg1 = sp [0]->dreg;
-				ins->inst_newa_class = klass;
-				ins->type = STACK_OBJ;
-				ins->klass = array_type;
-				MONO_ADD_INS (cfg->cbb, ins);
-				cfg->flags |= MONO_CFG_NEEDS_DECOMPOSE;
-				cfg->cbb->needs_decompose = TRUE;
-				/* Needed so mono_emit_load_get_addr () gets called */
-				mono_get_got_var (cfg);
-			}
-			len_ins = sp [0];
-			ip += 5;
-			*sp++ = ins;
-			inline_costs += 1;
-			/*
-			 * we inline/optimize the initialization sequence if possible.
-			 * we should also allocate the array as not cleared, since we spend as much time clearing to 0 as initializing
-			 * for small sizes open code the memcpy
-			 * ensure the rva field is big enough
-			 */
-			if ((cfg->opt & MONO_OPT_INTRINS) && next_ip < end
-					&& ip_in_bb (cfg, cfg->cbb, next_ip)
-					&& (len_ins->opcode == OP_ICONST)
-					&& (data_ptr = initialize_array_data (cfg, method,
-						cfg->compile_aot, next_ip, end, klass,
-						GTMREG_TO_UINT32 (len_ins->inst_c0), &data_size, &field_token,
-						&il_op, &next_ip))) {
-				MonoMethod *memcpy_method = mini_get_memcpy_method ();
-				MonoInst *iargs [3];
-				int add_reg = alloc_ireg_mp (cfg);
-				EMIT_NEW_BIALU_IMM (cfg, iargs [0], OP_PADD_IMM, add_reg, ins->dreg, MONO_STRUCT_OFFSET (MonoArray, vector));
-				if (cfg->compile_aot) {
-					EMIT_NEW_AOTCONST_TOKEN (cfg, iargs [1], MONO_PATCH_INFO_RVA, m_class_get_image (method->klass), field_token, STACK_PTR, NULL);
-				} else {
-					EMIT_NEW_PCONST (cfg, iargs [1], (char*)data_ptr);
-				}
-				EMIT_NEW_ICONST (cfg, iargs [2], data_size);
-				mono_emit_method_call (cfg, memcpy_method, iargs, NULL);
-			}
-			break;
-		}
-		case MONO_CEE_LDLEN:
-			--sp;
-			if (sp [0]->type != STACK_OBJ)
-				UNVERIFIED;
-			MONO_INST_NEW (cfg, ins, OP_LDLEN);
-			ins->dreg = alloc_preg (cfg);
-			ins->sreg1 = sp [0]->dreg;
-			ins->inst_imm = MONO_STRUCT_OFFSET (MonoArray, max_length);
-			ins->type = STACK_I4;
-			/* This flag will be inherited by the decomposition */
-			ins->flags |= MONO_INST_FAULT | MONO_INST_INVARIANT_LOAD;
-			MONO_ADD_INS (cfg->cbb, ins);
-			cfg->flags |= MONO_CFG_NEEDS_DECOMPOSE;
-			cfg->cbb->needs_decompose = TRUE;
-			MONO_EMIT_NEW_UNALU (cfg, OP_NOT_NULL, -1, sp [0]->dreg);
-			*sp++ = ins;
-			break;
-		case MONO_CEE_LDELEMA:
-			sp -= 2;
-			if (sp [0]->type != STACK_OBJ)
-				UNVERIFIED;
-			cfg->flags |= MONO_CFG_HAS_LDELEMA;
-			klass = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (klass);
-			/* we need to make sure that this array is exactly the type it needs
-			 * to be for correctness. the wrappers are lax with their usage
-			 * so we need to ignore them here
-			 */
-			if (!m_class_is_valuetype (klass) && method->wrapper_type == MONO_WRAPPER_NONE && !readonly) {
-				MonoClass *array_class = mono_class_create_array (klass, 1);
-				mini_emit_check_array_type (cfg, sp [0], array_class);
-				CHECK_TYPELOAD (array_class);
-			}
-			readonly = FALSE;
-			ins = mini_emit_ldelema_1_ins (cfg, klass, sp [0], sp [1], TRUE, FALSE);
-			*sp++ = ins;
-			break;
-		case MONO_CEE_LDELEM:
-		case MONO_CEE_LDELEM_I1:
-		case MONO_CEE_LDELEM_U1:
-		case MONO_CEE_LDELEM_I2:
-		case MONO_CEE_LDELEM_U2:
-		case MONO_CEE_LDELEM_I4:
-		case MONO_CEE_LDELEM_U4:
-		case MONO_CEE_LDELEM_I8:
-		case MONO_CEE_LDELEM_I:
-		case MONO_CEE_LDELEM_R4:
-		case MONO_CEE_LDELEM_R8:
-		case MONO_CEE_LDELEM_REF: {
-			MonoInst *addr;
-			sp -= 2;
-			if (il_op == MONO_CEE_LDELEM) {
-				klass = mini_get_class (method, token, generic_context);
-				CHECK_TYPELOAD (klass);
-				mono_class_init_internal (klass);
-			}
-			else
-				klass = array_access_to_klass (il_op);
-			if (sp [0]->type != STACK_OBJ)
-				UNVERIFIED;
-			cfg->flags |= MONO_CFG_HAS_LDELEMA;
-			if (mini_is_gsharedvt_variable_klass (klass)) {
-				addr = mini_emit_ldelema_1_ins (cfg, klass, sp [0], sp [1], TRUE, FALSE);
-				EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), addr->dreg, 0);
-				ins->opcode = OP_LOADV_MEMBASE;
-			} else if (sp [1]->opcode == OP_ICONST) {
-				int array_reg = sp [0]->dreg;
-				int index_reg = sp [1]->dreg;
-				size_t offset = (mono_class_array_element_size (klass) * sp [1]->inst_c0) + MONO_STRUCT_OFFSET (MonoArray, vector);
-				if (SIZEOF_REGISTER == 8 && COMPILE_LLVM (cfg))
-					MONO_EMIT_NEW_UNALU (cfg, OP_ZEXT_I4, index_reg, index_reg);
-				MONO_EMIT_BOUNDS_CHECK (cfg, array_reg, MonoArray, max_length, index_reg);
-				EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), array_reg, (target_mgreg_t)offset);
-			} else {
-				addr = mini_emit_ldelema_1_ins (cfg, klass, sp [0], sp [1], TRUE, FALSE);
-				EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (klass), addr->dreg, 0);
-			}
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_STELEM_I:
-		case MONO_CEE_STELEM_I1:
-		case MONO_CEE_STELEM_I2:
-		case MONO_CEE_STELEM_I4:
-		case MONO_CEE_STELEM_I8:
-		case MONO_CEE_STELEM_R4:
-		case MONO_CEE_STELEM_R8:
-		case MONO_CEE_STELEM_REF:
-		case MONO_CEE_STELEM: {
-			sp -= 3;
-			cfg->flags |= MONO_CFG_HAS_LDELEMA;
-			if (il_op == MONO_CEE_STELEM) {
-				klass = mini_get_class (method, token, generic_context);
-				CHECK_TYPELOAD (klass);
-				mono_class_init_internal (klass);
-			}
-			else
-				klass = array_access_to_klass (il_op);
-			if (sp [0]->type != STACK_OBJ)
-				UNVERIFIED;
-			sp [2] = convert_value (cfg, m_class_get_byval_arg (klass), sp [2]);
-			mini_emit_array_store (cfg, klass, sp, TRUE);
-			inline_costs += 1;
-			break;
-		}
-		case MONO_CEE_CKFINITE: {
-			--sp;
-			if (cfg->llvm_only) {
-				MonoInst *iargs [1];
-				iargs [0] = sp [0];
-				*sp++ = mono_emit_jit_icall (cfg, mono_ckfinite, iargs);
-			} else  {
-				sp [0] = convert_value (cfg, m_class_get_byval_arg (mono_defaults.double_class), sp [0]);
-				MONO_INST_NEW (cfg, ins, OP_CKFINITE);
-				ins->sreg1 = sp [0]->dreg;
-				ins->dreg = alloc_freg (cfg);
-				ins->type = STACK_R8;
-				MONO_ADD_INS (cfg->cbb, ins);
-				*sp++ = mono_decompose_opcode (cfg, ins);
-			}
-			break;
-		}
-		case MONO_CEE_REFANYVAL: {
-			MonoInst *src_var, *src;
-			int klass_reg = alloc_preg (cfg);
-			int dreg = alloc_preg (cfg);
-			GSHAREDVT_FAILURE (il_op);
-			MONO_INST_NEW (cfg, ins, il_op);
-			--sp;
-			klass = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (klass);
-			context_used = mini_class_check_context_used (cfg, klass);
-			src_var = get_vreg_to_inst (cfg, sp [0]->dreg);
-			if (!src_var)
-				src_var = mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (mono_defaults.typed_reference_class), OP_LOCAL, sp [0]->dreg);
-			EMIT_NEW_VARLOADA (cfg, src, src_var, src_var->inst_vtype);
-			MONO_EMIT_NEW_LOAD_MEMBASE (cfg, klass_reg, src->dreg, MONO_STRUCT_OFFSET (MonoTypedRef, klass));
-			if (context_used) {
-				MonoInst *klass_ins;
-				klass_ins = mini_emit_get_rgctx_klass (cfg, context_used,
-						klass, MONO_RGCTX_INFO_KLASS);
-				MONO_EMIT_NEW_BIALU (cfg, OP_COMPARE, -1, klass_reg, klass_ins->dreg);
-				MONO_EMIT_NEW_COND_EXC (cfg, NE_UN, "InvalidCastException");
-			} else {
-				mini_emit_class_check (cfg, klass_reg, klass);
-			}
-			EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, dreg, src->dreg, MONO_STRUCT_OFFSET (MonoTypedRef, value));
-			ins->type = STACK_MP;
-			ins->klass = klass;
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_MKREFANY: {
-			MonoInst *loc, *addr;
-			GSHAREDVT_FAILURE (il_op);
-			MONO_INST_NEW (cfg, ins, il_op);
-			--sp;
-			klass = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (klass);
-			context_used = mini_class_check_context_used (cfg, klass);
-			loc = mono_compile_create_var (cfg, m_class_get_byval_arg (mono_defaults.typed_reference_class), OP_LOCAL);
-			EMIT_NEW_TEMPLOADA (cfg, addr, loc->inst_c0);
-			MonoInst *const_ins = mini_emit_get_rgctx_klass (cfg, context_used, klass, MONO_RGCTX_INFO_KLASS);
-			int type_reg = alloc_preg (cfg);
-			MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STOREP_MEMBASE_REG, addr->dreg, MONO_STRUCT_OFFSET (MonoTypedRef, klass), const_ins->dreg);
-			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_ADD_IMM, type_reg, const_ins->dreg, m_class_offsetof_byval_arg ());
-			MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STOREP_MEMBASE_REG, addr->dreg, MONO_STRUCT_OFFSET (MonoTypedRef, type), type_reg);
-			MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STOREP_MEMBASE_REG, addr->dreg, MONO_STRUCT_OFFSET (MonoTypedRef, value), sp [0]->dreg);
-			EMIT_NEW_TEMPLOAD (cfg, ins, loc->inst_c0);
-			ins->type = STACK_VTYPE;
-			ins->klass = mono_defaults.typed_reference_class;
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_LDTOKEN: {
-			gpointer handle;
-			MonoClass *handle_class;
-			if (method->wrapper_type == MONO_WRAPPER_DYNAMIC_METHOD ||
-					method->wrapper_type == MONO_WRAPPER_SYNCHRONIZED) {
-				handle = mono_method_get_wrapper_data (method, n);
-				handle_class = (MonoClass *)mono_method_get_wrapper_data (method, n + 1);
-				if (handle_class == mono_defaults.typehandle_class)
-					handle = m_class_get_byval_arg ((MonoClass*)handle);
-			}
-			else {
-				handle = mono_ldtoken_checked (image, n, &handle_class, generic_context, cfg->error);
-				CHECK_CFG_ERROR;
-			}
-			if (!handle)
-				LOAD_ERROR;
-			mono_class_init_internal (handle_class);
-			if (cfg->gshared) {
-				if (mono_metadata_token_table (n) == MONO_TABLE_TYPEDEF ||
-						mono_metadata_token_table (n) == MONO_TABLE_TYPEREF) {
-					/* This case handles ldtoken
-					   of an open type, like for
-					   typeof(Gen<>). */
-					context_used = 0;
-				} else if (handle_class == mono_defaults.typehandle_class) {
-					context_used = mini_class_check_context_used (cfg, mono_class_from_mono_type_internal ((MonoType *)handle));
-				} else if (handle_class == mono_defaults.fieldhandle_class)
-					context_used = mini_class_check_context_used (cfg, m_field_get_parent (((MonoClassField*)handle)));
-				else if (handle_class == mono_defaults.methodhandle_class)
-					context_used = mini_method_check_context_used (cfg, (MonoMethod *)handle);
-				else
-					g_assert_not_reached ();
-			}
-			{
-				if ((next_ip + 4 < end) && ip_in_bb (cfg, cfg->cbb, next_ip) &&
-					((next_ip [0] == CEE_CALL) || (next_ip [0] == CEE_CALLVIRT)) &&
-					(cmethod = mini_get_method (cfg, method, read32 (next_ip + 1), NULL, generic_context)) &&
-					(cmethod->klass == mono_defaults.systemtype_class) &&
-					(strcmp (cmethod->name, "GetTypeFromHandle") == 0)) {
-					MonoClass *tclass = mono_class_from_mono_type_internal ((MonoType *)handle);
-					mono_class_init_internal (tclass);
-					guchar *is_vt_ip;
-					guint32 is_vt_token;
-					if ((is_vt_ip = il_read_call (next_ip + 5, end, &is_vt_token)) && ip_in_bb (cfg, cfg->cbb, is_vt_ip)) {
-						MonoMethod *is_vt_method = mini_get_method (cfg, method, is_vt_token, NULL, generic_context);
-						if (is_vt_method->klass == mono_defaults.systemtype_class &&
-							!mini_is_gsharedvt_variable_klass (tclass) &&
-							!mono_class_is_open_constructed_type (m_class_get_byval_arg (tclass)) &&
-							!strcmp ("get_IsValueType", is_vt_method->name)) {
-							next_ip = is_vt_ip;
-							EMIT_NEW_ICONST (cfg, ins, m_class_is_valuetype (tclass) ? 1 : 0);
-							ins->type = STACK_I4;
-							*sp++ = ins;
-							break;
-						}
-					}
-					if (context_used) {
-						MONO_INST_NEW (cfg, ins, OP_RTTYPE);
-						ins->dreg = alloc_ireg_ref (cfg);
-						ins->inst_p0 = tclass;
-						ins->type = STACK_OBJ;
-						MONO_ADD_INS (cfg->cbb, ins);
-						cfg->flags |= MONO_CFG_NEEDS_DECOMPOSE;
-						cfg->cbb->needs_decompose = TRUE;
-					} else if (cfg->compile_aot) {
-						if (method->wrapper_type) {
-							error_init (error); //got to do it since there are multiple conditionals below
-							if (mono_class_get_checked (m_class_get_image (tclass), m_class_get_type_token (tclass), error) == tclass && !generic_context) {
-								/* Special case for static synchronized wrappers */
-								EMIT_NEW_TYPE_FROM_HANDLE_CONST (cfg, ins, m_class_get_image (tclass), m_class_get_type_token (tclass), generic_context);
-							} else {
-								mono_error_cleanup (error); /* FIXME don't swallow the error */
-								/* FIXME: n is not a normal token */
-								DISABLE_AOT (cfg);
-								EMIT_NEW_PCONST (cfg, ins, NULL);
-							}
-						} else {
-							EMIT_NEW_TYPE_FROM_HANDLE_CONST (cfg, ins, image, n, generic_context);
-						}
-					} else {
-						MonoReflectionType *rt = mono_type_get_object_checked ((MonoType *)handle, cfg->error);
-						CHECK_CFG_ERROR;
-						EMIT_NEW_PCONST (cfg, ins, rt);
-					}
-					ins->type = STACK_OBJ;
-					ins->klass = mono_defaults.runtimetype_class;
-					il_op = (MonoOpcodeEnum)next_ip [0];
-					next_ip += 5;
-				} else {
-					MonoInst *addr, *vtvar;
-					vtvar = mono_compile_create_var (cfg, m_class_get_byval_arg (handle_class), OP_LOCAL);
-					if (context_used) {
-						if (handle_class == mono_defaults.typehandle_class) {
-							ins = mini_emit_get_rgctx_klass (cfg, context_used,
-									mono_class_from_mono_type_internal ((MonoType *)handle),
-									MONO_RGCTX_INFO_TYPE);
-						} else if (handle_class == mono_defaults.methodhandle_class) {
-							ins = emit_get_rgctx_method (cfg, context_used,
-									(MonoMethod *)handle, MONO_RGCTX_INFO_METHOD);
-						} else if (handle_class == mono_defaults.fieldhandle_class) {
-							ins = emit_get_rgctx_field (cfg, context_used,
-									(MonoClassField *)handle, MONO_RGCTX_INFO_CLASS_FIELD);
-						} else {
-							g_assert_not_reached ();
-						}
-					} else if (cfg->compile_aot) {
-						EMIT_NEW_LDTOKENCONST (cfg, ins, image, n, generic_context);
-					} else {
-						EMIT_NEW_PCONST (cfg, ins, handle);
-					}
-					EMIT_NEW_TEMPLOADA (cfg, addr, vtvar->inst_c0);
-					MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, addr->dreg, 0, ins->dreg);
-					EMIT_NEW_TEMPLOAD (cfg, ins, vtvar->inst_c0);
-					if (handle_class == mono_defaults.fieldhandle_class) {
-						ins->opcode = OP_LDTOKEN_FIELD;
-						ins->inst_c0 = n;
-						ins->inst_p1 = handle;
-						cfg->flags |= MONO_CFG_NEEDS_DECOMPOSE;
-						cfg->cbb->needs_decompose = TRUE;
-					}
-				}
-			}
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_THROW:
-			if (sp [-1]->type != STACK_OBJ)
-				UNVERIFIED;
-			MONO_INST_NEW (cfg, ins, OP_THROW);
-			--sp;
-			ins->sreg1 = sp [0]->dreg;
-			cfg->cbb->out_of_line = TRUE;
-			MONO_ADD_INS (cfg->cbb, ins);
-			MONO_INST_NEW (cfg, ins, OP_NOT_REACHED);
-			MONO_ADD_INS (cfg->cbb, ins);
-			sp = stack_start;
-			link_bblock (cfg, cfg->cbb, end_bblock);
-			start_new_bblock = 1;
-			/* This can complicate code generation for llvm since the return value might not be defined */
-			if (COMPILE_LLVM (cfg))
-				INLINE_FAILURE ("throw");
-			break;
-		case MONO_CEE_ENDFINALLY:
-			if (!ip_in_finally_clause (cfg, GPTRDIFF_TO_INT (ip - header->code)))
-				UNVERIFIED;
-			/* mono_save_seq_point_info () depends on this */
-			if (sp != stack_start)
-				emit_seq_point (cfg, method, ip, FALSE, FALSE);
-			MONO_INST_NEW (cfg, ins, OP_ENDFINALLY);
-			MONO_ADD_INS (cfg->cbb, ins);
-			start_new_bblock = 1;
-			ins_has_side_effect = FALSE;
-			/*
-			 * Control will leave the method so empty the stack, otherwise
-			 * the next basic block will start with a nonempty stack.
-			 */
-			while (sp != stack_start) {
-				sp--;
-			}
-			break;
-		case MONO_CEE_LEAVE:
-		case MONO_CEE_LEAVE_S: {
-			GList *handlers;
-			/* empty the stack */
-			g_assert (sp >= stack_start);
-			sp = stack_start;
-			/*
-			 * If this leave statement is in a catch block, check for a
-			 * pending exception, and rethrow it if necessary.
-			 * We avoid doing this in runtime invoke wrappers, since those are called
-			 * by native code which excepts the wrapper to catch all exceptions.
-			 */
-			for (unsigned int i = 0; i < header->num_clauses; ++i) {
-				MonoExceptionClause *clause = &header->clauses [i];
-				/*
-				 * Use <= in the final comparison to handle clauses with multiple
-				 * leave statements, like in bug #78024.
-				 * The ordering of the exception clauses guarantees that we find the
-				 * innermost clause.
-				 */
-				if (MONO_OFFSET_IN_HANDLER (clause, GPTRDIFF_TO_UINT32(ip - header->code)) && (clause->flags == MONO_EXCEPTION_CLAUSE_NONE) && GPTRDIFF_TO_UINT32(ip - header->code + ((il_op == MONO_CEE_LEAVE) ? 5 : 2)) <= (clause->handler_offset + clause->handler_len) && method->wrapper_type != MONO_WRAPPER_RUNTIME_INVOKE) {
-					MonoInst *exc_ins;
-					MonoBasicBlock *dont_throw;
-					/*
-					  MonoInst *load;
-					  NEW_TEMPLOAD (cfg, load, mono_find_exvar_for_offset (cfg, clause->handler_offset)->inst_c0);
-					*/
-					exc_ins = mono_emit_jit_icall (cfg, mono_thread_get_undeniable_exception, NULL);
-					NEW_BBLOCK (cfg, dont_throw);
-					/*
-					 * Currently, we always rethrow the abort exception, despite the
-					 * fact that this is not correct. See thread6.cs for an example.
-					 * But propagating the abort exception is more important than
-					 * getting the semantics right.
-					 */
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, exc_ins->dreg, 0);
-					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, dont_throw);
-					MONO_EMIT_NEW_UNALU (cfg, OP_THROW, -1, exc_ins->dreg);
-					MONO_START_BB (cfg, dont_throw);
-				}
-			}
-#ifdef ENABLE_LLVM
-			cfg->cbb->try_end = (intptr_t)(ip - header->code);
-#endif
-			if ((handlers = mono_find_leave_clauses (cfg, ip, target))) {
-				GList *tmp;
-				/*
-				 * For each finally clause that we exit we need to invoke the finally block.
-				 * After each invocation we need to add try holes for all the clauses that
-				 * we already exited.
-				 */
-				for (tmp = handlers; tmp; tmp = tmp->next) {
-					MonoLeaveClause *leave = (MonoLeaveClause *) tmp->data;
-					MonoExceptionClause *clause = leave->clause;
-					if (clause->flags != MONO_EXCEPTION_CLAUSE_FINALLY)
-						continue;
-					MonoInst *abort_exc = (MonoInst *)mono_find_exvar_for_offset (cfg, clause->handler_offset);
-					MonoBasicBlock *dont_throw;
-					/*
-					 * Emit instrumentation code before linking the basic blocks below as this
-					 * will alter cfg->cbb.
-					 */
-					mini_profiler_emit_call_finally (cfg, header, ip, leave->index, clause);
-					tblock = cfg->cil_offset_to_bb [clause->handler_offset];
-					g_assert (tblock);
-					link_bblock (cfg, cfg->cbb, tblock);
-					MONO_EMIT_NEW_PCONST (cfg, abort_exc->dreg, 0);
-					MONO_INST_NEW (cfg, ins, OP_CALL_HANDLER);
-					ins->inst_target_bb = tblock;
-					ins->inst_eh_blocks = tmp;
-					MONO_ADD_INS (cfg->cbb, ins);
-					cfg->cbb->has_call_handler = 1;
-					/* Throw exception if exvar is set */
-					/* FIXME Do we need this for calls from catch/filter ? */
-					NEW_BBLOCK (cfg, dont_throw);
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, abort_exc->dreg, 0);
-					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBEQ, dont_throw);
-					mono_emit_jit_icall (cfg, ves_icall_thread_finish_async_abort, NULL);
-					cfg->cbb->clause_holes = tmp;
-					MONO_START_BB (cfg, dont_throw);
-					cfg->cbb->clause_holes = tmp;
-					if (COMPILE_LLVM (cfg)) {
-						MonoBasicBlock *target_bb;
-						/*
-						 * Link the finally bblock with the target, since it will
-						 * conceptually branch there.
-						 */
-						GET_BBLOCK (cfg, tblock, cfg->cil_start + clause->handler_offset + clause->handler_len - 1);
-						GET_BBLOCK (cfg, target_bb, target);
-						link_bblock (cfg, tblock, target_bb);
-					}
-				}
-			}
-			MONO_INST_NEW (cfg, ins, OP_BR);
-			MONO_ADD_INS (cfg->cbb, ins);
-			GET_BBLOCK (cfg, tblock, target);
-			link_bblock (cfg, cfg->cbb, tblock);
-			ins->inst_target_bb = tblock;
-			start_new_bblock = 1;
-			break;
-		}
-		/*
-		 * Mono specific opcodes
-		 */
-		case MONO_CEE_MONO_ICALL: {
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			const MonoJitICallId jit_icall_id = (MonoJitICallId)token;
-			MonoJitICallInfo * const jit_icall_info = mono_find_jit_icall_info (jit_icall_id);
-			CHECK_STACK (jit_icall_info->sig->param_count);
-			sp -= jit_icall_info->sig->param_count;
-			if (token == MONO_JIT_ICALL_mono_threads_attach_coop) {
-				MonoInst *addr;
-				MonoBasicBlock *next_bb;
-				if (cfg->compile_aot) {
-					/*
-					 * This is called on unattached threads, so it cannot go through the trampoline
-					 * infrastructure. Use an indirect call through a got slot initialized at load time
-					 * instead.
-					 */
-					EMIT_NEW_AOTCONST (cfg, addr, MONO_PATCH_INFO_JIT_ICALL_ADDR_NOCALL, GUINT_TO_POINTER (jit_icall_id));
-					ins = mini_emit_calli (cfg, jit_icall_info->sig, sp, addr, NULL, NULL);
-				} else {
-					ins = mono_emit_jit_icall_id (cfg, jit_icall_id, sp);
-				}
-				/*
-				 * Parts of the initlocals code needs to come after this, since it might call methods like memset.
-				 * Also profiling needs to be after attach.
-				 */
-				init_localsbb2 = cfg->cbb;
-				NEW_BBLOCK (cfg, next_bb);
-				MONO_START_BB (cfg, next_bb);
-			} else {
-				if (token == MONO_JIT_ICALL_mono_threads_detach_coop) {
-					/* can't emit profiling code after a detach, so emit it now */
-					mini_profiler_emit_leave (cfg, NULL);
-					detached_before_ret = TRUE;
-				}
-				ins = mono_emit_jit_icall_id (cfg, jit_icall_id, sp);
-			}
-			if (!MONO_TYPE_IS_VOID (jit_icall_info->sig->ret))
-				*sp++ = ins;
-			inline_costs += CALL_COST * MIN(10, num_calls++);
-			break;
-		}
-		MonoJumpInfoType ldptr_type;
-		case MONO_CEE_MONO_LDPTR_CARD_TABLE:
-			ldptr_type = MONO_PATCH_INFO_GC_CARD_TABLE_ADDR;
-			goto mono_ldptr;
-		case MONO_CEE_MONO_LDPTR_NURSERY_START:
-			ldptr_type = MONO_PATCH_INFO_GC_NURSERY_START;
-			goto mono_ldptr;
-		case MONO_CEE_MONO_LDPTR_NURSERY_BITS:
-			ldptr_type = MONO_PATCH_INFO_GC_NURSERY_BITS;
-			goto mono_ldptr;
-		case MONO_CEE_MONO_LDPTR_INT_REQ_FLAG:
-			ldptr_type = MONO_PATCH_INFO_INTERRUPTION_REQUEST_FLAG;
-			goto mono_ldptr;
-		case MONO_CEE_MONO_LDPTR_PROFILER_ALLOCATION_COUNT:
-			ldptr_type = MONO_PATCH_INFO_PROFILER_ALLOCATION_COUNT;
-mono_ldptr:
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			ins = mini_emit_runtime_constant (cfg, ldptr_type, NULL);
-			*sp++ = ins;
-			inline_costs += CALL_COST * MIN(10, num_calls++);
-			break;
-		case MONO_CEE_MONO_LDPTR: {
-			gpointer ptr;
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			ptr = mono_method_get_wrapper_data (method, token);
-			EMIT_NEW_PCONST (cfg, ins, ptr);
-			*sp++ = ins;
-			inline_costs += CALL_COST * MIN(10, num_calls++);
-			/* Can't embed random pointers into AOT code */
-			DISABLE_AOT (cfg);
-			break;
-		}
-		case MONO_CEE_MONO_JIT_ICALL_ADDR:
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			EMIT_NEW_JIT_ICALL_ADDRCONST (cfg, ins, GUINT_TO_POINTER (token));
-			*sp++ = ins;
-			inline_costs += CALL_COST * MIN(10, num_calls++);
-			break;
-		case MONO_CEE_MONO_ICALL_ADDR: {
-			gpointer ptr;
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			cmethod = (MonoMethod *)mono_method_get_wrapper_data (method, token);
-			if (cfg->compile_aot) {
-				if (cfg->direct_pinvoke && ip + 6 < end && (ip [6] == CEE_POP)) {
-					/*
-					 * This is generated by emit_native_wrapper () to resolve the pinvoke address
-					 * before the call, its not needed when using direct pinvoke.
-					 * This is not an optimization, but its used to avoid looking up pinvokes
-					 * on platforms which don't support dlopen ().
-					 */
-					EMIT_NEW_PCONST (cfg, ins, NULL);
-				} else {
-					EMIT_NEW_AOTCONST (cfg, ins, MONO_PATCH_INFO_ICALL_ADDR, cmethod);
-				}
-			} else {
-				ptr = mono_lookup_internal_call (cmethod);
-				g_assert (ptr);
-				EMIT_NEW_PCONST (cfg, ins, ptr);
-			}
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_MONO_VTADDR: {
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			MonoInst *src_var, *src;
-			--sp;
-			src_var = get_vreg_to_inst (cfg, sp [0]->dreg);
-			EMIT_NEW_VARLOADA ((cfg), (src), src_var, src_var->inst_vtype);
-			*sp++ = src;
-			break;
-		}
-		case MONO_CEE_MONO_NEWOBJ: {
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			MonoInst *iargs [2];
-			klass = (MonoClass *)mono_method_get_wrapper_data (method, token);
-			mono_class_init_internal (klass);
-			NEW_CLASSCONST (cfg, iargs [0], klass);
-			MONO_ADD_INS (cfg->cbb, iargs [0]);
-			*sp++ = mono_emit_jit_icall (cfg, ves_icall_object_new, iargs);
-			inline_costs += CALL_COST * MIN(10, num_calls++);
-			break;
-		}
-		case MONO_CEE_MONO_OBJADDR:
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			--sp;
-			MONO_INST_NEW (cfg, ins, OP_MOVE);
-			ins->dreg = alloc_ireg_mp (cfg);
-			ins->sreg1 = sp [0]->dreg;
-			ins->type = STACK_MP;
-			MONO_ADD_INS (cfg->cbb, ins);
-			*sp++ = ins;
-			break;
-		case MONO_CEE_MONO_LDNATIVEOBJ:
-			/*
-			 * Similar to LDOBJ, but instead load the unmanaged
-			 * representation of the vtype to the stack.
-			 */
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			--sp;
-			klass = (MonoClass *)mono_method_get_wrapper_data (method, token);
-			g_assert (m_class_is_valuetype (klass));
-			mono_class_init_internal (klass);
-			{
-				MonoInst *src, *dest, *temp;
-				src = sp [0];
-				temp = mono_compile_create_var (cfg, m_class_get_byval_arg (klass), OP_LOCAL);
-				temp->backend.is_pinvoke = 1;
-				EMIT_NEW_TEMPLOADA (cfg, dest, temp->inst_c0);
-				mini_emit_memory_copy (cfg, dest, src, klass, TRUE, 0);
-				EMIT_NEW_TEMPLOAD (cfg, dest, temp->inst_c0);
-				dest->type = STACK_VTYPE;
-				dest->klass = klass;
-				*sp ++ = dest;
-			}
-			break;
-		case MONO_CEE_MONO_RETOBJ: {
-			/*
-			 * Same as RET, but return the native representation of a vtype
-			 * to the caller.
-			 */
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			g_assert (cfg->ret);
-			g_assert (mono_method_signature_internal (method)->pinvoke);
-			--sp;
-			klass = (MonoClass *)mono_method_get_wrapper_data (method, token);
-			if (!cfg->vret_addr) {
-				g_assert (cfg->ret_var_is_local);
-				EMIT_NEW_VARLOADA (cfg, ins, cfg->ret, cfg->ret->inst_vtype);
-			} else {
-				EMIT_NEW_RETLOADA (cfg, ins);
-			}
-			mini_emit_memory_copy (cfg, ins, sp [0], klass, TRUE, 0);
-			if (sp != stack_start)
-				UNVERIFIED;
-			if (!detached_before_ret)
-				mini_profiler_emit_leave (cfg, sp [0]);
-			MONO_INST_NEW (cfg, ins, OP_BR);
-			ins->inst_target_bb = end_bblock;
-			MONO_ADD_INS (cfg->cbb, ins);
-			link_bblock (cfg, cfg->cbb, end_bblock);
-			start_new_bblock = 1;
-			break;
-		}
-		case MONO_CEE_MONO_SAVE_LMF:
-		case MONO_CEE_MONO_RESTORE_LMF:
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			break;
-		case MONO_CEE_MONO_CLASSCONST:
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			EMIT_NEW_CLASSCONST (cfg, ins, mono_method_get_wrapper_data (method, token));
-			*sp++ = ins;
-			inline_costs += CALL_COST * MIN(10, num_calls++);
-			break;
-		case MONO_CEE_MONO_METHODCONST:
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			EMIT_NEW_METHODCONST (cfg, ins, mono_method_get_wrapper_data (method, token));
-			*sp++ = ins;
-			break;
-		case MONO_CEE_MONO_PINVOKE_ADDR_CACHE: {
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			MonoMethod *pinvoke_method = (MonoMethod*)mono_method_get_wrapper_data (method, token);
-			/* This is a memory slot used by the wrapper */
-			if (cfg->compile_aot) {
-				EMIT_NEW_AOTCONST (cfg, ins, MONO_PATCH_INFO_METHOD_PINVOKE_ADDR_CACHE, pinvoke_method);
-			} else {
-				gpointer addr = mono_mem_manager_alloc0 (cfg->mem_manager, sizeof (gpointer));
-				EMIT_NEW_PCONST (cfg, ins, addr);
-			}
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_MONO_NOT_TAKEN:
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			cfg->cbb->out_of_line = TRUE;
-			break;
-		case MONO_CEE_MONO_TLS: {
-			MonoTlsKey key;
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			key = (MonoTlsKey)n;
-			g_assert (key < TLS_KEY_NUM);
-			ins = mono_create_tls_get (cfg, key);
-			g_assert (ins);
-			ins->type = STACK_PTR;
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_MONO_DYN_CALL: {
-			MonoCallInst *call;
-			/* It would be easier to call a trampoline, but that would put an
-			 * extra frame on the stack, confusing exception handling. So
-			 * implement it inline using an opcode for now.
-			 */
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			if (!cfg->dyn_call_var) {
-				cfg->dyn_call_var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
-				/* prevent it from being register allocated */
-				cfg->dyn_call_var->flags |= MONO_INST_VOLATILE;
-			}
-			/* Has to use a call inst since local regalloc expects it */
-			MONO_INST_NEW_CALL (cfg, call, OP_DYN_CALL);
-			ins = (MonoInst*)call;
-			sp -= 2;
-			ins->sreg1 = sp [0]->dreg;
-			ins->sreg2 = sp [1]->dreg;
-			MONO_ADD_INS (cfg->cbb, ins);
-			cfg->param_area = MAX (cfg->param_area, GINT_TO_UINT(cfg->backend->dyn_call_param_area));
-			/* OP_DYN_CALL might need to allocate a dynamically sized param area */
-			cfg->flags |= MONO_CFG_HAS_ALLOCA;
-			inline_costs += CALL_COST * MIN(10, num_calls++);
-			break;
-		}
-		case MONO_CEE_MONO_MEMORY_BARRIER: {
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			mini_emit_memory_barrier (cfg, (int)n);
-			break;
-		}
-		case MONO_CEE_MONO_ATOMIC_STORE_I4: {
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			g_assert (mono_arch_opcode_supported (OP_ATOMIC_STORE_I4));
-			sp -= 2;
-			MONO_INST_NEW (cfg, ins, OP_ATOMIC_STORE_I4);
-			ins->dreg = sp [0]->dreg;
-			ins->sreg1 = sp [1]->dreg;
-			ins->backend.memory_barrier_kind = (int)n;
-			MONO_ADD_INS (cfg->cbb, ins);
-			break;
-		}
-		case MONO_CEE_MONO_LD_DELEGATE_METHOD_PTR: {
-			CHECK_STACK (1);
-			--sp;
-			int dreg = alloc_preg (cfg);
-			EMIT_NEW_LOAD_MEMBASE (cfg, ins, OP_LOAD_MEMBASE, dreg, sp [0]->dreg, MONO_STRUCT_OFFSET (MonoDelegate, method_ptr));
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_MONO_CALLI_EXTRA_ARG: {
-			MonoInst *addr;
-			MonoInst *arg;
-			/*
-			 * This is the same as CEE_CALLI, but passes an additional argument
-			 * to the called method in llvmonly mode.
-			 * This is only used by delegate invoke wrappers to call the
-			 * actual delegate method.
-			 */
-			g_assert (method->wrapper_type == MONO_WRAPPER_DELEGATE_INVOKE);
-			ins = NULL;
-			cmethod = NULL;
-			CHECK_STACK (1);
-			--sp;
-			addr = *sp;
-			fsig = mini_get_signature (method, token, generic_context, cfg->error);
-			CHECK_CFG_ERROR;
-			if (cfg->llvm_only)
-				cfg->signatures = g_slist_prepend_mempool (cfg->mempool, cfg->signatures, fsig);
-			n = fsig->param_count + fsig->hasthis + 1;
-			CHECK_STACK (n);
-			sp -= n;
-			arg = sp [n - 1];
-			if (cfg->llvm_only) {
-				/*
-				 * The lowest bit of 'arg' determines whenever the callee uses the gsharedvt
-				 * cconv. This is set by mono_init_delegate ().
-				 */
-				if (cfg->gsharedvt && mini_is_gsharedvt_variable_signature (fsig)) {
-					MonoInst *callee = addr;
-					MonoInst *call, *localloc_ins;
-					MonoBasicBlock *is_gsharedvt_bb, *end_bb;
-					int low_bit_reg = alloc_preg (cfg);
-					NEW_BBLOCK (cfg, is_gsharedvt_bb);
-					NEW_BBLOCK (cfg, end_bb);
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_PAND_IMM, low_bit_reg, arg->dreg, 1);
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, low_bit_reg, 0);
-					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBNE_UN, is_gsharedvt_bb);
-					/* Normal case: callee uses a normal cconv, have to add an out wrapper */
-					addr = emit_get_rgctx_sig (cfg, context_used,
-											   fsig, MONO_RGCTX_INFO_SIG_GSHAREDVT_OUT_TRAMPOLINE_CALLI);
-					/*
-					 * ADDR points to a gsharedvt-out wrapper, have to pass <callee, arg> as an extra arg.
-					 */
-					MONO_INST_NEW (cfg, ins, OP_LOCALLOC_IMM);
-					ins->dreg = alloc_preg (cfg);
-					ins->inst_imm = 2 * TARGET_SIZEOF_VOID_P;
-					MONO_ADD_INS (cfg->cbb, ins);
-					localloc_ins = ins;
-					cfg->flags |= MONO_CFG_HAS_ALLOCA;
-					MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, localloc_ins->dreg, 0, callee->dreg);
-					MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, localloc_ins->dreg, TARGET_SIZEOF_VOID_P, arg->dreg);
-					call = mini_emit_extra_arg_calli (cfg, fsig, sp, localloc_ins->dreg, addr);
-					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-					/* Gsharedvt case: callee uses a gsharedvt cconv, no conversion is needed */
-					MONO_START_BB (cfg, is_gsharedvt_bb);
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_PXOR_IMM, arg->dreg, arg->dreg, 1);
-					ins = mini_emit_extra_arg_calli (cfg, fsig, sp, arg->dreg, callee);
-					ins->dreg = call->dreg;
-					MONO_START_BB (cfg, end_bb);
-				} else {
-					/* Caller uses a normal calling conv */
-					MonoInst *callee = addr;
-					MonoInst *call, *localloc_ins;
-					MonoBasicBlock *is_gsharedvt_bb, *end_bb;
-					int low_bit_reg = alloc_preg (cfg);
-					NEW_BBLOCK (cfg, is_gsharedvt_bb);
-					NEW_BBLOCK (cfg, end_bb);
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_PAND_IMM, low_bit_reg, arg->dreg, 1);
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, low_bit_reg, 0);
-					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBNE_UN, is_gsharedvt_bb);
-					/* Normal case: callee uses a normal cconv, no conversion is needed */
-					call = mini_emit_extra_arg_calli (cfg, fsig, sp, arg->dreg, callee);
-					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-					/* Gsharedvt case: callee uses a gsharedvt cconv, have to add an in wrapper */
-					MONO_START_BB (cfg, is_gsharedvt_bb);
-					MONO_EMIT_NEW_BIALU_IMM (cfg, OP_PXOR_IMM, arg->dreg, arg->dreg, 1);
-					NEW_AOTCONST (cfg, addr, MONO_PATCH_INFO_GSHAREDVT_IN_WRAPPER, fsig);
-					MONO_ADD_INS (cfg->cbb, addr);
-					/*
-					 * ADDR points to a gsharedvt-in wrapper, have to pass <callee, arg> as an extra arg.
-					 */
-					MONO_INST_NEW (cfg, ins, OP_LOCALLOC_IMM);
-					ins->dreg = alloc_preg (cfg);
-					ins->inst_imm = 2 * TARGET_SIZEOF_VOID_P;
-					MONO_ADD_INS (cfg->cbb, ins);
-					localloc_ins = ins;
-					cfg->flags |= MONO_CFG_HAS_ALLOCA;
-					MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, localloc_ins->dreg, 0, callee->dreg);
-					MONO_EMIT_NEW_STORE_MEMBASE (cfg, OP_STORE_MEMBASE_REG, localloc_ins->dreg, TARGET_SIZEOF_VOID_P, arg->dreg);
-					ins = mini_emit_extra_arg_calli (cfg, fsig, sp, localloc_ins->dreg, addr);
-					ins->dreg = call->dreg;
-					MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-					MONO_START_BB (cfg, end_bb);
-				}
-			} else {
-				/* Same as CEE_CALLI */
-				if (cfg->gsharedvt && mini_is_gsharedvt_signature (fsig)) {
-					/*
-					 * We pass the address to the gsharedvt trampoline in the rgctx reg
-					 */
-					MonoInst *callee = addr;
-					addr = emit_get_rgctx_sig (cfg, context_used,
-											   fsig, MONO_RGCTX_INFO_SIG_GSHAREDVT_OUT_TRAMPOLINE_CALLI);
-					ins = (MonoInst*)mini_emit_calli (cfg, fsig, sp, addr, NULL, callee);
-				} else {
-					ins = (MonoInst*)mini_emit_calli (cfg, fsig, sp, addr, NULL, NULL);
-				}
-			}
-			if (!MONO_TYPE_IS_VOID (fsig->ret))
-				*sp++ = mono_emit_widen_call_res (cfg, ins, fsig);
-			CHECK_CFG_EXCEPTION;
-			ins_flag = 0;
-			constrained_class = NULL;
-			break;
-		}
-		case MONO_CEE_MONO_LDDOMAIN: {
-			MonoDomain *domain = mono_get_root_domain ();
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			EMIT_NEW_PCONST (cfg, ins, cfg->compile_aot ? NULL : domain);
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_MONO_SAVE_LAST_ERROR:
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			save_last_error = TRUE;
-			break;
-		case MONO_CEE_MONO_GET_RGCTX_ARG:
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			mono_create_rgctx_var (cfg);
-			MONO_INST_NEW (cfg, ins, OP_MOVE);
-			ins->dreg = alloc_dreg (cfg, STACK_PTR);
-			ins->sreg1 = cfg->rgctx_var->dreg;
-			ins->type = STACK_PTR;
-			MONO_ADD_INS (cfg->cbb, ins);
-			*sp++ = ins;
-			break;
-		case MONO_CEE_MONO_GET_SP: {
-			/* Used by COOP only, so this is good enough */
-			MonoInst *var = mono_compile_create_var (cfg, mono_get_int_type (), OP_LOCAL);
-			EMIT_NEW_VARLOADA (cfg, ins, var, NULL);
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_MONO_REMAP_OVF_EXC:
-			/* Remap the exception thrown by the next _OVF opcode */
-			g_assert (method->wrapper_type != MONO_WRAPPER_NONE);
-			ovf_exc = (const char*)mono_method_get_wrapper_data (method, token);
-			break;
-		case MONO_CEE_ARGLIST: {
-			/* somewhat similar to LDTOKEN */
-			MonoInst *addr, *vtvar;
-			vtvar = mono_compile_create_var (cfg, m_class_get_byval_arg (mono_defaults.argumenthandle_class), OP_LOCAL);
-			EMIT_NEW_TEMPLOADA (cfg, addr, vtvar->inst_c0);
-			EMIT_NEW_UNALU (cfg, ins, OP_ARGLIST, -1, addr->dreg);
-			EMIT_NEW_TEMPLOAD (cfg, ins, vtvar->inst_c0);
-			ins->type = STACK_VTYPE;
-			ins->klass = mono_defaults.argumenthandle_class;
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_CEQ:
-		case MONO_CEE_CGT:
-		case MONO_CEE_CGT_UN:
-		case MONO_CEE_CLT:
-		case MONO_CEE_CLT_UN: {
-			MonoInst *cmp, *arg1, *arg2;
-			sp -= 2;
-			arg1 = sp [0];
-			arg2 = sp [1];
-			/*
-			 * The following transforms:
-			 *    CEE_CEQ    into OP_CEQ
-			 *    CEE_CGT    into OP_CGT
-			 *    CEE_CGT_UN into OP_CGT_UN
-			 *    CEE_CLT    into OP_CLT
-			 *    CEE_CLT_UN into OP_CLT_UN
-			 */
-			MONO_INST_NEW (cfg, cmp, (OP_CEQ - CEE_CEQ) + ip [1]);
-			MONO_INST_NEW (cfg, ins, cmp->opcode);
-			cmp->sreg1 = arg1->dreg;
-			cmp->sreg2 = arg2->dreg;
-			type_from_op (cfg, cmp, arg1, arg2);
-			CHECK_TYPE (cmp);
-			add_widen_op (cfg, cmp, &arg1, &arg2);
-			if ((arg1->type == STACK_I8) || ((TARGET_SIZEOF_VOID_P == 8) && ((arg1->type == STACK_PTR) || (arg1->type == STACK_OBJ) || (arg1->type == STACK_MP))))
-				cmp->opcode = OP_LCOMPARE;
-			else if (arg1->type == STACK_R4)
-				cmp->opcode = OP_RCOMPARE;
-			else if (arg1->type == STACK_R8)
-				cmp->opcode = OP_FCOMPARE;
-			else
-				cmp->opcode = OP_ICOMPARE;
-			MONO_ADD_INS (cfg->cbb, cmp);
-			ins->type = STACK_I4;
-			ins->dreg = alloc_dreg (cfg, (MonoStackType)ins->type);
-			type_from_op (cfg, ins, arg1, arg2);
-			if (cmp->opcode == OP_FCOMPARE || cmp->opcode == OP_RCOMPARE) {
-				/*
-				 * The backends expect the fceq opcodes to do the
-				 * comparison too.
-				 */
-				ins->sreg1 = cmp->sreg1;
-				ins->sreg2 = cmp->sreg2;
-				NULLIFY_INS (cmp);
-			}
-			MONO_ADD_INS (cfg->cbb, ins);
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_LDFTN: {
-			MonoInst *argconst;
-			MonoMethod *cil_method;
-			gboolean gshared_static_virtual = FALSE;
-			cil_method = cmethod = mini_get_method (cfg, method, n, NULL, generic_context);
-			CHECK_CFG_ERROR;
-			if (!dont_verify && !cfg->skip_visibility && !mono_method_can_access_method (method, cmethod))
-				emit_method_access_failure (cfg, method, cil_method);
-			if (constrained_class) {
-				if (m_method_is_static (cmethod) && mini_class_check_context_used (cfg, constrained_class)) {
-					gshared_static_virtual = TRUE;
-				} else {
-					cmethod = get_constrained_method (cfg, image, n, cmethod, constrained_class, generic_context);
-					CHECK_CFG_ERROR;
-					if (mono_class_has_dim_conflicts (constrained_class) &&
-							mono_class_is_method_ambiguous (constrained_class, cil_method))
-						mono_emit_jit_icall (cfg, mono_throw_ambiguous_implementation, NULL);
-					constrained_class = NULL;
-				}
-			} else {
-				mono_save_token_info (cfg, image, n, cmethod);
-			}
-			mono_class_init_internal (cmethod->klass);
-			context_used = mini_method_check_context_used (cfg, cmethod);
-			const gboolean has_unmanaged_callers_only =
-				cmethod->wrapper_type == MONO_WRAPPER_NONE &&
-				mono_method_has_unmanaged_callers_only_attribute (cmethod);
-			/*
-			 * Optimize the common case of ldftn+delegate creation
-			 */
-			if (!gshared_static_virtual && (sp > stack_start) && (next_ip + 4 < end) && ip_in_bb (cfg, cfg->cbb, next_ip) && (next_ip [0] == CEE_NEWOBJ)) {
-				MonoMethod *ctor_method = mini_get_method (cfg, method, read32 (next_ip + 1), NULL, generic_context);
-				if (ctor_method && (m_class_get_parent (ctor_method->klass) == mono_defaults.multicastdelegate_class)) {
-					MonoInst *target_ins, *handle_ins;
-					MonoMethod *invoke;
-					int invoke_context_used;
-					if (G_UNLIKELY (has_unmanaged_callers_only)) {
-						mono_error_set_not_supported (cfg->error, "Cannot create delegate from method with UnmanagedCallersOnlyAttribute");
-						CHECK_CFG_ERROR;
-					}
-					invoke = mono_get_delegate_invoke_internal (ctor_method->klass);
-					if (!invoke || !mono_method_signature_internal (invoke))
-						LOAD_ERROR;
-					invoke_context_used = mini_method_check_context_used (cfg, invoke);
-					target_ins = sp [-1];
-					if (!(cmethod->flags & METHOD_ATTRIBUTE_STATIC)) {
-						/*BAD IMPL: We must not add a null check for virtual invoke delegates.*/
-						if (mono_method_signature_internal (invoke)->param_count == mono_method_signature_internal (cmethod)->param_count) {
-							MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, target_ins->dreg, 0);
-							MONO_EMIT_NEW_COND_EXC (cfg, EQ, "ArgumentException");
-						}
-					}
-					if ((invoke_context_used == 0 || !cfg->gsharedvt) || cfg->llvm_only) {
-						if (cfg->verbose_level > 3)
-							g_print ("converting (in B%d: stack: %d) %s", cfg->cbb->block_num, GPTRDIFF_TO_INT (sp - stack_start), mono_disasm_code_one (NULL, method, ip + 6, NULL));
-						if ((handle_ins = handle_delegate_ctor (cfg, ctor_method->klass, target_ins, cmethod, context_used, invoke_context_used, FALSE))) {
-							sp --;
-							*sp = handle_ins;
-							CHECK_CFG_EXCEPTION;
-							sp ++;
-							next_ip += 5;
-							il_op = MONO_CEE_NEWOBJ;
-							break;
-						} else {
-							CHECK_CFG_ERROR;
-						}
-					}
-				}
-			}
-			/* UnmanagedCallersOnlyAttribute means ldftn should return a method callable from native */
-			if (G_UNLIKELY (has_unmanaged_callers_only)) {
-				if (G_UNLIKELY (cmethod->flags & METHOD_ATTRIBUTE_PINVOKE_IMPL)) {
-					emit_not_supported_failure (cfg);
-					EMIT_NEW_PCONST (cfg, ins, NULL);
-					*sp++ = ins;
-					inline_costs += CALL_COST * MIN(10, num_calls++);
-					break;
-				}
-				MonoClass *delegate_klass = NULL;
-				MonoGCHandle target_handle = 0;
-				ERROR_DECL (wrapper_error);
-				MonoMethod *wrapped_cmethod;
-				wrapped_cmethod = mono_marshal_get_managed_wrapper (cmethod, delegate_klass, target_handle, wrapper_error);
-				if (!is_ok (wrapper_error)) {
-					/* if we couldn't create a wrapper because cmethod isn't supposed to have an
-					UnmanagedCallersOnly attribute, follow CoreCLR behavior and throw when the
-					method with the ldftn is executing, not when it is being compiled. */
-					emit_invalid_program_with_msg (cfg, wrapper_error, method, cmethod);
-					mono_error_cleanup (wrapper_error);
-					EMIT_NEW_PCONST (cfg, ins, NULL);
-					*sp++ = ins;
-					inline_costs += CALL_COST * MIN(10, num_calls++);
-					break;
-				} else {
-					cmethod = wrapped_cmethod;
-				}
-			}
-			if (gshared_static_virtual) {
-				argconst = emit_get_rgctx_virt_method (cfg, -1, constrained_class, cmethod, MONO_RGCTX_INFO_VIRT_METHOD);
-				constrained_class = NULL;
-			} else {
-				argconst = emit_get_rgctx_method (cfg, context_used, cmethod, MONO_RGCTX_INFO_METHOD);
-			}
-			ins = mono_emit_jit_icall (cfg, mono_ldftn, &argconst);
-			*sp++ = ins;
-			inline_costs += CALL_COST * MIN(10, num_calls++);
-			break;
-		}
-		case MONO_CEE_LDVIRTFTN: {
-			MonoInst *args [2];
-			cmethod = mini_get_method (cfg, method, n, NULL, generic_context);
-			CHECK_CFG_ERROR;
-			mono_class_init_internal (cmethod->klass);
-			context_used = mini_method_check_context_used (cfg, cmethod);
-			/*
-			 * Optimize the common case of ldvirtftn+delegate creation
-			 */
-			if (previous_il_op == MONO_CEE_DUP && (sp > stack_start) && (next_ip + 4 < end) && ip_in_bb (cfg, cfg->cbb, next_ip) && (next_ip [0] == CEE_NEWOBJ)) {
-				MonoMethod *ctor_method = mini_get_method (cfg, method, read32 (next_ip + 1), NULL, generic_context);
-				if (ctor_method && (m_class_get_parent (ctor_method->klass) == mono_defaults.multicastdelegate_class)) {
-					MonoInst *target_ins, *handle_ins;
-					MonoMethod *invoke;
-					int invoke_context_used;
-					const gboolean is_virtual = (cmethod->flags & METHOD_ATTRIBUTE_VIRTUAL) != 0;
-					invoke = mono_get_delegate_invoke_internal (ctor_method->klass);
-					if (!invoke || !mono_method_signature_internal (invoke))
-						LOAD_ERROR;
-					invoke_context_used = mini_method_check_context_used (cfg, invoke);
-					target_ins = sp [-1];
-					if (invoke_context_used == 0 || !cfg->gsharedvt || cfg->llvm_only) {
-						if (cfg->verbose_level > 3)
-							g_print ("converting (in B%d: stack: %d) %s", cfg->cbb->block_num, GPTRDIFF_TO_INT (sp - stack_start), mono_disasm_code_one (NULL, method, ip + 6, NULL));
-						if ((handle_ins = handle_delegate_ctor (cfg, ctor_method->klass, target_ins, cmethod, context_used, invoke_context_used, is_virtual))) {
-							sp -= 2;
-							*sp = handle_ins;
-							CHECK_CFG_EXCEPTION;
-							next_ip += 5;
-							previous_il_op = MONO_CEE_NEWOBJ;
-							sp ++;
-							break;
-						} else {
-							CHECK_CFG_ERROR;
-						}
-					}
-				}
-			}
-			--sp;
-			args [0] = *sp;
-			args [1] = emit_get_rgctx_method (cfg, context_used,
-											  cmethod, MONO_RGCTX_INFO_METHOD);
-			if (context_used)
-				*sp++ = mono_emit_jit_icall (cfg, mono_ldvirtfn_gshared, args);
-			else
-				*sp++ = mono_emit_jit_icall (cfg, mono_ldvirtfn, args);
-			inline_costs += CALL_COST * MIN(10, num_calls++);
-			break;
-		}
-		case MONO_CEE_LOCALLOC: {
-			MonoBasicBlock *non_zero_bb, *end_bb;
-			int alloc_ptr = alloc_preg (cfg);
-			--sp;
-			if (sp != stack_start)
-				UNVERIFIED;
-			if (cfg->method != method)
-				/*
-				 * Inlining this into a loop in a parent could lead to
-				 * stack overflows which is different behavior than the
-				 * non-inlined case, thus disable inlining in this case.
-				 */
-				INLINE_FAILURE("localloc");
-			NEW_BBLOCK (cfg, non_zero_bb);
-			NEW_BBLOCK (cfg, end_bb);
-			/* if size != zero */
-			MONO_EMIT_NEW_BIALU_IMM (cfg, OP_COMPARE_IMM, -1, sp [0]->dreg, 0);
-			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_PBNE_UN, non_zero_bb);
-			MONO_EMIT_NEW_PCONST (cfg, alloc_ptr, NULL);
-			MONO_EMIT_NEW_BRANCH_BLOCK (cfg, OP_BR, end_bb);
-			MONO_START_BB (cfg, non_zero_bb);
-			MONO_INST_NEW (cfg, ins, OP_LOCALLOC);
-			ins->dreg = alloc_ptr;
-			ins->sreg1 = sp [0]->dreg;
-			ins->type = STACK_PTR;
-			MONO_ADD_INS (cfg->cbb, ins);
-			cfg->flags |= MONO_CFG_HAS_ALLOCA;
-			if (header->init_locals)
-				ins->flags |= MONO_INST_INIT;
-			MONO_START_BB (cfg, end_bb);
-			EMIT_NEW_UNALU (cfg, ins, OP_MOVE, alloc_preg (cfg), alloc_ptr);
-			ins->type = STACK_PTR;
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_ENDFILTER: {
-			MonoExceptionClause *clause, *nearest;
-			--sp;
-			if ((sp != stack_start) || (sp [0]->type != STACK_I4))
-				UNVERIFIED;
-			MONO_INST_NEW (cfg, ins, OP_ENDFILTER);
-			ins->sreg1 = (*sp)->dreg;
-			MONO_ADD_INS (cfg->cbb, ins);
-			start_new_bblock = 1;
-			nearest = NULL;
-			for (guint cc = 0; cc < header->num_clauses; ++cc) {
-				clause = &header->clauses [cc];
-				if ((clause->flags & MONO_EXCEPTION_CLAUSE_FILTER) &&
-					(GPTRDIFF_TO_UINT32(next_ip - header->code) > clause->data.filter_offset && GPTRDIFF_TO_UINT32(next_ip - header->code) <= clause->handler_offset) &&
-				    (!nearest || (clause->data.filter_offset < nearest->data.filter_offset)))
-					nearest = clause;
-			}
-			g_assert (nearest);
-			if ((next_ip - header->code) != nearest->handler_offset)
-				UNVERIFIED;
-			break;
-		}
-		case MONO_CEE_UNALIGNED_:
-			ins_flag |= MONO_INST_UNALIGNED;
-			/* FIXME: record alignment? we can assume 1 for now */
-			break;
-		case MONO_CEE_VOLATILE_:
-			ins_flag |= MONO_INST_VOLATILE;
-			break;
-		case MONO_CEE_TAIL_:
-			ins_flag   |= MONO_INST_TAILCALL;
-			cfg->flags |= MONO_CFG_HAS_TAILCALL;
-			/* Can't inline tailcalls at this time */
-			inline_costs += 100000;
-			break;
-		case MONO_CEE_INITOBJ:
-			--sp;
-			klass = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (klass);
-			if (mini_class_is_reference (klass))
-				MONO_EMIT_NEW_STORE_MEMBASE_IMM (cfg, OP_STORE_MEMBASE_IMM, sp [0]->dreg, 0, 0);
-			else
-				mini_emit_initobj (cfg, *sp, NULL, klass);
-			inline_costs += 1;
-			break;
-		case MONO_CEE_CONSTRAINED_:
-			constrained_class = mini_get_class (method, token, generic_context);
-			CHECK_TYPELOAD (constrained_class);
-			ins_has_side_effect = FALSE;
-			break;
-		case MONO_CEE_CPBLK:
-			sp -= 3;
-			mini_emit_memory_copy_bytes (cfg, sp [0], sp [1], sp [2], ins_flag);
-			ins_flag = 0;
-			inline_costs += 1;
-			break;
-		case MONO_CEE_INITBLK:
-			sp -= 3;
-			mini_emit_memory_init_bytes (cfg, sp [0], sp [1], sp [2], ins_flag);
-			ins_flag = 0;
-			inline_costs += 1;
-			break;
-		case MONO_CEE_NO_:
-			if (ip [2] & CEE_NO_TYPECHECK)
-				ins_flag |= MONO_INST_NOTYPECHECK;
-			if (ip [2] & CEE_NO_RANGECHECK)
-				ins_flag |= MONO_INST_NORANGECHECK;
-			if (ip [2] & CEE_NO_NULLCHECK)
-				ins_flag |= MONO_INST_NONULLCHECK;
-			break;
-		case MONO_CEE_RETHROW: {
-			MonoInst *load;
-			int handler_offset = -1;
-			for (unsigned int i = 0; i < header->num_clauses; ++i) {
-				MonoExceptionClause *clause = &header->clauses [i];
-				if (MONO_OFFSET_IN_HANDLER (clause, GPTRDIFF_TO_UINT32(ip - header->code)) && !(clause->flags & MONO_EXCEPTION_CLAUSE_FINALLY)) {
-					handler_offset = clause->handler_offset;
-					break;
-				}
-			}
-			cfg->cbb->flags |= BB_EXCEPTION_UNSAFE;
-			if (handler_offset == -1)
-				UNVERIFIED;
-			EMIT_NEW_TEMPLOAD (cfg, load, mono_find_exvar_for_offset (cfg, handler_offset)->inst_c0);
-			MONO_INST_NEW (cfg, ins, OP_RETHROW);
-			ins->sreg1 = load->dreg;
-			MONO_ADD_INS (cfg->cbb, ins);
-			MONO_INST_NEW (cfg, ins, OP_NOT_REACHED);
-			MONO_ADD_INS (cfg->cbb, ins);
-			sp = stack_start;
-			link_bblock (cfg, cfg->cbb, end_bblock);
-			start_new_bblock = 1;
-			break;
-		}
-		case MONO_CEE_MONO_RETHROW: {
-			if (sp [-1]->type != STACK_OBJ)
-				UNVERIFIED;
-			MONO_INST_NEW (cfg, ins, OP_RETHROW);
-			--sp;
-			ins->sreg1 = sp [0]->dreg;
-			cfg->cbb->out_of_line = TRUE;
-			MONO_ADD_INS (cfg->cbb, ins);
-			MONO_INST_NEW (cfg, ins, OP_NOT_REACHED);
-			MONO_ADD_INS (cfg->cbb, ins);
-			sp = stack_start;
-			link_bblock (cfg, cfg->cbb, end_bblock);
-			start_new_bblock = 1;
-			/* This can complicate code generation for llvm since the return value might not be defined */
-			if (COMPILE_LLVM (cfg))
-				INLINE_FAILURE ("mono_rethrow");
-			break;
-		}
-		case MONO_CEE_SIZEOF: {
-			guint32 val;
-			int ialign;
-			if (mono_metadata_token_table (token) == MONO_TABLE_TYPESPEC && !image_is_dynamic (m_class_get_image (method->klass)) && !generic_context) {
-				MonoType *type = mono_type_create_from_typespec_checked (image, token, cfg->error);
-				CHECK_CFG_ERROR;
-				val = mono_type_size (type, &ialign);
-				EMIT_NEW_ICONST (cfg, ins, val);
-			} else {
-				klass = mini_get_class (method, token, generic_context);
-				CHECK_TYPELOAD (klass);
-				if (mini_is_gsharedvt_klass (klass)) {
-					ins = mini_emit_get_gsharedvt_info_klass (cfg, klass, MONO_RGCTX_INFO_CLASS_SIZEOF);
-					ins->type = STACK_I4;
-				} else {
-					val = mono_type_size (m_class_get_byval_arg (klass), &ialign);
-					EMIT_NEW_ICONST (cfg, ins, val);
-				}
-			}
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_REFANYTYPE: {
-			MonoInst *src_var, *src;
-			GSHAREDVT_FAILURE (il_op);
-			--sp;
-			src_var = get_vreg_to_inst (cfg, sp [0]->dreg);
-			if (!src_var)
-				src_var = mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (mono_defaults.typed_reference_class), OP_LOCAL, sp [0]->dreg);
-			EMIT_NEW_VARLOADA (cfg, src, src_var, src_var->inst_vtype);
-			EMIT_NEW_LOAD_MEMBASE_TYPE (cfg, ins, m_class_get_byval_arg (mono_defaults.typehandle_class), src->dreg, MONO_STRUCT_OFFSET (MonoTypedRef, type));
-			*sp++ = ins;
-			break;
-		}
-		case MONO_CEE_READONLY_:
-			readonly = TRUE;
-			break;
-		case MONO_CEE_UNUSED56:
-		case MONO_CEE_UNUSED57:
-		case MONO_CEE_UNUSED70:
-		case MONO_CEE_UNUSED:
-		case MONO_CEE_UNUSED99:
-		case MONO_CEE_UNUSED58:
-		case MONO_CEE_UNUSED1:
-			UNVERIFIED;
-		default:
-			g_warning ("opcode 0x%02x not handled", il_op);
-			UNVERIFIED;
-		}
-		if (ins_has_side_effect)
-			cfg->cbb->flags |= BB_HAS_SIDE_EFFECTS;
-	}
-	if (start_new_bblock != 1)
-		UNVERIFIED;
-	cfg->cbb->cil_length = GPTRDIFF_TO_INT32 (ip - cfg->cbb->cil_code);
-	if (cfg->cbb->next_bb) {
-		/* This could already be set because of inlining, #693905 */
-		MonoBasicBlock *cbb = cfg->cbb;
-		while (cbb->next_bb)
-			cbb = cbb->next_bb;
-		cbb->next_bb = end_bblock;
-	} else {
-		cfg->cbb->next_bb = end_bblock;
-	}
-#if defined(TARGET_POWERPC) || defined(TARGET_X86)
-	if (cfg->compile_aot)
-		/* FIXME: The plt slots require a GOT var even if the method doesn't use it */
-		mono_get_got_var (cfg);
-#endif
-#ifdef TARGET_WASM
-	if (cfg->lmf_var && !cfg->deopt) {
-		cfg->cbb = init_localsbb;
-		EMIT_NEW_VARLOADA (cfg, ins, cfg->lmf_var, NULL);
-		int lmf_reg = ins->dreg;
-		EMIT_NEW_STORE_MEMBASE (cfg, ins, OP_STORE_MEMBASE_IMM, lmf_reg, MONO_STRUCT_OFFSET (MonoLMF, previous_lmf), 0);
-	}
-#endif
-	if (cfg->method == method && cfg->got_var)
-		mono_emit_load_got_addr (cfg);
-	if (init_localsbb) {
-		cfg->cbb = init_localsbb;
-		cfg->ip = NULL;
-		for (int i = 0; i < header->num_locals; ++i) {
-			/*
-			 * Vtype initialization might need to be done after CEE_JIT_ATTACH, since it can make calls to memset (),
-			 * which need the trampoline code to work.
-			 */
-			if (MONO_TYPE_ISSTRUCT (header->locals [i]))
-				cfg->cbb = init_localsbb2;
-			else
-				cfg->cbb = init_localsbb;
-			emit_init_local (cfg, i, header->locals [i], init_locals);
-		}
-	}
-	if (cfg->init_ref_vars && cfg->method == method) {
-		/* Emit initialization for ref vars */
-		for (guint i = 0; i < cfg->num_varinfo; ++i) {
-			MonoInst *var_ins = cfg->varinfo [i];
-			if (var_ins->opcode == OP_LOCAL && var_ins->type == STACK_OBJ)
-				MONO_EMIT_NEW_PCONST (cfg, var_ins->dreg, NULL);
-		}
-	}
-	if (cfg->lmf_var && cfg->method == method && !cfg->llvm_only) {
-		cfg->cbb = init_localsbb;
-		emit_push_lmf (cfg);
-	}
-	/* emit profiler enter code after a jit attach if there is one */
-	cfg->cbb = init_localsbb2;
-	mini_profiler_emit_enter (cfg);
-	cfg->cbb = init_localsbb;
-	if (seq_points) {
-		MonoBasicBlock *cbb;
-		/*
-		 * Make seq points at backward branch targets interruptible.
-		 */
-		for (cbb = cfg->bb_entry; cbb; cbb = cbb->next_bb)
-			if (cbb->code && cbb->in_count > 1 && cbb->code->opcode == OP_SEQ_POINT)
-				cbb->code->flags |= MONO_INST_SINGLE_STEP_LOC;
-	}
-	/* Add a sequence point for method entry/exit events */
-	if (seq_points && cfg->gen_sdb_seq_points) {
-		NEW_SEQ_POINT (cfg, ins, METHOD_ENTRY_IL_OFFSET, FALSE);
-		MONO_ADD_INS (init_localsbb, ins);
-		NEW_SEQ_POINT (cfg, ins, METHOD_EXIT_IL_OFFSET, FALSE);
-		MONO_ADD_INS (cfg->bb_exit, ins);
-	}
-	/*
-	 * Add seq points for IL offsets which have line number info, but wasn't generated a seq point during JITting because
-	 * the code they refer to was dead (#11880).
-	 */
-	if (sym_seq_points) {
-		for (guint32 i = 0; i < header->code_size; ++i) {
-			if (mono_bitset_test_fast (seq_point_locs, i) && !mono_bitset_test_fast (seq_point_set_locs, i)) {
-				MonoInst *seq_point_ins;
-				NEW_SEQ_POINT (cfg, seq_point_ins, i, FALSE);
-				mono_add_seq_point (cfg, NULL, seq_point_ins, SEQ_POINT_NATIVE_OFFSET_DEAD_CODE);
-			}
-		}
-	}
-	cfg->ip = NULL;
-	if (cfg->method == method) {
-		compute_bb_regions (cfg);
-	} else {
-		MonoBasicBlock *cbb;
-		/* get_most_deep_clause () in mini-llvm.c depends on this for inlined bblocks */
-		for (cbb = start_bblock; cbb != end_bblock; cbb  = cbb->next_bb) {
-			cbb->real_offset = inline_offset;
-		}
-	}
-	if (inline_costs < 0) {
-		char *mname;
-		/* Method is too large */
-		mname = mono_method_full_name (method, TRUE);
-		mono_cfg_set_exception_invalid_program (cfg, g_strdup_printf ("Method %s is too complex.", mname));
-		g_free (mname);
-	}
-	if ((cfg->verbose_level > 2) && (cfg->method == method))
-		mono_print_code (cfg, "AFTER METHOD-TO-IR");
-	goto cleanup;
-mono_error_exit:
-	if (cfg->verbose_level > 3)
-		g_print ("exiting due to error\n");
-	g_assert (!is_ok (cfg->error));
-	goto cleanup;
- exception_exit:
-	if (cfg->verbose_level > 3)
-		g_print ("exiting due to exception\n");
-	g_assert (cfg->exception_type != MONO_EXCEPTION_NONE);
-	goto cleanup;
- unverified:
-	if (cfg->verbose_level > 3)
-		g_print ("exiting due to invalid il\n");
-	set_exception_type_from_invalid_il (cfg, method, ip);
-	goto cleanup;
- cleanup:
-	g_slist_free (class_inits);
-	mono_basic_block_free (original_bb);
-	cfg->dont_inline = g_list_remove (cfg->dont_inline, method);
-	if (cfg->exception_type)
-		return -1;
-	else
-		return inline_costs;
-}
-static int
-store_membase_reg_to_store_membase_imm (int opcode)
-{
-	switch (opcode) {
-	case OP_STORE_MEMBASE_REG:
-		return OP_STORE_MEMBASE_IMM;
-	case OP_STOREI1_MEMBASE_REG:
-		return OP_STOREI1_MEMBASE_IMM;
-	case OP_STOREI2_MEMBASE_REG:
-		return OP_STOREI2_MEMBASE_IMM;
-	case OP_STOREI4_MEMBASE_REG:
-		return OP_STOREI4_MEMBASE_IMM;
-	case OP_STOREI8_MEMBASE_REG:
-		return OP_STOREI8_MEMBASE_IMM;
-	default:
-		g_assert_not_reached ();
-	}
-	return -1;
-}
-int
-mono_op_to_op_imm (int opcode)
-{
-	switch (opcode) {
-	case OP_IADD:
-		return OP_IADD_IMM;
-	case OP_ISUB:
-		return OP_ISUB_IMM;
-	case OP_IDIV:
-		return OP_IDIV_IMM;
-	case OP_IDIV_UN:
-		return OP_IDIV_UN_IMM;
-	case OP_IREM:
-		return OP_IREM_IMM;
-	case OP_IREM_UN:
-		return OP_IREM_UN_IMM;
-	case OP_IMUL:
-		return OP_IMUL_IMM;
-	case OP_IAND:
-		return OP_IAND_IMM;
-	case OP_IOR:
-		return OP_IOR_IMM;
-	case OP_IXOR:
-		return OP_IXOR_IMM;
-	case OP_ISHL:
-		return OP_ISHL_IMM;
-	case OP_ISHR:
-		return OP_ISHR_IMM;
-	case OP_ISHR_UN:
-		return OP_ISHR_UN_IMM;
-	case OP_LADD:
-		return OP_LADD_IMM;
-	case OP_LSUB:
-		return OP_LSUB_IMM;
-	case OP_LAND:
-		return OP_LAND_IMM;
-	case OP_LOR:
-		return OP_LOR_IMM;
-	case OP_LXOR:
-		return OP_LXOR_IMM;
-	case OP_LSHL:
-		return OP_LSHL_IMM;
-	case OP_LSHR:
-		return OP_LSHR_IMM;
-	case OP_LSHR_UN:
-		return OP_LSHR_UN_IMM;
-#if SIZEOF_REGISTER == 8
-	case OP_LMUL:
-		return OP_LMUL_IMM;
-	case OP_LREM:
-		return OP_LREM_IMM;
-#endif
-	case OP_COMPARE:
-		return OP_COMPARE_IMM;
-	case OP_ICOMPARE:
-		return OP_ICOMPARE_IMM;
-	case OP_LCOMPARE:
-		return OP_LCOMPARE_IMM;
-	case OP_STORE_MEMBASE_REG:
-		return OP_STORE_MEMBASE_IMM;
-	case OP_STOREI1_MEMBASE_REG:
-		return OP_STOREI1_MEMBASE_IMM;
-	case OP_STOREI2_MEMBASE_REG:
-		return OP_STOREI2_MEMBASE_IMM;
-	case OP_STOREI4_MEMBASE_REG:
-		return OP_STOREI4_MEMBASE_IMM;
-#if defined(TARGET_X86) || defined (TARGET_AMD64)
-	case OP_X86_PUSH:
-		return OP_X86_PUSH_IMM;
-	case OP_X86_COMPARE_MEMBASE_REG:
-		return OP_X86_COMPARE_MEMBASE_IMM;
-#endif
-#if defined(TARGET_AMD64)
-	case OP_AMD64_ICOMPARE_MEMBASE_REG:
-		return OP_AMD64_ICOMPARE_MEMBASE_IMM;
-#endif
-	case OP_VOIDCALL_REG:
-		return OP_VOIDCALL;
-	case OP_CALL_REG:
-		return OP_CALL;
-	case OP_LCALL_REG:
-		return OP_LCALL;
-	case OP_FCALL_REG:
-		return OP_FCALL;
-	case OP_LOCALLOC:
-		return OP_LOCALLOC_IMM;
-	}
-	return -1;
-}
-int
-mono_load_membase_to_load_mem (int opcode)
-{
-#if defined(TARGET_X86) || defined(TARGET_AMD64)
-	switch (opcode) {
-	case OP_LOAD_MEMBASE:
-		return OP_LOAD_MEM;
-	case OP_LOADU1_MEMBASE:
-		return OP_LOADU1_MEM;
-	case OP_LOADU2_MEMBASE:
-		return OP_LOADU2_MEM;
-	case OP_LOADI4_MEMBASE:
-		return OP_LOADI4_MEM;
-	case OP_LOADU4_MEMBASE:
-		return OP_LOADU4_MEM;
-#if SIZEOF_REGISTER == 8
-	case OP_LOADI8_MEMBASE:
-		return OP_LOADI8_MEM;
-#endif
-	}
-#endif
-	return -1;
-}
-static int
-op_to_op_dest_membase (int store_opcode, int opcode)
-{
-#if defined(TARGET_X86)
-	if (!((store_opcode == OP_STORE_MEMBASE_REG) || (store_opcode == OP_STOREI4_MEMBASE_REG)))
-		return -1;
-	switch (opcode) {
-	case OP_IADD:
-		return OP_X86_ADD_MEMBASE_REG;
-	case OP_ISUB:
-		return OP_X86_SUB_MEMBASE_REG;
-	case OP_IAND:
-		return OP_X86_AND_MEMBASE_REG;
-	case OP_IOR:
-		return OP_X86_OR_MEMBASE_REG;
-	case OP_IXOR:
-		return OP_X86_XOR_MEMBASE_REG;
-	case OP_ADD_IMM:
-	case OP_IADD_IMM:
-		return OP_X86_ADD_MEMBASE_IMM;
-	case OP_SUB_IMM:
-	case OP_ISUB_IMM:
-		return OP_X86_SUB_MEMBASE_IMM;
-	case OP_AND_IMM:
-	case OP_IAND_IMM:
-		return OP_X86_AND_MEMBASE_IMM;
-	case OP_OR_IMM:
-	case OP_IOR_IMM:
-		return OP_X86_OR_MEMBASE_IMM;
-	case OP_XOR_IMM:
-	case OP_IXOR_IMM:
-		return OP_X86_XOR_MEMBASE_IMM;
-	case OP_MOVE:
-		return OP_NOP;
-	}
-#endif
-#if defined(TARGET_AMD64)
-	if (!((store_opcode == OP_STORE_MEMBASE_REG) || (store_opcode == OP_STOREI4_MEMBASE_REG) || (store_opcode == OP_STOREI8_MEMBASE_REG)))
-		return -1;
-	switch (opcode) {
-	case OP_IADD:
-		return OP_X86_ADD_MEMBASE_REG;
-	case OP_ISUB:
-		return OP_X86_SUB_MEMBASE_REG;
-	case OP_IAND:
-		return OP_X86_AND_MEMBASE_REG;
-	case OP_IOR:
-		return OP_X86_OR_MEMBASE_REG;
-	case OP_IXOR:
-		return OP_X86_XOR_MEMBASE_REG;
-	case OP_IADD_IMM:
-		return OP_X86_ADD_MEMBASE_IMM;
-	case OP_ISUB_IMM:
-		return OP_X86_SUB_MEMBASE_IMM;
-	case OP_IAND_IMM:
-		return OP_X86_AND_MEMBASE_IMM;
-	case OP_IOR_IMM:
-		return OP_X86_OR_MEMBASE_IMM;
-	case OP_IXOR_IMM:
-		return OP_X86_XOR_MEMBASE_IMM;
-	case OP_LADD:
-		return OP_AMD64_ADD_MEMBASE_REG;
-	case OP_LSUB:
-		return OP_AMD64_SUB_MEMBASE_REG;
-	case OP_LAND:
-		return OP_AMD64_AND_MEMBASE_REG;
-	case OP_LOR:
-		return OP_AMD64_OR_MEMBASE_REG;
-	case OP_LXOR:
-		return OP_AMD64_XOR_MEMBASE_REG;
-	case OP_ADD_IMM:
-	case OP_LADD_IMM:
-		return OP_AMD64_ADD_MEMBASE_IMM;
-	case OP_SUB_IMM:
-	case OP_LSUB_IMM:
-		return OP_AMD64_SUB_MEMBASE_IMM;
-	case OP_AND_IMM:
-	case OP_LAND_IMM:
-		return OP_AMD64_AND_MEMBASE_IMM;
-	case OP_OR_IMM:
-	case OP_LOR_IMM:
-		return OP_AMD64_OR_MEMBASE_IMM;
-	case OP_XOR_IMM:
-	case OP_LXOR_IMM:
-		return OP_AMD64_XOR_MEMBASE_IMM;
-	case OP_MOVE:
-		return OP_NOP;
-	}
-#endif
-	return -1;
-}
-static int
-op_to_op_store_membase (int store_opcode, int opcode)
-{
-#if defined(TARGET_X86) || defined(TARGET_AMD64)
-	switch (opcode) {
-	case OP_ICEQ:
-		if (store_opcode == OP_STOREI1_MEMBASE_REG)
-			return OP_X86_SETEQ_MEMBASE;
-	case OP_CNE:
-		if (store_opcode == OP_STOREI1_MEMBASE_REG)
-			return OP_X86_SETNE_MEMBASE;
-	}
-#endif
-	return -1;
-}
-static int
-op_to_op_src1_membase (MonoCompile *cfg, int load_opcode, int opcode)
-{
-#ifdef TARGET_X86
-	/* FIXME: This has sign extension issues */
-	/*
-	if ((opcode == OP_ICOMPARE_IMM) && (load_opcode == OP_LOADU1_MEMBASE))
-		return OP_X86_COMPARE_MEMBASE8_IMM;
-	*/
-	if (!((load_opcode == OP_LOAD_MEMBASE) || (load_opcode == OP_LOADI4_MEMBASE) || (load_opcode == OP_LOADU4_MEMBASE)))
-		return -1;
-	switch (opcode) {
-	case OP_X86_PUSH:
-		return OP_X86_PUSH_MEMBASE;
-	case OP_COMPARE_IMM:
-	case OP_ICOMPARE_IMM:
-		return OP_X86_COMPARE_MEMBASE_IMM;
-	case OP_COMPARE:
-	case OP_ICOMPARE:
-		return OP_X86_COMPARE_MEMBASE_REG;
-	}
-#endif
-#ifdef TARGET_AMD64
-	/* FIXME: This has sign extension issues */
-	/*
-	if ((opcode == OP_ICOMPARE_IMM) && (load_opcode == OP_LOADU1_MEMBASE))
-		return OP_X86_COMPARE_MEMBASE8_IMM;
-	*/
-	switch (opcode) {
-	case OP_X86_PUSH:
-		if ((load_opcode == OP_LOAD_MEMBASE && !cfg->backend->ilp32) || (load_opcode == OP_LOADI8_MEMBASE))
-			return OP_X86_PUSH_MEMBASE;
-		break;
-		/* FIXME: This only works for 32 bit immediates
-	case OP_COMPARE_IMM:
-	case OP_LCOMPARE_IMM:
-		if ((load_opcode == OP_LOAD_MEMBASE) || (load_opcode == OP_LOADI8_MEMBASE))
-			return OP_AMD64_COMPARE_MEMBASE_IMM;
-		*/
-	case OP_ICOMPARE_IMM:
-		if ((load_opcode == OP_LOADI4_MEMBASE) || (load_opcode == OP_LOADU4_MEMBASE))
-			return OP_AMD64_ICOMPARE_MEMBASE_IMM;
-		break;
-	case OP_COMPARE:
-	case OP_LCOMPARE:
-		if (cfg->backend->ilp32 && load_opcode == OP_LOAD_MEMBASE)
-			return OP_AMD64_ICOMPARE_MEMBASE_REG;
-		if ((load_opcode == OP_LOAD_MEMBASE && !cfg->backend->ilp32) || (load_opcode == OP_LOADI8_MEMBASE))
-			return OP_AMD64_COMPARE_MEMBASE_REG;
-		break;
-	case OP_ICOMPARE:
-		if ((load_opcode == OP_LOADI4_MEMBASE) || (load_opcode == OP_LOADU4_MEMBASE))
-			return OP_AMD64_ICOMPARE_MEMBASE_REG;
-		break;
-	}
-#endif
-	return -1;
-}
-static int
-op_to_op_src2_membase (MonoCompile *cfg, int load_opcode, int opcode)
-{
-#ifdef TARGET_X86
-	if (!((load_opcode == OP_LOAD_MEMBASE) || (load_opcode == OP_LOADI4_MEMBASE) || (load_opcode == OP_LOADU4_MEMBASE)))
-		return -1;
-	switch (opcode) {
-	case OP_COMPARE:
-	case OP_ICOMPARE:
-		return OP_X86_COMPARE_REG_MEMBASE;
-	case OP_IADD:
-		return OP_X86_ADD_REG_MEMBASE;
-	case OP_ISUB:
-		return OP_X86_SUB_REG_MEMBASE;
-	case OP_IAND:
-		return OP_X86_AND_REG_MEMBASE;
-	case OP_IOR:
-		return OP_X86_OR_REG_MEMBASE;
-	case OP_IXOR:
-		return OP_X86_XOR_REG_MEMBASE;
-	}
-#endif
-#ifdef TARGET_AMD64
-	if ((load_opcode == OP_LOADI4_MEMBASE) || (load_opcode == OP_LOADU4_MEMBASE) || (load_opcode == OP_LOAD_MEMBASE && cfg->backend->ilp32)) {
-		switch (opcode) {
-		case OP_ICOMPARE:
-			return OP_AMD64_ICOMPARE_REG_MEMBASE;
-		case OP_IADD:
-			return OP_X86_ADD_REG_MEMBASE;
-		case OP_ISUB:
-			return OP_X86_SUB_REG_MEMBASE;
-		case OP_IAND:
-			return OP_X86_AND_REG_MEMBASE;
-		case OP_IOR:
-			return OP_X86_OR_REG_MEMBASE;
-		case OP_IXOR:
-			return OP_X86_XOR_REG_MEMBASE;
-		}
-	} else if ((load_opcode == OP_LOADI8_MEMBASE) || (load_opcode == OP_LOAD_MEMBASE && !cfg->backend->ilp32)) {
-		switch (opcode) {
-		case OP_COMPARE:
-		case OP_LCOMPARE:
-			return OP_AMD64_COMPARE_REG_MEMBASE;
-		case OP_LADD:
-			return OP_AMD64_ADD_REG_MEMBASE;
-		case OP_LSUB:
-			return OP_AMD64_SUB_REG_MEMBASE;
-		case OP_LAND:
-			return OP_AMD64_AND_REG_MEMBASE;
-		case OP_LOR:
-			return OP_AMD64_OR_REG_MEMBASE;
-		case OP_LXOR:
-			return OP_AMD64_XOR_REG_MEMBASE;
-		}
-	}
-#endif
-	return -1;
-}
-int
-mono_op_to_op_imm_noemul (int opcode)
-{
-MONO_DISABLE_WARNING(4065) // switch with default but no case
-	switch (opcode) {
-#if SIZEOF_REGISTER == 4 && !defined(MONO_ARCH_NO_EMULATE_LONG_SHIFT_OPS)
-	case OP_LSHR:
-	case OP_LSHL:
-	case OP_LSHR_UN:
-		return -1;
-#endif
-#if defined(MONO_ARCH_EMULATE_MUL_DIV) || defined(MONO_ARCH_EMULATE_DIV)
-	case OP_IDIV:
-	case OP_IDIV_UN:
-	case OP_IREM:
-	case OP_IREM_UN:
-		return -1;
-#endif
-#if defined(MONO_ARCH_EMULATE_MUL_DIV)
-	case OP_IMUL:
-		return -1;
-#endif
-	default:
-		return mono_op_to_op_imm (opcode);
-	}
-MONO_RESTORE_WARNING
-}
-gboolean
-mono_op_no_side_effects (int opcode)
-{
-	/* FIXME: Add more instructions */
-	/* INEG sets the condition codes, and the OP_LNEG decomposition depends on this on x86 */
-	switch (opcode) {
-	case OP_MOVE:
-	case OP_FMOVE:
-	case OP_VMOVE:
-	case OP_XMOVE:
-	case OP_RMOVE:
-	case OP_VZERO:
-	case OP_XZERO:
-	case OP_XONES:
-	case OP_XCONST:
-	case OP_ICONST:
-	case OP_I8CONST:
-	case OP_ADD_IMM:
-	case OP_R8CONST:
-	case OP_LADD_IMM:
-	case OP_ISUB_IMM:
-	case OP_IADD_IMM:
-	case OP_LNEG:
-	case OP_ISUB:
-	case OP_CMOV_IGE:
-	case OP_ISHL_IMM:
-	case OP_ISHR_IMM:
-	case OP_ISHR_UN_IMM:
-	case OP_IAND_IMM:
-	case OP_ICONV_TO_U1:
-	case OP_ICONV_TO_I1:
-	case OP_SEXT_I4:
-	case OP_LCONV_TO_U1:
-	case OP_ICONV_TO_U2:
-	case OP_ICONV_TO_I2:
-	case OP_LCONV_TO_I2:
-	case OP_LDADDR:
-	case OP_PHI:
-	case OP_NOP:
-	case OP_ZEXT_I4:
-	case OP_NOT_NULL:
-	case OP_IL_SEQ_POINT:
-	case OP_RTTYPE:
-		return TRUE;
-	default:
-		return FALSE;
-	}
-}
-gboolean
-mono_ins_no_side_effects (MonoInst *ins)
-{
-	if (mono_op_no_side_effects (ins->opcode))
-		return TRUE;
-	if (ins->opcode == OP_AOTCONST) {
-		MonoJumpInfoType type = (MonoJumpInfoType)(intptr_t)ins->inst_p1;
-		switch (type) {
-		case MONO_PATCH_INFO_TYPE_FROM_HANDLE:
-		case MONO_PATCH_INFO_LDSTR:
-		case MONO_PATCH_INFO_VTABLE:
-		case MONO_PATCH_INFO_METHOD_RGCTX:
-			return TRUE;
-		}
-	}
-	return FALSE;
-}
-/**
- * mono_handle_global_vregs:
- *
- *   Make vregs used in more than one bblock 'global', i.e. allocate a variable
- * for them.
- */
-void
-mono_handle_global_vregs (MonoCompile *cfg)
-{
-	gint32 *vreg_to_bb;
-	MonoBasicBlock *bb;
-	vreg_to_bb = (gint32 *)mono_mempool_alloc0 (cfg->mempool, sizeof (gint32*) * cfg->next_vreg + 1);
-	/* Find local vregs used in more than one bb */
-	for (bb = cfg->bb_entry; bb; bb = bb->next_bb) {
-		MonoInst *ins = bb->code;
-		int block_num = bb->block_num;
-		if (cfg->verbose_level > 2)
-			printf ("\nHANDLE-GLOBAL-VREGS BLOCK %d:\n", bb->block_num);
-		cfg->cbb = bb;
-		for (; ins; ins = ins->next) {
-			const char *spec = INS_INFO (ins->opcode);
-			int regtype = 0, regindex;
-			gint32 prev_bb;
-			if (G_UNLIKELY (cfg->verbose_level > 2))
-				mono_print_ins (ins);
-			g_assert (ins->opcode >= MONO_CEE_LAST);
-			for (regindex = 0; regindex < 4; regindex ++) {
-				int vreg = 0;
-				if (regindex == 0) {
-					regtype = spec [MONO_INST_DEST];
-					if (regtype == ' ')
-						continue;
-					vreg = ins->dreg;
-				} else if (regindex == 1) {
-					regtype = spec [MONO_INST_SRC1];
-					if (regtype == ' ')
-						continue;
-					vreg = ins->sreg1;
-				} else if (regindex == 2) {
-					regtype = spec [MONO_INST_SRC2];
-					if (regtype == ' ')
-						continue;
-					vreg = ins->sreg2;
-				} else if (regindex == 3) {
-					regtype = spec [MONO_INST_SRC3];
-					if (regtype == ' ')
-						continue;
-					vreg = ins->sreg3;
-				}
-#if SIZEOF_REGISTER == 4
-				/* In the LLVM case, the long opcodes are not decomposed */
-				if (regtype == 'l' && !COMPILE_LLVM (cfg)) {
-					/*
-					 * Since some instructions reference the original long vreg,
-					 * and some reference the two component vregs, it is quite hard
-					 * to determine when it needs to be global. So be conservative.
-					 */
-					if (!get_vreg_to_inst (cfg, vreg)) {
-						mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (mono_defaults.int64_class), OP_LOCAL, vreg);
-						if (cfg->verbose_level > 2)
-							printf ("LONG VREG R%d made global.\n", vreg);
-					}
-					/*
-					 * Make the component vregs volatile since the optimizations can
-					 * get confused otherwise.
-					 */
-					get_vreg_to_inst (cfg, MONO_LVREG_LS (vreg))->flags |= MONO_INST_VOLATILE;
-					get_vreg_to_inst (cfg, MONO_LVREG_MS (vreg))->flags |= MONO_INST_VOLATILE;
-				}
-#endif
-				g_assert (vreg != -1);
-				prev_bb = vreg_to_bb [vreg];
-				if (prev_bb == 0) {
-					/* 0 is a valid block num */
-					vreg_to_bb [vreg] = block_num + 1;
-				} else if ((prev_bb != block_num + 1) && (prev_bb != -1)) {
-					if (((regtype == 'i' && (vreg < MONO_MAX_IREGS))) || (regtype == 'f' && (vreg < MONO_MAX_FREGS)))
-						continue;
-					if (!get_vreg_to_inst (cfg, vreg)) {
-						if (G_UNLIKELY (cfg->verbose_level > 2))
-							printf ("VREG R%d used in BB%d and BB%d made global.\n", vreg, vreg_to_bb [vreg], block_num);
-						switch (regtype) {
-						case 'i':
-							if (vreg_is_ref (cfg, vreg))
-								mono_compile_create_var_for_vreg (cfg, mono_get_object_type (), OP_LOCAL, vreg);
-							else
-								mono_compile_create_var_for_vreg (cfg, mono_get_int_type (), OP_LOCAL, vreg);
-							break;
-						case 'l':
-							mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (mono_defaults.int64_class), OP_LOCAL, vreg);
-							break;
-						case 'f':
-							mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (mono_defaults.double_class), OP_LOCAL, vreg);
-							break;
-						case 'v':
-						case 'x':
-							mono_compile_create_var_for_vreg (cfg, m_class_get_byval_arg (ins->klass), OP_LOCAL, vreg);
-							break;
-						default:
-							g_assert_not_reached ();
-						}
-					}
-					/* Flag as having been used in more than one bb */
-					vreg_to_bb [vreg] = -1;
-				}
-			}
-		}
-	}
-	/* If a variable is used in only one bblock, convert it into a local vreg */
-	for (guint i = 0; i < cfg->num_varinfo; i++) {
-		MonoInst *var = cfg->varinfo [i];
-		MonoMethodVar *vmv = MONO_VARINFO (cfg, i);
-		switch (var->type) {
-		case STACK_I4:
-		case STACK_OBJ:
-		case STACK_PTR:
-		case STACK_MP:
-		case STACK_VTYPE:
-#if SIZEOF_REGISTER == 8
-		case STACK_I8:
-#endif
-#if !defined(TARGET_X86)
-		/* Enabling this screws up the fp stack on x86 */
-		case STACK_R8:
-#endif
-			if (mono_arch_is_soft_float ())
-				break;
-			/*
-			if (var->type == STACK_VTYPE && cfg->gsharedvt && mini_is_gsharedvt_variable_type (var->inst_vtype))
-				break;
-			*/
-			/* Arguments are implicitly global */
-			/* Putting R4 vars into registers doesn't work currently */
-			/* The gsharedvt vars are implicitly referenced by ldaddr opcodes, but those opcodes are only generated later */
-			if ((var->opcode != OP_ARG) && (var != cfg->ret) && !(var->flags & (MONO_INST_VOLATILE|MONO_INST_INDIRECT)) && (vreg_to_bb [var->dreg] != -1) && (m_class_get_byval_arg (var->klass)->type != MONO_TYPE_R4) && !cfg->disable_vreg_to_lvreg && var != cfg->gsharedvt_info_var && var != cfg->gsharedvt_locals_var && var != cfg->lmf_addr_var) {
-				/*
-				 * Make that the variable's liveness interval doesn't contain a call, since
-				 * that would cause the lvreg to be spilled, making the whole optimization
-				 * useless.
-				 */
-				/* This is too slow for JIT compilation */
-#if 0
-				if (cfg->compile_aot && vreg_to_bb [var->dreg]) {
-					MonoInst *ins;
-					int def_index, call_index, ins_index;
-					gboolean spilled = FALSE;
-					def_index = -1;
-					call_index = -1;
-					ins_index = 0;
-					for (ins = vreg_to_bb [var->dreg]->code; ins; ins = ins->next) {
-						const char *spec = INS_INFO (ins->opcode);
-						if ((spec [MONO_INST_DEST] != ' ') && (ins->dreg == var->dreg))
-							def_index = ins_index;
-						if (((spec [MONO_INST_SRC1] != ' ') && (ins->sreg1 == var->dreg)) ||
-							((spec [MONO_INST_SRC1] != ' ') && (ins->sreg1 == var->dreg))) {
-							if (call_index > def_index) {
-								spilled = TRUE;
-								break;
-							}
-						}
-						if (MONO_IS_CALL (ins))
-							call_index = ins_index;
-						ins_index ++;
-					}
-					if (spilled)
-						break;
-				}
-#endif
-				if (G_UNLIKELY (cfg->verbose_level > 2))
-					printf ("CONVERTED R%d(%d) TO VREG.\n", var->dreg, vmv->idx);
-				var->flags |= MONO_INST_IS_DEAD;
-				cfg->vreg_to_inst [var->dreg] = NULL;
-			}
-			break;
-		}
-	}
-	/*
-	 * Compress the varinfo and vars tables so the liveness computation is faster and
-	 * takes up less space.
-	 */
-	guint pos = 0;
-	for (guint i = 0; i < cfg->num_varinfo; ++i) {
-		MonoInst *var = cfg->varinfo [i];
-		if (pos < i && cfg->locals_start == i)
-			cfg->locals_start = pos;
-		if (!(var->flags & MONO_INST_IS_DEAD)) {
-			if (pos < i) {
-				cfg->varinfo [pos] = cfg->varinfo [i];
-				cfg->varinfo [pos]->inst_c0 = pos;
-				memcpy (&cfg->vars [pos], &cfg->vars [i], sizeof (MonoMethodVar));
-				cfg->vars [pos].idx = pos;
-#if SIZEOF_REGISTER == 4
-				if (cfg->varinfo [pos]->type == STACK_I8) {
-					/* Modify the two component vars too */
-					MonoInst *var1;
-					var1 = get_vreg_to_inst (cfg, MONO_LVREG_LS (cfg->varinfo [pos]->dreg));
-					var1->inst_c0 = pos;
-					var1 = get_vreg_to_inst (cfg, MONO_LVREG_MS (cfg->varinfo [pos]->dreg));
-					var1->inst_c0 = pos;
-				}
-#endif
-			}
-			pos ++;
-		}
-	}
-	cfg->num_varinfo = pos;
-	if (cfg->locals_start > cfg->num_varinfo)
-		cfg->locals_start = cfg->num_varinfo;
-}
-/*
- * mono_allocate_gsharedvt_vars:
- *
- *   Allocate variables with gsharedvt types to entries in the MonoGSharedVtMethodRuntimeInfo.entries array.
- * Initialize cfg->gsharedvt_vreg_to_idx with the mapping between vregs and indexes.
- */
-void
-mono_allocate_gsharedvt_vars (MonoCompile *cfg)
-{
-	cfg->gsharedvt_vreg_to_idx = (int *)mono_mempool_alloc0 (cfg->mempool, sizeof (int) * cfg->next_vreg);
-	for (guint i = 0; i < cfg->num_varinfo; ++i) {
-		MonoInst *ins = cfg->varinfo [i];
-		int idx;
-		if (mini_is_gsharedvt_variable_type (ins->inst_vtype)) {
-			if (i >= cfg->locals_start) {
-				/* Local */
-				idx = get_gsharedvt_info_slot (cfg, ins->inst_vtype, MONO_RGCTX_INFO_LOCAL_OFFSET);
-				cfg->gsharedvt_vreg_to_idx [ins->dreg] = idx + 1;
-				ins->opcode = OP_GSHAREDVT_LOCAL;
-				ins->inst_imm = idx;
-			} else {
-				/* Arg */
-				cfg->gsharedvt_vreg_to_idx [ins->dreg] = -1;
-				ins->opcode = OP_GSHAREDVT_ARG_REGOFFSET;
-			}
-		}
-	}
-}
-/**
- * mono_spill_global_vars:
- *
- *   Generate spill code for variables which are not allocated to registers,
- * and replace vregs with their allocated hregs. *need_local_opts is set to TRUE if
- * code is generated which could be optimized by the local optimization passes.
- */
-void
-mono_spill_global_vars (MonoCompile *cfg, gboolean *need_local_opts)
-{
-	MonoBasicBlock *bb;
-	char spec2 [16];
-	int orig_next_vreg;
-	guint32 *vreg_to_lvreg;
-	guint32 *lvregs;
-	guint32 i, lvregs_len, lvregs_size;
-	gboolean dest_has_lvreg = FALSE;
-	MonoStackType stacktypes [128];
-	MonoInst **live_range_start, **live_range_end;
-	MonoBasicBlock **live_range_start_bb, **live_range_end_bb;
-	*need_local_opts = FALSE;
-	memset (spec2, 0, sizeof (spec2));
-	/* FIXME: Move this function to mini.c */
-	stacktypes [(int)'i'] = STACK_PTR;
-	stacktypes [(int)'l'] = STACK_I8;
-	stacktypes [(int)'f'] = STACK_R8;
-#ifdef MONO_ARCH_SIMD_INTRINSICS
-	stacktypes [(int)'x'] = STACK_VTYPE;
-#endif
-#if SIZEOF_REGISTER == 4
-	/* Create MonoInsts for longs */
-	for (i = 0; i < cfg->num_varinfo; i++) {
-		MonoInst *ins = cfg->varinfo [i];
-		if ((ins->opcode != OP_REGVAR) && !(ins->flags & MONO_INST_IS_DEAD)) {
-			switch (ins->type) {
-			case STACK_R8:
-			case STACK_I8: {
-				MonoInst *tree;
-				if (ins->type == STACK_R8 && !COMPILE_SOFT_FLOAT (cfg))
-					break;
-				g_assert (ins->opcode == OP_REGOFFSET);
-				tree = get_vreg_to_inst (cfg, MONO_LVREG_LS (ins->dreg));
-				g_assert (tree);
-				tree->opcode = OP_REGOFFSET;
-				tree->inst_basereg = ins->inst_basereg;
-				tree->inst_offset = ins->inst_offset + MINI_LS_WORD_OFFSET;
-				tree = get_vreg_to_inst (cfg, MONO_LVREG_MS (ins->dreg));
-				g_assert (tree);
-				tree->opcode = OP_REGOFFSET;
-				tree->inst_basereg = ins->inst_basereg;
-				tree->inst_offset = ins->inst_offset + MINI_MS_WORD_OFFSET;
-				break;
-			}
-			default:
-				break;
-			}
-		}
-	}
-#endif
-	if (cfg->compute_gc_maps) {
-		/* registers need liveness info even for !non refs */
-		for (i = 0; i < cfg->num_varinfo; i++) {
-			MonoInst *ins = cfg->varinfo [i];
-			if (ins->opcode == OP_REGVAR)
-				ins->flags |= MONO_INST_GC_TRACK;
-		}
-	}
-	/* FIXME: widening and truncation */
-	/*
-	 * As an optimization, when a variable allocated to the stack is first loaded into
-	 * an lvreg, we will remember the lvreg and use it the next time instead of loading
-	 * the variable again.
-	 */
-	orig_next_vreg = cfg->next_vreg;
-	vreg_to_lvreg = (guint32 *)mono_mempool_alloc0 (cfg->mempool, sizeof (guint32) * cfg->next_vreg);
-	lvregs_size = 1024;
-	lvregs = (guint32 *)mono_mempool_alloc (cfg->mempool, sizeof (guint32) * lvregs_size);
-	lvregs_len = 0;
-	/*
-	 * These arrays contain the first and last instructions accessing a given
-	 * variable.
-	 * Since we emit bblocks in the same order we process them here, and we
-	 * don't split live ranges, these will precisely describe the live range of
-	 * the variable, i.e. the instruction range where a valid value can be found
-	 * in the variables location.
-	 * The live range is computed using the liveness info computed by the liveness pass.
-	 * We can't use vmv->range, since that is an abstract live range, and we need
-	 * one which is instruction precise.
-	 * FIXME: Variables used in out-of-line bblocks have a hole in their live range.
-	 */
-	/* FIXME: Only do this if debugging info is requested */
-	live_range_start = g_new0 (MonoInst*, cfg->next_vreg);
-	live_range_end = g_new0 (MonoInst*, cfg->next_vreg);
-	live_range_start_bb = g_new (MonoBasicBlock*, cfg->next_vreg);
-	live_range_end_bb = g_new (MonoBasicBlock*, cfg->next_vreg);
-	/* Add spill loads/stores */
-	for (bb = cfg->bb_entry; bb; bb = bb->next_bb) {
-		MonoInst *ins;
-		if (cfg->verbose_level > 2)
-			printf ("\nSPILL BLOCK %d:\n", bb->block_num);
-		/* Clear vreg_to_lvreg array */
-		for (i = 0; i < lvregs_len; i++)
-			vreg_to_lvreg [lvregs [i]] = 0;
-		lvregs_len = 0;
-		cfg->cbb = bb;
-		MONO_BB_FOR_EACH_INS (bb, ins) {
-			const char *spec = INS_INFO (ins->opcode);
-			int regtype, srcindex, sreg, tmp_reg, prev_dreg, num_sregs;
-			gboolean store, no_lvreg;
-			int sregs [MONO_MAX_SRC_REGS];
-			if (G_UNLIKELY (cfg->verbose_level > 2))
-				mono_print_ins (ins);
-			if (ins->opcode == OP_NOP)
-				continue;
-			/*
-			 * We handle LDADDR here as well, since it can only be decomposed
-			 * when variable addresses are known.
-			 */
-			if (ins->opcode == OP_LDADDR) {
-				MonoInst *var = (MonoInst *)ins->inst_p0;
-				if (var->opcode == OP_VTARG_ADDR) {
-					/* Happens on SPARC/S390 where vtypes are passed by reference */
-					MonoInst *vtaddr = var->inst_left;
-					if (vtaddr->opcode == OP_REGVAR) {
-						ins->opcode = OP_MOVE;
-						ins->sreg1 = vtaddr->dreg;
-					}
-					else if (var->inst_left->opcode == OP_REGOFFSET) {
-						ins->opcode = OP_LOAD_MEMBASE;
-						ins->inst_basereg = vtaddr->inst_basereg;
-						ins->inst_offset = vtaddr->inst_offset;
-					} else
-						NOT_IMPLEMENTED;
-				} else if (cfg->gsharedvt && cfg->gsharedvt_vreg_to_idx [var->dreg] < 0) {
-					/* gsharedvt arg passed by ref */
-					g_assert (var->opcode == OP_GSHAREDVT_ARG_REGOFFSET);
-					ins->opcode = OP_LOAD_MEMBASE;
-					ins->inst_basereg = var->inst_basereg;
-					ins->inst_offset = var->inst_offset;
-				} else if (cfg->gsharedvt && cfg->gsharedvt_vreg_to_idx [var->dreg]) {
-					MonoInst *load, *load2, *load3;
-					int idx = cfg->gsharedvt_vreg_to_idx [var->dreg] - 1;
-					int reg1, reg2, reg3;
-					MonoInst *info_var = cfg->gsharedvt_info_var;
-					MonoInst *locals_var = cfg->gsharedvt_locals_var;
-					/*
-					 * gsharedvt local.
-					 * Compute the address of the local as gsharedvt_locals_var + gsharedvt_info_var->locals_offsets [idx].
-					 */
-					g_assert (var->opcode == OP_GSHAREDVT_LOCAL);
-					g_assert (info_var);
-					g_assert (locals_var);
-					/* Mark the instruction used to compute the locals var as used */
-					cfg->gsharedvt_locals_var_ins = NULL;
-					/* Load the offset */
-					if (info_var->opcode == OP_REGOFFSET) {
-						reg1 = alloc_ireg (cfg);
-						NEW_LOAD_MEMBASE (cfg, load, OP_LOAD_MEMBASE, reg1, info_var->inst_basereg, info_var->inst_offset);
-					} else if (info_var->opcode == OP_REGVAR) {
-						load = NULL;
-						reg1 = info_var->dreg;
-					} else {
-						g_assert_not_reached ();
-					}
-					reg2 = alloc_ireg (cfg);
-					NEW_LOAD_MEMBASE (cfg, load2, OP_LOADI4_MEMBASE, reg2, reg1, MONO_STRUCT_OFFSET (MonoGSharedVtMethodRuntimeInfo, entries) + (idx * TARGET_SIZEOF_VOID_P));
-					/* Load the locals area address */
-					reg3 = alloc_ireg (cfg);
-					if (locals_var->opcode == OP_REGOFFSET) {
-						NEW_LOAD_MEMBASE (cfg, load3, OP_LOAD_MEMBASE, reg3, locals_var->inst_basereg, locals_var->inst_offset);
-					} else if (locals_var->opcode == OP_REGVAR) {
-						NEW_UNALU (cfg, load3, OP_MOVE, reg3, locals_var->dreg);
-					} else {
-						g_assert_not_reached ();
-					}
-					/* Compute the address */
-					ins->opcode = OP_PADD;
-					ins->sreg1 = reg3;
-					ins->sreg2 = reg2;
-					mono_bblock_insert_before_ins (bb, ins, load3);
-					mono_bblock_insert_before_ins (bb, load3, load2);
-					if (load)
-						mono_bblock_insert_before_ins (bb, load2, load);
-				} else {
-					g_assert (var->opcode == OP_REGOFFSET);
-					ins->opcode = OP_ADD_IMM;
-					ins->sreg1 = var->inst_basereg;
-					ins->inst_imm = var->inst_offset;
-				}
-				*need_local_opts = TRUE;
-				spec = INS_INFO (ins->opcode);
-			}
-			if (ins->opcode < MONO_CEE_LAST) {
-				mono_print_ins (ins);
-				g_assert_not_reached ();
-			}
-			/*
-			 * Store opcodes have destbasereg in the dreg, but in reality, it is an
-			 * src register.
-			 * FIXME:
-			 */
-			if (MONO_IS_STORE_MEMBASE (ins)) {
-				tmp_reg = ins->dreg;
-				ins->dreg = ins->sreg2;
-				ins->sreg2 = tmp_reg;
-				store = TRUE;
-				spec2 [MONO_INST_DEST] = ' ';
-				spec2 [MONO_INST_SRC1] = spec [MONO_INST_SRC1];
-				spec2 [MONO_INST_SRC2] = spec [MONO_INST_DEST];
-				spec2 [MONO_INST_SRC3] = ' ';
-				spec = spec2;
-			} else if (MONO_IS_STORE_MEMINDEX (ins))
-				g_assert_not_reached ();
-			else
-				store = FALSE;
-			no_lvreg = FALSE;
-			if (G_UNLIKELY (cfg->verbose_level > 2)) {
-				printf ("\t %.3s %d", spec, ins->dreg);
-				num_sregs = mono_inst_get_src_registers (ins, sregs);
-				for (srcindex = 0; srcindex < num_sregs; ++srcindex)
-					printf (" %d", sregs [srcindex]);
-				printf ("\n");
-			}
-			/***************/
-			/*    DREG     */
-			/***************/
-			regtype = spec [MONO_INST_DEST];
-			g_assert (((ins->dreg == -1) && (regtype == ' ')) || ((ins->dreg != -1) && (regtype != ' ')));
-			prev_dreg = -1;
-			int dreg_using_dest_to_membase_op = -1;
-			if ((ins->dreg != -1) && get_vreg_to_inst (cfg, ins->dreg)) {
-				MonoInst *var = get_vreg_to_inst (cfg, ins->dreg);
-				MonoInst *store_ins;
-				int store_opcode;
-				MonoInst *def_ins = ins;
-				int dreg = ins->dreg; /* The original vreg */
-				store_opcode = mono_type_to_store_membase (cfg, var->inst_vtype);
-				if (var->opcode == OP_REGVAR) {
-					ins->dreg = var->dreg;
-				} else if ((ins->dreg == ins->sreg1) && (spec [MONO_INST_DEST] == 'i') && (spec [MONO_INST_SRC1] == 'i') && !vreg_to_lvreg [ins->dreg] && (op_to_op_dest_membase (store_opcode, ins->opcode) != -1)) {
-					/*
-					 * Instead of emitting a load+store, use a _membase opcode.
-					 */
-					g_assert (var->opcode == OP_REGOFFSET);
-					if (ins->opcode == OP_MOVE) {
-						NULLIFY_INS (ins);
-						def_ins = NULL;
-					} else {
-						dreg_using_dest_to_membase_op = ins->dreg;
-						ins->opcode = GINT_TO_OPCODE (op_to_op_dest_membase (store_opcode, ins->opcode));
-						ins->inst_basereg = var->inst_basereg;
-						ins->inst_offset = var->inst_offset;
-						ins->dreg = -1;
-					}
-					spec = INS_INFO (ins->opcode);
-				} else {
-					guint32 lvreg;
-					g_assert (var->opcode == OP_REGOFFSET);
-					prev_dreg = ins->dreg;
-					/* Invalidate any previous lvreg for this vreg */
-					vreg_to_lvreg [ins->dreg] = 0;
-					lvreg = 0;
-					if (COMPILE_SOFT_FLOAT (cfg) && store_opcode == OP_STORER8_MEMBASE_REG) {
-						regtype = 'l';
-						store_opcode = OP_STOREI8_MEMBASE_REG;
-					}
-					ins->dreg = alloc_dreg (cfg, stacktypes [regtype]);
-#if SIZEOF_REGISTER != 8
-					if (regtype == 'l') {
-						NEW_STORE_MEMBASE (cfg, store_ins, OP_STOREI4_MEMBASE_REG, var->inst_basereg, var->inst_offset + MINI_LS_WORD_OFFSET, MONO_LVREG_LS (ins->dreg));
-						mono_bblock_insert_after_ins (bb, ins, store_ins);
-						NEW_STORE_MEMBASE (cfg, store_ins, OP_STOREI4_MEMBASE_REG, var->inst_basereg, var->inst_offset + MINI_MS_WORD_OFFSET, MONO_LVREG_MS (ins->dreg));
-						mono_bblock_insert_after_ins (bb, ins, store_ins);
-						def_ins = store_ins;
-					}
-					else
-#endif
-					{
-						g_assert (store_opcode != OP_STOREV_MEMBASE);
-						/* Try to fuse the store into the instruction itself */
-						/* FIXME: Add more instructions */
-						if (!lvreg && ((ins->opcode == OP_ICONST) || ((ins->opcode == OP_I8CONST) && (ins->inst_c0 == 0)))) {
-							ins->opcode = GINT_TO_OPCODE (store_membase_reg_to_store_membase_imm (store_opcode));
-							ins->inst_imm = ins->inst_c0;
-							ins->inst_destbasereg = var->inst_basereg;
-							ins->inst_offset = var->inst_offset;
-							spec = INS_INFO (ins->opcode);
-						} else if (!lvreg && ((ins->opcode == OP_MOVE) || (ins->opcode == OP_FMOVE) || (ins->opcode == OP_LMOVE) || (ins->opcode == OP_RMOVE))) {
-							ins->opcode = GINT_TO_OPCODE (store_opcode);
-							ins->inst_destbasereg = var->inst_basereg;
-							ins->inst_offset = var->inst_offset;
-							no_lvreg = TRUE;
-							tmp_reg = ins->dreg;
-							ins->dreg = ins->sreg2;
-							ins->sreg2 = tmp_reg;
-							store = TRUE;
-							spec2 [MONO_INST_DEST] = ' ';
-							spec2 [MONO_INST_SRC1] = spec [MONO_INST_SRC1];
-							spec2 [MONO_INST_SRC2] = spec [MONO_INST_DEST];
-							spec2 [MONO_INST_SRC3] = ' ';
-							spec = spec2;
-						} else if (!lvreg && (op_to_op_store_membase (store_opcode, ins->opcode) != -1)) {
-							ins->opcode = GINT_TO_OPCODE (op_to_op_store_membase (store_opcode, ins->opcode));
-							ins->dreg = -1;
-							ins->inst_basereg = var->inst_basereg;
-							ins->inst_offset = var->inst_offset;
-							spec = INS_INFO (ins->opcode);
-						} else {
-							/* printf ("INS: "); mono_print_ins (ins); */
-							/* Create a store instruction */
-							NEW_STORE_MEMBASE (cfg, store_ins, store_opcode, var->inst_basereg, var->inst_offset, ins->dreg);
-							if (store_ins->opcode == OP_STOREX_MEMBASE)
-								mini_type_to_eval_stack_type (cfg, var->inst_vtype, store_ins);
-							/* Insert it after the instruction */
-							mono_bblock_insert_after_ins (bb, ins, store_ins);
-							def_ins = store_ins;
-							/*
-							 * We can't assign ins->dreg to var->dreg here, since the
-							 * sregs could use it. So set a flag, and do it after
-							 * the sregs.
-							 */
-							if (!((var)->flags & (MONO_INST_VOLATILE|MONO_INST_INDIRECT)))
-								dest_has_lvreg = TRUE;
-						}
-					}
-				}
-				if (def_ins && !live_range_start [dreg]) {
-					live_range_start [dreg] = def_ins;
-					live_range_start_bb [dreg] = bb;
-				}
-				if (cfg->compute_gc_maps && def_ins && (var->flags & MONO_INST_GC_TRACK)) {
-					MonoInst *tmp;
-					MONO_INST_NEW (cfg, tmp, OP_GC_LIVENESS_DEF);
-					tmp->inst_c1 = dreg;
-					mono_bblock_insert_after_ins (bb, def_ins, tmp);
-				}
-			}
-			/************/
-			/*  SREGS   */
-			/************/
-			num_sregs = mono_inst_get_src_registers (ins, sregs);
-			for (srcindex = 0; srcindex < 3; ++srcindex) {
-				regtype = spec [MONO_INST_SRC1 + srcindex];
-				sreg = sregs [srcindex];
-				g_assert (((sreg == -1) && (regtype == ' ')) || ((sreg != -1) && (regtype != ' ')));
-				if ((sreg != -1) && get_vreg_to_inst (cfg, sreg)) {
-					MonoInst *var = get_vreg_to_inst (cfg, sreg);
-					MonoInst *use_ins = ins;
-					MonoInst *load_ins;
-					guint32 load_opcode;
-					if (var->opcode == OP_REGVAR) {
-						sregs [srcindex] = var->dreg;
-						live_range_end [sreg] = use_ins;
-						live_range_end_bb [sreg] = bb;
-						if (cfg->compute_gc_maps && var->dreg < orig_next_vreg && (var->flags & MONO_INST_GC_TRACK)) {
-							MonoInst *tmp;
-							MONO_INST_NEW (cfg, tmp, OP_GC_LIVENESS_USE);
-							/* var->dreg is a hreg */
-							tmp->inst_c1 = sreg;
-							mono_bblock_insert_after_ins (bb, ins, tmp);
-						}
-						continue;
-					}
-					g_assert (var->opcode == OP_REGOFFSET);
-					load_opcode = mono_type_to_load_membase (cfg, var->inst_vtype);
-					g_assert (load_opcode != OP_LOADV_MEMBASE);
-					if (vreg_to_lvreg [sreg]) {
-						g_assert (vreg_to_lvreg [sreg] != -1);
-						/* The variable is already loaded to an lvreg */
-						if (G_UNLIKELY (cfg->verbose_level > 2))
-							printf ("\t\tUse lvreg R%d for R%d.\n", vreg_to_lvreg [sreg], sreg);
-						sregs [srcindex] = vreg_to_lvreg [sreg];
-						continue;
-					}
-					/* Try to fuse the load into the instruction */
-					if ((srcindex == 0) && (op_to_op_src1_membase (cfg, load_opcode, ins->opcode) != -1)) {
-						ins->opcode = GINT_TO_OPCODE (op_to_op_src1_membase (cfg, load_opcode, ins->opcode));
-						sregs [0] = var->inst_basereg;
-						ins->inst_offset = var->inst_offset;
-					} else if ((srcindex == 1) && (op_to_op_src2_membase (cfg, load_opcode, ins->opcode) != -1)) {
-						ins->opcode = GINT_TO_OPCODE (op_to_op_src2_membase (cfg, load_opcode, ins->opcode));
-						sregs [1] = var->inst_basereg;
-						ins->inst_offset = var->inst_offset;
-					} else {
-						if (MONO_IS_REAL_MOVE (ins)) {
-							ins->opcode = OP_NOP;
-							sreg = ins->dreg;
-						} else {
-							sreg = alloc_dreg (cfg, stacktypes [regtype]);
-							if (!((var)->flags & (MONO_INST_VOLATILE|MONO_INST_INDIRECT)) && !no_lvreg) {
-								if (var->dreg == prev_dreg) {
-									/*
-									 * sreg refers to the value loaded by the load
-									 * emitted below, but we need to use ins->dreg
-									 * since it refers to the store emitted earlier.
-									 */
-									sreg = ins->dreg;
-								}
-								g_assert (sreg != -1);
-								if (var->dreg == dreg_using_dest_to_membase_op) {
-									if (cfg->verbose_level > 2)
-										printf ("\tCan't cache R%d because it's part of a dreg dest_membase optimization\n", var->dreg);
-								} else {
-									vreg_to_lvreg [var->dreg] = sreg;
-								}
-								if (lvregs_len >= lvregs_size) {
-									guint32 *new_lvregs = mono_mempool_alloc0 (cfg->mempool, sizeof (guint32) * lvregs_size * 2);
-									memcpy (new_lvregs, lvregs, sizeof (guint32) * lvregs_size);
-									lvregs = new_lvregs;
-									lvregs_size *= 2;
-								}
-								lvregs [lvregs_len ++] = var->dreg;
-							}
-						}
-						sregs [srcindex] = sreg;
-#if SIZEOF_REGISTER != 8
-						if (regtype == 'l') {
-							NEW_LOAD_MEMBASE (cfg, load_ins, OP_LOADI4_MEMBASE, MONO_LVREG_MS (sreg), var->inst_basereg, var->inst_offset + MINI_MS_WORD_OFFSET);
-							mono_bblock_insert_before_ins (bb, ins, load_ins);
-							NEW_LOAD_MEMBASE (cfg, load_ins, OP_LOADI4_MEMBASE, MONO_LVREG_LS (sreg), var->inst_basereg, var->inst_offset + MINI_LS_WORD_OFFSET);
-							mono_bblock_insert_before_ins (bb, ins, load_ins);
-							use_ins = load_ins;
-						}
-						else
-#endif
-						{
-#if SIZEOF_REGISTER == 4
-							g_assert (load_opcode != OP_LOADI8_MEMBASE);
-#endif
-							NEW_LOAD_MEMBASE (cfg, load_ins, load_opcode, sreg, var->inst_basereg, var->inst_offset);
-							if (load_ins->opcode == OP_LOADX_MEMBASE)
-								mini_type_to_eval_stack_type (cfg, var->inst_vtype, load_ins);
-							mono_bblock_insert_before_ins (bb, ins, load_ins);
-							use_ins = load_ins;
-						}
-						if (cfg->verbose_level > 2)
-							mono_print_ins_index (0, use_ins);
-					}
-					if (var->dreg < orig_next_vreg) {
-						live_range_end [var->dreg] = use_ins;
-						live_range_end_bb [var->dreg] = bb;
-					}
-					if (cfg->compute_gc_maps && var->dreg < orig_next_vreg && (var->flags & MONO_INST_GC_TRACK)) {
-						MonoInst *tmp;
-						MONO_INST_NEW (cfg, tmp, OP_GC_LIVENESS_USE);
-						tmp->inst_c1 = var->dreg;
-						mono_bblock_insert_after_ins (bb, ins, tmp);
-					}
-				}
-			}
-			mono_inst_set_src_registers (ins, sregs);
-			if (dest_has_lvreg) {
-				g_assert (ins->dreg != -1);
-				vreg_to_lvreg [prev_dreg] = ins->dreg;
-				if (lvregs_len >= lvregs_size) {
-					guint32 *new_lvregs = mono_mempool_alloc0 (cfg->mempool, sizeof (guint32) * lvregs_size * 2);
-					memcpy (new_lvregs, lvregs, sizeof (guint32) * lvregs_size);
-					lvregs = new_lvregs;
-					lvregs_size *= 2;
-				}
-				lvregs [lvregs_len ++] = prev_dreg;
-				dest_has_lvreg = FALSE;
-			}
-			if (store) {
-				tmp_reg = ins->dreg;
-				ins->dreg = ins->sreg2;
-				ins->sreg2 = tmp_reg;
-			}
-			if (MONO_IS_CALL (ins)) {
-				/* Clear vreg_to_lvreg array */
-				for (i = 0; i < lvregs_len; i++)
-					vreg_to_lvreg [lvregs [i]] = 0;
-				lvregs_len = 0;
-			} else if (ins->opcode == OP_NOP) {
-				ins->dreg = -1;
-				MONO_INST_NULLIFY_SREGS (ins);
-			}
-			if (cfg->verbose_level > 2)
-				mono_print_ins_index (1, ins);
-		}
-		/* Extend the live range based on the liveness info */
-		if (cfg->compute_precise_live_ranges && bb->live_out_set && bb->code) {
-			for (i = 0; i < cfg->num_varinfo; i ++) {
-				MonoMethodVar *vi = MONO_VARINFO (cfg, i);
-				if (vreg_is_volatile (cfg, vi->vreg))
-					/* The liveness info is incomplete */
-					continue;
-				if (mono_bitset_test_fast (bb->live_in_set, i) && !live_range_start [vi->vreg]) {
-					/* Live from at least the first ins of this bb */
-					live_range_start [vi->vreg] = bb->code;
-					live_range_start_bb [vi->vreg] = bb;
-				}
-				if (mono_bitset_test_fast (bb->live_out_set, i)) {
-					/* Live at least until the last ins of this bb */
-					live_range_end [vi->vreg] = bb->last_ins;
-					live_range_end_bb [vi->vreg] = bb;
-				}
-			}
-		}
-	}
-	/*
-	 * Emit LIVERANGE_START/LIVERANGE_END opcodes, the backend will implement them
-	 * by storing the current native offset into MonoMethodVar->live_range_start/end.
-	 */
-	if (cfg->compute_precise_live_ranges && cfg->comp_done & MONO_COMP_LIVENESS) {
-		for (i = 0; i < cfg->num_varinfo; ++i) {
-			int vreg = MONO_VARINFO (cfg, i)->vreg;
-			MonoInst *ins;
-			if (live_range_start [vreg]) {
-				MONO_INST_NEW (cfg, ins, OP_LIVERANGE_START);
-				ins->inst_c0 = i;
-				ins->inst_c1 = vreg;
-				mono_bblock_insert_after_ins (live_range_start_bb [vreg], live_range_start [vreg], ins);
-			}
-			if (live_range_end [vreg]) {
-				MONO_INST_NEW (cfg, ins, OP_LIVERANGE_END);
-				ins->inst_c0 = i;
-				ins->inst_c1 = vreg;
-				if (live_range_end [vreg] == live_range_end_bb [vreg]->last_ins)
-					mono_add_ins_to_end (live_range_end_bb [vreg], ins);
-				else
-					mono_bblock_insert_after_ins (live_range_end_bb [vreg], live_range_end [vreg], ins);
-			}
-		}
-	}
-	if (cfg->gsharedvt_locals_var_ins) {
-		/* Nullify if unused */
-		cfg->gsharedvt_locals_var_ins->opcode = OP_PCONST;
-		cfg->gsharedvt_locals_var_ins->inst_imm = 0;
-	}
-	g_free (live_range_start);
-	g_free (live_range_end);
-	g_free (live_range_start_bb);
-	g_free (live_range_end_bb);
-}
-/**
- * FIXME:
- * - use 'iadd' instead of 'int_add'
- * - handling ovf opcodes: decompose in method_to_ir.
- * - unify iregs/fregs
- *   -> partly done, the missing parts are:
- *   - a more complete unification would involve unifying the hregs as well, so
- *     code wouldn't need if (fp) all over the place. but that would mean the hregs
- *     would no longer map to the machine hregs, so the code generators would need to
- *     be modified. Also, on ia64 for example, niregs + nfregs > 256 -> bitmasks
- *     wouldn't work any more. Duplicating the code in mono_local_regalloc () into
- *     fp/non-fp branches speeds it up by about 15%.
- * - use sext/zext opcodes instead of shifts
- * - add OP_ICALL
- * - get rid of TEMPLOADs if possible and use vregs instead
- * - clean up usage of OP_P/OP_ opcodes
- * - cleanup usage of DUMMY_USE
- * - cleanup the setting of ins->type for MonoInst's which are pushed on the
- *   stack
- * - set the stack type and allocate a dreg in the EMIT_NEW macros
- * - get rid of all the <foo>2 stuff when the new JIT is ready.
- * - make sure handle_stack_args () is called before the branch is emitted
- * - when the new IR is done, get rid of all unused stuff
- * - COMPARE/BEQ as separate instructions or unify them ?
- *   - keeping them separate allows specialized compare instructions like
- *     compare_imm, compare_membase
- *   - most back ends unify fp compare+branch, fp compare+ceq
- * - integrate mono_save_args into inline_method
- * - get rid of the empty bblocks created by MONO_EMIT_NEW_BRACH_BLOCK2
- * - handle long shift opts on 32 bit platforms somehow: they require
- *   3 sregs (2 for arg1 and 1 for arg2)
- * - make byref a 'normal' type.
- * - use vregs for bb->out_stacks if possible, handle_global_vreg will make them a
- *   variable if needed.
- * - do not start a new IL level bblock when cfg->cbb is changed by a function call
- *   like inline_method.
- * - remove inlining restrictions
- * - fix LNEG and enable cfold of INEG
- * - generalize x86 optimizations like ldelema as a peephole optimization
- * - add store_mem_imm for amd64
- * - optimize the loading of the interruption flag in the managed->native wrappers
- * - avoid special handling of OP_NOP in passes
- * - move code inserting instructions into one function/macro.
- * - try a coalescing phase after liveness analysis
- * - add float -> vreg conversion + local optimizations on !x86
- * - figure out how to handle decomposed branches during optimizations, ie.
- *   compare+branch, op_jump_table+op_br etc.
- * - promote RuntimeXHandles to vregs
- * - vtype cleanups:
- *   - add a NEW_VARLOADA_VREG macro
- * - the vtype optimizations are blocked by the LDADDR opcodes generated for
- *   accessing vtype fields.
- * - get rid of I8CONST on 64 bit platforms
- * - dealing with the increase in code size due to branches created during opcode
- *   decomposition:
- *   - use extended basic blocks
- *     - all parts of the JIT
- *     - handle_global_vregs () && local regalloc
- *   - avoid introducing global vregs during decomposition, like 'vtable' in isinst
- * - sources of increase in code size:
- *   - vtypes
- *   - long compares
- *   - isinst and castclass
- *   - lvregs not allocated to global registers even if used multiple times
- * - call cctors outside the JIT, to make -v output more readable and JIT timings more
- *   meaningful.
- * - check for fp stack leakage in other opcodes too. (-> 'exceptions' optimization)
- * - add all micro optimizations from the old JIT
- * - put tree optimizations into the deadce pass
- * - decompose op_start_handler/op_endfilter/op_endfinally earlier using an arch
- *   specific function.
- * - unify the float comparison opcodes with the other comparison opcodes, i.e.
- *   fcompare + branchCC.
- * - create a helper function for allocating a stack slot, taking into account
- *   MONO_CFG_HAS_SPILLUP.
- * - merge r68207.
- * - optimize mono_regstate2_alloc_int/float.
- * - fix the pessimistic handling of variables accessed in exception handler blocks.
- * - need to write a tree optimization pass, but the creation of trees is difficult, i.e.
- *   parts of the tree could be separated by other instructions, killing the tree
- *   arguments, or stores killing loads etc. Also, should we fold loads into other
- *   instructions if the result of the load is used multiple times ?
- * - make the REM_IMM optimization in mini-x86.c arch-independent.
- * - LAST MERGE: 108395.
- * - when returning vtypes in registers, generate IR and append it to the end of the
- *   last bb instead of doing it in the epilog.
- * - change the store opcodes so they use sreg1 instead of dreg to store the base register.
- */
-/*
-NOTES
------
-- When to decompose opcodes:
-  - earlier: this makes some optimizations hard to implement, since the low level IR
-  no longer contains the necessary information. But it is easier to do.
-  - later: harder to implement, enables more optimizations.
-- Branches inside bblocks:
-  - created when decomposing complex opcodes.
-    - branches to another bblock: harmless, but not tracked by the branch
-      optimizations, so need to branch to a label at the start of the bblock.
-    - branches to inside the same bblock: very problematic, trips up the local
-      reg allocator. Can be fixed by spitting the current bblock, but that is a
-      complex operation, since some local vregs can become global vregs etc.
-- Local/global vregs:
-  - local vregs: temporary vregs used inside one bblock. Assigned to hregs by the
-    local register allocator.
-  - global vregs: used in more than one bblock. Have an associated MonoMethodVar
-    structure, created by mono_create_var (). Assigned to hregs or the stack by
-    the global register allocator.
-- When to do optimizations like alu->alu_imm:
-  - earlier -> saves work later on since the IR will be smaller/simpler
-  - later -> can work on more instructions
-- Handling of valuetypes:
-  - When a vtype is pushed on the stack, a new temporary is created, an
-    instruction computing its address (LDADDR) is emitted and pushed on
-    the stack. Need to optimize cases when the vtype is used immediately as in
-    argument passing, stloc etc.
-- Instead of the to_end stuff in the old JIT, simply call the function handling
-  the values on the stack before emitting the last instruction of the bb.
-*/
-#else /* !DISABLE_JIT */
-MONO_EMPTY_SOURCE_FILE (method_to_ir);
-#endif /* !DISABLE_JIT */

--- a/src/tasks/Microsoft.NET.Sdk.WebAssembly.Pack.Tasks/ComputeWasmBuildAssets.cs
+++ b//dev/null
@@ -1,226 +0,0 @@
-// Licensed to the .NET Foundation under one or more agreements.
-using System;
-using System.Collections.Generic;
-using System.IO;
-using System.Linq;
-using Microsoft.Build.Framework;
-using Microsoft.Build.Utilities;
-using Microsoft.NET.Sdk.WebAssembly;
-namespace Microsoft.NET.Sdk.WebAssembly;
-public class ComputeWasmBuildAssets : Task
-{
-    [Required]
-    public ITaskItem[] Candidates { get; set; }
-    public ITaskItem CustomIcuCandidate { get; set; }
-    [Required]
-    public ITaskItem[] ProjectAssembly { get; set; }
-    [Required]
-    public ITaskItem[] ProjectDebugSymbols { get; set; }
-    [Required]
-    public ITaskItem[] SatelliteAssemblies { get; set; }
-    [Required]
-    public ITaskItem[] ProjectSatelliteAssemblies { get; set; }
-    [Required]
-    public string DotNetJsVersion { get; set; }
-    [Required]
-    public string OutputPath { get; set; }
-    [Required]
-    public bool TimeZoneSupport { get; set; }
-    [Required]
-    public bool InvariantGlobalization { get; set; }
-    [Required]
-    public bool HybridGlobalization { get; set; }
-    [Required]
-    public bool LoadFullICUData { get; set; }
-    [Required]
-    public bool CopySymbols { get; set; }
-    public bool FingerprintDotNetJs { get; set; }
-    public bool EnableThreads { get; set; }
-    public bool EmitSourceMap { get; set; }
-    [Output]
-    public ITaskItem[] AssetCandidates { get; set; }
-    [Output]
-    public ITaskItem[] FilesToRemove { get; set; }
-    public override bool Execute()
-    {
-        var filesToRemove = new List<ITaskItem>();
-        var assetCandidates = new List<ITaskItem>();
-        try
-        {
-            if (ProjectAssembly.Length != 1)
-            {
-                Log.LogError("Invalid number of project assemblies '{0}'", string.Join("," + Environment.NewLine, ProjectAssembly.Select(a => a.ItemSpec)));
-                return true;
-            }
-            if (ProjectDebugSymbols.Length > 1)
-            {
-                Log.LogError("Invalid number of symbol assemblies '{0}'", string.Join("," + Environment.NewLine, ProjectDebugSymbols.Select(a => a.ItemSpec)));
-                return true;
-            }
-            if (!AssetsComputingHelper.TryGetAssetFilename(CustomIcuCandidate, out string customIcuCandidateFilename))
-            {
-                Log.LogMessage(MessageImportance.Low, "Custom icu asset was passed as empty.");
-            }
-            for (int i = 0; i < Candidates.Length; i++)
-            {
-                var candidate = Candidates[i];
-                if (AssetsComputingHelper.ShouldFilterCandidate(candidate, TimeZoneSupport, InvariantGlobalization, HybridGlobalization, LoadFullICUData, CopySymbols, customIcuCandidateFilename, EnableThreads, EmitSourceMap, out var reason))
-                {
-                    Log.LogMessage(MessageImportance.Low, "Skipping asset '{0}' because '{1}'", candidate.ItemSpec, reason);
-                    filesToRemove.Add(candidate);
-                    continue;
-                }
-                var satelliteAssembly = SatelliteAssemblies.FirstOrDefault(s => s.ItemSpec == candidate.ItemSpec);
-                if (satelliteAssembly != null)
-                {
-                    var inferredCulture = satelliteAssembly.GetMetadata("DestinationSubDirectory").Trim('\\', '/');
-                    Log.LogMessage(MessageImportance.Low, "Found satellite assembly '{0}' asset for candidate '{1}' with inferred culture '{2}'", satelliteAssembly.ItemSpec, candidate.ItemSpec, inferredCulture);
-                    var assetCandidate = new TaskItem(satelliteAssembly);
-                    assetCandidate.SetMetadata("AssetKind", "Build");
-                    assetCandidate.SetMetadata("AssetRole", "Related");
-                    assetCandidate.SetMetadata("AssetTraitName", "Culture");
-                    assetCandidate.SetMetadata("AssetTraitValue", inferredCulture);
-                    assetCandidate.SetMetadata("RelativePath", $"_framework/{inferredCulture}/{satelliteAssembly.GetMetadata("FileName")}{satelliteAssembly.GetMetadata("Extension")}");
-                    var resolvedFrom = assetCandidate.GetMetadata("ResolvedFrom");
-                    if (resolvedFrom == "{RawFileName}") // Satellite assembly found from `<Reference />` element
-                        resolvedFrom = candidate.GetMetadata("OriginalItemSpec");
-                    assetCandidate.SetMetadata("RelatedAsset", Path.GetFullPath(Path.Combine(OutputPath, "wwwroot", "_framework", Path.GetFileName(resolvedFrom))));
-                    assetCandidates.Add(assetCandidate);
-                    continue;
-                }
-                string candidateFileName = candidate.GetMetadata("FileName");
-                if (candidateFileName.StartsWith("dotnet") && candidate.GetMetadata("Extension") == ".js")
-                {
-                    string newDotnetJSFileName = null;
-                    string newDotNetJSFullPath = null;
-                    if (candidateFileName != "dotnet" || FingerprintDotNetJs)
-                    {
-                        var itemHash = FileHasher.GetFileHash(candidate.ItemSpec);
-                        newDotnetJSFileName = $"{candidateFileName}.{DotNetJsVersion}.{itemHash}.js";
-                        var originalFileFullPath = Path.GetFullPath(candidate.ItemSpec);
-                        var originalFileDirectory = Path.GetDirectoryName(originalFileFullPath);
-                        newDotNetJSFullPath = Path.Combine(originalFileDirectory, newDotnetJSFileName);
-                    }
-                    else
-                    {
-                        newDotNetJSFullPath = candidate.ItemSpec;
-                        newDotnetJSFileName = Path.GetFileName(newDotNetJSFullPath);
-                    }
-                    var newDotNetJs = new TaskItem(newDotNetJSFullPath, candidate.CloneCustomMetadata());
-                    newDotNetJs.SetMetadata("OriginalItemSpec", candidate.ItemSpec);
-                    var newRelativePath = $"_framework/{newDotnetJSFileName}";
-                    newDotNetJs.SetMetadata("RelativePath", newRelativePath);
-                    newDotNetJs.SetMetadata("AssetTraitName", "WasmResource");
-                    newDotNetJs.SetMetadata("AssetTraitValue", "native");
-                    assetCandidates.Add(newDotNetJs);
-                    continue;
-                }
-                else
-                {
-                    string relativePath = AssetsComputingHelper.GetCandidateRelativePath(candidate);
-                    candidate.SetMetadata("RelativePath", relativePath);
-                }
-                if (candidate.GetMetadata("ReferenceSourceTarget") == "ProjectReference")
-                {
-                    candidate.SetMetadata("OriginalItemSpec", candidate.ItemSpec);
-                }
-                var culture = candidate.GetMetadata("Culture");
-                if (!string.IsNullOrEmpty(culture))
-                {
-                    candidate.SetMetadata("AssetKind", "Build");
-                    candidate.SetMetadata("AssetRole", "Related");
-                    candidate.SetMetadata("AssetTraitName", "Culture");
-                    candidate.SetMetadata("AssetTraitValue", culture);
-                    var fileName = candidate.GetMetadata("FileName");
-                    var suffixIndex = fileName.Length - ".resources".Length;
-                    var relatedAssetPath = Path.GetFullPath(Path.Combine(
-                        OutputPath,
-                        "wwwroot",
-                        "_framework",
-                        fileName.Substring(0, suffixIndex) + ProjectAssembly[0].GetMetadata("Extension")));
-                    candidate.SetMetadata("RelatedAsset", relatedAssetPath);
-                    Log.LogMessage(MessageImportance.Low, "Found satellite assembly '{0}' asset for inferred candidate '{1}' with culture '{2}'", candidate.ItemSpec, relatedAssetPath, culture);
-                }
-                assetCandidates.Add(candidate);
-            }
-            var intermediateAssembly = new TaskItem(ProjectAssembly[0]);
-            intermediateAssembly.SetMetadata("RelativePath", $"_framework/{intermediateAssembly.GetMetadata("FileName")}{intermediateAssembly.GetMetadata("Extension")}");
-            assetCandidates.Add(intermediateAssembly);
-            if (ProjectDebugSymbols.Length > 0)
-            {
-                var debugSymbols = new TaskItem(ProjectDebugSymbols[0]);
-                debugSymbols.SetMetadata("RelativePath", $"_framework/{debugSymbols.GetMetadata("FileName")}{debugSymbols.GetMetadata("Extension")}");
-                assetCandidates.Add(debugSymbols);
-            }
-            for (int i = 0; i < ProjectSatelliteAssemblies.Length; i++)
-            {
-                var projectSatelliteAssembly = ProjectSatelliteAssemblies[i];
-                var candidateCulture = projectSatelliteAssembly.GetMetadata("Culture");
-                Log.LogMessage(
-                    "Found satellite assembly '{0}' asset for project '{1}' with culture '{2}'",
-                    projectSatelliteAssembly.ItemSpec,
-                    intermediateAssembly.ItemSpec,
-                    candidateCulture);
-                var assetCandidate = new TaskItem(Path.GetFullPath(projectSatelliteAssembly.ItemSpec), projectSatelliteAssembly.CloneCustomMetadata());
-                var projectAssemblyAssetPath = Path.GetFullPath(Path.Combine(
-                    OutputPath,
-                    "wwwroot",
-                    "_framework",
-                    ProjectAssembly[0].GetMetadata("FileName") + ProjectAssembly[0].GetMetadata("Extension")));
-                var normalizedPath = assetCandidate.GetMetadata("TargetPath").Replace('\\', '/');
-                assetCandidate.SetMetadata("AssetKind", "Build");
-                assetCandidate.SetMetadata("AssetRole", "Related");
-                assetCandidate.SetMetadata("AssetTraitName", "Culture");
-                assetCandidate.SetMetadata("AssetTraitValue", candidateCulture);
-                assetCandidate.SetMetadata("RelativePath", Path.Combine("_framework", normalizedPath));
-                assetCandidate.SetMetadata("RelatedAsset", projectAssemblyAssetPath);
-                assetCandidates.Add(assetCandidate);
-            }
-            for (var i = 0; i < assetCandidates.Count; i++)
-            {
-                var candidate = assetCandidates[i];
-                ApplyUniqueMetadataProperties(candidate);
-            }
-        }
-        catch (Exception ex)
-        {
-            Log.LogError(ex.ToString());
-            return false;
-        }
-        FilesToRemove = filesToRemove.ToArray();
-        AssetCandidates = assetCandidates.ToArray();
-        return !Log.HasLoggedErrors;
-    }
-    private static void ApplyUniqueMetadataProperties(ITaskItem candidate)
-    {
-        var extension = candidate.GetMetadata("Extension");
-        var filename = candidate.GetMetadata("FileName");
-        switch (extension)
-        {
-            case ".dll":
-                if (string.IsNullOrEmpty(candidate.GetMetadata("AssetTraitName")))
-                {
-                    candidate.SetMetadata("AssetTraitName", "WasmResource");
-                    candidate.SetMetadata("AssetTraitValue", "runtime");
-                }
-                if (string.Equals(candidate.GetMetadata("ResolvedFrom"), "{HintPathFromItem}", StringComparison.Ordinal))
-                {
-                    candidate.RemoveMetadata("OriginalItemSpec");
-                }
-                break;
-            case ".wasm":
-            case ".blat":
-            case ".dat" when filename.StartsWith("icudt"):
-                candidate.SetMetadata("AssetTraitName", "WasmResource");
-                candidate.SetMetadata("AssetTraitValue", "native");
-                break;
-            case ".pdb":
-                candidate.SetMetadata("AssetTraitName", "WasmResource");
-                candidate.SetMetadata("AssetTraitValue", "symbol");
-                candidate.RemoveMetadata("OriginalItemSpec");
-                break;
-            default:
-                break;
-        }
-    }
-}
