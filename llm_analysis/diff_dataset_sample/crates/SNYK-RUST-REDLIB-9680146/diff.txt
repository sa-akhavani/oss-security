--- a/scripts/update_oauth_resources.sh
+++ b//dev/null
@@ -1,59 +0,0 @@
-ios_version_list=$(curl -s "https://ipaarchive.com/app/usa/1064216828" | rg "(20\d{2}\.\d+.\d+) / (\d+)" --only-matching -r "Version \$1/Build \$2" | sort | uniq)
-ios_app_count=$(echo "$ios_version_list" | wc -l)
-echo -e "Fetching \e[34m$ios_app_count iOS app versions...\e[0m"
-filename="src/oauth_resources.rs"
-echo "// This file was generated by scripts/update_oauth_resources.sh" > "$filename"
-echo "// Rerun scripts/update_oauth_resources.sh to update this file" >> "$filename"
-echo "// Please do not edit manually" >> "$filename"
-echo "// Filled in with real app versions" >> "$filename"
-echo "pub const _IOS_APP_VERSION_LIST: &[&str; $ios_app_count] = &[" >> "$filename"
-num=0
-echo "$ios_version_list" | while IFS= read -r line; do
-  num=$((num+1))
-  echo "	\"$line\"," >> "$filename"
-  echo -e "[$num/$ios_app_count] Fetched \e[34m$line\e[0m."
-done
-echo "];" >> "$filename"
-page_1=$(curl -s "https://apkcombo.com/reddit/com.reddit.frontpage/old-versions/" | rg "<a class=\"ver-item\" href=\"(/reddit/com\.reddit\.frontpage/download/phone-20\d{2}\.\d+\.\d+-apk)\" rel=\"nofollow\">" -r "https://apkcombo.com\$1" | sort | uniq | sed 's/      //g')
-page_2=$(curl -s "https://apkcombo.com/reddit/com.reddit.frontpage/old-versions?page=2" | rg "<a class=\"ver-item\" href=\"(/reddit/com\.reddit\.frontpage/download/phone-20\d{2}\.\d+\.\d+-apk)\" rel=\"nofollow\">" -r "https://apkcombo.com\$1" | sort | uniq | sed 's/      //g')
-page_3=$(curl -s "https://apkcombo.com/reddit/com.reddit.frontpage/old-versions?page=3" | rg "<a class=\"ver-item\" href=\"(/reddit/com\.reddit\.frontpage/download/phone-20\d{2}\.\d+\.\d+-apk)\" rel=\"nofollow\">" -r "https://apkcombo.com\$1" | sort | uniq | sed 's/      //g')
-page_4=$(curl -s "https://apkcombo.com/reddit/com.reddit.frontpage/old-versions?page=4" | rg "<a class=\"ver-item\" href=\"(/reddit/com\.reddit\.frontpage/download/phone-20\d{2}\.\d+\.\d+-apk)\" rel=\"nofollow\">" -r "https://apkcombo.com\$1" | sort | uniq | sed 's/      //g')
-page_5=$(curl -s "https://apkcombo.com/reddit/com.reddit.frontpage/old-versions?page=5" | rg "<a class=\"ver-item\" href=\"(/reddit/com\.reddit\.frontpage/download/phone-20\d{2}\.\d+\.\d+-apk)\" rel=\"nofollow\">" -r "https://apkcombo.com\$1" | sort | uniq | sed 's/      //g')
-versions="${page_1}"
-versions+=$'\n'
-versions+="${page_2}"
-versions+=$'\n'
-versions+="${page_3}"
-versions+=$'\n'
-versions+="${page_4}"
-versions+=$'\n'
-versions+="${page_5}"
-android_count=$(echo "$versions" | wc -l)
-echo -e "Fetching \e[32m$android_count Android app versions...\e[0m"
-echo "pub const ANDROID_APP_VERSION_LIST: &[&str; $android_count] = &[" >> "$filename"
-num=0
-echo "$versions" | while IFS= read -r line; do
-  num=$((num+1))
-  fetch_page=$(curl -s "$line")
-  build=$(echo "$fetch_page" | rg "<span class=\"vercode\">\((\d+)\)</span>" --only-matching -r "\$1" | head -n1)
-  version=$(echo "$fetch_page" | rg "<span class=\"vername\">Reddit (20\d{2}\.\d+\.\d+)</span>" --only-matching -r "\$1" | head -n1)
-  echo "	\"Version $version/Build $build\"," >> "$filename"
-  echo -e "[$num/$android_count] Fetched \e[32mVersion $version/Build $build\e[0m."
-done
-echo "];" >> "$filename"
-table=$(curl -s "https://en.wikipedia.org/w/api.php?action=parse&page=IOS_17&prop=wikitext&section=31&format=json" | jq ".parse.wikitext.\"*\"" | rg "(17\.[\d\.]*)\\\n\|(\w*)\\\n\|" --only-matching -r "Version \$1 (Build \$2)")
-ios_count=$(echo "$table" | wc -l)
-echo -e "Fetching \e[34m$ios_count iOS versions...\e[0m"
-echo "pub const _IOS_OS_VERSION_LIST: &[&str; $ios_count] = &[" >> "$filename"
-num=0
-echo "$table" | while IFS= read -r line; do
-  num=$((num+1))
-  echo "	\"$line\"," >> "$filename"
-  echo -e "\e[34m[$num/$ios_count] Fetched $line\e[0m."
-done
-echo "];" >> "$filename"
-echo -e "\e[34mRetrieved $ios_app_count iOS app versions.\e[0m"
-echo -e "\e[32mRetrieved $android_count Android app versions.\e[0m"
-echo -e "\e[34mRetrieved $ios_count iOS versions.\e[0m"
-echo -e "\e[34mTotal: $((ios_app_count + android_count + ios_count))\e[0m"
-echo -e "\e[32mSuccess!\e[0m"

--- a/src/client.rs
+++ b//dev/null
@@ -1,389 +0,0 @@
-use arc_swap::ArcSwap;
-use cached::proc_macro::cached;
-use futures_lite::future::block_on;
-use futures_lite::{future::Boxed, FutureExt};
-use hyper::client::HttpConnector;
-use hyper::header::HeaderValue;
-use hyper::{body, body::Buf, header, Body, Client, Method, Request, Response, Uri};
-use hyper_rustls::HttpsConnector;
-use libflate::gzip;
-use log::{error, trace, warn};
-use once_cell::sync::Lazy;
-use percent_encoding::{percent_encode, CONTROLS};
-use serde_json::Value;
-use std::sync::atomic::Ordering;
-use std::sync::atomic::{AtomicBool, AtomicU16};
-use std::{io, result::Result};
-use crate::dbg_msg;
-use crate::oauth::{force_refresh_token, token_daemon, Oauth};
-use crate::server::RequestExt;
-use crate::utils::{format_url, Post};
-const REDDIT_URL_BASE: &str = "https://oauth.reddit.com";
-const REDDIT_URL_BASE_HOST: &str = "oauth.reddit.com";
-const REDDIT_SHORT_URL_BASE: &str = "https://redd.it";
-const REDDIT_SHORT_URL_BASE_HOST: &str = "redd.it";
-const ALTERNATIVE_REDDIT_URL_BASE: &str = "https://www.reddit.com";
-const ALTERNATIVE_REDDIT_URL_BASE_HOST: &str = "www.reddit.com";
-pub static HTTPS_CONNECTOR: Lazy<HttpsConnector<HttpConnector>> =
-	Lazy::new(|| hyper_rustls::HttpsConnectorBuilder::new().with_native_roots().https_only().enable_http2().build());
-pub static CLIENT: Lazy<Client<HttpsConnector<HttpConnector>>> = Lazy::new(|| Client::builder().build::<_, Body>(HTTPS_CONNECTOR.clone()));
-pub static OAUTH_CLIENT: Lazy<ArcSwap<Oauth>> = Lazy::new(|| {
-	let client = block_on(Oauth::new());
-	tokio::spawn(token_daemon());
-	ArcSwap::new(client.into())
-});
-pub static OAUTH_RATELIMIT_REMAINING: AtomicU16 = AtomicU16::new(99);
-pub static OAUTH_IS_ROLLING_OVER: AtomicBool = AtomicBool::new(false);
-const URL_PAIRS: [(&str, &str); 2] = [
-	(ALTERNATIVE_REDDIT_URL_BASE, ALTERNATIVE_REDDIT_URL_BASE_HOST),
-	(REDDIT_SHORT_URL_BASE, REDDIT_SHORT_URL_BASE_HOST),
-];
-#[cached(size = 1024, time = 600, result = true)]
-#[async_recursion::async_recursion]
-pub async fn canonical_path(path: String, tries: i8) -> Result<Option<String>, String> {
-	if tries == 0 {
-		return Ok(None);
-	}
-	let res = {
-		let mut res = None;
-		for (url_base, url_base_host) in URL_PAIRS {
-			res = reddit_short_head(path.clone(), true, url_base, url_base_host).await.ok();
-			if let Some(res) = &res {
-				if !res.status().is_client_error() {
-					break;
-				}
-			}
-		}
-		res
-	};
-	let res = res.ok_or_else(|| "Unable to make HEAD request to Reddit.".to_string())?;
-	let status = res.status().as_u16();
-	let policy_error = res.headers().get(header::RETRY_AFTER).is_some();
-	match status {
-		200..=299 => Ok(Some(path)),
-		301 => match res.headers().get(header::LOCATION) {
-			Some(val) => {
-				let Ok(original) = val.to_str() else {
-					return Err("Unable to decode Location header.".to_string());
-				};
-				let stripped_uri = original.strip_suffix(".json").unwrap_or(original).split('?').next().unwrap_or_default();
-				let uri = format_url(stripped_uri);
-				canonical_path(uri, tries - 1).await
-			}
-			None => Ok(None),
-		},
-		300..=399 => Ok(None),
-		429 => Err("Too many requests.".to_string()),
-		403 if policy_error => Err("Too many requests.".to_string()),
-		_ => Ok(
-			res
-				.headers()
-				.get(header::LOCATION)
-				.map(|val| percent_encode(val.as_bytes(), CONTROLS).to_string().trim_start_matches(REDDIT_URL_BASE).to_string()),
-		),
-	}
-}
-pub async fn proxy(req: Request<Body>, format: &str) -> Result<Response<Body>, String> {
-	let mut url = format!("{format}?{}", req.uri().query().unwrap_or_default());
-	for (name, value) in &req.params() {
-		url = url.replace(&format!("{{{name}}}"), value);
-	}
-	stream(&url, &req).await
-}
-async fn stream(url: &str, req: &Request<Body>) -> Result<Response<Body>, String> {
-	let parsed_uri = url.parse::<Uri>().map_err(|_| "Couldn't parse URL".to_string())?;
-	let client: &Lazy<Client<_, Body>> = &CLIENT;
-	let mut builder = Request::get(parsed_uri);
-	for &key in &["Range", "If-Modified-Since", "Cache-Control"] {
-		if let Some(value) = req.headers().get(key) {
-			builder = builder.header(key, value);
-		}
-	}
-	let stream_request = builder.body(Body::empty()).map_err(|_| "Couldn't build empty body in stream".to_string())?;
-	client
-		.request(stream_request)
-		.await
-		.map(|mut res| {
-			let mut rm = |key: &str| res.headers_mut().remove(key);
-			rm("access-control-expose-headers");
-			rm("server");
-			rm("vary");
-			rm("etag");
-			rm("x-cdn");
-			rm("x-cdn-client-region");
-			rm("x-cdn-name");
-			rm("x-cdn-server-region");
-			rm("x-reddit-cdn");
-			rm("x-reddit-video-features");
-			rm("Nel");
-			rm("Report-To");
-			res
-		})
-		.map_err(|e| e.to_string())
-}
-fn reddit_get(path: String, quarantine: bool) -> Boxed<Result<Response<Body>, String>> {
-	request(&Method::GET, path, true, quarantine, REDDIT_URL_BASE, REDDIT_URL_BASE_HOST)
-}
-fn reddit_short_head(path: String, quarantine: bool, base_path: &'static str, host: &'static str) -> Boxed<Result<Response<Body>, String>> {
-	request(&Method::HEAD, path, false, quarantine, base_path, host)
-}
-fn request(method: &'static Method, path: String, redirect: bool, quarantine: bool, base_path: &'static str, host: &'static str) -> Boxed<Result<Response<Body>, String>> {
-	let url = format!("{base_path}{path}");
-	let client: &Lazy<Client<_, Body>> = &CLIENT;
-	let mut headers: Vec<(String, String)> = vec![
-		("Host".into(), host.into()),
-		("Accept-Encoding".into(), if method == Method::GET { "gzip".into() } else { "identity".into() }),
-		(
-			"Cookie".into(),
-			if quarantine {
-				"_options=%7B%22pref_quarantine_optin%22%3A%20true%2C%20%22pref_gated_sr_optin%22%3A%20true%7D".into()
-			} else {
-				"".into()
-			},
-		),
-	];
-	{
-		let client = OAUTH_CLIENT.load_full();
-		for (key, value) in client.headers_map.clone() {
-			headers.push((key, value));
-		}
-	}
-	fastrand::shuffle(&mut headers);
-	let mut builder = Request::builder().method(method).uri(&url);
-	for (key, value) in headers {
-		builder = builder.header(key, value);
-	}
-	let builder = builder.body(Body::empty());
-	async move {
-		match builder {
-			Ok(req) => match client.request(req).await {
-				Ok(mut response) => {
-					if response.status().is_redirection() {
-						if !redirect {
-							return Ok(response);
-						};
-						let location_header = response.headers().get(header::LOCATION);
-						if location_header == Some(&HeaderValue::from_static(ALTERNATIVE_REDDIT_URL_BASE)) {
-							return Err("Reddit response was invalid".to_string());
-						}
-						return request(
-							method,
-							location_header
-								.map(|val| {
-									let new_path = percent_encode(val.as_bytes(), CONTROLS)
-										.to_string()
-										.trim_start_matches(REDDIT_URL_BASE)
-										.trim_start_matches(ALTERNATIVE_REDDIT_URL_BASE)
-										.to_string();
-									format!("{new_path}{}raw_json=1", if new_path.contains('?') { "&" } else { "?" })
-								})
-								.unwrap_or_default()
-								.to_string(),
-							true,
-							quarantine,
-							base_path,
-							host,
-						)
-						.await;
-					};
-					match response.headers().get(header::CONTENT_ENCODING) {
-						None => Ok(response),
-						Some(hdr) => {
-							match hdr.to_str() {
-								Ok(val) => match val {
-									"gzip" => {}
-									"identity" => return Ok(response),
-									_ => return Err("Reddit response was encoded with an unsupported compressor".to_string()),
-								},
-								Err(_) => return Err("Reddit response was invalid".to_string()),
-							}
-							let mut decompressed: Vec<u8>;
-							{
-								let mut aggregated_body = match body::aggregate(response.body_mut()).await {
-									Ok(b) => b.reader(),
-									Err(e) => return Err(e.to_string()),
-								};
-								let mut decoder = match gzip::Decoder::new(&mut aggregated_body) {
-									Ok(decoder) => decoder,
-									Err(e) => return Err(e.to_string()),
-								};
-								decompressed = Vec::<u8>::new();
-								if let Err(e) = io::copy(&mut decoder, &mut decompressed) {
-									return Err(e.to_string());
-								};
-							}
-							response.headers_mut().remove(header::CONTENT_ENCODING);
-							response.headers_mut().insert(header::CONTENT_LENGTH, decompressed.len().into());
-							*(response.body_mut()) = Body::from(decompressed);
-							Ok(response)
-						}
-					}
-				}
-				Err(e) => {
-					dbg_msg!("{method} {REDDIT_URL_BASE}{path}: {}", e);
-					Err(e.to_string())
-				}
-			},
-			Err(_) => Err("Post url contains non-ASCII characters".to_string()),
-		}
-	}
-	.boxed()
-}
-#[cached(size = 100, time = 30, result = true)]
-pub async fn json(path: String, quarantine: bool) -> Result<Value, String> {
-	let err = |msg: &str, e: String, path: String| -> Result<Value, String> {
-		Err(format!("{msg}: {e} | {path}"))
-	};
-	let current_rate_limit = OAUTH_RATELIMIT_REMAINING.load(Ordering::SeqCst);
-	let is_rolling_over = OAUTH_IS_ROLLING_OVER.load(Ordering::SeqCst);
-	if current_rate_limit < 10 && !is_rolling_over {
-		warn!("Rate limit {current_rate_limit} is low. Spawning force_refresh_token()");
-		tokio::spawn(force_refresh_token());
-	}
-	OAUTH_RATELIMIT_REMAINING.fetch_sub(1, Ordering::SeqCst);
-	match reddit_get(path.clone(), quarantine).await {
-		Ok(response) => {
-			let status = response.status();
-			let reset: Option<String> = if let (Some(remaining), Some(reset), Some(used)) = (
-				response.headers().get("x-ratelimit-remaining").and_then(|val| val.to_str().ok().map(|s| s.to_string())),
-				response.headers().get("x-ratelimit-reset").and_then(|val| val.to_str().ok().map(|s| s.to_string())),
-				response.headers().get("x-ratelimit-used").and_then(|val| val.to_str().ok().map(|s| s.to_string())),
-			) {
-				trace!(
-					"Ratelimit remaining: Header says {remaining}, we have {current_rate_limit}. Resets in {reset}. Rollover: {}. Ratelimit used: {used}",
-					if is_rolling_over { "yes" } else { "no" },
-				);
-				if let Ok(val) = remaining.parse::<f32>() {
-					OAUTH_RATELIMIT_REMAINING.store(val.round() as u16, Ordering::SeqCst);
-				}
-				Some(reset)
-			} else {
-				None
-			};
-			match hyper::body::aggregate(response).await {
-				Ok(body) => {
-					let has_remaining = body.has_remaining();
-					if !has_remaining {
-						tokio::spawn(force_refresh_token());
-						return match reset {
-							Some(val) => Err(format!(
-								"Reddit rate limit exceeded. Try refreshing in a few seconds.\
-								 Rate limit will reset in: {val}"
-							)),
-							None => Err("Reddit rate limit exceeded".to_string()),
-						};
-					}
-					match serde_json::from_reader(body.reader()) {
-						Ok(value) => {
-							let json: Value = value;
-							if let Some(data) = json.get("data") {
-								if let Some(is_suspended) = data.get("is_suspended").and_then(Value::as_bool) {
-									if is_suspended {
-										return Err("suspended".into());
-									}
-								}
-							}
-							if json["error"].is_i64() {
-								if json["message"] == "Unauthorized" {
-									error!("Forcing a token refresh");
-									let () = force_refresh_token().await;
-									return Err("OAuth token has expired. Please refresh the page!".to_string());
-								}
-								if json["reason"] == "quarantined" {
-									return Err("quarantined".into());
-								}
-								if json["reason"] == "gated" {
-									return Err("gated".into());
-								}
-								if json["reason"] == "private" {
-									return Err("private".into());
-								}
-								if json["reason"] == "banned" {
-									return Err("banned".into());
-								}
-								Err(format!("Reddit error {} \"{}\": {} | {path}", json["error"], json["reason"], json["message"]))
-							} else {
-								Ok(json)
-							}
-						}
-						Err(e) => {
-							error!("Got an invalid response from reddit {e}. Status code: {status}");
-							if status.is_server_error() {
-								Err("Reddit is having issues, check if there's an outage".to_string())
-							} else {
-								err("Failed to parse page JSON data", e.to_string(), path)
-							}
-						}
-					}
-				}
-				Err(e) => err("Failed receiving body from Reddit", e.to_string(), path),
-			}
-		}
-		Err(e) => err("Couldn't send request to Reddit", e, path),
-	}
-}
-async fn self_check(sub: &str) -> Result<(), String> {
-	let query = format!("/r/{sub}/hot.json?&raw_json=1");
-	match Post::fetch(&query, true).await {
-		Ok(_) => Ok(()),
-		Err(e) => Err(e),
-	}
-}
-pub async fn rate_limit_check() -> Result<(), String> {
-	self_check("reddit").await?;
-	if OAUTH_RATELIMIT_REMAINING.load(Ordering::SeqCst) != 99 {
-		return Err(format!("Rate limit check failed: expected 99, got {}", OAUTH_RATELIMIT_REMAINING.load(Ordering::SeqCst)));
-	}
-	force_refresh_token().await;
-	self_check("rust").await?;
-	if OAUTH_RATELIMIT_REMAINING.load(Ordering::SeqCst) != 99 {
-		return Err(format!("Rate limit check failed: expected 99, got {}", OAUTH_RATELIMIT_REMAINING.load(Ordering::SeqCst)));
-	}
-	Ok(())
-}
-#[cfg(test)]
-use {crate::config::get_setting, sealed_test::prelude::*};
-#[tokio::test(flavor = "multi_thread")]
-async fn test_rate_limit_check() {
-	rate_limit_check().await.unwrap();
-}
-#[test]
-#[sealed_test(env = [("REDLIB_DEFAULT_SUBSCRIPTIONS", "rust")])]
-fn test_default_subscriptions() {
-	tokio::runtime::Builder::new_multi_thread().enable_all().build().unwrap().block_on(async {
-		let subscriptions = get_setting("REDLIB_DEFAULT_SUBSCRIPTIONS");
-		assert!(subscriptions.is_some());
-		rate_limit_check().await.unwrap();
-	});
-}
-#[cfg(test)]
-const POPULAR_URL: &str = "/r/popular/hot.json?&raw_json=1&geo_filter=GLOBAL";
-#[tokio::test(flavor = "multi_thread")]
-async fn test_localization_popular() {
-	let val = json(POPULAR_URL.to_string(), false).await.unwrap();
-	assert_eq!("GLOBAL", val["data"]["geo_filter"].as_str().unwrap());
-}
-#[tokio::test(flavor = "multi_thread")]
-async fn test_obfuscated_share_link() {
-	let share_link = "/r/rust/s/kPgq8WNHRK".into();
-	let canonical_link = "/r/rust/comments/18t5968/why_use_tuple_struct_over_standard_struct/kfbqlbc/".into();
-	assert_eq!(canonical_path(share_link, 3).await, Ok(Some(canonical_link)));
-}
-#[tokio::test(flavor = "multi_thread")]
-async fn test_private_sub() {
-	let link = json("/r/suicide/about.json?raw_json=1".into(), true).await;
-	assert!(link.is_err());
-	assert_eq!(link, Err("private".into()));
-}
-#[tokio::test(flavor = "multi_thread")]
-async fn test_banned_sub() {
-	let link = json("/r/aaa/about.json?raw_json=1".into(), true).await;
-	assert!(link.is_err());
-	assert_eq!(link, Err("banned".into()));
-}
-#[tokio::test(flavor = "multi_thread")]
-async fn test_gated_sub() {
-	let link = json("/r/drugs/about.json?raw_json=1".into(), false).await;
-	assert!(link.is_err());
-	assert_eq!(link, Err("gated".into()));
-}

--- a/src/config.rs
+++ b//dev/null
@@ -1,211 +0,0 @@
-use once_cell::sync::Lazy;
-use serde::{Deserialize, Serialize};
-use std::{env::var, fs::read_to_string};
-pub static CONFIG: Lazy<Config> = Lazy::new(Config::load);
-pub const DEFAULT_PUSHSHIFT_FRONTEND: &str = "undelete.pullpush.io";
-#[derive(Default, Serialize, Deserialize, Clone, Debug)]
-pub struct Config {
-	#[serde(rename = "REDLIB_SFW_ONLY")]
-	#[serde(alias = "LIBREDDIT_SFW_ONLY")]
-	pub(crate) sfw_only: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_THEME")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_THEME")]
-	pub(crate) default_theme: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_FRONT_PAGE")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_FRONT_PAGE")]
-	pub(crate) default_front_page: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_LAYOUT")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_LAYOUT")]
-	pub(crate) default_layout: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_WIDE")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_WIDE")]
-	pub(crate) default_wide: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_COMMENT_SORT")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_COMMENT_SORT")]
-	pub(crate) default_comment_sort: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_POST_SORT")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_POST_SORT")]
-	pub(crate) default_post_sort: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_BLUR_SPOILER")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_BLUR_SPOILER")]
-	pub(crate) default_blur_spoiler: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_SHOW_NSFW")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_SHOW_NSFW")]
-	pub(crate) default_show_nsfw: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_BLUR_NSFW")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_BLUR_NSFW")]
-	pub(crate) default_blur_nsfw: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_USE_HLS")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_USE_HLS")]
-	pub(crate) default_use_hls: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_HIDE_HLS_NOTIFICATION")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_HIDE_HLS_NOTIFICATION")]
-	pub(crate) default_hide_hls_notification: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_HIDE_AWARDS")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_HIDE_AWARDS")]
-	pub(crate) default_hide_awards: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_HIDE_SIDEBAR_AND_SUMMARY")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_HIDE_SIDEBAR_AND_SUMMARY")]
-	pub(crate) default_hide_sidebar_and_summary: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_HIDE_SCORE")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_HIDE_SCORE")]
-	pub(crate) default_hide_score: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_SUBSCRIPTIONS")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_SUBSCRIPTIONS")]
-	pub(crate) default_subscriptions: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_FILTERS")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_FILTERS")]
-	pub(crate) default_filters: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_DISABLE_VISIT_REDDIT_CONFIRMATION")]
-	#[serde(alias = "LIBREDDIT_DEFAULT_DISABLE_VISIT_REDDIT_CONFIRMATION")]
-	pub(crate) default_disable_visit_reddit_confirmation: Option<String>,
-	#[serde(rename = "REDLIB_BANNER")]
-	#[serde(alias = "LIBREDDIT_BANNER")]
-	pub(crate) banner: Option<String>,
-	#[serde(rename = "REDLIB_ROBOTS_DISABLE_INDEXING")]
-	#[serde(alias = "LIBREDDIT_ROBOTS_DISABLE_INDEXING")]
-	pub(crate) robots_disable_indexing: Option<String>,
-	#[serde(rename = "REDLIB_PUSHSHIFT_FRONTEND")]
-	#[serde(alias = "LIBREDDIT_PUSHSHIFT_FRONTEND")]
-	pub(crate) pushshift: Option<String>,
-	#[serde(rename = "REDLIB_ENABLE_RSS")]
-	pub(crate) enable_rss: Option<String>,
-	#[serde(rename = "REDLIB_FULL_URL")]
-	pub(crate) full_url: Option<String>,
-	#[serde(rename = "REDLIB_DEFAULT_REMOVE_DEFAULT_FEEDS")]
-	pub(crate) default_remove_default_feeds: Option<String>,
-}
-impl Config {
-	pub fn load() -> Self {
-		let load_config = |name: &str| {
-			let new_file = read_to_string(name);
-			new_file.ok().and_then(|new_file| toml::from_str::<Self>(&new_file).ok())
-		};
-		let config = load_config("redlib.toml").or_else(|| load_config("libreddit.toml")).unwrap_or_default();
-		let parse = |key: &str| -> Option<String> {
-			let legacy_key = key.replace("REDLIB_", "LIBREDDIT_");
-			var(key).ok().or_else(|| var(legacy_key).ok()).or_else(|| get_setting_from_config(key, &config))
-		};
-		Self {
-			sfw_only: parse("REDLIB_SFW_ONLY"),
-			default_theme: parse("REDLIB_DEFAULT_THEME"),
-			default_front_page: parse("REDLIB_DEFAULT_FRONT_PAGE"),
-			default_layout: parse("REDLIB_DEFAULT_LAYOUT"),
-			default_post_sort: parse("REDLIB_DEFAULT_POST_SORT"),
-			default_wide: parse("REDLIB_DEFAULT_WIDE"),
-			default_comment_sort: parse("REDLIB_DEFAULT_COMMENT_SORT"),
-			default_blur_spoiler: parse("REDLIB_DEFAULT_BLUR_SPOILER"),
-			default_show_nsfw: parse("REDLIB_DEFAULT_SHOW_NSFW"),
-			default_blur_nsfw: parse("REDLIB_DEFAULT_BLUR_NSFW"),
-			default_use_hls: parse("REDLIB_DEFAULT_USE_HLS"),
-			default_hide_hls_notification: parse("REDLIB_DEFAULT_HIDE_HLS_NOTIFICATION"),
-			default_hide_awards: parse("REDLIB_DEFAULT_HIDE_AWARDS"),
-			default_hide_sidebar_and_summary: parse("REDLIB_DEFAULT_HIDE_SIDEBAR_AND_SUMMARY"),
-			default_hide_score: parse("REDLIB_DEFAULT_HIDE_SCORE"),
-			default_subscriptions: parse("REDLIB_DEFAULT_SUBSCRIPTIONS"),
-			default_filters: parse("REDLIB_DEFAULT_FILTERS"),
-			default_disable_visit_reddit_confirmation: parse("REDLIB_DEFAULT_DISABLE_VISIT_REDDIT_CONFIRMATION"),
-			banner: parse("REDLIB_BANNER"),
-			robots_disable_indexing: parse("REDLIB_ROBOTS_DISABLE_INDEXING"),
-			pushshift: parse("REDLIB_PUSHSHIFT_FRONTEND"),
-			enable_rss: parse("REDLIB_ENABLE_RSS"),
-			full_url: parse("REDLIB_FULL_URL"),
-			default_remove_default_feeds: parse("REDLIB_DEFAULT_REMOVE_DEFAULT_FEEDS"),
-		}
-	}
-}
-fn get_setting_from_config(name: &str, config: &Config) -> Option<String> {
-	match name {
-		"REDLIB_SFW_ONLY" => config.sfw_only.clone(),
-		"REDLIB_DEFAULT_THEME" => config.default_theme.clone(),
-		"REDLIB_DEFAULT_FRONT_PAGE" => config.default_front_page.clone(),
-		"REDLIB_DEFAULT_LAYOUT" => config.default_layout.clone(),
-		"REDLIB_DEFAULT_COMMENT_SORT" => config.default_comment_sort.clone(),
-		"REDLIB_DEFAULT_POST_SORT" => config.default_post_sort.clone(),
-		"REDLIB_DEFAULT_BLUR_SPOILER" => config.default_blur_spoiler.clone(),
-		"REDLIB_DEFAULT_SHOW_NSFW" => config.default_show_nsfw.clone(),
-		"REDLIB_DEFAULT_BLUR_NSFW" => config.default_blur_nsfw.clone(),
-		"REDLIB_DEFAULT_USE_HLS" => config.default_use_hls.clone(),
-		"REDLIB_DEFAULT_HIDE_HLS_NOTIFICATION" => config.default_hide_hls_notification.clone(),
-		"REDLIB_DEFAULT_WIDE" => config.default_wide.clone(),
-		"REDLIB_DEFAULT_HIDE_AWARDS" => config.default_hide_awards.clone(),
-		"REDLIB_DEFAULT_HIDE_SIDEBAR_AND_SUMMARY" => config.default_hide_sidebar_and_summary.clone(),
-		"REDLIB_DEFAULT_HIDE_SCORE" => config.default_hide_score.clone(),
-		"REDLIB_DEFAULT_SUBSCRIPTIONS" => config.default_subscriptions.clone(),
-		"REDLIB_DEFAULT_FILTERS" => config.default_filters.clone(),
-		"REDLIB_DEFAULT_DISABLE_VISIT_REDDIT_CONFIRMATION" => config.default_disable_visit_reddit_confirmation.clone(),
-		"REDLIB_BANNER" => config.banner.clone(),
-		"REDLIB_ROBOTS_DISABLE_INDEXING" => config.robots_disable_indexing.clone(),
-		"REDLIB_PUSHSHIFT_FRONTEND" => config.pushshift.clone(),
-		"REDLIB_ENABLE_RSS" => config.enable_rss.clone(),
-		"REDLIB_FULL_URL" => config.full_url.clone(),
-		"REDLIB_DEFAULT_REMOVE_DEFAULT_FEEDS" => config.default_remove_default_feeds.clone(),
-		_ => None,
-	}
-}
-pub fn get_setting(name: &str) -> Option<String> {
-	get_setting_from_config(name, &CONFIG)
-}
-#[cfg(test)]
-use {sealed_test::prelude::*, std::fs::write};
-#[test]
-fn test_deserialize() {
-	let result = toml::from_str::<Config>("");
-	assert!(result.is_ok(), "Error: {}", result.unwrap_err());
-}
-#[test]
-#[sealed_test(env = [("REDLIB_SFW_ONLY", "on")])]
-fn test_env_var() {
-	assert!(crate::utils::sfw_only())
-}
-#[test]
-#[sealed_test]
-fn test_config() {
-	let config_to_write = r#"REDLIB_DEFAULT_COMMENT_SORT = "best""#;
-	write("redlib.toml", config_to_write).unwrap();
-	assert_eq!(get_setting("REDLIB_DEFAULT_COMMENT_SORT"), Some("best".into()));
-}
-#[test]
-#[sealed_test]
-fn test_config_legacy() {
-	let config_to_write = r#"LIBREDDIT_DEFAULT_COMMENT_SORT = "best""#;
-	write("libreddit.toml", config_to_write).unwrap();
-	assert_eq!(get_setting("REDLIB_DEFAULT_COMMENT_SORT"), Some("best".into()));
-}
-#[test]
-#[sealed_test(env = [("LIBREDDIT_SFW_ONLY", "on")])]
-fn test_env_var_legacy() {
-	assert!(crate::utils::sfw_only())
-}
-#[test]
-#[sealed_test(env = [("REDLIB_DEFAULT_COMMENT_SORT", "top")])]
-fn test_env_config_precedence() {
-	let config_to_write = r#"REDLIB_DEFAULT_COMMENT_SORT = "best""#;
-	write("redlib.toml", config_to_write).unwrap();
-	assert_eq!(get_setting("REDLIB_DEFAULT_COMMENT_SORT"), Some("top".into()))
-}
-#[test]
-#[sealed_test(env = [("REDLIB_DEFAULT_COMMENT_SORT", "top")])]
-fn test_alt_env_config_precedence() {
-	let config_to_write = r#"REDLIB_DEFAULT_COMMENT_SORT = "best""#;
-	write("redlib.toml", config_to_write).unwrap();
-	assert_eq!(get_setting("REDLIB_DEFAULT_COMMENT_SORT"), Some("top".into()))
-}
-#[test]
-#[sealed_test(env = [("REDLIB_DEFAULT_SUBSCRIPTIONS", "news+bestof")])]
-fn test_default_subscriptions() {
-	assert_eq!(get_setting("REDLIB_DEFAULT_SUBSCRIPTIONS"), Some("news+bestof".into()));
-}
-#[test]
-#[sealed_test(env = [("REDLIB_DEFAULT_FILTERS", "news+bestof")])]
-fn test_default_filters() {
-	assert_eq!(get_setting("REDLIB_DEFAULT_FILTERS"), Some("news+bestof".into()));
-}
-#[test]
-#[sealed_test]
-fn test_pushshift() {
-	let config_to_write = r#"REDLIB_PUSHSHIFT_FRONTEND = "https://api.pushshift.io""#;
-	write("redlib.toml", config_to_write).unwrap();
-	assert!(get_setting("REDLIB_PUSHSHIFT_FRONTEND").is_some());
-	assert_eq!(get_setting("REDLIB_PUSHSHIFT_FRONTEND"), Some("https://api.pushshift.io".into()));
-}

--- a/src/duplicates.rs
+++ b//dev/null
@@ -1,128 +0,0 @@
-use crate::client::json;
-use crate::server::RequestExt;
-use crate::subreddit::{can_access_quarantine, quarantine};
-use crate::utils::{error, filter_posts, get_filters, nsfw_landing, parse_post, template, Post, Preferences};
-use hyper::{Body, Request, Response};
-use rinja::Template;
-use serde_json::Value;
-use std::borrow::ToOwned;
-use std::collections::HashSet;
-use std::vec::Vec;
-struct DuplicatesParams {
-	before: String,
-	after: String,
-	sort: String,
-}
-#[derive(Template)]
-#[template(path = "duplicates.html")]
-struct DuplicatesTemplate {
-	params: DuplicatesParams,
-	post: Post,
-	duplicates: Vec<Post>,
-	prefs: Preferences,
-	url: String,
-	num_posts_filtered: u64,
-	all_posts_filtered: bool,
-}
-pub async fn item(req: Request<Body>) -> Result<Response<Body>, String> {
-	let path: String = format!("{}.json?{}&raw_json=1", req.uri().path(), req.uri().query().unwrap_or_default());
-	let sub = req.param("sub").unwrap_or_default();
-	let quarantined = can_access_quarantine(&req, &sub);
-	#[cfg(debug_assertions)]
-	req.param("id").unwrap_or_default();
-	match json(path, quarantined).await {
-		Ok(response) => {
-			let post = parse_post(&response[0]["data"]["children"][0]).await;
-			let req_url = req.uri().to_string();
-			if post.nsfw && crate::utils::should_be_nsfw_gated(&req, &req_url) {
-				return Ok(nsfw_landing(req, req_url).await.unwrap_or_default());
-			}
-			let filters = get_filters(&req);
-			let (duplicates, num_posts_filtered, all_posts_filtered) = parse_duplicates(&response[1], &filters).await;
-			let mut before: String = String::new();
-			let mut after: String = String::new();
-			let mut sort: String = String::new();
-			let l = duplicates.len();
-			if l > 0 {
-				let mut have_before: bool = false;
-				let mut have_after: bool = false;
-				let query_str = req.uri().query().unwrap_or_default().to_string();
-				if !query_str.is_empty() {
-					for param in query_str.split('&') {
-						let kv: Vec<&str> = param.split('=').collect();
-						if kv.len() < 2 {
-							continue;
-						}
-						let key: &str = kv[0];
-						match key {
-							"before" => have_before = true,
-							"after" => have_after = true,
-							"sort" => {
-								let val: &str = kv[1];
-								match val {
-									"new" | "num_comments" => sort = val.to_string(),
-									_ => {}
-								}
-							}
-							_ => {}
-						}
-					}
-				}
-				if have_after {
-					"t3_".clone_into(&mut before);
-					before.push_str(&duplicates[0].id);
-				}
-				if have_before {
-					"t3_".clone_into(&mut after);
-					after.push_str(&duplicates[l - 1].id);
-					let new_path: String = format!(
-						"{}.json?before=t3_{}&sort={}&limit=1&raw_json=1",
-						req.uri().path(),
-						&duplicates[0].id,
-						if sort.is_empty() { "num_comments".to_string() } else { sort.clone() }
-					);
-					match json(new_path, true).await {
-						Ok(response) => {
-							if !response[1]["data"]["children"].as_array().unwrap_or(&Vec::new()).is_empty() {
-								"t3_".clone_into(&mut before);
-								before.push_str(&duplicates[0].id);
-							}
-						}
-						Err(msg) => {
-							return error(req, &msg).await;
-						}
-					}
-				} else {
-					after = response[1]["data"]["after"].as_str().unwrap_or_default().to_string();
-				}
-			}
-			Ok(template(&DuplicatesTemplate {
-				params: DuplicatesParams { before, after, sort },
-				post,
-				duplicates,
-				prefs: Preferences::new(&req),
-				url: req_url,
-				num_posts_filtered,
-				all_posts_filtered,
-			}))
-		}
-		Err(msg) => {
-			if msg == "quarantined" || msg == "gated" {
-				let sub = req.param("sub").unwrap_or_default();
-				Ok(quarantine(&req, sub, &msg))
-			} else {
-				error(req, &msg).await
-			}
-		}
-	}
-}
-async fn parse_duplicates(json: &Value, filters: &HashSet<String>) -> (Vec<Post>, u64, bool) {
-	let post_duplicates: &Vec<Value> = &json["data"]["children"].as_array().map_or(Vec::new(), ToOwned::to_owned);
-	let mut duplicates: Vec<Post> = Vec::new();
-	for val in post_duplicates {
-		let post: Post = parse_post(val).await;
-		duplicates.push(post);
-	}
-	let (num_posts_filtered, all_posts_filtered) = filter_posts(&mut duplicates, filters);
-	(duplicates, num_posts_filtered, all_posts_filtered)
-}

--- a/src/instance_info.rs
+++ b//dev/null
@@ -1,219 +0,0 @@
-use crate::{
-	config::{Config, CONFIG},
-	server::RequestExt,
-	utils::{ErrorTemplate, Preferences},
-};
-use build_html::{Container, Html, HtmlContainer, Table};
-use hyper::{http::Error, Body, Request, Response};
-use once_cell::sync::Lazy;
-use rinja::Template;
-use serde::{Deserialize, Serialize};
-use time::OffsetDateTime;
-pub static INSTANCE_INFO: Lazy<InstanceInfo> = Lazy::new(InstanceInfo::new);
-pub async fn instance_info(req: Request<Body>) -> Result<Response<Body>, String> {
-	let extension = req.param("extension").unwrap_or_default();
-	let response = match extension.as_str() {
-		"yaml" | "yml" => info_yaml(),
-		"txt" => info_txt(),
-		"json" => info_json(),
-		"html" | "" => info_html(&req),
-		_ => {
-			let error = ErrorTemplate {
-				msg: "Error: Invalid info extension".into(),
-				prefs: Preferences::new(&req),
-				url: req.uri().to_string(),
-			}
-			.render()
-			.unwrap();
-			Response::builder().status(404).header("content-type", "text/html; charset=utf-8").body(error.into())
-		}
-	};
-	response.map_err(|err| format!("{err}"))
-}
-fn info_json() -> Result<Response<Body>, Error> {
-	if let Ok(body) = serde_json::to_string(&*INSTANCE_INFO) {
-		Response::builder().status(200).header("content-type", "application/json").body(body.into())
-	} else {
-		Response::builder()
-			.status(500)
-			.header("content-type", "text/plain")
-			.body(Body::from("Error serializing JSON"))
-	}
-}
-fn info_yaml() -> Result<Response<Body>, Error> {
-	if let Ok(body) = serde_yaml::to_string(&*INSTANCE_INFO) {
-		Response::builder().status(200).header("content-type", "application/yaml").body(body.into())
-	} else {
-		Response::builder()
-			.status(500)
-			.header("content-type", "text/plain")
-			.body(Body::from("Error serializing YAML."))
-	}
-}
-fn info_txt() -> Result<Response<Body>, Error> {
-	Response::builder()
-		.status(200)
-		.header("content-type", "text/plain")
-		.body(Body::from(INSTANCE_INFO.to_string(&StringType::Raw)))
-}
-fn info_html(req: &Request<Body>) -> Result<Response<Body>, Error> {
-	let message = MessageTemplate {
-		title: String::from("Instance information"),
-		body: INSTANCE_INFO.to_string(&StringType::Html),
-		prefs: Preferences::new(req),
-		url: req.uri().to_string(),
-	}
-	.render()
-	.unwrap();
-	Response::builder().status(200).header("content-type", "text/html; charset=utf8").body(Body::from(message))
-}
-#[derive(Serialize, Deserialize, Default)]
-pub struct InstanceInfo {
-	package_name: String,
-	crate_version: String,
-	pub git_commit: String,
-	deploy_date: String,
-	compile_mode: String,
-	deploy_unix_ts: i64,
-	config: Config,
-}
-impl InstanceInfo {
-	pub fn new() -> Self {
-		Self {
-			package_name: env!("CARGO_PKG_NAME").to_string(),
-			crate_version: env!("CARGO_PKG_VERSION").to_string(),
-			git_commit: env!("GIT_HASH").to_string(),
-			deploy_date: OffsetDateTime::now_local().unwrap_or_else(|_| OffsetDateTime::now_utc()).to_string(),
-			#[cfg(debug_assertions)]
-			compile_mode: "Debug".into(),
-			#[cfg(not(debug_assertions))]
-			compile_mode: "Release".into(),
-			deploy_unix_ts: OffsetDateTime::now_local().unwrap_or_else(|_| OffsetDateTime::now_utc()).unix_timestamp(),
-			config: CONFIG.clone(),
-		}
-	}
-	fn to_table(&self) -> String {
-		let mut container = Container::default();
-		let convert = |o: &Option<String>| -> String { o.clone().unwrap_or_else(|| "<span class=\"unset\"><i>Unset</i></span>".to_owned()) };
-		if let Some(banner) = &self.config.banner {
-			container.add_header(3, "Instance banner");
-			container.add_raw("<br />");
-			container.add_paragraph(banner);
-			container.add_raw("<br />");
-		}
-		container.add_table(
-			Table::from([
-				["Package name", &self.package_name],
-				["Crate version", &self.crate_version],
-				["Git commit", &self.git_commit],
-				["Deploy date", &self.deploy_date],
-				["Deploy timestamp", &self.deploy_unix_ts.to_string()],
-				["Compile mode", &self.compile_mode],
-				["SFW only", &convert(&self.config.sfw_only)],
-				["Pushshift frontend", &convert(&self.config.pushshift)],
-				["RSS enabled", &convert(&self.config.enable_rss)],
-				["Full URL", &convert(&self.config.full_url)],
-				["Remove default feeds", &convert(&self.config.default_remove_default_feeds)],
-			])
-			.with_header_row(["Settings"]),
-		);
-		container.add_raw("<br />");
-		container.add_table(
-			Table::from([
-				["Hide awards", &convert(&self.config.default_hide_awards)],
-				["Hide score", &convert(&self.config.default_hide_score)],
-				["Theme", &convert(&self.config.default_theme)],
-				["Front page", &convert(&self.config.default_front_page)],
-				["Layout", &convert(&self.config.default_layout)],
-				["Wide", &convert(&self.config.default_wide)],
-				["Comment sort", &convert(&self.config.default_comment_sort)],
-				["Post sort", &convert(&self.config.default_post_sort)],
-				["Blur Spoiler", &convert(&self.config.default_blur_spoiler)],
-				["Show NSFW", &convert(&self.config.default_show_nsfw)],
-				["Blur NSFW", &convert(&self.config.default_blur_nsfw)],
-				["Use HLS", &convert(&self.config.default_use_hls)],
-				["Hide HLS notification", &convert(&self.config.default_hide_hls_notification)],
-				["Subscriptions", &convert(&self.config.default_subscriptions)],
-				["Filters", &convert(&self.config.default_filters)],
-			])
-			.with_header_row(["Default preferences"]),
-		);
-		container.to_html_string().replace("<th>", "<th colspan=\"2\">")
-	}
-	fn to_string(&self, string_type: &StringType) -> String {
-		match string_type {
-			StringType::Raw => {
-				format!(
-					"Package name: {}\n
-				Crate version: {}\n
-                Git commit: {}\n
-                Deploy date: {}\n
-                Deploy timestamp: {}\n
-                Compile mode: {}\n
-				SFW only: {:?}\n
-				Pushshift frontend: {:?}\n
-				RSS enabled: {:?}\n
-				Full URL: {:?}\n
-				Remove default feeds: {:?}\n
-                Config:\n
-                    Banner: {:?}\n
-                    Hide awards: {:?}\n
-                    Hide score: {:?}\n
-                    Default theme: {:?}\n
-                    Default front page: {:?}\n
-                    Default layout: {:?}\n
-                    Default wide: {:?}\n
-                    Default comment sort: {:?}\n
-                    Default post sort: {:?}\n
-					Default blur Spoiler: {:?}\n
-                    Default show NSFW: {:?}\n
-                    Default blur NSFW: {:?}\n
-                    Default use HLS: {:?}\n
-                    Default hide HLS notification: {:?}\n
-                    Default subscriptions: {:?}\n
-                    Default filters: {:?}\n",
-					self.package_name,
-					self.crate_version,
-					self.git_commit,
-					self.deploy_date,
-					self.deploy_unix_ts,
-					self.compile_mode,
-					self.config.sfw_only,
-					self.config.enable_rss,
-					self.config.full_url,
-					self.config.default_remove_default_feeds,
-					self.config.pushshift,
-					self.config.banner,
-					self.config.default_hide_awards,
-					self.config.default_hide_score,
-					self.config.default_theme,
-					self.config.default_front_page,
-					self.config.default_layout,
-					self.config.default_wide,
-					self.config.default_comment_sort,
-					self.config.default_post_sort,
-					self.config.default_blur_spoiler,
-					self.config.default_show_nsfw,
-					self.config.default_blur_nsfw,
-					self.config.default_use_hls,
-					self.config.default_hide_hls_notification,
-					self.config.default_subscriptions,
-					self.config.default_filters,
-				)
-			}
-			StringType::Html => self.to_table(),
-		}
-	}
-}
-enum StringType {
-	Raw,
-	Html,
-}
-#[derive(Template)]
-#[template(path = "message.html")]
-struct MessageTemplate {
-	title: String,
-	body: String,
-	prefs: Preferences,
-	url: String,
-}

--- a/src/lib.rs
+++ b//dev/null
@@ -1,13 +0,0 @@
-pub mod client;
-pub mod config;
-pub mod duplicates;
-pub mod instance_info;
-pub mod oauth;
-pub mod oauth_resources;
-pub mod post;
-pub mod search;
-pub mod server;
-pub mod settings;
-pub mod subreddit;
-pub mod user;
-pub mod utils;

--- a/src/main.rs
+++ b//dev/null
@@ -1,363 +0,0 @@
-#![forbid(unsafe_code)]
-#![allow(clippy::cmp_owned)]
-use cached::proc_macro::cached;
-use clap::{Arg, ArgAction, Command};
-use std::str::FromStr;
-use futures_lite::FutureExt;
-use hyper::Uri;
-use hyper::{header::HeaderValue, Body, Request, Response};
-use log::{info, warn};
-use once_cell::sync::Lazy;
-use redlib::client::{canonical_path, proxy, rate_limit_check, CLIENT};
-use redlib::server::{self, RequestExt};
-use redlib::utils::{error, redirect, ThemeAssets};
-use redlib::{config, duplicates, headers, instance_info, post, search, settings, subreddit, user};
-use redlib::client::OAUTH_CLIENT;
-async fn pwa_logo() -> Result<Response<Body>, String> {
-	Ok(
-		Response::builder()
-			.status(200)
-			.header("content-type", "image/png")
-			.body(include_bytes!("../static/logo.png").as_ref().into())
-			.unwrap_or_default(),
-	)
-}
-async fn iphone_logo() -> Result<Response<Body>, String> {
-	Ok(
-		Response::builder()
-			.status(200)
-			.header("content-type", "image/png")
-			.body(include_bytes!("../static/apple-touch-icon.png").as_ref().into())
-			.unwrap_or_default(),
-	)
-}
-async fn favicon() -> Result<Response<Body>, String> {
-	Ok(
-		Response::builder()
-			.status(200)
-			.header("content-type", "image/vnd.microsoft.icon")
-			.header("Cache-Control", "public, max-age=1209600, s-maxage=86400")
-			.body(include_bytes!("../static/favicon.ico").as_ref().into())
-			.unwrap_or_default(),
-	)
-}
-async fn font() -> Result<Response<Body>, String> {
-	Ok(
-		Response::builder()
-			.status(200)
-			.header("content-type", "font/woff2")
-			.header("Cache-Control", "public, max-age=1209600, s-maxage=86400")
-			.body(include_bytes!("../static/Inter.var.woff2").as_ref().into())
-			.unwrap_or_default(),
-	)
-}
-async fn opensearch() -> Result<Response<Body>, String> {
-	Ok(
-		Response::builder()
-			.status(200)
-			.header("content-type", "application/opensearchdescription+xml")
-			.header("Cache-Control", "public, max-age=1209600, s-maxage=86400")
-			.body(include_bytes!("../static/opensearch.xml").as_ref().into())
-			.unwrap_or_default(),
-	)
-}
-async fn resource(body: &str, content_type: &str, cache: bool) -> Result<Response<Body>, String> {
-	let mut res = Response::builder()
-		.status(200)
-		.header("content-type", content_type)
-		.body(body.to_string().into())
-		.unwrap_or_default();
-	if cache {
-		if let Ok(val) = HeaderValue::from_str("public, max-age=1209600, s-maxage=86400") {
-			res.headers_mut().insert("Cache-Control", val);
-		}
-	}
-	Ok(res)
-}
-async fn style() -> Result<Response<Body>, String> {
-	let mut res = include_str!("../static/style.css").to_string();
-	for file in ThemeAssets::iter() {
-		res.push('\n');
-		let theme = ThemeAssets::get(file.as_ref()).unwrap();
-		res.push_str(std::str::from_utf8(theme.data.as_ref()).unwrap());
-	}
-	Ok(
-		Response::builder()
-			.status(200)
-			.header("content-type", "text/css")
-			.header("Cache-Control", "public, max-age=1209600, s-maxage=86400")
-			.body(res.to_string().into())
-			.unwrap_or_default(),
-	)
-}
-#[tokio::main]
-async fn main() {
-	_ = dotenvy::dotenv();
-	pretty_env_logger::init();
-	let matches = Command::new("Redlib")
-		.version(env!("CARGO_PKG_VERSION"))
-		.about("Private front-end for Reddit written in Rust ")
-		.arg(Arg::new("ipv4-only").short('4').long("ipv4-only").help("Listen on IPv4 only").num_args(0))
-		.arg(Arg::new("ipv6-only").short('6').long("ipv6-only").help("Listen on IPv6 only").num_args(0))
-		.arg(
-			Arg::new("redirect-https")
-				.short('r')
-				.long("redirect-https")
-				.help("Redirect all HTTP requests to HTTPS (no longer functional)")
-				.num_args(0),
-		)
-		.arg(
-			Arg::new("address")
-				.short('a')
-				.long("address")
-				.value_name("ADDRESS")
-				.help("Sets address to listen on")
-				.default_value("[::]")
-				.num_args(1),
-		)
-		.arg(
-			Arg::new("port")
-				.short('p')
-				.long("port")
-				.value_name("PORT")
-				.env("PORT")
-				.help("Port to listen on")
-				.default_value("8080")
-				.action(ArgAction::Set)
-				.num_args(1),
-		)
-		.arg(
-			Arg::new("hsts")
-				.short('H')
-				.long("hsts")
-				.value_name("EXPIRE_TIME")
-				.help("HSTS header to tell browsers that this site should only be accessed over HTTPS")
-				.default_value("604800")
-				.num_args(1),
-		)
-		.get_matches();
-	match rate_limit_check().await {
-		Ok(()) => {
-			info!("[âœ…] Rate limit check passed");
-		}
-		Err(e) => {
-			let mut message = format!("Rate limit check failed: {}", e);
-			message += "\nThis may cause issues with the rate limit.";
-			message += "\nPlease report this error with the above information.";
-			message += "\nhttps://github.com/redlib-org/redlib/issues/new?assignees=sigaloid&labels=bug&title=%F0%9F%90%9B+Bug+Report%3A+Rate+limit+mismatch";
-			warn!("{}", message);
-			eprintln!("{}", message);
-		}
-	}
-	let address = matches.get_one::<String>("address").unwrap();
-	let port = matches.get_one::<String>("port").unwrap();
-	let hsts = matches.get_one("hsts").map(|m: &String| m.as_str());
-	let ipv4_only = std::env::var("IPV4_ONLY").is_ok() || matches.get_flag("ipv4-only");
-	let ipv6_only = std::env::var("IPV6_ONLY").is_ok() || matches.get_flag("ipv6-only");
-	let listener = if ipv4_only {
-		format!("0.0.0.0:{}", port)
-	} else if ipv6_only {
-		format!("[::]:{}", port)
-	} else {
-		[address, ":", port].concat()
-	};
-	println!("Starting Redlib...");
-	let mut app = server::Server::new();
-	info!("Evaluating config.");
-	Lazy::force(&config::CONFIG);
-	info!("Evaluating instance info.");
-	Lazy::force(&instance_info::INSTANCE_INFO);
-	info!("Creating OAUTH client.");
-	Lazy::force(&OAUTH_CLIENT);
-	app.default_headers = headers! {
-		"Referrer-Policy" => "no-referrer",
-		"X-Content-Type-Options" => "nosniff",
-		"X-Frame-Options" => "DENY",
-		"Content-Security-Policy" => "default-src 'none'; font-src 'self'; script-src 'self' blob:; manifest-src 'self'; media-src 'self' data: blob: about:; style-src 'self' 'unsafe-inline'; base-uri 'none'; img-src 'self' data:; form-action 'self'; frame-ancestors 'none'; connect-src 'self'; worker-src blob:;"
-	};
-	if let Some(expire_time) = hsts {
-		if let Ok(val) = HeaderValue::from_str(&format!("max-age={expire_time}")) {
-			app.default_headers.insert("Strict-Transport-Security", val);
-		}
-	}
-	app.at("/style.css").get(|_| style().boxed());
-	app
-		.at("/manifest.json")
-		.get(|_| resource(include_str!("../static/manifest.json"), "application/json", false).boxed());
-	app.at("/robots.txt").get(|_| {
-		resource(
-			if match config::get_setting("REDLIB_ROBOTS_DISABLE_INDEXING") {
-				Some(val) => val == "on",
-				None => false,
-			} {
-				"User-agent: *\nDisallow: /"
-			} else {
-				"User-agent: *\nDisallow: /u/\nDisallow: /user/"
-			},
-			"text/plain",
-			true,
-		)
-		.boxed()
-	});
-	app.at("/favicon.ico").get(|_| favicon().boxed());
-	app.at("/logo.png").get(|_| pwa_logo().boxed());
-	app.at("/Inter.var.woff2").get(|_| font().boxed());
-	app.at("/touch-icon-iphone.png").get(|_| iphone_logo().boxed());
-	app.at("/apple-touch-icon.png").get(|_| iphone_logo().boxed());
-	app.at("/opensearch.xml").get(|_| opensearch().boxed());
-	app
-		.at("/playHLSVideo.js")
-		.get(|_| resource(include_str!("../static/playHLSVideo.js"), "text/javascript", false).boxed());
-	app
-		.at("/hls.min.js")
-		.get(|_| resource(include_str!("../static/hls.min.js"), "text/javascript", false).boxed());
-	app
-		.at("/highlighted.js")
-		.get(|_| resource(include_str!("../static/highlighted.js"), "text/javascript", false).boxed());
-	app
-		.at("/check_update.js")
-		.get(|_| resource(include_str!("../static/check_update.js"), "text/javascript", false).boxed());
-	app.at("/copy.js").get(|_| resource(include_str!("../static/copy.js"), "text/javascript", false).boxed());
-	app.at("/commits.atom").get(|_| async move { proxy_commit_info().await }.boxed());
-	app.at("/instances.json").get(|_| async move { proxy_instances().await }.boxed());
-	app.at("/vid/:id/:size").get(|r| proxy(r, "https://v.redd.it/{id}/DASH_{size}").boxed());
-	app.at("/hls/:id/*path").get(|r| proxy(r, "https://v.redd.it/{id}/{path}").boxed());
-	app.at("/img/*path").get(|r| proxy(r, "https://i.redd.it/{path}").boxed());
-	app.at("/thumb/:point/:id").get(|r| proxy(r, "https://{point}.thumbs.redditmedia.com/{id}").boxed());
-	app.at("/emoji/:id/:name").get(|r| proxy(r, "https://emoji.redditmedia.com/{id}/{name}").boxed());
-	app
-		.at("/emote/:subreddit_id/:filename")
-		.get(|r| proxy(r, "https://reddit-econ-prod-assets-permanent.s3.amazonaws.com/asset-manager/{subreddit_id}/{filename}").boxed());
-	app
-		.at("/preview/:loc/award_images/:fullname/:id")
-		.get(|r| proxy(r, "https://{loc}view.redd.it/award_images/{fullname}/{id}").boxed());
-	app.at("/preview/:loc/:id").get(|r| proxy(r, "https://{loc}view.redd.it/{id}").boxed());
-	app.at("/style/*path").get(|r| proxy(r, "https://styles.redditmedia.com/{path}").boxed());
-	app.at("/static/*path").get(|r| proxy(r, "https://www.redditstatic.com/{path}").boxed());
-	app
-		.at("/u/:name")
-		.get(|r| async move { Ok(redirect(&format!("/user/{}", r.param("name").unwrap_or_default()))) }.boxed());
-	app.at("/u/:name/comments/:id/:title").get(|r| post::item(r).boxed());
-	app.at("/u/:name/comments/:id/:title/:comment_id").get(|r| post::item(r).boxed());
-	app.at("/user/[deleted]").get(|req| error(req, "User has deleted their account").boxed());
-	app.at("/user/:name.rss").get(|r| user::rss(r).boxed());
-	app.at("/user/:name").get(|r| user::profile(r).boxed());
-	app.at("/user/:name/:listing").get(|r| user::profile(r).boxed());
-	app.at("/user/:name/comments/:id").get(|r| post::item(r).boxed());
-	app.at("/user/:name/comments/:id/:title").get(|r| post::item(r).boxed());
-	app.at("/user/:name/comments/:id/:title/:comment_id").get(|r| post::item(r).boxed());
-	app.at("/settings").get(|r| settings::get(r).boxed()).post(|r| settings::set(r).boxed());
-	app.at("/settings/restore").get(|r| settings::restore(r).boxed());
-	app.at("/settings/encoded-restore").post(|r| settings::encoded_restore(r).boxed());
-	app.at("/settings/update").get(|r| settings::update(r).boxed());
-	app.at("/r/:sub.rss").get(|r| subreddit::rss(r).boxed());
-	app
-		.at("/r/:sub")
-		.get(|r| subreddit::community(r).boxed())
-		.post(|r| subreddit::add_quarantine_exception(r).boxed());
-	app
-		.at("/r/u_:name")
-		.get(|r| async move { Ok(redirect(&format!("/user/{}", r.param("name").unwrap_or_default()))) }.boxed());
-	app.at("/r/:sub/subscribe").post(|r| subreddit::subscriptions_filters(r).boxed());
-	app.at("/r/:sub/unsubscribe").post(|r| subreddit::subscriptions_filters(r).boxed());
-	app.at("/r/:sub/filter").post(|r| subreddit::subscriptions_filters(r).boxed());
-	app.at("/r/:sub/unfilter").post(|r| subreddit::subscriptions_filters(r).boxed());
-	app.at("/r/:sub/comments/:id").get(|r| post::item(r).boxed());
-	app.at("/r/:sub/comments/:id/:title").get(|r| post::item(r).boxed());
-	app.at("/r/:sub/comments/:id/:title/:comment_id").get(|r| post::item(r).boxed());
-	app.at("/comments/:id").get(|r| post::item(r).boxed());
-	app.at("/comments/:id/comments").get(|r| post::item(r).boxed());
-	app.at("/comments/:id/comments/:comment_id").get(|r| post::item(r).boxed());
-	app.at("/comments/:id/:title").get(|r| post::item(r).boxed());
-	app.at("/comments/:id/:title/:comment_id").get(|r| post::item(r).boxed());
-	app.at("/r/:sub/duplicates/:id").get(|r| duplicates::item(r).boxed());
-	app.at("/r/:sub/duplicates/:id/:title").get(|r| duplicates::item(r).boxed());
-	app.at("/duplicates/:id").get(|r| duplicates::item(r).boxed());
-	app.at("/duplicates/:id/:title").get(|r| duplicates::item(r).boxed());
-	app.at("/r/:sub/search").get(|r| search::find(r).boxed());
-	app
-		.at("/r/:sub/w")
-		.get(|r| async move { Ok(redirect(&format!("/r/{}/wiki", r.param("sub").unwrap_or_default()))) }.boxed());
-	app
-		.at("/r/:sub/w/*page")
-		.get(|r| async move { Ok(redirect(&format!("/r/{}/wiki/{}", r.param("sub").unwrap_or_default(), r.param("wiki").unwrap_or_default()))) }.boxed());
-	app.at("/r/:sub/wiki").get(|r| subreddit::wiki(r).boxed());
-	app.at("/r/:sub/wiki/*page").get(|r| subreddit::wiki(r).boxed());
-	app.at("/r/:sub/about/sidebar").get(|r| subreddit::sidebar(r).boxed());
-	app.at("/r/:sub/:sort").get(|r| subreddit::community(r).boxed());
-	app.at("/").get(|r| subreddit::community(r).boxed());
-	app.at("/w").get(|_| async { Ok(redirect("/wiki")) }.boxed());
-	app
-		.at("/w/*page")
-		.get(|r| async move { Ok(redirect(&format!("/wiki/{}", r.param("page").unwrap_or_default()))) }.boxed());
-	app.at("/wiki").get(|r| subreddit::wiki(r).boxed());
-	app.at("/wiki/*page").get(|r| subreddit::wiki(r).boxed());
-	app.at("/search").get(|r| search::find(r).boxed());
-	app.at("/about").get(|req| error(req, "About pages aren't added yet").boxed());
-	app.at("/info").get(|r| instance_info::instance_info(r).boxed());
-	app.at("/info.:extension").get(|r| instance_info::instance_info(r).boxed());
-	app.at("/r/:sub/s/:id").get(|req: Request<Body>| {
-		Box::pin(async move {
-			let sub = req.param("sub").unwrap_or_default();
-			match req.param("id").as_deref() {
-				Some(id) if (8..12).contains(&id.len()) => match canonical_path(format!("/r/{sub}/s/{id}"), 3).await {
-					Ok(Some(path)) => Ok(redirect(&path)),
-					Ok(None) => error(req, "Post ID is invalid. It may point to a post on a community that has been banned.").await,
-					Err(e) => error(req, &e).await,
-				},
-				_ => error(req, "Nothing here").await,
-			}
-		})
-	});
-	app.at("/:id").get(|req: Request<Body>| {
-		Box::pin(async move {
-			match req.param("id").as_deref() {
-				Some("best" | "hot" | "new" | "top" | "rising" | "controversial") => subreddit::community(req).await,
-				Some(id) if (5..8).contains(&id.len()) => match canonical_path(format!("/comments/{id}"), 3).await {
-					Ok(path_opt) => match path_opt {
-						Some(path) => Ok(redirect(&path)),
-						None => error(req, "Post ID is invalid. It may point to a post on a community that has been banned.").await,
-					},
-					Err(e) => error(req, &e).await,
-				},
-				_ => error(req, "Nothing here").await,
-			}
-		})
-	});
-	app.at("/*").get(|req| error(req, "Nothing here").boxed());
-	println!("Running Redlib v{} on {listener}!", env!("CARGO_PKG_VERSION"));
-	let server = app.listen(&listener);
-	if let Err(e) = server.await {
-		eprintln!("Server error: {e}");
-	}
-}
-pub async fn proxy_commit_info() -> Result<Response<Body>, String> {
-	Ok(
-		Response::builder()
-			.status(200)
-			.header("content-type", "application/atom+xml")
-			.body(Body::from(fetch_commit_info().await))
-			.unwrap_or_default(),
-	)
-}
-#[cached(time = 600)]
-async fn fetch_commit_info() -> String {
-	let uri = Uri::from_str("https://github.com/redlib-org/redlib/commits/main.atom").expect("Invalid URI");
-	let resp: Body = CLIENT.get(uri).await.expect("Failed to request GitHub").into_body();
-	hyper::body::to_bytes(resp).await.expect("Failed to read body").iter().copied().map(|x| x as char).collect()
-}
-pub async fn proxy_instances() -> Result<Response<Body>, String> {
-	Ok(
-		Response::builder()
-			.status(200)
-			.header("content-type", "application/json")
-			.body(Body::from(fetch_instances().await))
-			.unwrap_or_default(),
-	)
-}
-#[cached(time = 600)]
-async fn fetch_instances() -> String {
-	let uri = Uri::from_str("https://raw.githubusercontent.com/redlib-org/redlib-instances/refs/heads/main/instances.json").expect("Invalid URI");
-	let resp: Body = CLIENT.get(uri).await.expect("Failed to request GitHub").into_body();
-	hyper::body::to_bytes(resp).await.expect("Failed to read body").iter().copied().map(|x| x as char).collect()
-}

--- a/src/oauth.rs
+++ b//dev/null
@@ -1,174 +0,0 @@
-use std::{collections::HashMap, sync::atomic::Ordering, time::Duration};
-use crate::{
-	client::{CLIENT, OAUTH_CLIENT, OAUTH_IS_ROLLING_OVER, OAUTH_RATELIMIT_REMAINING},
-	oauth_resources::ANDROID_APP_VERSION_LIST,
-};
-use base64::{engine::general_purpose, Engine as _};
-use hyper::{client, Body, Method, Request};
-use log::{error, info, trace};
-use serde_json::json;
-use tegen::tegen::TextGenerator;
-use tokio::time::{error::Elapsed, timeout};
-const REDDIT_ANDROID_OAUTH_CLIENT_ID: &str = "ohXpoqrZYub1kg";
-const AUTH_ENDPOINT: &str = "https://www.reddit.com";
-#[derive(Debug, Clone, Default)]
-pub struct Oauth {
-	pub(crate) initial_headers: HashMap<String, String>,
-	pub(crate) headers_map: HashMap<String, String>,
-	pub(crate) token: String,
-	expires_in: u64,
-	device: Device,
-}
-impl Oauth {
-	pub(crate) async fn new() -> Self {
-		loop {
-			let attempt = Self::new_with_timeout().await;
-			match attempt {
-				Ok(Some(oauth)) => {
-					info!("[âœ…] Successfully created OAuth client");
-					return oauth;
-				}
-				Ok(None) => {
-					error!("Failed to create OAuth client. Retrying in 5 seconds...");
-				}
-				Err(duration) => {
-					error!("Failed to create OAuth client in {duration:?}. Retrying in 5 seconds...");
-				}
-			}
-			tokio::time::sleep(Duration::from_secs(5)).await;
-		}
-	}
-	async fn new_with_timeout() -> Result<Option<Self>, Elapsed> {
-		let mut oauth = Self::default();
-		timeout(Duration::from_secs(5), oauth.login()).await.map(|result| result.map(|_| oauth))
-	}
-	pub(crate) fn default() -> Self {
-		let device = Device::new();
-		let headers_map = device.headers.clone();
-		let initial_headers = device.initial_headers.clone();
-		Self {
-			headers_map,
-			initial_headers,
-			token: String::new(),
-			expires_in: 0,
-			device,
-		}
-	}
-	async fn login(&mut self) -> Option<()> {
-		let url = format!("{AUTH_ENDPOINT}/auth/v2/oauth/access-token/loid");
-		let mut builder = Request::builder().method(Method::POST).uri(&url);
-		for (key, value) in &self.initial_headers {
-			builder = builder.header(key, value);
-		}
-		let auth = general_purpose::STANDARD.encode(format!("{}:", self.device.oauth_id));
-		builder = builder.header("Authorization", format!("Basic {auth}"));
-		let json = json!({
-				"scopes": ["*","email", "pii"]
-		});
-		let body = Body::from(json.to_string());
-		let request = builder.body(body).unwrap();
-		trace!("Sending token request...\n\n{request:?}");
-		let client: &once_cell::sync::Lazy<client::Client<_, Body>> = &CLIENT;
-		let resp = client.request(request).await.ok()?;
-		trace!("Received response with status {} and length {:?}", resp.status(), resp.headers().get("content-length"));
-		trace!("OAuth headers: {:#?}", resp.headers());
-		if let Some(header) = resp.headers().get("x-reddit-loid") {
-			self.headers_map.insert("x-reddit-loid".to_owned(), header.to_str().ok()?.to_string());
-		}
-		if let Some(header) = resp.headers().get("x-reddit-session") {
-			self.headers_map.insert("x-reddit-session".to_owned(), header.to_str().ok()?.to_string());
-		}
-		trace!("Serializing response...");
-		let body_bytes = hyper::body::to_bytes(resp.into_body()).await.ok()?;
-		let json: serde_json::Value = serde_json::from_slice(&body_bytes).ok()?;
-		trace!("Accessing relevant fields...");
-		self.token = json.get("access_token")?.as_str()?.to_string();
-		self.expires_in = json.get("expires_in")?.as_u64()?;
-		self.headers_map.insert("Authorization".to_owned(), format!("Bearer {}", self.token));
-		info!("[âœ…] Success - Retrieved token \"{}...\", expires in {}", &self.token[..32], self.expires_in);
-		Some(())
-	}
-}
-pub async fn token_daemon() {
-	loop {
-		let expires_in = { OAUTH_CLIENT.load_full().expires_in };
-		let duration = Duration::from_secs(expires_in - 120);
-		info!("[â³] Waiting for {duration:?} seconds before refreshing OAuth token...");
-		tokio::time::sleep(duration).await;
-		info!("[âŒ›] {duration:?} Elapsed! Refreshing OAuth token...");
-		{
-			force_refresh_token().await;
-		}
-	}
-}
-pub async fn force_refresh_token() {
-	if OAUTH_IS_ROLLING_OVER.compare_exchange(false, true, Ordering::SeqCst, Ordering::SeqCst).is_err() {
-		trace!("Skipping refresh token roll over, already in progress");
-		return;
-	}
-	trace!("Rolling over refresh token. Current rate limit: {}", OAUTH_RATELIMIT_REMAINING.load(Ordering::SeqCst));
-	let new_client = Oauth::new().await;
-	OAUTH_CLIENT.swap(new_client.into());
-	OAUTH_RATELIMIT_REMAINING.store(99, Ordering::SeqCst);
-	OAUTH_IS_ROLLING_OVER.store(false, Ordering::SeqCst);
-}
-#[derive(Debug, Clone, Default)]
-struct Device {
-	oauth_id: String,
-	initial_headers: HashMap<String, String>,
-	headers: HashMap<String, String>,
-}
-impl Device {
-	fn android() -> Self {
-		let uuid = uuid::Uuid::new_v4().to_string();
-		let android_app_version = choose(ANDROID_APP_VERSION_LIST).to_string();
-		let android_version = fastrand::u8(9..=14);
-		let android_user_agent = format!("Reddit/{android_app_version}/Android {android_version}");
-		let qos = fastrand::u32(1000..=100_000);
-		let qos: f32 = qos as f32 / 1000.0;
-		let qos = format!("{:.3}", qos);
-		let codecs = TextGenerator::new().generate("available-codecs=video/avc, video/hevc{, video/x-vnd.on2.vp9|}");
-		let headers: HashMap<String, String> = HashMap::from([
-			("User-Agent".into(), android_user_agent),
-			("x-reddit-retry".into(), "algo=no-retries".into()),
-			("x-reddit-compression".into(), "1".into()),
-			("x-reddit-qos".into(), qos),
-			("x-reddit-media-codecs".into(), codecs),
-			("Content-Type".into(), "application/json; charset=UTF-8".into()),
-			("client-vendor-id".into(), uuid.clone()),
-			("X-Reddit-Device-Id".into(), uuid.clone()),
-		]);
-		info!("[ðŸ”„] Spoofing Android client with headers: {headers:?}, uuid: \"{uuid}\", and OAuth ID \"{REDDIT_ANDROID_OAUTH_CLIENT_ID}\"");
-		Self {
-			oauth_id: REDDIT_ANDROID_OAUTH_CLIENT_ID.to_string(),
-			headers: headers.clone(),
-			initial_headers: headers,
-		}
-	}
-	fn new() -> Self {
-		Self::android()
-	}
-}
-fn choose<T: Copy>(list: &[T]) -> T {
-	*fastrand::choose_multiple(list.iter(), 1)[0]
-}
-#[tokio::test(flavor = "multi_thread")]
-async fn test_oauth_client() {
-	assert!(!OAUTH_CLIENT.load_full().token.is_empty());
-}
-#[tokio::test(flavor = "multi_thread")]
-async fn test_oauth_client_refresh() {
-	force_refresh_token().await;
-}
-#[tokio::test(flavor = "multi_thread")]
-async fn test_oauth_token_exists() {
-	assert!(!OAUTH_CLIENT.load_full().token.is_empty());
-}
-#[tokio::test(flavor = "multi_thread")]
-async fn test_oauth_headers_len() {
-	assert!(OAUTH_CLIENT.load_full().headers_map.len() >= 3);
-}
-#[test]
-fn test_creating_device() {
-	Device::new();
-}

--- a/src/oauth_resources.rs
+++ b//dev/null
@@ -1,154 +0,0 @@
-pub const _IOS_APP_VERSION_LIST: &[&str; 1] = &[""];
-pub const ANDROID_APP_VERSION_LIST: &[&str; 150] = &[
-	"Version 2024.22.1/Build 1652272",
-	"Version 2024.23.1/Build 1665606",
-	"Version 2024.24.1/Build 1682520",
-	"Version 2024.25.0/Build 1693595",
-	"Version 2024.25.2/Build 1700401",
-	"Version 2024.25.3/Build 1703490",
-	"Version 2024.26.0/Build 1710470",
-	"Version 2024.26.1/Build 1717435",
-	"Version 2024.28.0/Build 1737665",
-	"Version 2024.28.1/Build 1741165",
-	"Version 2024.30.0/Build 1770787",
-	"Version 2024.31.0/Build 1786202",
-	"Version 2024.32.0/Build 1809095",
-	"Version 2024.32.1/Build 1813258",
-	"Version 2024.33.0/Build 1819908",
-	"Version 2024.34.0/Build 1837909",
-	"Version 2024.35.0/Build 1861437",
-	"Version 2024.36.0/Build 1875012",
-	"Version 2024.37.0/Build 1888053",
-	"Version 2024.38.0/Build 1902791",
-	"Version 2024.39.0/Build 1916713",
-	"Version 2024.40.0/Build 1928580",
-	"Version 2024.41.0/Build 1941199",
-	"Version 2024.41.1/Build 1947805",
-	"Version 2024.42.0/Build 1952440",
-	"Version 2024.43.0/Build 1972250",
-	"Version 2024.44.0/Build 1988458",
-	"Version 2024.45.0/Build 2001943",
-	"Version 2024.46.0/Build 2012731",
-	"Version 2024.47.0/Build 2029755",
-	"Version 2023.48.0/Build 1319123",
-	"Version 2023.49.0/Build 1321715",
-	"Version 2023.49.1/Build 1322281",
-	"Version 2023.50.0/Build 1332338",
-	"Version 2023.50.1/Build 1345844",
-	"Version 2024.02.0/Build 1368985",
-	"Version 2024.03.0/Build 1379408",
-	"Version 2024.04.0/Build 1391236",
-	"Version 2024.05.0/Build 1403584",
-	"Version 2024.06.0/Build 1418489",
-	"Version 2024.07.0/Build 1429651",
-	"Version 2024.08.0/Build 1439531",
-	"Version 2024.10.0/Build 1470045",
-	"Version 2024.10.1/Build 1478645",
-	"Version 2024.11.0/Build 1480707",
-	"Version 2024.12.0/Build 1494694",
-	"Version 2024.13.0/Build 1505187",
-	"Version 2024.14.0/Build 1520556",
-	"Version 2024.15.0/Build 1536823",
-	"Version 2024.16.0/Build 1551366",
-	"Version 2024.17.0/Build 1568106",
-	"Version 2024.18.0/Build 1577901",
-	"Version 2024.18.1/Build 1585304",
-	"Version 2024.19.0/Build 1593346",
-	"Version 2024.20.0/Build 1612800",
-	"Version 2024.20.1/Build 1615586",
-	"Version 2024.20.2/Build 1624969",
-	"Version 2024.20.3/Build 1624970",
-	"Version 2024.21.0/Build 1631686",
-	"Version 2024.22.0/Build 1645257",
-	"Version 2023.21.0/Build 956283",
-	"Version 2023.22.0/Build 968223",
-	"Version 2023.23.0/Build 983896",
-	"Version 2023.24.0/Build 998541",
-	"Version 2023.25.0/Build 1014750",
-	"Version 2023.25.1/Build 1018737",
-	"Version 2023.26.0/Build 1019073",
-	"Version 2023.27.0/Build 1031923",
-	"Version 2023.28.0/Build 1046887",
-	"Version 2023.29.0/Build 1059855",
-	"Version 2023.30.0/Build 1078734",
-	"Version 2023.31.0/Build 1091027",
-	"Version 2023.32.0/Build 1109919",
-	"Version 2023.32.1/Build 1114141",
-	"Version 2023.33.1/Build 1129741",
-	"Version 2023.34.0/Build 1144243",
-	"Version 2023.35.0/Build 1157967",
-	"Version 2023.36.0/Build 1168982",
-	"Version 2023.37.0/Build 1182743",
-	"Version 2023.38.0/Build 1198522",
-	"Version 2023.39.0/Build 1211607",
-	"Version 2023.39.1/Build 1221505",
-	"Version 2023.40.0/Build 1221521",
-	"Version 2023.41.0/Build 1233125",
-	"Version 2023.41.1/Build 1239615",
-	"Version 2023.42.0/Build 1245088",
-	"Version 2023.43.0/Build 1257426",
-	"Version 2023.44.0/Build 1268622",
-	"Version 2023.45.0/Build 1281371",
-	"Version 2023.47.0/Build 1303604",
-	"Version 2022.42.0/Build 638508",
-	"Version 2022.43.0/Build 648277",
-	"Version 2022.44.0/Build 664348",
-	"Version 2022.45.0/Build 677985",
-	"Version 2023.01.0/Build 709875",
-	"Version 2023.02.0/Build 717912",
-	"Version 2023.03.0/Build 729220",
-	"Version 2023.04.0/Build 744681",
-	"Version 2023.05.0/Build 755453",
-	"Version 2023.06.0/Build 775017",
-	"Version 2023.07.0/Build 788827",
-	"Version 2023.07.1/Build 790267",
-	"Version 2023.08.0/Build 798718",
-	"Version 2023.09.0/Build 812015",
-	"Version 2023.09.1/Build 816833",
-	"Version 2023.10.0/Build 821148",
-	"Version 2023.11.0/Build 830610",
-	"Version 2023.12.0/Build 841150",
-	"Version 2023.13.0/Build 852246",
-	"Version 2023.14.0/Build 861593",
-	"Version 2023.14.1/Build 864826",
-	"Version 2023.15.0/Build 870628",
-	"Version 2023.16.0/Build 883294",
-	"Version 2023.16.1/Build 886269",
-	"Version 2023.17.0/Build 896030",
-	"Version 2023.17.1/Build 900542",
-	"Version 2023.18.0/Build 911877",
-	"Version 2023.19.0/Build 927681",
-	"Version 2023.20.0/Build 943980",
-	"Version 2023.20.1/Build 946732",
-	"Version 2022.20.0/Build 487703",
-	"Version 2022.21.0/Build 492436",
-	"Version 2022.22.0/Build 498700",
-	"Version 2022.23.0/Build 502374",
-	"Version 2022.23.1/Build 506606",
-	"Version 2022.24.0/Build 510950",
-	"Version 2022.24.1/Build 513462",
-	"Version 2022.25.0/Build 515072",
-	"Version 2022.25.1/Build 516394",
-	"Version 2022.25.2/Build 519915",
-	"Version 2022.26.0/Build 521193",
-	"Version 2022.27.0/Build 527406",
-	"Version 2022.27.1/Build 529687",
-	"Version 2022.28.0/Build 533235",
-	"Version 2022.30.0/Build 548620",
-	"Version 2022.31.0/Build 556666",
-	"Version 2022.31.1/Build 562612",
-	"Version 2022.32.0/Build 567875",
-	"Version 2022.33.0/Build 572600",
-	"Version 2022.34.0/Build 579352",
-	"Version 2022.35.0/Build 588016",
-	"Version 2022.35.1/Build 589034",
-	"Version 2022.36.0/Build 593102",
-	"Version 2022.37.0/Build 601691",
-	"Version 2022.38.0/Build 607460",
-	"Version 2022.39.0/Build 615385",
-	"Version 2022.39.1/Build 619019",
-	"Version 2022.40.0/Build 624782",
-	"Version 2022.41.0/Build 630468",
-	"Version 2022.41.1/Build 634168",
-];
-pub const _IOS_OS_VERSION_LIST: &[&str; 1] = &[""];

--- a/src/post.rs
+++ b//dev/null
@@ -1,195 +0,0 @@
-#![allow(clippy::cmp_owned)]
-use crate::client::json;
-use crate::config::get_setting;
-use crate::server::RequestExt;
-use crate::subreddit::{can_access_quarantine, quarantine};
-use crate::utils::{
-	error, format_num, get_filters, nsfw_landing, param, parse_post, rewrite_emotes, setting, template, time, val, Author, Awards, Comment, Flair, FlairPart, Post, Preferences,
-};
-use hyper::{Body, Request, Response};
-use once_cell::sync::Lazy;
-use regex::Regex;
-use rinja::Template;
-use std::collections::{HashMap, HashSet};
-#[derive(Template)]
-#[template(path = "post.html")]
-struct PostTemplate {
-	comments: Vec<Comment>,
-	post: Post,
-	sort: String,
-	prefs: Preferences,
-	single_thread: bool,
-	url: String,
-	url_without_query: String,
-	comment_query: String,
-}
-static COMMENT_SEARCH_CAPTURE: Lazy<Regex> = Lazy::new(|| Regex::new(r"\?q=(.*)&type=comment").unwrap());
-pub async fn item(req: Request<Body>) -> Result<Response<Body>, String> {
-	let mut path: String = format!("{}.json?{}&raw_json=1", req.uri().path(), req.uri().query().unwrap_or_default());
-	let sub = req.param("sub").unwrap_or_default();
-	let quarantined = can_access_quarantine(&req, &sub);
-	let url = req.uri().to_string();
-	let sort = param(&path, "sort").unwrap_or_else(|| {
-		let default_sort = setting(&req, "comment_sort");
-		if default_sort.is_empty() {
-			String::new()
-		} else {
-			path = format!("{}.json?{}&sort={}&raw_json=1", req.uri().path(), req.uri().query().unwrap_or_default(), default_sort);
-			default_sort
-		}
-	});
-	#[cfg(debug_assertions)]
-	req.param("id").unwrap_or_default();
-	let single_thread = req.param("comment_id").is_some();
-	let highlighted_comment = &req.param("comment_id").unwrap_or_default();
-	match json(path, quarantined).await {
-		Ok(response) => {
-			let post = parse_post(&response[0]["data"]["children"][0]).await;
-			let req_url = req.uri().to_string();
-			if post.nsfw && crate::utils::should_be_nsfw_gated(&req, &req_url) {
-				return Ok(nsfw_landing(req, req_url).await.unwrap_or_default());
-			}
-			let query_body = match COMMENT_SEARCH_CAPTURE.captures(&url) {
-				Some(captures) => captures.get(1).unwrap().as_str().replace("%20", " ").replace('+', " "),
-				None => String::new(),
-			};
-			let query_string = format!("q={query_body}&type=comment");
-			let form = url::form_urlencoded::parse(query_string.as_bytes()).collect::<HashMap<_, _>>();
-			let query = form.get("q").unwrap().clone().to_string();
-			let comments = match query.as_str() {
-				"" => parse_comments(&response[1], &post.permalink, &post.author.name, highlighted_comment, &get_filters(&req), &req),
-				_ => query_comments(&response[1], &post.permalink, &post.author.name, highlighted_comment, &get_filters(&req), &query, &req),
-			};
-			Ok(template(&PostTemplate {
-				comments,
-				post,
-				url_without_query: url.clone().trim_end_matches(&format!("?q={query}&type=comment")).to_string(),
-				sort,
-				prefs: Preferences::new(&req),
-				single_thread,
-				url: req_url,
-				comment_query: query,
-			}))
-		}
-		Err(msg) => {
-			if msg == "quarantined" || msg == "gated" {
-				let sub = req.param("sub").unwrap_or_default();
-				Ok(quarantine(&req, sub, &msg))
-			} else {
-				error(req, &msg).await
-			}
-		}
-	}
-}
-fn parse_comments(json: &serde_json::Value, post_link: &str, post_author: &str, highlighted_comment: &str, filters: &HashSet<String>, req: &Request<Body>) -> Vec<Comment> {
-	let comments = json["data"]["children"].as_array().map_or(Vec::new(), std::borrow::ToOwned::to_owned);
-	comments
-		.into_iter()
-		.map(|comment| {
-			let data = &comment["data"];
-			let replies: Vec<Comment> = if data["replies"].is_object() {
-				parse_comments(&data["replies"], post_link, post_author, highlighted_comment, filters, req)
-			} else {
-				Vec::new()
-			};
-			build_comment(&comment, data, replies, post_link, post_author, highlighted_comment, filters, req)
-		})
-		.collect()
-}
-fn query_comments(
-	json: &serde_json::Value,
-	post_link: &str,
-	post_author: &str,
-	highlighted_comment: &str,
-	filters: &HashSet<String>,
-	query: &str,
-	req: &Request<Body>,
-) -> Vec<Comment> {
-	let comments = json["data"]["children"].as_array().map_or(Vec::new(), std::borrow::ToOwned::to_owned);
-	let mut results = Vec::new();
-	for comment in comments {
-		let data = &comment["data"];
-		if data["replies"].is_object() {
-			results.append(&mut query_comments(&data["replies"], post_link, post_author, highlighted_comment, filters, query, req));
-		}
-		let c = build_comment(&comment, data, Vec::new(), post_link, post_author, highlighted_comment, filters, req);
-		if c.body.to_lowercase().contains(&query.to_lowercase()) {
-			results.push(c);
-		}
-	}
-	results
-}
-#[allow(clippy::too_many_arguments)]
-fn build_comment(
-	comment: &serde_json::Value,
-	data: &serde_json::Value,
-	replies: Vec<Comment>,
-	post_link: &str,
-	post_author: &str,
-	highlighted_comment: &str,
-	filters: &HashSet<String>,
-	req: &Request<Body>,
-) -> Comment {
-	let id = val(comment, "id");
-	let body = if (val(comment, "author") == "[deleted]" && val(comment, "body") == "[removed]") || val(comment, "body") == "[ Removed by Reddit ]" {
-		format!(
-			"<div class=\"md\"><p>[removed] â€” <a href=\"https://{}{post_link}{id}\">view removed comment</a></p></div>",
-			get_setting("REDLIB_PUSHSHIFT_FRONTEND").unwrap_or_else(|| String::from(crate::config::DEFAULT_PUSHSHIFT_FRONTEND)),
-		)
-	} else {
-		rewrite_emotes(&data["media_metadata"], val(comment, "body_html"))
-	};
-	let kind = comment["kind"].as_str().unwrap_or_default().to_string();
-	let unix_time = data["created_utc"].as_f64().unwrap_or_default();
-	let (rel_time, created) = time(unix_time);
-	let edited = data["edited"].as_f64().map_or((String::new(), String::new()), time);
-	let score = data["score"].as_i64().unwrap_or(0);
-	let more_count = data["count"].as_i64().unwrap_or_default();
-	let awards: Awards = Awards::parse(&data["all_awardings"]);
-	let parent_kind_and_id = val(comment, "parent_id");
-	let parent_info = parent_kind_and_id.split('_').collect::<Vec<&str>>();
-	let highlighted = id == highlighted_comment;
-	let author = Author {
-		name: val(comment, "author"),
-		flair: Flair {
-			flair_parts: FlairPart::parse(
-				data["author_flair_type"].as_str().unwrap_or_default(),
-				data["author_flair_richtext"].as_array(),
-				data["author_flair_text"].as_str(),
-			),
-			text: val(comment, "link_flair_text"),
-			background_color: val(comment, "author_flair_background_color"),
-			foreground_color: val(comment, "author_flair_text_color"),
-		},
-		distinguished: val(comment, "distinguished"),
-	};
-	let is_filtered = filters.contains(&["u_", author.name.as_str()].concat());
-	let is_moderator_comment = data["distinguished"].as_str().unwrap_or_default() == "moderator";
-	let is_stickied = data["stickied"].as_bool().unwrap_or_default();
-	let collapsed = (is_moderator_comment && is_stickied) || is_filtered;
-	Comment {
-		id,
-		kind,
-		parent_id: parent_info[1].to_string(),
-		parent_kind: parent_info[0].to_string(),
-		post_link: post_link.to_string(),
-		post_author: post_author.to_string(),
-		body,
-		author,
-		score: if data["score_hidden"].as_bool().unwrap_or_default() {
-			("\u{2022}".to_string(), "Hidden".to_string())
-		} else {
-			format_num(score)
-		},
-		rel_time,
-		created,
-		edited,
-		replies,
-		highlighted,
-		awards,
-		collapsed,
-		is_filtered,
-		more_count,
-		prefs: Preferences::new(req),
-	}
-}

--- a/src/search.rs
+++ b//dev/null
@@ -1,159 +0,0 @@
-#![allow(clippy::cmp_owned)]
-use crate::utils::{self, catch_random, error, filter_posts, format_num, format_url, get_filters, param, redirect, setting, template, val, Post, Preferences};
-use crate::{
-	client::json,
-	server::RequestExt,
-	subreddit::{can_access_quarantine, quarantine},
-};
-use hyper::{Body, Request, Response};
-use once_cell::sync::Lazy;
-use regex::Regex;
-use rinja::Template;
-struct SearchParams {
-	q: String,
-	sort: String,
-	t: String,
-	before: String,
-	after: String,
-	restrict_sr: String,
-	typed: String,
-}
-struct Subreddit {
-	name: String,
-	url: String,
-	icon: String,
-	description: String,
-	subscribers: (String, String),
-}
-#[derive(Template)]
-#[template(path = "search.html")]
-struct SearchTemplate {
-	posts: Vec<Post>,
-	subreddits: Vec<Subreddit>,
-	sub: String,
-	params: SearchParams,
-	prefs: Preferences,
-	url: String,
-	is_filtered: bool,
-	all_posts_filtered: bool,
-	all_posts_hidden_nsfw: bool,
-	no_posts: bool,
-}
-static REDDIT_URL_MATCH: Lazy<Regex> = Lazy::new(|| Regex::new(r"^https?://([^\./]+\.)*reddit.com/").unwrap());
-pub async fn find(req: Request<Body>) -> Result<Response<Body>, String> {
-	let nsfw_results = if setting(&req, "show_nsfw") == "on" && !utils::sfw_only() {
-		"&include_over_18=on"
-	} else {
-		""
-	};
-	let uri_path = req.uri().path().replace("+", "%2B");
-	let path = format!("{}.json?{}{}&raw_json=1", uri_path, req.uri().query().unwrap_or_default(), nsfw_results);
-	let mut query = param(&path, "q").unwrap_or_default();
-	query = REDDIT_URL_MATCH.replace(&query, "").to_string();
-	if query.is_empty() {
-		return Ok(redirect("/"));
-	}
-	if query.starts_with("r/") || query.starts_with("user/") {
-		return Ok(redirect(&format!("/{query}")));
-	}
-	if query.starts_with("R/") {
-		return Ok(redirect(&format!("/r{}", &query[1..])));
-	}
-	if query.starts_with("u/") || query.starts_with("U/") {
-		return Ok(redirect(&format!("/user{}", &query[1..])));
-	}
-	let sub = req.param("sub").unwrap_or_default();
-	let quarantined = can_access_quarantine(&req, &sub);
-	if let Ok(random) = catch_random(&sub, "/find").await {
-		return Ok(random);
-	}
-	let typed = param(&path, "type").unwrap_or_default();
-	let sort = param(&path, "sort").unwrap_or_else(|| "relevance".to_string());
-	let filters = get_filters(&req);
-	let subreddits = if param(&path, "restrict_sr").is_none() {
-		let mut subreddits = search_subreddits(&query, &typed).await;
-		subreddits.retain(|s| !filters.contains(s.name.as_str()));
-		subreddits
-	} else {
-		Vec::new()
-	};
-	let url = String::from(req.uri().path_and_query().map_or("", |val| val.as_str()));
-	if sub.split('+').all(|s| filters.contains(s)) {
-		Ok(template(&SearchTemplate {
-			posts: Vec::new(),
-			subreddits,
-			sub,
-			params: SearchParams {
-				q: query.replace('"', "&quot;"),
-				sort,
-				t: param(&path, "t").unwrap_or_default(),
-				before: param(&path, "after").unwrap_or_default(),
-				after: String::new(),
-				restrict_sr: param(&path, "restrict_sr").unwrap_or_default(),
-				typed,
-			},
-			prefs: Preferences::new(&req),
-			url,
-			is_filtered: true,
-			all_posts_filtered: false,
-			all_posts_hidden_nsfw: false,
-			no_posts: false,
-		}))
-	} else {
-		match Post::fetch(&path, quarantined).await {
-			Ok((mut posts, after)) => {
-				let (_, all_posts_filtered) = filter_posts(&mut posts, &filters);
-				let no_posts = posts.is_empty();
-				let all_posts_hidden_nsfw = !no_posts && (posts.iter().all(|p| p.flags.nsfw) && setting(&req, "show_nsfw") != "on");
-				Ok(template(&SearchTemplate {
-					posts,
-					subreddits,
-					sub,
-					params: SearchParams {
-						q: query.replace('"', "&quot;"),
-						sort,
-						t: param(&path, "t").unwrap_or_default(),
-						before: param(&path, "after").unwrap_or_default(),
-						after,
-						restrict_sr: param(&path, "restrict_sr").unwrap_or_default(),
-						typed,
-					},
-					prefs: Preferences::new(&req),
-					url,
-					is_filtered: false,
-					all_posts_filtered,
-					all_posts_hidden_nsfw,
-					no_posts,
-				}))
-			}
-			Err(msg) => {
-				if msg == "quarantined" || msg == "gated" {
-					let sub = req.param("sub").unwrap_or_default();
-					Ok(quarantine(&req, sub, &msg))
-				} else {
-					error(req, &msg).await
-				}
-			}
-		}
-	}
-}
-async fn search_subreddits(q: &str, typed: &str) -> Vec<Subreddit> {
-	let limit = if typed == "sr_user" { "50" } else { "3" };
-	let subreddit_search_path = format!("/subreddits/search.json?q={}&limit={limit}", q.replace(' ', "+"));
-	json(subreddit_search_path, false).await.unwrap_or_default()["data"]["children"]
-		.as_array()
-		.map(ToOwned::to_owned)
-		.unwrap_or_default()
-		.iter()
-		.map(|subreddit| {
-			let icon = subreddit["data"]["community_icon"].as_str().map_or_else(|| val(subreddit, "icon_img"), ToString::to_string);
-			Subreddit {
-				name: val(subreddit, "display_name"),
-				url: val(subreddit, "url"),
-				icon: format_url(&icon),
-				description: val(subreddit, "public_description"),
-				subscribers: format_num(subreddit["data"]["subscribers"].as_f64().unwrap_or_default() as i64),
-			}
-		})
-		.collect::<Vec<Subreddit>>()
-}

--- a/src/server.rs
+++ b//dev/null
@@ -1,493 +0,0 @@
-#![allow(dead_code)]
-#![allow(clippy::cmp_owned)]
-use brotli::enc::{BrotliCompress, BrotliEncoderParams};
-use cached::proc_macro::cached;
-use cookie::Cookie;
-use core::f64;
-use futures_lite::{future::Boxed, Future, FutureExt};
-use hyper::{
-	body,
-	body::HttpBody,
-	header,
-	service::{make_service_fn, service_fn},
-	HeaderMap,
-};
-use hyper::{Body, Method, Request, Response, Server as HyperServer};
-use libflate::gzip;
-use route_recognizer::{Params, Router};
-use std::{
-	cmp::Ordering,
-	fmt::Display,
-	io,
-	pin::Pin,
-	result::Result,
-	str::{from_utf8, Split},
-	string::ToString,
-};
-use time::OffsetDateTime;
-use crate::dbg_msg;
-type BoxResponse = Pin<Box<dyn Future<Output = Result<Response<Body>, String>> + Send>>;
-#[derive(Copy, Clone, Debug, Eq, Hash, Ord, PartialEq, PartialOrd)]
-enum CompressionType {
-	Passthrough,
-	Gzip,
-	Brotli,
-}
-const DEFAULT_COMPRESSOR: CompressionType = CompressionType::Gzip;
-impl CompressionType {
-	fn parse(s: &str) -> Option<Self> {
-		let c = match s {
-			"gzip" => Self::Gzip,
-			"br" => Self::Brotli,
-			"*" => DEFAULT_COMPRESSOR,
-			_ => return None,
-		};
-		Some(c)
-	}
-}
-impl Display for CompressionType {
-	fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-		match self {
-			Self::Gzip => write!(f, "gzip"),
-			Self::Brotli => write!(f, "br"),
-			Self::Passthrough => Ok(()),
-		}
-	}
-}
-pub struct Route<'a> {
-	router: &'a mut Router<fn(Request<Body>) -> BoxResponse>,
-	path: String,
-}
-pub struct Server {
-	pub default_headers: HeaderMap,
-	router: Router<fn(Request<Body>) -> BoxResponse>,
-}
-#[macro_export]
-macro_rules! headers(
-	{ $($key:expr => $value:expr),+ } => {
-		{
-			let mut m = hyper::HeaderMap::new();
-			$(
-				if let Ok(val) = hyper::header::HeaderValue::from_str($value) {
-					m.insert($key, val);
-				}
-			)+
-			m
-		}
-	 };
-);
-pub trait RequestExt {
-	fn params(&self) -> Params;
-	fn param(&self, name: &str) -> Option<String>;
-	fn set_params(&mut self, params: Params) -> Option<Params>;
-	fn cookies(&self) -> Vec<Cookie<'_>>;
-	fn cookie(&self, name: &str) -> Option<Cookie<'_>>;
-}
-pub trait ResponseExt {
-	fn cookies(&self) -> Vec<Cookie<'_>>;
-	fn insert_cookie(&mut self, cookie: Cookie<'_>);
-	fn remove_cookie(&mut self, name: String);
-}
-impl RequestExt for Request<Body> {
-	fn params(&self) -> Params {
-		self.extensions().get::<Params>().unwrap_or(&Params::new()).clone()
-	}
-	fn param(&self, name: &str) -> Option<String> {
-		self.params().find(name).map(std::borrow::ToOwned::to_owned)
-	}
-	fn set_params(&mut self, params: Params) -> Option<Params> {
-		self.extensions_mut().insert(params)
-	}
-	fn cookies(&self) -> Vec<Cookie<'_>> {
-		self.headers().get("Cookie").map_or(Vec::new(), |header| {
-			header
-				.to_str()
-				.unwrap_or_default()
-				.split("; ")
-				.map(|cookie| Cookie::parse(cookie).unwrap_or_else(|_| Cookie::from("")))
-				.collect()
-		})
-	}
-	fn cookie(&self, name: &str) -> Option<Cookie<'_>> {
-		self.cookies().into_iter().find(|c| c.name() == name)
-	}
-}
-impl ResponseExt for Response<Body> {
-	fn cookies(&self) -> Vec<Cookie<'_>> {
-		self.headers().get("Cookie").map_or(Vec::new(), |header| {
-			header
-				.to_str()
-				.unwrap_or_default()
-				.split("; ")
-				.map(|cookie| Cookie::parse(cookie).unwrap_or_else(|_| Cookie::from("")))
-				.collect()
-		})
-	}
-	fn insert_cookie(&mut self, cookie: Cookie<'_>) {
-		if let Ok(val) = header::HeaderValue::from_str(&cookie.to_string()) {
-			self.headers_mut().append("Set-Cookie", val);
-		}
-	}
-	fn remove_cookie(&mut self, name: String) {
-		let removal_cookie = Cookie::build(name).path("/").http_only(true).expires(OffsetDateTime::now_utc());
-		if let Ok(val) = header::HeaderValue::from_str(&removal_cookie.to_string()) {
-			self.headers_mut().append("Set-Cookie", val);
-		}
-	}
-}
-impl Route<'_> {
-	fn method(&mut self, method: &Method, dest: fn(Request<Body>) -> BoxResponse) -> &mut Self {
-		self.router.add(&format!("/{}{}", method.as_str(), self.path), dest);
-		self
-	}
-	pub fn get(&mut self, dest: fn(Request<Body>) -> BoxResponse) -> &mut Self {
-		self.method(&Method::GET, dest)
-	}
-	pub fn post(&mut self, dest: fn(Request<Body>) -> BoxResponse) -> &mut Self {
-		self.method(&Method::POST, dest)
-	}
-}
-impl Default for Server {
-	fn default() -> Self {
-		Self::new()
-	}
-}
-impl Server {
-	pub fn new() -> Self {
-		Self {
-			default_headers: HeaderMap::new(),
-			router: Router::new(),
-		}
-	}
-	pub fn at(&mut self, path: &str) -> Route<'_> {
-		Route {
-			path: path.to_owned(),
-			router: &mut self.router,
-		}
-	}
-	pub fn listen(self, addr: &str) -> Boxed<Result<(), hyper::Error>> {
-		let make_svc = make_service_fn(move |_conn| {
-			let router = self.router.clone();
-			let default_headers = self.default_headers.clone();
-			async move {
-				Ok::<_, String>(service_fn(move |req: Request<Body>| {
-					let req_headers = req.headers().clone();
-					let def_headers = default_headers.clone();
-					let mut path = req.uri().path().replace("//", "/").replace("%2F", "/");
-					if path != "/" && path.ends_with('/') {
-						path.pop();
-					}
-					let (method, is_head) = match req.method() {
-						&Method::HEAD => (&Method::GET, true),
-						method => (method, false),
-					};
-					match router.recognize(&format!("/{}{}", method.as_str(), path)) {
-						Ok(found) => {
-							let mut parammed = req;
-							parammed.set_params(found.params().clone());
-							let func = (found.handler().to_owned().to_owned())(parammed);
-							async move {
-								match func.await {
-									Ok(mut res) => {
-										res.headers_mut().extend(def_headers);
-										if is_head {
-											*res.body_mut() = Body::empty();
-										} else {
-											let _ = compress_response(&req_headers, &mut res).await;
-										}
-										Ok(res)
-									}
-									Err(msg) => new_boilerplate(def_headers, req_headers, 500, if is_head { Body::empty() } else { Body::from(msg) }).await,
-								}
-							}
-							.boxed()
-						}
-						Err(e) => new_boilerplate(def_headers, req_headers, 404, if is_head { Body::empty() } else { e.into() }).boxed(),
-					}
-				}))
-			}
-		});
-		let address = &addr.parse().unwrap_or_else(|_| panic!("Cannot parse {addr} as address (example format: 0.0.0.0:8080)"));
-		let server = HyperServer::bind(address).serve(make_svc).with_graceful_shutdown(async {
-			#[cfg(windows)]
-			tokio::signal::ctrl_c().await.expect("Failed to install CTRL+C signal handler");
-			#[cfg(unix)]
-			{
-				let mut signal_terminate = tokio::signal::unix::signal(tokio::signal::unix::SignalKind::terminate()).expect("Failed to install SIGTERM signal handler");
-				tokio::select! {
-					_ = tokio::signal::ctrl_c() => (),
-					_ = signal_terminate.recv() => ()
-				}
-			}
-		});
-		server.boxed()
-	}
-}
-async fn new_boilerplate(
-	default_headers: HeaderMap<header::HeaderValue>,
-	req_headers: HeaderMap<header::HeaderValue>,
-	status: u16,
-	body: Body,
-) -> Result<Response<Body>, String> {
-	match Response::builder().status(status).body(body) {
-		Ok(mut res) => {
-			let _ = compress_response(&req_headers, &mut res).await;
-			res.headers_mut().extend(default_headers.clone());
-			Ok(res)
-		}
-		Err(msg) => Err(msg.to_string()),
-	}
-}
-#[cached]
-fn determine_compressor(accept_encoding: String) -> Option<CompressionType> {
-	if accept_encoding.is_empty() {
-		return None;
-	};
-	struct CompressorCandidate {
-		alg: CompressionType,
-		q: f64,
-	}
-	impl Ord for CompressorCandidate {
-		fn cmp(&self, other: &Self) -> Ordering {
-			match self.q.total_cmp(&other.q) {
-				Ordering::Equal => self.alg.cmp(&other.alg),
-				ord => ord,
-			}
-		}
-	}
-	impl PartialOrd for CompressorCandidate {
-		fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
-			Some(self.cmp(other))
-		}
-	}
-	impl PartialEq for CompressorCandidate {
-		fn eq(&self, other: &Self) -> bool {
-			(self.q == other.q) && (self.alg == other.alg)
-		}
-	}
-	impl Eq for CompressorCandidate {}
-	let mut cur_candidate = CompressorCandidate {
-		alg: CompressionType::Passthrough,
-		q: f64::NEG_INFINITY,
-	};
-	for val in accept_encoding.split(',') {
-		let mut q: f64 = 1.0;
-		let mut spl: Split<'_, char> = val.split(';');
-		let compressor: CompressionType = match spl.next() {
-			Some(s) => match CompressionType::parse(s.trim()) {
-				Some(candidate) => candidate,
-				None => continue,
-			},
-			None => continue,
-		};
-		if let Some(s) = spl.next() {
-			if !(s.len() > 2 && s.starts_with("q=")) {
-				return None;
-			}
-			match s[2..].parse::<f64>() {
-				Ok(val) => {
-					if (0.0..=1.0).contains(&val) {
-						q = val;
-					} else {
-						return None;
-					};
-				}
-				Err(_) => {
-					return None;
-				}
-			}
-		};
-		let new_candidate = CompressorCandidate { alg: compressor, q };
-		if let Some(ord) = new_candidate.partial_cmp(&cur_candidate) {
-			if ord == Ordering::Greater {
-				cur_candidate = new_candidate;
-			}
-		};
-	}
-	if cur_candidate.q == f64::NEG_INFINITY {
-		None
-	} else {
-		Some(cur_candidate.alg)
-	}
-}
-async fn compress_response(req_headers: &HeaderMap<header::HeaderValue>, res: &mut Response<Body>) -> Result<(), String> {
-	if let Some(hdr) = res.headers().get(header::CONTENT_TYPE) {
-		match from_utf8(hdr.as_bytes()) {
-			Ok(val) => {
-				let s = val.to_string();
-				if !(s.starts_with("text/") || s.starts_with("application/json")) {
-					return Ok(());
-				};
-			}
-			Err(e) => {
-				dbg_msg!(e);
-				return Err(e.to_string());
-			}
-		};
-	} else {
-		return Ok(());
-	};
-	if res.body().size_hint().lower() < 1452 {
-		return Ok(());
-	};
-	let accept_encoding: String = match req_headers.get(header::ACCEPT_ENCODING) {
-		None => return Ok(()), // Client requested no compression.
-		Some(hdr) => match String::from_utf8(hdr.as_bytes().into()) {
-			Ok(val) => val,
-			#[cfg(debug_assertions)]
-			Err(e) => {
-				dbg_msg!(e);
-				return Ok(());
-			}
-			#[cfg(not(debug_assertions))]
-			Err(_) => return Ok(()),
-		},
-	};
-	let compressor: CompressionType = match determine_compressor(accept_encoding) {
-		Some(c) => c,
-		None => return Ok(()),
-	};
-	let body_bytes: Vec<u8> = match body::to_bytes(res.body_mut()).await {
-		Ok(b) => b.to_vec(),
-		Err(e) => {
-			dbg_msg!(e);
-			return Err(e.to_string());
-		}
-	};
-	match compress_body(compressor, body_bytes) {
-		Ok(compressed) => {
-			res.headers_mut().insert(header::CONTENT_ENCODING, compressor.to_string().parse().unwrap());
-			*(res.body_mut()) = Body::from(compressed);
-		}
-		Err(e) => return Err(e),
-	}
-	Ok(())
-}
-#[cached(size = 100, time = 600, result = true)]
-fn compress_body(compressor: CompressionType, body_bytes: Vec<u8>) -> Result<Vec<u8>, String> {
-	let mut reader = io::Cursor::new(body_bytes);
-	let compressed: Vec<u8> = match compressor {
-		CompressionType::Gzip => {
-			let mut gz: gzip::Encoder<Vec<u8>> = match gzip::Encoder::new(Vec::new()) {
-				Ok(gz) => gz,
-				Err(e) => {
-					dbg_msg!(e);
-					return Err(e.to_string());
-				}
-			};
-			match io::copy(&mut reader, &mut gz) {
-				Ok(_) => match gz.finish().into_result() {
-					Ok(compressed) => compressed,
-					Err(e) => {
-						dbg_msg!(e);
-						return Err(e.to_string());
-					}
-				},
-				Err(e) => {
-					dbg_msg!(e);
-					return Err(e.to_string());
-				}
-			}
-		}
-		CompressionType::Brotli => {
-			let brotli_params = BrotliEncoderParams::default();
-			let mut compressed = Vec::<u8>::new();
-			match BrotliCompress(&mut reader, &mut compressed, &brotli_params) {
-				Ok(_) => compressed,
-				Err(e) => {
-					dbg_msg!(e);
-					return Err(e.to_string());
-				}
-			}
-		}
-		CompressionType::Passthrough => {
-			let msg = "unsupported compressor".to_string();
-			return Err(msg);
-		}
-	};
-	Ok(compressed)
-}
-#[cfg(test)]
-mod tests {
-	use super::*;
-	use brotli::Decompressor as BrotliDecompressor;
-	use futures_lite::future::block_on;
-	use lipsum::lipsum;
-	use std::{boxed::Box, io};
-	#[test]
-	fn test_determine_compressor() {
-		assert_eq!(determine_compressor("unsupported".to_string()), None);
-		assert_eq!(determine_compressor("gzip".to_string()), Some(CompressionType::Gzip));
-		assert_eq!(determine_compressor("*".to_string()), Some(DEFAULT_COMPRESSOR));
-		assert_eq!(determine_compressor("gzip, br".to_string()), Some(CompressionType::Brotli));
-		assert_eq!(determine_compressor("gzip;q=0.8, br;q=0.3".to_string()), Some(CompressionType::Gzip));
-		assert_eq!(determine_compressor("br, gzip".to_string()), Some(CompressionType::Brotli));
-		assert_eq!(determine_compressor("br;q=0.3, gzip;q=0.4".to_string()), Some(CompressionType::Gzip));
-		assert_eq!(determine_compressor("gzip;q=NAN".to_string()), None);
-	}
-	#[test]
-	fn test_compress_response() {
-		macro_rules! ae_gen {
-			($x:expr) => {
-				$x.to_string().as_str()
-			};
-			($x:expr, $($y:expr),+) => {
-				format!("{}, {}", $x.to_string(), ae_gen!($($y),+)).as_str()
-			};
-		}
-		for accept_encoding in [
-			"*",
-			ae_gen!(CompressionType::Gzip),
-			ae_gen!(CompressionType::Brotli, CompressionType::Gzip),
-			ae_gen!(CompressionType::Brotli),
-		] {
-			let expected_encoding: CompressionType = match determine_compressor(accept_encoding.to_string()) {
-				Some(s) => s,
-				None => panic!("determine_compressor(accept_encoding.to_string()) => None"),
-			};
-			let mut req_headers = HeaderMap::new();
-			req_headers.insert(header::ACCEPT_ENCODING, header::HeaderValue::from_str(accept_encoding).unwrap());
-			let lorem_ipsum: String = lipsum(10000);
-			let expected_lorem_ipsum = Vec::<u8>::from(lorem_ipsum.as_str());
-			let mut res = Response::builder()
-				.status(200)
-				.header(header::CONTENT_TYPE, "text/plain")
-				.body(Body::from(lorem_ipsum))
-				.unwrap();
-			if let Err(e) = block_on(compress_response(&req_headers, &mut res)) {
-				panic!("compress_response(&req_headers, &mut res) => Err(\"{e}\")");
-			};
-			assert_eq!(
-				res
-					.headers()
-					.get(header::CONTENT_ENCODING)
-					.unwrap_or_else(|| panic!("missing content-encoding header"))
-					.to_str()
-					.unwrap_or_else(|_| panic!("failed to convert Content-Encoding header::HeaderValue to String")),
-				expected_encoding.to_string()
-			);
-			let body_vec = match block_on(body::to_bytes(res.body_mut())) {
-				Ok(b) => b.to_vec(),
-				Err(e) => panic!("{e}"),
-			};
-			if expected_encoding == CompressionType::Passthrough {
-				assert!(body_vec.eq(&expected_lorem_ipsum));
-				continue;
-			}
-			let mut body_cursor: io::Cursor<Vec<u8>> = io::Cursor::new(body_vec);
-			let mut decoder: Box<dyn io::Read> = match expected_encoding {
-				CompressionType::Gzip => match gzip::Decoder::new(&mut body_cursor) {
-					Ok(dgz) => Box::new(dgz),
-					Err(e) => panic!("{e}"),
-				},
-				CompressionType::Brotli => Box::new(BrotliDecompressor::new(body_cursor, expected_lorem_ipsum.len())),
-				_ => panic!("no decompressor for {}", expected_encoding),
-			};
-			let mut decompressed = Vec::<u8>::new();
-			if let Err(e) = io::copy(&mut decoder, &mut decompressed) {
-				panic!("{e}");
-			};
-			assert!(decompressed.eq(&expected_lorem_ipsum));
-		}
-	}
-}

--- a/src/settings.rs
+++ b//dev/null
@@ -1,206 +0,0 @@
-#![allow(clippy::cmp_owned)]
-use std::collections::HashMap;
-use crate::server::ResponseExt;
-use crate::subreddit::join_until_size_limit;
-use crate::utils::{deflate_decompress, redirect, template, Preferences};
-use cookie::Cookie;
-use futures_lite::StreamExt;
-use hyper::{Body, Request, Response};
-use rinja::Template;
-use time::{Duration, OffsetDateTime};
-use tokio::time::timeout;
-use url::form_urlencoded;
-#[derive(Template)]
-#[template(path = "settings.html")]
-struct SettingsTemplate {
-	prefs: Preferences,
-	url: String,
-}
-const PREFS: [&str; 19] = [
-	"theme",
-	"front_page",
-	"layout",
-	"wide",
-	"comment_sort",
-	"post_sort",
-	"blur_spoiler",
-	"show_nsfw",
-	"blur_nsfw",
-	"use_hls",
-	"hide_hls_notification",
-	"autoplay_videos",
-	"hide_sidebar_and_summary",
-	"fixed_navbar",
-	"hide_awards",
-	"hide_score",
-	"disable_visit_reddit_confirmation",
-	"video_quality",
-	"remove_default_feeds",
-];
-pub async fn get(req: Request<Body>) -> Result<Response<Body>, String> {
-	let url = req.uri().to_string();
-	Ok(template(&SettingsTemplate {
-		prefs: Preferences::new(&req),
-		url,
-	}))
-}
-pub async fn set(req: Request<Body>) -> Result<Response<Body>, String> {
-	let (parts, mut body) = req.into_parts();
-	let _cookies: Vec<Cookie<'_>> = parts
-		.headers
-		.get_all("Cookie")
-		.iter()
-		.filter_map(|header| Cookie::parse(header.to_str().unwrap_or_default()).ok())
-		.collect();
-	let body_bytes = body
-		.try_fold(Vec::new(), |mut data, chunk| {
-			data.extend_from_slice(&chunk);
-			Ok(data)
-		})
-		.await
-		.map_err(|e| e.to_string())?;
-	let form = url::form_urlencoded::parse(&body_bytes).collect::<HashMap<_, _>>();
-	let mut response = redirect("/settings");
-	for &name in &PREFS {
-		match form.get(name) {
-			Some(value) => response.insert_cookie(
-				Cookie::build((name.to_owned(), value.clone()))
-					.path("/")
-					.http_only(true)
-					.expires(OffsetDateTime::now_utc() + Duration::weeks(52))
-					.into(),
-			),
-			None => response.remove_cookie(name.to_string()),
-		};
-	}
-	Ok(response)
-}
-fn set_cookies_method(req: Request<Body>, remove_cookies: bool) -> Response<Body> {
-	let (parts, _) = req.into_parts();
-	let _cookies: Vec<Cookie<'_>> = parts
-		.headers
-		.get_all("Cookie")
-		.iter()
-		.filter_map(|header| Cookie::parse(header.to_str().unwrap_or_default()).ok())
-		.collect();
-	let query = parts.uri.query().unwrap_or_default().as_bytes();
-	let form = url::form_urlencoded::parse(query).collect::<HashMap<_, _>>();
-	let path = match form.get("redirect") {
-		Some(value) => format!("/{}", value.replace("%26", "&").replace("%23", "#")),
-		None => "/".to_string(),
-	};
-	let mut response = redirect(&path);
-	for name in PREFS {
-		match form.get(name) {
-			Some(value) => response.insert_cookie(
-				Cookie::build((name.to_owned(), value.clone()))
-					.path("/")
-					.http_only(true)
-					.expires(OffsetDateTime::now_utc() + Duration::weeks(52))
-					.into(),
-			),
-			None => {
-				if remove_cookies {
-					response.remove_cookie(name.to_string());
-				}
-			}
-		};
-	}
-	let subscriptions = form.get("subscriptions");
-	let filters = form.get("filters");
-	let cookies_string = parts
-		.headers
-		.get("cookie")
-		.map(|hv| hv.to_str().unwrap_or("").to_string()) // Return String
-		.unwrap_or_else(String::new); // Return an empty string if None
-	if subscriptions.is_some() {
-		let sub_list: Vec<String> = subscriptions.expect("Subscriptions").split('+').map(str::to_string).collect();
-		let mut subscriptions_number_to_delete_from = 0;
-		for (subscriptions_number, list) in join_until_size_limit(&sub_list).into_iter().enumerate() {
-			let subscriptions_cookie = if subscriptions_number == 0 {
-				"subscriptions".to_string()
-			} else {
-				format!("subscriptions{}", subscriptions_number)
-			};
-			response.insert_cookie(
-				Cookie::build((subscriptions_cookie, list))
-					.path("/")
-					.http_only(true)
-					.expires(OffsetDateTime::now_utc() + Duration::weeks(52))
-					.into(),
-			);
-			subscriptions_number_to_delete_from += 1;
-		}
-		while cookies_string.contains(&format!("subscriptions{subscriptions_number_to_delete_from}=")) {
-			response.remove_cookie(format!("subscriptions{subscriptions_number_to_delete_from}"));
-			subscriptions_number_to_delete_from += 1;
-		}
-	} else {
-		response.remove_cookie("subscriptions".to_string());
-		let mut subscriptions_number_to_delete_from = 1;
-		while cookies_string.contains(&format!("subscriptions{subscriptions_number_to_delete_from}=")) {
-			response.remove_cookie(format!("subscriptions{subscriptions_number_to_delete_from}"));
-			subscriptions_number_to_delete_from += 1;
-		}
-	}
-	if filters.is_some() {
-		let filters_list: Vec<String> = filters.expect("Filters").split('+').map(str::to_string).collect();
-		let mut filters_number_to_delete_from = 0;
-		for (filters_number, list) in join_until_size_limit(&filters_list).into_iter().enumerate() {
-			let filters_cookie = if filters_number == 0 {
-				"filters".to_string()
-			} else {
-				format!("filters{}", filters_number)
-			};
-			response.insert_cookie(
-				Cookie::build((filters_cookie, list))
-					.path("/")
-					.http_only(true)
-					.expires(OffsetDateTime::now_utc() + Duration::weeks(52))
-					.into(),
-			);
-			filters_number_to_delete_from += 1;
-		}
-		while cookies_string.contains(&format!("filters{filters_number_to_delete_from}=")) {
-			response.remove_cookie(format!("filters{filters_number_to_delete_from}"));
-			filters_number_to_delete_from += 1;
-		}
-	} else {
-		response.remove_cookie("filters".to_string());
-		let mut filters_number_to_delete_from = 1;
-		while cookies_string.contains(&format!("filters{filters_number_to_delete_from}=")) {
-			response.remove_cookie(format!("filters{filters_number_to_delete_from}"));
-			filters_number_to_delete_from += 1;
-		}
-	}
-	response
-}
-pub async fn restore(req: Request<Body>) -> Result<Response<Body>, String> {
-	Ok(set_cookies_method(req, true))
-}
-pub async fn update(req: Request<Body>) -> Result<Response<Body>, String> {
-	Ok(set_cookies_method(req, false))
-}
-pub async fn encoded_restore(req: Request<Body>) -> Result<Response<Body>, String> {
-	let body = hyper::body::to_bytes(req.into_body())
-		.await
-		.map_err(|e| format!("Failed to get bytes from request body: {}", e))?;
-	if body.len() > 1024 * 1024 {
-		return Err("Request body too large".to_string());
-	}
-	let encoded_prefs = form_urlencoded::parse(&body)
-		.find(|(key, _)| key == "encoded_prefs")
-		.map(|(_, value)| value)
-		.ok_or_else(|| "encoded_prefs parameter not found in request body".to_string())?;
-	let bytes = base2048::decode(&encoded_prefs).ok_or_else(|| "Failed to decode base2048 encoded preferences".to_string())?;
-	let out = timeout(std::time::Duration::from_secs(1), async { deflate_decompress(bytes) })
-		.await
-		.map_err(|e| format!("Failed to decompress bytes: {}", e))??;
-	let mut prefs: Preferences = timeout(std::time::Duration::from_secs(1), async { bincode::deserialize(&out) })
-		.await
-		.map_err(|e| format!("Failed to deserialize preferences: {}", e))?
-		.map_err(|e| format!("Failed to deserialize bytes into Preferences struct: {}", e))?;
-	prefs.available_themes = vec![];
-	let url = format!("/settings/restore/?{}", prefs.to_urlencoded()?);
-	Ok(redirect(&url))
-}

--- a/src/subreddit.rs
+++ b//dev/null
@@ -1,458 +0,0 @@
-#![allow(clippy::cmp_owned)]
-use crate::{config, utils};
-use crate::utils::{
-	catch_random, error, filter_posts, format_num, format_url, get_filters, info, nsfw_landing, param, redirect, rewrite_urls, setting, template, val, Post, Preferences,
-	Subreddit,
-};
-use crate::{client::json, server::RequestExt, server::ResponseExt};
-use cookie::Cookie;
-use htmlescape::decode_html;
-use hyper::{Body, Request, Response};
-use rinja::Template;
-use chrono::DateTime;
-use once_cell::sync::Lazy;
-use regex::Regex;
-use time::{Duration, OffsetDateTime};
-#[derive(Template)]
-#[template(path = "subreddit.html")]
-struct SubredditTemplate {
-	sub: Subreddit,
-	posts: Vec<Post>,
-	sort: (String, String),
-	ends: (String, String),
-	prefs: Preferences,
-	url: String,
-	redirect_url: String,
-	is_filtered: bool,
-	all_posts_filtered: bool,
-	all_posts_hidden_nsfw: bool,
-	no_posts: bool,
-}
-#[derive(Template)]
-#[template(path = "wiki.html")]
-struct WikiTemplate {
-	sub: String,
-	wiki: String,
-	page: String,
-	prefs: Preferences,
-	url: String,
-}
-#[derive(Template)]
-#[template(path = "wall.html")]
-struct WallTemplate {
-	title: String,
-	sub: String,
-	msg: String,
-	prefs: Preferences,
-	url: String,
-}
-static GEO_FILTER_MATCH: Lazy<Regex> = Lazy::new(|| Regex::new(r"geo_filter=(?<region>\w+)").unwrap());
-pub async fn community(req: Request<Body>) -> Result<Response<Body>, String> {
-	let root = req.uri().path() == "/";
-	let query = req.uri().query().unwrap_or_default().to_string();
-	let subscribed = setting(&req, "subscriptions");
-	let front_page = setting(&req, "front_page");
-	let remove_default_feeds = setting(&req, "remove_default_feeds") == "on";
-	let post_sort = req.cookie("post_sort").map_or_else(|| "hot".to_string(), |c| c.value().to_string());
-	let sort = req.param("sort").unwrap_or_else(|| req.param("id").unwrap_or(post_sort));
-	let sub_name = req.param("sub").unwrap_or(if front_page == "default" || front_page.is_empty() {
-		if subscribed.is_empty() {
-			"popular".to_string()
-		} else {
-			subscribed.clone()
-		}
-	} else {
-		front_page.clone()
-	});
-	if (sub_name == "popular" || sub_name == "all") && remove_default_feeds {
-		if subscribed.is_empty() {
-			return info(req, "Subscribe to some subreddits! (Default feeds disabled in settings)").await;
-		} else {
-			return info(
-				req,
-				"You have subscribed to some subreddits, but your front page is not set to default. Visit settings and change front page to default.",
-			)
-			.await;
-		}
-	}
-	let quarantined = can_access_quarantine(&req, &sub_name) || root;
-	if let Ok(random) = catch_random(&sub_name, "").await {
-		return Ok(random);
-	}
-	if req.param("sub").is_some() && sub_name.starts_with("u_") {
-		return Ok(redirect(&["/user/", &sub_name[2..]].concat()));
-	}
-	let sub = if !sub_name.contains('+') && sub_name != subscribed && sub_name != "popular" && sub_name != "all" {
-		subreddit(&sub_name, quarantined).await.unwrap_or_default()
-	} else if sub_name == subscribed {
-		if req.uri().path().starts_with("/r/") {
-			subreddit(&sub_name, quarantined).await.unwrap_or_default()
-		} else {
-			Subreddit::default()
-		}
-	} else {
-		Subreddit {
-			name: sub_name.clone(),
-			..Subreddit::default()
-		}
-	};
-	let req_url = req.uri().to_string();
-	if sub.nsfw && crate::utils::should_be_nsfw_gated(&req, &req_url) {
-		return Ok(nsfw_landing(req, req_url).await.unwrap_or_default());
-	}
-	let mut params = String::from("&raw_json=1");
-	if sub_name == "popular" {
-		let geo_filter = match GEO_FILTER_MATCH.captures(&query) {
-			Some(geo_filter) => geo_filter["region"].to_string(),
-			None => "GLOBAL".to_owned(),
-		};
-		params.push_str(&format!("&geo_filter={geo_filter}"));
-	}
-	let path = format!("/r/{}/{sort}.json?{}{params}", sub_name.replace('+', "%2B"), req.uri().query().unwrap_or_default());
-	let url = String::from(req.uri().path_and_query().map_or("", |val| val.as_str()));
-	let redirect_url = url[1..].replace('?', "%3F").replace('&', "%26").replace('+', "%2B");
-	let filters = get_filters(&req);
-	if sub_name.split('+').all(|s| filters.contains(s)) {
-		Ok(template(&SubredditTemplate {
-			sub,
-			posts: Vec::new(),
-			sort: (sort, param(&path, "t").unwrap_or_default()),
-			ends: (param(&path, "after").unwrap_or_default(), String::new()),
-			prefs: Preferences::new(&req),
-			url,
-			redirect_url,
-			is_filtered: true,
-			all_posts_filtered: false,
-			all_posts_hidden_nsfw: false,
-			no_posts: false,
-		}))
-	} else {
-		match Post::fetch(&path, quarantined).await {
-			Ok((mut posts, after)) => {
-				let (_, all_posts_filtered) = filter_posts(&mut posts, &filters);
-				let no_posts = posts.is_empty();
-				let all_posts_hidden_nsfw = !no_posts && (posts.iter().all(|p| p.flags.nsfw) && setting(&req, "show_nsfw") != "on");
-				if sort == "new" {
-					posts.sort_by(|a, b| b.created_ts.cmp(&a.created_ts));
-					posts.sort_by(|a, b| b.flags.stickied.cmp(&a.flags.stickied));
-				}
-				Ok(template(&SubredditTemplate {
-					sub,
-					posts,
-					sort: (sort, param(&path, "t").unwrap_or_default()),
-					ends: (param(&path, "after").unwrap_or_default(), after),
-					prefs: Preferences::new(&req),
-					url,
-					redirect_url,
-					is_filtered: false,
-					all_posts_filtered,
-					all_posts_hidden_nsfw,
-					no_posts,
-				}))
-			}
-			Err(msg) => match msg.as_str() {
-				"quarantined" | "gated" => Ok(quarantine(&req, sub_name, &msg)),
-				"private" => error(req, &format!("r/{sub_name} is a private community")).await,
-				"banned" => error(req, &format!("r/{sub_name} has been banned from Reddit")).await,
-				_ => error(req, &msg).await,
-			},
-		}
-	}
-}
-pub fn quarantine(req: &Request<Body>, sub: String, restriction: &str) -> Response<Body> {
-	let wall = WallTemplate {
-		title: format!("r/{sub} is {restriction}"),
-		msg: "Please click the button below to continue to this subreddit.".to_string(),
-		url: req.uri().to_string(),
-		sub,
-		prefs: Preferences::new(req),
-	};
-	Response::builder()
-		.status(403)
-		.header("content-type", "text/html")
-		.body(wall.render().unwrap_or_default().into())
-		.unwrap_or_default()
-}
-pub async fn add_quarantine_exception(req: Request<Body>) -> Result<Response<Body>, String> {
-	let subreddit = req.param("sub").ok_or("Invalid URL")?;
-	let redir = param(&format!("?{}", req.uri().query().unwrap_or_default()), "redir").ok_or("Invalid URL")?;
-	let mut response = redirect(&redir);
-	response.insert_cookie(
-		Cookie::build((&format!("allow_quaran_{}", subreddit.to_lowercase()), "true"))
-			.path("/")
-			.http_only(true)
-			.expires(cookie::Expiration::Session)
-			.into(),
-	);
-	Ok(response)
-}
-pub fn can_access_quarantine(req: &Request<Body>, sub: &str) -> bool {
-	setting(req, &format!("allow_quaran_{}", sub.to_lowercase())).parse().unwrap_or_default()
-}
-pub fn join_until_size_limit<T: std::fmt::Display>(vec: &[T]) -> Vec<std::string::String> {
-	let mut result = Vec::new();
-	let mut list = String::new();
-	let mut current_size = 0;
-	for item in vec {
-		let item_size = item.to_string().len();
-		if current_size + item_size > 4000 {
-			list.push('+');
-			result.push(list);
-			list = String::new();
-		}
-		if !list.is_empty() {
-			list.push('+');
-		}
-		list.push_str(&item.to_string());
-		current_size = list.len() + item_size;
-	}
-	result.push(list);
-	result
-}
-pub async fn subscriptions_filters(req: Request<Body>) -> Result<Response<Body>, String> {
-	let sub = req.param("sub").unwrap_or_default();
-	let action: Vec<String> = req.uri().path().split('/').map(String::from).collect();
-	if sub == "random" || sub == "randnsfw" {
-		if action.contains(&"filter".to_string()) || action.contains(&"unfilter".to_string()) {
-			return Err("Can't filter random subreddit!".to_string());
-		}
-		return Err("Can't subscribe to random subreddit!".to_string());
-	}
-	let query = req.uri().query().unwrap_or_default().to_string();
-	let preferences = Preferences::new(&req);
-	let mut sub_list = preferences.subscriptions;
-	let mut filters = preferences.filters;
-	let posts = json(format!("/r/{sub}/hot.json?raw_json=1"), true).await;
-	let display_lookup: Vec<(String, &str)> = match &posts {
-		Ok(posts) => posts["data"]["children"]
-			.as_array()
-			.map(|list| {
-				list
-					.iter()
-					.map(|post| {
-						let display_name = post["data"]["subreddit"].as_str().unwrap_or_default();
-						(display_name.to_lowercase(), display_name)
-					})
-					.collect::<Vec<_>>()
-			})
-			.unwrap_or_default(),
-		Err(_) => vec![],
-	};
-	for part in sub.split('+').filter(|x| x != &"") {
-		let display;
-		let part = if part.starts_with("u_") {
-			part
-		} else if let Some(&(_, display)) = display_lookup.iter().find(|x| x.0 == part.to_lowercase()) {
-			display
-		} else {
-			let path: String = format!("/r/{part}/about.json?raw_json=1");
-			display = json(path, true).await;
-			match &display {
-				Ok(display) => display["data"]["display_name"].as_str(),
-				Err(_) => None,
-			}
-			.unwrap_or(part)
-		};
-		if action.contains(&"subscribe".to_string()) && !sub_list.contains(&part.to_owned()) {
-			sub_list.push(part.to_owned());
-			filters.retain(|s| s.to_lowercase() != part.to_lowercase());
-			sub_list.sort_by_key(|a| a.to_lowercase());
-			filters.sort_by_key(|a| a.to_lowercase());
-		} else if action.contains(&"unsubscribe".to_string()) {
-			sub_list.retain(|s| s.to_lowercase() != part.to_lowercase());
-		} else if action.contains(&"filter".to_string()) && !filters.contains(&part.to_owned()) {
-			filters.push(part.to_owned());
-			sub_list.retain(|s| s.to_lowercase() != part.to_lowercase());
-			filters.sort_by_key(|a| a.to_lowercase());
-			sub_list.sort_by_key(|a| a.to_lowercase());
-		} else if action.contains(&"unfilter".to_string()) {
-			filters.retain(|s| s.to_lowercase() != part.to_lowercase());
-		}
-	}
-	let path = if let Some(redirect_path) = param(&format!("?{query}"), "redirect") {
-		format!("/{redirect_path}")
-	} else {
-		format!("/r/{sub}")
-	};
-	let mut response = redirect(&path);
-	if sub_list.is_empty() {
-		response.remove_cookie("subscriptions".to_string());
-		let mut subscriptions_number = 1;
-		while req.cookie(&format!("subscriptions{}", subscriptions_number)).is_some() {
-			response.remove_cookie(format!("subscriptions{}", subscriptions_number));
-			subscriptions_number += 1;
-		}
-	} else {
-		let mut subscriptions_number_to_delete_from = 0;
-		for (subscriptions_number, list) in join_until_size_limit(&sub_list).into_iter().enumerate() {
-			let subscriptions_cookie = if subscriptions_number == 0 {
-				"subscriptions".to_string()
-			} else {
-				format!("subscriptions{}", subscriptions_number)
-			};
-			response.insert_cookie(
-				Cookie::build((subscriptions_cookie, list))
-					.path("/")
-					.http_only(true)
-					.expires(OffsetDateTime::now_utc() + Duration::weeks(52))
-					.into(),
-			);
-			subscriptions_number_to_delete_from += 1;
-		}
-		while req.cookie(&format!("subscriptions{}", subscriptions_number_to_delete_from)).is_some() {
-			response.remove_cookie(format!("subscriptions{}", subscriptions_number_to_delete_from));
-			subscriptions_number_to_delete_from += 1;
-		}
-	}
-	if filters.is_empty() {
-		response.remove_cookie("filters".to_string());
-		let mut filters_number = 1;
-		while req.cookie(&format!("filters{}", filters_number)).is_some() {
-			response.remove_cookie(format!("filters{}", filters_number));
-			filters_number += 1;
-		}
-	} else {
-		let mut filters_number_to_delete_from = 0;
-		for (filters_number, list) in join_until_size_limit(&filters).into_iter().enumerate() {
-			let filters_cookie = if filters_number == 0 {
-				"filters".to_string()
-			} else {
-				format!("filters{}", filters_number)
-			};
-			response.insert_cookie(
-				Cookie::build((filters_cookie, list))
-					.path("/")
-					.http_only(true)
-					.expires(OffsetDateTime::now_utc() + Duration::weeks(52))
-					.into(),
-			);
-			filters_number_to_delete_from += 1;
-		}
-		while req.cookie(&format!("filters{}", filters_number_to_delete_from)).is_some() {
-			response.remove_cookie(format!("filters{}", filters_number_to_delete_from));
-			filters_number_to_delete_from += 1;
-		}
-	}
-	Ok(response)
-}
-pub async fn wiki(req: Request<Body>) -> Result<Response<Body>, String> {
-	let sub = req.param("sub").unwrap_or_else(|| "reddit.com".to_string());
-	let quarantined = can_access_quarantine(&req, &sub);
-	if let Ok(random) = catch_random(&sub, "/wiki").await {
-		return Ok(random);
-	}
-	let page = req.param("page").unwrap_or_else(|| "index".to_string());
-	let path: String = format!("/r/{sub}/wiki/{page}.json?raw_json=1");
-	let url = req.uri().to_string();
-	match json(path, quarantined).await {
-		Ok(response) => Ok(template(&WikiTemplate {
-			sub,
-			wiki: rewrite_urls(response["data"]["content_html"].as_str().unwrap_or("<h3>Wiki not found</h3>")),
-			page,
-			prefs: Preferences::new(&req),
-			url,
-		})),
-		Err(msg) => {
-			if msg == "quarantined" || msg == "gated" {
-				Ok(quarantine(&req, sub, &msg))
-			} else {
-				error(req, &msg).await
-			}
-		}
-	}
-}
-pub async fn sidebar(req: Request<Body>) -> Result<Response<Body>, String> {
-	let sub = req.param("sub").unwrap_or_else(|| "reddit.com".to_string());
-	let quarantined = can_access_quarantine(&req, &sub);
-	if let Ok(random) = catch_random(&sub, "/about/sidebar").await {
-		return Ok(random);
-	}
-	let path: String = format!("/r/{sub}/about.json?raw_json=1");
-	let url = req.uri().to_string();
-	match json(path, quarantined).await {
-		Ok(response) => Ok(template(&WikiTemplate {
-			wiki: rewrite_urls(&val(&response, "description_html")),
-			sub,
-			page: "Sidebar".to_string(),
-			prefs: Preferences::new(&req),
-			url,
-		})),
-		Err(msg) => {
-			if msg == "quarantined" || msg == "gated" {
-				Ok(quarantine(&req, sub, &msg))
-			} else {
-				error(req, &msg).await
-			}
-		}
-	}
-}
-async fn subreddit(sub: &str, quarantined: bool) -> Result<Subreddit, String> {
-	let path: String = format!("/r/{sub}/about.json?raw_json=1");
-	let res = json(path, quarantined).await?;
-	let members: i64 = res["data"]["subscribers"].as_u64().unwrap_or_default() as i64;
-	let active: i64 = res["data"]["accounts_active"].as_u64().unwrap_or_default() as i64;
-	let community_icon: &str = res["data"]["community_icon"].as_str().unwrap_or_default();
-	let icon = if community_icon.is_empty() { val(&res, "icon_img") } else { community_icon.to_string() };
-	Ok(Subreddit {
-		name: val(&res, "display_name"),
-		title: val(&res, "title"),
-		description: val(&res, "public_description"),
-		info: rewrite_urls(&val(&res, "description_html")),
-		icon: format_url(&icon),
-		members: format_num(members),
-		active: format_num(active),
-		wiki: res["data"]["wiki_enabled"].as_bool().unwrap_or_default(),
-		nsfw: res["data"]["over18"].as_bool().unwrap_or_default(),
-	})
-}
-pub async fn rss(req: Request<Body>) -> Result<Response<Body>, String> {
-	if config::get_setting("REDLIB_ENABLE_RSS").is_none() {
-		return Ok(error(req, "RSS is disabled on this instance.").await.unwrap_or_default());
-	}
-	use hyper::header::CONTENT_TYPE;
-	use rss::{ChannelBuilder, Item};
-	let sub = req.param("sub").unwrap_or_default();
-	let post_sort = req.cookie("post_sort").map_or_else(|| "hot".to_string(), |c| c.value().to_string());
-	let sort = req.param("sort").unwrap_or_else(|| req.param("id").unwrap_or(post_sort));
-	let path = format!("/r/{sub}/{sort}.json?{}", req.uri().query().unwrap_or_default());
-	let subreddit = subreddit(&sub, false).await?;
-	let (posts, _) = Post::fetch(&path, false).await?;
-	let channel = ChannelBuilder::default()
-		.title(&subreddit.title)
-		.description(&subreddit.description)
-		.items(
-			posts
-				.into_iter()
-				.map(|post| Item {
-					title: Some(post.title.to_string()),
-					link: Some(format_url(&utils::get_post_url(&post))),
-					author: Some(post.author.name),
-					content: Some(rewrite_urls(&decode_html(&post.body).unwrap())),
-					pub_date: Some(DateTime::from_timestamp(post.created_ts as i64, 0).unwrap_or_default().to_rfc2822()),
-					description: Some(format!(
-						"<a href='{}{}'>Comments</a>",
-						config::get_setting("REDLIB_FULL_URL").unwrap_or_default(),
-						post.permalink
-					)),
-					..Default::default()
-				})
-				.collect::<Vec<_>>(),
-		)
-		.build();
-	let body = channel.to_string().into_bytes();
-	let mut res = Response::new(Body::from(body));
-	res.headers_mut().insert(CONTENT_TYPE, hyper::header::HeaderValue::from_static("application/rss+xml"));
-	Ok(res)
-}
-#[tokio::test(flavor = "multi_thread")]
-async fn test_fetching_subreddit() {
-	let subreddit = subreddit("rust", false).await;
-	assert!(subreddit.is_ok());
-}
-#[tokio::test(flavor = "multi_thread")]
-async fn test_gated_and_quarantined() {
-	let quarantined = subreddit("edgy", true).await;
-	assert!(quarantined.is_ok());
-	let gated = subreddit("drugs", true).await;
-	assert!(gated.is_ok());
-}

--- a/src/user.rs
+++ b//dev/null
@@ -1,141 +0,0 @@
-#![allow(clippy::cmp_owned)]
-use crate::client::json;
-use crate::server::RequestExt;
-use crate::utils::{error, filter_posts, format_url, get_filters, nsfw_landing, param, setting, template, Post, Preferences, User};
-use crate::{config, utils};
-use chrono::DateTime;
-use htmlescape::decode_html;
-use hyper::{Body, Request, Response};
-use rinja::Template;
-use time::{macros::format_description, OffsetDateTime};
-#[derive(Template)]
-#[template(path = "user.html")]
-struct UserTemplate {
-	user: User,
-	posts: Vec<Post>,
-	sort: (String, String),
-	ends: (String, String),
-	listing: String,
-	prefs: Preferences,
-	url: String,
-	redirect_url: String,
-	is_filtered: bool,
-	all_posts_filtered: bool,
-	all_posts_hidden_nsfw: bool,
-	no_posts: bool,
-}
-pub async fn profile(req: Request<Body>) -> Result<Response<Body>, String> {
-	let listing = req.param("listing").unwrap_or_else(|| "overview".to_string());
-	let path = format!(
-		"/user/{}/{listing}.json?{}&raw_json=1",
-		req.param("name").unwrap_or_else(|| "reddit".to_string()),
-		req.uri().query().unwrap_or_default(),
-	);
-	let url = String::from(req.uri().path_and_query().map_or("", |val| val.as_str()));
-	let redirect_url = url[1..].replace('?', "%3F").replace('&', "%26");
-	let sort = param(&path, "sort").unwrap_or_default();
-	let username = req.param("name").unwrap_or_default();
-	let user = user(&username).await.unwrap_or_default();
-	let req_url = req.uri().to_string();
-	if user.nsfw && crate::utils::should_be_nsfw_gated(&req, &req_url) {
-		return Ok(nsfw_landing(req, req_url).await.unwrap_or_default());
-	}
-	let filters = get_filters(&req);
-	if filters.contains(&["u_", &username].concat()) {
-		Ok(template(&UserTemplate {
-			user,
-			posts: Vec::new(),
-			sort: (sort, param(&path, "t").unwrap_or_default()),
-			ends: (param(&path, "after").unwrap_or_default(), String::new()),
-			listing,
-			prefs: Preferences::new(&req),
-			url,
-			redirect_url,
-			is_filtered: true,
-			all_posts_filtered: false,
-			all_posts_hidden_nsfw: false,
-			no_posts: false,
-		}))
-	} else {
-		match Post::fetch(&path, false).await {
-			Ok((mut posts, after)) => {
-				let (_, all_posts_filtered) = filter_posts(&mut posts, &filters);
-				let no_posts = posts.is_empty();
-				let all_posts_hidden_nsfw = !no_posts && (posts.iter().all(|p| p.flags.nsfw) && setting(&req, "show_nsfw") != "on");
-				Ok(template(&UserTemplate {
-					user,
-					posts,
-					sort: (sort, param(&path, "t").unwrap_or_default()),
-					ends: (param(&path, "after").unwrap_or_default(), after),
-					listing,
-					prefs: Preferences::new(&req),
-					url,
-					redirect_url,
-					is_filtered: false,
-					all_posts_filtered,
-					all_posts_hidden_nsfw,
-					no_posts,
-				}))
-			}
-			Err(msg) => error(req, &msg).await,
-		}
-	}
-}
-async fn user(name: &str) -> Result<User, String> {
-	let path: String = format!("/user/{name}/about.json?raw_json=1");
-	json(path, false).await.map(|res| {
-		let created_unix = res["data"]["created"].as_f64().unwrap_or(0.0).round() as i64;
-		let created = OffsetDateTime::from_unix_timestamp(created_unix).unwrap_or(OffsetDateTime::UNIX_EPOCH);
-		let about = |item| res["data"]["subreddit"][item].as_str().unwrap_or_default().to_string();
-		User {
-			name: res["data"]["name"].as_str().unwrap_or(name).to_owned(),
-			title: about("title"),
-			icon: format_url(&about("icon_img")),
-			karma: res["data"]["total_karma"].as_i64().unwrap_or(0),
-			created: created.format(format_description!("[month repr:short] [day] '[year repr:last_two]")).unwrap_or_default(),
-			banner: about("banner_img"),
-			description: about("public_description"),
-			nsfw: res["data"]["subreddit"]["over_18"].as_bool().unwrap_or_default(),
-		}
-	})
-}
-pub async fn rss(req: Request<Body>) -> Result<Response<Body>, String> {
-	if config::get_setting("REDLIB_ENABLE_RSS").is_none() {
-		return Ok(error(req, "RSS is disabled on this instance.").await.unwrap_or_default());
-	}
-	use crate::utils::rewrite_urls;
-	use hyper::header::CONTENT_TYPE;
-	use rss::{ChannelBuilder, Item};
-	let user_str = req.param("name").unwrap_or_default();
-	let listing = req.param("listing").unwrap_or_else(|| "overview".to_string());
-	let path = format!("/user/{user_str}/{listing}.json?{}&raw_json=1", req.uri().query().unwrap_or_default(),);
-	let user_obj = user(&user_str).await.unwrap_or_default();
-	let (posts, _) = Post::fetch(&path, false).await?;
-	let channel = ChannelBuilder::default()
-		.title(user_str)
-		.description(user_obj.description)
-		.items(
-			posts
-				.into_iter()
-				.map(|post| Item {
-					title: Some(post.title.to_string()),
-					link: Some(format_url(&utils::get_post_url(&post))),
-					author: Some(post.author.name),
-					pub_date: Some(DateTime::from_timestamp(post.created_ts as i64, 0).unwrap_or_default().to_rfc2822()),
-					content: Some(rewrite_urls(&decode_html(&post.body).unwrap())),
-					..Default::default()
-				})
-				.collect::<Vec<_>>(),
-		)
-		.build();
-	let body = channel.to_string().into_bytes();
-	let mut res = Response::new(Body::from(body));
-	res.headers_mut().insert(CONTENT_TYPE, hyper::header::HeaderValue::from_static("application/rss+xml"));
-	Ok(res)
-}
-#[tokio::test(flavor = "multi_thread")]
-async fn test_fetching_user() {
-	let user = user("spez").await;
-	assert!(user.is_ok());
-	assert!(user.unwrap().karma > 100);
-}

--- a/src/utils.rs
+++ b/src/utils.rs
@@ -1,58 +1,50 @@
 #![allow(dead_code)]
-#![allow(clippy::cmp_owned)]
-use crate::config::{self, get_setting};
+use crate::config::get_setting;
 use crate::{client::json, server::RequestExt};
+use askama::Template;
 use cookie::Cookie;
 use hyper::{Body, Request, Response};
-use libflate::deflate::{Decoder, Encoder};
 use log::error;
 use once_cell::sync::Lazy;
 use regex::Regex;
-use revision::revisioned;
-use rinja::Template;
 use rust_embed::RustEmbed;
-use serde::{Deserialize, Deserializer, Serialize, Serializer};
 use serde_json::Value;
-use serde_json_path::{JsonPath, JsonPathExt};
 use std::collections::{HashMap, HashSet};
 use std::env;
-use std::io::{Read, Write};
 use std::str::FromStr;
-use std::string::ToString;
 use time::{macros::format_description, Duration, OffsetDateTime};
 use url::Url;
 #[macro_export]
 macro_rules! dbg_msg {
 	($x:expr) => {
 		#[cfg(debug_assertions)]
 		eprintln!("{}:{}: {}", file!(), line!(), $x.to_string())
 	};
 	($($x:expr),+) => {
 		#[cfg(debug_assertions)]
 		dbg_msg!(format!($($x),+))
 	};
 }
 #[derive(PartialEq, Eq)]
 pub enum ResourceType {
 	Subreddit,
 	User,
 	Post,
 }
-#[derive(Serialize)]
 pub struct Flair {
 	pub flair_parts: Vec<FlairPart>,
 	pub text: String,
 	pub background_color: String,
 	pub foreground_color: String,
 }
-#[derive(Clone, Serialize)]
+#[derive(Clone)]
 pub struct FlairPart {
 	pub flair_part_type: String,
 	pub value: String,
 }
 impl FlairPart {
 	pub fn parse(flair_type: &str, rich_flair: Option<&Vec<Value>>, text_flair: Option<&str>) -> Vec<Self> {
 		match flair_type {
 			"richtext" => match rich_flair {
 				Some(rich) => rich
 					.iter()
@@ -74,27 +66,25 @@
 				Some(text) => vec![Self {
 					flair_part_type: "text".to_string(),
 					value: text.to_string(),
 				}],
 				None => Vec::new(),
 			},
 			_ => Vec::new(),
 		}
 	}
 }
-#[derive(Serialize)]
 pub struct Author {
 	pub name: String,
 	pub flair: Flair,
 	pub distinguished: String,
 }
-#[derive(Serialize)]
 pub struct Poll {
 	pub poll_options: Vec<PollOption>,
 	pub voting_end_timestamp: (String, String),
 	pub total_vote_count: u64,
 }
 impl Poll {
 	pub fn parse(poll_data: &Value) -> Option<Self> {
 		poll_data.as_object()?;
 		let total_vote_count = poll_data["total_vote_count"].as_u64()?;
 		let voting_end_timestamp = time(poll_data["voting_end_timestamp"].as_f64()? / 1000.0);
@@ -102,21 +92,20 @@
 		Some(Self {
 			poll_options,
 			voting_end_timestamp,
 			total_vote_count,
 		})
 	}
 	pub fn most_votes(&self) -> u64 {
 		self.poll_options.iter().filter_map(|o| o.vote_count).max().unwrap_or(0)
 	}
 }
-#[derive(Serialize)]
 pub struct PollOption {
 	pub id: u64,
 	pub text: String,
 	pub vote_count: Option<u64>,
 }
 impl PollOption {
 	pub fn parse(options: &Value) -> Option<Vec<Self>> {
 		Some(
 			options
 				.as_array()?
@@ -124,34 +113,32 @@
 				.filter_map(|option| {
 					let id = option["id"].as_str()?.parse::<u64>().ok()?;
 					let text = option["text"].as_str()?.to_owned();
 					let vote_count = option["vote_count"].as_u64();
 					Some(Self { id, text, vote_count })
 				})
 				.collect::<Vec<Self>>(),
 		)
 	}
 }
-#[derive(Serialize)]
 pub struct Flags {
 	pub spoiler: bool,
 	pub nsfw: bool,
 	pub stickied: bool,
 }
-#[derive(Debug, Serialize)]
+#[derive(Debug)]
 pub struct Media {
 	pub url: String,
 	pub alt_url: String,
 	pub width: i64,
 	pub height: i64,
 	pub poster: String,
-	pub download_name: String,
 }
 impl Media {
 	pub async fn parse(data: &Value) -> (String, Self, Vec<GalleryMedia>) {
 		let mut gallery = Vec::new();
 		let data_preview = &data["preview"]["reddit_video_preview"];
 		let secure_media = &data["secure_media"]["reddit_video"];
 		let crosspost_parent_media = &data["crosspost_parent_list"][0]["secure_media"]["reddit_video"];
 		let (post_type, url_val, alt_url_val) = if data_preview["fallback_url"].is_string() {
 			(
 				if data_preview["is_gif"].as_bool().unwrap_or(false) { "gif" } else { "video" },
@@ -180,55 +167,40 @@
 					("image", &data["url"], None)
 				} else {
 					("image", &preview["source"]["url"], None)
 				}
 			}
 		} else if data["is_self"].as_bool().unwrap_or_default() {
 			("self", &data["permalink"], None)
 		} else if data["is_gallery"].as_bool().unwrap_or_default() {
 			gallery = GalleryMedia::parse(&data["gallery_data"]["items"], &data["media_metadata"]);
 			("gallery", &data["url"], None)
-		} else if data["crosspost_parent_list"][0]["is_gallery"].as_bool().unwrap_or_default() {
-			gallery = GalleryMedia::parse(
-				&data["crosspost_parent_list"][0]["gallery_data"]["items"],
-				&data["crosspost_parent_list"][0]["media_metadata"],
-			);
-			("gallery", &data["url"], None)
 		} else if data["is_reddit_media_domain"].as_bool().unwrap_or_default() && data["domain"] == "i.redd.it" {
 			("image", &data["url"], None)
 		} else {
 			("link", &data["url"], None)
 		};
 		let source = &data["preview"]["images"][0]["source"];
 		let alt_url = alt_url_val.map_or(String::new(), |val| format_url(val.as_str().unwrap_or_default()));
-		let download_name = if post_type == "image" || post_type == "gif" || post_type == "video" {
-			let permalink_base = url_path_basename(data["permalink"].as_str().unwrap_or_default());
-			let media_url_base = url_path_basename(url_val.as_str().unwrap_or_default());
-			format!("redlib_{permalink_base}_{media_url_base}")
-		} else {
-			String::new()
-		};
 		(
 			post_type.to_string(),
 			Self {
 				url: format_url(url_val.as_str().unwrap_or_default()),
 				alt_url,
 				width: source["width"].as_i64().unwrap_or_default(),
 				height: source["height"].as_i64().unwrap_or_default(),
 				poster: format_url(source["url"].as_str().unwrap_or_default()),
-				download_name,
 			},
 			gallery,
 		)
 	}
 }
-#[derive(Serialize)]
 pub struct GalleryMedia {
 	pub url: String,
 	pub width: i64,
 	pub height: i64,
 	pub caption: String,
 	pub outbound_url: String,
 }
 impl GalleryMedia {
 	fn parse(items: &Value, metadata: &Value) -> Vec<Self> {
 		items
@@ -248,47 +220,44 @@
 					url: format_url(url),
 					width: image["x"].as_i64().unwrap_or_default(),
 					height: image["y"].as_i64().unwrap_or_default(),
 					caption: item["caption"].as_str().unwrap_or_default().to_string(),
 					outbound_url: item["outbound_url"].as_str().unwrap_or_default().to_string(),
 				}
 			})
 			.collect::<Vec<Self>>()
 	}
 }
-#[derive(Serialize)]
 pub struct Post {
 	pub id: String,
 	pub title: String,
 	pub community: String,
 	pub body: String,
 	pub author: Author,
 	pub permalink: String,
-	pub link_title: String,
 	pub poll: Option<Poll>,
 	pub score: (String, String),
 	pub upvote_ratio: i64,
 	pub post_type: String,
 	pub flair: Flair,
 	pub flags: Flags,
 	pub thumbnail: Media,
 	pub media: Media,
 	pub domain: String,
 	pub rel_time: String,
 	pub created: String,
 	pub created_ts: u64,
 	pub num_duplicates: u64,
 	pub comments: (String, String),
 	pub gallery: Vec<GalleryMedia>,
 	pub awards: Awards,
 	pub nsfw: bool,
-	pub out_url: Option<String>,
 	pub ws_url: String,
 }
 impl Post {
 	pub async fn fetch(path: &str, quarantine: bool) -> Result<(Vec<Self>, String), String> {
 		let res = match json(path.to_string(), quarantine).await {
 			Ok(response) => response,
 			Err(msg) => return Err(msg),
 		};
 		let Some(post_list) = res["data"]["children"].as_array() else {
 			return Err("No posts found".to_string());
@@ -332,21 +301,20 @@
 					format_num(score)
 				},
 				upvote_ratio: ratio as i64,
 				post_type,
 				thumbnail: Media {
 					url: format_url(val(post, "thumbnail").as_str()),
 					alt_url: String::new(),
 					width: data["thumbnail_width"].as_i64().unwrap_or_default(),
 					height: data["thumbnail_height"].as_i64().unwrap_or_default(),
 					poster: String::new(),
-					download_name: String::new(),
 				},
 				media,
 				domain: val(post, "domain"),
 				flair: Flair {
 					flair_parts: FlairPart::parse(
 						data["link_flair_type"].as_str().unwrap_or_default(),
 						data["link_flair_richtext"].as_array(),
 						data["link_flair_text"].as_str(),
 					),
 					text: val(post, "link_flair_text"),
@@ -356,34 +324,34 @@
 					} else {
 						"white".to_string()
 					},
 				},
 				flags: Flags {
 					spoiler: data["spoiler"].as_bool().unwrap_or_default(),
 					nsfw: data["over_18"].as_bool().unwrap_or_default(),
 					stickied: data["stickied"].as_bool().unwrap_or_default() || data["pinned"].as_bool().unwrap_or_default(),
 				},
 				permalink: val(post, "permalink"),
-				link_title: val(post, "link_title"),
 				poll: Poll::parse(&data["poll_data"]),
 				rel_time,
 				created,
 				created_ts,
 				num_duplicates: post["data"]["num_duplicates"].as_u64().unwrap_or(0),
 				comments: format_num(data["num_comments"].as_i64().unwrap_or_default()),
 				gallery,
 				awards,
 				nsfw: post["data"]["over_18"].as_bool().unwrap_or_default(),
 				ws_url: val(post, "websocket_url"),
-				out_url: post["data"]["url_overridden_by_dest"].as_str().map(|a| a.to_string()),
 			});
 		}
+		posts.sort_by(|a, b| b.created_ts.cmp(&a.created_ts));
+		posts.sort_by(|a, b| b.flags.stickied.cmp(&a.flags.stickied));
 		Ok((posts, res["data"]["after"].as_str().unwrap_or_default().to_string()))
 	}
 }
 #[derive(Template)]
 #[template(path = "comment.html")]
 pub struct Comment {
 	pub id: String,
 	pub kind: String,
 	pub parent_id: String,
 	pub parent_kind: String,
@@ -396,43 +364,42 @@
 	pub created: String,
 	pub edited: (String, String),
 	pub replies: Vec<Comment>,
 	pub highlighted: bool,
 	pub awards: Awards,
 	pub collapsed: bool,
 	pub is_filtered: bool,
 	pub more_count: i64,
 	pub prefs: Preferences,
 }
-#[derive(Default, Clone, Serialize)]
+#[derive(Default, Clone)]
 pub struct Award {
 	pub name: String,
 	pub icon_url: String,
 	pub description: String,
 	pub count: i64,
 }
 impl std::fmt::Display for Award {
 	fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
 		write!(f, "{} {} {}", self.name, self.icon_url, self.description)
 	}
 }
-#[derive(Serialize)]
 pub struct Awards(pub Vec<Award>);
 impl std::ops::Deref for Awards {
 	type Target = Vec<Award>;
 	fn deref(&self) -> &Self::Target {
 		&self.0
 	}
 }
 impl std::fmt::Display for Awards {
 	fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-		self.iter().try_fold((), |_, award| writeln!(f, "{award}"))
+		self.iter().fold(Ok(()), |result, award| result.and_then(|()| writeln!(f, "{award}")))
 	}
 }
 impl Awards {
 	pub fn parse(items: &Value) -> Self {
 		let parsed = items.as_array().unwrap_or(&Vec::new()).iter().fold(Vec::new(), |mut awards, item| {
 			let name = item["name"].as_str().unwrap_or_default().to_string();
 			let icon_url = format_url(item["resized_icons"][0]["url"].as_str().unwrap_or_default());
 			let description = item["description"].as_str().unwrap_or_default().to_string();
 			let count: i64 = i64::from_str(&item["count"].to_string()).unwrap_or(1);
 			awards.push(Award {
@@ -447,27 +414,20 @@
 	}
 }
 #[derive(Template)]
 #[template(path = "error.html")]
 pub struct ErrorTemplate {
 	pub msg: String,
 	pub prefs: Preferences,
 	pub url: String,
 }
 #[derive(Template)]
-#[template(path = "info.html")]
-pub struct InfoTemplate {
-	pub msg: String,
-	pub prefs: Preferences,
-	pub url: String,
-}
-#[derive(Template)]
 #[template(path = "nsfwlanding.html")]
 pub struct NSFWLandingTemplate {
 	pub res: String,
 	pub res_type: ResourceType,
 	pub prefs: Preferences,
 	pub url: String,
 }
 #[derive(Default)]
 pub struct User {
 	pub name: String,
@@ -492,86 +452,42 @@
 	pub nsfw: bool,
 }
 #[derive(serde::Deserialize)]
 pub struct Params {
 	pub t: Option<String>,
 	pub q: Option<String>,
 	pub sort: Option<String>,
 	pub after: Option<String>,
 	pub before: Option<String>,
 }
-#[derive(Default, Serialize, Deserialize, Debug, PartialEq, Eq)]
-#[revisioned(revision = 1)]
+#[derive(Default)]
 pub struct Preferences {
-	#[revision(start = 1)]
-	#[serde(skip_serializing, skip_deserializing)]
 	pub available_themes: Vec<String>,
-	#[revision(start = 1)]
 	pub theme: String,
-	#[revision(start = 1)]
 	pub front_page: String,
-	#[revision(start = 1)]
 	pub layout: String,
-	#[revision(start = 1)]
 	pub wide: String,
-	#[revision(start = 1)]
 	pub blur_spoiler: String,
-	#[revision(start = 1)]
 	pub show_nsfw: String,
-	#[revision(start = 1)]
 	pub blur_nsfw: String,
-	#[revision(start = 1)]
 	pub hide_hls_notification: String,
-	#[revision(start = 1)]
-	pub video_quality: String,
-	#[revision(start = 1)]
 	pub hide_sidebar_and_summary: String,
-	#[revision(start = 1)]
 	pub use_hls: String,
-	#[revision(start = 1)]
 	pub autoplay_videos: String,
-	#[revision(start = 1)]
 	pub fixed_navbar: String,
-	#[revision(start = 1)]
 	pub disable_visit_reddit_confirmation: String,
-	#[revision(start = 1)]
 	pub comment_sort: String,
-	#[revision(start = 1)]
 	pub post_sort: String,
-	#[revision(start = 1)]
-	#[serde(serialize_with = "serialize_vec_with_plus", deserialize_with = "deserialize_vec_with_plus")]
 	pub subscriptions: Vec<String>,
-	#[revision(start = 1)]
-	#[serde(serialize_with = "serialize_vec_with_plus", deserialize_with = "deserialize_vec_with_plus")]
 	pub filters: Vec<String>,
-	#[revision(start = 1)]
 	pub hide_awards: String,
-	#[revision(start = 1)]
 	pub hide_score: String,
-	#[revision(start = 1)]
-	pub remove_default_feeds: String,
-}
-fn serialize_vec_with_plus<S>(vec: &[String], serializer: S) -> Result<S::Ok, S::Error>
-where
-	S: Serializer,
-{
-	serializer.serialize_str(&vec.join("+"))
-}
-fn deserialize_vec_with_plus<'de, D>(deserializer: D) -> Result<Vec<String>, D::Error>
-where
-	D: Deserializer<'de>,
-{
-	let string = String::deserialize(deserializer)?;
-	if string.is_empty() {
-		return Ok(Vec::new());
-	}
-	Ok(string.split('+').map(|s| s.to_string()).collect())
 }
 #[derive(RustEmbed)]
 #[folder = "static/themes/"]
 #[include = "*.css"]
 pub struct ThemeAssets;
 impl Preferences {
 	pub fn new(req: &Request<Body>) -> Self {
 		let mut themes = vec!["system".to_string()];
 		for file in ThemeAssets::iter() {
 			let chunks: Vec<&str> = file.as_ref().split(".css").collect();
@@ -582,56 +498,31 @@
 			theme: setting(req, "theme"),
 			front_page: setting(req, "front_page"),
 			layout: setting(req, "layout"),
 			wide: setting(req, "wide"),
 			blur_spoiler: setting(req, "blur_spoiler"),
 			show_nsfw: setting(req, "show_nsfw"),
 			hide_sidebar_and_summary: setting(req, "hide_sidebar_and_summary"),
 			blur_nsfw: setting(req, "blur_nsfw"),
 			use_hls: setting(req, "use_hls"),
 			hide_hls_notification: setting(req, "hide_hls_notification"),
-			video_quality: setting(req, "video_quality"),
 			autoplay_videos: setting(req, "autoplay_videos"),
 			fixed_navbar: setting_or_default(req, "fixed_navbar", "on".to_string()),
 			disable_visit_reddit_confirmation: setting(req, "disable_visit_reddit_confirmation"),
 			comment_sort: setting(req, "comment_sort"),
 			post_sort: setting(req, "post_sort"),
 			subscriptions: setting(req, "subscriptions").split('+').map(String::from).filter(|s| !s.is_empty()).collect(),
 			filters: setting(req, "filters").split('+').map(String::from).filter(|s| !s.is_empty()).collect(),
 			hide_awards: setting(req, "hide_awards"),
 			hide_score: setting(req, "hide_score"),
-			remove_default_feeds: setting(req, "remove_default_feeds"),
 		}
 	}
-	pub fn to_urlencoded(&self) -> Result<String, String> {
-		serde_urlencoded::to_string(self).map_err(|e| e.to_string())
-	}
-	pub fn to_bincode(&self) -> Result<Vec<u8>, String> {
-		bincode::serialize(self).map_err(|e| e.to_string())
-	}
-	pub fn to_compressed_bincode(&self) -> Result<Vec<u8>, String> {
-		deflate_compress(self.to_bincode()?)
-	}
-	pub fn to_bincode_str(&self) -> Result<String, String> {
-		Ok(base2048::encode(&self.to_compressed_bincode()?))
-	}
-}
-pub fn deflate_compress(i: Vec<u8>) -> Result<Vec<u8>, String> {
-	let mut e = Encoder::new(Vec::new());
-	e.write_all(&i).map_err(|e| e.to_string())?;
-	e.finish().into_result().map_err(|e| e.to_string())
-}
-pub fn deflate_decompress(i: Vec<u8>) -> Result<Vec<u8>, String> {
-	let mut decoder = Decoder::new(&i[..]);
-	let mut out = Vec::new();
-	decoder.read_to_end(&mut out).map_err(|e| format!("Failed to read from gzip decoder: {}", e))?;
-	Ok(out)
 }
 pub fn get_filters(req: &Request<Body>) -> HashSet<String> {
 	setting(req, "filters").split('+').map(String::from).filter(|s| !s.is_empty()).collect::<HashSet<String>>()
 }
 pub fn filter_posts(posts: &mut Vec<Post>, filters: &HashSet<String>) -> (u64, bool) {
 	let lb: u64 = posts.len().try_into().unwrap_or(0);
 	if posts.is_empty() {
 		(0, false)
 	} else {
 		posts.retain(|p| !(filters.contains(&p.community) || filters.contains(&["u_", &p.author.name].concat())));
@@ -647,29 +538,21 @@
 	let created_ts = post["data"]["created_utc"].as_f64().unwrap_or_default().round() as u64;
 	let awards: Awards = Awards::parse(&post["data"]["all_awardings"]);
 	let permalink = val(post, "permalink");
 	let poll = Poll::parse(&post["data"]["poll_data"]);
 	let body = if val(post, "removed_by_category") == "moderator" {
 		format!(
 			"<div class=\"md\"><p>[removed] â€” <a href=\"https://{}{permalink}\">view removed post</a></p></div>",
 			get_setting("REDLIB_PUSHSHIFT_FRONTEND").unwrap_or_else(|| String::from(crate::config::DEFAULT_PUSHSHIFT_FRONTEND)),
 		)
 	} else {
-		let selftext = val(post, "selftext");
-		if selftext.contains("```") {
-			let mut html_output = String::new();
-			let parser = pulldown_cmark::Parser::new(&selftext);
-			pulldown_cmark::html::push_html(&mut html_output, parser);
-			rewrite_urls(&html_output)
-		} else {
-			rewrite_urls(&val(post, "selftext_html"))
-		}
+		rewrite_urls(&val(post, "selftext_html"))
 	};
 	Post {
 		id: val(post, "id"),
 		title: val(post, "title"),
 		community: val(post, "subreddit"),
 		body,
 		author: Author {
 			name: val(post, "author"),
 			flair: Flair {
 				flair_parts: FlairPart::parse(
@@ -677,33 +560,31 @@
 					post["data"]["author_flair_richtext"].as_array(),
 					post["data"]["author_flair_text"].as_str(),
 				),
 				text: val(post, "link_flair_text"),
 				background_color: val(post, "author_flair_background_color"),
 				foreground_color: val(post, "author_flair_text_color"),
 			},
 			distinguished: val(post, "distinguished"),
 		},
 		permalink,
-		link_title: val(post, "link_title"),
 		poll,
 		score: format_num(score),
 		upvote_ratio: ratio as i64,
 		post_type,
 		media,
 		thumbnail: Media {
 			url: format_url(val(post, "thumbnail").as_str()),
 			alt_url: String::new(),
 			width: post["data"]["thumbnail_width"].as_i64().unwrap_or_default(),
 			height: post["data"]["thumbnail_height"].as_i64().unwrap_or_default(),
 			poster: String::new(),
-			download_name: String::new(),
 		},
 		flair: Flair {
 			flair_parts: FlairPart::parse(
 				post["data"]["link_flair_type"].as_str().unwrap_or_default(),
 				post["data"]["link_flair_richtext"].as_array(),
 				post["data"]["link_flair_text"].as_str(),
 			),
 			text: val(post, "link_flair_text"),
 			background_color: val(post, "link_flair_background_color"),
 			foreground_color: if val(post, "link_flair_text_color") == "dark" {
@@ -720,89 +601,61 @@
 		domain: val(post, "domain"),
 		rel_time,
 		created,
 		created_ts,
 		num_duplicates: post["data"]["num_duplicates"].as_u64().unwrap_or(0),
 		comments: format_num(post["data"]["num_comments"].as_i64().unwrap_or_default()),
 		gallery,
 		awards,
 		nsfw: post["data"]["over_18"].as_bool().unwrap_or_default(),
 		ws_url: val(post, "websocket_url"),
-		out_url: post["data"]["url_overridden_by_dest"].as_str().map(|a| a.to_string()),
 	}
 }
 pub fn param(path: &str, value: &str) -> Option<String> {
 	Some(
 		Url::parse(format!("https://libredd.it/{path}").as_str())
 			.ok()?
 			.query_pairs()
 			.into_owned()
 			.collect::<HashMap<_, _>>()
 			.get(value)?
 			.clone(),
 	)
 }
 pub fn setting(req: &Request<Body>, name: &str) -> String {
-	if name == "subscriptions" && req.cookie("subscriptions").is_some() {
-		let mut subscriptions = String::new();
-		if req.cookie("subscriptions").is_some() {
-			subscriptions.push_str(req.cookie("subscriptions").unwrap().value());
-		}
-		let mut subscriptions_number = 1;
-		while req.cookie(&format!("subscriptions{}", subscriptions_number)).is_some() {
-			subscriptions.push_str(req.cookie(&format!("subscriptions{}", subscriptions_number)).unwrap().value());
-			subscriptions_number += 1;
-		}
-		subscriptions
-	}
-	else if name == "filters" && req.cookie("filters").is_some() {
-		let mut filters = String::new();
-		if req.cookie("filters").is_some() {
-			filters.push_str(req.cookie("filters").unwrap().value());
-		}
-		let mut filters_number = 1;
-		while req.cookie(&format!("filters{}", filters_number)).is_some() {
-			filters.push_str(req.cookie(&format!("filters{}", filters_number)).unwrap().value());
-			filters_number += 1;
-		}
-		filters
-	}
-	else {
-		req
-			.cookie(name)
-			.unwrap_or_else(|| {
-				if let Some(default) = get_setting(&format!("REDLIB_DEFAULT_{}", name.to_uppercase())) {
-					Cookie::new(name, default)
-				} else {
-					Cookie::from(name)
-				}
-			})
-			.value()
-			.to_string()
-	}
+	req
+		.cookie(name)
+		.unwrap_or_else(|| {
+			if let Some(default) = get_setting(&format!("REDLIB_DEFAULT_{}", name.to_uppercase())) {
+				Cookie::new(name, default)
+			} else {
+				Cookie::from(name)
+			}
+		})
+		.value()
+		.to_string()
 }
 pub fn setting_or_default(req: &Request<Body>, name: &str, default: String) -> String {
 	let value = setting(req, name);
 	if value.is_empty() {
 		default
 	} else {
 		value
 	}
 }
 pub async fn catch_random(sub: &str, additional: &str) -> Result<Response<Body>, String> {
 	if sub == "random" || sub == "randnsfw" {
-		Ok(redirect(&format!(
-			"/r/{}{additional}",
-			json(format!("/r/{sub}/about.json?raw_json=1"), false).await?["data"]["display_name"]
-				.as_str()
-				.unwrap_or_default()
-		)))
+		let new_sub = json(format!("/r/{sub}/about.json?raw_json=1"), false).await?["data"]["display_name"]
+			.as_str()
+			.unwrap_or_default()
+			.to_string();
+		Ok(redirect(&format!("/r/{new_sub}{additional}")))
 	} else {
 		Err("No redirect needed".to_string())
 	}
 }
 static REGEX_URL_WWW: Lazy<Regex> = Lazy::new(|| Regex::new(r"https?://www\.reddit\.com/(.*)").unwrap());
 static REGEX_URL_OLD: Lazy<Regex> = Lazy::new(|| Regex::new(r"https?://old\.reddit\.com/(.*)").unwrap());
 static REGEX_URL_NP: Lazy<Regex> = Lazy::new(|| Regex::new(r"https?://np\.reddit\.com/(.*)").unwrap());
 static REGEX_URL_PLAIN: Lazy<Regex> = Lazy::new(|| Regex::new(r"https?://reddit\.com/(.*)").unwrap());
 static REGEX_URL_VIDEOS: Lazy<Regex> = Lazy::new(|| Regex::new(r"https?://v\.redd\.it/(.*)/DASH_([0-9]{2,4}(\.mp4|$|\?source=fallback))").unwrap());
 static REGEX_URL_VIDEOS_HLS: Lazy<Regex> = Lazy::new(|| Regex::new(r"https?://v\.redd\.it/(.+)/(HLSPlaylist\.m3u8.*)$").unwrap());
@@ -858,131 +711,74 @@
 				"emoji.redditmedia.com" => capture(&REGEX_URL_EMOJI, "/emoji/", 2),
 				"preview.redd.it" => capture(&REGEX_URL_PREVIEW, "/preview/pre/", 1),
 				"external-preview.redd.it" => capture(&REGEX_URL_EXTERNAL_PREVIEW, "/preview/external-pre/", 1),
 				"styles.redditmedia.com" => capture(&REGEX_URL_STYLES, "/style/", 1),
 				"www.redditstatic.com" => capture(&REGEX_URL_STATIC_MEDIA, "/static/", 1),
 				_ => url.to_string(),
 			}
 		})
 	}
 }
-static REGEX_BULLET: Lazy<Regex> = Lazy::new(|| Regex::new(r"(?m)^- (.*)$").unwrap());
-static REGEX_BULLET_CONSECUTIVE_LINES: Lazy<Regex> = Lazy::new(|| Regex::new(r"</ul>\n<ul>").unwrap());
-pub fn render_bullet_lists(input_text: &str) -> String {
-	let text1 = REGEX_BULLET.replace_all(input_text, "<ul><li>$1</li></ul>").to_string();
-	REGEX_BULLET_CONSECUTIVE_LINES.replace_all(&text1, "").to_string()
-}
 static REDDIT_REGEX: Lazy<Regex> = Lazy::new(|| Regex::new(r#"href="(https|http|)://(www\.|old\.|np\.|amp\.|new\.|)(reddit\.com|redd\.it)/"#).unwrap());
-static REDDIT_PREVIEW_REGEX: Lazy<Regex> = Lazy::new(|| Regex::new(r"https?://(external-preview|preview|i)\.redd\.it(.*)").unwrap());
+static REDDIT_PREVIEW_REGEX: Lazy<Regex> = Lazy::new(|| Regex::new(r"https?://(external-preview|preview|i)\.redd\.it(.*)[^?]").unwrap());
 static REDDIT_EMOJI_REGEX: Lazy<Regex> = Lazy::new(|| Regex::new(r"https?://(www|).redditstatic\.com/(.*)").unwrap());
 static REDLIB_PREVIEW_LINK_REGEX: Lazy<Regex> = Lazy::new(|| Regex::new(r#"/(img|preview/)(pre|external-pre)?/(.*?)>"#).unwrap());
 static REDLIB_PREVIEW_TEXT_REGEX: Lazy<Regex> = Lazy::new(|| Regex::new(r">(.*?)</a>").unwrap());
 pub fn rewrite_urls(input_text: &str) -> String {
 	let mut text1 =
-		REDDIT_REGEX.replace_all(input_text, r#"href="/"#).to_string();
-	loop {
-		if REDDIT_EMOJI_REGEX.find(&text1).is_none() {
-			break;
-		} else {
-			text1 = REDDIT_EMOJI_REGEX
-				.replace_all(&text1, format_url(REDDIT_EMOJI_REGEX.find(&text1).map(|x| x.as_str()).unwrap_or_default()))
-				.to_string()
-		}
-	}
-	text1 = text1.replace("%5C", "").replace("\\_", "_");
+		REDDIT_REGEX.replace_all(input_text, r#"href="/"#)
+			.to_string();
+	text1 = REDDIT_EMOJI_REGEX
+		.replace_all(&text1, format_url(REDDIT_EMOJI_REGEX.find(&text1).map(|x| x.as_str()).unwrap_or_default()))
+		.to_string()
+		.replace("%5C", "")
+		.replace("\\_", "_");
 	loop {
 		if REDDIT_PREVIEW_REGEX.find(&text1).is_none() {
 			return text1;
 		} else {
 			let formatted_url = format_url(REDDIT_PREVIEW_REGEX.find(&text1).map(|x| x.as_str()).unwrap_or_default());
-			let image_url = REDLIB_PREVIEW_LINK_REGEX.find(&formatted_url).map_or("", |m| m.as_str());
-			let mut image_caption = REDLIB_PREVIEW_TEXT_REGEX.find(&formatted_url).map_or("", |m| m.as_str());
+			let image_url = REDLIB_PREVIEW_LINK_REGEX.find(&formatted_url).map_or("", |m| m.as_str()).to_string();
+			let mut image_caption = REDLIB_PREVIEW_TEXT_REGEX.find(&formatted_url).map_or("", |m| m.as_str()).to_string();
 			/* As long as image_caption isn't empty remove first and last four characters of image_text to leave us with just the text in the caption without any HTML.
 			This makes it possible to enclose it in a <figcaption> later on without having stray HTML breaking it */
 			if !image_caption.is_empty() {
-				image_caption = &image_caption[1..image_caption.len() - 4];
+				image_caption = image_caption[1..image_caption.len() - 4].to_string();
 			}
-			let image_to_replace = format!("<p><a href=\"{image_url}{image_caption}</a></p>");
+			let image_to_replace = format!("<a href=\"{image_url}{image_caption}</a>");
+			let mut _image_replacement = String::new();
 			/* We don't want to show a caption that's just the image's link, so we check if we find a Reddit preview link within the image's caption.
 			If we don't find one we must have actual text, so we include a <figcaption> block that contains it.
 			Otherwise we don't include the <figcaption> block as we don't need it. */
-			let _image_replacement = if REDDIT_PREVIEW_REGEX.find(image_caption).is_none() {
-				format!(
-					"<figure><a href=\"{image_url}<img loading=\"lazy\" src=\"{image_url}</a><figcaption>{}</figcaption></figure>",
-					image_caption.replace("\\&quot;", "\"")
-				)
+			if REDDIT_PREVIEW_REGEX.find(&image_caption).is_none() {
+				image_caption = image_caption.replace("\\&quot;", "\"");
+				_image_replacement = format!("<figure><a href=\"{image_url}<img loading=\"lazy\" src=\"{image_url}</a><figcaption>{image_caption}</figcaption></figure>");
 			} else {
-				format!("<figure><a href=\"{image_url}<img loading=\"lazy\" src=\"{image_url}</a></figure>")
-			};
+				_image_replacement = format!("<figure><a href=\"{image_url}<img loading=\"lazy\" src=\"{image_url}</a></figure>");
+			}
 			/* In order to know if we're dealing with a normal or external preview we need to take a look at the first capture group of REDDIT_PREVIEW_REGEX
 			if it's preview we're dealing with something that needs /preview/pre, external-preview is /preview/external-pre, and i is /img */
-			let reddit_preview_regex_capture = REDDIT_PREVIEW_REGEX.captures(&text1).unwrap().get(1).map_or("", |m| m.as_str());
-			let _preview_type = match reddit_preview_regex_capture {
-				"preview" => "/preview/pre",
-				"external-preview" => "/preview/external-pre",
-				_ => "/img",
-			};
+			let reddit_preview_regex_capture = REDDIT_PREVIEW_REGEX.captures(&text1).unwrap().get(1).map_or("", |m| m.as_str()).to_string();
+			let mut _preview_type = String::new();
+			if reddit_preview_regex_capture == "preview" {
+				_preview_type = "/preview/pre".to_string();
+			} else if reddit_preview_regex_capture == "external-preview" {
+				_preview_type = "/preview/external-pre".to_string();
+			} else {
+				_preview_type = "/img".to_string();
+			}
 			text1 = REDDIT_PREVIEW_REGEX
 				.replace(&text1, format!("{_preview_type}$2"))
 				.replace(&image_to_replace, &_image_replacement)
+				.to_string()
 		}
 	}
-}
-static REDDIT_EMOTE_LINK_REGEX: Lazy<Regex> = Lazy::new(|| Regex::new(r#"https://reddit-econ-prod-assets-permanent.s3.amazonaws.com/asset-manager/(.*)"#).unwrap());
-static REDDIT_EMOTE_ID_NUMBER_REGEX: Lazy<Regex> = Lazy::new(|| Regex::new(r#""emote\|.*\|(.*)""#).unwrap());
-pub fn rewrite_emotes(media_metadata: &Value, comment: String) -> String {
-	/* Create the paths we'll use to look for our data inside the json.
-	Because we don't know the name of any given emote we use a wildcard to parse them. */
-	let link_path = JsonPath::parse("$[*].s.u").expect("valid JSON Path");
-	let id_path = JsonPath::parse("$[*].id").expect("valid JSON Path");
-	let size_path = JsonPath::parse("$[*].s.y").expect("valid JSON Path");
-	let link_nodes = media_metadata.json_path(&link_path);
-	let id_nodes = media_metadata.json_path(&id_path);
-	let mut id_vec = Vec::new();
-	let mut link_vec = Vec::new();
-	for current_id in id_nodes {
-		id_vec.push(current_id)
-	}
-	for current_link in link_nodes {
-		link_vec.push(current_link)
-	}
-	/* Set index to the length of link_vec.
-	This is one larger than we'll actually be looking at, but we correct that later */
-	let mut index = link_vec.len();
-	let mut comment = comment;
-	/* Loop until index hits zero.
-	This also prevents us from trying to do anything on an empty vector */
-	while index != 0 {
-		/* Subtract 1 from index to get the real index we should be looking at.
-		Then continue on each subsequent loop to continue until we hit the last entry in the vector.
-		This is how we get this to deal with multiple emotes in a single message and properly replace each ID with it's link */
-		index -= 1;
-		let current_id = id_vec[index].to_string();
-		/* The ID number can be multiple lengths, so we capture it with regex.
-		We also want to only attempt anything when we get matches to avoid panicking */
-		if let Some(id_capture) = REDDIT_EMOTE_ID_NUMBER_REGEX.captures(&current_id) {
-			let id = format!(":{}:", &id_capture[1]);
-			let link = link_vec[index].to_string();
-			if let Some(link_capture) = REDDIT_EMOTE_LINK_REGEX.captures(&link) {
-				/* Reddit sends a size for the image based on whether it's alone or accompanied by text.
-				It's a good idea and makes everything look nicer, so we'll do the same. */
-				let size = media_metadata.json_path(&size_path).first().unwrap().to_string();
-				let to_replace_with = format!(
-					"<img loading=\"lazy\" src=\"/emote/{} width=\"{size}\" height=\"{size}\" style=\"vertical-align:text-bottom\">",
-					&link_capture[1]
-				);
-				comment = comment.replace(&id, &to_replace_with);
-			}
-		}
-	}
-	comment = render_bullet_lists(&comment);
-	rewrite_urls(&comment)
 }
 pub fn format_num(num: i64) -> (String, String) {
 	let truncated = if num >= 1_000_000 || num <= -1_000_000 {
 		format!("{:.1}m", num as f64 / 1_000_000.0)
 	} else if num >= 1000 || num <= -1000 {
 		format!("{:.1}k", num as f64 / 1_000.0)
 	} else {
 		num.to_string()
 	};
 	(truncated, num.to_string())
@@ -1039,45 +835,22 @@
 	let url = req.uri().to_string();
 	let body = ErrorTemplate {
 		msg: msg.to_string(),
 		prefs: Preferences::new(&req),
 		url,
 	}
 	.render()
 	.unwrap_or_default();
 	Ok(Response::builder().status(404).header("content-type", "text/html").body(body.into()).unwrap_or_default())
 }
-pub async fn info(req: Request<Body>, msg: &str) -> Result<Response<Body>, String> {
-	let url = req.uri().to_string();
-	let body = InfoTemplate {
-		msg: msg.to_string(),
-		prefs: Preferences::new(&req),
-		url,
-	}
-	.render()
-	.unwrap_or_default();
-	Ok(Response::builder().status(200).header("content-type", "text/html").body(body.into()).unwrap_or_default())
-}
 pub fn sfw_only() -> bool {
 	match get_setting("REDLIB_SFW_ONLY") {
-		Some(val) => val == "on",
-		None => false,
-	}
-}
-pub fn enable_rss() -> bool {
-	match get_setting("REDLIB_ENABLE_RSS") {
-		Some(val) => val == "on",
-		None => false,
-	}
-}
-pub fn disable_indexing() -> bool {
-	match get_setting("REDLIB_ROBOTS_DISABLE_INDEXING") {
 		Some(val) => val == "on",
 		None => false,
 	}
 }
 pub fn should_be_nsfw_gated(req: &Request<Body>, req_url: &str) -> bool {
 	let sfw_instance = sfw_only();
 	let gate_nsfw = (setting(req, "show_nsfw") != "on") || sfw_instance;
 	let bypass_gate = !sfw_instance && req_url.contains("&bypass_nsfw_landing");
 	gate_nsfw && !bypass_gate
 }
@@ -1096,44 +869,23 @@
 	let body = NSFWLandingTemplate {
 		res: resource,
 		res_type,
 		prefs: Preferences::new(&req),
 		url: req_url,
 	}
 	.render()
 	.unwrap_or_default();
 	Ok(Response::builder().status(403).header("content-type", "text/html").body(body.into()).unwrap_or_default())
 }
-pub fn url_path_basename(path: &str) -> String {
-	let url_result = Url::parse(format!("https://libredd.it/{path}").as_str());
-	if url_result.is_err() {
-		path.to_string()
-	} else {
-		let mut url = url_result.unwrap();
-		url.path_segments_mut().unwrap().pop_if_empty();
-		url.path_segments().unwrap().next_back().unwrap().to_string()
-	}
-}
-pub fn get_post_url(post: &Post) -> String {
-	if let Some(out_url) = &post.out_url {
-		if out_url.starts_with("/r/") {
-			format!("{}{}", config::get_setting("REDLIB_FULL_URL").unwrap_or_default(), out_url)
-		} else {
-			out_url.to_string()
-		}
-	} else {
-		format!("{}{}", config::get_setting("REDLIB_FULL_URL").unwrap_or_default(), post.permalink)
-	}
-}
 #[cfg(test)]
 mod tests {
-	use super::{format_num, format_url, rewrite_urls, Preferences};
+	use super::{format_num, format_url, rewrite_urls};
 	#[test]
 	fn format_num_works() {
 		assert_eq!(format_num(567), ("567".to_string(), "567".to_string()));
 		assert_eq!(format_num(1234), ("1.2k".to_string(), "1234".to_string()));
 		assert_eq!(format_num(1999), ("2.0k".to_string(), "1999".to_string()));
 		assert_eq!(format_num(1001), ("1.0k".to_string(), "1001".to_string()));
 		assert_eq!(format_num(1_999_999), ("2.0m".to_string(), "1999999".to_string()));
 	}
 	#[test]
 	fn rewrite_urls_removes_backslashes_and_rewrites_url() {
@@ -1179,155 +931,44 @@
 		assert_eq!(
 			format_url("https://www.redditstatic.com/marketplace-assets/v1/core/emotes/snoomoji_emotes/free_emotes_pack/shrug.gif"),
 			"/static/marketplace-assets/v1/core/emotes/snoomoji_emotes/free_emotes_pack/shrug.gif"
 		);
 		assert_eq!(format_url(""), "");
 		assert_eq!(format_url("self"), "");
 		assert_eq!(format_url("default"), "");
 		assert_eq!(format_url("nsfw"), "");
 		assert_eq!(format_url("spoiler"), "");
 	}
-	#[test]
-	fn serialize_prefs() {
-		let prefs = Preferences {
-			available_themes: vec![],
-			theme: "laserwave".to_owned(),
-			front_page: "default".to_owned(),
-			layout: "compact".to_owned(),
-			wide: "on".to_owned(),
-			blur_spoiler: "on".to_owned(),
-			show_nsfw: "off".to_owned(),
-			blur_nsfw: "on".to_owned(),
-			hide_hls_notification: "off".to_owned(),
-			video_quality: "best".to_owned(),
-			hide_sidebar_and_summary: "off".to_owned(),
-			use_hls: "on".to_owned(),
-			autoplay_videos: "on".to_owned(),
-			fixed_navbar: "on".to_owned(),
-			disable_visit_reddit_confirmation: "on".to_owned(),
-			comment_sort: "confidence".to_owned(),
-			post_sort: "top".to_owned(),
-			subscriptions: vec!["memes".to_owned(), "mildlyinteresting".to_owned()],
-			filters: vec![],
-			hide_awards: "off".to_owned(),
-			hide_score: "off".to_owned(),
-			remove_default_feeds: "off".to_owned(),
-		};
-		let urlencoded = serde_urlencoded::to_string(prefs).expect("Failed to serialize Prefs");
-		assert_eq!(urlencoded, "theme=laserwave&front_page=default&layout=compact&wide=on&blur_spoiler=on&show_nsfw=off&blur_nsfw=on&hide_hls_notification=off&video_quality=best&hide_sidebar_and_summary=off&use_hls=on&autoplay_videos=on&fixed_navbar=on&disable_visit_reddit_confirmation=on&comment_sort=confidence&post_sort=top&subscriptions=memes%2Bmildlyinteresting&filters=&hide_awards=off&hide_score=off&remove_default_feeds=off");
-	}
 }
 #[test]
 fn test_rewriting_emoji() {
 	let input = r#"<div class="md"><p>How can you have such hard feelings towards a license? <img src="https://www.redditstatic.com/marketplace-assets/v1/core/emotes/snoomoji_emotes/free_emotes_pack/shrug.gif" width="20" height="20" style="vertical-align:middle"> Let people use what license they want, and BSD is one of the least restrictive ones AFAIK.</p>"#;
 	let output = r#"<div class="md"><p>How can you have such hard feelings towards a license? <img src="/static/marketplace-assets/v1/core/emotes/snoomoji_emotes/free_emotes_pack/shrug.gif" width="20" height="20" style="vertical-align:middle"> Let people use what license they want, and BSD is one of the least restrictive ones AFAIK.</p>"#;
 	assert_eq!(rewrite_urls(input), output);
 }
 #[tokio::test(flavor = "multi_thread")]
 async fn test_fetching_subreddit_quarantined() {
 	let subreddit = Post::fetch("/r/drugs", true).await;
 	assert!(subreddit.is_ok());
 	assert!(!subreddit.unwrap().0.is_empty());
 }
 #[tokio::test(flavor = "multi_thread")]
 async fn test_fetching_nsfw_subreddit() {
-	let subreddit = Post::fetch("/r/gonwild", false).await;
+	let subreddit = Post::fetch("/r/randnsfw", false).await;
 	assert!(subreddit.is_ok());
 	assert!(!subreddit.unwrap().0.is_empty());
 }
 #[tokio::test(flavor = "multi_thread")]
 async fn test_fetching_ws() {
 	let subreddit = Post::fetch("/r/popular", false).await;
 	assert!(subreddit.is_ok());
 	for post in subreddit.unwrap().0 {
 		assert!(post.ws_url.starts_with("wss://k8s-lb.wss.redditmedia.com/link/"));
 	}
 }
 #[test]
 fn test_rewriting_image_links() {
 	let input =
 		r#"<p><a href="https://preview.redd.it/6awags382xo31.png?width=2560&amp;format=png&amp;auto=webp&amp;s=9c563aed4f07a91bdd249b5a3cea43a79710dcfc">caption 1</a></p>"#;
-	let output = r#"<figure><a href="/preview/pre/6awags382xo31.png?width=2560&amp;format=png&amp;auto=webp&amp;s=9c563aed4f07a91bdd249b5a3cea43a79710dcfc"><img loading="lazy" src="/preview/pre/6awags382xo31.png?width=2560&amp;format=png&amp;auto=webp&amp;s=9c563aed4f07a91bdd249b5a3cea43a79710dcfc"></a><figcaption>caption 1</figcaption></figure>"#;
+	let output = r#"<p><figure><a href="/preview/pre/6awags382xo31.png?width=2560&amp;format=png&amp;auto=webp&amp;s=9c563aed4f07a91bdd249b5a3cea43a79710dcfc"><img loading="lazy" src="/preview/pre/6awags382xo31.png?width=2560&amp;format=png&amp;auto=webp&amp;s=9c563aed4f07a91bdd249b5a3cea43a79710dcfc"></a><figcaption>caption 1</figcaption></figure></p"#;
 	assert_eq!(rewrite_urls(input), output);
 }
-#[test]
-fn test_url_path_basename() {
-	assert_eq!(url_path_basename("/first/last"), "last");
-	assert_eq!(url_path_basename("/first/last/"), "last");
-	assert_eq!(url_path_basename("/first/last/?some=query"), "last");
-	assert_eq!(url_path_basename("/cdn/image.jpg"), "image.jpg");
-	assert_eq!(url_path_basename("https://doma.in/first/last"), "last");
-	assert_eq!(url_path_basename("/"), "");
-}
-#[test]
-fn test_rewriting_emotes() {
-	let json_input = serde_json::from_str(r#"{"emote|t5_31hpy|2028":{"e":"Image","id":"emote|t5_31hpy|2028","m":"image/png","s":{"u":"https://reddit-econ-prod-assets-permanent.s3.amazonaws.com/asset-manager/t5_31hpy/PW6WsOaLcd.png","x":60,"y":60},"status":"valid","t":"sticker"}}"#).expect("Valid JSON");
-	let comment_input = r#"<div class="comment_body "><div class="md"><p>:2028:</p></div></div>"#;
-	let output = r#"<div class="comment_body "><div class="md"><p><img loading="lazy" src="/emote/t5_31hpy/PW6WsOaLcd.png" width="60" height="60" style="vertical-align:text-bottom"></p></div></div>"#;
-	assert_eq!(rewrite_emotes(&json_input, comment_input.to_string()), output);
-}
-#[test]
-fn test_rewriting_bullet_list() {
-	let input = r#"<div class="md"><p>Hi, I&#39;ve bought this very same monitor and found no calibration whatsoever. I have an ICC profile that has been set up since I&#39;ve installed its driver from the LG website and it works ok. I also used <a href="http://www.lagom.nl/lcd-test/">http://www.lagom.nl/lcd-test/</a> to calibrate it. After some good tinkering I&#39;ve found the following settings + the color profile from the driver gets me past all the tests perfectly:
-- Brightness 50 (still have to settle on this one, it&#39;s personal preference, it controls the backlight, not the colors)
-- Contrast 70 (which for me was the default one)
-- Picture mode Custom
-- Super resolution + Off (it looks horrible anyway)
-- Sharpness 50 (default one I think)
-- Black level High (low messes up gray colors)
-- DFC Off 
-- Response Time Middle (personal preference, <a href="https://www.blurbusters.com/">https://www.blurbusters.com/</a> show horrible overdrive with it on high)
-- Freesync doesn&#39;t matter
-- Black stabilizer 50
-- Gamma setting on 0 
-- Color Temp Medium
-How`s your monitor by the way? Any IPS bleed whatsoever? I either got lucky or the panel is pretty good, 0 bleed for me, just the usual IPS glow. How about the pixels? I see the pixels even at one meter away, especially on Microsoft Edge&#39;s icon for example, the blue background is just blocky, don&#39;t know why.</p>
-</div>"#;
-	let output = r#"<div class="md"><p>Hi, I&#39;ve bought this very same monitor and found no calibration whatsoever. I have an ICC profile that has been set up since I&#39;ve installed its driver from the LG website and it works ok. I also used <a href="http://www.lagom.nl/lcd-test/">http://www.lagom.nl/lcd-test/</a> to calibrate it. After some good tinkering I&#39;ve found the following settings + the color profile from the driver gets me past all the tests perfectly:
-<ul><li>Brightness 50 (still have to settle on this one, it&#39;s personal preference, it controls the backlight, not the colors)</li><li>Contrast 70 (which for me was the default one)</li><li>Picture mode Custom</li><li>Super resolution + Off (it looks horrible anyway)</li><li>Sharpness 50 (default one I think)</li><li>Black level High (low messes up gray colors)</li><li>DFC Off </li><li>Response Time Middle (personal preference, <a href="https://www.blurbusters.com/">https://www.blurbusters.com/</a> show horrible overdrive with it on high)</li><li>Freesync doesn&#39;t matter</li><li>Black stabilizer 50</li><li>Gamma setting on 0 </li><li>Color Temp Medium</li></ul>
-How`s your monitor by the way? Any IPS bleed whatsoever? I either got lucky or the panel is pretty good, 0 bleed for me, just the usual IPS glow. How about the pixels? I see the pixels even at one meter away, especially on Microsoft Edge&#39;s icon for example, the blue background is just blocky, don&#39;t know why.</p>
-</div>"#;
-	assert_eq!(render_bullet_lists(input), output);
-}
-#[test]
-fn test_default_prefs_serialization_loop_json() {
-	let prefs = Preferences::default();
-	let serialized = serde_json::to_string(&prefs).unwrap();
-	let deserialized: Preferences = serde_json::from_str(&serialized).unwrap();
-	assert_eq!(prefs, deserialized);
-}
-#[test]
-fn test_default_prefs_serialization_loop_bincode() {
-	let prefs = Preferences::default();
-	test_round_trip(&prefs, false);
-	test_round_trip(&prefs, true);
-}
-static KNOWN_GOOD_CONFIGS: &[&str] = &[
-	"à°´Ó…Î²Ã˜Ã˜ÒžÃ‰á€á‚¢Õ±Ä¬à¼§È’Ê¯à¤ŒÔ”Óµà­®à¼",
-	"à¨§ÕŠÎ¥Ã€ÃƒÇŽÆ±Ð“Û¸à¶£à´®Ä–à¸¤á‚™ÊŸà¸²Ãºà»œÏ¾à¯É¥à¦€Äœà»ƒàª¹à½žàª Ñ«Ò²É‚à°™à¿”Ç²àª‰Æ²ÓŸÓ»Ä»à¸…ÎœÎ´à»–ÔœÇ—á€–á€„Æ¦Æ¡à§¶Ä„à¯©Ô¹Ê›à¹ƒÐ›Êƒà·Ð°Î",
-	"à¨§Ô©Î¥Ã€ÃƒÃŽÅ à±­àµ©à¶”á‚ Ï¼Ò­Ã¶ÒªÆ¸Õ¼àª‡Ô¾à¥áƒœÉ”àº²Ç’ÕÒ°à¤šà¯¨à²–àº¡ÅƒÐ‰Åà½‘Æ¦à¹™Ï©à¦à° Èà´½Ð¹Ê®áƒ¯à¶’Ï°à¤³Õ‹à¯®àºªà§µà¤ŽÎ¦Ñ§à¨¹à²§à¬ŸÆ™Åƒà¥©Ã®à¼¦ÅŒá€•Õ²à¤¯ÆŸà¹Òœà¼",
-];
-#[test]
-fn test_known_good_configs_deserialization() {
-	for config in KNOWN_GOOD_CONFIGS {
-		let bytes = base2048::decode(config).unwrap();
-		let decompressed = deflate_decompress(bytes).unwrap();
-		assert!(bincode::deserialize::<Preferences>(&decompressed).is_ok());
-	}
-}
-#[test]
-fn test_known_good_configs_full_round_trip() {
-	for config in KNOWN_GOOD_CONFIGS {
-		let bytes = base2048::decode(config).unwrap();
-		let decompressed = deflate_decompress(bytes).unwrap();
-		let prefs: Preferences = bincode::deserialize(&decompressed).unwrap();
-		test_round_trip(&prefs, false);
-		test_round_trip(&prefs, true);
-	}
-}
-fn test_round_trip(input: &Preferences, compression: bool) {
-	let serialized = bincode::serialize(input).unwrap();
-	let compressed = if compression { deflate_compress(serialized).unwrap() } else { serialized };
-	let decompressed = if compression { deflate_decompress(compressed).unwrap() } else { compressed };
-	let deserialized: Preferences = bincode::deserialize(&decompressed).unwrap();
-	assert_eq!(*input, deserialized);
-}

--- a/static/check_update.js
+++ b//dev/null
@@ -1,48 +0,0 @@
-async function checkInstanceUpdateStatus() {
-    try {
-        const response = await fetch('/commits.atom');
-        const text = await response.text();
-        const parser = new DOMParser();
-        const xmlDoc = parser.parseFromString(text, "application/xml");
-        const entries = xmlDoc.getElementsByTagName('entry');
-        const localCommit = document.getElementById('git_commit').dataset.value;
-        let statusMessage = '';
-        if (entries.length > 0) {
-            const commitHashes = Array.from(entries).map(entry => {
-                const id = entry.getElementsByTagName('id')[0].textContent;
-                return id.split('/').pop();
-            });
-            const commitIndex = commitHashes.indexOf(localCommit);
-            if (commitIndex === 0) {
-                statusMessage = 'âœ… Instance is up to date.';
-            } else if (commitIndex > 0) {
-                statusMessage = `âš ï¸ This instance is not up to date and is ${commitIndex} commits old. Test and confirm on an up-to-date instance before reporting.`;
-                document.getElementById('error-318').remove();
-            } else {
-                statusMessage = `âš ï¸ This instance is not up to date and is at least ${commitHashes.length} commits old. Test and confirm on an up-to-date instance before reporting.`;
-                document.getElementById('error-318').remove();
-            }
-        } else {
-            statusMessage = 'âš ï¸ Unable to fetch commit information.';
-        }
-        document.getElementById('update-status').innerText = statusMessage;
-    } catch (error) {
-        console.error('Error fetching commits:', error);
-        document.getElementById('update-status').innerText = 'âš ï¸ Error checking update status: ' + error;
-    }
-}
-async function checkOtherInstances() {
-    try {
-        const response = await fetch('/instances.json');
-        const data = await response.json();
-        const randomInstance = data.instances[Math.floor(Math.random() * data.instances.length)];
-        const instanceUrl = randomInstance.url;
-        document.getElementById('random-instance').href = instanceUrl + window.location.pathname;
-        document.getElementById('random-instance').innerText = "Visit Random Instance";
-    } catch (error) {
-        console.error('Error fetching instances:', error);
-        document.getElementById('update-status').innerText = 'âš ï¸ Error checking other instances: ' + error;
-    }
-}
-window.addEventListener('load', checkOtherInstances);
-checkInstanceUpdateStatus();

--- a/static/copy.js
+++ b//dev/null
@@ -1,7 +0,0 @@
-async function copy() {
-    await navigator.clipboard.writeText(document.getElementById('bincode_str').value);
-}
-async function set_listener() {
-    document.getElementById('copy').addEventListener('click', copy);
-}
-window.addEventListener('load', set_listener);

--- a/static/playHLSVideo.js
+++ b//dev/null
@@ -1,105 +0,0 @@
-(function () {
-    const configElement = document.getElementById('video_quality');
-    const qualitySetting = configElement.getAttribute('data-value');
-    if (Hls.isSupported()) {
-        var videoSources = document.querySelectorAll("video source[type='application/vnd.apple.mpegurl']");
-        videoSources.forEach(function (source) {
-            var playlist = source.src;
-            var oldVideo = source.parentNode;
-            var autoplay = oldVideo.classList.contains("hls_autoplay");
-            if (oldVideo.canPlayType(source.type) === "probably") {
-                if (autoplay) {
-                    oldVideo.play();
-                }
-                return;
-            }
-            var newVideo = oldVideo.cloneNode(true);
-            var allSources = newVideo.querySelectorAll("source");
-            allSources.forEach(function (source) {
-                source.remove();
-            });
-            newVideo.src = "about:blank";
-            oldVideo.parentNode.replaceChild(newVideo, oldVideo);
-            function getIndexOfDefault(length) {
-                switch (qualitySetting) {
-                    case 'best':
-                        return length - 1;
-                    case 'medium':
-                        return Math.floor(length / 2);
-                    case 'worst':
-                        return 0;
-                    default:
-                        return length - 1;
-                }
-            }
-            function initializeHls() {
-                newVideo.removeEventListener('play', initializeHls);
-                var hls = new Hls({ autoStartLoad: false });
-                hls.loadSource(playlist);
-                hls.attachMedia(newVideo);
-                hls.on(Hls.Events.MANIFEST_PARSED, function () {
-                    hls.loadLevel = getIndexOfDefault(hls.levels.length);
-                    var availableLevels = hls.levels.map(function(level) {
-                        return {
-                            height: level.height,
-                            width: level.width,
-                            bitrate: level.bitrate,
-                        };
-                    });
-                    addQualitySelector(newVideo, hls, availableLevels);
-                    hls.startLoad();
-                    newVideo.play();
-                });
-                hls.on(Hls.Events.ERROR, function (event, data) {
-                    var errorType = data.type;
-                    var errorFatal = data.fatal;
-                    if (errorFatal) {
-                        switch (errorType) {
-                            case Hls.ErrorType.NETWORK_ERROR:
-                                hls.startLoad();
-                                break;
-                            case Hls.ErrorType.MEDIA_ERROR:
-                                hls.recoverMediaError();
-                                break;
-                            default:
-                                hls.destroy();
-                                break;
-                        }
-                    }
-                    console.error("HLS error", data);
-                });
-            }
-            function addQualitySelector(videoElement, hlsInstance, availableLevels) {
-                var qualitySelector = document.createElement('select');
-                qualitySelector.classList.add('quality-selector');
-                var defaultIndex = getIndexOfDefault(availableLevels.length);
-                availableLevels.forEach(function (level, index) {
-                    var option = document.createElement('option');
-                    option.value = index.toString();
-                    var bitrate = (level.bitrate / 1_000).toFixed(0);
-                    option.text = level.height + 'p (' + bitrate + ' kbps)';
-                    if (index === defaultIndex) {
-                        option.selected = "selected";
-                    }
-                    qualitySelector.appendChild(option);
-                });
-                qualitySelector.selectedIndex = defaultIndex;
-                qualitySelector.addEventListener('change', function () {
-                    var selectedIndex = qualitySelector.selectedIndex;
-                    hlsInstance.nextLevel = selectedIndex;
-                    hlsInstance.startLoad();
-                });
-                videoElement.parentNode.appendChild(qualitySelector);
-            }
-            newVideo.addEventListener('play', initializeHls);
-            if (autoplay) {
-                newVideo.play();
-            }
-        });
-    } else {
-        var videos = document.querySelectorAll("video.hls_autoplay");
-        videos.forEach(function (video) {
-            video.setAttribute("autoplay", "");
-        });
-    }
-})();
