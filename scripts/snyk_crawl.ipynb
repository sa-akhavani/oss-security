{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_repo_link(link):\n",
    "    try:\n",
    "        link_args = link.split('/')\n",
    "        author = link_args[3]\n",
    "        repo_name = link_args[4]\n",
    "        return f'https://github.com/{author}/{repo_name}'\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_github_links(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    github_links = set()\n",
    "    \n",
    "    # Find all <a> tags\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        if 'https://github.com' in href and 'github.com/snyk' not in href.lower():\n",
    "            repo_link = get_repo_link(href)\n",
    "            if repo_link:\n",
    "                github_links.add(repo_link)\n",
    "    \n",
    "    return github_links\n",
    "\n",
    "def process_saved_html_files(directory):\n",
    "    all_github_links = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.html'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "            \n",
    "            # Extract GitHub links\n",
    "            github_links = extract_github_links(html_content)\n",
    "            \n",
    "            if github_links:\n",
    "                vuln_id = filename.split('.')[0]  # Get the vulnerability ID from the filename\n",
    "                all_github_links[vuln_id] = github_links\n",
    "    \n",
    "    return all_github_links\n",
    "\n",
    "# Directory where the saved HTML files are located\n",
    "html_files_directory = 'snyk_crawls/htmls/'\n",
    "\n",
    "# Extract all GitHub links from the HTML files\n",
    "github_links_by_vuln = process_saved_html_files(html_files_directory)\n",
    "\n",
    "# Print the results\n",
    "for vuln_id, links in github_links_by_vuln.items():\n",
    "    if len(links) > 1:\n",
    "        print(vuln_id)\n",
    "    # print(f\"Vulnerability ID: {vuln_id}\")\n",
    "    # for link in links:\n",
    "    #     print(f\"  GitHub Link: {link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('snyk_data_git_urls.pickle', 'wb') as pickle_file:\n",
    "        pickle.dump(github_links_by_vuln, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_vuln_rows_disclosed(html_content):\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Array to store extracted vulnerabilities\n",
    "    vulnerabilities = []\n",
    "\n",
    "    # Find all <li> elements corresponding to vulnerabilities\n",
    "    vuln_items = soup.find_all('li', {'data-snyk-test': 'ProprietaryVulns: item'})\n",
    "\n",
    "    # Extract the required data for each item\n",
    "    for item in vuln_items:\n",
    "        severity = item.find('span', class_='vue--severity__label').text.strip()\n",
    "        _type = item.find('a').text.strip()\n",
    "        link = item.find('a')['href']\n",
    "        _id = link.split('/')[-1]\n",
    "        description = item.find('p').text.strip().split(' in ')[-1].strip()[:-2]\n",
    "        pkg = description.split()[0]\n",
    "        platform = description.split()[1][1:-1]\n",
    "        publication_date = item.find('div', {'data-snyk-test': 'ProprietaryVulns: publicationTime'}).text.strip().split('\\\\n')[1].strip()\n",
    "\n",
    "\n",
    "        vuln = [_id, _type, pkg, 'NA', platform, publication_date]\n",
    "        # Append the extracted data as a dictionary to the list\n",
    "        vulnerabilities.append(vuln)\n",
    "\n",
    "    # # Output the extracted vulnerabilities\n",
    "    # for vuln in vulnerabilities:\n",
    "    #     print(vuln)\n",
    "    return vulnerabilities\n",
    "\n",
    "\n",
    "html = str(curl('https://security.snyk.io/disclosed-vulnerabilities/8'))\n",
    "rows = get_vuln_rows_disclosed(html)\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "def curl(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    response = urllib.request.urlopen(req)\n",
    "    data = response.read()\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "class SnykNPM(HTMLParser):\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.counter = 0\n",
    "\n",
    "        self.row_complete = False\n",
    "\n",
    "        self.vulns = []\n",
    "        self.packages = []\n",
    "        self.dates = []\n",
    "        self.vuln_flag = False\n",
    "        self.package_flag = False\n",
    "        self.date_flag = False\n",
    "        \n",
    "        self.table_row_flag = False\n",
    "        self.rows = [[]]\n",
    "\n",
    "        self.tag = None\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        self.tag = tag\n",
    "        self.attrs = attrs\n",
    "\n",
    "        if tag != 'tr':\n",
    "            # self.table_row_flag = False\n",
    "            return\n",
    "\n",
    "        for attr in attrs:\n",
    "            if attr[0] == 'class':\n",
    "                # print(attr)\n",
    "                if 'vue--table__row' == attr[1]:\n",
    "                    self.table_row_flag = True\n",
    "                else:\n",
    "                    self.table_row_flag = False\n",
    "\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if not self.table_row_flag:\n",
    "            return\n",
    "\n",
    "        if len(data) < 2:\n",
    "            return\n",
    "        if not self.row_complete:\n",
    "            if '\\\\n' in data.strip():\n",
    "                data = data.split('\\\\n')[1].strip()\n",
    "            self.rows[-1].append(data)\n",
    "        else:\n",
    "            if '\\\\n' in data.strip():\n",
    "                data = data.split('\\\\n')[1].strip()\n",
    "\n",
    "            vuln_id = None\n",
    "            for attr in self.attrs:\n",
    "                if attr[0] == 'href':\n",
    "                    vuln_id = attr[1].split('/')[-1]\n",
    "            # print(self.tag, data, )\n",
    "            self.rows.append([vuln_id, data])\n",
    "            self.row_complete = False\n",
    "        \n",
    "        if '2024' in data or '2023' in data  or '2022' in data  or '2021' in data or 'PUBLISHED' in data:\n",
    "            self.row_complete = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vuln_rows(html):\n",
    "    # print(html)\n",
    "    parser = SnykNPM()\n",
    "    parser.feed(html)\n",
    "    return parser.rows\n",
    "\n",
    "html = str(curl('https://security.snyk.io/vuln/golang/2'))\n",
    "rows = get_vuln_rows(html)\n",
    "# print(len(rows))\n",
    "\n",
    "res = []\n",
    "for t in rows:\n",
    "    print(t)\n",
    "    if len(t) == 6:\n",
    "        t.insert(3, None)\n",
    "    if len(t) == 7:\n",
    "        print(t)\n",
    "        res.append(t)\n",
    "\n",
    "print(len(res))\n",
    "\n",
    "# html = str(curl('https://www.npmjs.com/package/jsreport'))\n",
    "# n_dep = get_n_dependent_from_npm_html(html)\n",
    "# print(n_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnykVuln(HTMLParser):\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.score = None\n",
    "        self.CWE = None\n",
    "        self.CVE = None\n",
    "\n",
    "        self.detailsbox = []\n",
    "\n",
    "\n",
    "        self.cve_flag = False\n",
    "        self.cwe_flag = False\n",
    "        self.score_flag = False\n",
    "        self.detailsbox_flag = False\n",
    "\n",
    "\n",
    "        self.tag = None\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        self.tag = tag\n",
    "        self.attrs = attrs\n",
    "\n",
    "        for attr in attrs:\n",
    "            if attr[0] == 'class':\n",
    "                if 'cve' == attr[1]:\n",
    "                    self.cve_flag = True\n",
    "                else:\n",
    "                    if tag == 'a' and self.cve_flag:\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.cve_flag = False\n",
    "            \n",
    "            if attr[0] == 'data-snyk-test':\n",
    "                if 'VendorCvssCard: Badge' in attr[1]:\n",
    "                    self.score_flag = True\n",
    "                else:\n",
    "                    if tag == 'span' and self.score_flag:\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.score_flag = False\n",
    "\n",
    "                if 'vuln detailsbox value' == attr[1]:\n",
    "                    self.detailsbox_flag = True\n",
    "                else:\n",
    "                    self.detailsbox_flag = False\n",
    "\n",
    "                if 'cwe' == attr[1]:\n",
    "                    self.cwe_flag = True\n",
    "                else:\n",
    "                    if tag == 'a' and self.cwe_flag:\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.cwe_flag = False\n",
    "\n",
    "            \n",
    "    def handle_data(self, data):\n",
    "        if self.cve_flag:\n",
    "            self.CVE = data\n",
    "            # print('cve', data)\n",
    "        \n",
    "        if self.cwe_flag:\n",
    "            # print('cwe', data)\n",
    "            self.CWE = data\n",
    "        \n",
    "        if self.detailsbox_flag:\n",
    "            if len(data.strip()) > 1:\n",
    "                self.detailsbox.append(data)\n",
    "            # print(self.detailsbox)\n",
    "\n",
    "        if self.score_flag:\n",
    "            tmp = data.split('\\\\n')\n",
    "            if len(tmp)> 1:\n",
    "                value = tmp[1].strip()\n",
    "                if 'Rec' not in value:\n",
    "                    if not self.score:\n",
    "                        self.score = value\n",
    "\n",
    "def get_vuln_detail(html):\n",
    "    # print(html)\n",
    "    parser = SnykVuln()\n",
    "    parser.feed(html)\n",
    "    return parser.score, parser.CVE, parser.CWE, parser.detailsbox\n",
    "\n",
    "html = str(curl('https://security.snyk.io/vuln/SNYK-JS-EVOLUTIONDS-7925464'))\n",
    "score, cve, cwe, detailsbox = get_vuln_detail(html)\n",
    "\n",
    "print(score, cve, cwe, detailsbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def clean_row(row):\n",
    "    id = row[0]\n",
    "    vuln = row[1]\n",
    "    pkg = row[2]\n",
    "    version = ''\n",
    "    for i in range(3, len(row)-2):\n",
    "        version += str(row[i])\n",
    "    platform = row[-2]\n",
    "    date = row[-1]\n",
    "\n",
    "    return [id, vuln, pkg, version, platform, date]\n",
    "\n",
    "def first_crawl_disclosed_dataset():\n",
    "    result = []\n",
    "    \n",
    "    urls = [\n",
    "        'https://security.snyk.io/disclosed-vulnerabilities/',\n",
    "    ] + \\\n",
    "    [\n",
    "        f'https://security.snyk.io/disclosed-vulnerabilities/{i}' for i in range(2, 336+1)\n",
    "    ]\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            html = str(curl(url))\n",
    "            rows = get_vuln_rows_disclosed(html)\n",
    "\n",
    "            time.sleep(0.5)\n",
    "            for t in rows:\n",
    "                # new_row = clean_row(t)\n",
    "\n",
    "                result.append(t)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('error for url first crawl', url)\n",
    "        \n",
    "    print(len(result))\n",
    "\n",
    "    df = pd.DataFrame(result, columns=['ID', 'Vulnerability', 'Package', 'Version Range', 'Platform', 'Date'])\n",
    "    return df\n",
    "\n",
    "def first_crawl(platform, last_page_number):\n",
    "    platform_alt = platform\n",
    "    \n",
    "    if platform == 'golang':\n",
    "        platform_alt = 'go'\n",
    "    if platform == 'unmanaged':\n",
    "        platform_alt = 'unmanaged (c/c++)'\n",
    "    result = []\n",
    "    urls = [\n",
    "        f'https://security.snyk.io/vuln/{platform}/',\n",
    "    ] + \\\n",
    "    [\n",
    "        f'https://security.snyk.io/vuln/{platform}/{i}' for i in range(2, last_page_number+1)\n",
    "    ]\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            html = str(curl(url))\n",
    "            rows = get_vuln_rows(html)\n",
    "\n",
    "            time.sleep(1)\n",
    "            for t in rows:\n",
    "                # print(t)\n",
    "                if len(t) < 4:\n",
    "                    continue\n",
    "\n",
    "                if len(t) == 5:\n",
    "                    t.insert(3, None)\n",
    "\n",
    "                if t[-2].lower() == platform or t[-2].lower() == platform_alt:\n",
    "\n",
    "                    new_row = clean_row(t)\n",
    "\n",
    "                    result.append(new_row)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('error for url first crawl', url)\n",
    "        \n",
    "    print(len(result))\n",
    "\n",
    "    df = pd.DataFrame(result, columns=['ID', 'Vulnerability', 'Package', 'Version Range', 'Platform', 'Date'])\n",
    "    return df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_first_page_results(platform):\n",
    "    platform_alt = platform\n",
    "    \n",
    "    if platform == 'golang':\n",
    "        platform_alt = 'go'\n",
    "    if platform == 'uunmanaged':\n",
    "        platform_alt = 'unmanaged (c/c++)'\n",
    "\n",
    "    result = []\n",
    "    url = f'https://security.snyk.io/vuln/{platform}/'\n",
    "\n",
    "    html = str(curl(url))\n",
    "    rows = get_vuln_rows(html)\n",
    "\n",
    "    for t in rows:\n",
    "        # print(t)\n",
    "        if len(t) == 5:\n",
    "            t.insert(3, None)\n",
    "        \n",
    "        if t[-2].lower() == platform or t[-2].lower() == platform_alt:\n",
    "            new_row = clean_row(t)\n",
    "\n",
    "            result.append(new_row)\n",
    "\n",
    "    return result\n",
    "\n",
    "def update_details_by_ids(ids, df, base_url='https://security.snyk.io/vuln/'):\n",
    "    for id in ids:\n",
    "        try:\n",
    "            id_url = base_url + str(id)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            html = str(curl(id_url))\n",
    "            score, cve, cwe, detailsbox = get_vuln_detail(html)\n",
    "            \n",
    "            if len(detailsbox) < 4:\n",
    "                detailsbox.append('Unknown')\n",
    "\n",
    "            # Find the index of the row where 'ID' matches\n",
    "            row_index = df[df['ID'] == id].index\n",
    "\n",
    "            # Check if the row is found and update the relevant columns\n",
    "            \n",
    "            if not row_index.empty:\n",
    "                try:\n",
    "                    df.loc[row_index, 'score'] = score.replace('\\\\xc2\\\\xa0', '-')\n",
    "                except:\n",
    "                    print('Score Error. Url:', id_url)\n",
    "                df.loc[row_index, 'CWE'] = cwe\n",
    "                df.loc[row_index, 'CVE'] = cve\n",
    "                df.loc[row_index, 'published'] = detailsbox[1]\n",
    "                df.loc[row_index, 'disclosed'] = detailsbox[2]\n",
    "                df.loc[row_index, 'credit'] = detailsbox[3]\n",
    "        except:\n",
    "            print('Error in df update. Url:', id_url)\n",
    "\n",
    "\n",
    "        with open(f\"snyk_crawls/htmls/{id}.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(html[2:-1])\n",
    "    \n",
    "    return df\n",
    "        \n",
    "\n",
    "def update_the_list(platform):\n",
    "    # Load CSV into DataFrame\n",
    "    df = pd.read_csv(f'snyk_crawls/{platform}.csv')\n",
    "    \n",
    "    result = get_first_page_results(platform)\n",
    "    first_page_df = pd.DataFrame(result, columns=['ID', 'Vulnerability', 'Package', 'Version Range', 'Platform', 'Date'])\n",
    "\n",
    "    old_ids = df['ID'].tolist()\n",
    "    first_page_ids = first_page_df['ID'].tolist()\n",
    "\n",
    "    new_ids = [id for id in first_page_ids if id not in old_ids]\n",
    "\n",
    "    print('new_ids', len(new_ids), new_ids)\n",
    "\n",
    "    first_page_df = update_details_by_ids(new_ids, first_page_df)\n",
    "\n",
    "    # Concatenate the two DataFrames\n",
    "    new_df = pd.concat([df, first_page_df], ignore_index=True)\n",
    "\n",
    "    # Remove rows with duplicate 'id' values, keeping the first occurrence\n",
    "    df_unique = new_df.drop_duplicates(subset='ID')\n",
    "\n",
    "\n",
    "    df_unique.to_csv(f'snyk_crawls/{platform}.csv', index=False)\n",
    "    print(len(df_unique))\n",
    "    return df_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disclosed_res = first_crawl_disclosed_dataset()\n",
    "disclosed_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disclosed_res = first_crawl_disclosed_dataset()\n",
    "print(len(disclosed_res))\n",
    "\n",
    "ids = disclosed_res['ID'].tolist()\n",
    "disclosed_res = update_details_by_ids(ids, disclosed_res)\n",
    "disclosed_res.to_csv(f'snyk_crawls/disclosed_vulns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platforms = {\n",
    "    'npm': 30,\n",
    "    'pip': 30,\n",
    "    'maven': 30,\n",
    "    'composer': 30,\n",
    "    'golang': 30,\n",
    "    'cargo': 26,\n",
    "    'cocoapods': 30,\n",
    "    'nuget': 30,\n",
    "    'rubygems': 30,\n",
    "    # 'swift': 2,\n",
    "    'unmanaged': 30,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_platform = {}\n",
    "\n",
    "for platform, last_page_number in platforms.items():\n",
    "    print('..... Crawling', platform)\n",
    "    first_crawl_df = first_crawl(platform, last_page_number)\n",
    "    print('first crawl lengths', platform, len(first_crawl_df))\n",
    "    df_platform[platform] = first_crawl_df\n",
    "    \n",
    "    ids = first_crawl_df['ID'].tolist()\n",
    "    first_crawl_df = update_details_by_ids(ids, first_crawl_df)\n",
    "    first_crawl_df.to_csv(f'snyk_crawls/{platform}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_platform = {}\n",
    "\n",
    "# for platform, last_page_number in platforms.items():\n",
    "#     df_platform[platform] = update_the_list(platform)\n",
    "#     # # print('..... Crawling', platform)\n",
    "#     # # first_crawl_df = first_crawl(platform, last_page_number)\n",
    "#     # # print('first crawl lengths', platform, len(first_crawl_df))\n",
    "#     # # df_platform[platform] = first_crawl_df\n",
    "    \n",
    "#     # ids = first_crawl_df['ID'].tolist()\n",
    "#     # first_crawl_df = update_details_by_ids(ids, first_crawl_df)\n",
    "#     # first_crawl_df.to_csv(f'data/21-sep/{platform}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
